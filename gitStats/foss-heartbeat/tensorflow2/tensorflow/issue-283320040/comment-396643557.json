{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/396643557", "html_url": "https://github.com/tensorflow/tensorflow/issues/15488#issuecomment-396643557", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15488", "id": 396643557, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjY0MzU1Nw==", "user": {"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-12T16:00:56Z", "updated_at": "2018-06-12T16:00:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I just went over your example, and it should work without issues.</p>\n<p>My guess is that your inference code is not holding on to the flatbuffer bytes (or of the FlatBufferModel) for the whole duration of inference.</p>\n<p>Here's what I've done:</p>\n<ul>\n<li>Conversion:</li>\n</ul>\n<pre><code>bazel run tensorflow/contrib/lite/toco:toco -- --input_format=TENSORFLOW_GRAPHDEF \\\n   --input_file=frozen_graph.pb --inference_type=FLOAT --inference_input_type=FLOAT \\\n   --input_arrays=x --output_arrays=logit --input_shapes=1,1 \\\n   --output_file=frozen_graph.tflite\n</code></pre>\n<ul>\n<li>Accuracy (vs TF):</li>\n</ul>\n<pre><code>bazel run tensorflow/contrib/lite/testing:tflite_diff --  --tensorflow_model=frozen_graph.pb \\\n   --tflite_model=frozen_graph.tflite --input_layer=x --input_layer_type=float \\\n   --input_layer_shape=1,1 --output_layer=logit\n</code></pre>\n<ul>\n<li>Latency</li>\n</ul>\n<pre><code>bazel run tensorflow/contrib/lite/tools/benchmark:benchmark_model -- \\\n    --num_threads=4 --num_runs=1000  --graph=frozen_graph.tflite\n</code></pre>", "body_text": "I just went over your example, and it should work without issues.\nMy guess is that your inference code is not holding on to the flatbuffer bytes (or of the FlatBufferModel) for the whole duration of inference.\nHere's what I've done:\n\nConversion:\n\nbazel run tensorflow/contrib/lite/toco:toco -- --input_format=TENSORFLOW_GRAPHDEF \\\n   --input_file=frozen_graph.pb --inference_type=FLOAT --inference_input_type=FLOAT \\\n   --input_arrays=x --output_arrays=logit --input_shapes=1,1 \\\n   --output_file=frozen_graph.tflite\n\n\nAccuracy (vs TF):\n\nbazel run tensorflow/contrib/lite/testing:tflite_diff --  --tensorflow_model=frozen_graph.pb \\\n   --tflite_model=frozen_graph.tflite --input_layer=x --input_layer_type=float \\\n   --input_layer_shape=1,1 --output_layer=logit\n\n\nLatency\n\nbazel run tensorflow/contrib/lite/tools/benchmark:benchmark_model -- \\\n    --num_threads=4 --num_runs=1000  --graph=frozen_graph.tflite", "body": "I just went over your example, and it should work without issues. \r\n\r\nMy guess is that your inference code is not holding on to the flatbuffer bytes (or of the FlatBufferModel) for the whole duration of inference. \r\n\r\nHere's what I've done:\r\n\r\n- Conversion:\r\n```\r\nbazel run tensorflow/contrib/lite/toco:toco -- --input_format=TENSORFLOW_GRAPHDEF \\\r\n   --input_file=frozen_graph.pb --inference_type=FLOAT --inference_input_type=FLOAT \\\r\n   --input_arrays=x --output_arrays=logit --input_shapes=1,1 \\\r\n   --output_file=frozen_graph.tflite\r\n```\r\n- Accuracy (vs TF):\r\n```\r\nbazel run tensorflow/contrib/lite/testing:tflite_diff --  --tensorflow_model=frozen_graph.pb \\\r\n   --tflite_model=frozen_graph.tflite --input_layer=x --input_layer_type=float \\\r\n   --input_layer_shape=1,1 --output_layer=logit\r\n```\r\n- Latency\r\n```\r\nbazel run tensorflow/contrib/lite/tools/benchmark:benchmark_model -- \\\r\n    --num_threads=4 --num_runs=1000  --graph=frozen_graph.tflite\r\n```"}