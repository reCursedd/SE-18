{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354959628", "html_url": "https://github.com/tensorflow/tensorflow/issues/15488#issuecomment-354959628", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15488", "id": 354959628, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDk1OTYyOA==", "user": {"login": "ushnish", "id": 3603839, "node_id": "MDQ6VXNlcjM2MDM4Mzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3603839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ushnish", "html_url": "https://github.com/ushnish", "followers_url": "https://api.github.com/users/ushnish/followers", "following_url": "https://api.github.com/users/ushnish/following{/other_user}", "gists_url": "https://api.github.com/users/ushnish/gists{/gist_id}", "starred_url": "https://api.github.com/users/ushnish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ushnish/subscriptions", "organizations_url": "https://api.github.com/users/ushnish/orgs", "repos_url": "https://api.github.com/users/ushnish/repos", "events_url": "https://api.github.com/users/ushnish/events{/privacy}", "received_events_url": "https://api.github.com/users/ushnish/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-03T08:36:50Z", "updated_at": "2018-01-03T08:39:18Z", "author_association": "NONE", "body_html": "<p>Hey guys, wanted to add an update to this as I tried to get something much simpler to work. Please let me know if there is something not quite right in what I attempted.</p>\n<p>I installed TF 1.5 from source.</p>\n<p>This is the simplest possible DNN model - no hidden layer, dropout, or activation function, bias and weight are 1 dimensional.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\n\nmodel_dir = \"simple_lr_ckpt/\"\nx = tf.placeholder(tf.float32, shape=[None,1], name=\"x\")\ny = tf.placeholder(tf.float32, shape=[None,1], name=\"y\")\nW = tf.Variable(tf.truncated_normal(shape=[1, 1], mean=0, stddev=0.001, dtype=tf.float32), name=\"W\")\nb = tf.Variable(tf.truncated_normal(shape=[1], mean=0, stddev=0.001, dtype=tf.float32), name=\"b\")\noutput_name = \"logit\"\nlogit = tf.add(tf.matmul(x, W), b, name=output_name)\nloss = tf.reduce_mean(tf.square(logit - y))\ntf.summary.scalar('loss', loss)\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\nmerged = tf.summary.merge_all()\nN = 1000000\nbatch_size = 100\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    train_writer = tf.summary.FileWriter(model_dir, sess.graph)\n    for start in xrange(0, N, batch_size):\n        x_ = np.reshape(np.random.rand(batch_size), (-1,1))\n        y_ = 2*x_ + 1\n        _, mse, summary = sess.run([train_op, loss, merged], feed_dict={x: x_, y: y_})\n        if start % (batch_size*100) == 0:\n            print mse\n        # if start % batch_size * 100 == 0:\n            saver.save(sess, model_dir + \"model.ckpt-old\", global_step=start)\n        train_writer.add_summary(summary, start)\n\n    tf.train.write_graph(sess.graph_def, model_dir, 'graph.pbtxt')\n    frozen_graph_def = convert_variables_to_constants(sess, sess.graph_def, [output_name])\n\n    with tf.gfile.GFile(model_dir+ \"graph.pb\", \"w\") as f:\n        f.write(frozen_graph_def.SerializeToString())\n</code></pre>\n<p>To freeze the graph this is the code I used</p>\n<pre><code>import os\nfrom tensorflow.python.tools import freeze_graph\nimport tensorflow as tf\n\ndir(tf.contrib)\noutput_path = model_dir\ninput_graph_path = os.path.join(output_path, \"graph.pbtxt\")\ninput_saver_def_path = \"\"\ninput_binary = False\nrestore_op_name = \"save/restore_all\"\nfilename_tensor_name = None\noutput_graph_path = os.path.join(output_path, \"frozen_graph.pb\")\nclear_devices = False\nfreeze_graph.freeze_graph(\n    input_graph_path, input_saver_def_path, input_binary, tf.train.latest_checkpoint(model_dir),\n    output_name, restore_op_name, filename_tensor_name,\n    output_graph_path, clear_devices, \"\", \"\")\n</code></pre>\n<p>After this, I convert the .pb file to .lite using the following</p>\n<p><code>bazel-bin/tensorflow/contrib/lite/toco/toco \\ --input_format=TENSORFLOW_GRAPHDEF \\                                                                         --input_file=/Users/ushnishde/tf_lite_example_new/simple_lr_ckpt/frozen_graph.pb \\ --output_format=TFLITE \\ --output_file=/Users/ushnishde/tf_lite_example_new/simple_lr_ckpt/frozen_graph.lite \\ --inference_type=FLOAT \\ --inference_input_type=FLOAT \\ --input_arrays=x \\ --output_arrays=logit \\ --input_shapes=1,1 </code></p>\n<p>I kept the names of the tensors consistent from when I created the model. Is the y tensor considered as one of the input_arrays?</p>\n<p>When I try to deserialize the .lite model in C++, I followed the code statements which ultimately lead to the EXC_BAD_ACCESS error</p>\n<p>tflite::Interpreter::Invoke() interpreter.cc:414<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3603839/34513536-8fc5d04c-f01d-11e7-9ffa-df64ccf53545.png\"><img width=\"522\" alt=\"screen shot 2018-01-03 at 12 32 01 am\" src=\"https://user-images.githubusercontent.com/3603839/34513536-8fc5d04c-f01d-11e7-9ffa-df64ccf53545.png\" style=\"max-width:100%;\"></a></p>\n<p>TfLiteStatus tflite::ops::builtin::fully_connected::Eval&lt;(tflite::ops::builtin::fully_connected::KernelType)3&gt;(TfLiteContext*, TfLiteNode*) fully_connected.cc:250<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3603839/34513554-a37ad13c-f01d-11e7-8a05-b3768ac7612e.png\"><img width=\"664\" alt=\"screen shot 2018-01-03 at 12 32 37 am\" src=\"https://user-images.githubusercontent.com/3603839/34513554-a37ad13c-f01d-11e7-8a05-b3768ac7612e.png\" style=\"max-width:100%;\"></a></p>\n<p>TfLiteStatus tflite::ops::builtin::fully_connected::EvalFloat&lt;(tflite::ops::builtin::fully_connected::KernelType)3&gt;(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) fully_connected.cc:226<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3603839/34513573-b61ae930-f01d-11e7-9fa5-c58c6ba74d14.png\"><img width=\"652\" alt=\"screen shot 2018-01-03 at 12 33 15 am\" src=\"https://user-images.githubusercontent.com/3603839/34513573-b61ae930-f01d-11e7-9fa5-c58c6ba74d14.png\" style=\"max-width:100%;\"></a></p>\n<p>tflite::ops::builtin::fully_connected::EvalPie(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) fully_connected.cc:148<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3603839/34513588-c9a6ea08-f01d-11e7-8b07-16b40857b3fe.png\"><img width=\"647\" alt=\"screen shot 2018-01-03 at 12 33 45 am\" src=\"https://user-images.githubusercontent.com/3603839/34513588-c9a6ea08-f01d-11e7-8b07-16b40857b3fe.png\" style=\"max-width:100%;\"></a></p>\n<p>tflite::tensor_utils::VectorBatchVectorAssign(float const*, int, int, float*) portable_tensor_utils.h:147<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3603839/34513599-dca703d6-f01d-11e7-8bed-b5a83ab9fa74.png\"><img width=\"513\" alt=\"screen shot 2018-01-03 at 12 34 16 am\" src=\"https://user-images.githubusercontent.com/3603839/34513599-dca703d6-f01d-11e7-8bed-b5a83ab9fa74.png\" style=\"max-width:100%;\"></a></p>\n<p>tflite::tensor_utils::PortableVectorBatchVectorAssign(float const*, int, int, float*) portable_tensor_utils.cc:104<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3603839/34513615-f2a41b24-f01d-11e7-87c4-ef2be1638d84.png\"><img width=\"524\" alt=\"screen shot 2018-01-03 at 12 34 54 am\" src=\"https://user-images.githubusercontent.com/3603839/34513615-f2a41b24-f01d-11e7-87c4-ef2be1638d84.png\" style=\"max-width:100%;\"></a></p>\n<pre><code>void PortableVectorBatchVectorAssign(const float* vector, int v_size,\n                                     int n_batch, float* batch_vector) {\n  for (int b = 0; b &lt; n_batch; b++) {\n    memcpy(batch_vector + b * v_size, vector, v_size * sizeof(float));\n  }\n}\n\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3603839/34513633-10fa2c3a-f01e-11e7-93de-7bcdf1a23f87.png\"><img width=\"554\" alt=\"screen shot 2018-01-03 at 12 35 48 am\" src=\"https://user-images.githubusercontent.com/3603839/34513633-10fa2c3a-f01e-11e7-93de-7bcdf1a23f87.png\" style=\"max-width:100%;\"></a></p>\n<p>Any ideas what could be going on? I'm eager to try any suggestions at this point.</p>", "body_text": "Hey guys, wanted to add an update to this as I tried to get something much simpler to work. Please let me know if there is something not quite right in what I attempted.\nI installed TF 1.5 from source.\nThis is the simplest possible DNN model - no hidden layer, dropout, or activation function, bias and weight are 1 dimensional.\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\n\nmodel_dir = \"simple_lr_ckpt/\"\nx = tf.placeholder(tf.float32, shape=[None,1], name=\"x\")\ny = tf.placeholder(tf.float32, shape=[None,1], name=\"y\")\nW = tf.Variable(tf.truncated_normal(shape=[1, 1], mean=0, stddev=0.001, dtype=tf.float32), name=\"W\")\nb = tf.Variable(tf.truncated_normal(shape=[1], mean=0, stddev=0.001, dtype=tf.float32), name=\"b\")\noutput_name = \"logit\"\nlogit = tf.add(tf.matmul(x, W), b, name=output_name)\nloss = tf.reduce_mean(tf.square(logit - y))\ntf.summary.scalar('loss', loss)\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\nmerged = tf.summary.merge_all()\nN = 1000000\nbatch_size = 100\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    train_writer = tf.summary.FileWriter(model_dir, sess.graph)\n    for start in xrange(0, N, batch_size):\n        x_ = np.reshape(np.random.rand(batch_size), (-1,1))\n        y_ = 2*x_ + 1\n        _, mse, summary = sess.run([train_op, loss, merged], feed_dict={x: x_, y: y_})\n        if start % (batch_size*100) == 0:\n            print mse\n        # if start % batch_size * 100 == 0:\n            saver.save(sess, model_dir + \"model.ckpt-old\", global_step=start)\n        train_writer.add_summary(summary, start)\n\n    tf.train.write_graph(sess.graph_def, model_dir, 'graph.pbtxt')\n    frozen_graph_def = convert_variables_to_constants(sess, sess.graph_def, [output_name])\n\n    with tf.gfile.GFile(model_dir+ \"graph.pb\", \"w\") as f:\n        f.write(frozen_graph_def.SerializeToString())\n\nTo freeze the graph this is the code I used\nimport os\nfrom tensorflow.python.tools import freeze_graph\nimport tensorflow as tf\n\ndir(tf.contrib)\noutput_path = model_dir\ninput_graph_path = os.path.join(output_path, \"graph.pbtxt\")\ninput_saver_def_path = \"\"\ninput_binary = False\nrestore_op_name = \"save/restore_all\"\nfilename_tensor_name = None\noutput_graph_path = os.path.join(output_path, \"frozen_graph.pb\")\nclear_devices = False\nfreeze_graph.freeze_graph(\n    input_graph_path, input_saver_def_path, input_binary, tf.train.latest_checkpoint(model_dir),\n    output_name, restore_op_name, filename_tensor_name,\n    output_graph_path, clear_devices, \"\", \"\")\n\nAfter this, I convert the .pb file to .lite using the following\nbazel-bin/tensorflow/contrib/lite/toco/toco \\ --input_format=TENSORFLOW_GRAPHDEF \\                                                                         --input_file=/Users/ushnishde/tf_lite_example_new/simple_lr_ckpt/frozen_graph.pb \\ --output_format=TFLITE \\ --output_file=/Users/ushnishde/tf_lite_example_new/simple_lr_ckpt/frozen_graph.lite \\ --inference_type=FLOAT \\ --inference_input_type=FLOAT \\ --input_arrays=x \\ --output_arrays=logit \\ --input_shapes=1,1 \nI kept the names of the tensors consistent from when I created the model. Is the y tensor considered as one of the input_arrays?\nWhen I try to deserialize the .lite model in C++, I followed the code statements which ultimately lead to the EXC_BAD_ACCESS error\ntflite::Interpreter::Invoke() interpreter.cc:414\n\nTfLiteStatus tflite::ops::builtin::fully_connected::Eval<(tflite::ops::builtin::fully_connected::KernelType)3>(TfLiteContext*, TfLiteNode*) fully_connected.cc:250\n\nTfLiteStatus tflite::ops::builtin::fully_connected::EvalFloat<(tflite::ops::builtin::fully_connected::KernelType)3>(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) fully_connected.cc:226\n\ntflite::ops::builtin::fully_connected::EvalPie(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) fully_connected.cc:148\n\ntflite::tensor_utils::VectorBatchVectorAssign(float const*, int, int, float*) portable_tensor_utils.h:147\n\ntflite::tensor_utils::PortableVectorBatchVectorAssign(float const*, int, int, float*) portable_tensor_utils.cc:104\n\nvoid PortableVectorBatchVectorAssign(const float* vector, int v_size,\n                                     int n_batch, float* batch_vector) {\n  for (int b = 0; b < n_batch; b++) {\n    memcpy(batch_vector + b * v_size, vector, v_size * sizeof(float));\n  }\n}\n\n\n\nAny ideas what could be going on? I'm eager to try any suggestions at this point.", "body": "Hey guys, wanted to add an update to this as I tried to get something much simpler to work. Please let me know if there is something not quite right in what I attempted.\r\n\r\nI installed TF 1.5 from source.\r\n\r\nThis is the simplest possible DNN model - no hidden layer, dropout, or activation function, bias and weight are 1 dimensional.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n\r\nmodel_dir = \"simple_lr_ckpt/\"\r\nx = tf.placeholder(tf.float32, shape=[None,1], name=\"x\")\r\ny = tf.placeholder(tf.float32, shape=[None,1], name=\"y\")\r\nW = tf.Variable(tf.truncated_normal(shape=[1, 1], mean=0, stddev=0.001, dtype=tf.float32), name=\"W\")\r\nb = tf.Variable(tf.truncated_normal(shape=[1], mean=0, stddev=0.001, dtype=tf.float32), name=\"b\")\r\noutput_name = \"logit\"\r\nlogit = tf.add(tf.matmul(x, W), b, name=output_name)\r\nloss = tf.reduce_mean(tf.square(logit - y))\r\ntf.summary.scalar('loss', loss)\r\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\r\nmerged = tf.summary.merge_all()\r\nN = 1000000\r\nbatch_size = 100\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    train_writer = tf.summary.FileWriter(model_dir, sess.graph)\r\n    for start in xrange(0, N, batch_size):\r\n        x_ = np.reshape(np.random.rand(batch_size), (-1,1))\r\n        y_ = 2*x_ + 1\r\n        _, mse, summary = sess.run([train_op, loss, merged], feed_dict={x: x_, y: y_})\r\n        if start % (batch_size*100) == 0:\r\n            print mse\r\n        # if start % batch_size * 100 == 0:\r\n            saver.save(sess, model_dir + \"model.ckpt-old\", global_step=start)\r\n        train_writer.add_summary(summary, start)\r\n\r\n    tf.train.write_graph(sess.graph_def, model_dir, 'graph.pbtxt')\r\n    frozen_graph_def = convert_variables_to_constants(sess, sess.graph_def, [output_name])\r\n\r\n    with tf.gfile.GFile(model_dir+ \"graph.pb\", \"w\") as f:\r\n        f.write(frozen_graph_def.SerializeToString())\r\n```\r\n\r\nTo freeze the graph this is the code I used\r\n\r\n```\r\nimport os\r\nfrom tensorflow.python.tools import freeze_graph\r\nimport tensorflow as tf\r\n\r\ndir(tf.contrib)\r\noutput_path = model_dir\r\ninput_graph_path = os.path.join(output_path, \"graph.pbtxt\")\r\ninput_saver_def_path = \"\"\r\ninput_binary = False\r\nrestore_op_name = \"save/restore_all\"\r\nfilename_tensor_name = None\r\noutput_graph_path = os.path.join(output_path, \"frozen_graph.pb\")\r\nclear_devices = False\r\nfreeze_graph.freeze_graph(\r\n    input_graph_path, input_saver_def_path, input_binary, tf.train.latest_checkpoint(model_dir),\r\n    output_name, restore_op_name, filename_tensor_name,\r\n    output_graph_path, clear_devices, \"\", \"\")\r\n```\r\n\r\nAfter this, I convert the .pb file to .lite using the following\r\n\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\                                                                        \r\n  --input_file=/Users/ushnishde/tf_lite_example_new/simple_lr_ckpt/frozen_graph.pb \\\r\n  --output_format=TFLITE \\\r\n  --output_file=/Users/ushnishde/tf_lite_example_new/simple_lr_ckpt/frozen_graph.lite \\\r\n  --inference_type=FLOAT \\\r\n  --inference_input_type=FLOAT \\\r\n  --input_arrays=x \\\r\n  --output_arrays=logit \\\r\n  --input_shapes=1,1\r\n`\r\n\r\nI kept the names of the tensors consistent from when I created the model. Is the y tensor considered as one of the input_arrays?\r\n\r\n\r\nWhen I try to deserialize the .lite model in C++, I followed the code statements which ultimately lead to the EXC_BAD_ACCESS error\r\n\r\ntflite::Interpreter::Invoke() interpreter.cc:414\r\n<img width=\"522\" alt=\"screen shot 2018-01-03 at 12 32 01 am\" src=\"https://user-images.githubusercontent.com/3603839/34513536-8fc5d04c-f01d-11e7-9ffa-df64ccf53545.png\">\r\n\r\nTfLiteStatus tflite::ops::builtin::fully_connected::Eval<(tflite::ops::builtin::fully_connected::KernelType)3>(TfLiteContext*, TfLiteNode*) fully_connected.cc:250\r\n<img width=\"664\" alt=\"screen shot 2018-01-03 at 12 32 37 am\" src=\"https://user-images.githubusercontent.com/3603839/34513554-a37ad13c-f01d-11e7-8a05-b3768ac7612e.png\">\r\n\r\nTfLiteStatus tflite::ops::builtin::fully_connected::EvalFloat<(tflite::ops::builtin::fully_connected::KernelType)3>(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) fully_connected.cc:226\r\n<img width=\"652\" alt=\"screen shot 2018-01-03 at 12 33 15 am\" src=\"https://user-images.githubusercontent.com/3603839/34513573-b61ae930-f01d-11e7-9fa5-c58c6ba74d14.png\">\r\n\r\ntflite::ops::builtin::fully_connected::EvalPie(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) fully_connected.cc:148\r\n<img width=\"647\" alt=\"screen shot 2018-01-03 at 12 33 45 am\" src=\"https://user-images.githubusercontent.com/3603839/34513588-c9a6ea08-f01d-11e7-8b07-16b40857b3fe.png\">\r\n\r\ntflite::tensor_utils::VectorBatchVectorAssign(float const*, int, int, float*) portable_tensor_utils.h:147\r\n<img width=\"513\" alt=\"screen shot 2018-01-03 at 12 34 16 am\" src=\"https://user-images.githubusercontent.com/3603839/34513599-dca703d6-f01d-11e7-8bed-b5a83ab9fa74.png\">\r\n\r\ntflite::tensor_utils::PortableVectorBatchVectorAssign(float const*, int, int, float*) portable_tensor_utils.cc:104\r\n<img width=\"524\" alt=\"screen shot 2018-01-03 at 12 34 54 am\" src=\"https://user-images.githubusercontent.com/3603839/34513615-f2a41b24-f01d-11e7-87c4-ef2be1638d84.png\">\r\n\r\n```\r\nvoid PortableVectorBatchVectorAssign(const float* vector, int v_size,\r\n                                     int n_batch, float* batch_vector) {\r\n  for (int b = 0; b < n_batch; b++) {\r\n    memcpy(batch_vector + b * v_size, vector, v_size * sizeof(float));\r\n  }\r\n}\r\n\r\n```\r\n<img width=\"554\" alt=\"screen shot 2018-01-03 at 12 35 48 am\" src=\"https://user-images.githubusercontent.com/3603839/34513633-10fa2c3a-f01e-11e7-93de-7bcdf1a23f87.png\">\r\n\r\nAny ideas what could be going on? I'm eager to try any suggestions at this point.\r\n  "}