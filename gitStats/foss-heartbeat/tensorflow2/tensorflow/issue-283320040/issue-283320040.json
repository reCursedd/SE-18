{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15488", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15488/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15488/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15488/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15488", "id": 283320040, "node_id": "MDU6SXNzdWUyODMzMjAwNDA=", "number": 15488, "title": "Bad access error when deserializing a fully connected TF Lite model", "user": {"login": "ushnish", "id": 3603839, "node_id": "MDQ6VXNlcjM2MDM4Mzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3603839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ushnish", "html_url": "https://github.com/ushnish", "followers_url": "https://api.github.com/users/ushnish/followers", "following_url": "https://api.github.com/users/ushnish/following{/other_user}", "gists_url": "https://api.github.com/users/ushnish/gists{/gist_id}", "starred_url": "https://api.github.com/users/ushnish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ushnish/subscriptions", "organizations_url": "https://api.github.com/users/ushnish/orgs", "repos_url": "https://api.github.com/users/ushnish/repos", "events_url": "https://api.github.com/users/ushnish/events{/privacy}", "received_events_url": "https://api.github.com/users/ushnish/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 17, "created_at": "2017-12-19T18:02:25Z", "updated_at": "2018-06-12T16:00:56Z", "closed_at": "2018-06-12T16:00:56Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n<li><strong>Python version</strong>:</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"<br>\n<code>('v1.3.0-rc1-5910-ge2174cc943', '1.4.0')</code></p>\n<h3>Describe the problem</h3>\n<p>I trained a very simple feed forward network (model structure is y = W*x + b so basically linear regression) using raw tensorflow code (matrix multiply, bias add and relu) so not using any Estimators or higher order APIs. My main purpose was to see if I could serialize the model, convert it to .lite format and then deserialize it correctly in C++.</p>\n<p>I did freeze_graph on this graph.pbtxt and model checkpoints using tensorflow's provided freeze_graph method, and converted to .lite format using the bazel command line instruction (in the lite documentation) Lite conversion command used is</p>\n<pre><code>bazel run --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=frozen_model.pb  \\               \n --output_file=frozen_model.lite \\               \n --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --inference_type=FLOAT \\\n  --input_shape=**1,1** \\ \n --input_array=**x** \\          \n --output_array=**output**\n</code></pre>\n<p>As x is the name of the input 1-Dim placeholder tensor, output is the name of the result of W*x+b. I did not include y as part of the input_array as running with --input_array=x,y, --input_shape=1,1:1,1 as this produced a .lite model with 0 bytes (no output)</p>\n<p>Using code very similar to what is in tflite_driver.cc and tflite_driver_test.cc, I was able to get the mobile_net example working fine but this simple model I described above fails with a EXC_BAD_ACCESS error. Upon debugging I saw that the error happens at fully_conncted.cc</p>\n<pre><code>template &lt;KernelType kernel_type&gt;\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n....\n\u2002\u2002switch (input-&gt;type) {\u2002\u2002// Already know in/out types are same.\n\u2002\u2002\u2002\u2002case kTfLiteFloat32:\n\u2002\u2002\u2002\u2002\u2002\u2002return EvalFloat&lt;**kernel_type**&gt;(context, node, params, data, input, filter,\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002bias, output);\n....\n\u2002\u2002}\n\u2002\u2002return kTfLiteOk;\n}\n\n</code></pre>\n<p>kernel_type is <strong>kPie</strong> at this point. Looking further into fully_connected.cc, I notice two things.</p>\n<pre><code>enum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n  kNeonOptimized,\n  kPie,  **// Used by the PIE team**\n};\n\n</code></pre>\n<p>And also</p>\n<pre><code>TfLiteRegistration* Register_FULLY_CONNECTED() {\n\u2002\u2002**// TODO(ahentz): We don't have a dedicated quantized version of the PIE\n\u2002\u2002// kernel. For now, the quantized version just defer to the corresponding\n\u2002\u2002// optimized MINI kernel. At some point we will allow different libraries to\n\u2002\u2002// be built with different kernels, but for now we have to pick one here.\n\u2002\u2002return Register_FULLY_CONNECTED_PIE();**\n#ifdef USE_NEON\n\u2002\u2002return Register_FULLY_CONNECTED_NEON_OPT();\n#else\n\u2002\u2002return Register_FULLY_CONNECTED_GENERIC_OPT();\n#endif\n}\n\n</code></pre>\n<p>Specifically, the EXC_BAD_ACCESS error happens at</p>\n<pre><code>TfLiteStatus EvalPie(TfLiteContext* context, TfLiteNode* node,\n                     ...\n  // Output = bias if bias tensor exists.\n  if (bias) {\n    tensor_utils::VectorBatchVectorAssign(bias-&gt;data.f, num_units, batch_size,\n                                          output-&gt;data.f);\n  } \n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3603839/34171209-63703e96-e4a2-11e7-89fd-446b1c1c44cb.png\"><img src=\"https://user-images.githubusercontent.com/3603839/34171209-63703e96-e4a2-11e7-89fd-446b1c1c44cb.png\" alt=\"upload\" style=\"max-width:100%;\"></a></p>\n<p>As we can see the filter and bias terms have bad memory addresses, the input and output tensors have valid memory addresses though</p>\n<p>My questions are</p>\n<ol>\n<li>Why is kPie kernel_type being picked for my simple model?</li>\n<li>Based on the comments it looks like kPie is not supported for external use, so could this explain the bad access error?</li>\n<li>Is this an issue with how I converted to lite format? Here is my command again</li>\n</ol>\n<pre><code>bazel run --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=frozen_model.pb  \\               \n --output_file=frozen_model.lite \\               \n --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --inference_type=FLOAT \\\n  --input_shape=**1,1** \\ \n --input_array=**x** \\          \n --output_array=**output**\n</code></pre>\n<ol start=\"4\">\n<li>Is there any example of a very simple model composed out of matmul, add and relu which can be converted to .lite in such a way that it can be correctly deserialized in C++, just like the mobile_net example? I can build up from such an example</li>\n</ol>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):\nPython version:\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\n('v1.3.0-rc1-5910-ge2174cc943', '1.4.0')\nDescribe the problem\nI trained a very simple feed forward network (model structure is y = W*x + b so basically linear regression) using raw tensorflow code (matrix multiply, bias add and relu) so not using any Estimators or higher order APIs. My main purpose was to see if I could serialize the model, convert it to .lite format and then deserialize it correctly in C++.\nI did freeze_graph on this graph.pbtxt and model checkpoints using tensorflow's provided freeze_graph method, and converted to .lite format using the bazel command line instruction (in the lite documentation) Lite conversion command used is\nbazel run --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=frozen_model.pb  \\               \n --output_file=frozen_model.lite \\               \n --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --inference_type=FLOAT \\\n  --input_shape=**1,1** \\ \n --input_array=**x** \\          \n --output_array=**output**\n\nAs x is the name of the input 1-Dim placeholder tensor, output is the name of the result of W*x+b. I did not include y as part of the input_array as running with --input_array=x,y, --input_shape=1,1:1,1 as this produced a .lite model with 0 bytes (no output)\nUsing code very similar to what is in tflite_driver.cc and tflite_driver_test.cc, I was able to get the mobile_net example working fine but this simple model I described above fails with a EXC_BAD_ACCESS error. Upon debugging I saw that the error happens at fully_conncted.cc\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n....\n\u2002\u2002switch (input->type) {\u2002\u2002// Already know in/out types are same.\n\u2002\u2002\u2002\u2002case kTfLiteFloat32:\n\u2002\u2002\u2002\u2002\u2002\u2002return EvalFloat<**kernel_type**>(context, node, params, data, input, filter,\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002bias, output);\n....\n\u2002\u2002}\n\u2002\u2002return kTfLiteOk;\n}\n\n\nkernel_type is kPie at this point. Looking further into fully_connected.cc, I notice two things.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n  kNeonOptimized,\n  kPie,  **// Used by the PIE team**\n};\n\n\nAnd also\nTfLiteRegistration* Register_FULLY_CONNECTED() {\n\u2002\u2002**// TODO(ahentz): We don't have a dedicated quantized version of the PIE\n\u2002\u2002// kernel. For now, the quantized version just defer to the corresponding\n\u2002\u2002// optimized MINI kernel. At some point we will allow different libraries to\n\u2002\u2002// be built with different kernels, but for now we have to pick one here.\n\u2002\u2002return Register_FULLY_CONNECTED_PIE();**\n#ifdef USE_NEON\n\u2002\u2002return Register_FULLY_CONNECTED_NEON_OPT();\n#else\n\u2002\u2002return Register_FULLY_CONNECTED_GENERIC_OPT();\n#endif\n}\n\n\nSpecifically, the EXC_BAD_ACCESS error happens at\nTfLiteStatus EvalPie(TfLiteContext* context, TfLiteNode* node,\n                     ...\n  // Output = bias if bias tensor exists.\n  if (bias) {\n    tensor_utils::VectorBatchVectorAssign(bias->data.f, num_units, batch_size,\n                                          output->data.f);\n  } \n\n\nAs we can see the filter and bias terms have bad memory addresses, the input and output tensors have valid memory addresses though\nMy questions are\n\nWhy is kPie kernel_type being picked for my simple model?\nBased on the comments it looks like kPie is not supported for external use, so could this explain the bad access error?\nIs this an issue with how I converted to lite format? Here is my command again\n\nbazel run --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=frozen_model.pb  \\               \n --output_file=frozen_model.lite \\               \n --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --inference_type=FLOAT \\\n  --input_shape=**1,1** \\ \n --input_array=**x** \\          \n --output_array=**output**\n\n\nIs there any example of a very simple model composed out of matmul, add and relu which can be converted to .lite in such a way that it can be correctly deserialized in C++, just like the mobile_net example? I can build up from such an example", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n`('v1.3.0-rc1-5910-ge2174cc943', '1.4.0')`\r\n\r\n### Describe the problem\r\nI trained a very simple feed forward network (model structure is y = W*x + b so basically linear regression) using raw tensorflow code (matrix multiply, bias add and relu) so not using any Estimators or higher order APIs. My main purpose was to see if I could serialize the model, convert it to .lite format and then deserialize it correctly in C++.\r\n\r\nI did freeze_graph on this graph.pbtxt and model checkpoints using tensorflow's provided freeze_graph method, and converted to .lite format using the bazel command line instruction (in the lite documentation) Lite conversion command used is\r\n\r\n```\r\nbazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=frozen_model.pb  \\               \r\n --output_file=frozen_model.lite \\               \r\n --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shape=**1,1** \\ \r\n --input_array=**x** \\          \r\n --output_array=**output**\r\n``` \r\nAs x is the name of the input 1-Dim placeholder tensor, output is the name of the result of W*x+b. I did not include y as part of the input_array as running with --input_array=x,y, --input_shape=1,1:1,1 as this produced a .lite model with 0 bytes (no output)\r\n\r\nUsing code very similar to what is in tflite_driver.cc and tflite_driver_test.cc, I was able to get the mobile_net example working fine but this simple model I described above fails with a EXC_BAD_ACCESS error. Upon debugging I saw that the error happens at fully_conncted.cc\r\n\r\n```\r\ntemplate <KernelType kernel_type>\r\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\r\n....\r\n\u2002\u2002switch (input->type) {\u2002\u2002// Already know in/out types are same.\r\n\u2002\u2002\u2002\u2002case kTfLiteFloat32:\r\n\u2002\u2002\u2002\u2002\u2002\u2002return EvalFloat<**kernel_type**>(context, node, params, data, input, filter,\r\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002bias, output);\r\n....\r\n\u2002\u2002}\r\n\u2002\u2002return kTfLiteOk;\r\n}\r\n\r\n```\r\n\r\nkernel_type is **kPie** at this point. Looking further into fully_connected.cc, I notice two things.\r\n\r\n```\r\nenum KernelType {\r\n  kReference,\r\n  kGenericOptimized,  // Neon-free\r\n  kNeonOptimized,\r\n  kPie,  **// Used by the PIE team**\r\n};\r\n\r\n```\r\nAnd also\r\n\r\n```\r\nTfLiteRegistration* Register_FULLY_CONNECTED() {\r\n\u2002\u2002**// TODO(ahentz): We don't have a dedicated quantized version of the PIE\r\n\u2002\u2002// kernel. For now, the quantized version just defer to the corresponding\r\n\u2002\u2002// optimized MINI kernel. At some point we will allow different libraries to\r\n\u2002\u2002// be built with different kernels, but for now we have to pick one here.\r\n\u2002\u2002return Register_FULLY_CONNECTED_PIE();**\r\n#ifdef USE_NEON\r\n\u2002\u2002return Register_FULLY_CONNECTED_NEON_OPT();\r\n#else\r\n\u2002\u2002return Register_FULLY_CONNECTED_GENERIC_OPT();\r\n#endif\r\n}\r\n\r\n```\r\nSpecifically, the EXC_BAD_ACCESS error happens at\r\n\r\n```\r\nTfLiteStatus EvalPie(TfLiteContext* context, TfLiteNode* node,\r\n                     ...\r\n  // Output = bias if bias tensor exists.\r\n  if (bias) {\r\n    tensor_utils::VectorBatchVectorAssign(bias->data.f, num_units, batch_size,\r\n                                          output->data.f);\r\n  } \r\n```\r\n\r\n![upload](https://user-images.githubusercontent.com/3603839/34171209-63703e96-e4a2-11e7-89fd-446b1c1c44cb.png)\r\n\r\nAs we can see the filter and bias terms have bad memory addresses, the input and output tensors have valid memory addresses though\r\n\r\nMy questions are \r\n1) Why is kPie kernel_type being picked for my simple model?\r\n2) Based on the comments it looks like kPie is not supported for external use, so could this explain the bad access error?\r\n3) Is this an issue with how I converted to lite format? Here is my command again\r\n```\r\nbazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=frozen_model.pb  \\               \r\n --output_file=frozen_model.lite \\               \r\n --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shape=**1,1** \\ \r\n --input_array=**x** \\          \r\n --output_array=**output**\r\n``` \r\n4) Is there any example of a very simple model composed out of matmul, add and relu which can be converted to .lite in such a way that it can be correctly deserialized in C++, just like the mobile_net example? I can build up from such an example\r\n"}