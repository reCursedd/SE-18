{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309940293", "html_url": "https://github.com/tensorflow/tensorflow/pull/10828#issuecomment-309940293", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10828", "id": 309940293, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTk0MDI5Mw==", "user": {"login": "andydavis1", "id": 15696327, "node_id": "MDQ6VXNlcjE1Njk2MzI3", "avatar_url": "https://avatars0.githubusercontent.com/u/15696327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andydavis1", "html_url": "https://github.com/andydavis1", "followers_url": "https://api.github.com/users/andydavis1/followers", "following_url": "https://api.github.com/users/andydavis1/following{/other_user}", "gists_url": "https://api.github.com/users/andydavis1/gists{/gist_id}", "starred_url": "https://api.github.com/users/andydavis1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andydavis1/subscriptions", "organizations_url": "https://api.github.com/users/andydavis1/orgs", "repos_url": "https://api.github.com/users/andydavis1/repos", "events_url": "https://api.github.com/users/andydavis1/events{/privacy}", "received_events_url": "https://api.github.com/users/andydavis1/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-21T01:55:25Z", "updated_at": "2017-06-21T01:55:25Z", "author_association": "MEMBER", "body_html": "<p>Maybe try reducing the size of the dimension that gets reduced for your unit test case.  Also, some gradients can be sensitive to initial value when the gradient check is used, so you play with 'x_init_value' (see ReluGrad for an example).</p>", "body_text": "Maybe try reducing the size of the dimension that gets reduced for your unit test case.  Also, some gradients can be sensitive to initial value when the gradient check is used, so you play with 'x_init_value' (see ReluGrad for an example).", "body": "Maybe try reducing the size of the dimension that gets reduced for your unit test case.  Also, some gradients can be sensitive to initial value when the gradient check is used, so you play with 'x_init_value' (see ReluGrad for an example). "}