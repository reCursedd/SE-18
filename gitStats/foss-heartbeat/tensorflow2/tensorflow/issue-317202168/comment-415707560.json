{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/415707560", "html_url": "https://github.com/tensorflow/tensorflow/issues/18829#issuecomment-415707560", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18829", "id": 415707560, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNTcwNzU2MA==", "user": {"login": "alokranjan007", "id": 19912807, "node_id": "MDQ6VXNlcjE5OTEyODA3", "avatar_url": "https://avatars1.githubusercontent.com/u/19912807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alokranjan007", "html_url": "https://github.com/alokranjan007", "followers_url": "https://api.github.com/users/alokranjan007/followers", "following_url": "https://api.github.com/users/alokranjan007/following{/other_user}", "gists_url": "https://api.github.com/users/alokranjan007/gists{/gist_id}", "starred_url": "https://api.github.com/users/alokranjan007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alokranjan007/subscriptions", "organizations_url": "https://api.github.com/users/alokranjan007/orgs", "repos_url": "https://api.github.com/users/alokranjan007/repos", "events_url": "https://api.github.com/users/alokranjan007/events{/privacy}", "received_events_url": "https://api.github.com/users/alokranjan007/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-24T09:36:20Z", "updated_at": "2018-08-24T09:36:20Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20219876\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jass-singh\">@jass-singh</a> Assuming that you have already trained you model and have set the python path. I would suggest to follow below steps for inferncin.</p>\n<p>Step 1.<br>\n<code>python export_tflite_ssd_graph.py \\ --pipeline_config_path=/home/username/tensorflow-master/models/research/object_detection/path to .config file \\ --trained_checkpoint_prefix=/home/username/tensorflow-master/models/research/object_detection/path to the trained model ckpt point(your total no of steps after the training) \\ --output_directory=/home/username/tensorflow-master/models/research/object_detection/any folder name \\ --add_postprocessing_op=true</code><br>\nThis will generate tflite_graph.pb file and tflite_graph.pbtxt</p>\n<p>Step 2:<br>\n<code>bazel run -c opt tensorflow/contrib/lite/toco:toco -- \\ --input_file=/home/username/tensorflow-master/models/research/object_detection/customized_model/path to tflite_graph.pb \\ --output_file=/home/username/tensorflow-master/models/research/object_detection/your any folder name/detect.tflite \\ --input_shapes=1,300,300,3 \\ --input_arrays=normalized_input_image_tensor \\ --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\ --inference_type=FLOAT \\ --mean_values=128 \\ --std_values=128 \\ --change_concat_input_ranges=false \\ --allow_custom_ops</code></p>\n<p>After this detect.tflite file will be generated then you can use this to port on device.</p>", "body_text": "@jass-singh Assuming that you have already trained you model and have set the python path. I would suggest to follow below steps for inferncin.\nStep 1.\npython export_tflite_ssd_graph.py \\ --pipeline_config_path=/home/username/tensorflow-master/models/research/object_detection/path to .config file \\ --trained_checkpoint_prefix=/home/username/tensorflow-master/models/research/object_detection/path to the trained model ckpt point(your total no of steps after the training) \\ --output_directory=/home/username/tensorflow-master/models/research/object_detection/any folder name \\ --add_postprocessing_op=true\nThis will generate tflite_graph.pb file and tflite_graph.pbtxt\nStep 2:\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- \\ --input_file=/home/username/tensorflow-master/models/research/object_detection/customized_model/path to tflite_graph.pb \\ --output_file=/home/username/tensorflow-master/models/research/object_detection/your any folder name/detect.tflite \\ --input_shapes=1,300,300,3 \\ --input_arrays=normalized_input_image_tensor \\ --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\ --inference_type=FLOAT \\ --mean_values=128 \\ --std_values=128 \\ --change_concat_input_ranges=false \\ --allow_custom_ops\nAfter this detect.tflite file will be generated then you can use this to port on device.", "body": "@jass-singh Assuming that you have already trained you model and have set the python path. I would suggest to follow below steps for inferncin.\r\n\r\nStep 1.\r\n`python export_tflite_ssd_graph.py \\\r\n--pipeline_config_path=/home/username/tensorflow-master/models/research/object_detection/path to .config file \\\r\n--trained_checkpoint_prefix=/home/username/tensorflow-master/models/research/object_detection/path to the trained model ckpt point(your total no of steps after the training) \\\r\n--output_directory=/home/username/tensorflow-master/models/research/object_detection/any folder name \\\r\n--add_postprocessing_op=true`\r\nThis will generate tflite_graph.pb file and tflite_graph.pbtxt\r\n\r\nStep 2:\r\n`bazel run -c opt tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=/home/username/tensorflow-master/models/research/object_detection/customized_model/path to tflite_graph.pb \\\r\n--output_file=/home/username/tensorflow-master/models/research/object_detection/your any folder name/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\r\n--inference_type=FLOAT \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops`\r\n\r\nAfter this detect.tflite file will be generated then you can use this to port on device."}