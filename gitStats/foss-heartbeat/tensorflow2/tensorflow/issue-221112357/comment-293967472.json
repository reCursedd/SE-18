{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293967472", "html_url": "https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293967472", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9150", "id": 293967472, "node_id": "MDEyOklzc3VlQ29tbWVudDI5Mzk2NzQ3Mg==", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-13T17:26:44Z", "updated_at": "2017-04-13T17:26:44Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">I see. In this case, how about we allocate the DirectByteBuffer on the C side wrapping the TF_Tensor buffer? This way it cannot be deallocated by the GC (only the wrapping DirectByteBuffer object can be GCed, but the underlying memory will still be there for TF to manage), and will be managed by the TensorFlow native library. Should the Scala user ever have to explicitly delete Tensor objects in this case (i.e., through the C API), or will the TF library manage them?\n\nThis also comes down to the more basic question of how this is done in the Python API. You are passing the TF_Tensor constructor the numpy deallocator. Does this mean that the TF native library can delete a Tensor even though it might still be used in the Python side as a numpy array, referencing the same underlying memory? If not, then how and when are TF tensors deallocated for the Python API?\n\nI'm sorry for the series of questions, but given that I'm building a Scala API aimed to be as close as possible to the Python API, in terms of functionality, I'm trying to get a better understanding of how the interfacing with the native library works and how to do it efficiently, without unnecessary copies.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Apr 13, 2017, 1:14 PM -0400, Alexandre Passos ***@***.***&gt;, wrote:\n There is a chance that the C library will reuse the Tensor you pass to\n session.run after it returns (for example, if the Tensor is enqueued into a\n fifo queue we reserve the right to not copy it until it's time to dequeue).\n\n On Thu, Apr 13, 2017 at 10:07 AM, Anthony Platanios &lt;\n ***@***.***&gt; wrote:\n\n &gt; <a class=\"user-mention\" href=\"https://github.com/alextp\">@alextp</a> &lt;<a href=\"https://github.com/alextp\">https://github.com/alextp</a>&gt; As long as I create a\n &gt; DirectByteBuffer on the Scala side, the GC will be able to deallocate its\n &gt; underlying buffer. Furthermore, as long as a reference to that\n &gt; DirectByteBuffer exists on the Scala side, the GC will not deallocate it\n &gt; and the C side will be able to use it (e.g., when you call session.run). So\n &gt; I think there would be no memory leakage since the GC will take care of\n &gt; that buffer. The only occasion where there might be an issue if the GC\n &gt; deallocates it and then the C library tries to use it. This should not\n &gt; happen though unless the C library tries to reuse the tensor you pass in\n &gt; session.run, after that function returns. Does that sound reasonable?\n &gt;\n &gt; \u2014\n &gt; You are receiving this because you were mentioned.\n &gt; Reply to this email directly, view it on GitHub\n &gt; &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"221112357\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9150\" href=\"https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293962227\">#9150 (comment)</a>&gt;,\n &gt; or mute the thread\n &gt; &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxX-5lcLpShE_6vTCyUw1V_eDjPZHks5rvlZOgaJpZM4M6yMu\">https://github.com/notifications/unsubscribe-auth/AAATxX-5lcLpShE_6vTCyUw1V_eDjPZHks5rvlZOgaJpZM4M6yMu</a>&gt;\n &gt; .\n &gt;\n\n\n\n --\n - Alex\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"221112357\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9150\" href=\"https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293963686\">#9150 (comment)</a>), or mute the thread (<a href=\"https://github.com/notifications/unsubscribe-auth/ABPCXGJtaeZVo4HCPoL0adq5IhTk63gnks5rvlgDgaJpZM4M6yMu\">https://github.com/notifications/unsubscribe-auth/ABPCXGJtaeZVo4HCPoL0adq5IhTk63gnks5rvlgDgaJpZM4M6yMu</a>).\n\n\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "I see. In this case, how about we allocate the DirectByteBuffer on the C side wrapping the TF_Tensor buffer? This way it cannot be deallocated by the GC (only the wrapping DirectByteBuffer object can be GCed, but the underlying memory will still be there for TF to manage), and will be managed by the TensorFlow native library. Should the Scala user ever have to explicitly delete Tensor objects in this case (i.e., through the C API), or will the TF library manage them?\n\nThis also comes down to the more basic question of how this is done in the Python API. You are passing the TF_Tensor constructor the numpy deallocator. Does this mean that the TF native library can delete a Tensor even though it might still be used in the Python side as a numpy array, referencing the same underlying memory? If not, then how and when are TF tensors deallocated for the Python API?\n\nI'm sorry for the series of questions, but given that I'm building a Scala API aimed to be as close as possible to the Python API, in terms of functionality, I'm trying to get a better understanding of how the interfacing with the native library works and how to do it efficiently, without unnecessary copies.\n\u2026\nOn Apr 13, 2017, 1:14 PM -0400, Alexandre Passos ***@***.***>, wrote:\n There is a chance that the C library will reuse the Tensor you pass to\n session.run after it returns (for example, if the Tensor is enqueued into a\n fifo queue we reserve the right to not copy it until it's time to dequeue).\n\n On Thu, Apr 13, 2017 at 10:07 AM, Anthony Platanios <\n ***@***.***> wrote:\n\n > @alextp <https://github.com/alextp> As long as I create a\n > DirectByteBuffer on the Scala side, the GC will be able to deallocate its\n > underlying buffer. Furthermore, as long as a reference to that\n > DirectByteBuffer exists on the Scala side, the GC will not deallocate it\n > and the C side will be able to use it (e.g., when you call session.run). So\n > I think there would be no memory leakage since the GC will take care of\n > that buffer. The only occasion where there might be an issue if the GC\n > deallocates it and then the C library tries to use it. This should not\n > happen though unless the C library tries to reuse the tensor you pass in\n > session.run, after that function returns. Does that sound reasonable?\n >\n > \u2014\n > You are receiving this because you were mentioned.\n > Reply to this email directly, view it on GitHub\n > <#9150 (comment)>,\n > or mute the thread\n > <https://github.com/notifications/unsubscribe-auth/AAATxX-5lcLpShE_6vTCyUw1V_eDjPZHks5rvlZOgaJpZM4M6yMu>\n > .\n >\n\n\n\n --\n - Alex\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub (#9150 (comment)), or mute the thread (https://github.com/notifications/unsubscribe-auth/ABPCXGJtaeZVo4HCPoL0adq5IhTk63gnks5rvlgDgaJpZM4M6yMu).", "body": "I see. In this case, how about we allocate the DirectByteBuffer on the C side wrapping the TF_Tensor buffer? This way it cannot be deallocated by the GC (only the wrapping DirectByteBuffer object can be GCed, but the underlying memory will still be there for TF to manage), and will be managed by the TensorFlow native library. Should the Scala user ever have to explicitly delete Tensor objects in this case (i.e., through the C API), or will the TF library manage them?\n\nThis also comes down to the more basic question of how this is done in the Python API. You are passing the TF_Tensor constructor the numpy deallocator. Does this mean that the TF native library can delete a Tensor even though it might still be used in the Python side as a numpy array, referencing the same underlying memory? If not, then how and when are TF tensors deallocated for the Python API?\n\nI'm sorry for the series of questions, but given that I'm building a Scala API aimed to be as close as possible to the Python API, in terms of functionality, I'm trying to get a better understanding of how the interfacing with the native library works and how to do it efficiently, without unnecessary copies.\n\nOn Apr 13, 2017, 1:14 PM -0400, Alexandre Passos <notifications@github.com>, wrote:\n> There is a chance that the C library will reuse the Tensor you pass to\n> session.run after it returns (for example, if the Tensor is enqueued into a\n> fifo queue we reserve the right to not copy it until it's time to dequeue).\n>\n> On Thu, Apr 13, 2017 at 10:07 AM, Anthony Platanios <\n> notifications@github.com> wrote:\n>\n> > @alextp <https://github.com/alextp> As long as I create a\n> > DirectByteBuffer on the Scala side, the GC will be able to deallocate its\n> > underlying buffer. Furthermore, as long as a reference to that\n> > DirectByteBuffer exists on the Scala side, the GC will not deallocate it\n> > and the C side will be able to use it (e.g., when you call session.run). So\n> > I think there would be no memory leakage since the GC will take care of\n> > that buffer. The only occasion where there might be an issue if the GC\n> > deallocates it and then the C library tries to use it. This should not\n> > happen though unless the C library tries to reuse the tensor you pass in\n> > session.run, after that function returns. Does that sound reasonable?\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293962227>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/AAATxX-5lcLpShE_6vTCyUw1V_eDjPZHks5rvlZOgaJpZM4M6yMu>\n> > .\n> >\n>\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub (https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293963686), or mute the thread (https://github.com/notifications/unsubscribe-auth/ABPCXGJtaeZVo4HCPoL0adq5IhTk63gnks5rvlgDgaJpZM4M6yMu).\n>\n>\n>\n\n"}