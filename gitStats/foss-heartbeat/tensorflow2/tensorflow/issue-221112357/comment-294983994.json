{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/294983994", "html_url": "https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-294983994", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9150", "id": 294983994, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDk4Mzk5NA==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-18T21:11:42Z", "updated_at": "2017-04-18T21:11:42Z", "author_association": "MEMBER", "body_html": "<div class=\"email-quoted-reply\">On Tue, Apr 18, 2017 at 1:48 PM, Anthony Platanios ***@***.*** &gt; wrote:\n I have worked out all the core parts except for the fetching tensors from\n sessions. <a class=\"user-mention\" href=\"https://github.com/alextp\">@alextp</a> &lt;<a href=\"https://github.com/alextp\">https://github.com/alextp</a>&gt; you mentioned that if I can\n trigger unrefing of the scala object when TF calls the destructor and\n trigger unrefing of the Tensor when scala is done with the byte buffer all\n will be well I think. I have managed the first part, but I have some\n confusion regarding the second. I currently have no way of unrefing the TF\n tensor when Scala is done with the byte buffer, but I'm not sure if that's\n necessary. The C API documentation for session.run says this:\n\n On success, the tensors corresponding to outputs[0,noutputs-1] are placed\n in\n output_values[]. Ownership of the elements of output_values[] is\n transferred\n to the caller, which must eventually call TF_DeleteTensor on them.\n\n Does this mean that the TensorFlow native library is guaranteed to not use\n that tensor again?\n</div>\n<div class=\"email-fragment\">No, this means the native library will not delete the memory until you call\nTF_DeleteTensor on the tensor (but aliasing considerations involving\nvariables and queues and other stateful things can mean that the tf runtime\nmight still use that memory).</div>\n<div class=\"email-quoted-reply\"> Because in that case, I could return a pointer to the underlying byte\n buffer, along with it's size and let the garbage collector delete the byte\n buffer when it's done using it. Is there anything else that TF_DeleteTensor\n deletes other than that byte buffer?\n</div>\n<div class=\"email-fragment\">The current implementation of TF_Tensor is a struct with a pointer to a\nTensorBuffer structure (which has a pointer to the byte buffer), a dtype,\nand a shape. This structure also gets deleted by TF_DeleteTensor.\n\nBut the API makes no promises that other things won't have to be deleted in\nthe future.</div>\n<div class=\"email-quoted-reply\"> And if so, could I delete that data manually without deleting the buffer?\n\n Looking at the Python API implementation, I think this is the relevant\n piece of code:\n\n   // 4. We now own the fetched tensors, so set up a safe container to\n   // delete them when we exit this scope.\n   Safe_TF_TensorVector tf_outputs_safe;\n   for (const auto&amp; output : outputs) {\n     tf_outputs_safe.emplace_back(make_safe(output));\n   }\n\n   // 5. Convert the fetched tensors into numpy ndarrays. Store them in a safe\n   // container so that we do not leak\n   Safe_PyObjectVector py_outputs_safe;\n   for (size_t i = 0; i &lt; output_names.size(); ++i) {\n     PyObject* py_array;\n     s = TF_Tensor_to_PyObject(std::move(tf_outputs_safe[i]), &amp;py_array);\n     if (!s.ok()) {\n       Set_TF_Status_from_Status(out_status, s);\n       return;\n     }\n     py_outputs_safe.emplace_back(make_safe(py_array));\n   }\n\n   // 6. If we reach this point, we have successfully built a list of objects\n   // so we can release them from the safe container.\n   for (auto&amp; output : py_outputs_safe) {\n     out_values-&gt;push_back(output.release());\n   }\n\n and the make_safe function creates a unique_ptr that calls\n TF_DeleteTensor when unreferenced. My question is whether there is a way to\n avoid that call entirely and let the garbage collector deallocate the\n underlying byte array when necessary.\n</div>\n<div class=\"email-fragment\">You need to make the garbage collector call TF_DeleteTensor instead of\ndeleting the byte array.\n\nSpecially because since that byte array wasn't allocated by the allocator\nattached to your garbage collector there's no way for your garbage\ncollector to know how to return it to the free memory pool.\n-- \n - Alex</div>", "body_text": "On Tue, Apr 18, 2017 at 1:48 PM, Anthony Platanios ***@***.*** > wrote:\n I have worked out all the core parts except for the fetching tensors from\n sessions. @alextp <https://github.com/alextp> you mentioned that if I can\n trigger unrefing of the scala object when TF calls the destructor and\n trigger unrefing of the Tensor when scala is done with the byte buffer all\n will be well I think. I have managed the first part, but I have some\n confusion regarding the second. I currently have no way of unrefing the TF\n tensor when Scala is done with the byte buffer, but I'm not sure if that's\n necessary. The C API documentation for session.run says this:\n\n On success, the tensors corresponding to outputs[0,noutputs-1] are placed\n in\n output_values[]. Ownership of the elements of output_values[] is\n transferred\n to the caller, which must eventually call TF_DeleteTensor on them.\n\n Does this mean that the TensorFlow native library is guaranteed to not use\n that tensor again?\n\nNo, this means the native library will not delete the memory until you call\nTF_DeleteTensor on the tensor (but aliasing considerations involving\nvariables and queues and other stateful things can mean that the tf runtime\nmight still use that memory).\n Because in that case, I could return a pointer to the underlying byte\n buffer, along with it's size and let the garbage collector delete the byte\n buffer when it's done using it. Is there anything else that TF_DeleteTensor\n deletes other than that byte buffer?\n\nThe current implementation of TF_Tensor is a struct with a pointer to a\nTensorBuffer structure (which has a pointer to the byte buffer), a dtype,\nand a shape. This structure also gets deleted by TF_DeleteTensor.\n\nBut the API makes no promises that other things won't have to be deleted in\nthe future.\n And if so, could I delete that data manually without deleting the buffer?\n\n Looking at the Python API implementation, I think this is the relevant\n piece of code:\n\n   // 4. We now own the fetched tensors, so set up a safe container to\n   // delete them when we exit this scope.\n   Safe_TF_TensorVector tf_outputs_safe;\n   for (const auto& output : outputs) {\n     tf_outputs_safe.emplace_back(make_safe(output));\n   }\n\n   // 5. Convert the fetched tensors into numpy ndarrays. Store them in a safe\n   // container so that we do not leak\n   Safe_PyObjectVector py_outputs_safe;\n   for (size_t i = 0; i < output_names.size(); ++i) {\n     PyObject* py_array;\n     s = TF_Tensor_to_PyObject(std::move(tf_outputs_safe[i]), &py_array);\n     if (!s.ok()) {\n       Set_TF_Status_from_Status(out_status, s);\n       return;\n     }\n     py_outputs_safe.emplace_back(make_safe(py_array));\n   }\n\n   // 6. If we reach this point, we have successfully built a list of objects\n   // so we can release them from the safe container.\n   for (auto& output : py_outputs_safe) {\n     out_values->push_back(output.release());\n   }\n\n and the make_safe function creates a unique_ptr that calls\n TF_DeleteTensor when unreferenced. My question is whether there is a way to\n avoid that call entirely and let the garbage collector deallocate the\n underlying byte array when necessary.\n\nYou need to make the garbage collector call TF_DeleteTensor instead of\ndeleting the byte array.\n\nSpecially because since that byte array wasn't allocated by the allocator\nattached to your garbage collector there's no way for your garbage\ncollector to know how to return it to the free memory pool.\n-- \n - Alex", "body": "On Tue, Apr 18, 2017 at 1:48 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> I have worked out all the core parts except for the fetching tensors from\n> sessions. @alextp <https://github.com/alextp> you mentioned that if I can\n> trigger unrefing of the scala object when TF calls the destructor and\n> trigger unrefing of the Tensor when scala is done with the byte buffer all\n> will be well I think. I have managed the first part, but I have some\n> confusion regarding the second. I currently have no way of unrefing the TF\n> tensor when Scala is done with the byte buffer, but I'm not sure if that's\n> necessary. The C API documentation for session.run says this:\n>\n> On success, the tensors corresponding to outputs[0,noutputs-1] are placed\n> in\n> output_values[]. Ownership of the elements of output_values[] is\n> transferred\n> to the caller, which must eventually call TF_DeleteTensor on them.\n>\n> Does this mean that the TensorFlow native library is guaranteed to not use\n> that tensor again?\n>\nNo, this means the native library will not delete the memory until you call\nTF_DeleteTensor on the tensor (but aliasing considerations involving\nvariables and queues and other stateful things can mean that the tf runtime\nmight still use that memory).\n\n\n\n> Because in that case, I could return a pointer to the underlying byte\n> buffer, along with it's size and let the garbage collector delete the byte\n> buffer when it's done using it. Is there anything else that TF_DeleteTensor\n> deletes other than that byte buffer?\n>\nThe current implementation of TF_Tensor is a struct with a pointer to a\nTensorBuffer structure (which has a pointer to the byte buffer), a dtype,\nand a shape. This structure also gets deleted by TF_DeleteTensor.\n\nBut the API makes no promises that other things won't have to be deleted in\nthe future.\n\n\n> And if so, could I delete that data manually without deleting the buffer?\n>\n> Looking at the Python API implementation, I think this is the relevant\n> piece of code:\n>\n>   // 4. We now own the fetched tensors, so set up a safe container to\n>   // delete them when we exit this scope.\n>   Safe_TF_TensorVector tf_outputs_safe;\n>   for (const auto& output : outputs) {\n>     tf_outputs_safe.emplace_back(make_safe(output));\n>   }\n>\n>   // 5. Convert the fetched tensors into numpy ndarrays. Store them in a safe\n>   // container so that we do not leak\n>   Safe_PyObjectVector py_outputs_safe;\n>   for (size_t i = 0; i < output_names.size(); ++i) {\n>     PyObject* py_array;\n>     s = TF_Tensor_to_PyObject(std::move(tf_outputs_safe[i]), &py_array);\n>     if (!s.ok()) {\n>       Set_TF_Status_from_Status(out_status, s);\n>       return;\n>     }\n>     py_outputs_safe.emplace_back(make_safe(py_array));\n>   }\n>\n>   // 6. If we reach this point, we have successfully built a list of objects\n>   // so we can release them from the safe container.\n>   for (auto& output : py_outputs_safe) {\n>     out_values->push_back(output.release());\n>   }\n>\n> and the make_safe function creates a unique_ptr that calls\n> TF_DeleteTensor when unreferenced. My question is whether there is a way to\n> avoid that call entirely and let the garbage collector deallocate the\n> underlying byte array when necessary.\n>\nYou need to make the garbage collector call TF_DeleteTensor instead of\ndeleting the byte array.\n\nSpecially because since that byte array wasn't allocated by the allocator\nattached to your garbage collector there's no way for your garbage\ncollector to know how to return it to the free memory pool.\n-- \n - Alex\n"}