{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296821163", "html_url": "https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-296821163", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9150", "id": 296821163, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjgyMTE2Mw==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-24T21:01:14Z", "updated_at": "2017-04-24T21:01:14Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">It is possible to implement cond without rewiring (though the python\nimplementation uses rewiring to capture inputs).\n\nOn Mon, Apr 24, 2017 at 1:58 PM, Alexandre Passos &lt;notifications@github.com&gt;\nwrote:</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\"> I believe the gradients for loops in python add loop variables and stacks\n and other complicated things to the forward pass so I doubt you will be\n able to replicate the same code.\n\n On Mon, Apr 24, 2017 at 1:55 PM, Anthony Platanios &lt;\n ***@***.***\n &gt; wrote:\n\n &gt; I see. I hadn't gotten to that part yet. And yes, you're right; the\n &gt; experimental flag has been removed or I simply remembered wrong. In that\n &gt; case, my question is whether I can add gradient computation code on top\n of\n &gt; the TF_NewWhile/TF_FinishWhile methods. I was planning to replicate the\n way\n &gt; in which gradients are computed in Python, but is that possible or does\n &gt; that also require adding inputs to ops after constructing them? Thanks!\n &gt;\n &gt; \u2014\n &gt; You are receiving this because you were mentioned.\n &gt; Reply to this email directly, view it on GitHub\n &gt; &lt;<a href=\"https://github.com/tensorflow/tensorflow/issues/\">https://github.com/tensorflow/tensorflow/issues/</a>\n 9150#issuecomment-296819142&gt;,\n &gt; or mute the thread\n &gt; &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/\">https://github.com/notifications/unsubscribe-auth/</a>\n AAATxVeyFQNrz9w6IK6OfUdQVmQcbbd6ks5rzQwogaJpZM4M6yMu&gt;\n &gt; .\n &gt;\n\n\n\n --\n - Alex\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"221112357\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9150\" href=\"https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-296820045\">#9150 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxfsDKB8OPrmhYh3ELApDpUSjJfAHks5rzQ0HgaJpZM4M6yMu\">https://github.com/notifications/unsubscribe-auth/AAATxfsDKB8OPrmhYh3ELApDpUSjJfAHks5rzQ0HgaJpZM4M6yMu</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "It is possible to implement cond without rewiring (though the python\nimplementation uses rewiring to capture inputs).\n\nOn Mon, Apr 24, 2017 at 1:58 PM, Alexandre Passos <notifications@github.com>\nwrote:\n\u2026\n I believe the gradients for loops in python add loop variables and stacks\n and other complicated things to the forward pass so I doubt you will be\n able to replicate the same code.\n\n On Mon, Apr 24, 2017 at 1:55 PM, Anthony Platanios <\n ***@***.***\n > wrote:\n\n > I see. I hadn't gotten to that part yet. And yes, you're right; the\n > experimental flag has been removed or I simply remembered wrong. In that\n > case, my question is whether I can add gradient computation code on top\n of\n > the TF_NewWhile/TF_FinishWhile methods. I was planning to replicate the\n way\n > in which gradients are computed in Python, but is that possible or does\n > that also require adding inputs to ops after constructing them? Thanks!\n >\n > \u2014\n > You are receiving this because you were mentioned.\n > Reply to this email directly, view it on GitHub\n > <https://github.com/tensorflow/tensorflow/issues/\n 9150#issuecomment-296819142>,\n > or mute the thread\n > <https://github.com/notifications/unsubscribe-auth/\n AAATxVeyFQNrz9w6IK6OfUdQVmQcbbd6ks5rzQwogaJpZM4M6yMu>\n > .\n >\n\n\n\n --\n - Alex\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n <#9150 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAATxfsDKB8OPrmhYh3ELApDpUSjJfAHks5rzQ0HgaJpZM4M6yMu>\n .\n\n\n-- \n - Alex", "body": "It is possible to implement cond without rewiring (though the python\nimplementation uses rewiring to capture inputs).\n\nOn Mon, Apr 24, 2017 at 1:58 PM, Alexandre Passos <notifications@github.com>\nwrote:\n\n> I believe the gradients for loops in python add loop variables and stacks\n> and other complicated things to the forward pass so I doubt you will be\n> able to replicate the same code.\n>\n> On Mon, Apr 24, 2017 at 1:55 PM, Anthony Platanios <\n> notifications@github.com\n> > wrote:\n>\n> > I see. I hadn't gotten to that part yet. And yes, you're right; the\n> > experimental flag has been removed or I simply remembered wrong. In that\n> > case, my question is whether I can add gradient computation code on top\n> of\n> > the TF_NewWhile/TF_FinishWhile methods. I was planning to replicate the\n> way\n> > in which gradients are computed in Python, but is that possible or does\n> > that also require adding inputs to ops after constructing them? Thanks!\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 9150#issuecomment-296819142>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AAATxVeyFQNrz9w6IK6OfUdQVmQcbbd6ks5rzQwogaJpZM4M6yMu>\n> > .\n> >\n>\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-296820045>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxfsDKB8OPrmhYh3ELApDpUSjJfAHks5rzQ0HgaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n"}