{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293927995", "html_url": "https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-293927995", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9150", "id": 293927995, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MzkyNzk5NQ==", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-13T15:24:58Z", "updated_at": "2017-04-13T15:24:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> Thank you for that response! That's what I am currently doing in the Scala library (sharing the buffer), and I manage the indexing separately on top of it. If you slice it, I return a TensorSlice object which simply contains the slice information in it along with the original Tensor reference, and adjusts the indexing appropriately to account for the slice. The reason I am not returning a new Tensor with a view to part of the buffer is that a slice does have to be contiguous in memory (except for slices along the first dimension with stride 1). This is quite inefficient in my opinion and that's why I was thinking of the possibility of using the kernel implementation.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> I see. That makes sense. I assume you do that <a href=\"https://github.com/tensorflow/tensorflow/blob/97c6203bb3f3978ac67920c66b6234ef82051c57/tensorflow/python/client/tf_session_helper.cc#L546\">here</a>, where you create a tensor by providing a pointer to the numpy array data. I could do that using Java ByteBuffers in Scala. My only problem is the deallocator. Would it be reasonable to provide a no-op to the deallocator (as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> mentioned) and let the JVM GC handle memory? As long as it is only used for a session.run call, then the GC should not mess with it (given it's in scope).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> Would it be easy to expose the CPU kernels only for CPU Tensors? That would provide a numpy-like library effectively, that could be used from Java, Scala, or other languages (through the C API). The kernel implementations could be called using something equivalent of the TF_OperationDecsription that's currently being used for creating ops. That would be very convenient as currently I have to build support for a tensor library from scratch on the Scala side, manipulating byte buffers that have the exact same structure as the TF_Tensor buffers.</p>", "body_text": "@asimshankar Thank you for that response! That's what I am currently doing in the Scala library (sharing the buffer), and I manage the indexing separately on top of it. If you slice it, I return a TensorSlice object which simply contains the slice information in it along with the original Tensor reference, and adjusts the indexing appropriately to account for the slice. The reason I am not returning a new Tensor with a view to part of the buffer is that a slice does have to be contiguous in memory (except for slices along the first dimension with stride 1). This is quite inefficient in my opinion and that's why I was thinking of the possibility of using the kernel implementation.\n@alextp I see. That makes sense. I assume you do that here, where you create a tensor by providing a pointer to the numpy array data. I could do that using Java ByteBuffers in Scala. My only problem is the deallocator. Would it be reasonable to provide a no-op to the deallocator (as @asimshankar mentioned) and let the JVM GC handle memory? As long as it is only used for a session.run call, then the GC should not mess with it (given it's in scope).\n@alextp Would it be easy to expose the CPU kernels only for CPU Tensors? That would provide a numpy-like library effectively, that could be used from Java, Scala, or other languages (through the C API). The kernel implementations could be called using something equivalent of the TF_OperationDecsription that's currently being used for creating ops. That would be very convenient as currently I have to build support for a tensor library from scratch on the Scala side, manipulating byte buffers that have the exact same structure as the TF_Tensor buffers.", "body": "@asimshankar Thank you for that response! That's what I am currently doing in the Scala library (sharing the buffer), and I manage the indexing separately on top of it. If you slice it, I return a TensorSlice object which simply contains the slice information in it along with the original Tensor reference, and adjusts the indexing appropriately to account for the slice. The reason I am not returning a new Tensor with a view to part of the buffer is that a slice does have to be contiguous in memory (except for slices along the first dimension with stride 1). This is quite inefficient in my opinion and that's why I was thinking of the possibility of using the kernel implementation.\r\n\r\n@alextp I see. That makes sense. I assume you do that [here](https://github.com/tensorflow/tensorflow/blob/97c6203bb3f3978ac67920c66b6234ef82051c57/tensorflow/python/client/tf_session_helper.cc#L546), where you create a tensor by providing a pointer to the numpy array data. I could do that using Java ByteBuffers in Scala. My only problem is the deallocator. Would it be reasonable to provide a no-op to the deallocator (as @asimshankar mentioned) and let the JVM GC handle memory? As long as it is only used for a session.run call, then the GC should not mess with it (given it's in scope).\r\n\r\n@alextp Would it be easy to expose the CPU kernels only for CPU Tensors? That would provide a numpy-like library effectively, that could be used from Java, Scala, or other languages (through the C API). The kernel implementations could be called using something equivalent of the TF_OperationDecsription that's currently being used for creating ops. That would be very convenient as currently I have to build support for a tensor library from scratch on the Scala side, manipulating byte buffers that have the exact same structure as the TF_Tensor buffers."}