{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296820045", "html_url": "https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-296820045", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9150", "id": 296820045, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjgyMDA0NQ==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-24T20:56:58Z", "updated_at": "2017-04-24T20:56:58Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">I believe the gradients for loops in python add loop variables and stacks\nand other complicated things to the forward pass so I doubt you will be\nable to replicate the same code.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Mon, Apr 24, 2017 at 1:55 PM, Anthony Platanios ***@***.*** &gt; wrote:\n I see. I hadn't gotten to that part yet. And yes, you're right; the\n experimental flag has been removed or I simply remembered wrong. In that\n case, my question is whether I can add gradient computation code on top of\n the TF_NewWhile/TF_FinishWhile methods. I was planning to replicate the way\n in which gradients are computed in Python, but is that possible or does\n that also require adding inputs to ops after constructing them? Thanks!\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"221112357\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9150\" href=\"https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-296819142\">#9150 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxVeyFQNrz9w6IK6OfUdQVmQcbbd6ks5rzQwogaJpZM4M6yMu\">https://github.com/notifications/unsubscribe-auth/AAATxVeyFQNrz9w6IK6OfUdQVmQcbbd6ks5rzQwogaJpZM4M6yMu</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "I believe the gradients for loops in python add loop variables and stacks\nand other complicated things to the forward pass so I doubt you will be\nable to replicate the same code.\n\u2026\nOn Mon, Apr 24, 2017 at 1:55 PM, Anthony Platanios ***@***.*** > wrote:\n I see. I hadn't gotten to that part yet. And yes, you're right; the\n experimental flag has been removed or I simply remembered wrong. In that\n case, my question is whether I can add gradient computation code on top of\n the TF_NewWhile/TF_FinishWhile methods. I was planning to replicate the way\n in which gradients are computed in Python, but is that possible or does\n that also require adding inputs to ops after constructing them? Thanks!\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#9150 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAATxVeyFQNrz9w6IK6OfUdQVmQcbbd6ks5rzQwogaJpZM4M6yMu>\n .\n\n\n-- \n - Alex", "body": "I believe the gradients for loops in python add loop variables and stacks\nand other complicated things to the forward pass so I doubt you will be\nable to replicate the same code.\n\nOn Mon, Apr 24, 2017 at 1:55 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> I see. I hadn't gotten to that part yet. And yes, you're right; the\n> experimental flag has been removed or I simply remembered wrong. In that\n> case, my question is whether I can add gradient computation code on top of\n> the TF_NewWhile/TF_FinishWhile methods. I was planning to replicate the way\n> in which gradients are computed in Python, but is that possible or does\n> that also require adding inputs to ops after constructing them? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9150#issuecomment-296819142>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVeyFQNrz9w6IK6OfUdQVmQcbbd6ks5rzQwogaJpZM4M6yMu>\n> .\n>\n\n\n\n-- \n - Alex\n"}