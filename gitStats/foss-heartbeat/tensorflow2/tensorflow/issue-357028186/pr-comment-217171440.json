{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/217171440", "pull_request_review_id": 153971420, "id": 217171440, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzE3MTQ0MA==", "diff_hunk": "@@ -0,0 +1,356 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+\n+#ifdef INTEL_MKL\n+#ifndef INTEL_MKL_ML_ONLY\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/lib/gtl/array_slice.h\"\n+#include \"tensorflow/core/platform/prefetch.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+#include \"mkldnn.hpp\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+\n+using mkldnn::stream;\n+using mkldnn::view;\n+\n+namespace tensorflow {\n+\n+namespace {\n+\n+gtl::InlinedVector<int64, 4> IntTensorToInt64Vec(const Tensor& tensor) {\n+  gtl::InlinedVector<int64, 4> out;\n+  if (tensor.dtype() == DT_INT32) {\n+    for (int64 i = 0; i < tensor.NumElements(); ++i) {\n+      out.push_back(tensor.flat<int32>()(i));\n+    }\n+  } else if (tensor.dtype() == DT_INT64) {\n+    for (int64 i = 0; i < tensor.NumElements(); ++i) {\n+      out.push_back(tensor.flat<int64>()(i));\n+    }\n+  } else {\n+    LOG(FATAL) << \"begin must be either int32 or int64\";\n+  }\n+  return out;\n+}\n+\n+}  // namespace\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+// A version of SharedValidation (slice_op.h) written for input that is in\n+// either Mkl layout or Tensorflow layout.\n+static void ValidateMklInputs(OpKernelContext* context, bool* is_identity,\n+                              gtl::InlinedVector<int64, 4>* begin,\n+                              gtl::InlinedVector<int64, 4>* size) {\n+  const int kInputTensorIndex = 0;\n+  const int kInputBeginIndex = 1;\n+  const int kInputSizeIndex = 2;\n+  const Tensor& input = MklGetInput(context, kInputTensorIndex);\n+  const Tensor& begin_tensor = MklGetInput(context, kInputBeginIndex);\n+  const Tensor& size_tensor = MklGetInput(context, kInputSizeIndex);\n+\n+  MklDnnShape input_mkl_shape, begin_mkl_shape, size_mkl_shape;\n+  GetMklShape(context, kInputTensorIndex, &input_mkl_shape);\n+  GetMklShape(context, kInputBeginIndex, &begin_mkl_shape);\n+  GetMklShape(context, kInputSizeIndex, &size_mkl_shape);\n+\n+  // Begin and size tensors cannot be in MklDnn layout.\n+  CHECK_EQ(begin_mkl_shape.IsMklTensor(), false);\n+  CHECK_EQ(size_mkl_shape.IsMklTensor(), false);\n+\n+  TensorShape input_tf_shape = input_mkl_shape.IsMklTensor()\n+                                   ? input_mkl_shape.GetTfShape()\n+                                   : input.shape();\n+\n+  OP_REQUIRES(\n+      context, context->op_kernel().IsLegacyVector(begin_tensor.shape()) &&\n+                   context->op_kernel().IsLegacyVector(size_tensor.shape()) &&\n+                   begin_tensor.NumElements() == input_tf_shape.dims() &&\n+                   size_tensor.NumElements() == input_tf_shape.dims(),\n+      errors::InvalidArgument(\n+          \"Expected begin and size arguments to be 1-D tensors of size \",\n+          input_tf_shape.dims(), \", but got shapes \",\n+          begin_tensor.shape().DebugString(), \" and \",\n+          size_tensor.shape().DebugString(), \" instead.\"));\n+\n+  const int input_dims = input_tf_shape.dims();\n+  *begin = IntTensorToInt64Vec(begin_tensor);\n+  *size = IntTensorToInt64Vec(size_tensor);\n+  for (int i = 0; i < input_dims; ++i) {\n+    if ((*size)[i] == -1) {\n+      // A size[i] of -1 means \"all elements from begin[i] to dim_size(i)\".\n+      (*size)[i] = input_tf_shape.dim_size(i) - (*begin)[i];\n+    }\n+  }\n+\n+  *is_identity = true;\n+  for (int i = 0; i < input_dims; ++i) {\n+    int64 b = (*begin)[i];\n+    int64 s = (*size)[i];\n+    if (input_tf_shape.dim_size(i) == 0) {\n+      OP_REQUIRES(\n+          context, b == 0 && s == 0,\n+          errors::InvalidArgument(\"Expected begin[\", i, \"] == 0 (got \", b,\n+                                  \") and size[\", i, \"] == 0 \", \"(got \", s,\n+                                  \") when \", \"input.dim_size(\", i, \") == 0\"));\n+    } else {\n+      OP_REQUIRES(context, 0 <= b && b <= input_tf_shape.dim_size(i),\n+                  errors::InvalidArgument(\"Expected begin[\", i, \"] in [0, \",\n+                                          input_tf_shape.dim_size(i),\n+                                          \"], but got \", b));\n+      OP_REQUIRES(context, 0 <= s && b + s <= input_tf_shape.dim_size(i),\n+                  errors::InvalidArgument(\"Expected size[\", i, \"] in [0, \",\n+                                          input_tf_shape.dim_size(i) - b,\n+                                          \"], but \", \"got \", s));\n+    }\n+    const bool take_all = (b == 0) && (s == input_tf_shape.dim_size(i));\n+    (*is_identity) &= take_all;\n+  }\n+}\n+\n+// A version of SharedSliceCommonCases function written for input tensor\n+// that may be in MklDnn layout or in Tensorflow layout.\n+template <typename T>\n+static void CheckCommonCasesForMklInputs(OpKernelContext* context,\n+                                         gtl::InlinedVector<int64, 4>* begin,\n+                                         gtl::InlinedVector<int64, 4>* size,\n+                                         bool* done) {\n+  bool is_identity = true;\n+  *done = false;\n+\n+  ValidateMklInputs(context, &is_identity, begin, size);\n+  if (!context->status().ok()) return;\n+\n+  const Tensor& input = MklGetInput(context, 0);\n+  MklDnnShape input_mkl_shape;\n+  GetMklShape(context, 0, &input_mkl_shape);\n+\n+  if (is_identity) {\n+    VLOG(1) << \"Slice identity\";\n+    context->set_output(0, input);\n+    // Mkl metadata tensor in this case can just be forwarded from input to\n+    // output.\n+    AllocateOutputSetMklShape(context, 0, input_mkl_shape);\n+    *done = true;\n+    return;\n+  }\n+}\n+\n+// MKL-DNN implementation of Slice\n+template <typename Device, typename T>\n+class MklDnnSliceOp : public OpKernel {\n+ public:\n+  explicit MklDnnSliceOp(OpKernelConstruction* context) : OpKernel(context) {}\n+\n+  ~MklDnnSliceOp() {}\n+\n+  void Compute(OpKernelContext* context) override {\n+    gtl::InlinedVector<int64, 4> begin;\n+    gtl::InlinedVector<int64, 4> size;\n+    bool done = false;\n+\n+    CheckCommonCasesForMklInputs<T>(context, &begin, &size, &done);\n+    if (!context->status().ok() || done == true) return;\n+\n+    // MKL-DNN does not have this limitation of supporting less than 8 dimension\n+    // tensor. But we are mimicking functionality of Eigen Slice op for CPU.\n+    if (begin.size() >= 8) {\n+      OP_REQUIRES(\n+          context, false,\n+          errors::Unimplemented(\"MklDnnSliceOp : Unhandled input dimensions\"));\n+    }\n+\n+    ComputeMklDnnSlice(context, begin, size);\n+    return;", "path": "tensorflow/core/kernels/mkl_slice_op.cc", "position": null, "original_position": 184, "commit_id": "d1ab8b71c2115caacfec19d849ddabf7f1f4287b", "original_commit_id": "6712df7f3c73bfabab51e7c7eed2130d7bcff6ec", "user": {"login": "penpornk", "id": 38085909, "node_id": "MDQ6VXNlcjM4MDg1OTA5", "avatar_url": "https://avatars3.githubusercontent.com/u/38085909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/penpornk", "html_url": "https://github.com/penpornk", "followers_url": "https://api.github.com/users/penpornk/followers", "following_url": "https://api.github.com/users/penpornk/following{/other_user}", "gists_url": "https://api.github.com/users/penpornk/gists{/gist_id}", "starred_url": "https://api.github.com/users/penpornk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/penpornk/subscriptions", "organizations_url": "https://api.github.com/users/penpornk/orgs", "repos_url": "https://api.github.com/users/penpornk/repos", "events_url": "https://api.github.com/users/penpornk/events{/privacy}", "received_events_url": "https://api.github.com/users/penpornk/received_events", "type": "User", "site_admin": false}, "body": "We don't really need a `return` here.", "created_at": "2018-09-12T20:09:31Z", "updated_at": "2018-09-25T03:16:24Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22076#discussion_r217171440", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22076", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/217171440"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22076#discussion_r217171440"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22076"}}, "body_html": "<p>We don't really need a <code>return</code> here.</p>", "body_text": "We don't really need a return here."}