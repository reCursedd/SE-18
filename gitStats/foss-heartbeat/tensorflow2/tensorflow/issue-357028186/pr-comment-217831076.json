{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/217831076", "pull_request_review_id": 153971420, "id": 217831076, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzgzMTA3Ng==", "diff_hunk": "@@ -0,0 +1,356 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/array_ops.cc.\n+\n+#ifdef INTEL_MKL\n+#ifndef INTEL_MKL_ML_ONLY\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/lib/gtl/array_slice.h\"\n+#include \"tensorflow/core/platform/prefetch.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+#include \"mkldnn.hpp\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+\n+using mkldnn::stream;\n+using mkldnn::view;\n+\n+namespace tensorflow {\n+\n+namespace {\n+\n+gtl::InlinedVector<int64, 4> IntTensorToInt64Vec(const Tensor& tensor) {\n+  gtl::InlinedVector<int64, 4> out;\n+  if (tensor.dtype() == DT_INT32) {\n+    for (int64 i = 0; i < tensor.NumElements(); ++i) {\n+      out.push_back(tensor.flat<int32>()(i));\n+    }\n+  } else if (tensor.dtype() == DT_INT64) {\n+    for (int64 i = 0; i < tensor.NumElements(); ++i) {\n+      out.push_back(tensor.flat<int64>()(i));\n+    }\n+  } else {\n+    LOG(FATAL) << \"begin must be either int32 or int64\";\n+  }\n+  return out;\n+}\n+\n+}  // namespace\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+// A version of SharedValidation (slice_op.h) written for input that is in\n+// either Mkl layout or Tensorflow layout.\n+static void ValidateMklInputs(OpKernelContext* context, bool* is_identity,\n+                              gtl::InlinedVector<int64, 4>* begin,\n+                              gtl::InlinedVector<int64, 4>* size) {\n+  const int kInputTensorIndex = 0;\n+  const int kInputBeginIndex = 1;\n+  const int kInputSizeIndex = 2;\n+  const Tensor& input = MklGetInput(context, kInputTensorIndex);\n+  const Tensor& begin_tensor = MklGetInput(context, kInputBeginIndex);\n+  const Tensor& size_tensor = MklGetInput(context, kInputSizeIndex);\n+\n+  MklDnnShape input_mkl_shape, begin_mkl_shape, size_mkl_shape;\n+  GetMklShape(context, kInputTensorIndex, &input_mkl_shape);\n+  GetMklShape(context, kInputBeginIndex, &begin_mkl_shape);\n+  GetMklShape(context, kInputSizeIndex, &size_mkl_shape);\n+\n+  // Begin and size tensors cannot be in MklDnn layout.\n+  CHECK_EQ(begin_mkl_shape.IsMklTensor(), false);\n+  CHECK_EQ(size_mkl_shape.IsMklTensor(), false);\n+\n+  TensorShape input_tf_shape = input_mkl_shape.IsMklTensor()\n+                                   ? input_mkl_shape.GetTfShape()\n+                                   : input.shape();\n+\n+  OP_REQUIRES(\n+      context, context->op_kernel().IsLegacyVector(begin_tensor.shape()) &&\n+                   context->op_kernel().IsLegacyVector(size_tensor.shape()) &&\n+                   begin_tensor.NumElements() == input_tf_shape.dims() &&\n+                   size_tensor.NumElements() == input_tf_shape.dims(),\n+      errors::InvalidArgument(\n+          \"Expected begin and size arguments to be 1-D tensors of size \",\n+          input_tf_shape.dims(), \", but got shapes \",\n+          begin_tensor.shape().DebugString(), \" and \",\n+          size_tensor.shape().DebugString(), \" instead.\"));\n+\n+  const int input_dims = input_tf_shape.dims();\n+  *begin = IntTensorToInt64Vec(begin_tensor);\n+  *size = IntTensorToInt64Vec(size_tensor);\n+  for (int i = 0; i < input_dims; ++i) {\n+    if ((*size)[i] == -1) {\n+      // A size[i] of -1 means \"all elements from begin[i] to dim_size(i)\".\n+      (*size)[i] = input_tf_shape.dim_size(i) - (*begin)[i];\n+    }\n+  }\n+\n+  *is_identity = true;\n+  for (int i = 0; i < input_dims; ++i) {\n+    int64 b = (*begin)[i];\n+    int64 s = (*size)[i];\n+    if (input_tf_shape.dim_size(i) == 0) {\n+      OP_REQUIRES(\n+          context, b == 0 && s == 0,\n+          errors::InvalidArgument(\"Expected begin[\", i, \"] == 0 (got \", b,\n+                                  \") and size[\", i, \"] == 0 \", \"(got \", s,\n+                                  \") when \", \"input.dim_size(\", i, \") == 0\"));\n+    } else {\n+      OP_REQUIRES(context, 0 <= b && b <= input_tf_shape.dim_size(i),\n+                  errors::InvalidArgument(\"Expected begin[\", i, \"] in [0, \",\n+                                          input_tf_shape.dim_size(i),\n+                                          \"], but got \", b));\n+      OP_REQUIRES(context, 0 <= s && b + s <= input_tf_shape.dim_size(i),\n+                  errors::InvalidArgument(\"Expected size[\", i, \"] in [0, \",\n+                                          input_tf_shape.dim_size(i) - b,\n+                                          \"], but \", \"got \", s));\n+    }\n+    const bool take_all = (b == 0) && (s == input_tf_shape.dim_size(i));\n+    (*is_identity) &= take_all;\n+  }\n+}\n+\n+// A version of SharedSliceCommonCases function written for input tensor\n+// that may be in MklDnn layout or in Tensorflow layout.\n+template <typename T>\n+static void CheckCommonCasesForMklInputs(OpKernelContext* context,\n+                                         gtl::InlinedVector<int64, 4>* begin,\n+                                         gtl::InlinedVector<int64, 4>* size,\n+                                         bool* done) {\n+  bool is_identity = true;\n+  *done = false;\n+\n+  ValidateMklInputs(context, &is_identity, begin, size);\n+  if (!context->status().ok()) return;\n+\n+  const Tensor& input = MklGetInput(context, 0);\n+  MklDnnShape input_mkl_shape;\n+  GetMklShape(context, 0, &input_mkl_shape);\n+\n+  if (is_identity) {\n+    VLOG(1) << \"Slice identity\";\n+    context->set_output(0, input);\n+    // Mkl metadata tensor in this case can just be forwarded from input to\n+    // output.\n+    AllocateOutputSetMklShape(context, 0, input_mkl_shape);\n+    *done = true;\n+    return;\n+  }\n+}\n+\n+// MKL-DNN implementation of Slice\n+template <typename Device, typename T>\n+class MklDnnSliceOp : public OpKernel {\n+ public:\n+  explicit MklDnnSliceOp(OpKernelConstruction* context) : OpKernel(context) {}\n+\n+  ~MklDnnSliceOp() {}\n+\n+  void Compute(OpKernelContext* context) override {\n+    gtl::InlinedVector<int64, 4> begin;\n+    gtl::InlinedVector<int64, 4> size;\n+    bool done = false;\n+\n+    CheckCommonCasesForMklInputs<T>(context, &begin, &size, &done);\n+    if (!context->status().ok() || done == true) return;\n+\n+    // MKL-DNN does not have this limitation of supporting less than 8 dimension\n+    // tensor. But we are mimicking functionality of Eigen Slice op for CPU.\n+    if (begin.size() >= 8) {\n+      OP_REQUIRES(\n+          context, false,\n+          errors::Unimplemented(\"MklDnnSliceOp : Unhandled input dimensions\"));\n+    }\n+\n+    ComputeMklDnnSlice(context, begin, size);\n+    return;\n+  }\n+\n+ private:\n+  // Slice op implemented using MKL-DNN APIs.\n+  void ComputeMklDnnSlice(OpKernelContext* context,\n+                          const gtl::InlinedVector<int64, 4>& begin,\n+                          const gtl::InlinedVector<int64, 4>& size) {\n+    try {\n+      // MKL-DNN API usage below is guided by description at:\n+      //  https://github.com/01org/mkl-dnn/issues/69\n+      //\n+      // Relevant part of the description is copied below:\n+      //\n+      // Let's say you want to copy a part of memory into another buffer (and\n+      // probably change the format). Then your steps are:\n+      //\n+      // 1. create memory primitive descriptor in_mem_pd and memory primitive\n+      //    in_mem_p for the entire source data.\n+      // 2. create view primitive descriptor in_submem_pd based on in_mem_pd,\n+      //    initial offsets, and sub-sizes\n+      // 3. create memory primitive descriptor out_mem_pd and memory primitive\n+      //    out_mem_p for the output (the logical sizes should much sub-sizes\n+      //    used in step 2, but the format might be arbitrary)\n+      // 4. create reorder primitive descriptor reorder_pd based on in_submem_pd\n+      //    and out_mem_pd\n+      // 5. create reorder primitive itself based on reorder_pd, in_mem_p, and\n+      //    out_mem_p.\n+      //\n+      // Please notice that there is no view primitive. There is only view\n+      // primitive descriptor. And the reorder uses source memory as input but\n+      // traverses it according to a view in_submem_pd.\n+\n+      auto cpu_engine = engine(engine::cpu, 0);\n+      MklDnnData<T> src(&cpu_engine);\n+      MklDnnData<T> output(&cpu_engine);\n+\n+      // Populate offsets and sizes in memory::dims format based on vector.\n+      memory::dims begin_dims = {};\n+      begin_dims.resize(begin.size());\n+      for (size_t i = 0; i < begin.size(); ++i) begin_dims[i] = begin[i];\n+      memory::dims size_dims = {};\n+      bool empty = false;\n+      size_dims.resize(size.size());\n+      for (size_t i = 0; i < size.size(); ++i) {\n+        size_dims[i] = size[i];\n+        if (size_dims[i] == 0) empty = true;\n+      }\n+\n+      Tensor* output_tensor = nullptr;\n+      MklDnnShape output_mkl_shape;\n+      if (empty) {  // for empty dims\n+        auto shape_to = MklDnnDimsToTFShape(size_dims);\n+        AllocateOutputSetMklShape(context, 0, &output_tensor, shape_to,\n+                                  output_mkl_shape);\n+        return;\n+      }\n+\n+      // Step 1 (as per above description) - Create memory for user data.\n+      // We use blocked format here to describe input tensor.\n+      const Tensor& input_tensor = MklGetInput(context, 0);\n+      MklDnnShape input_mkl_shape;\n+      GetMklShape(context, 0, &input_mkl_shape);\n+\n+      if (input_mkl_shape.IsMklTensor()) {\n+        auto input_mkl_format = input_mkl_shape.GetTfDataFormat();\n+        auto input_tf_format = MklDnnDataFormatToTFDataFormat(input_mkl_format);\n+        begin_dims = MklDnnDimsInNCHW(begin_dims, input_tf_format);\n+        size_dims = MklDnnDimsInNCHW(size_dims, input_tf_format);\n+      }\n+\n+      // Initialize input dimensions and strides to be used when input is not in\n+      // MklDnn layout.\n+      memory::dims input_dims, input_strides;\n+      if (!input_mkl_shape.IsMklTensor()) {\n+        input_dims = TFShapeToMklDnnDims(input_tensor.shape());\n+        input_strides = CalculateTFStrides(input_dims);\n+      }\n+\n+      // Create input memory descriptor.\n+      auto input_md =\n+          input_mkl_shape.IsMklTensor()\n+              ? input_mkl_shape.GetMklLayout()\n+              : MklDnnData<T>::CreateBlockedMemDesc(input_dims, input_strides);\n+      src.SetUsrMem(input_md, &input_tensor);\n+\n+      // Step 2 - create view primitive descriptor\n+      auto view_pd =\n+          view::primitive_desc(src.GetUsrMemPrimDesc(), size_dims, begin_dims)\n+              .dst_primitive_desc();\n+      auto output_strides = CalculateTFStrides(size_dims);\n+      auto output_md =\n+          MklDnnData<T>::CreateBlockedMemDesc(size_dims, output_strides);\n+      auto output_pd = memory::primitive_desc(output_md, cpu_engine);\n+\n+      // Step 3 - Create memory for output. If input is in MklDnn layout, then\n+      // output is also in MklDnn layout. Otherwise, output is in Tensorflow\n+      // layout.\n+      AllocateOutputTensor(context, input_mkl_shape, &output_pd, size_dims,\n+                           &output_tensor, &output_mkl_shape);\n+      CHECK_NOTNULL(output_tensor);\n+      CHECK_EQ(input_mkl_shape.IsMklTensor(), output_mkl_shape.IsMklTensor());\n+      output.SetUsrMem(output_md, output_tensor);\n+\n+      std::vector<primitive> net;\n+      // Step 4 - create reorder primitive desc between view_pd and output_pd.\n+      auto reorder_pd =\n+          reorder::primitive_desc(view_pd, output.GetUsrMemPrimDesc());\n+      // Step 5 - create reorder primitive itself.\n+      net.push_back(reorder(reorder_pd, *src.GetUsrMem(), *output.GetUsrMem()));\n+      stream(stream::kind::eager).submit(net).wait();", "path": "tensorflow/core/kernels/mkl_slice_op.cc", "position": null, "original_position": 294, "commit_id": "d1ab8b71c2115caacfec19d849ddabf7f1f4287b", "original_commit_id": "6712df7f3c73bfabab51e7c7eed2130d7bcff6ec", "user": {"login": "penpornk", "id": 38085909, "node_id": "MDQ6VXNlcjM4MDg1OTA5", "avatar_url": "https://avatars3.githubusercontent.com/u/38085909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/penpornk", "html_url": "https://github.com/penpornk", "followers_url": "https://api.github.com/users/penpornk/followers", "following_url": "https://api.github.com/users/penpornk/following{/other_user}", "gists_url": "https://api.github.com/users/penpornk/gists{/gist_id}", "starred_url": "https://api.github.com/users/penpornk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/penpornk/subscriptions", "organizations_url": "https://api.github.com/users/penpornk/orgs", "repos_url": "https://api.github.com/users/penpornk/repos", "events_url": "https://api.github.com/users/penpornk/events{/privacy}", "received_events_url": "https://api.github.com/users/penpornk/received_events", "type": "User", "site_admin": false}, "body": "Please add a comment that this line executes the primitive.", "created_at": "2018-09-14T20:18:27Z", "updated_at": "2018-09-25T03:16:24Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22076#discussion_r217831076", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22076", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/217831076"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22076#discussion_r217831076"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22076"}}, "body_html": "<p>Please add a comment that this line executes the primitive.</p>", "body_text": "Please add a comment that this line executes the primitive."}