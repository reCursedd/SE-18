{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/350419663", "html_url": "https://github.com/tensorflow/tensorflow/issues/14726#issuecomment-350419663", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14726", "id": 350419663, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDQxOTY2Mw==", "user": {"login": "dennismckinnon", "id": 2585328, "node_id": "MDQ6VXNlcjI1ODUzMjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2585328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dennismckinnon", "html_url": "https://github.com/dennismckinnon", "followers_url": "https://api.github.com/users/dennismckinnon/followers", "following_url": "https://api.github.com/users/dennismckinnon/following{/other_user}", "gists_url": "https://api.github.com/users/dennismckinnon/gists{/gist_id}", "starred_url": "https://api.github.com/users/dennismckinnon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dennismckinnon/subscriptions", "organizations_url": "https://api.github.com/users/dennismckinnon/orgs", "repos_url": "https://api.github.com/users/dennismckinnon/repos", "events_url": "https://api.github.com/users/dennismckinnon/events{/privacy}", "received_events_url": "https://api.github.com/users/dennismckinnon/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-09T03:13:07Z", "updated_at": "2017-12-09T03:13:07Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23612416\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Erichliu00\">@Erichliu00</a> I'm not sure using copy works to solve the problem fully. If we use copy then we can't change the weights of the one model without it affecting the weights of the other model. For example:</p>\n<pre><code>W = tf.Variable(tf.constant(0.2))\nb= tf.Variable(tf.constant(0.5))\nx = tf.placeholder(tf.float32)\ny = W*x+b\n\ntrain = tf.assign(W, A2)\ny2 = copy(y)\n</code></pre>\n<p>the train here is just to mock what happens if we have the variable changed.</p>\n<pre><code>init = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    print(sess.run([y, y2], feed_dict={x:1}))\n    sess.run(train)\n    print(sess.run([y, y2], feed_dict={x:1}))\n    y2 = copy(y)\n    print(sess.run([y, y2], feed_dict={x:1}))\n</code></pre>\n<p>Prints out</p>\n<pre><code>[0.69999999, 0.69999999]\n[1.0, 1.0]  -&gt; This should be [1, 0.69999999] if a true copy was made\n[1.0, 1.0]\n</code></pre>\n<p>So I don't know how to make a clone of a model I would love to know though... I currently have very awkward work arounds for it. If you do find out a way please let me know</p>", "body_text": "@Erichliu00 I'm not sure using copy works to solve the problem fully. If we use copy then we can't change the weights of the one model without it affecting the weights of the other model. For example:\nW = tf.Variable(tf.constant(0.2))\nb= tf.Variable(tf.constant(0.5))\nx = tf.placeholder(tf.float32)\ny = W*x+b\n\ntrain = tf.assign(W, A2)\ny2 = copy(y)\n\nthe train here is just to mock what happens if we have the variable changed.\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    print(sess.run([y, y2], feed_dict={x:1}))\n    sess.run(train)\n    print(sess.run([y, y2], feed_dict={x:1}))\n    y2 = copy(y)\n    print(sess.run([y, y2], feed_dict={x:1}))\n\nPrints out\n[0.69999999, 0.69999999]\n[1.0, 1.0]  -> This should be [1, 0.69999999] if a true copy was made\n[1.0, 1.0]\n\nSo I don't know how to make a clone of a model I would love to know though... I currently have very awkward work arounds for it. If you do find out a way please let me know", "body": "@Erichliu00 I'm not sure using copy works to solve the problem fully. If we use copy then we can't change the weights of the one model without it affecting the weights of the other model. For example:\r\n\r\n```\r\nW = tf.Variable(tf.constant(0.2))\r\nb= tf.Variable(tf.constant(0.5))\r\nx = tf.placeholder(tf.float32)\r\ny = W*x+b\r\n\r\ntrain = tf.assign(W, A2)\r\ny2 = copy(y)\r\n```\r\n\r\nthe train here is just to mock what happens if we have the variable changed.\r\n\r\n```\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    print(sess.run([y, y2], feed_dict={x:1}))\r\n    sess.run(train)\r\n    print(sess.run([y, y2], feed_dict={x:1}))\r\n    y2 = copy(y)\r\n    print(sess.run([y, y2], feed_dict={x:1}))\r\n```\r\n\r\nPrints out\r\n```\r\n[0.69999999, 0.69999999]\r\n[1.0, 1.0]  -> This should be [1, 0.69999999] if a true copy was made\r\n[1.0, 1.0]\r\n```\r\n\r\nSo I don't know how to make a clone of a model I would love to know though... I currently have very awkward work arounds for it. If you do find out a way please let me know"}