{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19140", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19140/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19140/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19140/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19140", "id": 321118945, "node_id": "MDU6SXNzdWUzMjExMTg5NDU=", "number": 19140, "title": "[XLA] input data type support for DT_STRING?", "user": {"login": "ydp", "id": 1532805, "node_id": "MDQ6VXNlcjE1MzI4MDU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1532805?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ydp", "html_url": "https://github.com/ydp", "followers_url": "https://api.github.com/users/ydp/followers", "following_url": "https://api.github.com/users/ydp/following{/other_user}", "gists_url": "https://api.github.com/users/ydp/gists{/gist_id}", "starred_url": "https://api.github.com/users/ydp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ydp/subscriptions", "organizations_url": "https://api.github.com/users/ydp/orgs", "repos_url": "https://api.github.com/users/ydp/repos", "events_url": "https://api.github.com/users/ydp/events{/privacy}", "received_events_url": "https://api.github.com/users/ydp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-05-08T09:55:44Z", "updated_at": "2018-11-20T07:53:01Z", "closed_at": null, "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:  Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  CentOS 7.4.1708</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:  source</li>\n<li><strong>TensorFlow version (use command below)</strong>:  1.7.0</li>\n<li><strong>Python version</strong>:   2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:  0.11.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:  4.8.5</li>\n<li><strong>CUDA/cuDNN version</strong>:  cuda9 &amp; cuddn6</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>What I want to do here is that using XLA to speedup my model inference performance.  so I have a model(think it is just a wide and deep model) trained with Estimator, exported as savedModel format with feature_column, then transform to frozen graph.  Then I followed the <a href=\"https://www.tensorflow.org/performance/xla/tfcompile\" rel=\"nofollow\">AOT tutorial</a>:</p>\n<ul>\n<li>prepare my frozen graph.pb</li>\n<li>write a graph_config.pbtxt</li>\n<li>edit BUILD file to add cc_library of my own</li>\n<li>build</li>\n</ul>\n<p>in the meantime, in order to make it work, I have to add 3 more dependency in tf_compile library section of BUILD file(tensorflow/compiler/aot/BUILD). like below:</p>\n<pre><code>        \"//tensorflow/core/kernels:example_parsing_ops\",\n        \"//tensorflow/core/kernels:lookup_table_op\",\n        \"//tensorflow/core/kernels:logging_ops\",\n</code></pre>\n<p>then, after all dependency error resolved. Following error messages showed up:</p>\n<pre><code>INVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType string\n</code></pre>\n<p>So I checked the code here <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/type_util.cc\">tensorflow/compiler/tf2xla/type_util.cc</a> and here <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/xla_data.proto\">tensorflow/compiler/xla/xla_data.proto</a>; found that XLA actually does not support DT_STRING now. So I would like to know is it possible to support string? why?</p>\n<h3>Source code / logs</h3>\n<p>my graph.config.pbtxt is like below:</p>\n<pre><code>feed {\n  id { node_name: \"input_example_tensor\" }\n  shape {\n    dim { size: 1 }\n  }\n}\n\nfetch {\n  id { node_name: \"head/predictions/probabilities\" }\n}\n</code></pre>\n<p>bazel build error message:</p>\n<pre><code>ERROR: /data/tf/tensorflow-1.7.0/tensorflow/compiler/aot/tests/BUILD:155:1: Executing genrule //tensorflow/compiler/aot/tests:gen_feed_graph failed (Exit 1)\n2018-05-08 17:49:19.657209: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\nINVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType string\n\ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\nresulting in an object file compiled for your target architecture, and a\nheader file that gives access to the functionality in the object file.\nA typical invocation looks like this:\n\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt --cpp_class=\"mynamespace::MyComputation\"\n\nusage: bazel-out/host/bin/tensorflow/compiler/aot/tfcompile\nFlags:\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is &lt;arch&gt;&lt;sub&gt;-&lt;vendor&gt;-&lt;sys&gt;-&lt;abi&gt;.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\n\t--entry_point=\"entry\"            \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[&lt;optional_namespace&gt;::],...]&lt;class_name&gt;.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\n\t--out_function_object=\"out_model.o\"\tstring\tOutput object file containing the generated function for the TensorFlow model.\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\n\t--out_metadata_object=\"out_helper.o\"\tstring\tOutput object file name containing optional metadata for the generated function.\n\t--out_session_module=\"\"          \tstring\tOutput session module proto.\n\t--gen_name_to_index=false        \tbool\tGenerate name-to-index data for Lookup{Arg,Result}Index methods.\n\t--gen_program_shape=false        \tbool\tGenerate program shape data for the ProgramShape method.\n\t--xla_generate_hlo_graph=\"\"      \tstring\tHLO modules matching this regex will be dumped to a .dot file throughout various stages in compilation.\n\t--xla_hlo_graph_addresses=false  \tbool\tWith xla_generate_hlo_graph, show addresses of HLO ops in graph dump.\n\t--xla_hlo_graph_path=\"\"          \tstring\tWith xla_generate_hlo_graph, dump the graphs into this path.\n\t--xla_hlo_dump_as_graphdef=false \tbool\tDump HLO graphs as TensorFlow GraphDefs.\n\t--xla_hlo_graph_sharding_color=false\tbool\tAssign colors based on sharding assignments when generating the HLO graphs.\n\t--xla_hlo_tfgraph_device_scopes=false\tbool\tWhen generating TensorFlow HLO graphs, if the HLO instructions are assigned to a specific device, prefix the name scope with \"devX\" with X being the device ordinal.\n\t--xla_log_hlo_text=\"\"            \tstring\tHLO modules matching this regex will be dumped to LOG(INFO).\n\t--xla_generate_hlo_text_to=\"\"    \tstring\tDump all HLO modules as text into the provided directory path.\n\t--xla_enable_fast_math=true      \tbool\tEnable unsafe fast-math optimizations in the compiler; this may produce faster code at the expense of some accuracy.\n\t--xla_llvm_enable_alias_scope_metadata=true\tbool\tIn LLVM-based backends, enable the emission of !alias.scope metadata in the generated IR.\n\t--xla_llvm_enable_noalias_metadata=true\tbool\tIn LLVM-based backends, enable the emission of !noalias metadata in the generated IR.\n\t--xla_llvm_enable_invariant_load_metadata=true\tbool\tIn LLVM-based backends, enable the emission of !invariant.load metadata in the generated IR.\n\t--xla_llvm_disable_expensive_passes=false\tbool\tIn LLVM-based backends, disable a custom set of expensive optimization passes.\n\t--xla_backend_optimization_level=3\tint32\tNumerical optimization level for the XLA compiler backend.\n\t--xla_disable_hlo_passes=\"\"      \tstring\tComma-separated list of hlo passes to be disabled. These names must exactly match the passes' names; no whitespace around commas.\n\t--xla_embed_ir_in_executable=false\tbool\tEmbed the compiler IR as a string in the executable.\n\t--xla_dump_ir_to=\"\"              \tstring\tDump the compiler IR into this directory as individual files.\n\t--xla_eliminate_hlo_implicit_broadcast=true\tbool\tEliminate implicit broadcasts when lowering user computations to HLO instructions; use explicit broadcast instead.\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen in the CPU backend, use multi-threaded Eigen mode.\n\t--xla_gpu_cuda_data_dir=\"./cuda_sdk_lib\"\tstring\tIf non-empty, speficies a local directory containing ptxas and nvvm libdevice files; otherwise we use those from runfile directories.\n\t--xla_gpu_ftz=false              \tbool\tIf true, flush-to-zero semantics are enabled in the code generated for GPUs.\n\t--xla_gpu_disable_multi_streaming=false\tbool\tIf true, multi-streaming in the GPU backend is disabled.\n\t--xla_dump_optimized_hlo_proto_to=\"\"\tstring\tDump Hlo after all hlo passes are executed as proto binary into this directory.\n\t--xla_dump_unoptimized_hlo_proto_to=\"\"\tstring\tDump HLO before any hlo passes are executed as proto binary into this directory.\n\t--xla_dump_per_pass_hlo_proto_to=\"\"\tstring\tDump HLO after each pass as an HloProto in binary file format into this directory.\n\t--xla_test_all_output_layouts=false\tbool\tLet ClientLibraryTestBase::ComputeAndCompare* test all permutations of output layouts. For example, with a 3D shape, all permutations of the set {0, 1, 2} are tried.\n\t--xla_test_all_input_layouts=false\tbool\tLet ClientLibraryTestBase::ComputeAndCompare* test all permutations of *input* layouts. For example, for 2 input arguments with 2D shape and 4D shape, the computation will run 2! * 4! times for every possible layouts\n\t--xla_hlo_profile=false          \tbool\tInstrument the computation to collect per-HLO cycle counts\n\t--xla_dump_computations_to=\"\"    \tstring\tDump computations that XLA executes into the provided directory path\n\t--xla_dump_executions_to=\"\"      \tstring\tDump parameters and results of computations that XLA executes into the provided directory path\n\t--xla_backend_extra_options=\"\"   \tstring\tExtra options to pass to a backend; comma-separated list of 'key=val' strings (=val may be omitted); no whitespace around commas.\n\t--xla_reduce_precision=\"\"        \tstring\tDirections for adding reduce-precision operations. Format is 'LOCATION=E,M:OPS;NAMES' where LOCATION is the class of locations in which to insert the operations (e.g., 'OP_OUTPUTS'), E and M are the exponent and matissa bit counts respectively, and OPS and NAMES are comma-separated (no spaces) lists of the operation types and names to which to attach the reduce-precision operations.  The NAMES string and its preceding ';' may be omitted.  This option may be repeated to define multiple sets of added reduce-precision operations.\n\t--xla_gpu_use_cudnn_batchnorm=false\tbool\tAllows the GPU backend to implement batchnorm HLOs using cudnn, rather than expanding them to a soup of HLOs.\nTarget //tensorflow/compiler/aot/tests:feed_binary failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 12.475s, Critical Path: 3.04s\nFAILED: Build did NOT complete successfully\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS 7.4.1708\nTensorFlow installed from (source or binary):  source\nTensorFlow version (use command below):  1.7.0\nPython version:   2.7\nBazel version (if compiling from source):  0.11.1\nGCC/Compiler version (if compiling from source):  4.8.5\nCUDA/cuDNN version:  cuda9 & cuddn6\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nWhat I want to do here is that using XLA to speedup my model inference performance.  so I have a model(think it is just a wide and deep model) trained with Estimator, exported as savedModel format with feature_column, then transform to frozen graph.  Then I followed the AOT tutorial:\n\nprepare my frozen graph.pb\nwrite a graph_config.pbtxt\nedit BUILD file to add cc_library of my own\nbuild\n\nin the meantime, in order to make it work, I have to add 3 more dependency in tf_compile library section of BUILD file(tensorflow/compiler/aot/BUILD). like below:\n        \"//tensorflow/core/kernels:example_parsing_ops\",\n        \"//tensorflow/core/kernels:lookup_table_op\",\n        \"//tensorflow/core/kernels:logging_ops\",\n\nthen, after all dependency error resolved. Following error messages showed up:\nINVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType string\n\nSo I checked the code here tensorflow/compiler/tf2xla/type_util.cc and here tensorflow/compiler/xla/xla_data.proto; found that XLA actually does not support DT_STRING now. So I would like to know is it possible to support string? why?\nSource code / logs\nmy graph.config.pbtxt is like below:\nfeed {\n  id { node_name: \"input_example_tensor\" }\n  shape {\n    dim { size: 1 }\n  }\n}\n\nfetch {\n  id { node_name: \"head/predictions/probabilities\" }\n}\n\nbazel build error message:\nERROR: /data/tf/tensorflow-1.7.0/tensorflow/compiler/aot/tests/BUILD:155:1: Executing genrule //tensorflow/compiler/aot/tests:gen_feed_graph failed (Exit 1)\n2018-05-08 17:49:19.657209: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\nINVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType string\n\ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\nresulting in an object file compiled for your target architecture, and a\nheader file that gives access to the functionality in the object file.\nA typical invocation looks like this:\n\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt --cpp_class=\"mynamespace::MyComputation\"\n\nusage: bazel-out/host/bin/tensorflow/compiler/aot/tfcompile\nFlags:\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is <arch><sub>-<vendor>-<sys>-<abi>.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\n\t--entry_point=\"entry\"            \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[<optional_namespace>::],...]<class_name>.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\n\t--out_function_object=\"out_model.o\"\tstring\tOutput object file containing the generated function for the TensorFlow model.\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\n\t--out_metadata_object=\"out_helper.o\"\tstring\tOutput object file name containing optional metadata for the generated function.\n\t--out_session_module=\"\"          \tstring\tOutput session module proto.\n\t--gen_name_to_index=false        \tbool\tGenerate name-to-index data for Lookup{Arg,Result}Index methods.\n\t--gen_program_shape=false        \tbool\tGenerate program shape data for the ProgramShape method.\n\t--xla_generate_hlo_graph=\"\"      \tstring\tHLO modules matching this regex will be dumped to a .dot file throughout various stages in compilation.\n\t--xla_hlo_graph_addresses=false  \tbool\tWith xla_generate_hlo_graph, show addresses of HLO ops in graph dump.\n\t--xla_hlo_graph_path=\"\"          \tstring\tWith xla_generate_hlo_graph, dump the graphs into this path.\n\t--xla_hlo_dump_as_graphdef=false \tbool\tDump HLO graphs as TensorFlow GraphDefs.\n\t--xla_hlo_graph_sharding_color=false\tbool\tAssign colors based on sharding assignments when generating the HLO graphs.\n\t--xla_hlo_tfgraph_device_scopes=false\tbool\tWhen generating TensorFlow HLO graphs, if the HLO instructions are assigned to a specific device, prefix the name scope with \"devX\" with X being the device ordinal.\n\t--xla_log_hlo_text=\"\"            \tstring\tHLO modules matching this regex will be dumped to LOG(INFO).\n\t--xla_generate_hlo_text_to=\"\"    \tstring\tDump all HLO modules as text into the provided directory path.\n\t--xla_enable_fast_math=true      \tbool\tEnable unsafe fast-math optimizations in the compiler; this may produce faster code at the expense of some accuracy.\n\t--xla_llvm_enable_alias_scope_metadata=true\tbool\tIn LLVM-based backends, enable the emission of !alias.scope metadata in the generated IR.\n\t--xla_llvm_enable_noalias_metadata=true\tbool\tIn LLVM-based backends, enable the emission of !noalias metadata in the generated IR.\n\t--xla_llvm_enable_invariant_load_metadata=true\tbool\tIn LLVM-based backends, enable the emission of !invariant.load metadata in the generated IR.\n\t--xla_llvm_disable_expensive_passes=false\tbool\tIn LLVM-based backends, disable a custom set of expensive optimization passes.\n\t--xla_backend_optimization_level=3\tint32\tNumerical optimization level for the XLA compiler backend.\n\t--xla_disable_hlo_passes=\"\"      \tstring\tComma-separated list of hlo passes to be disabled. These names must exactly match the passes' names; no whitespace around commas.\n\t--xla_embed_ir_in_executable=false\tbool\tEmbed the compiler IR as a string in the executable.\n\t--xla_dump_ir_to=\"\"              \tstring\tDump the compiler IR into this directory as individual files.\n\t--xla_eliminate_hlo_implicit_broadcast=true\tbool\tEliminate implicit broadcasts when lowering user computations to HLO instructions; use explicit broadcast instead.\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen in the CPU backend, use multi-threaded Eigen mode.\n\t--xla_gpu_cuda_data_dir=\"./cuda_sdk_lib\"\tstring\tIf non-empty, speficies a local directory containing ptxas and nvvm libdevice files; otherwise we use those from runfile directories.\n\t--xla_gpu_ftz=false              \tbool\tIf true, flush-to-zero semantics are enabled in the code generated for GPUs.\n\t--xla_gpu_disable_multi_streaming=false\tbool\tIf true, multi-streaming in the GPU backend is disabled.\n\t--xla_dump_optimized_hlo_proto_to=\"\"\tstring\tDump Hlo after all hlo passes are executed as proto binary into this directory.\n\t--xla_dump_unoptimized_hlo_proto_to=\"\"\tstring\tDump HLO before any hlo passes are executed as proto binary into this directory.\n\t--xla_dump_per_pass_hlo_proto_to=\"\"\tstring\tDump HLO after each pass as an HloProto in binary file format into this directory.\n\t--xla_test_all_output_layouts=false\tbool\tLet ClientLibraryTestBase::ComputeAndCompare* test all permutations of output layouts. For example, with a 3D shape, all permutations of the set {0, 1, 2} are tried.\n\t--xla_test_all_input_layouts=false\tbool\tLet ClientLibraryTestBase::ComputeAndCompare* test all permutations of *input* layouts. For example, for 2 input arguments with 2D shape and 4D shape, the computation will run 2! * 4! times for every possible layouts\n\t--xla_hlo_profile=false          \tbool\tInstrument the computation to collect per-HLO cycle counts\n\t--xla_dump_computations_to=\"\"    \tstring\tDump computations that XLA executes into the provided directory path\n\t--xla_dump_executions_to=\"\"      \tstring\tDump parameters and results of computations that XLA executes into the provided directory path\n\t--xla_backend_extra_options=\"\"   \tstring\tExtra options to pass to a backend; comma-separated list of 'key=val' strings (=val may be omitted); no whitespace around commas.\n\t--xla_reduce_precision=\"\"        \tstring\tDirections for adding reduce-precision operations. Format is 'LOCATION=E,M:OPS;NAMES' where LOCATION is the class of locations in which to insert the operations (e.g., 'OP_OUTPUTS'), E and M are the exponent and matissa bit counts respectively, and OPS and NAMES are comma-separated (no spaces) lists of the operation types and names to which to attach the reduce-precision operations.  The NAMES string and its preceding ';' may be omitted.  This option may be repeated to define multiple sets of added reduce-precision operations.\n\t--xla_gpu_use_cudnn_batchnorm=false\tbool\tAllows the GPU backend to implement batchnorm HLOs using cudnn, rather than expanding them to a soup of HLOs.\nTarget //tensorflow/compiler/aot/tests:feed_binary failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 12.475s, Critical Path: 3.04s\nFAILED: Build did NOT complete successfully", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  CentOS 7.4.1708\r\n- **TensorFlow installed from (source or binary)**:  source\r\n- **TensorFlow version (use command below)**:  1.7.0\r\n- **Python version**:   2.7\r\n- **Bazel version (if compiling from source)**:  0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:  4.8.5\r\n- **CUDA/cuDNN version**:  cuda9 & cuddn6\r\n- **GPU model and memory**:  \r\n- **Exact command to reproduce**:  \r\n\r\n\r\n### Describe the problem\r\nWhat I want to do here is that using XLA to speedup my model inference performance.  so I have a model(think it is just a wide and deep model) trained with Estimator, exported as savedModel format with feature_column, then transform to frozen graph.  Then I followed the [AOT tutorial](https://www.tensorflow.org/performance/xla/tfcompile):\r\n\r\n* prepare my frozen graph.pb\r\n* write a graph_config.pbtxt\r\n* edit BUILD file to add cc_library of my own\r\n* build\r\n\r\nin the meantime, in order to make it work, I have to add 3 more dependency in tf_compile library section of BUILD file(tensorflow/compiler/aot/BUILD). like below:\r\n\r\n```\r\n        \"//tensorflow/core/kernels:example_parsing_ops\",\r\n        \"//tensorflow/core/kernels:lookup_table_op\",\r\n        \"//tensorflow/core/kernels:logging_ops\",\r\n```\r\n\r\nthen, after all dependency error resolved. Following error messages showed up:\r\n\r\n```\r\nINVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType string\r\n```\r\n\r\nSo I checked the code here [tensorflow/compiler/tf2xla/type_util.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/type_util.cc) and here [tensorflow/compiler/xla/xla_data.proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/xla_data.proto); found that XLA actually does not support DT_STRING now. So I would like to know is it possible to support string? why?\r\n\r\n\r\n### Source code / logs\r\n\r\nmy graph.config.pbtxt is like below:\r\n\r\n```\r\nfeed {\r\n  id { node_name: \"input_example_tensor\" }\r\n  shape {\r\n    dim { size: 1 }\r\n  }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"head/predictions/probabilities\" }\r\n}\r\n```\r\n\r\nbazel build error message:\r\n\r\n```\r\nERROR: /data/tf/tensorflow-1.7.0/tensorflow/compiler/aot/tests/BUILD:155:1: Executing genrule //tensorflow/compiler/aot/tests:gen_feed_graph failed (Exit 1)\r\n2018-05-08 17:49:19.657209: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nINVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType string\r\n\r\ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\r\nresulting in an object file compiled for your target architecture, and a\r\nheader file that gives access to the functionality in the object file.\r\nA typical invocation looks like this:\r\n\r\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt --cpp_class=\"mynamespace::MyComputation\"\r\n\r\nusage: bazel-out/host/bin/tensorflow/compiler/aot/tfcompile\r\nFlags:\r\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\r\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is <arch><sub>-<vendor>-<sys>-<abi>.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\r\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\r\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\r\n\t--entry_point=\"entry\"            \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\r\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[<optional_namespace>::],...]<class_name>.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\r\n\t--out_function_object=\"out_model.o\"\tstring\tOutput object file containing the generated function for the TensorFlow model.\r\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\r\n\t--out_metadata_object=\"out_helper.o\"\tstring\tOutput object file name containing optional metadata for the generated function.\r\n\t--out_session_module=\"\"          \tstring\tOutput session module proto.\r\n\t--gen_name_to_index=false        \tbool\tGenerate name-to-index data for Lookup{Arg,Result}Index methods.\r\n\t--gen_program_shape=false        \tbool\tGenerate program shape data for the ProgramShape method.\r\n\t--xla_generate_hlo_graph=\"\"      \tstring\tHLO modules matching this regex will be dumped to a .dot file throughout various stages in compilation.\r\n\t--xla_hlo_graph_addresses=false  \tbool\tWith xla_generate_hlo_graph, show addresses of HLO ops in graph dump.\r\n\t--xla_hlo_graph_path=\"\"          \tstring\tWith xla_generate_hlo_graph, dump the graphs into this path.\r\n\t--xla_hlo_dump_as_graphdef=false \tbool\tDump HLO graphs as TensorFlow GraphDefs.\r\n\t--xla_hlo_graph_sharding_color=false\tbool\tAssign colors based on sharding assignments when generating the HLO graphs.\r\n\t--xla_hlo_tfgraph_device_scopes=false\tbool\tWhen generating TensorFlow HLO graphs, if the HLO instructions are assigned to a specific device, prefix the name scope with \"devX\" with X being the device ordinal.\r\n\t--xla_log_hlo_text=\"\"            \tstring\tHLO modules matching this regex will be dumped to LOG(INFO).\r\n\t--xla_generate_hlo_text_to=\"\"    \tstring\tDump all HLO modules as text into the provided directory path.\r\n\t--xla_enable_fast_math=true      \tbool\tEnable unsafe fast-math optimizations in the compiler; this may produce faster code at the expense of some accuracy.\r\n\t--xla_llvm_enable_alias_scope_metadata=true\tbool\tIn LLVM-based backends, enable the emission of !alias.scope metadata in the generated IR.\r\n\t--xla_llvm_enable_noalias_metadata=true\tbool\tIn LLVM-based backends, enable the emission of !noalias metadata in the generated IR.\r\n\t--xla_llvm_enable_invariant_load_metadata=true\tbool\tIn LLVM-based backends, enable the emission of !invariant.load metadata in the generated IR.\r\n\t--xla_llvm_disable_expensive_passes=false\tbool\tIn LLVM-based backends, disable a custom set of expensive optimization passes.\r\n\t--xla_backend_optimization_level=3\tint32\tNumerical optimization level for the XLA compiler backend.\r\n\t--xla_disable_hlo_passes=\"\"      \tstring\tComma-separated list of hlo passes to be disabled. These names must exactly match the passes' names; no whitespace around commas.\r\n\t--xla_embed_ir_in_executable=false\tbool\tEmbed the compiler IR as a string in the executable.\r\n\t--xla_dump_ir_to=\"\"              \tstring\tDump the compiler IR into this directory as individual files.\r\n\t--xla_eliminate_hlo_implicit_broadcast=true\tbool\tEliminate implicit broadcasts when lowering user computations to HLO instructions; use explicit broadcast instead.\r\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen in the CPU backend, use multi-threaded Eigen mode.\r\n\t--xla_gpu_cuda_data_dir=\"./cuda_sdk_lib\"\tstring\tIf non-empty, speficies a local directory containing ptxas and nvvm libdevice files; otherwise we use those from runfile directories.\r\n\t--xla_gpu_ftz=false              \tbool\tIf true, flush-to-zero semantics are enabled in the code generated for GPUs.\r\n\t--xla_gpu_disable_multi_streaming=false\tbool\tIf true, multi-streaming in the GPU backend is disabled.\r\n\t--xla_dump_optimized_hlo_proto_to=\"\"\tstring\tDump Hlo after all hlo passes are executed as proto binary into this directory.\r\n\t--xla_dump_unoptimized_hlo_proto_to=\"\"\tstring\tDump HLO before any hlo passes are executed as proto binary into this directory.\r\n\t--xla_dump_per_pass_hlo_proto_to=\"\"\tstring\tDump HLO after each pass as an HloProto in binary file format into this directory.\r\n\t--xla_test_all_output_layouts=false\tbool\tLet ClientLibraryTestBase::ComputeAndCompare* test all permutations of output layouts. For example, with a 3D shape, all permutations of the set {0, 1, 2} are tried.\r\n\t--xla_test_all_input_layouts=false\tbool\tLet ClientLibraryTestBase::ComputeAndCompare* test all permutations of *input* layouts. For example, for 2 input arguments with 2D shape and 4D shape, the computation will run 2! * 4! times for every possible layouts\r\n\t--xla_hlo_profile=false          \tbool\tInstrument the computation to collect per-HLO cycle counts\r\n\t--xla_dump_computations_to=\"\"    \tstring\tDump computations that XLA executes into the provided directory path\r\n\t--xla_dump_executions_to=\"\"      \tstring\tDump parameters and results of computations that XLA executes into the provided directory path\r\n\t--xla_backend_extra_options=\"\"   \tstring\tExtra options to pass to a backend; comma-separated list of 'key=val' strings (=val may be omitted); no whitespace around commas.\r\n\t--xla_reduce_precision=\"\"        \tstring\tDirections for adding reduce-precision operations. Format is 'LOCATION=E,M:OPS;NAMES' where LOCATION is the class of locations in which to insert the operations (e.g., 'OP_OUTPUTS'), E and M are the exponent and matissa bit counts respectively, and OPS and NAMES are comma-separated (no spaces) lists of the operation types and names to which to attach the reduce-precision operations.  The NAMES string and its preceding ';' may be omitted.  This option may be repeated to define multiple sets of added reduce-precision operations.\r\n\t--xla_gpu_use_cudnn_batchnorm=false\tbool\tAllows the GPU backend to implement batchnorm HLOs using cudnn, rather than expanding them to a soup of HLOs.\r\nTarget //tensorflow/compiler/aot/tests:feed_binary failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 12.475s, Critical Path: 3.04s\r\nFAILED: Build did NOT complete successfully\r\n```"}