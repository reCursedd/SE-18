{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3769", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3769/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3769/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3769/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3769", "id": 170849781, "node_id": "MDU6SXNzdWUxNzA4NDk3ODE=", "number": 3769, "title": "Assigning Multiple Embeddings per Word in Tensorflow", "user": {"login": "FudanHeroSZN", "id": 20989236, "node_id": "MDQ6VXNlcjIwOTg5MjM2", "avatar_url": "https://avatars1.githubusercontent.com/u/20989236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FudanHeroSZN", "html_url": "https://github.com/FudanHeroSZN", "followers_url": "https://api.github.com/users/FudanHeroSZN/followers", "following_url": "https://api.github.com/users/FudanHeroSZN/following{/other_user}", "gists_url": "https://api.github.com/users/FudanHeroSZN/gists{/gist_id}", "starred_url": "https://api.github.com/users/FudanHeroSZN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FudanHeroSZN/subscriptions", "organizations_url": "https://api.github.com/users/FudanHeroSZN/orgs", "repos_url": "https://api.github.com/users/FudanHeroSZN/repos", "events_url": "https://api.github.com/users/FudanHeroSZN/events{/privacy}", "received_events_url": "https://api.github.com/users/FudanHeroSZN/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-08-12T11:13:13Z", "updated_at": "2016-08-12T14:56:47Z", "closed_at": "2016-08-12T14:56:47Z", "author_association": "NONE", "body_html": "<p>I am trying to reproduce the results described in the EMNLP 2014 paper \"Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space\". The authors proposed a way of handling multiple senses per word through the use of multiple embeddings associated with it. Each word is assigned a global context embedding and several sense-specific embeddings, and which sense the word belongs to in the current context is determined by the argmax index of the cosine similarity between the target word's sense-specific embeddings and the average of the global vectors in the context window.</p>\n<p>While the authors have made their Scala code publicly available on their website, I have found no similar implementations with a high level deep learning library like Tensorflow. Since I am new to Tensorflow, I am not sure whether such model can be built with the Python API. My main concern is how one can use the argmax operation as an intermediate layer in Tensorflow, and then to decide which embedding to use in the following layers. In addition, as each word might have different number of senses, is there an efficient way to handle this using embedding_lookup?</p>\n<p>Does anyone have ideas?</p>\n<p>Regards,<br>\nZenong</p>", "body_text": "I am trying to reproduce the results described in the EMNLP 2014 paper \"Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space\". The authors proposed a way of handling multiple senses per word through the use of multiple embeddings associated with it. Each word is assigned a global context embedding and several sense-specific embeddings, and which sense the word belongs to in the current context is determined by the argmax index of the cosine similarity between the target word's sense-specific embeddings and the average of the global vectors in the context window.\nWhile the authors have made their Scala code publicly available on their website, I have found no similar implementations with a high level deep learning library like Tensorflow. Since I am new to Tensorflow, I am not sure whether such model can be built with the Python API. My main concern is how one can use the argmax operation as an intermediate layer in Tensorflow, and then to decide which embedding to use in the following layers. In addition, as each word might have different number of senses, is there an efficient way to handle this using embedding_lookup?\nDoes anyone have ideas?\nRegards,\nZenong", "body": "I am trying to reproduce the results described in the EMNLP 2014 paper \"Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space\". The authors proposed a way of handling multiple senses per word through the use of multiple embeddings associated with it. Each word is assigned a global context embedding and several sense-specific embeddings, and which sense the word belongs to in the current context is determined by the argmax index of the cosine similarity between the target word's sense-specific embeddings and the average of the global vectors in the context window.\n\nWhile the authors have made their Scala code publicly available on their website, I have found no similar implementations with a high level deep learning library like Tensorflow. Since I am new to Tensorflow, I am not sure whether such model can be built with the Python API. My main concern is how one can use the argmax operation as an intermediate layer in Tensorflow, and then to decide which embedding to use in the following layers. In addition, as each word might have different number of senses, is there an efficient way to handle this using embedding_lookup?\n\nDoes anyone have ideas?\n\nRegards,\nZenong\n"}