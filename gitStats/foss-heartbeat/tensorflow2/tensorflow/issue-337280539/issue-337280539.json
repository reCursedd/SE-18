{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20452", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20452/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20452/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20452/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20452", "id": 337280539, "node_id": "MDU6SXNzdWUzMzcyODA1Mzk=", "number": 20452, "title": "Keras TimeDistributed with Input and no batch_size fails", "user": {"login": "sleighsoft", "id": 9438971, "node_id": "MDQ6VXNlcjk0Mzg5NzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/9438971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sleighsoft", "html_url": "https://github.com/sleighsoft", "followers_url": "https://api.github.com/users/sleighsoft/followers", "following_url": "https://api.github.com/users/sleighsoft/following{/other_user}", "gists_url": "https://api.github.com/users/sleighsoft/gists{/gist_id}", "starred_url": "https://api.github.com/users/sleighsoft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sleighsoft/subscriptions", "organizations_url": "https://api.github.com/users/sleighsoft/orgs", "repos_url": "https://api.github.com/users/sleighsoft/repos", "events_url": "https://api.github.com/users/sleighsoft/events{/privacy}", "received_events_url": "https://api.github.com/users/sleighsoft/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-07-01T10:29:15Z", "updated_at": "2018-07-27T15:23:24Z", "closed_at": "2018-07-26T17:30:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>OS: Windows 10 &amp; Ubuntu 16.04 tested<br>\nTensorflow 1.8<br>\nPython 3.6.5</p>\n<p>The following code shows the problem.<br>\nWhen using <code>Input</code> with a <code>batch_size</code> everything works fine, but without it it fails.<br>\nI assumed that when setting the <code>batch_size</code> in <code>batch_shape</code> to <code>None</code> or using <code>shape</code> instead that the <code>batch_size</code> would then be dynamic.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> tensorflow.python.keras.layers <span class=\"pl-k\">import</span> Input, Lambda, TimeDistributed\n<span class=\"pl-k\">from</span> tensorflow.python.keras <span class=\"pl-k\">import</span> backend <span class=\"pl-k\">as</span> K\n\n\ni_batch <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">batch_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">18</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">512</span>))\ni_batch_none_error <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">batch_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">18</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">512</span>))\ni_error <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">18</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">512</span>))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">reduce_sum</span>(<span class=\"pl-smi\">x</span>):\n  <span class=\"pl-k\">return</span> K.sum(x, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>)\n\n\nsumpool <span class=\"pl-k\">=</span> Lambda(reduce_sum, <span class=\"pl-v\">output_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">512</span>,))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Computes correct shape (?, 18, 512)</span>\n<span class=\"pl-c1\">print</span>(TimeDistributed(sumpool, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>t<span class=\"pl-pds\">'</span></span>)(i_batch))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Both throw warnings</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Computes incorrect shape (?, 18, 64, 512)</span>\n<span class=\"pl-c1\">print</span>(TimeDistributed(sumpool, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>t<span class=\"pl-pds\">'</span></span>)(i_batch_none_error))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Computes incorrect shape (?, 18, 64, 512)</span>\n<span class=\"pl-c1\">print</span>(TimeDistributed(sumpool, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>t<span class=\"pl-pds\">'</span></span>)(i_error))</pre></div>", "body_text": "OS: Windows 10 & Ubuntu 16.04 tested\nTensorflow 1.8\nPython 3.6.5\nThe following code shows the problem.\nWhen using Input with a batch_size everything works fine, but without it it fails.\nI assumed that when setting the batch_size in batch_shape to None or using shape instead that the batch_size would then be dynamic.\nfrom tensorflow.python.keras.layers import Input, Lambda, TimeDistributed\nfrom tensorflow.python.keras import backend as K\n\n\ni_batch = Input(batch_shape=(32, 18, 64, 512))\ni_batch_none_error = Input(batch_shape=(None, 18, 64, 512))\ni_error = Input(shape=(18, 64, 512))\n\n\ndef reduce_sum(x):\n  return K.sum(x, axis=-2)\n\n\nsumpool = Lambda(reduce_sum, output_shape=(512,))\n# Computes correct shape (?, 18, 512)\nprint(TimeDistributed(sumpool, name='t')(i_batch))\n\n# Both throw warnings\n# Computes incorrect shape (?, 18, 64, 512)\nprint(TimeDistributed(sumpool, name='t')(i_batch_none_error))\n# Computes incorrect shape (?, 18, 64, 512)\nprint(TimeDistributed(sumpool, name='t')(i_error))", "body": "OS: Windows 10 & Ubuntu 16.04 tested\r\nTensorflow 1.8\r\nPython 3.6.5\r\n\r\nThe following code shows the problem.\r\nWhen using `Input` with a `batch_size` everything works fine, but without it it fails.\r\nI assumed that when setting the `batch_size` in `batch_shape` to `None` or using `shape` instead that the `batch_size` would then be dynamic.\r\n\r\n```python\r\nfrom tensorflow.python.keras.layers import Input, Lambda, TimeDistributed\r\nfrom tensorflow.python.keras import backend as K\r\n\r\n\r\ni_batch = Input(batch_shape=(32, 18, 64, 512))\r\ni_batch_none_error = Input(batch_shape=(None, 18, 64, 512))\r\ni_error = Input(shape=(18, 64, 512))\r\n\r\n\r\ndef reduce_sum(x):\r\n  return K.sum(x, axis=-2)\r\n\r\n\r\nsumpool = Lambda(reduce_sum, output_shape=(512,))\r\n# Computes correct shape (?, 18, 512)\r\nprint(TimeDistributed(sumpool, name='t')(i_batch))\r\n\r\n# Both throw warnings\r\n# Computes incorrect shape (?, 18, 64, 512)\r\nprint(TimeDistributed(sumpool, name='t')(i_batch_none_error))\r\n# Computes incorrect shape (?, 18, 64, 512)\r\nprint(TimeDistributed(sumpool, name='t')(i_error))\r\n```"}