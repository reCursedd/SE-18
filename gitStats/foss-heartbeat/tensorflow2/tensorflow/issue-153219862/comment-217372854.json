{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/217372854", "html_url": "https://github.com/tensorflow/tensorflow/issues/2233#issuecomment-217372854", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2233", "id": 217372854, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNzM3Mjg1NA==", "user": {"login": "smartcat2010", "id": 10429114, "node_id": "MDQ6VXNlcjEwNDI5MTE0", "avatar_url": "https://avatars1.githubusercontent.com/u/10429114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smartcat2010", "html_url": "https://github.com/smartcat2010", "followers_url": "https://api.github.com/users/smartcat2010/followers", "following_url": "https://api.github.com/users/smartcat2010/following{/other_user}", "gists_url": "https://api.github.com/users/smartcat2010/gists{/gist_id}", "starred_url": "https://api.github.com/users/smartcat2010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smartcat2010/subscriptions", "organizations_url": "https://api.github.com/users/smartcat2010/orgs", "repos_url": "https://api.github.com/users/smartcat2010/repos", "events_url": "https://api.github.com/users/smartcat2010/events{/privacy}", "received_events_url": "https://api.github.com/users/smartcat2010/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-06T07:48:04Z", "updated_at": "2016-05-06T07:49:43Z", "author_association": "NONE", "body_html": "<p>Thanks very much for your quick response!</p>\n<ol>\n<li>\n<p>It passes sv.prepare_or_wait_for_session() and hangs in session.run(...)</p>\n</li>\n<li>\n<p>Yes. I just follow the tutorial <a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html#recurrent-neural-networks\" rel=\"nofollow\">ptb_word_lm.py</a>  RNN model and made just a little modification to make it do classfication work. The \"embedding\" is 90001*200 floats. The embedding definition is just same as ptb_word_lm.py:</p>\n<p>with tf.device(\"/cpu:0\"):<br>\nembedding = tf.get_variable(\"embedding\", [vocab_size, size]) # [90001, 200] in my code.<br>\ninputs = tf.nn.embedding_lookup(embedding, self._input_data)</p>\n</li>\n<li>\n<p>I run your code with \"python ./server_lib_test.py\", after showing a lot of log messages, it shows the following message:</p>\n</li>\n</ol>\n<h2>.......</h2>\n<p>Ran 12 tests in 4.028s</p>\n<p>OK</p>\n<p>When I try to decease the \"embedding\" matrix to smaller size such as [101, 200], it never hangs and  run correctly.<br>\nI run the [90001, 200] again today, the warning message \"Input size was 67108839 and expected 72000800\" still shows but it don't hang again. I don't known why...</p>\n<p>BTW, is there any RNN sample code that could run on multi-cards(multi-tower style) or distributed multi-machines? I tried to port multi-tower from CIFAR-10 example code to my RNN code, after 1 week's work I failed(I'll report the bugs in another issue). Then I gave up and turn to distributed style. The \"Inception\" example code needs BIG data set ILSVRC2012 and it's time consuming to download the data(especially from China...). Since RNN is especially usefull in many application domains, would you mind show me the multi-cards or multi-machines existing code if you know?</p>\n<p>When I decrease the embedding size to [101, 200], it starts running. But still it can't fully use my 4 GPU cards in the machine. There is only 1 card with more than 0% usage. Would you mind giving me an email then I can send my code to you?</p>\n<p>Thanks a lot again for your time!</p>", "body_text": "Thanks very much for your quick response!\n\n\nIt passes sv.prepare_or_wait_for_session() and hangs in session.run(...)\n\n\nYes. I just follow the tutorial ptb_word_lm.py  RNN model and made just a little modification to make it do classfication work. The \"embedding\" is 90001*200 floats. The embedding definition is just same as ptb_word_lm.py:\nwith tf.device(\"/cpu:0\"):\nembedding = tf.get_variable(\"embedding\", [vocab_size, size]) # [90001, 200] in my code.\ninputs = tf.nn.embedding_lookup(embedding, self._input_data)\n\n\nI run your code with \"python ./server_lib_test.py\", after showing a lot of log messages, it shows the following message:\n\n\n.......\nRan 12 tests in 4.028s\nOK\nWhen I try to decease the \"embedding\" matrix to smaller size such as [101, 200], it never hangs and  run correctly.\nI run the [90001, 200] again today, the warning message \"Input size was 67108839 and expected 72000800\" still shows but it don't hang again. I don't known why...\nBTW, is there any RNN sample code that could run on multi-cards(multi-tower style) or distributed multi-machines? I tried to port multi-tower from CIFAR-10 example code to my RNN code, after 1 week's work I failed(I'll report the bugs in another issue). Then I gave up and turn to distributed style. The \"Inception\" example code needs BIG data set ILSVRC2012 and it's time consuming to download the data(especially from China...). Since RNN is especially usefull in many application domains, would you mind show me the multi-cards or multi-machines existing code if you know?\nWhen I decrease the embedding size to [101, 200], it starts running. But still it can't fully use my 4 GPU cards in the machine. There is only 1 card with more than 0% usage. Would you mind giving me an email then I can send my code to you?\nThanks a lot again for your time!", "body": "Thanks very much for your quick response!\n1. It passes sv.prepare_or_wait_for_session() and hangs in session.run(...)\n2. Yes. I just follow the tutorial [ptb_word_lm.py](https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html#recurrent-neural-networks)  RNN model and made just a little modification to make it do classfication work. The \"embedding\" is 90001*200 floats. The embedding definition is just same as ptb_word_lm.py:\n   \n   with tf.device(\"/cpu:0\"):\n     embedding = tf.get_variable(\"embedding\", [vocab_size, size]) # [90001, 200] in my code.\n     inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n3. I run your code with \"python ./server_lib_test.py\", after showing a lot of log messages, it shows the following message:\n\n## .......\n\nRan 12 tests in 4.028s\n\nOK\n\nWhen I try to decease the \"embedding\" matrix to smaller size such as [101, 200], it never hangs and  run correctly.\nI run the [90001, 200] again today, the warning message \"Input size was 67108839 and expected 72000800\" still shows but it don't hang again. I don't known why...\n\nBTW, is there any RNN sample code that could run on multi-cards(multi-tower style) or distributed multi-machines? I tried to port multi-tower from CIFAR-10 example code to my RNN code, after 1 week's work I failed(I'll report the bugs in another issue). Then I gave up and turn to distributed style. The \"Inception\" example code needs BIG data set ILSVRC2012 and it's time consuming to download the data(especially from China...). Since RNN is especially usefull in many application domains, would you mind show me the multi-cards or multi-machines existing code if you know?\n\nWhen I decrease the embedding size to [101, 200], it starts running. But still it can't fully use my 4 GPU cards in the machine. There is only 1 card with more than 0% usage. Would you mind giving me an email then I can send my code to you?\n\nThanks a lot again for your time!\n"}