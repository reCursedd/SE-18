{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/221171646", "html_url": "https://github.com/tensorflow/tensorflow/issues/2233#issuecomment-221171646", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2233", "id": 221171646, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMTE3MTY0Ng==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-24T05:41:07Z", "updated_at": "2016-05-24T05:41:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi folks, sorry for the delay on this one. We've tracked down the issue: the generated code for gRPC uses a <a href=\"https://github.com/grpc/grpc/blob/305b0f4e2f99d2326bf1aaa881f857bb5fe1a817/include/grpc%2B%2B/impl/codegen/proto_utils.h#L209\">protobuf parsing routine</a> that doesn't override the default 64MB limit.</p>\n<p>Generally speaking, if your trainer is transferring large tensors in every step, there might be a more efficient way to implement it at the Python level. For example, instead of fetching a large fully connected layer to compute the logits, you might use a <a href=\"https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#sampled-loss-functions\" rel=\"nofollow\">sampled loss function</a>, and you can use <a href=\"https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#embedding_lookup\" rel=\"nofollow\"><code>tf.nn.embedding_lookup()</code></a> instead of <a href=\"https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#gather\" rel=\"nofollow\"><code>tf.gather()</code></a> to more efficiently access sparse rows in an embedding matrix. Alternatively, you can shard your variables manually to avoid the limit.</p>\n<p>Clearly, this isn't ideal, and we're working on a more robust fix.</p>", "body_text": "Hi folks, sorry for the delay on this one. We've tracked down the issue: the generated code for gRPC uses a protobuf parsing routine that doesn't override the default 64MB limit.\nGenerally speaking, if your trainer is transferring large tensors in every step, there might be a more efficient way to implement it at the Python level. For example, instead of fetching a large fully connected layer to compute the logits, you might use a sampled loss function, and you can use tf.nn.embedding_lookup() instead of tf.gather() to more efficiently access sparse rows in an embedding matrix. Alternatively, you can shard your variables manually to avoid the limit.\nClearly, this isn't ideal, and we're working on a more robust fix.", "body": "Hi folks, sorry for the delay on this one. We've tracked down the issue: the generated code for gRPC uses a [protobuf parsing routine](https://github.com/grpc/grpc/blob/305b0f4e2f99d2326bf1aaa881f857bb5fe1a817/include/grpc%2B%2B/impl/codegen/proto_utils.h#L209) that doesn't override the default 64MB limit.\n\nGenerally speaking, if your trainer is transferring large tensors in every step, there might be a more efficient way to implement it at the Python level. For example, instead of fetching a large fully connected layer to compute the logits, you might use a [sampled loss function](https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#sampled-loss-functions), and you can use [`tf.nn.embedding_lookup()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#embedding_lookup) instead of [`tf.gather()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#gather) to more efficiently access sparse rows in an embedding matrix. Alternatively, you can shard your variables manually to avoid the limit.\n\nClearly, this isn't ideal, and we're working on a more robust fix.\n"}