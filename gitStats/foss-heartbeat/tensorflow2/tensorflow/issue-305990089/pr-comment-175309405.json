{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175309405", "pull_request_review_id": 104816643, "id": 175309405, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTMwOTQwNQ==", "diff_hunk": "@@ -1875,71 +1833,105 @@ tensorflow::Status ConvertFusedBatchNorm(\n         \"only is_training=false is supported, at \" + node_def.name());\n   }\n   nvinfer1::ITensor const* tensor = inputs.at(0).tensor();\n-  TRT_ShapedWeights scale_weights = inputs.at(1).weights();\n-  TRT_ShapedWeights offset_weights = inputs.at(2).weights();\n-  TRT_ShapedWeights mean_weights = inputs.at(3).weights();\n-  TRT_ShapedWeights variance_weights = inputs.at(4).weights();\n-  TRT_ShapedWeights dummy_power_weights(scale_weights.type_);\n-  TRT_ShapedWeights combined_scale_weights =\n-      ctx.get_temp_weights_like(scale_weights);\n-  TRT_ShapedWeights combined_offset_weights =\n-      ctx.get_temp_weights_like(offset_weights);\n-  size_t nweight = scale_weights.count();\n-  if ((scale_weights.type_ == offset_weights.type_) &&\n-      (mean_weights.type_ == variance_weights.type_) &&\n-      (scale_weights.type_ == variance_weights.type_)) {\n-    if ((scale_weights.type_ != tensorflow::DataType::DT_FLOAT) &&\n-        (scale_weights.type_ != tensorflow::DataType::DT_HALF)) {\n+\n+  //  Check parameter types\n+  auto parameter_type = inputs.at(1).weights().type_;\n+  if ((parameter_type != tensorflow::DataType::DT_FLOAT) &&\n+      (parameter_type != tensorflow::DataType::DT_HALF)) {\n+    return tensorflow::errors::Unimplemented(\n+        \"only float32 or float16 weight data type is supported, for node \" +\n+        node_def.name() + \" got \" + tensorflow::DataTypeString(parameter_type));\n+  }\n+  for (int i = 1; i < 5; i++) {\n+    if (inputs.at(i).weights().type_ != parameter_type) {\n       return tensorflow::errors::Unimplemented(\n-          \"only float32 or float16 weight data type is supported, for node \" +\n-          node_def.name() + \" got \" +\n-          tensorflow::DataTypeString(scale_weights.type_));\n+          \"Inconsistent parameter type for batchnormis not supported, at: \" +\n+          node_def.name());\n+    }\n+  }\n+\n+  TRT_ShapedWeights dummy_power_weights(parameter_type);\n+  size_t nweight = 0;\n+  for (int i = 1; i < 5; i++) {\n+    nweight = std::max(nweight, (size_t)inputs.at(i).weights().count());\n+  }\n+  TRT_ShapedWeights* ptr_shape_weights = nullptr;\n+  for (int i = 1; i < 5; i++) {\n+    if (inputs.at(i).weights().count() == nweight) {\n+      ptr_shape_weights =\n+          const_cast<TRT_ShapedWeights*>(&(inputs.at(i).weights()));\n+    } else if (inputs.at(i).weights().count() != 1) {\n+      return tensorflow::errors::InvalidArgument(\n+          \"Inconsistent batchnorm parameter count, at: \" + node_def.name());\n     }\n-    if (scale_weights.type_ == tensorflow::DT_FLOAT) {\n-      for (size_t i = 0; i < nweight; ++i) {\n-        float scale = (static_cast<float const*>(scale_weights.GetValues()))[i];\n-        float offset =\n-            (static_cast<float const*>(offset_weights.GetValues()))[i];\n-        float mean = (static_cast<float const*>(mean_weights.GetValues()))[i];\n-        float variance =\n-            (static_cast<float const*>(variance_weights.GetValues()))[i];\n-        float& combined_scale_ref = const_cast<float*>(\n-            static_cast<float const*>(combined_scale_weights.GetValues()))[i];\n-        float& combined_offset_ref = const_cast<float*>(\n-            static_cast<float const*>(combined_offset_weights.GetValues()))[i];\n-        combined_scale_ref = scale / sqrtf(variance + epsilon);\n-        combined_offset_ref = offset - mean * combined_scale_ref;\n+  }\n+  //  We could technically have two weights with different shape.\n+  //  that requires two addScale op, arguably less performant\n+  TRT_ShapedWeights combined_scale_weights =\n+      ctx.get_temp_weights_like(*ptr_shape_weights);\n+  TRT_ShapedWeights combined_offset_weights =\n+      ctx.get_temp_weights_like(*ptr_shape_weights);\n+\n+  if (parameter_type == tensorflow::DT_FLOAT) {\n+    for (size_t i = 0; i < nweight; ++i) {\n+      float batchnorm_data[4];\n+      for (int j = 0; j < 4; j++) {\n+        if (inputs.at(j + 1).weights().count() != 1) {\n+          batchnorm_data[j] = (static_cast<float const*>(\n+              inputs.at(j + 1).weights().GetValues()))[i];\n+        } else {\n+          batchnorm_data[j] = (static_cast<float const*>(\n+              inputs.at(j + 1).weights().GetValues()))[0];\n+        }\n       }\n-    } else {\n-      const Eigen::half* scale_vals =\n-          (static_cast<Eigen::half const*>(scale_weights.GetValues()));\n-      const Eigen::half* off_vals =\n-          (static_cast<Eigen::half const*>(offset_weights.GetValues()));\n-      const Eigen::half* mean_vals =\n-          (static_cast<Eigen::half const*>(mean_weights.GetValues()));\n-      const Eigen::half* variance_vals =\n-          (static_cast<Eigen::half const*>(variance_weights.GetValues()));\n-      Eigen::half* comb_scale_vals = const_cast<Eigen::half*>(\n-          static_cast<Eigen::half const*>(combined_scale_weights.GetValues()));\n-      Eigen::half* comb_off_vals = const_cast<Eigen::half*>(\n-          static_cast<Eigen::half const*>(combined_offset_weights.GetValues()));\n-      for (size_t i = 0; i < nweight; ++i) {\n-        float scale(scale_vals[i]);\n-        float offset(off_vals[i]);\n-        float mean(mean_vals[i]);\n-        float variance(variance_vals[i]);\n-        float combined_scale_ref = scale / sqrtf(variance + epsilon);\n-        comb_scale_vals[i] = Eigen::half(combined_scale_ref);\n-        float combined_offset_ref = offset - mean * combined_scale_ref;\n-        comb_off_vals[i] = Eigen::half(combined_offset_ref);\n+      float scale = batchnorm_data[0];\n+      float offset = batchnorm_data[1];\n+      float mean = batchnorm_data[2];\n+      float variance = batchnorm_data[3];\n+      float& combined_scale_ref = const_cast<float*>(\n+          static_cast<float const*>(combined_scale_weights.GetValues()))[i];\n+      float& combined_offset_ref = const_cast<float*>(\n+          static_cast<float const*>(combined_offset_weights.GetValues()))[i];\n+      combined_scale_ref = scale / sqrtf(variance + epsilon);\n+      combined_offset_ref = offset - mean * combined_scale_ref;\n+    }\n+  } else {\n+    const Eigen::half* cast_vals_array[4];", "path": "tensorflow/contrib/tensorrt/convert/convert_nodes.cc", "position": null, "original_position": 247, "commit_id": "bd28c9e3e9bbc1bbabe801741aa67b057df26624", "original_commit_id": "4a1bb9ddd2c99ea8a068d299881e95873e60f309", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "I think this branch (starting from line 1908) can reuse code from line 1876 to 1897 in the `if` branch, by putting the `parameter_type` check inside the `for (size_t i..)` loop? `cast_vals_array` can be of float type as it gets assigned to `batchnorm_data` which is later used as float (line 1917)?", "created_at": "2018-03-18T23:30:29Z", "updated_at": "2018-03-19T18:12:59Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/17772#discussion_r175309405", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17772", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175309405"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/17772#discussion_r175309405"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17772"}}, "body_html": "<p>I think this branch (starting from line 1908) can reuse code from line 1876 to 1897 in the <code>if</code> branch, by putting the <code>parameter_type</code> check inside the <code>for (size_t i..)</code> loop? <code>cast_vals_array</code> can be of float type as it gets assigned to <code>batchnorm_data</code> which is later used as float (line 1917)?</p>", "body_text": "I think this branch (starting from line 1908) can reuse code from line 1876 to 1897 in the if branch, by putting the parameter_type check inside the for (size_t i..) loop? cast_vals_array can be of float type as it gets assigned to batchnorm_data which is later used as float (line 1917)?"}