{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175278055", "pull_request_review_id": 104785586, "id": 175278055, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI3ODA1NQ==", "diff_hunk": "@@ -1447,62 +1446,23 @@ tensorflow::Status ConvertConst(Converter& ctx,\n         scalar_shape.type[i] = nvinfer1::DimensionType::kSPATIAL;\n       }\n     }\n-    if (ctx.isFP16()) {\n-      auto dtype_new = tensorflow::DataType::DT_HALF;\n-      size_t len_data = tensorflow::DataTypeSize(dtype_new);\n-      for (int i = 0; i < scalar_shape.nbDims; i++)\n-        len_data *= scalar_shape.d[i];\n-      ctx.weight_store()->store_.push_back(std::vector<uint8_t>(len_data));\n-      void* dst = static_cast<void*>(&(ctx.weight_store()->store_.back()[0]));\n-      tensorflow::Tensor temp_tensor(tensorflow::DT_HALF, tensor.shape());\n-      TTypes<Eigen::half>::Flat half_tensor = temp_tensor.flat<Eigen::half>();\n-      Eigen::DefaultDevice defd;\n-      switch (dtype) {\n-        case (tensorflow::DT_INT32): {\n-          half_tensor.device(defd) =\n-              tensor.flat<int32>().template cast<Eigen::half>();\n-          break;\n-        }\n-        case (tensorflow::DT_INT16): {\n-          half_tensor.device(defd) =\n-              tensor.flat<int16>().template cast<Eigen::half>();\n-          break;\n-        }\n-        case (tensorflow::DT_INT8): {\n-          half_tensor.device(defd) =\n-              tensor.flat<int8>().template cast<Eigen::half>();\n-          break;\n-        }\n-        case (tensorflow::DT_UINT8): {\n-          half_tensor.device(defd) =\n-              tensor.flat<uint8>().template cast<Eigen::half>();\n-          break;\n-        }\n-        default:\n-          return tensorflow::errors::InvalidArgument(\n-              \"Datatype \" + tensorflow::DataTypeString(dtype) +\n-              \" for FP16 conversion\");\n-          break;\n-      };\n-      memcpy(dst, half_tensor.data(), len_data);  // store into weight store\n-      weights = TRT_ShapedWeights(dtype_new, dst, scalar_shape);\n-    } else {\n-      size_t len_data = tensorflow::DataTypeSize(dtype);\n-      for (int i = 0; i < scalar_shape.nbDims; i++)\n-        len_data *= scalar_shape.d[i];\n-      size_t len_tensor = weights_tensor.int_val_size() * sizeof(int32);\n-      len_data = std::max(len_data, len_tensor);\n-      ctx.weight_store()->store_.push_back(std::vector<uint8_t>(len_data));\n-      void* dst = static_cast<void*>(&(ctx.weight_store()->store_.back()[0]));\n-      std::vector<int32> tensor_data(\n-          weights_tensor.int_val().begin(),\n-          weights_tensor.int_val()\n-              .end());  //  make a local copy first to flatten\n-                        //  doesn't have to be contigous\n-      memcpy(dst, tensor_data.data(), len_tensor);  // store into weight store\n-      weights = TRT_ShapedWeights(dtype, dst, scalar_shape);\n-    }\n+    //  we should not have converted //if (ctx.isFP16()) {", "path": "tensorflow/contrib/tensorrt/convert/convert_nodes.cc", "position": 99, "original_position": 97, "commit_id": "bd28c9e3e9bbc1bbabe801741aa67b057df26624", "original_commit_id": "ab4dc7b9780217f9980b97114e3742c8997a50c6", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "I don't understand, why removing the fp16 logic?", "created_at": "2018-03-18T05:48:52Z", "updated_at": "2018-03-19T18:12:59Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/17772#discussion_r175278055", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17772", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175278055"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/17772#discussion_r175278055"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17772"}}, "body_html": "<p>I don't understand, why removing the fp16 logic?</p>", "body_text": "I don't understand, why removing the fp16 logic?"}