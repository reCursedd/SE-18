{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175280223", "pull_request_review_id": 104788550, "id": 175280223, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTI4MDIyMw==", "diff_hunk": "@@ -1447,62 +1446,23 @@ tensorflow::Status ConvertConst(Converter& ctx,\n         scalar_shape.type[i] = nvinfer1::DimensionType::kSPATIAL;\n       }\n     }\n-    if (ctx.isFP16()) {\n-      auto dtype_new = tensorflow::DataType::DT_HALF;\n-      size_t len_data = tensorflow::DataTypeSize(dtype_new);\n-      for (int i = 0; i < scalar_shape.nbDims; i++)\n-        len_data *= scalar_shape.d[i];\n-      ctx.weight_store()->store_.push_back(std::vector<uint8_t>(len_data));\n-      void* dst = static_cast<void*>(&(ctx.weight_store()->store_.back()[0]));\n-      tensorflow::Tensor temp_tensor(tensorflow::DT_HALF, tensor.shape());\n-      TTypes<Eigen::half>::Flat half_tensor = temp_tensor.flat<Eigen::half>();\n-      Eigen::DefaultDevice defd;\n-      switch (dtype) {\n-        case (tensorflow::DT_INT32): {\n-          half_tensor.device(defd) =\n-              tensor.flat<int32>().template cast<Eigen::half>();\n-          break;\n-        }\n-        case (tensorflow::DT_INT16): {\n-          half_tensor.device(defd) =\n-              tensor.flat<int16>().template cast<Eigen::half>();\n-          break;\n-        }\n-        case (tensorflow::DT_INT8): {\n-          half_tensor.device(defd) =\n-              tensor.flat<int8>().template cast<Eigen::half>();\n-          break;\n-        }\n-        case (tensorflow::DT_UINT8): {\n-          half_tensor.device(defd) =\n-              tensor.flat<uint8>().template cast<Eigen::half>();\n-          break;\n-        }\n-        default:\n-          return tensorflow::errors::InvalidArgument(\n-              \"Datatype \" + tensorflow::DataTypeString(dtype) +\n-              \" for FP16 conversion\");\n-          break;\n-      };\n-      memcpy(dst, half_tensor.data(), len_data);  // store into weight store\n-      weights = TRT_ShapedWeights(dtype_new, dst, scalar_shape);\n-    } else {\n-      size_t len_data = tensorflow::DataTypeSize(dtype);\n-      for (int i = 0; i < scalar_shape.nbDims; i++)\n-        len_data *= scalar_shape.d[i];\n-      size_t len_tensor = weights_tensor.int_val_size() * sizeof(int32);\n-      len_data = std::max(len_data, len_tensor);\n-      ctx.weight_store()->store_.push_back(std::vector<uint8_t>(len_data));\n-      void* dst = static_cast<void*>(&(ctx.weight_store()->store_.back()[0]));\n-      std::vector<int32> tensor_data(\n-          weights_tensor.int_val().begin(),\n-          weights_tensor.int_val()\n-              .end());  //  make a local copy first to flatten\n-                        //  doesn't have to be contigous\n-      memcpy(dst, tensor_data.data(), len_tensor);  // store into weight store\n-      weights = TRT_ShapedWeights(dtype, dst, scalar_shape);\n-    }\n+    //  we should not have converted //if (ctx.isFP16()) {", "path": "tensorflow/contrib/tensorrt/convert/convert_nodes.cc", "position": 99, "original_position": 97, "commit_id": "bd28c9e3e9bbc1bbabe801741aa67b057df26624", "original_commit_id": "ab4dc7b9780217f9980b97114e3742c8997a50c6", "user": {"login": "jjsjann123", "id": 3709243, "node_id": "MDQ6VXNlcjM3MDkyNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjsjann123", "html_url": "https://github.com/jjsjann123", "followers_url": "https://api.github.com/users/jjsjann123/followers", "following_url": "https://api.github.com/users/jjsjann123/following{/other_user}", "gists_url": "https://api.github.com/users/jjsjann123/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjsjann123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjsjann123/subscriptions", "organizations_url": "https://api.github.com/users/jjsjann123/orgs", "repos_url": "https://api.github.com/users/jjsjann123/repos", "events_url": "https://api.github.com/users/jjsjann123/events{/privacy}", "received_events_url": "https://api.github.com/users/jjsjann123/received_events", "type": "User", "site_admin": false}, "body": "This fp16 conversion is converting int value to fp16.\r\nWe should only convert fp32 to fp16 (we kept that one here). int values are used for attributes for supported ops in the converter.", "created_at": "2018-03-18T07:45:44Z", "updated_at": "2018-03-19T18:12:59Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/17772#discussion_r175280223", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17772", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175280223"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/17772#discussion_r175280223"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17772"}}, "body_html": "<p>This fp16 conversion is converting int value to fp16.<br>\nWe should only convert fp32 to fp16 (we kept that one here). int values are used for attributes for supported ops in the converter.</p>", "body_text": "This fp16 conversion is converting int value to fp16.\nWe should only convert fp32 to fp16 (we kept that one here). int values are used for attributes for supported ops in the converter.", "in_reply_to_id": 175278055}