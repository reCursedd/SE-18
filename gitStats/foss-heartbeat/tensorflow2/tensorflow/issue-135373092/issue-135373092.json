{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1234", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1234/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1234/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1234/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1234", "id": 135373092, "node_id": "MDU6SXNzdWUxMzUzNzMwOTI=", "number": 1234, "title": "Update documentation for softmax-with-cross-entropy loss functions", "user": {"login": "rdipietro", "id": 5150559, "node_id": "MDQ6VXNlcjUxNTA1NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5150559?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rdipietro", "html_url": "https://github.com/rdipietro", "followers_url": "https://api.github.com/users/rdipietro/followers", "following_url": "https://api.github.com/users/rdipietro/following{/other_user}", "gists_url": "https://api.github.com/users/rdipietro/gists{/gist_id}", "starred_url": "https://api.github.com/users/rdipietro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rdipietro/subscriptions", "organizations_url": "https://api.github.com/users/rdipietro/orgs", "repos_url": "https://api.github.com/users/rdipietro/repos", "events_url": "https://api.github.com/users/rdipietro/events{/privacy}", "received_events_url": "https://api.github.com/users/rdipietro/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-02-22T10:09:32Z", "updated_at": "2017-02-09T22:37:32Z", "closed_at": "2016-03-23T17:14:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><code>softmax_cross_entropy_with_logits</code> and <code>sparse_softmax_cross_entropy_with_logits</code> both have useful  behavior that conflicts with current documentation.</p>\n<p>In <code>softmax_cross_entropy_with_logits</code>: \"All that is required is that each row of <code>labels</code> is<br>\na valid probability distribution.\" In <code>sparse_softmax_cross_entropy_with_logits</code>: \"labels: Each entry <code>labels[i]</code> must be an index in <code>[0, num_classes)</code>.\"</p>\n<p>Neither of these statements is true, and using all 0s in the case of <code>softmax_cross_entropy_with_logits</code> or -1s in the case of <code>sparse_softmax_cross_entropy_with_logits</code> is useful when signals include entries for which we don't want to compute loss.</p>\n<p><code>softmax_cross_entropy_with_logits</code> example:</p>\n<pre><code>logits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]\nlabels = tf.Variable([[0.0, 0.0, 0.0]])\nloss_list = [tf.nn.softmax_cross_entropy_with_logits(logits, labels)\n             for logits in logits_list]\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(loss_list))\n\n# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]\n</code></pre>\n<p><code>sparse_softmax_cross_entropy_with_logits</code> example:</p>\n<pre><code>logits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]\nlabels = tf.cast(tf.Variable([-1]), tf.int64)\nloss_list = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n             for logits in logits_list]\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(loss_list))\n\n# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]\n</code></pre>\n<p>We could filter invalid entries out before computing logits/loss, but in my use cases this would add a lot of boilerplate code and make only a negligible difference performance wise. (RNNs with long sequences, a fairly small number of time steps, multiple predictions per time step, and batch entries with varying <code>sequence_length</code>s. Using this trick lets us avoid having to break up logits/targets etc. into valid chunks vs. invalid chunks.</p>\n<p>Edit: I ended up applying a boolean mask anyway since I wanted to view the loss over time (not just take gradients and minimize). In any case I like the current behavior so that I can apply masks after I obtain loss values over all targets, both valid and invalid.</p>", "body_text": "softmax_cross_entropy_with_logits and sparse_softmax_cross_entropy_with_logits both have useful  behavior that conflicts with current documentation.\nIn softmax_cross_entropy_with_logits: \"All that is required is that each row of labels is\na valid probability distribution.\" In sparse_softmax_cross_entropy_with_logits: \"labels: Each entry labels[i] must be an index in [0, num_classes).\"\nNeither of these statements is true, and using all 0s in the case of softmax_cross_entropy_with_logits or -1s in the case of sparse_softmax_cross_entropy_with_logits is useful when signals include entries for which we don't want to compute loss.\nsoftmax_cross_entropy_with_logits example:\nlogits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]\nlabels = tf.Variable([[0.0, 0.0, 0.0]])\nloss_list = [tf.nn.softmax_cross_entropy_with_logits(logits, labels)\n             for logits in logits_list]\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(loss_list))\n\n# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]\n\nsparse_softmax_cross_entropy_with_logits example:\nlogits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]\nlabels = tf.cast(tf.Variable([-1]), tf.int64)\nloss_list = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n             for logits in logits_list]\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(loss_list))\n\n# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]\n\nWe could filter invalid entries out before computing logits/loss, but in my use cases this would add a lot of boilerplate code and make only a negligible difference performance wise. (RNNs with long sequences, a fairly small number of time steps, multiple predictions per time step, and batch entries with varying sequence_lengths. Using this trick lets us avoid having to break up logits/targets etc. into valid chunks vs. invalid chunks.\nEdit: I ended up applying a boolean mask anyway since I wanted to view the loss over time (not just take gradients and minimize). In any case I like the current behavior so that I can apply masks after I obtain loss values over all targets, both valid and invalid.", "body": "`softmax_cross_entropy_with_logits` and `sparse_softmax_cross_entropy_with_logits` both have useful  behavior that conflicts with current documentation.\n\nIn `softmax_cross_entropy_with_logits`: \"All that is required is that each row of `labels` is\na valid probability distribution.\" In `sparse_softmax_cross_entropy_with_logits`: \"labels: Each entry `labels[i]` must be an index in `[0, num_classes)`.\"\n\nNeither of these statements is true, and using all 0s in the case of `softmax_cross_entropy_with_logits` or -1s in the case of `sparse_softmax_cross_entropy_with_logits` is useful when signals include entries for which we don't want to compute loss.\n\n`softmax_cross_entropy_with_logits` example:\n\n```\nlogits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]\nlabels = tf.Variable([[0.0, 0.0, 0.0]])\nloss_list = [tf.nn.softmax_cross_entropy_with_logits(logits, labels)\n             for logits in logits_list]\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(loss_list))\n\n# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]\n```\n\n`sparse_softmax_cross_entropy_with_logits` example:\n\n```\nlogits_list = [tf.Variable([[0.0, 0.0, 0.0]]), tf.Variable([[99.0, 0.0, 0.0]])]\nlabels = tf.cast(tf.Variable([-1]), tf.int64)\nloss_list = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n             for logits in logits_list]\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(loss_list))\n\n# [array([ 0.], dtype=float32), array([ 0.], dtype=float32)]\n```\n\nWe could filter invalid entries out before computing logits/loss, but in my use cases this would add a lot of boilerplate code and make only a negligible difference performance wise. (RNNs with long sequences, a fairly small number of time steps, multiple predictions per time step, and batch entries with varying `sequence_length`s. Using this trick lets us avoid having to break up logits/targets etc. into valid chunks vs. invalid chunks.\n\nEdit: I ended up applying a boolean mask anyway since I wanted to view the loss over time (not just take gradients and minimize). In any case I like the current behavior so that I can apply masks after I obtain loss values over all targets, both valid and invalid.\n"}