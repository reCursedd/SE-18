{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4297", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4297/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4297/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4297/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4297", "id": 176002951, "node_id": "MDU6SXNzdWUxNzYwMDI5NTE=", "number": 4297, "title": "'SAME' padding works incorrect", "user": {"login": "VaWheel", "id": 12466121, "node_id": "MDQ6VXNlcjEyNDY2MTIx", "avatar_url": "https://avatars2.githubusercontent.com/u/12466121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VaWheel", "html_url": "https://github.com/VaWheel", "followers_url": "https://api.github.com/users/VaWheel/followers", "following_url": "https://api.github.com/users/VaWheel/following{/other_user}", "gists_url": "https://api.github.com/users/VaWheel/gists{/gist_id}", "starred_url": "https://api.github.com/users/VaWheel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VaWheel/subscriptions", "organizations_url": "https://api.github.com/users/VaWheel/orgs", "repos_url": "https://api.github.com/users/VaWheel/repos", "events_url": "https://api.github.com/users/VaWheel/events{/privacy}", "received_events_url": "https://api.github.com/users/VaWheel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-09-09T12:57:39Z", "updated_at": "2016-09-13T16:55:28Z", "closed_at": "2016-09-10T01:53:23Z", "author_association": "NONE", "body_html": "<p>I have 28x28 image. When I apply convolution (or pooling) with 'SAME' padding, kernel size 2x2, stride 2, it produces feature map of size 14x14, but it should be 15x15, so the last (bottom and right) image pixels would't be processed. The output size should be calculated with formula output = [(input+2*padding-kernel) / stride] + 1 (look <a href=\"https://arxiv.org/pdf/1603.07285.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1603.07285.pdf</a>). In case of 'SAME', padding size would be [kernel/2]<br>\nTF version 0.9.0</p>", "body_text": "I have 28x28 image. When I apply convolution (or pooling) with 'SAME' padding, kernel size 2x2, stride 2, it produces feature map of size 14x14, but it should be 15x15, so the last (bottom and right) image pixels would't be processed. The output size should be calculated with formula output = [(input+2*padding-kernel) / stride] + 1 (look https://arxiv.org/pdf/1603.07285.pdf). In case of 'SAME', padding size would be [kernel/2]\nTF version 0.9.0", "body": "I have 28x28 image. When I apply convolution (or pooling) with 'SAME' padding, kernel size 2x2, stride 2, it produces feature map of size 14x14, but it should be 15x15, so the last (bottom and right) image pixels would't be processed. The output size should be calculated with formula output = [(input+2*padding-kernel) / stride] + 1 (look https://arxiv.org/pdf/1603.07285.pdf). In case of 'SAME', padding size would be [kernel/2]\nTF version 0.9.0\n"}