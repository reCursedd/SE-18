{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/329970141", "html_url": "https://github.com/tensorflow/tensorflow/issues/13002#issuecomment-329970141", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13002", "id": 329970141, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTk3MDE0MQ==", "user": {"login": "kudchikarsk", "id": 19891428, "node_id": "MDQ6VXNlcjE5ODkxNDI4", "avatar_url": "https://avatars2.githubusercontent.com/u/19891428?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kudchikarsk", "html_url": "https://github.com/kudchikarsk", "followers_url": "https://api.github.com/users/kudchikarsk/followers", "following_url": "https://api.github.com/users/kudchikarsk/following{/other_user}", "gists_url": "https://api.github.com/users/kudchikarsk/gists{/gist_id}", "starred_url": "https://api.github.com/users/kudchikarsk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kudchikarsk/subscriptions", "organizations_url": "https://api.github.com/users/kudchikarsk/orgs", "repos_url": "https://api.github.com/users/kudchikarsk/repos", "events_url": "https://api.github.com/users/kudchikarsk/events{/privacy}", "received_events_url": "https://api.github.com/users/kudchikarsk/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-16T14:02:12Z", "updated_at": "2017-09-16T14:02:12Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">yeah even I realised that so I totally get your point still, it was\nconfusing at first but thanks for clearing out this thing.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sat, Sep 16, 2017 at 12:20 AM, Cliff Young ***@***.***&gt; wrote:\n You're quite right that the Udacity slides and the code in the ipynb don't\n match up exactly. But I don't think it matters for educational purposes.\n\n I think there are two points in what Vincent is saying:\n\n    1. Having a zero mean makes it easier for the optimizer to work.\n    2. Normalizing different axes to have the same scale (so circular, or\n    standard deviations, rather than oval, as shown in slides) also makes it\n    easier for the optimizer to work.\n\n I think the code gets both right. The -128 moves the mean of the [0,255]\n image data close to zero.\n The /255 rescales so that values will be in the range [-.5,.5]. The\n formulas that Vincent showed would have scaled to [-1,1]. But as long as\n everything rescales to the same ranges (so you get circularity across\n dimensions, not ovals), then the optimizer will still have an easier time.\n It's not the absolute magnitude of the rescaling that matters; it's the\n relative scales across the dimensions.\n\n Not sure if the course has gotten to batch normalization at this point,\n but that strives to rescale a standard deviation to have value about 1.0.\n Then you don't know the absolute max/min bounds, but you do have some\n statistical beliefs that your data are now spread out similarly in multiple\n dimensions.\n\n Does that make sense? <a class=\"user-mention\" href=\"https://github.com/vincentvanhoucke\">@vincentvanhoucke</a>\n &lt;<a href=\"https://github.com/vincentvanhoucke\">https://github.com/vincentvanhoucke</a>&gt;, feel free to amend or comment, but\n I'll close for now.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"257152371\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13002\" href=\"https://github.com/tensorflow/tensorflow/issues/13002#issuecomment-329867691\">#13002 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AS-E5OT3O6vzZDOnRiMT2xyaXAlivBPiks5siscEgaJpZM4PVH1k\">https://github.com/notifications/unsubscribe-auth/AS-E5OT3O6vzZDOnRiMT2xyaXAlivBPiks5siscEgaJpZM4PVH1k</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "yeah even I realised that so I totally get your point still, it was\nconfusing at first but thanks for clearing out this thing.\n\u2026\nOn Sat, Sep 16, 2017 at 12:20 AM, Cliff Young ***@***.***> wrote:\n You're quite right that the Udacity slides and the code in the ipynb don't\n match up exactly. But I don't think it matters for educational purposes.\n\n I think there are two points in what Vincent is saying:\n\n    1. Having a zero mean makes it easier for the optimizer to work.\n    2. Normalizing different axes to have the same scale (so circular, or\n    standard deviations, rather than oval, as shown in slides) also makes it\n    easier for the optimizer to work.\n\n I think the code gets both right. The -128 moves the mean of the [0,255]\n image data close to zero.\n The /255 rescales so that values will be in the range [-.5,.5]. The\n formulas that Vincent showed would have scaled to [-1,1]. But as long as\n everything rescales to the same ranges (so you get circularity across\n dimensions, not ovals), then the optimizer will still have an easier time.\n It's not the absolute magnitude of the rescaling that matters; it's the\n relative scales across the dimensions.\n\n Not sure if the course has gotten to batch normalization at this point,\n but that strives to rescale a standard deviation to have value about 1.0.\n Then you don't know the absolute max/min bounds, but you do have some\n statistical beliefs that your data are now spread out similarly in multiple\n dimensions.\n\n Does that make sense? @vincentvanhoucke\n <https://github.com/vincentvanhoucke>, feel free to amend or comment, but\n I'll close for now.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#13002 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AS-E5OT3O6vzZDOnRiMT2xyaXAlivBPiks5siscEgaJpZM4PVH1k>\n .", "body": "yeah even I realised that so I totally get your point still, it was\nconfusing at first but thanks for clearing out this thing.\n\nOn Sat, Sep 16, 2017 at 12:20 AM, Cliff Young <notifications@github.com>\nwrote:\n\n> You're quite right that the Udacity slides and the code in the ipynb don't\n> match up exactly. But I don't think it matters for educational purposes.\n>\n> I think there are two points in what Vincent is saying:\n>\n>    1. Having a zero mean makes it easier for the optimizer to work.\n>    2. Normalizing different axes to have the same scale (so circular, or\n>    standard deviations, rather than oval, as shown in slides) also makes it\n>    easier for the optimizer to work.\n>\n> I think the code gets both right. The -128 moves the mean of the [0,255]\n> image data close to zero.\n> The /255 rescales so that values will be in the range [-.5,.5]. The\n> formulas that Vincent showed would have scaled to [-1,1]. But as long as\n> everything rescales to the same ranges (so you get circularity across\n> dimensions, not ovals), then the optimizer will still have an easier time.\n> It's not the absolute magnitude of the rescaling that matters; it's the\n> relative scales across the dimensions.\n>\n> Not sure if the course has gotten to batch normalization at this point,\n> but that strives to rescale a standard deviation to have value about 1.0.\n> Then you don't know the absolute max/min bounds, but you do have some\n> statistical beliefs that your data are now spread out similarly in multiple\n> dimensions.\n>\n> Does that make sense? @vincentvanhoucke\n> <https://github.com/vincentvanhoucke>, feel free to amend or comment, but\n> I'll close for now.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13002#issuecomment-329867691>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AS-E5OT3O6vzZDOnRiMT2xyaXAlivBPiks5siscEgaJpZM4PVH1k>\n> .\n>\n"}