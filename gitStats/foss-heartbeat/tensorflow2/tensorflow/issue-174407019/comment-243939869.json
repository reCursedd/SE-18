{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243939869", "html_url": "https://github.com/tensorflow/tensorflow/issues/4138#issuecomment-243939869", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4138", "id": 243939869, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzkzOTg2OQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-01T00:12:08Z", "updated_at": "2016-09-01T00:12:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I suspect the issue is that the weight matrix is transposed, and this is causing the automatically-generated gradients code to take a slow path. In particular, <code>tf.nn.sampled_softmax_loss()</code> will generate sparse gradients for the weights, but the <a href=\"https://github.com/tensorflow/tensorflow/blob/9725395b8d68df30cfc81d0076db2365e49753fb/tensorflow/python/ops/array_grad.py#L341\">gradient function for <code>tf.transpose()</code></a> doesn't have an optimized implementation for handling sparse <code>tf.IndexedSlices</code> objects.</p>\n<p>Can you try running an experiment where you define the <code>softmax_w</code> variable as having shape <code>[vocab_size, size]</code> so that you don't have to transpose it? I would expect the performance of the sampled softmax to be much better in that case.</p>", "body_text": "I suspect the issue is that the weight matrix is transposed, and this is causing the automatically-generated gradients code to take a slow path. In particular, tf.nn.sampled_softmax_loss() will generate sparse gradients for the weights, but the gradient function for tf.transpose() doesn't have an optimized implementation for handling sparse tf.IndexedSlices objects.\nCan you try running an experiment where you define the softmax_w variable as having shape [vocab_size, size] so that you don't have to transpose it? I would expect the performance of the sampled softmax to be much better in that case.", "body": "I suspect the issue is that the weight matrix is transposed, and this is causing the automatically-generated gradients code to take a slow path. In particular, `tf.nn.sampled_softmax_loss()` will generate sparse gradients for the weights, but the [gradient function for `tf.transpose()`](https://github.com/tensorflow/tensorflow/blob/9725395b8d68df30cfc81d0076db2365e49753fb/tensorflow/python/ops/array_grad.py#L341) doesn't have an optimized implementation for handling sparse `tf.IndexedSlices` objects.\n\nCan you try running an experiment where you define the `softmax_w` variable as having shape `[vocab_size, size]` so that you don't have to transpose it? I would expect the performance of the sampled softmax to be much better in that case.\n"}