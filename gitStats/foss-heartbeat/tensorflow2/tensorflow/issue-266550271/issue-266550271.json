{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13810", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13810/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13810/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13810/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13810", "id": 266550271, "node_id": "MDU6SXNzdWUyNjY1NTAyNzE=", "number": 13810, "title": "tf.nn.max_pool memory leak?", "user": {"login": "basveeling", "id": 536975, "node_id": "MDQ6VXNlcjUzNjk3NQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/536975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/basveeling", "html_url": "https://github.com/basveeling", "followers_url": "https://api.github.com/users/basveeling/followers", "following_url": "https://api.github.com/users/basveeling/following{/other_user}", "gists_url": "https://api.github.com/users/basveeling/gists{/gist_id}", "starred_url": "https://api.github.com/users/basveeling/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/basveeling/subscriptions", "organizations_url": "https://api.github.com/users/basveeling/orgs", "repos_url": "https://api.github.com/users/basveeling/repos", "events_url": "https://api.github.com/users/basveeling/events{/privacy}", "received_events_url": "https://api.github.com/users/basveeling/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-10-18T16:05:12Z", "updated_at": "2018-06-10T18:43:30Z", "closed_at": "2018-06-10T18:32:46Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip install tensorflow-gpu</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2.1</li>\n<li><strong>Python version</strong>: 3.6.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 5.</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I'm using a tensorflow graph for offline preprocessing, and I believe I've stumbled onto a memory leak problem with tf.nn.maxpool. After certain amount of feed-forward operations (only inference, no training), I get a <code>Allocator (cuda_host_bfc) ran out of memory trying to allocate ...</code> error. This happens consistently after a number of data<em>points</em> fed through the graph, independtly on how those datapoints are spread over batches. e.g. with batchsize 4 I get this after 64 iterations, with batchsize 2 I get this error after 128 iterations.<br>\nThe following rough code should reproduce this issue on a Titan XP with 12gb memory, didn't test this due to time constraints.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> keras <span class=\"pl-k\">import</span> Input\n<span class=\"pl-k\">from</span> keras.layers <span class=\"pl-k\">import</span> AvgPool2D, MaxPool2D, Lambda, Concatenate\n<span class=\"pl-k\">from</span> keras.engine <span class=\"pl-k\">import</span> Model\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nthumb_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8192</span>\nstrides <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\npooling <span class=\"pl-k\">=</span> <span class=\"pl-c1\">22</span>\ninputs, outputs <span class=\"pl-k\">=</span> [], []\nslide <span class=\"pl-k\">=</span> Input([thumb_dim, thumb_dim, <span class=\"pl-c1\">3</span>])\nmask <span class=\"pl-k\">=</span> Input([thumb_dim, thumb_dim, <span class=\"pl-c1\">1</span>])\ninputs.append(slide)\ninputs.append(mask)\n\nhsv_slide <span class=\"pl-k\">=</span> Lambda(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: tf.image.rgb_to_hsv(x))(slide)\nheatmap <span class=\"pl-k\">=</span> Lambda(\n    <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x[<span class=\"pl-c1\">...</span>, <span class=\"pl-c1\">1</span>:<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> tf.cast(x[<span class=\"pl-c1\">...</span>, <span class=\"pl-c1\">2</span>:<span class=\"pl-c1\">3</span>] <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0.3</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>))(\n    hsv_slide)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> heatmap = AvgPool2D(4, strides=1, padding='same')(heatmap)</span>\n\nto_pool <span class=\"pl-k\">=</span> Concatenate(<span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)([heatmap, mask])\nconcat <span class=\"pl-k\">=</span> Lambda(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: tf.stop_gradient(tf.nn.max_pool(x, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, pooling, pooling, <span class=\"pl-c1\">1</span>),\n                                                          <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, strides, strides, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>)))(to_pool)\n\noutputs.append(concat)\nrpn <span class=\"pl-k\">=</span> Model(inputs, outputs)\nrpn.predict_generator(([\n    np.random.random((<span class=\"pl-c1\">2</span>, thumb_dim, thumb_dim, <span class=\"pl-c1\">3</span>)),\n    np.random.random((<span class=\"pl-c1\">2</span>, thumb_dim, thumb_dim, <span class=\"pl-c1\">1</span>))] <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">200</span>)))</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTensorFlow installed from (source or binary): pip install tensorflow-gpu\nTensorFlow version (use command below): 1.2.1\nPython version: 3.6.1\nBazel version (if compiling from source):\nCUDA/cuDNN version: 5.\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nI'm using a tensorflow graph for offline preprocessing, and I believe I've stumbled onto a memory leak problem with tf.nn.maxpool. After certain amount of feed-forward operations (only inference, no training), I get a Allocator (cuda_host_bfc) ran out of memory trying to allocate ... error. This happens consistently after a number of datapoints fed through the graph, independtly on how those datapoints are spread over batches. e.g. with batchsize 4 I get this after 64 iterations, with batchsize 2 I get this error after 128 iterations.\nThe following rough code should reproduce this issue on a Titan XP with 12gb memory, didn't test this due to time constraints.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\nimport tensorflow as tf\nfrom keras import Input\nfrom keras.layers import AvgPool2D, MaxPool2D, Lambda, Concatenate\nfrom keras.engine import Model\nimport numpy as np\n\nthumb_dim = 8192\nstrides = 1\npooling = 22\ninputs, outputs = [], []\nslide = Input([thumb_dim, thumb_dim, 3])\nmask = Input([thumb_dim, thumb_dim, 1])\ninputs.append(slide)\ninputs.append(mask)\n\nhsv_slide = Lambda(lambda x: tf.image.rgb_to_hsv(x))(slide)\nheatmap = Lambda(\n    lambda x: x[..., 1:2] * tf.cast(x[..., 2:3] > 0.3, 'float32'))(\n    hsv_slide)\n# heatmap = AvgPool2D(4, strides=1, padding='same')(heatmap)\n\nto_pool = Concatenate(axis=-1)([heatmap, mask])\nconcat = Lambda(lambda x: tf.stop_gradient(tf.nn.max_pool(x, ksize=(1, pooling, pooling, 1),\n                                                          strides=(1, strides, strides, 1), padding='VALID')))(to_pool)\n\noutputs.append(concat)\nrpn = Model(inputs, outputs)\nrpn.predict_generator(([\n    np.random.random((2, thumb_dim, thumb_dim, 3)),\n    np.random.random((2, thumb_dim, thumb_dim, 1))] for _ in range(200)))", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 5.\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI'm using a tensorflow graph for offline preprocessing, and I believe I've stumbled onto a memory leak problem with tf.nn.maxpool. After certain amount of feed-forward operations (only inference, no training), I get a `Allocator (cuda_host_bfc) ran out of memory trying to allocate ...` error. This happens consistently after a number of data*points* fed through the graph, independtly on how those datapoints are spread over batches. e.g. with batchsize 4 I get this after 64 iterations, with batchsize 2 I get this error after 128 iterations.\r\nThe following rough code should reproduce this issue on a Titan XP with 12gb memory, didn't test this due to time constraints.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nfrom keras import Input\r\nfrom keras.layers import AvgPool2D, MaxPool2D, Lambda, Concatenate\r\nfrom keras.engine import Model\r\nimport numpy as np\r\n\r\nthumb_dim = 8192\r\nstrides = 1\r\npooling = 22\r\ninputs, outputs = [], []\r\nslide = Input([thumb_dim, thumb_dim, 3])\r\nmask = Input([thumb_dim, thumb_dim, 1])\r\ninputs.append(slide)\r\ninputs.append(mask)\r\n\r\nhsv_slide = Lambda(lambda x: tf.image.rgb_to_hsv(x))(slide)\r\nheatmap = Lambda(\r\n    lambda x: x[..., 1:2] * tf.cast(x[..., 2:3] > 0.3, 'float32'))(\r\n    hsv_slide)\r\n# heatmap = AvgPool2D(4, strides=1, padding='same')(heatmap)\r\n\r\nto_pool = Concatenate(axis=-1)([heatmap, mask])\r\nconcat = Lambda(lambda x: tf.stop_gradient(tf.nn.max_pool(x, ksize=(1, pooling, pooling, 1),\r\n                                                          strides=(1, strides, strides, 1), padding='VALID')))(to_pool)\r\n\r\noutputs.append(concat)\r\nrpn = Model(inputs, outputs)\r\nrpn.predict_generator(([\r\n    np.random.random((2, thumb_dim, thumb_dim, 3)),\r\n    np.random.random((2, thumb_dim, thumb_dim, 1))] for _ in range(200)))\r\n```"}