{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203428191", "pull_request_review_id": 138315381, "id": 203428191, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzQyODE5MQ==", "diff_hunk": "@@ -46,7 +47,7 @@ Status XlaGpuDeviceFactory::CreateDevices(const SessionOptions& options,\n \n   std::unique_ptr<XlaDevice> device;\n   Status status =\n-      XlaDevice::Create(\"CUDA\", DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,\n+      XlaDevice::Create(GPU_PLATFORM_NAME, DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,", "path": "tensorflow/compiler/jit/xla_gpu_device.cc", "position": null, "original_position": 13, "commit_id": "c8ae8965ea75a28f30d80ed7657fd9f31b18b48d", "original_commit_id": "9af995ecf3ba1815a0a9cf6e0b4ca372f1ff0b9f", "user": {"login": "whchung", "id": 1673574, "node_id": "MDQ6VXNlcjE2NzM1NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1673574?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whchung", "html_url": "https://github.com/whchung", "followers_url": "https://api.github.com/users/whchung/followers", "following_url": "https://api.github.com/users/whchung/following{/other_user}", "gists_url": "https://api.github.com/users/whchung/gists{/gist_id}", "starred_url": "https://api.github.com/users/whchung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whchung/subscriptions", "organizations_url": "https://api.github.com/users/whchung/orgs", "repos_url": "https://api.github.com/users/whchung/repos", "events_url": "https://api.github.com/users/whchung/events{/privacy}", "received_events_url": "https://api.github.com/users/whchung/received_events", "type": "User", "site_admin": false}, "body": "@jlebar / @hawkinsp Thus far I think whether \"having one binary to support both CUDA and ROCm\" is a true merit and worthwhile become a requirement remains unknown. In #20277 I was asked to change `configure.py` so CUDA / SYCL / ROCm are mutually exclusive at configure time.\r\n\r\nWould it be possible to consider keeping the current form of having CUDA / ROCm be mututally exclusive at configure/compile-time, and having ROCm-based TensorFlow be shipped in a different PyPI package, say `tensorflow-rocm`? Actually @parallelo and I are already working on the infrastructure to publish such package to PyPI.\r\n\r\nWe can leave the discussion of \"one binary to support both CUDA and ROCm\" for future versions when \"hip-clang\" becomes mature enough. By then we can discuss about how to merge CROSSTOOL for both platforms, how to merge kernel sections and emissions of hidden runtime function to register GPU kernels.", "created_at": "2018-07-18T15:38:20Z", "updated_at": "2018-08-29T21:18:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203428191", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203428191"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203428191"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a> / <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=348932\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hawkinsp\">@hawkinsp</a> Thus far I think whether \"having one binary to support both CUDA and ROCm\" is a true merit and worthwhile become a requirement remains unknown. In <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"335455607\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/20277\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/20277/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/20277\">#20277</a> I was asked to change <code>configure.py</code> so CUDA / SYCL / ROCm are mutually exclusive at configure time.</p>\n<p>Would it be possible to consider keeping the current form of having CUDA / ROCm be mututally exclusive at configure/compile-time, and having ROCm-based TensorFlow be shipped in a different PyPI package, say <code>tensorflow-rocm</code>? Actually <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2438230\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/parallelo\">@parallelo</a> and I are already working on the infrastructure to publish such package to PyPI.</p>\n<p>We can leave the discussion of \"one binary to support both CUDA and ROCm\" for future versions when \"hip-clang\" becomes mature enough. By then we can discuss about how to merge CROSSTOOL for both platforms, how to merge kernel sections and emissions of hidden runtime function to register GPU kernels.</p>", "body_text": "@jlebar / @hawkinsp Thus far I think whether \"having one binary to support both CUDA and ROCm\" is a true merit and worthwhile become a requirement remains unknown. In #20277 I was asked to change configure.py so CUDA / SYCL / ROCm are mutually exclusive at configure time.\nWould it be possible to consider keeping the current form of having CUDA / ROCm be mututally exclusive at configure/compile-time, and having ROCm-based TensorFlow be shipped in a different PyPI package, say tensorflow-rocm? Actually @parallelo and I are already working on the infrastructure to publish such package to PyPI.\nWe can leave the discussion of \"one binary to support both CUDA and ROCm\" for future versions when \"hip-clang\" becomes mature enough. By then we can discuss about how to merge CROSSTOOL for both platforms, how to merge kernel sections and emissions of hidden runtime function to register GPU kernels.", "in_reply_to_id": 203368119}