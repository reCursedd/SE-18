{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203434030", "pull_request_review_id": 138322656, "id": 203434030, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzQzNDAzMA==", "diff_hunk": "@@ -46,7 +47,7 @@ Status XlaGpuDeviceFactory::CreateDevices(const SessionOptions& options,\n \n   std::unique_ptr<XlaDevice> device;\n   Status status =\n-      XlaDevice::Create(\"CUDA\", DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,\n+      XlaDevice::Create(GPU_PLATFORM_NAME, DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,", "path": "tensorflow/compiler/jit/xla_gpu_device.cc", "position": null, "original_position": 13, "commit_id": "c8ae8965ea75a28f30d80ed7657fd9f31b18b48d", "original_commit_id": "9af995ecf3ba1815a0a9cf6e0b4ca372f1ff0b9f", "user": {"login": "hawkinsp", "id": 348932, "node_id": "MDQ6VXNlcjM0ODkzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/348932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hawkinsp", "html_url": "https://github.com/hawkinsp", "followers_url": "https://api.github.com/users/hawkinsp/followers", "following_url": "https://api.github.com/users/hawkinsp/following{/other_user}", "gists_url": "https://api.github.com/users/hawkinsp/gists{/gist_id}", "starred_url": "https://api.github.com/users/hawkinsp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hawkinsp/subscriptions", "organizations_url": "https://api.github.com/users/hawkinsp/orgs", "repos_url": "https://api.github.com/users/hawkinsp/repos", "events_url": "https://api.github.com/users/hawkinsp/events{/privacy}", "received_events_url": "https://api.github.com/users/hawkinsp/received_events", "type": "User", "site_admin": false}, "body": "I agree #20277 did point you towards enforcing that the two options are not simultaneously set, but that was mostly because it didn't make sense to do so (the assertion was that the two did not work at the same time). I also suspect the deployment question is separate: even if you *can* build a binary that supports multiple GPUs, you don't necessarily have to deploy that binary in that form via PyPI.\r\n\r\nI'm fine if the two are forced to be mutually exclusive at configure time for now.\r\n\r\nI'd just like to agree on how the TF devices are named and how the code is structured now before we check in much more, because changing it later will be very hard.", "created_at": "2018-07-18T15:54:18Z", "updated_at": "2018-08-29T21:18:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203434030", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203434030"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203434030"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845"}}, "body_html": "<p>I agree <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"335455607\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/20277\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/20277/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/20277\">#20277</a> did point you towards enforcing that the two options are not simultaneously set, but that was mostly because it didn't make sense to do so (the assertion was that the two did not work at the same time). I also suspect the deployment question is separate: even if you <em>can</em> build a binary that supports multiple GPUs, you don't necessarily have to deploy that binary in that form via PyPI.</p>\n<p>I'm fine if the two are forced to be mutually exclusive at configure time for now.</p>\n<p>I'd just like to agree on how the TF devices are named and how the code is structured now before we check in much more, because changing it later will be very hard.</p>", "body_text": "I agree #20277 did point you towards enforcing that the two options are not simultaneously set, but that was mostly because it didn't make sense to do so (the assertion was that the two did not work at the same time). I also suspect the deployment question is separate: even if you can build a binary that supports multiple GPUs, you don't necessarily have to deploy that binary in that form via PyPI.\nI'm fine if the two are forced to be mutually exclusive at configure time for now.\nI'd just like to agree on how the TF devices are named and how the code is structured now before we check in much more, because changing it later will be very hard.", "in_reply_to_id": 203368119}