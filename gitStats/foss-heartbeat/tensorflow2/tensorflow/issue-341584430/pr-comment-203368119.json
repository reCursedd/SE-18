{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203368119", "pull_request_review_id": 138235517, "id": 203368119, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzM2ODExOQ==", "diff_hunk": "@@ -46,7 +47,7 @@ Status XlaGpuDeviceFactory::CreateDevices(const SessionOptions& options,\n \n   std::unique_ptr<XlaDevice> device;\n   Status status =\n-      XlaDevice::Create(\"CUDA\", DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,\n+      XlaDevice::Create(GPU_PLATFORM_NAME, DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,", "path": "tensorflow/compiler/jit/xla_gpu_device.cc", "position": null, "original_position": 13, "commit_id": "c8ae8965ea75a28f30d80ed7657fd9f31b18b48d", "original_commit_id": "9af995ecf3ba1815a0a9cf6e0b4ca372f1ff0b9f", "user": {"login": "hawkinsp", "id": 348932, "node_id": "MDQ6VXNlcjM0ODkzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/348932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hawkinsp", "html_url": "https://github.com/hawkinsp", "followers_url": "https://api.github.com/users/hawkinsp/followers", "following_url": "https://api.github.com/users/hawkinsp/following{/other_user}", "gists_url": "https://api.github.com/users/hawkinsp/gists{/gist_id}", "starred_url": "https://api.github.com/users/hawkinsp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hawkinsp/subscriptions", "organizations_url": "https://api.github.com/users/hawkinsp/orgs", "repos_url": "https://api.github.com/users/hawkinsp/repos", "events_url": "https://api.github.com/users/hawkinsp/events{/privacy}", "received_events_url": "https://api.github.com/users/hawkinsp/received_events", "type": "User", "site_admin": false}, "body": "See the comment on gpu_init.h.\r\n\r\nI will note that this is slightly different. There are two different \"GPU\" devices in TensorFlow for Nvidia GPUs at the moment: \"GPU\", which is the TF classic device (running hand-written CUDA kernels, or XLA-compiled code via clustering), and \"XLA_GPU\", which is a device that operates by compiling everything with XLA with no hand-written kernels. This code is the latter (XLA_GPU).\r\n\r\nThe \"GPU\" device is more familiar to most folks, and can do pretty much everything the XLA_GPU device can do (if you opt into using XLA), so few people know about or use the XLA_GPU device, but we have it for a number of reasons:\r\na) testing XLA (e.g., the unit tests in tensorflow/compiler/tests).\r\nb) it's very little additional code (this file, pretty much)\r\nc) some other devices (e.g., TPU) don't have a corresponding \"regular\" device that XLA can piggy back on, so we need an XlaDevice code path to use those devices from TensorFlow, and this device lets us exercise that code path on GPU hardware too.\r\n\r\nHere I suggest the right thing to do is to create a separate XLA_AMDGPU device, mostly because I doubt that both GPUs will have exact feature parity, and we probably need the ability to treat them differently (e.g., with respect to supported types, etc.). We can then register both if they exist.\r\n\r\nFortunately, all the code you need (assuming you have an XLA backend for your device) is pretty much this file. I would branch it for a new, separate XLA_AMDGPU device. Look at xla_cpu_device.cc for another example of an XLA_... device.", "created_at": "2018-07-18T12:59:25Z", "updated_at": "2018-08-29T21:18:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203368119", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203368119"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203368119"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845"}}, "body_html": "<p>See the comment on gpu_init.h.</p>\n<p>I will note that this is slightly different. There are two different \"GPU\" devices in TensorFlow for Nvidia GPUs at the moment: \"GPU\", which is the TF classic device (running hand-written CUDA kernels, or XLA-compiled code via clustering), and \"XLA_GPU\", which is a device that operates by compiling everything with XLA with no hand-written kernels. This code is the latter (XLA_GPU).</p>\n<p>The \"GPU\" device is more familiar to most folks, and can do pretty much everything the XLA_GPU device can do (if you opt into using XLA), so few people know about or use the XLA_GPU device, but we have it for a number of reasons:<br>\na) testing XLA (e.g., the unit tests in tensorflow/compiler/tests).<br>\nb) it's very little additional code (this file, pretty much)<br>\nc) some other devices (e.g., TPU) don't have a corresponding \"regular\" device that XLA can piggy back on, so we need an XlaDevice code path to use those devices from TensorFlow, and this device lets us exercise that code path on GPU hardware too.</p>\n<p>Here I suggest the right thing to do is to create a separate XLA_AMDGPU device, mostly because I doubt that both GPUs will have exact feature parity, and we probably need the ability to treat them differently (e.g., with respect to supported types, etc.). We can then register both if they exist.</p>\n<p>Fortunately, all the code you need (assuming you have an XLA backend for your device) is pretty much this file. I would branch it for a new, separate XLA_AMDGPU device. Look at xla_cpu_device.cc for another example of an XLA_... device.</p>", "body_text": "See the comment on gpu_init.h.\nI will note that this is slightly different. There are two different \"GPU\" devices in TensorFlow for Nvidia GPUs at the moment: \"GPU\", which is the TF classic device (running hand-written CUDA kernels, or XLA-compiled code via clustering), and \"XLA_GPU\", which is a device that operates by compiling everything with XLA with no hand-written kernels. This code is the latter (XLA_GPU).\nThe \"GPU\" device is more familiar to most folks, and can do pretty much everything the XLA_GPU device can do (if you opt into using XLA), so few people know about or use the XLA_GPU device, but we have it for a number of reasons:\na) testing XLA (e.g., the unit tests in tensorflow/compiler/tests).\nb) it's very little additional code (this file, pretty much)\nc) some other devices (e.g., TPU) don't have a corresponding \"regular\" device that XLA can piggy back on, so we need an XlaDevice code path to use those devices from TensorFlow, and this device lets us exercise that code path on GPU hardware too.\nHere I suggest the right thing to do is to create a separate XLA_AMDGPU device, mostly because I doubt that both GPUs will have exact feature parity, and we probably need the ability to treat them differently (e.g., with respect to supported types, etc.). We can then register both if they exist.\nFortunately, all the code you need (assuming you have an XLA backend for your device) is pretty much this file. I would branch it for a new, separate XLA_AMDGPU device. Look at xla_cpu_device.cc for another example of an XLA_... device."}