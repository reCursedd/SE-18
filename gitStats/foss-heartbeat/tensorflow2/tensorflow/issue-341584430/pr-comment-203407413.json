{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203407413", "pull_request_review_id": 138289748, "id": 203407413, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzQwNzQxMw==", "diff_hunk": "@@ -46,7 +47,7 @@ Status XlaGpuDeviceFactory::CreateDevices(const SessionOptions& options,\n \n   std::unique_ptr<XlaDevice> device;\n   Status status =\n-      XlaDevice::Create(\"CUDA\", DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,\n+      XlaDevice::Create(GPU_PLATFORM_NAME, DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,", "path": "tensorflow/compiler/jit/xla_gpu_device.cc", "position": null, "original_position": 13, "commit_id": "c8ae8965ea75a28f30d80ed7657fd9f31b18b48d", "original_commit_id": "9af995ecf3ba1815a0a9cf6e0b4ca372f1ff0b9f", "user": {"login": "whchung", "id": 1673574, "node_id": "MDQ6VXNlcjE2NzM1NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1673574?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whchung", "html_url": "https://github.com/whchung", "followers_url": "https://api.github.com/users/whchung/followers", "following_url": "https://api.github.com/users/whchung/following{/other_user}", "gists_url": "https://api.github.com/users/whchung/gists{/gist_id}", "starred_url": "https://api.github.com/users/whchung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whchung/subscriptions", "organizations_url": "https://api.github.com/users/whchung/orgs", "repos_url": "https://api.github.com/users/whchung/repos", "events_url": "https://api.github.com/users/whchung/events{/privacy}", "received_events_url": "https://api.github.com/users/whchung/received_events", "type": "User", "site_admin": false}, "body": "@hawkinsp Again I don't think a new class of device for AMD GPU is a viable choice, due to how XLA is coupled with GPU common runtime. And how GPU-supported operators are currently registered. \r\n\r\nIn the implementation of `xla_op_registry` / `xla_launch_op` / `mark_for_compilation_pass`, `DEVICE_GPU_XLA_JIT` & `DEVICE_XLA_GPU` are tightly couple with `DEVICE_GPU` in GPU common runtime. And operator implementations in `tensorflow/core/kernels` or `contrib` also register themselves be executed on `DEVICE_GPU`.\r\n\r\nShall a new class of device for AMD GPU is introduced, all these places would have to be altered. We would have duplicated codes to simply register ops for the new device class. And due to the fact user codes could physically specify devices a op be executed, adding new device classes would surely mean such user codes have to be modified which is not feasible.\r\n\r\nFinally, there are other places in Python APIs where `device_type` of a device be checked. Adding a new device class could mean those places have to be amended as well. And again that defeats the purpose of letting user codes stay as is, and be able to run on implementations from different vendors.", "created_at": "2018-07-18T14:45:24Z", "updated_at": "2018-08-29T21:18:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203407413", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203407413"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203407413"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=348932\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hawkinsp\">@hawkinsp</a> Again I don't think a new class of device for AMD GPU is a viable choice, due to how XLA is coupled with GPU common runtime. And how GPU-supported operators are currently registered.</p>\n<p>In the implementation of <code>xla_op_registry</code> / <code>xla_launch_op</code> / <code>mark_for_compilation_pass</code>, <code>DEVICE_GPU_XLA_JIT</code> &amp; <code>DEVICE_XLA_GPU</code> are tightly couple with <code>DEVICE_GPU</code> in GPU common runtime. And operator implementations in <code>tensorflow/core/kernels</code> or <code>contrib</code> also register themselves be executed on <code>DEVICE_GPU</code>.</p>\n<p>Shall a new class of device for AMD GPU is introduced, all these places would have to be altered. We would have duplicated codes to simply register ops for the new device class. And due to the fact user codes could physically specify devices a op be executed, adding new device classes would surely mean such user codes have to be modified which is not feasible.</p>\n<p>Finally, there are other places in Python APIs where <code>device_type</code> of a device be checked. Adding a new device class could mean those places have to be amended as well. And again that defeats the purpose of letting user codes stay as is, and be able to run on implementations from different vendors.</p>", "body_text": "@hawkinsp Again I don't think a new class of device for AMD GPU is a viable choice, due to how XLA is coupled with GPU common runtime. And how GPU-supported operators are currently registered.\nIn the implementation of xla_op_registry / xla_launch_op / mark_for_compilation_pass, DEVICE_GPU_XLA_JIT & DEVICE_XLA_GPU are tightly couple with DEVICE_GPU in GPU common runtime. And operator implementations in tensorflow/core/kernels or contrib also register themselves be executed on DEVICE_GPU.\nShall a new class of device for AMD GPU is introduced, all these places would have to be altered. We would have duplicated codes to simply register ops for the new device class. And due to the fact user codes could physically specify devices a op be executed, adding new device classes would surely mean such user codes have to be modified which is not feasible.\nFinally, there are other places in Python APIs where device_type of a device be checked. Adding a new device class could mean those places have to be amended as well. And again that defeats the purpose of letting user codes stay as is, and be able to run on implementations from different vendors.", "in_reply_to_id": 203368119}