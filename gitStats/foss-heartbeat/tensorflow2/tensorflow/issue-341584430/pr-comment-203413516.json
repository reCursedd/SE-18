{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203413516", "pull_request_review_id": 138297027, "id": 203413516, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzQxMzUxNg==", "diff_hunk": "@@ -46,7 +47,7 @@ Status XlaGpuDeviceFactory::CreateDevices(const SessionOptions& options,\n \n   std::unique_ptr<XlaDevice> device;\n   Status status =\n-      XlaDevice::Create(\"CUDA\", DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,\n+      XlaDevice::Create(GPU_PLATFORM_NAME, DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,", "path": "tensorflow/compiler/jit/xla_gpu_device.cc", "position": null, "original_position": 13, "commit_id": "c8ae8965ea75a28f30d80ed7657fd9f31b18b48d", "original_commit_id": "9af995ecf3ba1815a0a9cf6e0b4ca372f1ff0b9f", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "FWIW I agree that it would be great for users to be able to have AMD and nvidia GPU support in a single binary, and in contrast pretty bad if we decided now that forevermore you'd never be able to build a single binary with support for both.\r\n\r\nAn example of where not being able to \"support both\" has bitten us in the past is with CPU optimizations.  C++ makes it very hard to compile one file for two ISA extensions (say, SSE2 and AVX), so by and large we don't do it.  This means that anyone who downloads the tensorflow package gets...something, but not what's optimal.\r\n\r\nOne could say, well, we should have tensorflow-sse, tensorflow-sse2, tensorflow-sse3, tensorflow-ssse3, tensorflow-avx, tensorflow-avx2, tensorflow-avx512, ...  But it should be clear how quickly that gets out of hand.  Plus we're adding another dimension with the gpu bit.\r\n\r\n> Shall a new class of device for AMD GPU is introduced, all these places would have to be altered. We would have duplicated codes to simply register ops for the new device class. \r\n\r\nThey're macros anyway, this doesn't seem like a big deal to change?\r\n\r\n> And due to the fact user codes could physically specify devices a op be executed, adding new device classes would surely mean such user codes have to be modified which is not feasible.\r\n\r\nThis is the bigger issue from my perspective.  But it's not clear to me that it's impossible to come up with a way to allow user code to be unaffected while still accomplishing what hawkinsp is trying to do.", "created_at": "2018-07-18T14:59:50Z", "updated_at": "2018-08-29T21:18:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203413516", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203413516"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203413516"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845"}}, "body_html": "<p>FWIW I agree that it would be great for users to be able to have AMD and nvidia GPU support in a single binary, and in contrast pretty bad if we decided now that forevermore you'd never be able to build a single binary with support for both.</p>\n<p>An example of where not being able to \"support both\" has bitten us in the past is with CPU optimizations.  C++ makes it very hard to compile one file for two ISA extensions (say, SSE2 and AVX), so by and large we don't do it.  This means that anyone who downloads the tensorflow package gets...something, but not what's optimal.</p>\n<p>One could say, well, we should have tensorflow-sse, tensorflow-sse2, tensorflow-sse3, tensorflow-ssse3, tensorflow-avx, tensorflow-avx2, tensorflow-avx512, ...  But it should be clear how quickly that gets out of hand.  Plus we're adding another dimension with the gpu bit.</p>\n<blockquote>\n<p>Shall a new class of device for AMD GPU is introduced, all these places would have to be altered. We would have duplicated codes to simply register ops for the new device class.</p>\n</blockquote>\n<p>They're macros anyway, this doesn't seem like a big deal to change?</p>\n<blockquote>\n<p>And due to the fact user codes could physically specify devices a op be executed, adding new device classes would surely mean such user codes have to be modified which is not feasible.</p>\n</blockquote>\n<p>This is the bigger issue from my perspective.  But it's not clear to me that it's impossible to come up with a way to allow user code to be unaffected while still accomplishing what hawkinsp is trying to do.</p>", "body_text": "FWIW I agree that it would be great for users to be able to have AMD and nvidia GPU support in a single binary, and in contrast pretty bad if we decided now that forevermore you'd never be able to build a single binary with support for both.\nAn example of where not being able to \"support both\" has bitten us in the past is with CPU optimizations.  C++ makes it very hard to compile one file for two ISA extensions (say, SSE2 and AVX), so by and large we don't do it.  This means that anyone who downloads the tensorflow package gets...something, but not what's optimal.\nOne could say, well, we should have tensorflow-sse, tensorflow-sse2, tensorflow-sse3, tensorflow-ssse3, tensorflow-avx, tensorflow-avx2, tensorflow-avx512, ...  But it should be clear how quickly that gets out of hand.  Plus we're adding another dimension with the gpu bit.\n\nShall a new class of device for AMD GPU is introduced, all these places would have to be altered. We would have duplicated codes to simply register ops for the new device class.\n\nThey're macros anyway, this doesn't seem like a big deal to change?\n\nAnd due to the fact user codes could physically specify devices a op be executed, adding new device classes would surely mean such user codes have to be modified which is not feasible.\n\nThis is the bigger issue from my perspective.  But it's not clear to me that it's impossible to come up with a way to allow user code to be unaffected while still accomplishing what hawkinsp is trying to do.", "in_reply_to_id": 203368119}