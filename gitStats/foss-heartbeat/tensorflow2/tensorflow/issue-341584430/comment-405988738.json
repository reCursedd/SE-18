{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/405988738", "html_url": "https://github.com/tensorflow/tensorflow/pull/20845#issuecomment-405988738", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20845", "id": 405988738, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTk4ODczOA==", "user": {"login": "gstoner", "id": 4129721, "node_id": "MDQ6VXNlcjQxMjk3MjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/4129721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gstoner", "html_url": "https://github.com/gstoner", "followers_url": "https://api.github.com/users/gstoner/followers", "following_url": "https://api.github.com/users/gstoner/following{/other_user}", "gists_url": "https://api.github.com/users/gstoner/gists{/gist_id}", "starred_url": "https://api.github.com/users/gstoner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gstoner/subscriptions", "organizations_url": "https://api.github.com/users/gstoner/orgs", "repos_url": "https://api.github.com/users/gstoner/repos", "events_url": "https://api.github.com/users/gstoner/events{/privacy}", "received_events_url": "https://api.github.com/users/gstoner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-18T16:14:46Z", "updated_at": "2018-07-18T16:14:46Z", "author_association": "NONE", "body_html": "<p>You have larger dependency beyond the code object for the kernel, but also call accelerator libraries which affect host code.    For you to understand how our compiler is different from NVIDIA.</p>\n<p>BLAS cuBLAS  for NVIDA and rocBLAS for AMD<br>\nParallel Primitives CUB for NVIDIA, rocPRIM for AMD<br>\nDeep learning primatives cuDNN. for NVIDIA, MIOpen for AMD,</p>\n<p>Also, we do not generate IL Binary  like NVIDIA aka PTX.  We generate native Binary  obj. Here is a link to the AMDPGU compiler overview.   <a href=\"https://llvm.org/docs/AMDGPUUsage.html\" rel=\"nofollow\">https://llvm.org/docs/AMDGPUUsage.html</a></p>", "body_text": "You have larger dependency beyond the code object for the kernel, but also call accelerator libraries which affect host code.    For you to understand how our compiler is different from NVIDIA.\nBLAS cuBLAS  for NVIDA and rocBLAS for AMD\nParallel Primitives CUB for NVIDIA, rocPRIM for AMD\nDeep learning primatives cuDNN. for NVIDIA, MIOpen for AMD,\nAlso, we do not generate IL Binary  like NVIDIA aka PTX.  We generate native Binary  obj. Here is a link to the AMDPGU compiler overview.   https://llvm.org/docs/AMDGPUUsage.html", "body": "You have larger dependency beyond the code object for the kernel, but also call accelerator libraries which affect host code.    For you to understand how our compiler is different from NVIDIA. \r\n\r\nBLAS cuBLAS  for NVIDA and rocBLAS for AMD \r\nParallel Primitives CUB for NVIDIA, rocPRIM for AMD  \r\nDeep learning primatives cuDNN. for NVIDIA, MIOpen for AMD,  \r\n\r\nAlso, we do not generate IL Binary  like NVIDIA aka PTX.  We generate native Binary  obj. Here is a link to the AMDPGU compiler overview.   https://llvm.org/docs/AMDGPUUsage.html"}