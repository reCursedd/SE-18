{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203452875", "pull_request_review_id": 138345089, "id": 203452875, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzQ1Mjg3NQ==", "diff_hunk": "@@ -46,7 +47,7 @@ Status XlaGpuDeviceFactory::CreateDevices(const SessionOptions& options,\n \n   std::unique_ptr<XlaDevice> device;\n   Status status =\n-      XlaDevice::Create(\"CUDA\", DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,\n+      XlaDevice::Create(GPU_PLATFORM_NAME, DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,", "path": "tensorflow/compiler/jit/xla_gpu_device.cc", "position": null, "original_position": 13, "commit_id": "c8ae8965ea75a28f30d80ed7657fd9f31b18b48d", "original_commit_id": "9af995ecf3ba1815a0a9cf6e0b4ca372f1ff0b9f", "user": {"login": "whchung", "id": 1673574, "node_id": "MDQ6VXNlcjE2NzM1NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1673574?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whchung", "html_url": "https://github.com/whchung", "followers_url": "https://api.github.com/users/whchung/followers", "following_url": "https://api.github.com/users/whchung/following{/other_user}", "gists_url": "https://api.github.com/users/whchung/gists{/gist_id}", "starred_url": "https://api.github.com/users/whchung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whchung/subscriptions", "organizations_url": "https://api.github.com/users/whchung/orgs", "repos_url": "https://api.github.com/users/whchung/repos", "events_url": "https://api.github.com/users/whchung/events{/privacy}", "received_events_url": "https://api.github.com/users/whchung/received_events", "type": "User", "site_admin": false}, "body": "@hawkinsp Thanks for all the comments. Based on the discussion I'll revise the PR so:\r\n\r\n- in XLA, a new class of device for AMD GPU be introduced: `XLA_AMDGPU`\r\n- in XLA, no conditioning based on `GOOGLE_CUDA` would be made\r\n\r\nThis allows XLA to possibly support emitting GPU kernels for both NVPTX and AMDGPU targets.\r\n\r\n\r\n- in GPU common runtime, keep \"GPU\" be either CUDA or ROCm via the mutually exclusive configure-time option\r\n\r\nChanges to `gpu_init` would likely be removed from this PR and be filed as a separate one. In fact I've also got some other PRs to be filed to decouple CUDA (ex: `CudaGpuId`) from GPU common runtime.", "created_at": "2018-07-18T16:50:44Z", "updated_at": "2018-08-29T21:18:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203452875", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203452875"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203452875"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=348932\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hawkinsp\">@hawkinsp</a> Thanks for all the comments. Based on the discussion I'll revise the PR so:</p>\n<ul>\n<li>in XLA, a new class of device for AMD GPU be introduced: <code>XLA_AMDGPU</code></li>\n<li>in XLA, no conditioning based on <code>GOOGLE_CUDA</code> would be made</li>\n</ul>\n<p>This allows XLA to possibly support emitting GPU kernels for both NVPTX and AMDGPU targets.</p>\n<ul>\n<li>in GPU common runtime, keep \"GPU\" be either CUDA or ROCm via the mutually exclusive configure-time option</li>\n</ul>\n<p>Changes to <code>gpu_init</code> would likely be removed from this PR and be filed as a separate one. In fact I've also got some other PRs to be filed to decouple CUDA (ex: <code>CudaGpuId</code>) from GPU common runtime.</p>", "body_text": "@hawkinsp Thanks for all the comments. Based on the discussion I'll revise the PR so:\n\nin XLA, a new class of device for AMD GPU be introduced: XLA_AMDGPU\nin XLA, no conditioning based on GOOGLE_CUDA would be made\n\nThis allows XLA to possibly support emitting GPU kernels for both NVPTX and AMDGPU targets.\n\nin GPU common runtime, keep \"GPU\" be either CUDA or ROCm via the mutually exclusive configure-time option\n\nChanges to gpu_init would likely be removed from this PR and be filed as a separate one. In fact I've also got some other PRs to be filed to decouple CUDA (ex: CudaGpuId) from GPU common runtime.", "in_reply_to_id": 203368119}