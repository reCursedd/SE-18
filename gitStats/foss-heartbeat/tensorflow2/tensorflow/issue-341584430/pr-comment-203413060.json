{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203413060", "pull_request_review_id": 138296509, "id": 203413060, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzQxMzA2MA==", "diff_hunk": "@@ -46,7 +47,7 @@ Status XlaGpuDeviceFactory::CreateDevices(const SessionOptions& options,\n \n   std::unique_ptr<XlaDevice> device;\n   Status status =\n-      XlaDevice::Create(\"CUDA\", DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,\n+      XlaDevice::Create(GPU_PLATFORM_NAME, DEVICE_XLA_GPU, 0, DEVICE_GPU_XLA_JIT, options,", "path": "tensorflow/compiler/jit/xla_gpu_device.cc", "position": null, "original_position": 13, "commit_id": "c8ae8965ea75a28f30d80ed7657fd9f31b18b48d", "original_commit_id": "9af995ecf3ba1815a0a9cf6e0b4ca372f1ff0b9f", "user": {"login": "hawkinsp", "id": 348932, "node_id": "MDQ6VXNlcjM0ODkzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/348932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hawkinsp", "html_url": "https://github.com/hawkinsp", "followers_url": "https://api.github.com/users/hawkinsp/followers", "following_url": "https://api.github.com/users/hawkinsp/following{/other_user}", "gists_url": "https://api.github.com/users/hawkinsp/gists{/gist_id}", "starred_url": "https://api.github.com/users/hawkinsp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hawkinsp/subscriptions", "organizations_url": "https://api.github.com/users/hawkinsp/orgs", "repos_url": "https://api.github.com/users/hawkinsp/repos", "events_url": "https://api.github.com/users/hawkinsp/events{/privacy}", "received_events_url": "https://api.github.com/users/hawkinsp/received_events", "type": "User", "site_admin": false}, "body": "I think it's important to distinguish between \"GPU\" and \"XLA_GPU\". They are to some extent completely separate issues because they share almost nothing (although both can call XLA). Let's discuss the \"GPU\" case in the other comment and resolve that first.\r\n\r\nFor the XLA_GPU case, I am confident that separating the two is feasible. The coupling between XLA_GPU_JIT and XLA_GPU is limited to a small number of registrations that say how they connect. It seems quite feasible to add another backend. As I observed above, this device is really only used by tests anyway, so compatibility isn't necessary.\r\n\r\nAnother concern here is that GOOGLE_CUDA is not necessarily defined when using XLA_GPU \u2014 XLA does not require a CUDA compiler to generate GPU code. The opensource configure script conflates \"targeting an NVidia GPU\" with \"having a CUDA compiler\", but they are not actually linked when using XLA and we depend on this behavior for some of our internal XLA testing. So I prefer that we separate XLA_GPU from XLA_AMDGPU, irrespective of what we choose to do for the TF core GPU device.", "created_at": "2018-07-18T14:58:52Z", "updated_at": "2018-08-29T21:18:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203413060", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/203413060"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20845#discussion_r203413060"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20845"}}, "body_html": "<p>I think it's important to distinguish between \"GPU\" and \"XLA_GPU\". They are to some extent completely separate issues because they share almost nothing (although both can call XLA). Let's discuss the \"GPU\" case in the other comment and resolve that first.</p>\n<p>For the XLA_GPU case, I am confident that separating the two is feasible. The coupling between XLA_GPU_JIT and XLA_GPU is limited to a small number of registrations that say how they connect. It seems quite feasible to add another backend. As I observed above, this device is really only used by tests anyway, so compatibility isn't necessary.</p>\n<p>Another concern here is that GOOGLE_CUDA is not necessarily defined when using XLA_GPU \u2014 XLA does not require a CUDA compiler to generate GPU code. The opensource configure script conflates \"targeting an NVidia GPU\" with \"having a CUDA compiler\", but they are not actually linked when using XLA and we depend on this behavior for some of our internal XLA testing. So I prefer that we separate XLA_GPU from XLA_AMDGPU, irrespective of what we choose to do for the TF core GPU device.</p>", "body_text": "I think it's important to distinguish between \"GPU\" and \"XLA_GPU\". They are to some extent completely separate issues because they share almost nothing (although both can call XLA). Let's discuss the \"GPU\" case in the other comment and resolve that first.\nFor the XLA_GPU case, I am confident that separating the two is feasible. The coupling between XLA_GPU_JIT and XLA_GPU is limited to a small number of registrations that say how they connect. It seems quite feasible to add another backend. As I observed above, this device is really only used by tests anyway, so compatibility isn't necessary.\nAnother concern here is that GOOGLE_CUDA is not necessarily defined when using XLA_GPU \u2014 XLA does not require a CUDA compiler to generate GPU code. The opensource configure script conflates \"targeting an NVidia GPU\" with \"having a CUDA compiler\", but they are not actually linked when using XLA and we depend on this behavior for some of our internal XLA testing. So I prefer that we separate XLA_GPU from XLA_AMDGPU, irrespective of what we choose to do for the TF core GPU device.", "in_reply_to_id": 203368119}