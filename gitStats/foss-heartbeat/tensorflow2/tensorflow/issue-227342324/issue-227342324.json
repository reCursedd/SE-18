{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9790", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9790/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9790/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9790/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9790", "id": 227342324, "node_id": "MDU6SXNzdWUyMjczNDIzMjQ=", "number": 9790, "title": "ResourceExhaustedError", "user": {"login": "StefanoD", "id": 1634862, "node_id": "MDQ6VXNlcjE2MzQ4NjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1634862?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StefanoD", "html_url": "https://github.com/StefanoD", "followers_url": "https://api.github.com/users/StefanoD/followers", "following_url": "https://api.github.com/users/StefanoD/following{/other_user}", "gists_url": "https://api.github.com/users/StefanoD/gists{/gist_id}", "starred_url": "https://api.github.com/users/StefanoD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StefanoD/subscriptions", "organizations_url": "https://api.github.com/users/StefanoD/orgs", "repos_url": "https://api.github.com/users/StefanoD/repos", "events_url": "https://api.github.com/users/StefanoD/events{/privacy}", "received_events_url": "https://api.github.com/users/StefanoD/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-09T12:01:51Z", "updated_at": "2017-12-28T13:29:58Z", "closed_at": "2017-05-10T08:54:37Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04.2</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.1.0</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: 8/6</li>\n<li><strong>GPU model and memory</strong>: NVIDIA GTX 1080 TI, 11 GB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Disable linter warnings to maintain consistency with tutorial.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> pylint: disable=invalid-name</span>\n\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">from</span> collections <span class=\"pl-k\">import</span> namedtuple\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.slim <span class=\"pl-k\">as</span> slim\n\n<span class=\"pl-k\">from</span> dataset <span class=\"pl-k\">import</span> DataSet\n\n<span class=\"pl-c1\">FLAGS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n<span class=\"pl-c1\">HEIGHT</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">276</span>\n<span class=\"pl-c1\">WIDTH</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">72</span>\n<span class=\"pl-c1\">DEPTH</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\n<span class=\"pl-c1\">INPUT_DIMENSION</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">HEIGHT</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">WIDTH</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">DEPTH</span>\n<span class=\"pl-c1\">NUMBER_CLASSES</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DataSet</span>(<span class=\"pl-c1\">object</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,\n                 <span class=\"pl-smi\">images</span>,\n                 <span class=\"pl-smi\">labels</span>,\n                 <span class=\"pl-smi\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n                 <span class=\"pl-smi\">reshape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        dtype <span class=\"pl-k\">=</span> tf.as_dtype(dtype).base_dtype\n        <span class=\"pl-k\">if</span> dtype <span class=\"pl-k\">not</span> <span class=\"pl-k\">in</span> (tf.uint8, tf.float32):\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">TypeError</span>(\n                <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Invalid image dtype <span class=\"pl-c1\">%r</span>, expected uint8 or float32<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> dtype)\n\n        <span class=\"pl-k\">assert</span> images.shape[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">==</span> labels.shape[<span class=\"pl-c1\">0</span>], (\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>images.shape: <span class=\"pl-c1\">%s</span> labels.shape: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (images.shape, labels.shape))\n        <span class=\"pl-c1\">self</span>._num_examples <span class=\"pl-k\">=</span> images.shape[<span class=\"pl-c1\">0</span>]\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Convert shape from [num examples, rows, columns, depth]</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> to [num examples, rows*columns] (assuming depth == 1)</span>\n        <span class=\"pl-k\">if</span> reshape:\n            <span class=\"pl-k\">assert</span> images.shape[<span class=\"pl-c1\">3</span>] <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>\n            images <span class=\"pl-k\">=</span> images.reshape(images.shape[<span class=\"pl-c1\">0</span>],\n                                    images.shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> images.shape[<span class=\"pl-c1\">2</span>])\n\n        <span class=\"pl-c1\">self</span>._images <span class=\"pl-k\">=</span> images\n        <span class=\"pl-c1\">self</span>._labels <span class=\"pl-k\">=</span> labels\n        <span class=\"pl-c1\">self</span>._epochs_completed <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-c1\">self</span>._index_in_epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">images</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._images\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">labels</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._labels\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">num_examples</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_examples\n\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">epochs_completed</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._epochs_completed\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">next_batch</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Return the next `batch_size` examples from this data set.<span class=\"pl-pds\">\"\"\"</span></span>\n        start <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._index_in_epoch\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Shuffle for the first epoch</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._epochs_completed <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> start <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> shuffle:\n            perm0 <span class=\"pl-k\">=</span> np.arange(<span class=\"pl-c1\">self</span>._num_examples)\n            np.random.shuffle(perm0)\n            <span class=\"pl-c1\">self</span>._images <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.images[perm0]\n            <span class=\"pl-c1\">self</span>._labels <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.labels[perm0]\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Go to the next epoch</span>\n        <span class=\"pl-k\">if</span> start <span class=\"pl-k\">+</span> batch_size <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">self</span>._num_examples:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Finished epoch</span>\n            <span class=\"pl-c1\">self</span>._epochs_completed <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get the rest examples in this epoch</span>\n            rest_num_examples <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._num_examples <span class=\"pl-k\">-</span> start\n            images_rest_part <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._images[start:<span class=\"pl-c1\">self</span>._num_examples]\n            labels_rest_part <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._labels[start:<span class=\"pl-c1\">self</span>._num_examples]\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Shuffle the data</span>\n            <span class=\"pl-k\">if</span> shuffle:\n                perm <span class=\"pl-k\">=</span> np.arange(<span class=\"pl-c1\">self</span>._num_examples)\n                np.random.shuffle(perm)\n                <span class=\"pl-c1\">self</span>._images <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.images[perm]\n                <span class=\"pl-c1\">self</span>._labels <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.labels[perm]\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Start next epoch</span>\n            start <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n            <span class=\"pl-c1\">self</span>._index_in_epoch <span class=\"pl-k\">=</span> batch_size <span class=\"pl-k\">-</span> rest_num_examples\n            end <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._index_in_epoch\n            images_new_part <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._images[start:end]\n            labels_new_part <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._labels[start:end]\n            <span class=\"pl-k\">return</span> np.concatenate((images_rest_part, images_new_part), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>),\\\n                np.concatenate((labels_rest_part, labels_new_part), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">self</span>._index_in_epoch <span class=\"pl-k\">+=</span> batch_size\n            end <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._index_in_epoch\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._images[start:end], <span class=\"pl-c1\">self</span>._labels[start:end]\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">deepnn</span>(<span class=\"pl-smi\">x</span>):\n    x_image <span class=\"pl-k\">=</span> tf.reshape(x, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">DEPTH</span>, <span class=\"pl-c1\">HEIGHT</span>, <span class=\"pl-c1\">WIDTH</span>])\n    is_training <span class=\"pl-k\">=</span> tf.placeholder(tf.bool)\n\n    <span class=\"pl-k\">with</span> slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>NCHW<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>):\n        <span class=\"pl-k\">with</span> slim.arg_scope([slim.conv2d, slim.fully_connected],\n                            <span class=\"pl-v\">weights_initializer</span><span class=\"pl-k\">=</span>tf.truncated_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n                            <span class=\"pl-v\">biases_initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.1</span>)):\n            <span class=\"pl-k\">with</span> slim.arg_scope([slim.dropout],\n                                <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training):\n                net <span class=\"pl-k\">=</span> slim.conv2d(x_image, <span class=\"pl-c1\">64</span>, [<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1<span class=\"pl-pds\">'</span></span>)\n                net <span class=\"pl-k\">=</span> slim.max_pool2d(net, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool1<span class=\"pl-pds\">'</span></span>)\n                net <span class=\"pl-k\">=</span> slim.conv2d(x_image, <span class=\"pl-c1\">64</span>, [<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2<span class=\"pl-pds\">'</span></span>)\n                net <span class=\"pl-k\">=</span> slim.max_pool2d(net, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool2<span class=\"pl-pds\">'</span></span>)\n                net <span class=\"pl-k\">=</span> slim.flatten(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Flatten<span class=\"pl-pds\">\"</span></span>)\n                net <span class=\"pl-k\">=</span> slim.fully_connected(net, <span class=\"pl-c1\">384</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc_1<span class=\"pl-pds\">'</span></span>,\n                                           <span class=\"pl-v\">weights_regularizer</span><span class=\"pl-k\">=</span>slim.l2_regularizer(<span class=\"pl-c1\">0.000005</span>))\n                net <span class=\"pl-k\">=</span> slim.fully_connected(net, <span class=\"pl-c1\">192</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc_2<span class=\"pl-pds\">'</span></span>,\n                                           <span class=\"pl-v\">weights_regularizer</span><span class=\"pl-k\">=</span>slim.l2_regularizer(<span class=\"pl-c1\">0.000005</span>))\n                net <span class=\"pl-k\">=</span> slim.fully_connected(net, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">activation_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc_out<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-k\">return</span> net, is_training\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">import_images_and_labels</span>():\n    file_path <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/path/to/file/samples.npz<span class=\"pl-pds\">\"</span></span>\n    data <span class=\"pl-k\">=</span> np.load(file_path)\n\n    <span class=\"pl-c1\">print</span>(data[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>one_hot_labels<span class=\"pl-pds\">'</span></span>].shape)\n    <span class=\"pl-c1\">print</span>(data[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>images<span class=\"pl-pds\">'</span></span>].shape)\n\n    images <span class=\"pl-k\">=</span> data[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>images<span class=\"pl-pds\">'</span></span>].astype(np.float32)\n    labels <span class=\"pl-k\">=</span> data[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>one_hot_labels<span class=\"pl-pds\">'</span></span>].astype(np.float32)\n\n    number_training_samples <span class=\"pl-k\">=</span> <span class=\"pl-c1\">17950</span>\n    train_data <span class=\"pl-k\">=</span> DataSet(<span class=\"pl-v\">images</span><span class=\"pl-k\">=</span>images[:number_training_samples, :],\n                         <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels[:number_training_samples, :])\n    validation_data <span class=\"pl-k\">=</span> DataSet(<span class=\"pl-v\">images</span><span class=\"pl-k\">=</span>images[<span class=\"pl-c1\">16000</span>:<span class=\"pl-c1\">17950</span>, :], <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels[<span class=\"pl-c1\">16000</span>:<span class=\"pl-c1\">17950</span>, :])\n    test_data <span class=\"pl-k\">=</span> DataSet(<span class=\"pl-v\">images</span><span class=\"pl-k\">=</span>images[<span class=\"pl-c1\">17950</span>:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, :], <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels[<span class=\"pl-c1\">17950</span>: <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, :])\n\n    DataSets <span class=\"pl-k\">=</span> namedtuple(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>DataSets<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>validation<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test<span class=\"pl-pds\">'</span></span>])\n\n    <span class=\"pl-k\">return</span> DataSets(<span class=\"pl-v\">train</span><span class=\"pl-k\">=</span>train_data,\n                    <span class=\"pl-v\">validation</span><span class=\"pl-k\">=</span>validation_data,\n                    <span class=\"pl-v\">test</span><span class=\"pl-k\">=</span>test_data)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">_</span>):\n    dataset <span class=\"pl-k\">=</span> import_images_and_labels()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the model</span>\n    x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">INPUT_DIMENSION</span>])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define loss and optimizer</span>\n    y_ <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">NUMBER_CLASSES</span>])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Build the graph for the deep net</span>\n    y_conv, is_training <span class=\"pl-k\">=</span> deepnn(x)\n\n    cross_entropy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>y_,\n                                                                           <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>y_conv))\n    train_step <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-4</span>).minimize(cross_entropy)\n    correct_prediction <span class=\"pl-k\">=</span> tf.equal(tf.argmax(y_conv, <span class=\"pl-c1\">1</span>), tf.argmax(y_, <span class=\"pl-c1\">1</span>))\n    accuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    <span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">inter_op_parallelism_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)) <span class=\"pl-k\">as</span> sess:\n        sess.run(tf.global_variables_initializer())\n\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">20000</span>):\n            batch <span class=\"pl-k\">=</span> dataset.train.next_batch(<span class=\"pl-c1\">50</span>)\n\n            <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                train_accuracy <span class=\"pl-k\">=</span> accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch[<span class=\"pl-c1\">0</span>],\n                                                          y_: batch[<span class=\"pl-c1\">1</span>],\n                                                          is_training: <span class=\"pl-c1\">False</span>})\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>step <span class=\"pl-c1\">%d</span>, training accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (i, train_accuracy))\n\n            train_step.run(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch[<span class=\"pl-c1\">0</span>],\n                                      y_: batch[<span class=\"pl-c1\">1</span>],\n                                      is_training: <span class=\"pl-c1\">True</span>})\n\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: dataset.test.images,\n                                                            y_: dataset.test.labels,\n                                                            is_training: <span class=\"pl-c1\">False</span>}))\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--data_dir<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">str</span>,\n                        <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tmp/tensorflow/mnist/input_data<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Directory for storing input data<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">FLAGS</span>, unparsed <span class=\"pl-k\">=</span> parser.parse_known_args()\n    tf.app.run(<span class=\"pl-v\">main</span><span class=\"pl-k\">=</span>main, <span class=\"pl-v\">argv</span><span class=\"pl-k\">=</span>[sys.argv[<span class=\"pl-c1\">0</span>]] <span class=\"pl-k\">+</span> unparsed)\n</pre></div>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<pre><code>\n== cat /etc/issue ===============================================\nLinux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.11.0)\nprotobuf (3.2.0)\ntensorflow (1.1.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nImportError: No module named tensorflow\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nTue May  9 13:52:52 2017       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Graphics Device     Off  | 0000:01:00.0      On |                  N/A |\n| 26%   46C    P8    18W / 250W |    378MiB / 11171MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1514    G   /usr/lib/xorg/Xorg                             190MiB |\n|    0      1904    G   kwin_x11                                        41MiB |\n|    0      1910    G   /usr/bin/krunner                                 2MiB |\n|    0      1915    G   /usr/bin/plasmashell                            94MiB |\n|    0      2047    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    46MiB |\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n\n</code></pre>\n<h3>Describe the problem</h3>\n<p>When loading the test images in order to determine the accuracy, an ResourceExhaustedError is thrown.<br>\nThis network model works fine with a private theano based framework.</p>\n<h3>Source code / logs</h3>\n<pre><code>...\n...\nstep 19800, training accuracy 1\nstep 19900, training accuracy 1\n2017-05-09 13:13:15.970193: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc)\nran out of memory trying to allocate 9.21GiB.  Current allocation summary follows.\n2017-05-09 13:13:15.970227: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):   Total Chu\nnks: 1, Chunks in use: 0 256B allocated for chunks. 4B client-requested for chunks. 0B in use in bin. 0B\nclient-requested in use in bin.\n2017-05-09 13:13:15.970239: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):   Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970247: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):  Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970257: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):  Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970266: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):  Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970275: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):  Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970285: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):         T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970295: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):         T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970304: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):         T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970313: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):        T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970325: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):        T\notal Chunks: 1, Chunks in use: 0 288.0KiB allocated for chunks. 18.8KiB client-requested for chunks. 0B i\nn use in bin. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970334: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):        T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970342: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):       T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970350: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):       T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970357: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):       T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970365: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):       T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970374: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):      T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970383: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):      T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970392: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):      T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970401: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):     T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970413: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):     T\notal Chunks: 1, Chunks in use: 0 7.61GiB allocated for chunks. 242.58MiB client-requested for chunks. 0B\nin use in bin. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970423: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 9.21GiB was 25\n6.00MiB, Chunk State:\n2017-05-09 13:13:15.970436: I tensorflow/core/common_runtime/bfc_allocator.cc:666]   Size: 7.61GiB | Requ\nested Size: 242.58MiB | in_use: 0, prev:   Size: 465.75MiB | Requested Size: 465.75MiB | in_use: 1\n2017-05-09 13:13:15.970444: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600000\n of size 1280\n2017-05-09 13:13:15.970450: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600500\n of size 256\n2017-05-09 13:13:15.970456: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600600\n of size 256\n2017-05-09 13:13:15.970463: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600700\n of size 256\n2017-05-09 13:13:15.970470: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600800\n of size 256\n2017-05-09 13:13:15.970476: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600900\n of size 256\n2017-05-09 13:13:15.970483: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600a00\n of size 256\n2017-05-09 13:13:15.970490: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600b00\n of size 256\n2017-05-09 13:13:15.970496: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600c00\n of size 256\n2017-05-09 13:13:15.970504: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600d00\n of size 1536\n2017-05-09 13:13:15.970510: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601300\n of size 256\n2017-05-09 13:13:15.970517: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601400\n of size 256\n2017-05-09 13:13:15.970524: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601500\n2017-05-09 13:13:15.970531: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601800\n of size 256\n2017-05-09 13:13:15.970538: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601900\n of size 256\n2017-05-09 13:13:15.970544: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601a00\n of size 256\n2017-05-09 13:13:15.970551: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601b00\n of size 256\n2017-05-09 13:13:15.970557: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601c00\n of size 256\n2017-05-09 13:13:15.970565: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601d00\n of size 19200\n2017-05-09 13:13:15.970571: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606800\n of size 256\n2017-05-09 13:13:15.970578: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606900\n of size 488374272\n2017-05-09 13:13:15.970585: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6900\n of size 1536\n2017-05-09 13:13:15.970592: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6f00\n of size 294912\n2017-05-09 13:13:15.970599: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80ef00\n of size 768\n2017-05-09 13:13:15.970606: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f200\n of size 1536\n2017-05-09 13:13:15.970612: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f800\n of size 256\n2017-05-09 13:13:15.970619: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f900\n of size 19200\n2017-05-09 13:13:15.970625: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814400\n of size 256\n2017-05-09 13:13:15.970632: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814500\n of size 1536\n2017-05-09 13:13:15.970639: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814b00\n of size 17664\n2017-05-09 13:13:15.970646: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819000\n of size 256\n2017-05-09 13:13:15.970653: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819100\n of size 294912\n2017-05-09 13:13:15.970659: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a861100\n of size 488079360\n2017-05-09 13:13:15.970666: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x102479d9100\n of size 1536\n2017-05-09 13:13:15.970673: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21700\n of size 768\n2017-05-09 13:13:15.970680: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21a00\n of size 256\n2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00\n of size 256\n2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00\n of size 256\n2017-05-09 13:13:15.970693: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21c00\n of size 256\n2017-05-09 13:13:15.970700: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21d00\n of size 256\n2017-05-09 13:13:15.970706: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21e00\n of size 256\n2017-05-09 13:13:15.970713: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22000\n of size 256\n2017-05-09 13:13:15.970720: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22100\n of size 256\n2017-05-09 13:13:15.970726: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22200\n of size 256\n2017-05-09 13:13:15.970733: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22300\n of size 19200\n2017-05-09 13:13:15.970740: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a26e00\n of size 19200\n2017-05-09 13:13:15.970747: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2b900\n of size 256\n2017-05-09 13:13:15.970753: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2ba00\n of size 256\n2017-05-09 13:13:15.970760: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2bb00\n of size 488374272\n2017-05-09 13:13:15.970767: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10264bebb00\n of size 488374272\n2017-05-09 13:13:15.970773: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dabb00\n of size 1536\n2017-05-09 13:13:15.970780: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac100\n of size 1536\n2017-05-09 13:13:15.970787: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac700\n of size 294912\n2017-05-09 13:13:15.970793: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281df4700\n of size 294912\n2017-05-09 13:13:15.970800: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3c700\n of size 768\n2017-05-09 13:13:15.970807: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3ca00\n of size 768\n2017-05-09 13:13:15.970814: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3cd00\n of size 1536\n2017-05-09 13:13:15.970820: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d300\n of size 1536\n2017-05-09 13:13:15.970827: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d900\n of size 256\n2017-05-09 13:13:15.970834: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3da00\n of size 256\n2017-05-09 13:13:15.970840: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3db00\n of size 19200\n2017-05-09 13:13:15.970847: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e42600\n of size 488374272\n2017-05-09 13:13:15.970855: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x102479d9700\nof size 294912\n2017-05-09 13:13:15.970861: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x10247a21f00\nof size 256\n2017-05-09 13:13:15.970868: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x1029f002600\nof size 8176048640\n2017-05-09 13:13:15.970875: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use\n Chunks by size:\n2017-05-09 13:13:15.970884: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 31 Chunks of size 256\ntotalling 7.8KiB\n2017-05-09 13:13:15.970892: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 768 t\notalling 3.8KiB\n2017-05-09 13:13:15.970899: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280\ntotalling 1.2KiB\n2017-05-09 13:13:15.970908: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 9 Chunks of size 1536\ntotalling 13.5KiB\n2017-05-09 13:13:15.970916: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 17664\n totalling 17.2KiB\n2017-05-09 13:13:15.970924: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 19200\n totalling 93.8KiB\n2017-05-09 13:13:15.970931: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 29491\n2 totalling 1.12MiB\n2017-05-09 13:13:15.970940: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 48807\n9360 totalling 465.47MiB\n2017-05-09 13:13:15.970947: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 48837\n4272 totalling 1.82GiB\n2017-05-09 13:13:15.970955: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use ch\nunks: 2.27GiB\n2017-05-09 13:13:15.970966: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\nLimit:                 10619240448\nInUse:                  2442896640\nMaxInUse:               2995339008\nNumAllocs:                  803473\nMaxAllocSize:            488374272\n2017-05-09 13:13:15.970984: W tensorflow/core/common_runtime/bfc_allocator.cc:277] **********************\n**____________________________________________________________________________\n2017-05-09 13:13:15.971003: W tensorflow/core/framework/op_kernel.cc:1152] Resource exhausted: OOM when a\nllocating tensor with shape[1945,64,276,72]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1039, in _do_ca\nll\n    return fn(*args)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1021, in _run_f\nn\n    status, run_metadata)\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in\nraise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194\n5,64,276,72]\n         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1\n, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2/weights/r\nead)]]\n         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/\ncpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge\n_20_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 145, in &lt;module&gt;\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 136, in main\n    is_training: False}))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 569, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3741, in _eval_u\nsing_default_session\n    return session.run(tensors, feed_dict)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 778, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 982, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1032, in _do_ru\nn\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1052, in _do_ca\nll\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194\n5,64,276,72]\n         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1\n, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2/weights/r\nead)]]\n         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/\ncpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge\n_20_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv2/convolution', defined at:\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 145, in &lt;module&gt;\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 110, in main\n    y_conv, is_training = deepnn(x)\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 56, in deepnn\n    net = slim.conv2d(x_image, 64, [5, 5], scope='conv2')\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", lin\ne 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 9\n18, in convolution\n    outputs = layer.apply(inputs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 290, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py\", line 156, in c\nall\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 661, in convolution\n    op=op)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 331, in with_space_\nto_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 653, in op\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 129, in _non_atrous\n_convolution\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 403, in conv2d\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 768,\nin apply_op\n    op_def=op_def)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_\nop\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init_\n_\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1945,64,276,72]\n         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1\n, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2/weights/r\nead)]]\n         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/\ncpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge\n_20_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n</code></pre>", "body_text": "System information\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.2\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.1.0\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: 8/6\nGPU model and memory: NVIDIA GTX 1080 TI, 11 GB\nExact command to reproduce:\n\n# Disable linter warnings to maintain consistency with tutorial.\n# pylint: disable=invalid-name\n\nimport argparse\nimport sys\nfrom collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom dataset import DataSet\n\nFLAGS = None\nHEIGHT = 276\nWIDTH = 72\nDEPTH = 3\nINPUT_DIMENSION = HEIGHT * WIDTH * DEPTH\nNUMBER_CLASSES = 2\n\nimport numpy as np\nimport tensorflow as tf\n\nclass DataSet(object):\n\n    def __init__(self,\n                 images,\n                 labels,\n                 dtype=tf.float32,\n                 reshape=False):\n        dtype = tf.as_dtype(dtype).base_dtype\n        if dtype not in (tf.uint8, tf.float32):\n            raise TypeError(\n                'Invalid image dtype %r, expected uint8 or float32' % dtype)\n\n        assert images.shape[0] == labels.shape[0], (\n            'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n        self._num_examples = images.shape[0]\n\n        # Convert shape from [num examples, rows, columns, depth]\n        # to [num examples, rows*columns] (assuming depth == 1)\n        if reshape:\n            assert images.shape[3] == 1\n            images = images.reshape(images.shape[0],\n                                    images.shape[1] * images.shape[2])\n\n        self._images = images\n        self._labels = labels\n        self._epochs_completed = 0\n        self._index_in_epoch = 0\n\n    @property\n    def images(self):\n        return self._images\n\n    @property\n    def labels(self):\n        return self._labels\n\n    @property\n    def num_examples(self):\n        return self._num_examples\n\n    @property\n    def epochs_completed(self):\n        return self._epochs_completed\n\n    def next_batch(self, batch_size, shuffle=True):\n        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n        start = self._index_in_epoch\n        # Shuffle for the first epoch\n        if self._epochs_completed == 0 and start == 0 and shuffle:\n            perm0 = np.arange(self._num_examples)\n            np.random.shuffle(perm0)\n            self._images = self.images[perm0]\n            self._labels = self.labels[perm0]\n        # Go to the next epoch\n        if start + batch_size > self._num_examples:\n            # Finished epoch\n            self._epochs_completed += 1\n            # Get the rest examples in this epoch\n            rest_num_examples = self._num_examples - start\n            images_rest_part = self._images[start:self._num_examples]\n            labels_rest_part = self._labels[start:self._num_examples]\n            # Shuffle the data\n            if shuffle:\n                perm = np.arange(self._num_examples)\n                np.random.shuffle(perm)\n                self._images = self.images[perm]\n                self._labels = self.labels[perm]\n            # Start next epoch\n            start = 0\n            self._index_in_epoch = batch_size - rest_num_examples\n            end = self._index_in_epoch\n            images_new_part = self._images[start:end]\n            labels_new_part = self._labels[start:end]\n            return np.concatenate((images_rest_part, images_new_part), axis=0),\\\n                np.concatenate((labels_rest_part, labels_new_part), axis=0)\n        else:\n            self._index_in_epoch += batch_size\n            end = self._index_in_epoch\n            return self._images[start:end], self._labels[start:end]\n\n\ndef deepnn(x):\n    x_image = tf.reshape(x, [-1, DEPTH, HEIGHT, WIDTH])\n    is_training = tf.placeholder(tf.bool)\n\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        data_format='NCHW', padding='SAME'):\n        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                            weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                            biases_initializer=tf.constant_initializer(0.1)):\n            with slim.arg_scope([slim.dropout],\n                                is_training=is_training):\n                net = slim.conv2d(x_image, 64, [5, 5], scope='conv1')\n                net = slim.max_pool2d(net, kernel_size=3, stride=2, scope='pool1')\n                net = slim.conv2d(x_image, 64, [5, 5], scope='conv2')\n                net = slim.max_pool2d(net, kernel_size=3, stride=2, scope='pool2')\n                net = slim.flatten(net, scope=\"Flatten\")\n                net = slim.fully_connected(net, 384, scope='fc_1',\n                                           weights_regularizer=slim.l2_regularizer(0.000005))\n                net = slim.fully_connected(net, 192, scope='fc_2',\n                                           weights_regularizer=slim.l2_regularizer(0.000005))\n                net = slim.fully_connected(net, 2, activation_fn=None, scope='fc_out')\n\n    return net, is_training\n\n\ndef import_images_and_labels():\n    file_path = \"/path/to/file/samples.npz\"\n    data = np.load(file_path)\n\n    print(data['one_hot_labels'].shape)\n    print(data['images'].shape)\n\n    images = data['images'].astype(np.float32)\n    labels = data['one_hot_labels'].astype(np.float32)\n\n    number_training_samples = 17950\n    train_data = DataSet(images=images[:number_training_samples, :],\n                         labels=labels[:number_training_samples, :])\n    validation_data = DataSet(images=images[16000:17950, :], labels=labels[16000:17950, :])\n    test_data = DataSet(images=images[17950:-1, :], labels=labels[17950: -1, :])\n\n    DataSets = namedtuple('DataSets', ['train', 'validation', 'test'])\n\n    return DataSets(train=train_data,\n                    validation=validation_data,\n                    test=test_data)\n\n\ndef main(_):\n    dataset = import_images_and_labels()\n\n    # Create the model\n    x = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])\n\n    # Define loss and optimizer\n    y_ = tf.placeholder(tf.float32, [None, NUMBER_CLASSES])\n\n    # Build the graph for the deep net\n    y_conv, is_training = deepnn(x)\n\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,\n                                                                           logits=y_conv))\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    with tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=1)) as sess:\n        sess.run(tf.global_variables_initializer())\n\n        for i in range(20000):\n            batch = dataset.train.next_batch(50)\n\n            if i % 100 == 0:\n                train_accuracy = accuracy.eval(feed_dict={x: batch[0],\n                                                          y_: batch[1],\n                                                          is_training: False})\n                print('step %d, training accuracy %g' % (i, train_accuracy))\n\n            train_step.run(feed_dict={x: batch[0],\n                                      y_: batch[1],\n                                      is_training: True})\n\n        print('test accuracy %g' % accuracy.eval(feed_dict={x: dataset.test.images,\n                                                            y_: dataset.test.labels,\n                                                            is_training: False}))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', type=str,\n                        default='/tmp/tensorflow/mnist/input_data',\n                        help='Directory for storing input data')\n    FLAGS, unparsed = parser.parse_known_args()\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\n\n== cat /etc/issue ===============================================\nLinux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.11.0)\nprotobuf (3.2.0)\ntensorflow (1.1.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nImportError: No module named tensorflow\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nTue May  9 13:52:52 2017       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Graphics Device     Off  | 0000:01:00.0      On |                  N/A |\n| 26%   46C    P8    18W / 250W |    378MiB / 11171MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1514    G   /usr/lib/xorg/Xorg                             190MiB |\n|    0      1904    G   kwin_x11                                        41MiB |\n|    0      1910    G   /usr/bin/krunner                                 2MiB |\n|    0      1915    G   /usr/bin/plasmashell                            94MiB |\n|    0      2047    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    46MiB |\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n\n\nDescribe the problem\nWhen loading the test images in order to determine the accuracy, an ResourceExhaustedError is thrown.\nThis network model works fine with a private theano based framework.\nSource code / logs\n...\n...\nstep 19800, training accuracy 1\nstep 19900, training accuracy 1\n2017-05-09 13:13:15.970193: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc)\nran out of memory trying to allocate 9.21GiB.  Current allocation summary follows.\n2017-05-09 13:13:15.970227: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):   Total Chu\nnks: 1, Chunks in use: 0 256B allocated for chunks. 4B client-requested for chunks. 0B in use in bin. 0B\nclient-requested in use in bin.\n2017-05-09 13:13:15.970239: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):   Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970247: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):  Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970257: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):  Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970266: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):  Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970275: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):  Total Chu\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\nient-requested in use in bin.\n2017-05-09 13:13:15.970285: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):         T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970295: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):         T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970304: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):         T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970313: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):        T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970325: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):        T\notal Chunks: 1, Chunks in use: 0 288.0KiB allocated for chunks. 18.8KiB client-requested for chunks. 0B i\nn use in bin. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970334: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):        T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970342: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):       T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970350: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):       T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970357: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):       T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970365: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):       T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970374: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):      T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970383: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):      T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970392: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):      T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970401: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):     T\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\nn. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970413: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):     T\notal Chunks: 1, Chunks in use: 0 7.61GiB allocated for chunks. 242.58MiB client-requested for chunks. 0B\nin use in bin. 0B client-requested in use in bin.\n2017-05-09 13:13:15.970423: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 9.21GiB was 25\n6.00MiB, Chunk State:\n2017-05-09 13:13:15.970436: I tensorflow/core/common_runtime/bfc_allocator.cc:666]   Size: 7.61GiB | Requ\nested Size: 242.58MiB | in_use: 0, prev:   Size: 465.75MiB | Requested Size: 465.75MiB | in_use: 1\n2017-05-09 13:13:15.970444: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600000\n of size 1280\n2017-05-09 13:13:15.970450: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600500\n of size 256\n2017-05-09 13:13:15.970456: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600600\n of size 256\n2017-05-09 13:13:15.970463: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600700\n of size 256\n2017-05-09 13:13:15.970470: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600800\n of size 256\n2017-05-09 13:13:15.970476: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600900\n of size 256\n2017-05-09 13:13:15.970483: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600a00\n of size 256\n2017-05-09 13:13:15.970490: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600b00\n of size 256\n2017-05-09 13:13:15.970496: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600c00\n of size 256\n2017-05-09 13:13:15.970504: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600d00\n of size 1536\n2017-05-09 13:13:15.970510: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601300\n of size 256\n2017-05-09 13:13:15.970517: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601400\n of size 256\n2017-05-09 13:13:15.970524: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601500\n2017-05-09 13:13:15.970531: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601800\n of size 256\n2017-05-09 13:13:15.970538: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601900\n of size 256\n2017-05-09 13:13:15.970544: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601a00\n of size 256\n2017-05-09 13:13:15.970551: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601b00\n of size 256\n2017-05-09 13:13:15.970557: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601c00\n of size 256\n2017-05-09 13:13:15.970565: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601d00\n of size 19200\n2017-05-09 13:13:15.970571: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606800\n of size 256\n2017-05-09 13:13:15.970578: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606900\n of size 488374272\n2017-05-09 13:13:15.970585: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6900\n of size 1536\n2017-05-09 13:13:15.970592: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6f00\n of size 294912\n2017-05-09 13:13:15.970599: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80ef00\n of size 768\n2017-05-09 13:13:15.970606: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f200\n of size 1536\n2017-05-09 13:13:15.970612: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f800\n of size 256\n2017-05-09 13:13:15.970619: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f900\n of size 19200\n2017-05-09 13:13:15.970625: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814400\n of size 256\n2017-05-09 13:13:15.970632: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814500\n of size 1536\n2017-05-09 13:13:15.970639: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814b00\n of size 17664\n2017-05-09 13:13:15.970646: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819000\n of size 256\n2017-05-09 13:13:15.970653: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819100\n of size 294912\n2017-05-09 13:13:15.970659: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a861100\n of size 488079360\n2017-05-09 13:13:15.970666: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x102479d9100\n of size 1536\n2017-05-09 13:13:15.970673: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21700\n of size 768\n2017-05-09 13:13:15.970680: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21a00\n of size 256\n2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00\n of size 256\n2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00\n of size 256\n2017-05-09 13:13:15.970693: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21c00\n of size 256\n2017-05-09 13:13:15.970700: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21d00\n of size 256\n2017-05-09 13:13:15.970706: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21e00\n of size 256\n2017-05-09 13:13:15.970713: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22000\n of size 256\n2017-05-09 13:13:15.970720: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22100\n of size 256\n2017-05-09 13:13:15.970726: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22200\n of size 256\n2017-05-09 13:13:15.970733: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22300\n of size 19200\n2017-05-09 13:13:15.970740: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a26e00\n of size 19200\n2017-05-09 13:13:15.970747: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2b900\n of size 256\n2017-05-09 13:13:15.970753: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2ba00\n of size 256\n2017-05-09 13:13:15.970760: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2bb00\n of size 488374272\n2017-05-09 13:13:15.970767: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10264bebb00\n of size 488374272\n2017-05-09 13:13:15.970773: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dabb00\n of size 1536\n2017-05-09 13:13:15.970780: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac100\n of size 1536\n2017-05-09 13:13:15.970787: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac700\n of size 294912\n2017-05-09 13:13:15.970793: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281df4700\n of size 294912\n2017-05-09 13:13:15.970800: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3c700\n of size 768\n2017-05-09 13:13:15.970807: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3ca00\n of size 768\n2017-05-09 13:13:15.970814: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3cd00\n of size 1536\n2017-05-09 13:13:15.970820: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d300\n of size 1536\n2017-05-09 13:13:15.970827: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d900\n of size 256\n2017-05-09 13:13:15.970834: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3da00\n of size 256\n2017-05-09 13:13:15.970840: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3db00\n of size 19200\n2017-05-09 13:13:15.970847: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e42600\n of size 488374272\n2017-05-09 13:13:15.970855: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x102479d9700\nof size 294912\n2017-05-09 13:13:15.970861: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x10247a21f00\nof size 256\n2017-05-09 13:13:15.970868: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x1029f002600\nof size 8176048640\n2017-05-09 13:13:15.970875: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use\n Chunks by size:\n2017-05-09 13:13:15.970884: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 31 Chunks of size 256\ntotalling 7.8KiB\n2017-05-09 13:13:15.970892: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 768 t\notalling 3.8KiB\n2017-05-09 13:13:15.970899: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280\ntotalling 1.2KiB\n2017-05-09 13:13:15.970908: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 9 Chunks of size 1536\ntotalling 13.5KiB\n2017-05-09 13:13:15.970916: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 17664\n totalling 17.2KiB\n2017-05-09 13:13:15.970924: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 19200\n totalling 93.8KiB\n2017-05-09 13:13:15.970931: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 29491\n2 totalling 1.12MiB\n2017-05-09 13:13:15.970940: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 48807\n9360 totalling 465.47MiB\n2017-05-09 13:13:15.970947: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 48837\n4272 totalling 1.82GiB\n2017-05-09 13:13:15.970955: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use ch\nunks: 2.27GiB\n2017-05-09 13:13:15.970966: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\nLimit:                 10619240448\nInUse:                  2442896640\nMaxInUse:               2995339008\nNumAllocs:                  803473\nMaxAllocSize:            488374272\n2017-05-09 13:13:15.970984: W tensorflow/core/common_runtime/bfc_allocator.cc:277] **********************\n**____________________________________________________________________________\n2017-05-09 13:13:15.971003: W tensorflow/core/framework/op_kernel.cc:1152] Resource exhausted: OOM when a\nllocating tensor with shape[1945,64,276,72]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1039, in _do_ca\nll\n    return fn(*args)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1021, in _run_f\nn\n    status, run_metadata)\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in\nraise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194\n5,64,276,72]\n         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1\n, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2/weights/r\nead)]]\n         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/\ncpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge\n_20_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 145, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 136, in main\n    is_training: False}))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 569, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3741, in _eval_u\nsing_default_session\n    return session.run(tensors, feed_dict)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 778, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 982, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1032, in _do_ru\nn\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1052, in _do_ca\nll\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194\n5,64,276,72]\n         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1\n, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2/weights/r\nead)]]\n         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/\ncpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge\n_20_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv2/convolution', defined at:\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 145, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 110, in main\n    y_conv, is_training = deepnn(x)\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\nlim.py\", line 56, in deepnn\n    net = slim.conv2d(x_image, 64, [5, 5], scope='conv2')\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", lin\ne 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 9\n18, in convolution\n    outputs = layer.apply(inputs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 290, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py\", line 156, in c\nall\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 661, in convolution\n    op=op)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 331, in with_space_\nto_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 653, in op\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 129, in _non_atrous\n_convolution\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 403, in conv2d\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 768,\nin apply_op\n    op_def=op_def)\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_\nop\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init_\n_\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1945,64,276,72]\n         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1\n, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2/weights/r\nead)]]\n         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/\ncpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge\n_20_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8/6\r\n- **GPU model and memory**: NVIDIA GTX 1080 TI, 11 GB\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\n# Disable linter warnings to maintain consistency with tutorial.\r\n# pylint: disable=invalid-name\r\n\r\nimport argparse\r\nimport sys\r\nfrom collections import namedtuple\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\n\r\nfrom dataset import DataSet\r\n\r\nFLAGS = None\r\nHEIGHT = 276\r\nWIDTH = 72\r\nDEPTH = 3\r\nINPUT_DIMENSION = HEIGHT * WIDTH * DEPTH\r\nNUMBER_CLASSES = 2\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass DataSet(object):\r\n\r\n    def __init__(self,\r\n                 images,\r\n                 labels,\r\n                 dtype=tf.float32,\r\n                 reshape=False):\r\n        dtype = tf.as_dtype(dtype).base_dtype\r\n        if dtype not in (tf.uint8, tf.float32):\r\n            raise TypeError(\r\n                'Invalid image dtype %r, expected uint8 or float32' % dtype)\r\n\r\n        assert images.shape[0] == labels.shape[0], (\r\n            'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\r\n        self._num_examples = images.shape[0]\r\n\r\n        # Convert shape from [num examples, rows, columns, depth]\r\n        # to [num examples, rows*columns] (assuming depth == 1)\r\n        if reshape:\r\n            assert images.shape[3] == 1\r\n            images = images.reshape(images.shape[0],\r\n                                    images.shape[1] * images.shape[2])\r\n\r\n        self._images = images\r\n        self._labels = labels\r\n        self._epochs_completed = 0\r\n        self._index_in_epoch = 0\r\n\r\n    @property\r\n    def images(self):\r\n        return self._images\r\n\r\n    @property\r\n    def labels(self):\r\n        return self._labels\r\n\r\n    @property\r\n    def num_examples(self):\r\n        return self._num_examples\r\n\r\n    @property\r\n    def epochs_completed(self):\r\n        return self._epochs_completed\r\n\r\n    def next_batch(self, batch_size, shuffle=True):\r\n        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\r\n        start = self._index_in_epoch\r\n        # Shuffle for the first epoch\r\n        if self._epochs_completed == 0 and start == 0 and shuffle:\r\n            perm0 = np.arange(self._num_examples)\r\n            np.random.shuffle(perm0)\r\n            self._images = self.images[perm0]\r\n            self._labels = self.labels[perm0]\r\n        # Go to the next epoch\r\n        if start + batch_size > self._num_examples:\r\n            # Finished epoch\r\n            self._epochs_completed += 1\r\n            # Get the rest examples in this epoch\r\n            rest_num_examples = self._num_examples - start\r\n            images_rest_part = self._images[start:self._num_examples]\r\n            labels_rest_part = self._labels[start:self._num_examples]\r\n            # Shuffle the data\r\n            if shuffle:\r\n                perm = np.arange(self._num_examples)\r\n                np.random.shuffle(perm)\r\n                self._images = self.images[perm]\r\n                self._labels = self.labels[perm]\r\n            # Start next epoch\r\n            start = 0\r\n            self._index_in_epoch = batch_size - rest_num_examples\r\n            end = self._index_in_epoch\r\n            images_new_part = self._images[start:end]\r\n            labels_new_part = self._labels[start:end]\r\n            return np.concatenate((images_rest_part, images_new_part), axis=0),\\\r\n                np.concatenate((labels_rest_part, labels_new_part), axis=0)\r\n        else:\r\n            self._index_in_epoch += batch_size\r\n            end = self._index_in_epoch\r\n            return self._images[start:end], self._labels[start:end]\r\n\r\n\r\ndef deepnn(x):\r\n    x_image = tf.reshape(x, [-1, DEPTH, HEIGHT, WIDTH])\r\n    is_training = tf.placeholder(tf.bool)\r\n\r\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\r\n                        data_format='NCHW', padding='SAME'):\r\n        with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                            weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\r\n                            biases_initializer=tf.constant_initializer(0.1)):\r\n            with slim.arg_scope([slim.dropout],\r\n                                is_training=is_training):\r\n                net = slim.conv2d(x_image, 64, [5, 5], scope='conv1')\r\n                net = slim.max_pool2d(net, kernel_size=3, stride=2, scope='pool1')\r\n                net = slim.conv2d(x_image, 64, [5, 5], scope='conv2')\r\n                net = slim.max_pool2d(net, kernel_size=3, stride=2, scope='pool2')\r\n                net = slim.flatten(net, scope=\"Flatten\")\r\n                net = slim.fully_connected(net, 384, scope='fc_1',\r\n                                           weights_regularizer=slim.l2_regularizer(0.000005))\r\n                net = slim.fully_connected(net, 192, scope='fc_2',\r\n                                           weights_regularizer=slim.l2_regularizer(0.000005))\r\n                net = slim.fully_connected(net, 2, activation_fn=None, scope='fc_out')\r\n\r\n    return net, is_training\r\n\r\n\r\ndef import_images_and_labels():\r\n    file_path = \"/path/to/file/samples.npz\"\r\n    data = np.load(file_path)\r\n\r\n    print(data['one_hot_labels'].shape)\r\n    print(data['images'].shape)\r\n\r\n    images = data['images'].astype(np.float32)\r\n    labels = data['one_hot_labels'].astype(np.float32)\r\n\r\n    number_training_samples = 17950\r\n    train_data = DataSet(images=images[:number_training_samples, :],\r\n                         labels=labels[:number_training_samples, :])\r\n    validation_data = DataSet(images=images[16000:17950, :], labels=labels[16000:17950, :])\r\n    test_data = DataSet(images=images[17950:-1, :], labels=labels[17950: -1, :])\r\n\r\n    DataSets = namedtuple('DataSets', ['train', 'validation', 'test'])\r\n\r\n    return DataSets(train=train_data,\r\n                    validation=validation_data,\r\n                    test=test_data)\r\n\r\n\r\ndef main(_):\r\n    dataset = import_images_and_labels()\r\n\r\n    # Create the model\r\n    x = tf.placeholder(tf.float32, [None, INPUT_DIMENSION])\r\n\r\n    # Define loss and optimizer\r\n    y_ = tf.placeholder(tf.float32, [None, NUMBER_CLASSES])\r\n\r\n    # Build the graph for the deep net\r\n    y_conv, is_training = deepnn(x)\r\n\r\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,\r\n                                                                           logits=y_conv))\r\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n    with tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=1)) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        for i in range(20000):\r\n            batch = dataset.train.next_batch(50)\r\n\r\n            if i % 100 == 0:\r\n                train_accuracy = accuracy.eval(feed_dict={x: batch[0],\r\n                                                          y_: batch[1],\r\n                                                          is_training: False})\r\n                print('step %d, training accuracy %g' % (i, train_accuracy))\r\n\r\n            train_step.run(feed_dict={x: batch[0],\r\n                                      y_: batch[1],\r\n                                      is_training: True})\r\n\r\n        print('test accuracy %g' % accuracy.eval(feed_dict={x: dataset.test.images,\r\n                                                            y_: dataset.test.labels,\r\n                                                            is_training: False}))\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--data_dir', type=str,\r\n                        default='/tmp/tensorflow/mnist/input_data',\r\n                        help='Directory for storing input data')\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux pcdekon0082 4.8.0-51-generic #54~16.04.1-Ubuntu SMP Wed Apr 26 16:00:28 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.11.0)\r\nprotobuf (3.2.0)\r\ntensorflow (1.1.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue May  9 13:52:52 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Graphics Device     Off  | 0000:01:00.0      On |                  N/A |\r\n| 26%   46C    P8    18W / 250W |    378MiB / 11171MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1514    G   /usr/lib/xorg/Xorg                             190MiB |\r\n|    0      1904    G   kwin_x11                                        41MiB |\r\n|    0      1910    G   /usr/bin/krunner                                 2MiB |\r\n|    0      1915    G   /usr/bin/plasmashell                            94MiB |\r\n|    0      2047    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    46MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n\r\n```\r\n\r\n### Describe the problem\r\nWhen loading the test images in order to determine the accuracy, an ResourceExhaustedError is thrown.\r\nThis network model works fine with a private theano based framework.\r\n\r\n### Source code / logs\r\n```\r\n...\r\n...\r\nstep 19800, training accuracy 1\r\nstep 19900, training accuracy 1\r\n2017-05-09 13:13:15.970193: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc)\r\nran out of memory trying to allocate 9.21GiB.  Current allocation summary follows.\r\n2017-05-09 13:13:15.970227: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):   Total Chu\r\nnks: 1, Chunks in use: 0 256B allocated for chunks. 4B client-requested for chunks. 0B in use in bin. 0B\r\nclient-requested in use in bin.\r\n2017-05-09 13:13:15.970239: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):   Total Chu\r\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\r\nient-requested in use in bin.\r\n2017-05-09 13:13:15.970247: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):  Total Chu\r\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\r\nient-requested in use in bin.\r\n2017-05-09 13:13:15.970257: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):  Total Chu\r\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\r\nient-requested in use in bin.\r\n2017-05-09 13:13:15.970266: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):  Total Chu\r\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\r\nient-requested in use in bin.\r\n2017-05-09 13:13:15.970275: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):  Total Chu\r\nnks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B cl\r\nient-requested in use in bin.\r\n2017-05-09 13:13:15.970285: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):         T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970295: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):         T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970304: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):         T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970313: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):        T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970325: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):        T\r\notal Chunks: 1, Chunks in use: 0 288.0KiB allocated for chunks. 18.8KiB client-requested for chunks. 0B i\r\nn use in bin. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970334: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):        T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970342: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):       T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970350: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):       T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970357: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):       T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970365: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):       T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970374: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):      T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970383: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):      T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970392: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):      T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970401: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):     T\r\notal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bi\r\nn. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970413: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):     T\r\notal Chunks: 1, Chunks in use: 0 7.61GiB allocated for chunks. 242.58MiB client-requested for chunks. 0B\r\nin use in bin. 0B client-requested in use in bin.\r\n2017-05-09 13:13:15.970423: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 9.21GiB was 25\r\n6.00MiB, Chunk State:\r\n2017-05-09 13:13:15.970436: I tensorflow/core/common_runtime/bfc_allocator.cc:666]   Size: 7.61GiB | Requ\r\nested Size: 242.58MiB | in_use: 0, prev:   Size: 465.75MiB | Requested Size: 465.75MiB | in_use: 1\r\n2017-05-09 13:13:15.970444: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600000\r\n of size 1280\r\n2017-05-09 13:13:15.970450: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600500\r\n of size 256\r\n2017-05-09 13:13:15.970456: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600600\r\n of size 256\r\n2017-05-09 13:13:15.970463: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600700\r\n of size 256\r\n2017-05-09 13:13:15.970470: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600800\r\n of size 256\r\n2017-05-09 13:13:15.970476: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600900\r\n of size 256\r\n2017-05-09 13:13:15.970483: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600a00\r\n of size 256\r\n2017-05-09 13:13:15.970490: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600b00\r\n of size 256\r\n2017-05-09 13:13:15.970496: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600c00\r\n of size 256\r\n2017-05-09 13:13:15.970504: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d600d00\r\n of size 1536\r\n2017-05-09 13:13:15.970510: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601300\r\n of size 256\r\n2017-05-09 13:13:15.970517: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601400\r\n of size 256\r\n2017-05-09 13:13:15.970524: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601500\r\n2017-05-09 13:13:15.970531: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601800\r\n of size 256\r\n2017-05-09 13:13:15.970538: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601900\r\n of size 256\r\n2017-05-09 13:13:15.970544: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601a00\r\n of size 256\r\n2017-05-09 13:13:15.970551: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601b00\r\n of size 256\r\n2017-05-09 13:13:15.970557: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601c00\r\n of size 256\r\n2017-05-09 13:13:15.970565: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d601d00\r\n of size 19200\r\n2017-05-09 13:13:15.970571: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606800\r\n of size 256\r\n2017-05-09 13:13:15.970578: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1020d606900\r\n of size 488374272\r\n2017-05-09 13:13:15.970585: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6900\r\n of size 1536\r\n2017-05-09 13:13:15.970592: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a7c6f00\r\n of size 294912\r\n2017-05-09 13:13:15.970599: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80ef00\r\n of size 768\r\n2017-05-09 13:13:15.970606: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f200\r\n of size 1536\r\n2017-05-09 13:13:15.970612: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f800\r\n of size 256\r\n2017-05-09 13:13:15.970619: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a80f900\r\n of size 19200\r\n2017-05-09 13:13:15.970625: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814400\r\n of size 256\r\n2017-05-09 13:13:15.970632: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814500\r\n of size 1536\r\n2017-05-09 13:13:15.970639: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a814b00\r\n of size 17664\r\n2017-05-09 13:13:15.970646: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819000\r\n of size 256\r\n2017-05-09 13:13:15.970653: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a819100\r\n of size 294912\r\n2017-05-09 13:13:15.970659: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1022a861100\r\n of size 488079360\r\n2017-05-09 13:13:15.970666: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x102479d9100\r\n of size 1536\r\n2017-05-09 13:13:15.970673: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21700\r\n of size 768\r\n2017-05-09 13:13:15.970680: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21a00\r\n of size 256\r\n2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00\r\n of size 256\r\n2017-05-09 13:13:15.970686: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21b00\r\n of size 256\r\n2017-05-09 13:13:15.970693: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21c00\r\n of size 256\r\n2017-05-09 13:13:15.970700: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21d00\r\n of size 256\r\n2017-05-09 13:13:15.970706: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a21e00\r\n of size 256\r\n2017-05-09 13:13:15.970713: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22000\r\n of size 256\r\n2017-05-09 13:13:15.970720: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22100\r\n of size 256\r\n2017-05-09 13:13:15.970726: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22200\r\n of size 256\r\n2017-05-09 13:13:15.970733: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a22300\r\n of size 19200\r\n2017-05-09 13:13:15.970740: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a26e00\r\n of size 19200\r\n2017-05-09 13:13:15.970747: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2b900\r\n of size 256\r\n2017-05-09 13:13:15.970753: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2ba00\r\n of size 256\r\n2017-05-09 13:13:15.970760: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10247a2bb00\r\n of size 488374272\r\n2017-05-09 13:13:15.970767: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10264bebb00\r\n of size 488374272\r\n2017-05-09 13:13:15.970773: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dabb00\r\n of size 1536\r\n2017-05-09 13:13:15.970780: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac100\r\n of size 1536\r\n2017-05-09 13:13:15.970787: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281dac700\r\n of size 294912\r\n2017-05-09 13:13:15.970793: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281df4700\r\n of size 294912\r\n2017-05-09 13:13:15.970800: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3c700\r\n of size 768\r\n2017-05-09 13:13:15.970807: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3ca00\r\n of size 768\r\n2017-05-09 13:13:15.970814: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3cd00\r\n of size 1536\r\n2017-05-09 13:13:15.970820: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d300\r\n of size 1536\r\n2017-05-09 13:13:15.970827: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3d900\r\n of size 256\r\n2017-05-09 13:13:15.970834: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3da00\r\n of size 256\r\n2017-05-09 13:13:15.970840: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e3db00\r\n of size 19200\r\n2017-05-09 13:13:15.970847: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10281e42600\r\n of size 488374272\r\n2017-05-09 13:13:15.970855: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x102479d9700\r\nof size 294912\r\n2017-05-09 13:13:15.970861: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x10247a21f00\r\nof size 256\r\n2017-05-09 13:13:15.970868: I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x1029f002600\r\nof size 8176048640\r\n2017-05-09 13:13:15.970875: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use\r\n Chunks by size:\r\n2017-05-09 13:13:15.970884: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 31 Chunks of size 256\r\ntotalling 7.8KiB\r\n2017-05-09 13:13:15.970892: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 768 t\r\notalling 3.8KiB\r\n2017-05-09 13:13:15.970899: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280\r\ntotalling 1.2KiB\r\n2017-05-09 13:13:15.970908: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 9 Chunks of size 1536\r\ntotalling 13.5KiB\r\n2017-05-09 13:13:15.970916: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 17664\r\n totalling 17.2KiB\r\n2017-05-09 13:13:15.970924: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 19200\r\n totalling 93.8KiB\r\n2017-05-09 13:13:15.970931: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 29491\r\n2 totalling 1.12MiB\r\n2017-05-09 13:13:15.970940: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 48807\r\n9360 totalling 465.47MiB\r\n2017-05-09 13:13:15.970947: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 48837\r\n4272 totalling 1.82GiB\r\n2017-05-09 13:13:15.970955: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use ch\r\nunks: 2.27GiB\r\n2017-05-09 13:13:15.970966: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\r\nLimit:                 10619240448\r\nInUse:                  2442896640\r\nMaxInUse:               2995339008\r\nNumAllocs:                  803473\r\nMaxAllocSize:            488374272\r\n2017-05-09 13:13:15.970984: W tensorflow/core/common_runtime/bfc_allocator.cc:277] **********************\r\n**____________________________________________________________________________\r\n2017-05-09 13:13:15.971003: W tensorflow/core/framework/op_kernel.cc:1152] Resource exhausted: OOM when a\r\nllocating tensor with shape[1945,64,276,72]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1039, in _do_ca\r\nll\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1021, in _run_f\r\nn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in\r\nraise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194\r\n5,64,276,72]\r\n         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1\r\n, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2/weights/r\r\nead)]]\r\n         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/\r\ncpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge\r\n_20_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\r\nlim.py\", line 145, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\r\nlim.py\", line 136, in main\r\n    is_training: False}))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 569, in eval\r\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3741, in _eval_u\r\nsing_default_session\r\n    return session.run(tensors, feed_dict)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1032, in _do_ru\r\nn\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1052, in _do_ca\r\nll\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[194\r\n5,64,276,72]\r\n         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1\r\n, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2/weights/r\r\nead)]]\r\n         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/\r\ncpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge\r\n_20_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op 'conv2/convolution', defined at:\r\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\r\nlim.py\", line 145, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\r\nlim.py\", line 110, in main\r\n    y_conv, is_training = deepnn(x)\r\n  File \"/localhome/bvde102/development/utilities/trunk/Untersuchungen/CNN/tf-tests/src/baumer_challenge_s\r\nlim.py\", line 56, in deepnn\r\n    net = slim.conv2d(x_image, 64, [5, 5], scope='conv2')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", lin\r\ne 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 9\r\n18, in convolution\r\n    outputs = layer.apply(inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 320, in apply\r\n    return self.__call__(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 290, in __call__\r\n    outputs = self.call(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py\", line 156, in c\r\nall\r\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 661, in convolution\r\n    op=op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 331, in with_space_\r\nto_batch\r\n    return op(input, num_spatial_dims, padding)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 653, in op\r\n    name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 129, in _non_atrous\r\n_convolution\r\n    name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 403, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 768,\r\nin apply_op\r\n    op_def=op_def)\r\nFile \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_\r\nop\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init_\r\n_\r\n    self._traceback = _extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1945,64,276,72]\r\n         [[Node: conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", padding=\"SAME\", strides=[1, 1\r\n, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, conv2/weights/r\r\nead)]]\r\n         [[Node: Mean_1/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/\r\ncpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge\r\n_20_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n"}