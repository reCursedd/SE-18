{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/231782112", "html_url": "https://github.com/tensorflow/tensorflow/issues/3201#issuecomment-231782112", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3201", "id": 231782112, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMTc4MjExMg==", "user": {"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-11T16:08:36Z", "updated_at": "2016-07-11T16:08:36Z", "author_association": "MEMBER", "body_html": "<p>We realize that unnecessary copies are undesirable: they can make you run out of GPU memory as you observe, and even when they don't they consume unnecessary memory bandwidth.</p>\n<p>There are two strategies being investigated that could help here, but both are non-trivial and will take some time to be done in a comprehensive fashion.</p>\n<p>The first is to allow general-purpose Ops to do in-place updates, i.e. re-use the input memory buffer instead of allocating a new buffer for the output. This is non-trivial because buffers in TensorFlow can be aliased, and obviously we can't update a buffer that another consumer believes is immutable. It isn't as simple as just reference counting because aliased variables add more complexity. I am working on a formal memory model for TensorFlow which is a step towards doing the analysis that would allow in-place updates to be done with confidence.</p>\n<p>The second is to do some kind of operator fusion as in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116471724\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/164\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/164/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/164\">#164</a> which again is being worked on but will take some time.</p>\n<p>In the meantime as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> indicates the workaround is do write a custom fused Op. It's not an ideal situation but should allow you to make progress if you are really blocked on memory.</p>", "body_text": "We realize that unnecessary copies are undesirable: they can make you run out of GPU memory as you observe, and even when they don't they consume unnecessary memory bandwidth.\nThere are two strategies being investigated that could help here, but both are non-trivial and will take some time to be done in a comprehensive fashion.\nThe first is to allow general-purpose Ops to do in-place updates, i.e. re-use the input memory buffer instead of allocating a new buffer for the output. This is non-trivial because buffers in TensorFlow can be aliased, and obviously we can't update a buffer that another consumer believes is immutable. It isn't as simple as just reference counting because aliased variables add more complexity. I am working on a formal memory model for TensorFlow which is a step towards doing the analysis that would allow in-place updates to be done with confidence.\nThe second is to do some kind of operator fusion as in #164 which again is being worked on but will take some time.\nIn the meantime as @aselle indicates the workaround is do write a custom fused Op. It's not an ideal situation but should allow you to make progress if you are really blocked on memory.", "body": "We realize that unnecessary copies are undesirable: they can make you run out of GPU memory as you observe, and even when they don't they consume unnecessary memory bandwidth.\n\nThere are two strategies being investigated that could help here, but both are non-trivial and will take some time to be done in a comprehensive fashion.\n\nThe first is to allow general-purpose Ops to do in-place updates, i.e. re-use the input memory buffer instead of allocating a new buffer for the output. This is non-trivial because buffers in TensorFlow can be aliased, and obviously we can't update a buffer that another consumer believes is immutable. It isn't as simple as just reference counting because aliased variables add more complexity. I am working on a formal memory model for TensorFlow which is a step towards doing the analysis that would allow in-place updates to be done with confidence.\n\nThe second is to do some kind of operator fusion as in #164 which again is being worked on but will take some time.\n\nIn the meantime as @aselle indicates the workaround is do write a custom fused Op. It's not an ideal situation but should allow you to make progress if you are really blocked on memory.\n"}