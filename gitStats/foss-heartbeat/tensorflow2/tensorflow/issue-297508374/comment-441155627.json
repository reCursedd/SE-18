{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/441155627", "html_url": "https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-441155627", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17048", "id": 441155627, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MTE1NTYyNw==", "user": {"login": "ed-alertedh", "id": 24605895, "node_id": "MDQ6VXNlcjI0NjA1ODk1", "avatar_url": "https://avatars1.githubusercontent.com/u/24605895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ed-alertedh", "html_url": "https://github.com/ed-alertedh", "followers_url": "https://api.github.com/users/ed-alertedh/followers", "following_url": "https://api.github.com/users/ed-alertedh/following{/other_user}", "gists_url": "https://api.github.com/users/ed-alertedh/gists{/gist_id}", "starred_url": "https://api.github.com/users/ed-alertedh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ed-alertedh/subscriptions", "organizations_url": "https://api.github.com/users/ed-alertedh/orgs", "repos_url": "https://api.github.com/users/ed-alertedh/repos", "events_url": "https://api.github.com/users/ed-alertedh/events{/privacy}", "received_events_url": "https://api.github.com/users/ed-alertedh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-23T04:44:31Z", "updated_at": "2018-11-23T04:44:31Z", "author_association": "NONE", "body_html": "<p>I've elaborated on the trick used by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10602509\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nathandouglas\">@nathandouglas</a> to create a decorator that will transparently launch your tf training/prediction function in a subprocess. It also takes care of hiding CUDA devices from TF so that it only uses the desired number of devices (you can easily remove that functionality if you don't need it). You'll need to wrap any code that creates a session to ensure that your GPU memory is released as it seems that TF carries out all the CUDA initialisation and device memory allocation when the first session is created. For example:</p>\n<pre><code>@RunAsCUDASubprocess(num_gpus=1)\ndef train_model():\n  ...\n\ntrain_model()\n</code></pre>\n<p><a href=\"https://gist.github.com/ed-alertedh/85dc3a70d3972e742ca0c4296de7bf00\">https://gist.github.com/ed-alertedh/85dc3a70d3972e742ca0c4296de7bf00</a></p>", "body_text": "I've elaborated on the trick used by @nathandouglas to create a decorator that will transparently launch your tf training/prediction function in a subprocess. It also takes care of hiding CUDA devices from TF so that it only uses the desired number of devices (you can easily remove that functionality if you don't need it). You'll need to wrap any code that creates a session to ensure that your GPU memory is released as it seems that TF carries out all the CUDA initialisation and device memory allocation when the first session is created. For example:\n@RunAsCUDASubprocess(num_gpus=1)\ndef train_model():\n  ...\n\ntrain_model()\n\nhttps://gist.github.com/ed-alertedh/85dc3a70d3972e742ca0c4296de7bf00", "body": "I've elaborated on the trick used by @nathandouglas to create a decorator that will transparently launch your tf training/prediction function in a subprocess. It also takes care of hiding CUDA devices from TF so that it only uses the desired number of devices (you can easily remove that functionality if you don't need it). You'll need to wrap any code that creates a session to ensure that your GPU memory is released as it seems that TF carries out all the CUDA initialisation and device memory allocation when the first session is created. For example:\r\n```\r\n@RunAsCUDASubprocess(num_gpus=1)\r\ndef train_model():\r\n  ...\r\n\r\ntrain_model()\r\n```\r\n\r\nhttps://gist.github.com/ed-alertedh/85dc3a70d3972e742ca0c4296de7bf00\r\n"}