{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/368156677", "html_url": "https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-368156677", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17048", "id": 368156677, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODE1NjY3Nw==", "user": {"login": "ahsteven", "id": 10161343, "node_id": "MDQ6VXNlcjEwMTYxMzQz", "avatar_url": "https://avatars2.githubusercontent.com/u/10161343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahsteven", "html_url": "https://github.com/ahsteven", "followers_url": "https://api.github.com/users/ahsteven/followers", "following_url": "https://api.github.com/users/ahsteven/following{/other_user}", "gists_url": "https://api.github.com/users/ahsteven/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahsteven/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahsteven/subscriptions", "organizations_url": "https://api.github.com/users/ahsteven/orgs", "repos_url": "https://api.github.com/users/ahsteven/repos", "events_url": "https://api.github.com/users/ahsteven/events{/privacy}", "received_events_url": "https://api.github.com/users/ahsteven/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-23T22:27:50Z", "updated_at": "2018-02-23T22:27:50Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">The problem is that even if I exit python in the terminal (ctl -Z) and\nrestart another python process the memory is still occupied. I have to\nclose the terminal for the memory to release.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Feb 23, 2018 at 12:41 PM, Nathan Douglas ***@***.***&gt; wrote:\n I'm currently building out a workflow using Celery to perform a set of\n image analytics in parallel. I ran into this issues since the celery\n daemon, once started, never released GPU resources after it completed\n processing images that used TensorFlow. As a result, I quickly depleted all\n of my GPU resources. My interim solution to the problem, was to wrap my\n analytic in a Pool that spawns a single Process. It's not pretty, but it at\n least solves the problem in releasing GPU resources if you have a long\n running process like a celery daemon.\n\n import tensorflow as tf\n from multiprocessing import Pool\n\n def _process(image):\n     sess = tf.Session()\n     sess.close()\n\n    def process_image(image):\n     with Pool(1) as p:\n         return p.apply(_process, (image,))\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"297508374\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/17048\" href=\"https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-368082470\">#17048 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AJsMvwR_3Po8S7ANCWN0MJNkZIOfGm9wks5tXvhGgaJpZM4SHHWg\">https://github.com/notifications/unsubscribe-auth/AJsMvwR_3Po8S7ANCWN0MJNkZIOfGm9wks5tXvhGgaJpZM4SHHWg</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "The problem is that even if I exit python in the terminal (ctl -Z) and\nrestart another python process the memory is still occupied. I have to\nclose the terminal for the memory to release.\n\u2026\nOn Fri, Feb 23, 2018 at 12:41 PM, Nathan Douglas ***@***.***> wrote:\n I'm currently building out a workflow using Celery to perform a set of\n image analytics in parallel. I ran into this issues since the celery\n daemon, once started, never released GPU resources after it completed\n processing images that used TensorFlow. As a result, I quickly depleted all\n of my GPU resources. My interim solution to the problem, was to wrap my\n analytic in a Pool that spawns a single Process. It's not pretty, but it at\n least solves the problem in releasing GPU resources if you have a long\n running process like a celery daemon.\n\n import tensorflow as tf\n from multiprocessing import Pool\n\n def _process(image):\n     sess = tf.Session()\n     sess.close()\n\n    def process_image(image):\n     with Pool(1) as p:\n         return p.apply(_process, (image,))\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#17048 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AJsMvwR_3Po8S7ANCWN0MJNkZIOfGm9wks5tXvhGgaJpZM4SHHWg>\n .", "body": "The problem is that even if I exit python in the terminal (ctl -Z) and\nrestart another python process the memory is still occupied. I have to\nclose the terminal for the memory to release.\n\nOn Fri, Feb 23, 2018 at 12:41 PM, Nathan Douglas <notifications@github.com>\nwrote:\n\n> I'm currently building out a workflow using Celery to perform a set of\n> image analytics in parallel. I ran into this issues since the celery\n> daemon, once started, never released GPU resources after it completed\n> processing images that used TensorFlow. As a result, I quickly depleted all\n> of my GPU resources. My interim solution to the problem, was to wrap my\n> analytic in a Pool that spawns a single Process. It's not pretty, but it at\n> least solves the problem in releasing GPU resources if you have a long\n> running process like a celery daemon.\n>\n> import tensorflow as tf\n> from multiprocessing import Pool\n>\n> def _process(image):\n>     sess = tf.Session()\n>     sess.close()\n>\n>    def process_image(image):\n>     with Pool(1) as p:\n>         return p.apply(_process, (image,))\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-368082470>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJsMvwR_3Po8S7ANCWN0MJNkZIOfGm9wks5tXvhGgaJpZM4SHHWg>\n> .\n>\n"}