{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19682", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19682/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19682/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19682/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/19682", "id": 328388936, "node_id": "MDExOlB1bGxSZXF1ZXN0MTkxOTM5MjMx", "number": 19682, "title": "Unify learning rate as normal Tensor for tf.contrib.layers.optimize_loss", "user": {"login": "yanboliang", "id": 1962026, "node_id": "MDQ6VXNlcjE5NjIwMjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1962026?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yanboliang", "html_url": "https://github.com/yanboliang", "followers_url": "https://api.github.com/users/yanboliang/followers", "following_url": "https://api.github.com/users/yanboliang/following{/other_user}", "gists_url": "https://api.github.com/users/yanboliang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yanboliang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yanboliang/subscriptions", "organizations_url": "https://api.github.com/users/yanboliang/orgs", "repos_url": "https://api.github.com/users/yanboliang/repos", "events_url": "https://api.github.com/users/yanboliang/events{/privacy}", "received_events_url": "https://api.github.com/users/yanboliang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-06-01T05:43:21Z", "updated_at": "2018-06-04T17:46:08Z", "closed_at": "2018-06-04T17:32:28Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19682", "html_url": "https://github.com/tensorflow/tensorflow/pull/19682", "diff_url": "https://github.com/tensorflow/tensorflow/pull/19682.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/19682.patch"}, "body_html": "<p>In <code>tf.contrib.layers.optimize_loss</code>, we handle <code>learning_rate</code> differently.</p>\n<ul>\n<li>If it's a Tensor, then keep intact.</li>\n<li>If it's a float, then we wrap it as TF Variable, thus be stored in checkpoint.</li>\n</ul>\n<p>It will throw exception if we train a model with constant learning rate(for example <code>0.2</code>), save to checkpoint, and then continue train with a Tensor learning rate(for example <code>tf.constant(0.2)</code>). This exceptions was reported at (here](<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"323709143\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensor2tensor/issues/809\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensor2tensor/issues/809/hovercard\" href=\"https://github.com/tensorflow/tensor2tensor/issues/809\">tensorflow/tensor2tensor#809</a>)</p>\n<p>Meanwhile, I think we don't need to store <code>learning rate</code> in checkpoint. If the checkpoint is used for inference only, no learning rate is needed. If the checkpoint is used for further training, we specify constant learning rate, or learning rate decay function(which takes constant <code>learning_rate</code> and <code>global_step</code> variable as input).</p>\n<p>Further more, if we export <code>learning rate</code> as variable, when continuing training a model with constant <code>learning rate</code>, we can't change it to different value. But there is need to use a new <code>learning rate</code> to continue training.</p>\n<p>To sum up, I would suggest to unify <code>learning rate</code> as normal Tensor regardless of it's Tensor or float input. I'm glad to hear TF core developers' opinion. Thanks.</p>", "body_text": "In tf.contrib.layers.optimize_loss, we handle learning_rate differently.\n\nIf it's a Tensor, then keep intact.\nIf it's a float, then we wrap it as TF Variable, thus be stored in checkpoint.\n\nIt will throw exception if we train a model with constant learning rate(for example 0.2), save to checkpoint, and then continue train with a Tensor learning rate(for example tf.constant(0.2)). This exceptions was reported at (here](tensorflow/tensor2tensor#809)\nMeanwhile, I think we don't need to store learning rate in checkpoint. If the checkpoint is used for inference only, no learning rate is needed. If the checkpoint is used for further training, we specify constant learning rate, or learning rate decay function(which takes constant learning_rate and global_step variable as input).\nFurther more, if we export learning rate as variable, when continuing training a model with constant learning rate, we can't change it to different value. But there is need to use a new learning rate to continue training.\nTo sum up, I would suggest to unify learning rate as normal Tensor regardless of it's Tensor or float input. I'm glad to hear TF core developers' opinion. Thanks.", "body": "In ```tf.contrib.layers.optimize_loss```, we handle ```learning_rate``` differently. \r\n* If it's a Tensor, then keep intact.\r\n* If it's a float, then we wrap it as TF Variable, thus be stored in checkpoint.\r\n\r\nIt will throw exception if we train a model with constant learning rate(for example ```0.2```), save to checkpoint, and then continue train with a Tensor learning rate(for example ```tf.constant(0.2)```). This exceptions was reported at (here](https://github.com/tensorflow/tensor2tensor/issues/809)\r\n\r\nMeanwhile, I think we don't need to store ```learning rate``` in checkpoint. If the checkpoint is used for inference only, no learning rate is needed. If the checkpoint is used for further training, we specify constant learning rate, or learning rate decay function(which takes constant ```learning_rate``` and ```global_step``` variable as input).\r\n\r\nFurther more, if we export ```learning rate``` as variable, when continuing training a model with constant ```learning rate```, we can't change it to different value. But there is need to use a new ```learning rate``` to continue training.\r\n\r\nTo sum up, I would suggest to unify ```learning rate``` as normal Tensor regardless of it's Tensor or float input. I'm glad to hear TF core developers' opinion. Thanks.\r\n\r\n"}