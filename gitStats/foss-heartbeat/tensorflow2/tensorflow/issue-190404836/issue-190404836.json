{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5700", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5700/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5700/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5700/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5700", "id": 190404836, "node_id": "MDU6SXNzdWUxOTA0MDQ4MzY=", "number": 5700, "title": "Definition of epoch_size in PTB model leads to skipping last sample batch", "user": {"login": "glarchev", "id": 958267, "node_id": "MDQ6VXNlcjk1ODI2Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/958267?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glarchev", "html_url": "https://github.com/glarchev", "followers_url": "https://api.github.com/users/glarchev/followers", "following_url": "https://api.github.com/users/glarchev/following{/other_user}", "gists_url": "https://api.github.com/users/glarchev/gists{/gist_id}", "starred_url": "https://api.github.com/users/glarchev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glarchev/subscriptions", "organizations_url": "https://api.github.com/users/glarchev/orgs", "repos_url": "https://api.github.com/users/glarchev/repos", "events_url": "https://api.github.com/users/glarchev/events{/privacy}", "received_events_url": "https://api.github.com/users/glarchev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-11-18T19:43:43Z", "updated_at": "2016-11-22T02:23:17Z", "closed_at": "2016-11-22T02:23:17Z", "author_association": "NONE", "body_html": "<p>Not 100% sure this is a bug, please take a look:</p>\n<p>The PTB RNN model (tensorflow/models/rnn/ptb/ptb_word_lm.py, line 92) defines a variable called epoch_size as</p>\n<p>self.epoch_size = ((len(data) // batch_size) - 1) // num_steps</p>\n<p>(This variable is also defined in the same way in tensorflow/models/rnn/ptb/reader.py, line 108.) The variable is used on line 278 in ptb_word_lm.py:</p>\n<p>for step in range(model.input.epoch_size):</p>\n<p>I'm not sure why the \"- 1\" term is in the equation -- that is, I think it should be just</p>\n<p>self.epoch_size = (len(data) // batch_size) // num_steps</p>\n<p>The equation already uses floor division to round down. Currently, if both batch_size and num_steps are 1, epoch_size will be equal to data length - 1, so the last data sample will be skipped.</p>", "body_text": "Not 100% sure this is a bug, please take a look:\nThe PTB RNN model (tensorflow/models/rnn/ptb/ptb_word_lm.py, line 92) defines a variable called epoch_size as\nself.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n(This variable is also defined in the same way in tensorflow/models/rnn/ptb/reader.py, line 108.) The variable is used on line 278 in ptb_word_lm.py:\nfor step in range(model.input.epoch_size):\nI'm not sure why the \"- 1\" term is in the equation -- that is, I think it should be just\nself.epoch_size = (len(data) // batch_size) // num_steps\nThe equation already uses floor division to round down. Currently, if both batch_size and num_steps are 1, epoch_size will be equal to data length - 1, so the last data sample will be skipped.", "body": "Not 100% sure this is a bug, please take a look:\r\n\r\nThe PTB RNN model (tensorflow/models/rnn/ptb/ptb_word_lm.py, line 92) defines a variable called epoch_size as \r\n\r\nself.epoch_size = ((len(data) // batch_size) - 1) // num_steps\r\n\r\n(This variable is also defined in the same way in tensorflow/models/rnn/ptb/reader.py, line 108.) The variable is used on line 278 in ptb_word_lm.py:\r\n\r\nfor step in range(model.input.epoch_size):\r\n\r\nI'm not sure why the \"- 1\" term is in the equation -- that is, I think it should be just\r\n\r\nself.epoch_size = (len(data) // batch_size) // num_steps\r\n\r\nThe equation already uses floor division to round down. Currently, if both batch_size and num_steps are 1, epoch_size will be equal to data length - 1, so the last data sample will be skipped."}