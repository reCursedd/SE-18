{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/288818636", "html_url": "https://github.com/tensorflow/tensorflow/issues/8670#issuecomment-288818636", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8670", "id": 288818636, "node_id": "MDEyOklzc3VlQ29tbWVudDI4ODgxODYzNg==", "user": {"login": "jorgemf", "id": 1029554, "node_id": "MDQ6VXNlcjEwMjk1NTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/1029554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jorgemf", "html_url": "https://github.com/jorgemf", "followers_url": "https://api.github.com/users/jorgemf/followers", "following_url": "https://api.github.com/users/jorgemf/following{/other_user}", "gists_url": "https://api.github.com/users/jorgemf/gists{/gist_id}", "starred_url": "https://api.github.com/users/jorgemf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jorgemf/subscriptions", "organizations_url": "https://api.github.com/users/jorgemf/orgs", "repos_url": "https://api.github.com/users/jorgemf/repos", "events_url": "https://api.github.com/users/jorgemf/events{/privacy}", "received_events_url": "https://api.github.com/users/jorgemf/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-23T18:31:07Z", "updated_at": "2017-03-23T18:31:07Z", "author_association": "NONE", "body_html": "<p>I am not sure but I think this could be related with the full connected layers with a lot of inputs/outputs. Somehow the operations are not performed in the same order and the round error is different per batch size, this leads to an accumulation of the error different per batch size. When the layers are big (1024 neurons) the error adds and the output is completely different.</p>", "body_text": "I am not sure but I think this could be related with the full connected layers with a lot of inputs/outputs. Somehow the operations are not performed in the same order and the round error is different per batch size, this leads to an accumulation of the error different per batch size. When the layers are big (1024 neurons) the error adds and the output is completely different.", "body": "I am not sure but I think this could be related with the full connected layers with a lot of inputs/outputs. Somehow the operations are not performed in the same order and the round error is different per batch size, this leads to an accumulation of the error different per batch size. When the layers are big (1024 neurons) the error adds and the output is completely different."}