{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289863584", "html_url": "https://github.com/tensorflow/tensorflow/issues/8670#issuecomment-289863584", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8670", "id": 289863584, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTg2MzU4NA==", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-28T18:34:31Z", "updated_at": "2017-03-28T18:34:31Z", "author_association": "MEMBER", "body_html": "<p>To be clear</p>\n<pre><code>batch size: 1\n        outputs: [  5.03433178e-13]\nbatch size: 2\n        outputs: [  5.03433233e-13]\nbatch size: 10\n        outputs: [  5.03433233e-13]\nbatch size: 16\n        outputs: [  5.03433233e-13]\nbatch size: 20\n        outputs: [  5.03433287e-13]\n</code></pre>\n<p>You are worried that these are not the same error? between batch size 1 and 2. Those numbers are practically the same near zero result.  The floating point operations used between different batches may in fact be different between different tensor sizes due to performance auto tuning. Do you have any concrete reason to worry about this? It looks like not a bug to me.</p>", "body_text": "To be clear\nbatch size: 1\n        outputs: [  5.03433178e-13]\nbatch size: 2\n        outputs: [  5.03433233e-13]\nbatch size: 10\n        outputs: [  5.03433233e-13]\nbatch size: 16\n        outputs: [  5.03433233e-13]\nbatch size: 20\n        outputs: [  5.03433287e-13]\n\nYou are worried that these are not the same error? between batch size 1 and 2. Those numbers are practically the same near zero result.  The floating point operations used between different batches may in fact be different between different tensor sizes due to performance auto tuning. Do you have any concrete reason to worry about this? It looks like not a bug to me.", "body": "To be clear\r\n```\r\nbatch size: 1\r\n        outputs: [  5.03433178e-13]\r\nbatch size: 2\r\n        outputs: [  5.03433233e-13]\r\nbatch size: 10\r\n        outputs: [  5.03433233e-13]\r\nbatch size: 16\r\n        outputs: [  5.03433233e-13]\r\nbatch size: 20\r\n        outputs: [  5.03433287e-13]\r\n```\r\nYou are worried that these are not the same error? between batch size 1 and 2. Those numbers are practically the same near zero result.  The floating point operations used between different batches may in fact be different between different tensor sizes due to performance auto tuning. Do you have any concrete reason to worry about this? It looks like not a bug to me.\r\n"}