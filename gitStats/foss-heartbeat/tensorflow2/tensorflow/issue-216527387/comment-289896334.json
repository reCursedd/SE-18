{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289896334", "html_url": "https://github.com/tensorflow/tensorflow/issues/8670#issuecomment-289896334", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8670", "id": 289896334, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTg5NjMzNA==", "user": {"login": "jorgemf", "id": 1029554, "node_id": "MDQ6VXNlcjEwMjk1NTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/1029554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jorgemf", "html_url": "https://github.com/jorgemf", "followers_url": "https://api.github.com/users/jorgemf/followers", "following_url": "https://api.github.com/users/jorgemf/following{/other_user}", "gists_url": "https://api.github.com/users/jorgemf/gists{/gist_id}", "starred_url": "https://api.github.com/users/jorgemf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jorgemf/subscriptions", "organizations_url": "https://api.github.com/users/jorgemf/orgs", "repos_url": "https://api.github.com/users/jorgemf/repos", "events_url": "https://api.github.com/users/jorgemf/events{/privacy}", "received_events_url": "https://api.github.com/users/jorgemf/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-28T20:36:01Z", "updated_at": "2017-03-28T20:36:01Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> It is almost near 0 in that example, but I have a trained model that the error accumulates and it is close to 0.4 which it is a lot when the expected output is between 0 and 1, and what lead me to investigate further what was going on. My model is inception_v1 plus a couple of full connected layers.</p>\n<p>I don't mind the floating precision point error. But I expect that error to be consistent. Sometimes I get error in the convolution layer, other times there is no error (with different batch sizes). Regardless any optimization the output of an operation must be the same with the same input, which it is not the case. For the same input I am getting different results in the operations.</p>\n<p>In some cases I am also getting different outputs with a batch size, like the batch size is 20, I use the same input 20 times and get 3 or 4 different results in the outputs. It is not consistent.</p>\n<p>Anyway, I did tweak my model to decrease that error and now I don't have the huge problem I had before. But I still think TensorFlow must return the same output for the same input, regardless of any optimization.</p>", "body_text": "@aselle It is almost near 0 in that example, but I have a trained model that the error accumulates and it is close to 0.4 which it is a lot when the expected output is between 0 and 1, and what lead me to investigate further what was going on. My model is inception_v1 plus a couple of full connected layers.\nI don't mind the floating precision point error. But I expect that error to be consistent. Sometimes I get error in the convolution layer, other times there is no error (with different batch sizes). Regardless any optimization the output of an operation must be the same with the same input, which it is not the case. For the same input I am getting different results in the operations.\nIn some cases I am also getting different outputs with a batch size, like the batch size is 20, I use the same input 20 times and get 3 or 4 different results in the outputs. It is not consistent.\nAnyway, I did tweak my model to decrease that error and now I don't have the huge problem I had before. But I still think TensorFlow must return the same output for the same input, regardless of any optimization.", "body": "@aselle It is almost near 0 in that example, but I have a trained model that the error accumulates and it is close to 0.4 which it is a lot when the expected output is between 0 and 1, and what lead me to investigate further what was going on. My model is inception_v1 plus a couple of full connected layers.\r\n\r\nI don't mind the floating precision point error. But I expect that error to be consistent. Sometimes I get error in the convolution layer, other times there is no error (with different batch sizes). Regardless any optimization the output of an operation must be the same with the same input, which it is not the case. For the same input I am getting different results in the operations.\r\n\r\nIn some cases I am also getting different outputs with a batch size, like the batch size is 20, I use the same input 20 times and get 3 or 4 different results in the outputs. It is not consistent.\r\n\r\nAnyway, I did tweak my model to decrease that error and now I don't have the huge problem I had before. But I still think TensorFlow must return the same output for the same input, regardless of any optimization."}