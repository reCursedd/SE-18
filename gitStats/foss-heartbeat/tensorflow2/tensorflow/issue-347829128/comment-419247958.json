{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/419247958", "html_url": "https://github.com/tensorflow/tensorflow/issues/21401#issuecomment-419247958", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21401", "id": 419247958, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTI0Nzk1OA==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-06T21:31:19Z", "updated_at": "2018-09-06T21:31:19Z", "author_association": "MEMBER", "body_html": "<p>Tensor cores are used on V100s for various ops, such as matrix multiplications and convolutions, if the op inputs are float16. So this question is equivalent to checking if float16 is used in the model.</p>\n<p>As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=29663194\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cy89\">@cy89</a> mentioned, one way is to measure the performance. Another way is to check the graph in TensorBoard to see the dtype ops are done in. A third option is to print <code>layer.input</code> and <code>layer.output</code> for each layer to verify the inputs and outputs are float16.</p>\n<p>A mixed precision API is being worked on, which when released, will make it easy to build a model in float16. Once released, it will be clear models that use the API will be done in float16, and therefore tensor cores are being used.</p>", "body_text": "Tensor cores are used on V100s for various ops, such as matrix multiplications and convolutions, if the op inputs are float16. So this question is equivalent to checking if float16 is used in the model.\nAs @cy89 mentioned, one way is to measure the performance. Another way is to check the graph in TensorBoard to see the dtype ops are done in. A third option is to print layer.input and layer.output for each layer to verify the inputs and outputs are float16.\nA mixed precision API is being worked on, which when released, will make it easy to build a model in float16. Once released, it will be clear models that use the API will be done in float16, and therefore tensor cores are being used.", "body": "Tensor cores are used on V100s for various ops, such as matrix multiplications and convolutions, if the op inputs are float16. So this question is equivalent to checking if float16 is used in the model.\r\n\r\nAs @cy89 mentioned, one way is to measure the performance. Another way is to check the graph in TensorBoard to see the dtype ops are done in. A third option is to print `layer.input` and `layer.output` for each layer to verify the inputs and outputs are float16.\r\n\r\nA mixed precision API is being worked on, which when released, will make it easy to build a model in float16. Once released, it will be clear models that use the API will be done in float16, and therefore tensor cores are being used."}