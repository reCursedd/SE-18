{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423210502", "html_url": "https://github.com/tensorflow/tensorflow/issues/21725#issuecomment-423210502", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21725", "id": 423210502, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzIxMDUwMg==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T14:44:40Z", "updated_at": "2018-09-20T14:44:40Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20149552\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/1icas\">@1icas</a>,</p>\n<p>The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the concats in your network need quantization information for TOCO and aren't supported out of the box. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after the concats, see how the rewriter does it: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475</a></p>\n<p>That being said this can be complicated and very error prone, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. If it doesn't provide sufficient speedup (or if it has an error, please file another issue!!) then we can consider this full quantization.</p>", "body_text": "Hi @1icas,\nThe issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the concats in your network need quantization information for TOCO and aren't supported out of the box. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after the concats, see how the rewriter does it: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\nThat being said this can be complicated and very error prone, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. If it doesn't provide sufficient speedup (or if it has an error, please file another issue!!) then we can consider this full quantization.", "body": "Hi @1icas, \r\n\r\nThe issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. In particular the concats in your network need quantization information for TOCO and aren't supported out of the box. This can be resolved by either manually adding a FakeQuantWithMinMaxVars node after the concats, see how the rewriter does it: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py#L475\r\n\r\nThat being said this can be complicated and very error prone, if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. If it doesn't provide sufficient speedup (or if it has an error, please file another issue!!) then we can consider this full quantization."}