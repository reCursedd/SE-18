{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/394604598", "html_url": "https://github.com/tensorflow/tensorflow/issues/19643#issuecomment-394604598", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19643", "id": 394604598, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDYwNDU5OA==", "user": {"login": "jackd", "id": 659115, "node_id": "MDQ6VXNlcjY1OTExNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/659115?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jackd", "html_url": "https://github.com/jackd", "followers_url": "https://api.github.com/users/jackd/followers", "following_url": "https://api.github.com/users/jackd/following{/other_user}", "gists_url": "https://api.github.com/users/jackd/gists{/gist_id}", "starred_url": "https://api.github.com/users/jackd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jackd/subscriptions", "organizations_url": "https://api.github.com/users/jackd/orgs", "repos_url": "https://api.github.com/users/jackd/repos", "events_url": "https://api.github.com/users/jackd/events{/privacy}", "received_events_url": "https://api.github.com/users/jackd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-05T07:08:30Z", "updated_at": "2018-06-05T07:08:30Z", "author_association": "NONE", "body_html": "<p>If this is a conscious design choice as mentioned above, I highly doubt the <code>estimator</code> framework will cause the update ops to run, so this should probably be dealt with by <code>keras</code>'s own mechanism during conversion. As far as I can tell, it isn't.</p>\n<p>I'm not overly familiar with the keras code base but I can't find any explicit reference to <code>model.updates</code>, and the place where I suspect they're meant to be collected in they just aren't.</p>\n<p>from <code>keras/engine/training.py</code>, <code>Model._make_train_function</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>        <span class=\"pl-k\">with</span> K.name_scope(<span class=\"pl-c1\">self</span>.optimizer.<span class=\"pl-c1\">__class__</span>.<span class=\"pl-c1\">__name__</span>):\n          <span class=\"pl-c\"><span class=\"pl-c\">#</span> Training updates</span>\n          updates <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.optimizer.get_updates(\n              <span class=\"pl-v\">params</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._collected_trainable_weights, <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.total_loss)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Unconditional updates</span>\n        updates <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">self</span>.get_updates_for(<span class=\"pl-c1\">None</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Conditional updates relevant to this model</span>\n        updates <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">self</span>.get_updates_for(<span class=\"pl-c1\">self</span>._feed_inputs)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Stateful metrics updates</span>\n        updates <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">self</span>.metrics_updates\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Gets loss and metrics. Updates weights at each call.</span></pre></div>\n<p>It's not an unconditional update (see below), and I doubt its a metric update or an update required for feed inputs. They're just not added to update_ops.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\ntf.keras.backend.set_learning_phase(<span class=\"pl-c1\">True</span>)\ninput_shape <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">3</span>,)\ninp <span class=\"pl-k\">=</span> tf.keras.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>input_shape, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nx <span class=\"pl-k\">=</span> tf.keras.layers.Dense(<span class=\"pl-c1\">4</span>, <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>input_shape)(inp)\nx <span class=\"pl-k\">=</span> tf.keras.layers.BatchNormalization()(x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\nmodel <span class=\"pl-k\">=</span> tf.keras.Model(inp, x)\nmodel.compile(tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-3</span>), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>mean_squared_error<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model_updates: <span class=\"pl-c1\">%d</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">len</span>(model.updates))\n<span class=\"pl-k\">for</span> update <span class=\"pl-k\">in</span> model.updates:\n    <span class=\"pl-c1\">print</span>(update.name, update._unconditional_update)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> False</span>\n\nestimator <span class=\"pl-k\">=</span> tf.keras.estimator.model_to_estimator(model)\n\nz <span class=\"pl-k\">=</span> tf.zeros((<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nlabels <span class=\"pl-k\">=</span> tf.zeros((<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\nspec <span class=\"pl-k\">=</span> estimator.model_fn(z, labels, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">len</span>(tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 0</span></pre></div>", "body_text": "If this is a conscious design choice as mentioned above, I highly doubt the estimator framework will cause the update ops to run, so this should probably be dealt with by keras's own mechanism during conversion. As far as I can tell, it isn't.\nI'm not overly familiar with the keras code base but I can't find any explicit reference to model.updates, and the place where I suspect they're meant to be collected in they just aren't.\nfrom keras/engine/training.py, Model._make_train_function:\n        with K.name_scope(self.optimizer.__class__.__name__):\n          # Training updates\n          updates = self.optimizer.get_updates(\n              params=self._collected_trainable_weights, loss=self.total_loss)\n        # Unconditional updates\n        updates += self.get_updates_for(None)\n        # Conditional updates relevant to this model\n        updates += self.get_updates_for(self._feed_inputs)\n        # Stateful metrics updates\n        updates += self.metrics_updates\n        # Gets loss and metrics. Updates weights at each call.\nIt's not an unconditional update (see below), and I doubt its a metric update or an update required for feed inputs. They're just not added to update_ops.\nimport tensorflow as tf\n\ntf.keras.backend.set_learning_phase(True)\ninput_shape = (3,)\ninp = tf.keras.Input(shape=input_shape, dtype=tf.float32)\nx = tf.keras.layers.Dense(4, input_shape=input_shape)(inp)\nx = tf.keras.layers.BatchNormalization()(x, training=True)\n\nmodel = tf.keras.Model(inp, x)\nmodel.compile(tf.train.AdamOptimizer(1e-3), 'mean_squared_error')\nprint('model_updates: %d' % len(model.updates))\nfor update in model.updates:\n    print(update.name, update._unconditional_update)  # False\n\nestimator = tf.keras.estimator.model_to_estimator(model)\n\nz = tf.zeros((2, 3), dtype=tf.float32)\nlabels = tf.zeros((2, 4), dtype=tf.float32)\n\nspec = estimator.model_fn(z, labels, mode='train', config=None)\n\nprint(len(tf.get_collection(tf.GraphKeys.UPDATE_OPS)))  # 0", "body": "If this is a conscious design choice as mentioned above, I highly doubt the `estimator` framework will cause the update ops to run, so this should probably be dealt with by `keras`'s own mechanism during conversion. As far as I can tell, it isn't.\r\n\r\nI'm not overly familiar with the keras code base but I can't find any explicit reference to `model.updates`, and the place where I suspect they're meant to be collected in they just aren't.\r\n\r\nfrom `keras/engine/training.py`, `Model._make_train_function`:\r\n```python\r\n        with K.name_scope(self.optimizer.__class__.__name__):\r\n          # Training updates\r\n          updates = self.optimizer.get_updates(\r\n              params=self._collected_trainable_weights, loss=self.total_loss)\r\n        # Unconditional updates\r\n        updates += self.get_updates_for(None)\r\n        # Conditional updates relevant to this model\r\n        updates += self.get_updates_for(self._feed_inputs)\r\n        # Stateful metrics updates\r\n        updates += self.metrics_updates\r\n        # Gets loss and metrics. Updates weights at each call.\r\n````\r\n\r\nIt's not an unconditional update (see below), and I doubt its a metric update or an update required for feed inputs. They're just not added to update_ops.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.keras.backend.set_learning_phase(True)\r\ninput_shape = (3,)\r\ninp = tf.keras.Input(shape=input_shape, dtype=tf.float32)\r\nx = tf.keras.layers.Dense(4, input_shape=input_shape)(inp)\r\nx = tf.keras.layers.BatchNormalization()(x, training=True)\r\n\r\nmodel = tf.keras.Model(inp, x)\r\nmodel.compile(tf.train.AdamOptimizer(1e-3), 'mean_squared_error')\r\nprint('model_updates: %d' % len(model.updates))\r\nfor update in model.updates:\r\n    print(update.name, update._unconditional_update)  # False\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(model)\r\n\r\nz = tf.zeros((2, 3), dtype=tf.float32)\r\nlabels = tf.zeros((2, 4), dtype=tf.float32)\r\n\r\nspec = estimator.model_fn(z, labels, mode='train', config=None)\r\n\r\nprint(len(tf.get_collection(tf.GraphKeys.UPDATE_OPS)))  # 0\r\n```"}