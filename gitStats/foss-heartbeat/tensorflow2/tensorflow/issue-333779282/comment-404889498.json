{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404889498", "html_url": "https://github.com/tensorflow/tensorflow/issues/20126#issuecomment-404889498", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20126", "id": 404889498, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDg4OTQ5OA==", "user": {"login": "Bocharick", "id": 6680697, "node_id": "MDQ6VXNlcjY2ODA2OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/6680697?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bocharick", "html_url": "https://github.com/Bocharick", "followers_url": "https://api.github.com/users/Bocharick/followers", "following_url": "https://api.github.com/users/Bocharick/following{/other_user}", "gists_url": "https://api.github.com/users/Bocharick/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bocharick/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bocharick/subscriptions", "organizations_url": "https://api.github.com/users/Bocharick/orgs", "repos_url": "https://api.github.com/users/Bocharick/repos", "events_url": "https://api.github.com/users/Bocharick/events{/privacy}", "received_events_url": "https://api.github.com/users/Bocharick/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-13T16:48:22Z", "updated_at": "2018-07-13T17:07:29Z", "author_association": "NONE", "body_html": "<p>I <strong>have same problem</strong> with my code. Just trying to use your MirroredStrategy functional, and it's not working for me, and I have only some <strong>assertion error</strong>;</p>\n<p><strong>0) Have I written custom code:</strong><br>\n<a href=\"https://pastebin.com/LzED1tF3\" rel=\"nofollow\">Too much lines, so my code on pastebin</a></p>\n<p><strong>1) OS Platform and Distribution:</strong><br>\n-Linux UbuntuPC 4.4.0-130-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116441460\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/156\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/156/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/156\">#156</a>-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux</p>\n<p><strong>2) TensorFlow installed from:</strong><br>\nsudo pip3.5 install ***</p>\n<p><strong>3) TensorFlow version:</strong><br>\nChecked on two versions of tf:<br>\na) 1.10.nightly<br>\ntf_nightly_gpu-1.10.0.dev20180620-cp35-cp35m-manylinux1_x86_64.whl<br>\nb) 1.9.rc2<br>\ntensorflow_gpu-1.9.0rc2-cp35-cp35m-manylinux1_x86_64.whl</p>\n<p><strong>4) Bazel version:</strong></p>\n<blockquote>\n<p>Extracting Bazel installation...<br>\nBuild label: 0.13.1<br>\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Wed May 23 11:17:23 2018 (1527074243)<br>\nBuild timestamp: 1527074243<br>\nBuild timestamp as int: 1527074243</p>\n</blockquote>\n<p><strong>5) CUDA/cuDNN version</strong><br>\nCUDA - 9.0.176.2<br>\ncuDNN - 7.1.2</p>\n<p><strong>6) GPU model and memory:</strong><br>\nNvidia GeForce GTX 1080 8Gb</p>\n<p><strong>7) Exact command to reproduce:</strong></p>\n<blockquote>\n<p>$ python3.5 my_tf_script.py</p>\n</blockquote>\n<p>When I use only 1 GPU (GPU_NUM = 1 on line \u211617) it works.<br>\nBut when I use your \"magic word\" -<br>\n<strong>distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=GPU_NUM)</strong><br>\nIt failed on next step where model.train()<br>\nSo my error:<br>\n<strong>++++++++++++++++ERROR:++++++++++++++++</strong></p>\n<pre><code>\n/usr/bin/python3.5 /home/user/SOME_PATH/bin/my_tf_script.py\nData #1 loaded\nData loading complete\nINFO:tensorflow:Using config: {'_master': '', '_task_type': 'worker', '_service': None, '_save_checkpoints_steps': None, '_tf_random_seed': None, '_session_config': None, '_save_summary_steps': 100, '_is_chief': True, '_global_id_in_cluster': 0, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_task_id': 0, '_num_worker_replicas': 1, '_model_dir': './checkpoints_train/', '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7f51e086b4a8&gt;, '_device_fn': None, '_evaluation_master': '', '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_train_distribute': &lt;tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f51e086b5f8&gt;}\n2018-07-14 00:04:57.193208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:83:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2018-07-14 00:04:57.489755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:84:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2018-07-14 00:04:57.491371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n2018-07-14 00:04:58.205491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-14 00:04:58.205548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n2018-07-14 00:04:58.205562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n2018-07-14 00:04:58.205575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n2018-07-14 00:04:58.206129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 7534 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:83:00.0, compute capability: 6.1)\n2018-07-14 00:04:58.282580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:1 with 7534 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1)\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\nINFO:tensorflow:Configured nccl all-reduce.\n2018-07-14 00:04:58.438084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n2018-07-14 00:04:58.438251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-14 00:04:58.438271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n2018-07-14 00:04:58.438298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n2018-07-14 00:04:58.438315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n2018-07-14 00:04:58.438712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7534 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:83:00.0, compute capability: 6.1)\n2018-07-14 00:04:58.438878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7534 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1)\nINFO:tensorflow:Calling model_fn.\nTRAINABLE_VARIABLES: [&lt;tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref&gt;, &lt;tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref&gt;]\nTRAINABLE_VARIABLES[:1]: [&lt;tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref&gt;]\nTRAINABLE_VARIABLES[1:]: [&lt;tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref&gt;]\nINFO:tensorflow:Calling model_fn.\nTRAINABLE_VARIABLES: [&lt;tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref&gt;, &lt;tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref&gt;, &lt;tf.Variable 'tower_1/V:0' shape=(3449, 13, 64) dtype=float32_ref&gt;, &lt;tf.Variable 'tower_1/S:0' shape=(902113, 64) dtype=float32_ref&gt;]\nTRAINABLE_VARIABLES[:1]: [&lt;tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref&gt;]\nTRAINABLE_VARIABLES[1:]: [&lt;tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref&gt;, &lt;tf.Variable 'tower_1/V:0' shape=(3449, 13, 64) dtype=float32_ref&gt;, &lt;tf.Variable 'tower_1/S:0' shape=(902113, 64) dtype=float32_ref&gt;]\nINFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\nWARNING:tensorflow:Efficient allreduce is not supported for IndexedSlices.\nINFO:tensorflow:Error reported to Coordinator: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n    yield\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 273, in _call_for_each_tower\n    self, *merge_args, **merge_kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 685, in _distributed_apply\n    for grad, var in grads_and_vars\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 686, in &lt;listcomp&gt;\n    for op in distribution.unwrap(distribution.update(var, update, grad))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 894, in update\n    return self._update(var, fn, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 325, in _update\n    assert isinstance(var, values.MirroredVariable)\nAssertionError\nTraceback (most recent call last):\n  File \"tst.py\", line 198, in &lt;module&gt;\n    model.train(input_fn=make_full_data_dataset, steps=300000)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 375, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1131, in _train_model\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1171, in _train_model_distributed\n    self.config)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 811, in call_for_each_tower\n    return self._call_for_each_tower(fn, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 279, in _call_for_each_tower\n    coord.join(threads)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\n    raise value\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n    yield\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 273, in _call_for_each_tower\n    self, *merge_args, **merge_kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 685, in _distributed_apply\n    for grad, var in grads_and_vars\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 686, in &lt;listcomp&gt;\n    for op in distribution.unwrap(distribution.update(var, update, grad))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 894, in update\n    return self._update(var, fn, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 325, in _update\n    assert isinstance(var, values.MirroredVariable)\nAssertionError\n</code></pre>", "body_text": "I have same problem with my code. Just trying to use your MirroredStrategy functional, and it's not working for me, and I have only some assertion error;\n0) Have I written custom code:\nToo much lines, so my code on pastebin\n1) OS Platform and Distribution:\n-Linux UbuntuPC 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n2) TensorFlow installed from:\nsudo pip3.5 install ***\n3) TensorFlow version:\nChecked on two versions of tf:\na) 1.10.nightly\ntf_nightly_gpu-1.10.0.dev20180620-cp35-cp35m-manylinux1_x86_64.whl\nb) 1.9.rc2\ntensorflow_gpu-1.9.0rc2-cp35-cp35m-manylinux1_x86_64.whl\n4) Bazel version:\n\nExtracting Bazel installation...\nBuild label: 0.13.1\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed May 23 11:17:23 2018 (1527074243)\nBuild timestamp: 1527074243\nBuild timestamp as int: 1527074243\n\n5) CUDA/cuDNN version\nCUDA - 9.0.176.2\ncuDNN - 7.1.2\n6) GPU model and memory:\nNvidia GeForce GTX 1080 8Gb\n7) Exact command to reproduce:\n\n$ python3.5 my_tf_script.py\n\nWhen I use only 1 GPU (GPU_NUM = 1 on line \u211617) it works.\nBut when I use your \"magic word\" -\ndistribution = tf.contrib.distribute.MirroredStrategy(num_gpus=GPU_NUM)\nIt failed on next step where model.train()\nSo my error:\n++++++++++++++++ERROR:++++++++++++++++\n\n/usr/bin/python3.5 /home/user/SOME_PATH/bin/my_tf_script.py\nData #1 loaded\nData loading complete\nINFO:tensorflow:Using config: {'_master': '', '_task_type': 'worker', '_service': None, '_save_checkpoints_steps': None, '_tf_random_seed': None, '_session_config': None, '_save_summary_steps': 100, '_is_chief': True, '_global_id_in_cluster': 0, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_task_id': 0, '_num_worker_replicas': 1, '_model_dir': './checkpoints_train/', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f51e086b4a8>, '_device_fn': None, '_evaluation_master': '', '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f51e086b5f8>}\n2018-07-14 00:04:57.193208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:83:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2018-07-14 00:04:57.489755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:84:00.0\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\n2018-07-14 00:04:57.491371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n2018-07-14 00:04:58.205491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-14 00:04:58.205548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n2018-07-14 00:04:58.205562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n2018-07-14 00:04:58.205575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n2018-07-14 00:04:58.206129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 7534 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:83:00.0, compute capability: 6.1)\n2018-07-14 00:04:58.282580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:1 with 7534 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1)\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\nINFO:tensorflow:Configured nccl all-reduce.\n2018-07-14 00:04:58.438084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n2018-07-14 00:04:58.438251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-14 00:04:58.438271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n2018-07-14 00:04:58.438298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n2018-07-14 00:04:58.438315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n2018-07-14 00:04:58.438712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7534 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:83:00.0, compute capability: 6.1)\n2018-07-14 00:04:58.438878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7534 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1)\nINFO:tensorflow:Calling model_fn.\nTRAINABLE_VARIABLES: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>]\nTRAINABLE_VARIABLES[:1]: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>]\nTRAINABLE_VARIABLES[1:]: [<tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>]\nINFO:tensorflow:Calling model_fn.\nTRAINABLE_VARIABLES: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>, <tf.Variable 'tower_1/V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'tower_1/S:0' shape=(902113, 64) dtype=float32_ref>]\nTRAINABLE_VARIABLES[:1]: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>]\nTRAINABLE_VARIABLES[1:]: [<tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>, <tf.Variable 'tower_1/V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'tower_1/S:0' shape=(902113, 64) dtype=float32_ref>]\nINFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\nWARNING:tensorflow:Efficient allreduce is not supported for IndexedSlices.\nINFO:tensorflow:Error reported to Coordinator: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n    yield\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 273, in _call_for_each_tower\n    self, *merge_args, **merge_kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 685, in _distributed_apply\n    for grad, var in grads_and_vars\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 686, in <listcomp>\n    for op in distribution.unwrap(distribution.update(var, update, grad))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 894, in update\n    return self._update(var, fn, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 325, in _update\n    assert isinstance(var, values.MirroredVariable)\nAssertionError\nTraceback (most recent call last):\n  File \"tst.py\", line 198, in <module>\n    model.train(input_fn=make_full_data_dataset, steps=300000)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 375, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1131, in _train_model\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1171, in _train_model_distributed\n    self.config)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 811, in call_for_each_tower\n    return self._call_for_each_tower(fn, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 279, in _call_for_each_tower\n    coord.join(threads)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\n    raise value\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n    yield\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 273, in _call_for_each_tower\n    self, *merge_args, **merge_kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 685, in _distributed_apply\n    for grad, var in grads_and_vars\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 686, in <listcomp>\n    for op in distribution.unwrap(distribution.update(var, update, grad))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 894, in update\n    return self._update(var, fn, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 325, in _update\n    assert isinstance(var, values.MirroredVariable)\nAssertionError", "body": "I **have same problem** with my code. Just trying to use your MirroredStrategy functional, and it's not working for me, and I have only some **assertion error**;\r\n\r\n**0) Have I written custom code:**\r\n[Too much lines, so my code on pastebin](https://pastebin.com/LzED1tF3)\r\n\r\n**1) OS Platform and Distribution:**\r\n-Linux UbuntuPC 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n**2) TensorFlow installed from:**\r\nsudo pip3.5 install ***\r\n\r\n**3) TensorFlow version:**\r\nChecked on two versions of tf:\r\n     a) 1.10.nightly\r\n     tf_nightly_gpu-1.10.0.dev20180620-cp35-cp35m-manylinux1_x86_64.whl\r\n     b) 1.9.rc2\r\n     tensorflow_gpu-1.9.0rc2-cp35-cp35m-manylinux1_x86_64.whl\r\n\r\n**4) Bazel version:**\r\n> Extracting Bazel installation...\r\n> Build label: 0.13.1\r\n> Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n> Build time: Wed May 23 11:17:23 2018 (1527074243)\r\n> Build timestamp: 1527074243\r\n> Build timestamp as int: 1527074243\r\n\r\n**5) CUDA/cuDNN version**\r\nCUDA - 9.0.176.2\r\ncuDNN - 7.1.2\r\n\r\n**6) GPU model and memory:**\r\nNvidia GeForce GTX 1080 8Gb\r\n\r\n**7) Exact command to reproduce:**\r\n\r\n> $ python3.5 my_tf_script.py\r\n\r\n\r\nWhen I use only 1 GPU (GPU_NUM = 1 on line \u211617) it works.\r\nBut when I use your \"magic word\" - \r\n**distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=GPU_NUM)**\r\nIt failed on next step where model.train()\r\nSo my error:\r\n**++++++++++++++++ERROR:++++++++++++++++**\r\n```\r\n\r\n/usr/bin/python3.5 /home/user/SOME_PATH/bin/my_tf_script.py\r\nData #1 loaded\r\nData loading complete\r\nINFO:tensorflow:Using config: {'_master': '', '_task_type': 'worker', '_service': None, '_save_checkpoints_steps': None, '_tf_random_seed': None, '_session_config': None, '_save_summary_steps': 100, '_is_chief': True, '_global_id_in_cluster': 0, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_task_id': 0, '_num_worker_replicas': 1, '_model_dir': './checkpoints_train/', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f51e086b4a8>, '_device_fn': None, '_evaluation_master': '', '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f51e086b5f8>}\r\n2018-07-14 00:04:57.193208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2018-07-14 00:04:57.489755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2018-07-14 00:04:57.491371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\r\n2018-07-14 00:04:58.205491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-14 00:04:58.205548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \r\n2018-07-14 00:04:58.205562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \r\n2018-07-14 00:04:58.205575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \r\n2018-07-14 00:04:58.206129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 7534 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n2018-07-14 00:04:58.282580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:1 with 7534 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\n2018-07-14 00:04:58.438084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\r\n2018-07-14 00:04:58.438251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-14 00:04:58.438271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \r\n2018-07-14 00:04:58.438298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \r\n2018-07-14 00:04:58.438315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \r\n2018-07-14 00:04:58.438712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7534 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n2018-07-14 00:04:58.438878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7534 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Calling model_fn.\r\nTRAINABLE_VARIABLES: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>]\r\nTRAINABLE_VARIABLES[:1]: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>]\r\nTRAINABLE_VARIABLES[1:]: [<tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>]\r\nINFO:tensorflow:Calling model_fn.\r\nTRAINABLE_VARIABLES: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>, <tf.Variable 'tower_1/V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'tower_1/S:0' shape=(902113, 64) dtype=float32_ref>]\r\nTRAINABLE_VARIABLES[:1]: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>]\r\nTRAINABLE_VARIABLES[1:]: [<tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>, <tf.Variable 'tower_1/V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'tower_1/S:0' shape=(902113, 64) dtype=float32_ref>]\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nWARNING:tensorflow:Efficient allreduce is not supported for IndexedSlices.\r\nINFO:tensorflow:Error reported to Coordinator: \r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 273, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 685, in _distributed_apply\r\n    for grad, var in grads_and_vars\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 686, in <listcomp>\r\n    for op in distribution.unwrap(distribution.update(var, update, grad))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 894, in update\r\n    return self._update(var, fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 325, in _update\r\n    assert isinstance(var, values.MirroredVariable)\r\nAssertionError\r\nTraceback (most recent call last):\r\n  File \"tst.py\", line 198, in <module>\r\n    model.train(input_fn=make_full_data_dataset, steps=300000)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 375, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1131, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1171, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 811, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 279, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 273, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 685, in _distributed_apply\r\n    for grad, var in grads_and_vars\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 686, in <listcomp>\r\n    for op in distribution.unwrap(distribution.update(var, update, grad))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 894, in update\r\n    return self._update(var, fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 325, in _update\r\n    assert isinstance(var, values.MirroredVariable)\r\nAssertionError\r\n```"}