{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18630", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18630/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18630/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18630/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18630", "id": 315347747, "node_id": "MDU6SXNzdWUzMTUzNDc3NDc=", "number": 18630, "title": "Grpc+RDMA problem", "user": {"login": "hustcat", "id": 1716915, "node_id": "MDQ6VXNlcjE3MTY5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1716915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hustcat", "html_url": "https://github.com/hustcat", "followers_url": "https://api.github.com/users/hustcat/followers", "following_url": "https://api.github.com/users/hustcat/following{/other_user}", "gists_url": "https://api.github.com/users/hustcat/gists{/gist_id}", "starred_url": "https://api.github.com/users/hustcat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hustcat/subscriptions", "organizations_url": "https://api.github.com/users/hustcat/orgs", "repos_url": "https://api.github.com/users/hustcat/repos", "events_url": "https://api.github.com/users/hustcat/events{/privacy}", "received_events_url": "https://api.github.com/users/hustcat/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-04-18T06:31:46Z", "updated_at": "2018-10-23T09:25:48Z", "closed_at": "2018-06-25T19:00:36Z", "author_association": "NONE", "body_html": "<p>For RDMA, when start the <code>ps</code> server, it will do RDMA connect to <code>worker</code> server, but failed because worker still not started:</p>\n<pre><code># python tf_rdma.py --ps_hosts='workernode2:1111' --worker_hosts='workernode3:2222' --job_name=ps --task_id=0\n....\n2018-04-04 11:23:38.912680: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:1111\n2018-04-04 11:23:38.920106: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got Transport closed. Retrying (1/5)...\n2018-04-04 11:23:41.920544: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (2/5)...\n2018-04-04 11:23:43.921039: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (3/5)...\n2018-04-04 11:23:46.922376: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (4/5)...\n2018-04-04 11:23:48.922817: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (5/5)...\n2018-04-04 11:23:48.922848: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0\n</code></pre>\n<p>Then start the worker server:</p>\n<pre><code># python tf_rdma.py --ps_hosts='workernode2:1111' --worker_hosts='workernode3:2222' --job_name=worker --task_id=0\n...\n2018-04-04 14:12:38.008214: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; workernode2:1111}\n2018-04-04 14:12:38.008256: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2222}\n2018-04-04 14:12:38.013784: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; workernode2:1111}\n2018-04-04 14:12:38.013803: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2222}\n2018-04-04 14:12:38.019732: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\n2018-04-04 14:12:38.028558: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:0\n</code></pre>\n<p>the worker server can do RDMA connect successfully. But hanged up with the follows:</p>\n<pre><code>(gdb) bt\n#0  0x00007fbdcc4f7a54 in mlx4_poll_cq () from /lib64/libmlx4-rdmav2.so\n#1  0x00007fbdf16301d8 in tensorflow::RdmaMgr::ConnectivityCheck() () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#2  0x00007fbdf1628874 in tensorflow::VerbsServer::Start() () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#3  0x00007fbdf12abf9b in _wrap_PyServer_Start () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#4  0x00007fbe01542aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n</code></pre>\n<p>I think <code>ps</code> server should wait <code>worker</code> server setup before <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/verbs_server_lib.cc#L105\">connect worker server</a>.</p>\n<p>Am I right?</p>", "body_text": "For RDMA, when start the ps server, it will do RDMA connect to worker server, but failed because worker still not started:\n# python tf_rdma.py --ps_hosts='workernode2:1111' --worker_hosts='workernode3:2222' --job_name=ps --task_id=0\n....\n2018-04-04 11:23:38.912680: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:1111\n2018-04-04 11:23:38.920106: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got Transport closed. Retrying (1/5)...\n2018-04-04 11:23:41.920544: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (2/5)...\n2018-04-04 11:23:43.921039: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (3/5)...\n2018-04-04 11:23:46.922376: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (4/5)...\n2018-04-04 11:23:48.922817: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (5/5)...\n2018-04-04 11:23:48.922848: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0\n\nThen start the worker server:\n# python tf_rdma.py --ps_hosts='workernode2:1111' --worker_hosts='workernode3:2222' --job_name=worker --task_id=0\n...\n2018-04-04 14:12:38.008214: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> workernode2:1111}\n2018-04-04 14:12:38.008256: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}\n2018-04-04 14:12:38.013784: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> workernode2:1111}\n2018-04-04 14:12:38.013803: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}\n2018-04-04 14:12:38.019732: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\n2018-04-04 14:12:38.028558: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:0\n\nthe worker server can do RDMA connect successfully. But hanged up with the follows:\n(gdb) bt\n#0  0x00007fbdcc4f7a54 in mlx4_poll_cq () from /lib64/libmlx4-rdmav2.so\n#1  0x00007fbdf16301d8 in tensorflow::RdmaMgr::ConnectivityCheck() () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#2  0x00007fbdf1628874 in tensorflow::VerbsServer::Start() () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#3  0x00007fbdf12abf9b in _wrap_PyServer_Start () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#4  0x00007fbe01542aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n\nI think ps server should wait worker server setup before connect worker server.\nAm I right?", "body": "For RDMA, when start the `ps` server, it will do RDMA connect to `worker` server, but failed because worker still not started:\r\n\r\n```\r\n# python tf_rdma.py --ps_hosts='workernode2:1111' --worker_hosts='workernode3:2222' --job_name=ps --task_id=0\r\n....\r\n2018-04-04 11:23:38.912680: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:1111\r\n2018-04-04 11:23:38.920106: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got Transport closed. Retrying (1/5)...\r\n2018-04-04 11:23:41.920544: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (2/5)...\r\n2018-04-04 11:23:43.921039: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (3/5)...\r\n2018-04-04 11:23:46.922376: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (4/5)...\r\n2018-04-04 11:23:48.922817: E tensorflow/contrib/verbs/rdma_mgr.cc:119] Connecting to /job:worker/replica:0/task:0: Got OS Error. Retrying (5/5)...\r\n2018-04-04 11:23:48.922848: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0\r\n```\r\n\r\nThen start the worker server:\r\n\r\n```\r\n# python tf_rdma.py --ps_hosts='workernode2:1111' --worker_hosts='workernode3:2222' --job_name=worker --task_id=0\r\n...\r\n2018-04-04 14:12:38.008214: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> workernode2:1111}\r\n2018-04-04 14:12:38.008256: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}\r\n2018-04-04 14:12:38.013784: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> workernode2:1111}\r\n2018-04-04 14:12:38.013803: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}\r\n2018-04-04 14:12:38.019732: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\r\n2018-04-04 14:12:38.028558: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:0\r\n```\r\n\r\nthe worker server can do RDMA connect successfully. But hanged up with the follows:\r\n\r\n```\r\n(gdb) bt\r\n#0  0x00007fbdcc4f7a54 in mlx4_poll_cq () from /lib64/libmlx4-rdmav2.so\r\n#1  0x00007fbdf16301d8 in tensorflow::RdmaMgr::ConnectivityCheck() () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fbdf1628874 in tensorflow::VerbsServer::Start() () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fbdf12abf9b in _wrap_PyServer_Start () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fbe01542aa4 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n```\r\n\r\nI think `ps` server should wait `worker` server setup before [connect worker server](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/verbs_server_lib.cc#L105).\r\n\r\nAm I right?"}