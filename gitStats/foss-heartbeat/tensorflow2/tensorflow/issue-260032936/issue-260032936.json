{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13262", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13262/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13262/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13262/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13262", "id": 260032936, "node_id": "MDU6SXNzdWUyNjAwMzI5MzY=", "number": 13262, "title": "Training Mini-batches of data (without labels) for unsupervised learning problem", "user": {"login": "vishalbhalla", "id": 10245689, "node_id": "MDQ6VXNlcjEwMjQ1Njg5", "avatar_url": "https://avatars0.githubusercontent.com/u/10245689?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishalbhalla", "html_url": "https://github.com/vishalbhalla", "followers_url": "https://api.github.com/users/vishalbhalla/followers", "following_url": "https://api.github.com/users/vishalbhalla/following{/other_user}", "gists_url": "https://api.github.com/users/vishalbhalla/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishalbhalla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishalbhalla/subscriptions", "organizations_url": "https://api.github.com/users/vishalbhalla/orgs", "repos_url": "https://api.github.com/users/vishalbhalla/repos", "events_url": "https://api.github.com/users/vishalbhalla/events{/privacy}", "received_events_url": "https://api.github.com/users/vishalbhalla/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-23T19:28:38Z", "updated_at": "2017-10-04T04:55:16Z", "closed_at": "2017-10-04T04:55:16Z", "author_association": "NONE", "body_html": "<p>Hi Everyone,</p>\n<p>Has anyone trained mini-batches of data for an unsupervised learning problem?<br>\nThe feed_dict uses the label and in an unsupervised setting. How do you overcome that? Could we use fake labels that never contribute to the loss function?</p>\n<p>Basically, I want to iterate over my huge dataset and then optimize a custom loss function. However, I couldn't figure out how to retain my training parameters (weights) when using a new mini-batch from the data explicitly.</p>\n<p>For example, the whole dataset is 6000 points and the mini-batch size is 600.<br>\nCurrently, for every mini-batch I could only use new independent weight parameters because the weights are initialized based on the data points from this mini-batch. When we optimize the loss over the first mini-batch of 600 data points, we get some optimized weights. How does one use these weights to optimize the next mini-batch of 600 data points and so on. The problem is we cannot use a shared global variable.</p>\n<p>Any help or pointers in this regard would be really appreciated. Thanks in advance!</p>", "body_text": "Hi Everyone,\nHas anyone trained mini-batches of data for an unsupervised learning problem?\nThe feed_dict uses the label and in an unsupervised setting. How do you overcome that? Could we use fake labels that never contribute to the loss function?\nBasically, I want to iterate over my huge dataset and then optimize a custom loss function. However, I couldn't figure out how to retain my training parameters (weights) when using a new mini-batch from the data explicitly.\nFor example, the whole dataset is 6000 points and the mini-batch size is 600.\nCurrently, for every mini-batch I could only use new independent weight parameters because the weights are initialized based on the data points from this mini-batch. When we optimize the loss over the first mini-batch of 600 data points, we get some optimized weights. How does one use these weights to optimize the next mini-batch of 600 data points and so on. The problem is we cannot use a shared global variable.\nAny help or pointers in this regard would be really appreciated. Thanks in advance!", "body": "Hi Everyone,\r\n\r\nHas anyone trained mini-batches of data for an unsupervised learning problem? \r\nThe feed_dict uses the label and in an unsupervised setting. How do you overcome that? Could we use fake labels that never contribute to the loss function?\r\n\r\nBasically, I want to iterate over my huge dataset and then optimize a custom loss function. However, I couldn't figure out how to retain my training parameters (weights) when using a new mini-batch from the data explicitly.\r\n\r\nFor example, the whole dataset is 6000 points and the mini-batch size is 600.\r\nCurrently, for every mini-batch I could only use new independent weight parameters because the weights are initialized based on the data points from this mini-batch. When we optimize the loss over the first mini-batch of 600 data points, we get some optimized weights. How does one use these weights to optimize the next mini-batch of 600 data points and so on. The problem is we cannot use a shared global variable.\r\n\r\nAny help or pointers in this regard would be really appreciated. Thanks in advance!"}