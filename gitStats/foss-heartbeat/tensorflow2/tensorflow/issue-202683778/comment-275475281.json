{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275475281", "html_url": "https://github.com/tensorflow/tensorflow/issues/7025#issuecomment-275475281", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7025", "id": 275475281, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTQ3NTI4MQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-26T18:47:55Z", "updated_at": "2017-01-26T18:47:55Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Sounds like you are running out of GPU memory</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Jan 26, 2017 10:33 AM, \"Atul Acharya\" ***@***.***&gt; wrote:\n Hi <a class=\"user-mention\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> &lt;<a href=\"https://github.com/yaroslavvb\">https://github.com/yaroslavvb</a>&gt; <a class=\"user-mention\" href=\"https://github.com/zheng-xq\">@zheng-xq</a>\n &lt;<a href=\"https://github.com/zheng-xq\">https://github.com/zheng-xq</a>&gt;\n\n I'm getting this Dst Tensor Not Initialized error.\n\n (See my comment (the last one) in this issue elsewhere:\n <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"158536133\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/aymericdamien/TensorFlow-Examples/issues/38\" href=\"https://github.com/aymericdamien/TensorFlow-Examples/issues/38\">aymericdamien/TensorFlow-Examples#38</a>\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"158536133\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/aymericdamien/TensorFlow-Examples/issues/38\" href=\"https://github.com/aymericdamien/TensorFlow-Examples/issues/38\">aymericdamien/TensorFlow-Examples#38</a>&gt;)\n\n I'm reproducing the stack trace here in case it helps diagnose the issue:\n\n \u25b6 python imagenet_inference.py\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\n I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\n name: GeForce GT 750M\n major: 3 minor: 0 memoryClockRate (GHz) 0.9255\n pciBusID 0000:01:00.0\n Total memory: 2.00GiB\n Free memory: 305.92MiB\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0 97.01MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 144.00MiB was 128.00MiB, Chunk State:\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60000 of size 1280\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60500 of size 139520\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82600 of size 512\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82800 of size 1228800\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700bae800 of size 1024\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700baec00 of size 3538944\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0ec00 of size 1536\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0f200 of size 2654208\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197200 of size 1536\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197800 of size 1769472\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701347800 of size 1024\n I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x701347c00 of size 101725184\n I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size:\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1024 totalling 2.0KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1536 totalling 3.0KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 139520 totalling 136.2KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1228800 totalling 1.17MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1769472 totalling 1.69MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2654208 totalling 2.53MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3538944 totalling 3.38MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 8.91MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\n Limit:                   111063040\n InUse:                     9337856\n MaxInUse:                  9337856\n NumAllocs:                      11\n MaxAllocSize:              3538944\n\n W tensorflow/core/common_runtime/bfc_allocator.cc:274] *********___________________________________________________________________________________________\n W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 144.00MiB.  See logs for memory state.\n W tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.\n E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...&gt;, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n Traceback (most recent call last):\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\n     return fn(*args)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\n     status, run_metadata)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/contextlib.py\", line 66, in __exit__\n     next(self.gen)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n     pywrap_tensorflow.TF_GetCode(status))\n tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...&gt;, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\n During handling of the above exception, another exception occurred:\n\n Traceback (most recent call last):\n   File \"imagenet_inference.py\", line 19, in &lt;module&gt;\n     sess.run(init)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\n     run_metadata_ptr)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n     feed_dict_string, options, run_metadata)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n     target_list, options, run_metadata)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n     raise type(e)(node_def, op, message)\n tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...&gt;, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\n Caused by op 'Variable_10/initial_value', defined at:\n   File \"imagenet_inference.py\", line 16, in &lt;module&gt;\n     probs = AlexNet(x, feature_extract=False)\n   File \"/Users/aa/Developer/courses/self_driving_carnd/traffic-signs/CarND-Alexnet-Feature-Extraction/alexnet.py\", line 139, in AlexNet\n     fc6W = tf.Variable(net_data[\"fc6\"][0])\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n     expected_shape=expected_shape)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\n     initial_value, name=\"initial_value\", dtype=dtype)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\n     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\n     return constant(v, dtype=dtype, name=name)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n     attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n     original_op=self._default_original_op, op_def=op_def)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n     self._traceback = _extract_stack()\n\n InternalError (see above for traceback): Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...&gt;, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\n\n Here's deviceQuery successfully reporting seeing the GPU:\n\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_HOME\n /usr/local/cuda\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_VISIBLE_DEVICES\n\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./deviceQuery\n ./deviceQuery Starting...\n\n  CUDA Device Query (Runtime API) version (CUDART static linking)\n\n Detected 1 CUDA Capable device(s)\n\n Device 0: \"GeForce GT 750M\"\n   CUDA Driver Version / Runtime Version          8.0 / 8.0\n   CUDA Capability Major/Minor version number:    3.0\n   Total amount of global memory:                 2048 MBytes (2147024896 bytes)\n   ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\n   GPU Max Clock rate:                            926 MHz (0.93 GHz)\n   Memory Clock rate:                             2508 Mhz\n   Memory Bus Width:                              128-bit\n   L2 Cache Size:                                 262144 bytes\n   Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n   Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n   Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n   Total amount of constant memory:               65536 bytes\n   Total amount of shared memory per block:       49152 bytes\n   Total number of registers available per block: 65536\n   Warp size:                                     32\n   Maximum number of threads per multiprocessor:  2048\n   Maximum number of threads per block:           1024\n   Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n   Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n   Maximum memory pitch:                          2147483647 bytes\n   Texture alignment:                             512 bytes\n   Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n   Run time limit on kernels:                     Yes\n   Integrated GPU sharing Host Memory:            No\n   Support host page-locked memory mapping:       Yes\n   Alignment requirement for Surfaces:            Yes\n   Device has ECC support:                        Disabled\n   Device supports Unified Addressing (UVA):      Yes\n   Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n   Compute Mode:\n      &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;\n\n deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M\n Result = PASS\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./bandwidthTest\n [CUDA Bandwidth Test] - Starting...\n Running on...\n\n  Device 0: GeForce GT 750M\n  Quick Mode\n\n  Host to Device Bandwidth, 1 Device(s)\n  PINNED Memory Transfers\n    Transfer Size (Bytes)\tBandwidth(MB/s)\n    33554432\t\t\t3633.5\n\n  Device to Host Bandwidth, 1 Device(s)\n  PINNED Memory Transfers\n    Transfer Size (Bytes)\tBandwidth(MB/s)\n    33554432\t\t\t6343.5\n\n  Device to Device Bandwidth, 1 Device(s)\n  PINNED Memory Transfers\n    Transfer Size (Bytes)\tBandwidth(MB/s)\n    33554432\t\t\t42554.1\n\n Result = PASS\n\n NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n\n My System:\n\n MacBook Pro (Retina, 15-inch, Late 2013)\n 2.3 GHz Intel Core i7\n 16 GB 1600 MHz DDR3\n NVIDIA GeForce GT 750M 2048 MB\n\n ---\n from System Report &gt; Graphics\n NVIDIA GeForce GT 750M:\n\n   Chipset Model:\tNVIDIA GeForce GT 750M\n   Type:\tGPU\n   Bus:\tPCIe\n   PCIe Lane Width:\tx8\n   VRAM (Total):\t2048 MB\n   Vendor:\tNVIDIA (0x10de)\n   Device ID:\t0x0fe9\n   Revision ID:\t0x00a2\n   ROM Revision:\t3776\n   gMux Version:\t4.0.8 [3.2.8]\n   Displays:\n Color LCD:\n   Display Type:\tRetina LCD\n   Resolution:\t2880 x 1800 Retina\n   Retina:\tYes\n   Pixel Depth:\t32-Bit Color (ARGB8888)\n   Main Display:\tYes\n   Mirror:\tOff\n   Online:\tYes\n   Built-In:\tYes\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"202683778\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7025\" href=\"https://github.com/tensorflow/tensorflow/issues/7025#issuecomment-275470982\">#7025 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AABaHJK2XuJg9IHUT3Rb63Nbtahdgr8sks5rWObHgaJpZM4Lrr-s\">https://github.com/notifications/unsubscribe-auth/AABaHJK2XuJg9IHUT3Rb63Nbtahdgr8sks5rWObHgaJpZM4Lrr-s</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Sounds like you are running out of GPU memory\n\u2026\nOn Jan 26, 2017 10:33 AM, \"Atul Acharya\" ***@***.***> wrote:\n Hi @yaroslavvb <https://github.com/yaroslavvb> @zheng-xq\n <https://github.com/zheng-xq>\n\n I'm getting this Dst Tensor Not Initialized error.\n\n (See my comment (the last one) in this issue elsewhere:\n aymericdamien/TensorFlow-Examples#38\n <aymericdamien/TensorFlow-Examples#38>)\n\n I'm reproducing the stack trace here in case it helps diagnose the issue:\n\n \u25b6 python imagenet_inference.py\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\n I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\n name: GeForce GT 750M\n major: 3 minor: 0 memoryClockRate (GHz) 0.9255\n pciBusID 0000:01:00.0\n Total memory: 2.00GiB\n Free memory: 305.92MiB\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0 97.01MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 144.00MiB was 128.00MiB, Chunk State:\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60000 of size 1280\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60500 of size 139520\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82600 of size 512\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82800 of size 1228800\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700bae800 of size 1024\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700baec00 of size 3538944\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0ec00 of size 1536\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0f200 of size 2654208\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197200 of size 1536\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197800 of size 1769472\n I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701347800 of size 1024\n I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x701347c00 of size 101725184\n I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size:\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1024 totalling 2.0KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1536 totalling 3.0KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 139520 totalling 136.2KiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1228800 totalling 1.17MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1769472 totalling 1.69MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2654208 totalling 2.53MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3538944 totalling 3.38MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 8.91MiB\n I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\n Limit:                   111063040\n InUse:                     9337856\n MaxInUse:                  9337856\n NumAllocs:                      11\n MaxAllocSize:              3538944\n\n W tensorflow/core/common_runtime/bfc_allocator.cc:274] *********___________________________________________________________________________________________\n W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 144.00MiB.  See logs for memory state.\n W tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.\n E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n Traceback (most recent call last):\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\n     return fn(*args)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\n     status, run_metadata)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/contextlib.py\", line 66, in __exit__\n     next(self.gen)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n     pywrap_tensorflow.TF_GetCode(status))\n tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\n During handling of the above exception, another exception occurred:\n\n Traceback (most recent call last):\n   File \"imagenet_inference.py\", line 19, in <module>\n     sess.run(init)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\n     run_metadata_ptr)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n     feed_dict_string, options, run_metadata)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n     target_list, options, run_metadata)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n     raise type(e)(node_def, op, message)\n tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\n Caused by op 'Variable_10/initial_value', defined at:\n   File \"imagenet_inference.py\", line 16, in <module>\n     probs = AlexNet(x, feature_extract=False)\n   File \"/Users/aa/Developer/courses/self_driving_carnd/traffic-signs/CarND-Alexnet-Feature-Extraction/alexnet.py\", line 139, in AlexNet\n     fc6W = tf.Variable(net_data[\"fc6\"][0])\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n     expected_shape=expected_shape)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\n     initial_value, name=\"initial_value\", dtype=dtype)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\n     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\n     return constant(v, dtype=dtype, name=name)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n     attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n     original_op=self._default_original_op, op_def=op_def)\n   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n     self._traceback = _extract_stack()\n\n InternalError (see above for traceback): Dst tensor is not initialized.\n \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\n\n Here's deviceQuery successfully reporting seeing the GPU:\n\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_HOME\n /usr/local/cuda\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_VISIBLE_DEVICES\n\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./deviceQuery\n ./deviceQuery Starting...\n\n  CUDA Device Query (Runtime API) version (CUDART static linking)\n\n Detected 1 CUDA Capable device(s)\n\n Device 0: \"GeForce GT 750M\"\n   CUDA Driver Version / Runtime Version          8.0 / 8.0\n   CUDA Capability Major/Minor version number:    3.0\n   Total amount of global memory:                 2048 MBytes (2147024896 bytes)\n   ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\n   GPU Max Clock rate:                            926 MHz (0.93 GHz)\n   Memory Clock rate:                             2508 Mhz\n   Memory Bus Width:                              128-bit\n   L2 Cache Size:                                 262144 bytes\n   Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n   Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n   Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n   Total amount of constant memory:               65536 bytes\n   Total amount of shared memory per block:       49152 bytes\n   Total number of registers available per block: 65536\n   Warp size:                                     32\n   Maximum number of threads per multiprocessor:  2048\n   Maximum number of threads per block:           1024\n   Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n   Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n   Maximum memory pitch:                          2147483647 bytes\n   Texture alignment:                             512 bytes\n   Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n   Run time limit on kernels:                     Yes\n   Integrated GPU sharing Host Memory:            No\n   Support host page-locked memory mapping:       Yes\n   Alignment requirement for Surfaces:            Yes\n   Device has ECC support:                        Disabled\n   Device supports Unified Addressing (UVA):      Yes\n   Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n   Compute Mode:\n      < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n\n deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M\n Result = PASS\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./bandwidthTest\n [CUDA Bandwidth Test] - Starting...\n Running on...\n\n  Device 0: GeForce GT 750M\n  Quick Mode\n\n  Host to Device Bandwidth, 1 Device(s)\n  PINNED Memory Transfers\n    Transfer Size (Bytes)\tBandwidth(MB/s)\n    33554432\t\t\t3633.5\n\n  Device to Host Bandwidth, 1 Device(s)\n  PINNED Memory Transfers\n    Transfer Size (Bytes)\tBandwidth(MB/s)\n    33554432\t\t\t6343.5\n\n  Device to Device Bandwidth, 1 Device(s)\n  PINNED Memory Transfers\n    Transfer Size (Bytes)\tBandwidth(MB/s)\n    33554432\t\t\t42554.1\n\n Result = PASS\n\n NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n\n My System:\n\n MacBook Pro (Retina, 15-inch, Late 2013)\n 2.3 GHz Intel Core i7\n 16 GB 1600 MHz DDR3\n NVIDIA GeForce GT 750M 2048 MB\n\n ---\n from System Report > Graphics\n NVIDIA GeForce GT 750M:\n\n   Chipset Model:\tNVIDIA GeForce GT 750M\n   Type:\tGPU\n   Bus:\tPCIe\n   PCIe Lane Width:\tx8\n   VRAM (Total):\t2048 MB\n   Vendor:\tNVIDIA (0x10de)\n   Device ID:\t0x0fe9\n   Revision ID:\t0x00a2\n   ROM Revision:\t3776\n   gMux Version:\t4.0.8 [3.2.8]\n   Displays:\n Color LCD:\n   Display Type:\tRetina LCD\n   Resolution:\t2880 x 1800 Retina\n   Retina:\tYes\n   Pixel Depth:\t32-Bit Color (ARGB8888)\n   Main Display:\tYes\n   Mirror:\tOff\n   Online:\tYes\n   Built-In:\tYes\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#7025 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AABaHJK2XuJg9IHUT3Rb63Nbtahdgr8sks5rWObHgaJpZM4Lrr-s>\n .", "body": "Sounds like you are running out of GPU memory\n\nOn Jan 26, 2017 10:33 AM, \"Atul Acharya\" <notifications@github.com> wrote:\n\n> Hi @yaroslavvb <https://github.com/yaroslavvb> @zheng-xq\n> <https://github.com/zheng-xq>\n>\n> I'm getting this Dst Tensor Not Initialized error.\n>\n> (See my comment (the last one) in this issue elsewhere:\n> aymericdamien/TensorFlow-Examples#38\n> <https://github.com/aymericdamien/TensorFlow-Examples/issues/38>)\n>\n> I'm reproducing the stack trace here in case it helps diagnose the issue:\n>\n> \u25b6 python imagenet_inference.py\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\n> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\n> name: GeForce GT 750M\n> major: 3 minor: 0 memoryClockRate (GHz) 0.9255\n> pciBusID 0000:01:00.0\n> Total memory: 2.00GiB\n> Free memory: 305.92MiB\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0 97.01MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 144.00MiB was 128.00MiB, Chunk State:\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60000 of size 1280\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60500 of size 139520\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82600 of size 512\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82800 of size 1228800\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700bae800 of size 1024\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700baec00 of size 3538944\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0ec00 of size 1536\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0f200 of size 2654208\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197200 of size 1536\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197800 of size 1769472\n> I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701347800 of size 1024\n> I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x701347c00 of size 101725184\n> I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size:\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1024 totalling 2.0KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1536 totalling 3.0KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 139520 totalling 136.2KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1228800 totalling 1.17MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1769472 totalling 1.69MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2654208 totalling 2.53MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3538944 totalling 3.38MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 8.91MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\n> Limit:                   111063040\n> InUse:                     9337856\n> MaxInUse:                  9337856\n> NumAllocs:                      11\n> MaxAllocSize:              3538944\n>\n> W tensorflow/core/common_runtime/bfc_allocator.cc:274] *********___________________________________________________________________________________________\n> W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 144.00MiB.  See logs for memory state.\n> W tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.\n> E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.\n> \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n> Traceback (most recent call last):\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\n>     return fn(*args)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\n>     status, run_metadata)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/contextlib.py\", line 66, in __exit__\n>     next(self.gen)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n>     pywrap_tensorflow.TF_GetCode(status))\n> tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n> \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n>\n> During handling of the above exception, another exception occurred:\n>\n> Traceback (most recent call last):\n>   File \"imagenet_inference.py\", line 19, in <module>\n>     sess.run(init)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\n>     run_metadata_ptr)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n>     feed_dict_string, options, run_metadata)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n>     target_list, options, run_metadata)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n>     raise type(e)(node_def, op, message)\n> tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\n> \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n>\n> Caused by op 'Variable_10/initial_value', defined at:\n>   File \"imagenet_inference.py\", line 16, in <module>\n>     probs = AlexNet(x, feature_extract=False)\n>   File \"/Users/aa/Developer/courses/self_driving_carnd/traffic-signs/CarND-Alexnet-Feature-Extraction/alexnet.py\", line 139, in AlexNet\n>     fc6W = tf.Variable(net_data[\"fc6\"][0])\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n>     expected_shape=expected_shape)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 333, in _init_from_args\n>     initial_value, name=\"initial_value\", dtype=dtype)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\n>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\n>     return constant(v, dtype=dtype, name=name)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 169, in constant\n>     attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File \"/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n>     self._traceback = _extract_stack()\n>\n> InternalError (see above for traceback): Dst tensor is not initialized.\n> \t [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n>\n>\n> Here's deviceQuery successfully reporting seeing the GPU:\n>\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_HOME\n> /usr/local/cuda\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 echo $CUDA_VISIBLE_DEVICES\n>\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./deviceQuery\n> ./deviceQuery Starting...\n>\n>  CUDA Device Query (Runtime API) version (CUDART static linking)\n>\n> Detected 1 CUDA Capable device(s)\n>\n> Device 0: \"GeForce GT 750M\"\n>   CUDA Driver Version / Runtime Version          8.0 / 8.0\n>   CUDA Capability Major/Minor version number:    3.0\n>   Total amount of global memory:                 2048 MBytes (2147024896 bytes)\n>   ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\n>   GPU Max Clock rate:                            926 MHz (0.93 GHz)\n>   Memory Clock rate:                             2508 Mhz\n>   Memory Bus Width:                              128-bit\n>   L2 Cache Size:                                 262144 bytes\n>   Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n>   Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n>   Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n>   Total amount of constant memory:               65536 bytes\n>   Total amount of shared memory per block:       49152 bytes\n>   Total number of registers available per block: 65536\n>   Warp size:                                     32\n>   Maximum number of threads per multiprocessor:  2048\n>   Maximum number of threads per block:           1024\n>   Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n>   Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n>   Maximum memory pitch:                          2147483647 bytes\n>   Texture alignment:                             512 bytes\n>   Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n>   Run time limit on kernels:                     Yes\n>   Integrated GPU sharing Host Memory:            No\n>   Support host page-locked memory mapping:       Yes\n>   Alignment requirement for Surfaces:            Yes\n>   Device has ECC support:                        Disabled\n>   Device supports Unified Addressing (UVA):      Yes\n>   Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n>   Compute Mode:\n>      < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n>\n> deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M\n> Result = PASS\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6\n>  py35 \u25b6 ~ \u25b6 Developer \u276f \u2026 \u276f x86_64 \u276f darwin \u276f release \u25b6 $ \u25b6 ./bandwidthTest\n> [CUDA Bandwidth Test] - Starting...\n> Running on...\n>\n>  Device 0: GeForce GT 750M\n>  Quick Mode\n>\n>  Host to Device Bandwidth, 1 Device(s)\n>  PINNED Memory Transfers\n>    Transfer Size (Bytes)\tBandwidth(MB/s)\n>    33554432\t\t\t3633.5\n>\n>  Device to Host Bandwidth, 1 Device(s)\n>  PINNED Memory Transfers\n>    Transfer Size (Bytes)\tBandwidth(MB/s)\n>    33554432\t\t\t6343.5\n>\n>  Device to Device Bandwidth, 1 Device(s)\n>  PINNED Memory Transfers\n>    Transfer Size (Bytes)\tBandwidth(MB/s)\n>    33554432\t\t\t42554.1\n>\n> Result = PASS\n>\n> NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n>\n> My System:\n>\n> MacBook Pro (Retina, 15-inch, Late 2013)\n> 2.3 GHz Intel Core i7\n> 16 GB 1600 MHz DDR3\n> NVIDIA GeForce GT 750M 2048 MB\n>\n> ---\n> from System Report > Graphics\n> NVIDIA GeForce GT 750M:\n>\n>   Chipset Model:\tNVIDIA GeForce GT 750M\n>   Type:\tGPU\n>   Bus:\tPCIe\n>   PCIe Lane Width:\tx8\n>   VRAM (Total):\t2048 MB\n>   Vendor:\tNVIDIA (0x10de)\n>   Device ID:\t0x0fe9\n>   Revision ID:\t0x00a2\n>   ROM Revision:\t3776\n>   gMux Version:\t4.0.8 [3.2.8]\n>   Displays:\n> Color LCD:\n>   Display Type:\tRetina LCD\n>   Resolution:\t2880 x 1800 Retina\n>   Retina:\tYes\n>   Pixel Depth:\t32-Bit Color (ARGB8888)\n>   Main Display:\tYes\n>   Mirror:\tOff\n>   Online:\tYes\n>   Built-In:\tYes\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7025#issuecomment-275470982>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHJK2XuJg9IHUT3Rb63Nbtahdgr8sks5rWObHgaJpZM4Lrr-s>\n> .\n>\n"}