{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279646674", "html_url": "https://github.com/tensorflow/tensorflow/issues/7469#issuecomment-279646674", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7469", "id": 279646674, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTY0NjY3NA==", "user": {"login": "soloice", "id": 8534653, "node_id": "MDQ6VXNlcjg1MzQ2NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8534653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soloice", "html_url": "https://github.com/soloice", "followers_url": "https://api.github.com/users/soloice/followers", "following_url": "https://api.github.com/users/soloice/following{/other_user}", "gists_url": "https://api.github.com/users/soloice/gists{/gist_id}", "starred_url": "https://api.github.com/users/soloice/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soloice/subscriptions", "organizations_url": "https://api.github.com/users/soloice/orgs", "repos_url": "https://api.github.com/users/soloice/repos", "events_url": "https://api.github.com/users/soloice/events{/privacy}", "received_events_url": "https://api.github.com/users/soloice/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-14T08:59:21Z", "updated_at": "2017-02-14T11:13:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for your reply.<br>\nI used <code>train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)</code> in my code. After I saw your comment, I changed it into <code>slim.learning.create_train_op</code>, but it didn't work either.</p>\n<p>This is my current <a href=\"https://github.com/soloice/mnist-bn/blob/master/mnist_bn.py\">code</a> (I've figured out the canonical way to use a normalizer is to config its parameters in <code>normalizer_params</code>, so I replaced the lambda expression with <code>normalizer_params</code>). The most relevant part might be those in function <code>model()</code>:</p>\n<pre><code>def model():\n    # Create the model\n    x = tf.placeholder(tf.float32, [None, 784])\n    keep_prob = tf.placeholder(tf.float32, [])\n    y_ = tf.placeholder(tf.float32, [None, 10])\n    is_training = tf.placeholder(tf.bool, [])\n\n    x_image = tf.reshape(x, [-1, 28, 28, 1])\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params={'is_training': is_training}):\n        conv1 = slim.conv2d(x_image, 32, [5, 5], scope='conv1')\n        pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')\n        conv2 = slim.conv2d(pool1, 64, [5, 5], scope='conv2')\n        pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')\n        flatten = slim.flatten(pool2)\n        fc = slim.fully_connected(flatten, 1024, scope='fc1')\n        drop = slim.dropout(fc, keep_prob=keep_prob)\n        logits = slim.fully_connected(drop, 10, activation_fn=None, scope='logits')\n\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    cross_entropy = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    if update_ops:\n        updates = tf.group(*update_ops)\n        cross_entropy = control_flow_ops.with_dependencies([updates], cross_entropy)\n\n    # train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n    train_op = slim.learning.create_train_op(cross_entropy, optimizer)\n\n    return {'x': x,\n            'y_': y_,\n            'keep_prob': keep_prob,\n            'is_training': is_training,\n            'train_step': train_op,\n            'accuracy': accuracy,\n            'cross_entropy': cross_entropy}\n</code></pre>\n<p>If <code>FLAGS.phase</code> is \"train\", the model is trained on the training set. But when I try to evaluate the model on the validation set and pass a <code>False</code> to placeholder <code>is_training</code>, the performance on validation set looks weird (it should be about 98% or higher).</p>\n<p>If <code>FLAGS.phase</code> is \"test\", the model restores a pre-trained model from checkpoint and evaluate on the test set. Again, performance is really poor.</p>\n<p>Am I using passing parameters to 'slim.batch_norm' incorrectly? How to use batch normalization for inference?</p>", "body_text": "Thanks for your reply.\nI used train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) in my code. After I saw your comment, I changed it into slim.learning.create_train_op, but it didn't work either.\nThis is my current code (I've figured out the canonical way to use a normalizer is to config its parameters in normalizer_params, so I replaced the lambda expression with normalizer_params). The most relevant part might be those in function model():\ndef model():\n    # Create the model\n    x = tf.placeholder(tf.float32, [None, 784])\n    keep_prob = tf.placeholder(tf.float32, [])\n    y_ = tf.placeholder(tf.float32, [None, 10])\n    is_training = tf.placeholder(tf.bool, [])\n\n    x_image = tf.reshape(x, [-1, 28, 28, 1])\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params={'is_training': is_training}):\n        conv1 = slim.conv2d(x_image, 32, [5, 5], scope='conv1')\n        pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')\n        conv2 = slim.conv2d(pool1, 64, [5, 5], scope='conv2')\n        pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')\n        flatten = slim.flatten(pool2)\n        fc = slim.fully_connected(flatten, 1024, scope='fc1')\n        drop = slim.dropout(fc, keep_prob=keep_prob)\n        logits = slim.fully_connected(drop, 10, activation_fn=None, scope='logits')\n\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    cross_entropy = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    if update_ops:\n        updates = tf.group(*update_ops)\n        cross_entropy = control_flow_ops.with_dependencies([updates], cross_entropy)\n\n    # train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n    train_op = slim.learning.create_train_op(cross_entropy, optimizer)\n\n    return {'x': x,\n            'y_': y_,\n            'keep_prob': keep_prob,\n            'is_training': is_training,\n            'train_step': train_op,\n            'accuracy': accuracy,\n            'cross_entropy': cross_entropy}\n\nIf FLAGS.phase is \"train\", the model is trained on the training set. But when I try to evaluate the model on the validation set and pass a False to placeholder is_training, the performance on validation set looks weird (it should be about 98% or higher).\nIf FLAGS.phase is \"test\", the model restores a pre-trained model from checkpoint and evaluate on the test set. Again, performance is really poor.\nAm I using passing parameters to 'slim.batch_norm' incorrectly? How to use batch normalization for inference?", "body": "Thanks for your reply.\r\nI used `train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)` in my code. After I saw your comment, I changed it into `slim.learning.create_train_op`, but it didn't work either.\r\n\r\nThis is my current [code](https://github.com/soloice/mnist-bn/blob/master/mnist_bn.py) (I've figured out the canonical way to use a normalizer is to config its parameters in `normalizer_params`, so I replaced the lambda expression with `normalizer_params`). The most relevant part might be those in function `model()`:\r\n\r\n```\r\ndef model():\r\n    # Create the model\r\n    x = tf.placeholder(tf.float32, [None, 784])\r\n    keep_prob = tf.placeholder(tf.float32, [])\r\n    y_ = tf.placeholder(tf.float32, [None, 10])\r\n    is_training = tf.placeholder(tf.bool, [])\r\n\r\n    x_image = tf.reshape(x, [-1, 28, 28, 1])\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                        normalizer_fn=slim.batch_norm,\r\n                        normalizer_params={'is_training': is_training}):\r\n        conv1 = slim.conv2d(x_image, 32, [5, 5], scope='conv1')\r\n        pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')\r\n        conv2 = slim.conv2d(pool1, 64, [5, 5], scope='conv2')\r\n        pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')\r\n        flatten = slim.flatten(pool2)\r\n        fc = slim.fully_connected(flatten, 1024, scope='fc1')\r\n        drop = slim.dropout(fc, keep_prob=keep_prob)\r\n        logits = slim.fully_connected(drop, 10, activation_fn=None, scope='logits')\r\n\r\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n    cross_entropy = tf.reduce_mean(\r\n        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))\r\n\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    if update_ops:\r\n        updates = tf.group(*update_ops)\r\n        cross_entropy = control_flow_ops.with_dependencies([updates], cross_entropy)\r\n\r\n    # train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\r\n    train_op = slim.learning.create_train_op(cross_entropy, optimizer)\r\n\r\n    return {'x': x,\r\n            'y_': y_,\r\n            'keep_prob': keep_prob,\r\n            'is_training': is_training,\r\n            'train_step': train_op,\r\n            'accuracy': accuracy,\r\n            'cross_entropy': cross_entropy}\r\n```\r\n\r\nIf `FLAGS.phase` is \"train\", the model is trained on the training set. But when I try to evaluate the model on the validation set and pass a `False` to placeholder `is_training`, the performance on validation set looks weird (it should be about 98% or higher).\r\n\r\nIf `FLAGS.phase` is \"test\", the model restores a pre-trained model from checkpoint and evaluate on the test set. Again, performance is really poor.\r\n\r\nAm I using passing parameters to 'slim.batch_norm' incorrectly? How to use batch normalization for inference?"}