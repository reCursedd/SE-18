{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/412338285", "html_url": "https://github.com/tensorflow/tensorflow/issues/3460#issuecomment-412338285", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3460", "id": 412338285, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjMzODI4NQ==", "user": {"login": "fenghou1st", "id": 8263678, "node_id": "MDQ6VXNlcjgyNjM2Nzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/8263678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fenghou1st", "html_url": "https://github.com/fenghou1st", "followers_url": "https://api.github.com/users/fenghou1st/followers", "following_url": "https://api.github.com/users/fenghou1st/following{/other_user}", "gists_url": "https://api.github.com/users/fenghou1st/gists{/gist_id}", "starred_url": "https://api.github.com/users/fenghou1st/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fenghou1st/subscriptions", "organizations_url": "https://api.github.com/users/fenghou1st/orgs", "repos_url": "https://api.github.com/users/fenghou1st/repos", "events_url": "https://api.github.com/users/fenghou1st/events{/privacy}", "received_events_url": "https://api.github.com/users/fenghou1st/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-12T12:03:55Z", "updated_at": "2018-08-12T12:58:15Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4450528\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/marcoadurno\">@marcoadurno</a><br>\nAlthough things have been going on for a long time, there is still something to explain.</p>\n<p><strong>The restoring operation should be executed only once, and before any inference.</strong></p>\n<p>In your code this operation will be executed every time <code>_infer()</code> is invoked, and if there are multiple layers (for example, your model has 3 dense layers), you can't restore the variables from the very beginning, you can only restore them after all layers (thus all variables) have been created. but when the variables are created, they have been calculated at least once, so the first batch will get the wrong result.</p>\n<p>The way to solve this problem is restoring the variables in other place.</p>\n<ol>\n<li>\n<p>In the training phase, create ema instance and track moving averages:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">track_params_averages</span>():\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Track the moving averages of parameters.</span>\n<span class=\"pl-s\">    Must be invoked after `infer()` and before `train()`.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    :return:</span>\n<span class=\"pl-s\">            ema:                    `tf.train.ExponentialMovingAverage`</span>\n<span class=\"pl-s\">            params_averages_op:     operator that tracking averages</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    ema <span class=\"pl-k\">=</span> tf.train.ExponentialMovingAverage(<span class=\"pl-c1\">FLAGS</span>.params_moving_average_decay)\n    params <span class=\"pl-k\">=</span> tf.trainable_variables()\n    params_averages_op <span class=\"pl-k\">=</span> ema.apply(params)\n    <span class=\"pl-k\">return</span> ema, params_averages_op\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">model_fn_train</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">params</span>):\n    model <span class=\"pl-k\">=</span> Model(params)\n    logits <span class=\"pl-k\">=</span> model.infer(features)\n\n    _, params_averages_op <span class=\"pl-k\">=</span> track_params_averages()\n\n    <span class=\"pl-c1\">...</span>.\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Note that the UPDATE_OPS must be executed before applying gradients,</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> and tracking variables averages must be done after applying gradients.</span>\n    update_ops <span class=\"pl-k\">=</span> tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)\n    <span class=\"pl-k\">with</span> tf.control_dependencies(update_ops):\n        gradients <span class=\"pl-k\">=</span> opt.compute_gradients(total_loss)\n        apply_gradients_op <span class=\"pl-k\">=</span> opt.apply_gradients(gradients, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step)\n\n    <span class=\"pl-k\">with</span> tf.control_dependencies([apply_gradients_op]):\n        params_averages_op <span class=\"pl-k\">=</span> tf.group(params_averages_op)\n\n    <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(\n        <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>tf.estimator.ModeKeys.<span class=\"pl-c1\">TRAIN</span>, <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss, <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op)</pre></div>\n</li>\n<li>\n<p>In the evaluation phase, create a SessionRunHook to restore the averages:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">RestoreParametersAverageValues</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">train</span>.<span class=\"pl-e\">SessionRunHook</span>):\n   <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">   Replace parameters with their moving averages.</span>\n<span class=\"pl-s\">   This operation should be executed only once, and before any inference.</span>\n<span class=\"pl-s\">   <span class=\"pl-pds\">\"\"\"</span></span>\n   <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">ema</span>):\n       <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">       :param ema:         tf.train.ExponentialMovingAverage</span>\n<span class=\"pl-s\">       <span class=\"pl-pds\">\"\"\"</span></span>\n       <span class=\"pl-c1\">super</span>(RestoreParametersAverageValues, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n       <span class=\"pl-c1\">self</span>._ema <span class=\"pl-k\">=</span> ema\n       <span class=\"pl-c1\">self</span>._restore_ops <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n   <span class=\"pl-k\">def</span> <span class=\"pl-en\">begin</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n       <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Create restoring operations before the graph been finalized. <span class=\"pl-pds\">\"\"\"</span></span>\n       ema_variables <span class=\"pl-k\">=</span> tf.moving_average_variables()\n       <span class=\"pl-c1\">self</span>._restore_ops <span class=\"pl-k\">=</span> [tf.assign(x, <span class=\"pl-c1\">self</span>._ema.average(x)) <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> ema_variables]\n\n   <span class=\"pl-k\">def</span> <span class=\"pl-en\">after_create_session</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">session</span>, <span class=\"pl-smi\">coord</span>):\n       <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Restore the parameters right after the session been created. <span class=\"pl-pds\">\"\"\"</span></span>\n       session.run(<span class=\"pl-c1\">self</span>._restore_ops)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">model_fn_eval</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">params</span>):\n   model <span class=\"pl-k\">=</span> Model(params)\n   logits <span class=\"pl-k\">=</span> model.infer(features)\n\n   ema, _ <span class=\"pl-k\">=</span> track_params_averages()\n\n   <span class=\"pl-c1\">...</span>\n\n   <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(\n       <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>tf.estimator.ModeKeys.<span class=\"pl-c1\">PREDICT</span>,\n       <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>{<span class=\"pl-c1\">...</span>},\n       <span class=\"pl-v\">prediction_hooks</span><span class=\"pl-k\">=</span>[RestoreParametersAverageValues(ema)],\n   )</pre></div>\n</li>\n</ol>", "body_text": "@marcoadurno\nAlthough things have been going on for a long time, there is still something to explain.\nThe restoring operation should be executed only once, and before any inference.\nIn your code this operation will be executed every time _infer() is invoked, and if there are multiple layers (for example, your model has 3 dense layers), you can't restore the variables from the very beginning, you can only restore them after all layers (thus all variables) have been created. but when the variables are created, they have been calculated at least once, so the first batch will get the wrong result.\nThe way to solve this problem is restoring the variables in other place.\n\n\nIn the training phase, create ema instance and track moving averages:\ndef track_params_averages():\n    \"\"\"\n    Track the moving averages of parameters.\n    Must be invoked after `infer()` and before `train()`.\n\n    :return:\n            ema:                    `tf.train.ExponentialMovingAverage`\n            params_averages_op:     operator that tracking averages\n    \"\"\"\n    ema = tf.train.ExponentialMovingAverage(FLAGS.params_moving_average_decay)\n    params = tf.trainable_variables()\n    params_averages_op = ema.apply(params)\n    return ema, params_averages_op\n\n\ndef model_fn_train(features, labels, params):\n    model = Model(params)\n    logits = model.infer(features)\n\n    _, params_averages_op = track_params_averages()\n\n    ....\n\n    # Note that the UPDATE_OPS must be executed before applying gradients,\n    # and tracking variables averages must be done after applying gradients.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        gradients = opt.compute_gradients(total_loss)\n        apply_gradients_op = opt.apply_gradients(gradients, global_step=global_step)\n\n    with tf.control_dependencies([apply_gradients_op]):\n        params_averages_op = tf.group(params_averages_op)\n\n    return tf.estimator.EstimatorSpec(\n        mode=tf.estimator.ModeKeys.TRAIN, loss=loss, train_op=train_op)\n\n\nIn the evaluation phase, create a SessionRunHook to restore the averages:\nclass RestoreParametersAverageValues(tf.train.SessionRunHook):\n   \"\"\"\n   Replace parameters with their moving averages.\n   This operation should be executed only once, and before any inference.\n   \"\"\"\n   def __init__(self, ema):\n       \"\"\"\n       :param ema:         tf.train.ExponentialMovingAverage\n       \"\"\"\n       super(RestoreParametersAverageValues, self).__init__()\n       self._ema = ema\n       self._restore_ops = None\n\n   def begin(self):\n       \"\"\" Create restoring operations before the graph been finalized. \"\"\"\n       ema_variables = tf.moving_average_variables()\n       self._restore_ops = [tf.assign(x, self._ema.average(x)) for x in ema_variables]\n\n   def after_create_session(self, session, coord):\n       \"\"\" Restore the parameters right after the session been created. \"\"\"\n       session.run(self._restore_ops)\n\n\ndef model_fn_eval(features, labels, params):\n   model = Model(params)\n   logits = model.infer(features)\n\n   ema, _ = track_params_averages()\n\n   ...\n\n   return tf.estimator.EstimatorSpec(\n       mode=tf.estimator.ModeKeys.PREDICT,\n       predictions={...},\n       prediction_hooks=[RestoreParametersAverageValues(ema)],\n   )", "body": "@marcoadurno \r\nAlthough things have been going on for a long time, there is still something to explain.\r\n\r\n**The restoring operation should be executed only once, and before any inference.**\r\n\r\nIn your code this operation will be executed every time `_infer()` is invoked, and if there are multiple layers (for example, your model has 3 dense layers), you can't restore the variables from the very beginning, you can only restore them after all layers (thus all variables) have been created. but when the variables are created, they have been calculated at least once, so the first batch will get the wrong result.\r\n\r\nThe way to solve this problem is restoring the variables in other place.\r\n\r\n1. In the training phase, create ema instance and track moving averages:\r\n    ```python\r\n    def track_params_averages():\r\n        \"\"\"\r\n        Track the moving averages of parameters.\r\n        Must be invoked after `infer()` and before `train()`.\r\n\r\n        :return:\r\n                ema:                    `tf.train.ExponentialMovingAverage`\r\n                params_averages_op:     operator that tracking averages\r\n        \"\"\"\r\n        ema = tf.train.ExponentialMovingAverage(FLAGS.params_moving_average_decay)\r\n        params = tf.trainable_variables()\r\n        params_averages_op = ema.apply(params)\r\n        return ema, params_averages_op\r\n\r\n\r\n    def model_fn_train(features, labels, params):\r\n        model = Model(params)\r\n        logits = model.infer(features)\r\n\r\n        _, params_averages_op = track_params_averages()\r\n\r\n        ....\r\n\r\n        # Note that the UPDATE_OPS must be executed before applying gradients,\r\n        # and tracking variables averages must be done after applying gradients.\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        with tf.control_dependencies(update_ops):\r\n            gradients = opt.compute_gradients(total_loss)\r\n            apply_gradients_op = opt.apply_gradients(gradients, global_step=global_step)\r\n\r\n        with tf.control_dependencies([apply_gradients_op]):\r\n            params_averages_op = tf.group(params_averages_op)\r\n\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=tf.estimator.ModeKeys.TRAIN, loss=loss, train_op=train_op)\r\n    ```\r\n\r\n1. In the evaluation phase, create a SessionRunHook to restore the averages:\r\n     ```python\r\n    class RestoreParametersAverageValues(tf.train.SessionRunHook):\r\n        \"\"\"\r\n        Replace parameters with their moving averages.\r\n        This operation should be executed only once, and before any inference.\r\n        \"\"\"\r\n        def __init__(self, ema):\r\n            \"\"\"\r\n            :param ema:         tf.train.ExponentialMovingAverage\r\n            \"\"\"\r\n            super(RestoreParametersAverageValues, self).__init__()\r\n            self._ema = ema\r\n            self._restore_ops = None\r\n\r\n        def begin(self):\r\n            \"\"\" Create restoring operations before the graph been finalized. \"\"\"\r\n            ema_variables = tf.moving_average_variables()\r\n            self._restore_ops = [tf.assign(x, self._ema.average(x)) for x in ema_variables]\r\n\r\n        def after_create_session(self, session, coord):\r\n            \"\"\" Restore the parameters right after the session been created. \"\"\"\r\n            session.run(self._restore_ops)\r\n\r\n\r\n    def model_fn_eval(features, labels, params):\r\n        model = Model(params)\r\n        logits = model.infer(features)\r\n\r\n        ema, _ = track_params_averages()\r\n\r\n        ...\r\n\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=tf.estimator.ModeKeys.PREDICT,\r\n            predictions={...},\r\n            prediction_hooks=[RestoreParametersAverageValues(ema)],\r\n        )\r\n    ```"}