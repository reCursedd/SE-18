{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341158862", "html_url": "https://github.com/tensorflow/tensorflow/issues/14155#issuecomment-341158862", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14155", "id": 341158862, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTE1ODg2Mg==", "user": {"login": "abieler", "id": 7083141, "node_id": "MDQ6VXNlcjcwODMxNDE=", "avatar_url": "https://avatars1.githubusercontent.com/u/7083141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abieler", "html_url": "https://github.com/abieler", "followers_url": "https://api.github.com/users/abieler/followers", "following_url": "https://api.github.com/users/abieler/following{/other_user}", "gists_url": "https://api.github.com/users/abieler/gists{/gist_id}", "starred_url": "https://api.github.com/users/abieler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abieler/subscriptions", "organizations_url": "https://api.github.com/users/abieler/orgs", "repos_url": "https://api.github.com/users/abieler/repos", "events_url": "https://api.github.com/users/abieler/events{/privacy}", "received_events_url": "https://api.github.com/users/abieler/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-01T16:26:42Z", "updated_at": "2017-11-01T16:27:53Z", "author_association": "NONE", "body_html": "<p>Ok, condensed it down to one script. There is only one checkpoint. So no dir crawling.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> matplotlib.pyplot <span class=\"pl-k\">as</span> plt\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">rebuild_model</span>(<span class=\"pl-smi\">modelname</span>):\n    gpu_options <span class=\"pl-k\">=</span> tf.GPUOptions(<span class=\"pl-v\">per_process_gpu_memory_fraction</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.25</span>)\n    config <span class=\"pl-k\">=</span> tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>gpu_options)\n    <span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n        saver <span class=\"pl-k\">=</span> tf.train.import_meta_graph(modelname <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.meta<span class=\"pl-pds\">\"</span></span>)\n        saver.restore(sess, tf.train.latest_checkpoint(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>./<span class=\"pl-pds\">\"</span></span>))\n        a <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">+</span> np.random.randn()\n        \n\nnVars <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span> \nnExamples <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nnClasses <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\nnEpochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span> \n\nX <span class=\"pl-k\">=</span> np.random.randn(nExamples, nVars)\ny <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">0</span>, nClasses, nExamples)\n\nnHidden1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span> \nnHidden2 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Build model</span>\ngraph <span class=\"pl-k\">=</span> tf.Graph()\n<span class=\"pl-k\">with</span> graph.as_default():\n    x_tf <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, nVars), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x_tf<span class=\"pl-pds\">\"</span></span>)\n    y_tf <span class=\"pl-k\">=</span> tf.one_hot(y, nClasses, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y_tf<span class=\"pl-pds\">\"</span></span>)\n\n    w1 <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([nVars, nHidden1]))\n    b1 <span class=\"pl-k\">=</span> tf.Variable(tf.zeros(nHidden1))\n    a1 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(x_tf, w1) <span class=\"pl-k\">+</span> b1)\n\n    w2 <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([nHidden1, nHidden2]))\n    b2 <span class=\"pl-k\">=</span> tf.Variable(tf.zeros(nHidden2))\n    a2 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(a1, w2) <span class=\"pl-k\">+</span> b2)\n\n    w_out <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([nHidden2, nClasses]))\n    b_out <span class=\"pl-k\">=</span> tf.Variable(tf.zeros(nClasses))\n    logits <span class=\"pl-k\">=</span> tf.add(tf.matmul(a2, w_out), b_out, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>logits<span class=\"pl-pds\">\"</span></span>)\n    yhat <span class=\"pl-k\">=</span> tf.nn.softmax(logits, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>yhat<span class=\"pl-pds\">\"</span></span>)\n\n\n    loss <span class=\"pl-k\">=</span> tf.reduce_mean(\n           tf.nn.softmax_cross_entropy_with_logits(\n           <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>y_tf, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits))\n\n    optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.05</span>).minimize(loss)\n    saver <span class=\"pl-k\">=</span> tf.train.Saver()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Train model</span>\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>graph) <span class=\"pl-k\">as</span> sess:\n    tf.global_variables_initializer().run()\n\n    <span class=\"pl-k\">for</span> step <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(nEpochs<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>):\n        _, current_loss, prediction, logi <span class=\"pl-k\">=</span> sess.run([optimizer, loss, yhat, logits], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x_tf:X})\n        <span class=\"pl-k\">if</span> (step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>):\n            accuracy <span class=\"pl-k\">=</span> np.mean(tf.equal(tf.argmax(prediction, <span class=\"pl-c1\">1</span>), y).eval())\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span> - <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span>Loss: <span class=\"pl-c1\">%.4f</span><span class=\"pl-cce\">\\t</span>Accuracy: <span class=\"pl-c1\">%.4f</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span>(step, current_loss, accuracy))\n\n    save_path <span class=\"pl-k\">=</span> saver.save(sess, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>feedforward.ckpt<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> loop </span>\ndt <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n    t0 <span class=\"pl-k\">=</span> time.time()\n    result <span class=\"pl-k\">=</span> rebuild_model(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>feedforward.ckpt<span class=\"pl-pds\">\"</span></span>)\n    t1 <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-c1\">print</span>(t1 <span class=\"pl-k\">-</span> t0)\n    dt.append(t1 <span class=\"pl-k\">-</span> t0)\n\n\nplt.figure()\nplt.plot(dt)\nplt.ylabel(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>execution time [s]<span class=\"pl-pds\">\"</span></span>)\nplt.xlabel(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>iteration<span class=\"pl-pds\">\"</span></span>)\nplt.title(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>saver.restore()<span class=\"pl-pds\">\"</span></span>)\n\nplt.show()\n<span class=\"pl-c1\">input</span>()\n</pre></div>", "body_text": "Ok, condensed it down to one script. There is only one checkpoint. So no dir crawling.\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef rebuild_model(modelname):\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.25)\n    config = tf.ConfigProto(gpu_options=gpu_options)\n    with tf.Session(config=config) as sess:\n        saver = tf.train.import_meta_graph(modelname + \".meta\")\n        saver.restore(sess, tf.train.latest_checkpoint(\"./\"))\n        a = 1 + np.random.randn()\n        \n\nnVars = 10 \nnExamples = 100\nnClasses = 2\nnEpochs = 100 \n\nX = np.random.randn(nExamples, nVars)\ny = np.random.randint(0, nClasses, nExamples)\n\nnHidden1 = 50 \nnHidden2 = 20\n\n\n# Build model\ngraph = tf.Graph()\nwith graph.as_default():\n    x_tf = tf.placeholder(tf.float32, shape=(None, nVars), name=\"x_tf\")\n    y_tf = tf.one_hot(y, nClasses, name=\"y_tf\")\n\n    w1 = tf.Variable(tf.random_normal([nVars, nHidden1]))\n    b1 = tf.Variable(tf.zeros(nHidden1))\n    a1 = tf.nn.relu(tf.matmul(x_tf, w1) + b1)\n\n    w2 = tf.Variable(tf.random_normal([nHidden1, nHidden2]))\n    b2 = tf.Variable(tf.zeros(nHidden2))\n    a2 = tf.nn.relu(tf.matmul(a1, w2) + b2)\n\n    w_out = tf.Variable(tf.random_normal([nHidden2, nClasses]))\n    b_out = tf.Variable(tf.zeros(nClasses))\n    logits = tf.add(tf.matmul(a2, w_out), b_out, name=\"logits\")\n    yhat = tf.nn.softmax(logits, name=\"yhat\")\n\n\n    loss = tf.reduce_mean(\n           tf.nn.softmax_cross_entropy_with_logits(\n           labels=y_tf, logits=logits))\n\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n    saver = tf.train.Saver()\n\n# Train model\nwith tf.Session(graph=graph) as sess:\n    tf.global_variables_initializer().run()\n\n    for step in range(nEpochs+1):\n        _, current_loss, prediction, logi = sess.run([optimizer, loss, yhat, logits], feed_dict={x_tf:X})\n        if (step % 10 == 0):\n            accuracy = np.mean(tf.equal(tf.argmax(prediction, 1), y).eval())\n            print(\" - %i\\tLoss: %.4f\\tAccuracy: %.4f\" %(step, current_loss, accuracy))\n\n    save_path = saver.save(sess, \"feedforward.ckpt\")\n\n\n# loop \ndt = []\nfor i in range(100):\n    t0 = time.time()\n    result = rebuild_model(\"feedforward.ckpt\")\n    t1 = time.time()\n    print(t1 - t0)\n    dt.append(t1 - t0)\n\n\nplt.figure()\nplt.plot(dt)\nplt.ylabel(\"execution time [s]\")\nplt.xlabel(\"iteration\")\nplt.title(\"saver.restore()\")\n\nplt.show()\ninput()", "body": "Ok, condensed it down to one script. There is only one checkpoint. So no dir crawling.\r\n\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\ndef rebuild_model(modelname):\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.25)\r\n    config = tf.ConfigProto(gpu_options=gpu_options)\r\n    with tf.Session(config=config) as sess:\r\n        saver = tf.train.import_meta_graph(modelname + \".meta\")\r\n        saver.restore(sess, tf.train.latest_checkpoint(\"./\"))\r\n        a = 1 + np.random.randn()\r\n        \r\n\r\nnVars = 10 \r\nnExamples = 100\r\nnClasses = 2\r\nnEpochs = 100 \r\n\r\nX = np.random.randn(nExamples, nVars)\r\ny = np.random.randint(0, nClasses, nExamples)\r\n\r\nnHidden1 = 50 \r\nnHidden2 = 20\r\n\r\n\r\n# Build model\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    x_tf = tf.placeholder(tf.float32, shape=(None, nVars), name=\"x_tf\")\r\n    y_tf = tf.one_hot(y, nClasses, name=\"y_tf\")\r\n\r\n    w1 = tf.Variable(tf.random_normal([nVars, nHidden1]))\r\n    b1 = tf.Variable(tf.zeros(nHidden1))\r\n    a1 = tf.nn.relu(tf.matmul(x_tf, w1) + b1)\r\n\r\n    w2 = tf.Variable(tf.random_normal([nHidden1, nHidden2]))\r\n    b2 = tf.Variable(tf.zeros(nHidden2))\r\n    a2 = tf.nn.relu(tf.matmul(a1, w2) + b2)\r\n\r\n    w_out = tf.Variable(tf.random_normal([nHidden2, nClasses]))\r\n    b_out = tf.Variable(tf.zeros(nClasses))\r\n    logits = tf.add(tf.matmul(a2, w_out), b_out, name=\"logits\")\r\n    yhat = tf.nn.softmax(logits, name=\"yhat\")\r\n\r\n\r\n    loss = tf.reduce_mean(\r\n           tf.nn.softmax_cross_entropy_with_logits(\r\n           labels=y_tf, logits=logits))\r\n\r\n    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n    saver = tf.train.Saver()\r\n\r\n# Train model\r\nwith tf.Session(graph=graph) as sess:\r\n    tf.global_variables_initializer().run()\r\n\r\n    for step in range(nEpochs+1):\r\n        _, current_loss, prediction, logi = sess.run([optimizer, loss, yhat, logits], feed_dict={x_tf:X})\r\n        if (step % 10 == 0):\r\n            accuracy = np.mean(tf.equal(tf.argmax(prediction, 1), y).eval())\r\n            print(\" - %i\\tLoss: %.4f\\tAccuracy: %.4f\" %(step, current_loss, accuracy))\r\n\r\n    save_path = saver.save(sess, \"feedforward.ckpt\")\r\n\r\n\r\n# loop \r\ndt = []\r\nfor i in range(100):\r\n    t0 = time.time()\r\n    result = rebuild_model(\"feedforward.ckpt\")\r\n    t1 = time.time()\r\n    print(t1 - t0)\r\n    dt.append(t1 - t0)\r\n\r\n\r\nplt.figure()\r\nplt.plot(dt)\r\nplt.ylabel(\"execution time [s]\")\r\nplt.xlabel(\"iteration\")\r\nplt.title(\"saver.restore()\")\r\n\r\nplt.show()\r\ninput()\r\n\r\n```"}