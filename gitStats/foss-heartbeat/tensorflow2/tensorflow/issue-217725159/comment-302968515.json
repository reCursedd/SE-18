{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/302968515", "html_url": "https://github.com/tensorflow/tensorflow/issues/8787#issuecomment-302968515", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8787", "id": 302968515, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjk2ODUxNQ==", "user": {"login": "ahundt", "id": 55744, "node_id": "MDQ6VXNlcjU1NzQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/55744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahundt", "html_url": "https://github.com/ahundt", "followers_url": "https://api.github.com/users/ahundt/followers", "following_url": "https://api.github.com/users/ahundt/following{/other_user}", "gists_url": "https://api.github.com/users/ahundt/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahundt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahundt/subscriptions", "organizations_url": "https://api.github.com/users/ahundt/orgs", "repos_url": "https://api.github.com/users/ahundt/repos", "events_url": "https://api.github.com/users/ahundt/events{/privacy}", "received_events_url": "https://api.github.com/users/ahundt/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-21T22:44:41Z", "updated_at": "2017-05-21T22:55:40Z", "author_association": "NONE", "body_html": "<p>I'm learning as I go but I've been looking into this a bit and here are my first thoughts.</p>\n<p>These are from upstream keras, but these two commits/prs take care of a couple of the key parameterizations necessary for TFRecords to work:</p>\n<ul>\n<li><a href=\"https://github.com/fchollet/keras/commit/e90b0713f2e86f9f540b57b4306a77e3f696213b\">tensors feeding models</a> - numpy arrays no longer required, merged some time ago</li>\n<li><a href=\"https://github.com/fchollet/keras/pull/6693\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/keras-team/keras/pull/6693/hovercard\">kwargs passed to session.run</a> - in progress at time of writing</li>\n</ul>\n<p>One possible design is to create a <code>TFRecordInput()</code> Keras layer that wraps <code>data_flow_ops.RecordInput</code> which might be able to fit well with the <a href=\"https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/preprocessing.py#L360\">tf_cnn_benchmarks suggested high performance design</a>. Then calls like <code>model.fit()</code> could get their data from those input tensors.</p>\n<p><strong>Two possible cases I've considered for loading tensors</strong></p>\n<p><code>Input()</code> or <code>TFRecordInput()</code> case:<br>\nSomething like <a href=\"https://github.com/farizrahman4u/keras-contrib/blob/master/keras_contrib/applications/densenet.py#L126\">these checks in the densenet model for the variable input_tensor</a> might be possible to fold into the Input, or to put on a backend.</p>\n<p><code>model.fit()</code> case:<br>\n<code>model.fit</code> is after compile which instantiates the tf graph so this might be untenable. Nonetheless, in an ideal world one could create <code>data_flow_ops.RecordInput()</code>, loading/preprocessing ops, then those tensors could be fed to <code>model.fit()</code>, which could check the input type, and then the tensors could be passed through the keras backend to the TensorFlow backend.</p>\n<p><strong>Preprocessing Layers</strong></p>\n<p>TensorFlow has preprocessing layers and it might be possible to simply use them to create <a href=\"https://github.com/fchollet/keras/issues/6655\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/6655/hovercard\">Keras Preprocessing/Augmentation Layers</a>. However there would ideally be some mechanism by which they can be separated from the Keras model when desired.</p>\n<p>Hopefully none of this is too far behind tf's ongoing changes, any suggestions/feedback?</p>", "body_text": "I'm learning as I go but I've been looking into this a bit and here are my first thoughts.\nThese are from upstream keras, but these two commits/prs take care of a couple of the key parameterizations necessary for TFRecords to work:\n\ntensors feeding models - numpy arrays no longer required, merged some time ago\nkwargs passed to session.run - in progress at time of writing\n\nOne possible design is to create a TFRecordInput() Keras layer that wraps data_flow_ops.RecordInput which might be able to fit well with the tf_cnn_benchmarks suggested high performance design. Then calls like model.fit() could get their data from those input tensors.\nTwo possible cases I've considered for loading tensors\nInput() or TFRecordInput() case:\nSomething like these checks in the densenet model for the variable input_tensor might be possible to fold into the Input, or to put on a backend.\nmodel.fit() case:\nmodel.fit is after compile which instantiates the tf graph so this might be untenable. Nonetheless, in an ideal world one could create data_flow_ops.RecordInput(), loading/preprocessing ops, then those tensors could be fed to model.fit(), which could check the input type, and then the tensors could be passed through the keras backend to the TensorFlow backend.\nPreprocessing Layers\nTensorFlow has preprocessing layers and it might be possible to simply use them to create Keras Preprocessing/Augmentation Layers. However there would ideally be some mechanism by which they can be separated from the Keras model when desired.\nHopefully none of this is too far behind tf's ongoing changes, any suggestions/feedback?", "body": "I'm learning as I go but I've been looking into this a bit and here are my first thoughts.\r\n\r\nThese are from upstream keras, but these two commits/prs take care of a couple of the key parameterizations necessary for TFRecords to work:\r\n- [tensors feeding models](https://github.com/fchollet/keras/commit/e90b0713f2e86f9f540b57b4306a77e3f696213b) - numpy arrays no longer required, merged some time ago\r\n- [kwargs passed to session.run](https://github.com/fchollet/keras/pull/6693) - in progress at time of writing\r\n\r\nOne possible design is to create a `TFRecordInput()` Keras layer that wraps `data_flow_ops.RecordInput` which might be able to fit well with the [tf_cnn_benchmarks suggested high performance design](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/preprocessing.py#L360). Then calls like `model.fit()` could get their data from those input tensors.\r\n\r\n**Two possible cases I've considered for loading tensors**\r\n\r\n`Input()` or `TFRecordInput()` case:\r\nSomething like [these checks in the densenet model for the variable input_tensor](https://github.com/farizrahman4u/keras-contrib/blob/master/keras_contrib/applications/densenet.py#L126) might be possible to fold into the Input, or to put on a backend. \r\n\r\n`model.fit()` case:\r\n`model.fit` is after compile which instantiates the tf graph so this might be untenable. Nonetheless, in an ideal world one could create `data_flow_ops.RecordInput()`, loading/preprocessing ops, then those tensors could be fed to `model.fit()`, which could check the input type, and then the tensors could be passed through the keras backend to the TensorFlow backend.\r\n\r\n**Preprocessing Layers**\r\n\r\nTensorFlow has preprocessing layers and it might be possible to simply use them to create [Keras Preprocessing/Augmentation Layers](https://github.com/fchollet/keras/issues/6655). However there would ideally be some mechanism by which they can be separated from the Keras model when desired.\r\n\r\nHopefully none of this is too far behind tf's ongoing changes, any suggestions/feedback?"}