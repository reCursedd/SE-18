{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8311", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8311/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8311/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8311/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8311", "id": 213562416, "node_id": "MDU6SXNzdWUyMTM1NjI0MTY=", "number": 8311, "title": "Different output using CudnnGRU vs GRUCell", "user": {"login": "eddiepierce", "id": 12822460, "node_id": "MDQ6VXNlcjEyODIyNDYw", "avatar_url": "https://avatars3.githubusercontent.com/u/12822460?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eddiepierce", "html_url": "https://github.com/eddiepierce", "followers_url": "https://api.github.com/users/eddiepierce/followers", "following_url": "https://api.github.com/users/eddiepierce/following{/other_user}", "gists_url": "https://api.github.com/users/eddiepierce/gists{/gist_id}", "starred_url": "https://api.github.com/users/eddiepierce/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eddiepierce/subscriptions", "organizations_url": "https://api.github.com/users/eddiepierce/orgs", "repos_url": "https://api.github.com/users/eddiepierce/repos", "events_url": "https://api.github.com/users/eddiepierce/events{/privacy}", "received_events_url": "https://api.github.com/users/eddiepierce/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-03-11T21:59:07Z", "updated_at": "2017-06-16T20:52:02Z", "closed_at": "2017-06-16T20:52:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Operating System: Arch Linux</p>\n<p>Installed version of CUDA and cuDNN:  libcudart.so.8.0.44, libcudnn.so.5.1.5</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>): 57e40363eb40a692f7c5dfea3f53031a52024321</li>\n<li>The output of <code>bazel version</code>: 0.4.4</li>\n</ol>\n<p>I set x = 1, previous_h = 0, all weights and biases = 0, output should be 0. Traditional tensorflow GRU returns 0, but CudnnGRU returns 0.20482421</p>\n<p>NOTE: need to use GPU for CudnnGRU to work properly</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n\n# GRU with x=1, h_t_minus_1=0, all weights and biases = 0; output (h) should equal 0\n# u = sig(Wx + Rh + b)\n# r = sig(Wx + Rh + b)\n# c = tanh(Wx + r(Rh + b) + b)\n# h = (1-u)c + uh\n\n\n# u = sig(0*1 + 0*0 + 0)\n# u = .5\n# r = sig(0*1 + 0*0 + 0)\n# r = .5\n# c = tanh(0*1 + .5(0*0 + 0) + 0)\n# c = 0\n# h = (1-.5)*0 + .5*0\n# h = 0\n\n\n\nbatch_size = 1\nn_time = 1\nx_depth = 1\nn_cell = 1\n\nx = tf.ones([batch_size, n_time, x_depth])\n\ny_tf, _ = tf.nn.dynamic_rnn(tf.contrib.rnn.GRUCell(n_cell), x, dtype=tf.float32)\nparam_tf = [v.assign(tf.zeros(tf.shape(v))) for v in tf.trainable_variables()] # y_tf uses these zeroed out params\n\nn_layer = 1\nrnn_cudnn = cudnn_rnn_ops.CudnnGRU(n_layer, n_cell, x_depth, 'skip_input')\nparam_cudnn = tf.Variable(tf.zeros([rnn_cudnn.params_size()]), validate_shape=False)\ny_cudnn, state_cudnn = rnn_cudnn(tf.transpose(x, [1,0,2]), tf.zeros([n_layer, batch_size, n_cell]), param_cudnn)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    # NOTE: cudnn has more params because they use twice as many biases\n    print('y_tf: {}\\ny_cudnn: {}\\nparam_tf\\n{}\\n\\nparam_cudnn\\n{}\\n\\n'.format(*sess.run([y_tf, y_cudnn, param_tf, param_cudnn])))\n\n\n\n# Output:\n# y_tf: [[[ 0.]]]\n# y_cudnn: [[[ 0.20482421]]]\n# param_tf\n# [array([[ 0.,  0.],\n#        [ 0.,  0.]], dtype=float32), array([ 0.,  0.], dtype=float32), array([[ 0.],\n#        [ 0.]], dtype=float32), array([ 0.], dtype=float32)]\n\n# param_cudnn\n# [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n</code></pre>", "body_text": "Operating System: Arch Linux\nInstalled version of CUDA and cuDNN:  libcudart.so.8.0.44, libcudnn.so.5.1.5\n\nThe commit hash (git rev-parse HEAD): 57e40363eb40a692f7c5dfea3f53031a52024321\nThe output of bazel version: 0.4.4\n\nI set x = 1, previous_h = 0, all weights and biases = 0, output should be 0. Traditional tensorflow GRU returns 0, but CudnnGRU returns 0.20482421\nNOTE: need to use GPU for CudnnGRU to work properly\nimport tensorflow as tf\nfrom tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n\n# GRU with x=1, h_t_minus_1=0, all weights and biases = 0; output (h) should equal 0\n# u = sig(Wx + Rh + b)\n# r = sig(Wx + Rh + b)\n# c = tanh(Wx + r(Rh + b) + b)\n# h = (1-u)c + uh\n\n\n# u = sig(0*1 + 0*0 + 0)\n# u = .5\n# r = sig(0*1 + 0*0 + 0)\n# r = .5\n# c = tanh(0*1 + .5(0*0 + 0) + 0)\n# c = 0\n# h = (1-.5)*0 + .5*0\n# h = 0\n\n\n\nbatch_size = 1\nn_time = 1\nx_depth = 1\nn_cell = 1\n\nx = tf.ones([batch_size, n_time, x_depth])\n\ny_tf, _ = tf.nn.dynamic_rnn(tf.contrib.rnn.GRUCell(n_cell), x, dtype=tf.float32)\nparam_tf = [v.assign(tf.zeros(tf.shape(v))) for v in tf.trainable_variables()] # y_tf uses these zeroed out params\n\nn_layer = 1\nrnn_cudnn = cudnn_rnn_ops.CudnnGRU(n_layer, n_cell, x_depth, 'skip_input')\nparam_cudnn = tf.Variable(tf.zeros([rnn_cudnn.params_size()]), validate_shape=False)\ny_cudnn, state_cudnn = rnn_cudnn(tf.transpose(x, [1,0,2]), tf.zeros([n_layer, batch_size, n_cell]), param_cudnn)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    # NOTE: cudnn has more params because they use twice as many biases\n    print('y_tf: {}\\ny_cudnn: {}\\nparam_tf\\n{}\\n\\nparam_cudnn\\n{}\\n\\n'.format(*sess.run([y_tf, y_cudnn, param_tf, param_cudnn])))\n\n\n\n# Output:\n# y_tf: [[[ 0.]]]\n# y_cudnn: [[[ 0.20482421]]]\n# param_tf\n# [array([[ 0.,  0.],\n#        [ 0.,  0.]], dtype=float32), array([ 0.,  0.], dtype=float32), array([[ 0.],\n#        [ 0.]], dtype=float32), array([ 0.], dtype=float32)]\n\n# param_cudnn\n# [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]", "body": "Operating System: Arch Linux\r\n\r\nInstalled version of CUDA and cuDNN:  libcudart.so.8.0.44, libcudnn.so.5.1.5\r\n\r\n1. The commit hash (`git rev-parse HEAD`): 57e40363eb40a692f7c5dfea3f53031a52024321\r\n2. The output of `bazel version`: 0.4.4\r\n\r\n\r\n\r\nI set x = 1, previous_h = 0, all weights and biases = 0, output should be 0. Traditional tensorflow GRU returns 0, but CudnnGRU returns 0.20482421\r\n\r\nNOTE: need to use GPU for CudnnGRU to work properly\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n\r\n# GRU with x=1, h_t_minus_1=0, all weights and biases = 0; output (h) should equal 0\r\n# u = sig(Wx + Rh + b)\r\n# r = sig(Wx + Rh + b)\r\n# c = tanh(Wx + r(Rh + b) + b)\r\n# h = (1-u)c + uh\r\n\r\n\r\n# u = sig(0*1 + 0*0 + 0)\r\n# u = .5\r\n# r = sig(0*1 + 0*0 + 0)\r\n# r = .5\r\n# c = tanh(0*1 + .5(0*0 + 0) + 0)\r\n# c = 0\r\n# h = (1-.5)*0 + .5*0\r\n# h = 0\r\n\r\n\r\n\r\nbatch_size = 1\r\nn_time = 1\r\nx_depth = 1\r\nn_cell = 1\r\n\r\nx = tf.ones([batch_size, n_time, x_depth])\r\n\r\ny_tf, _ = tf.nn.dynamic_rnn(tf.contrib.rnn.GRUCell(n_cell), x, dtype=tf.float32)\r\nparam_tf = [v.assign(tf.zeros(tf.shape(v))) for v in tf.trainable_variables()] # y_tf uses these zeroed out params\r\n\r\nn_layer = 1\r\nrnn_cudnn = cudnn_rnn_ops.CudnnGRU(n_layer, n_cell, x_depth, 'skip_input')\r\nparam_cudnn = tf.Variable(tf.zeros([rnn_cudnn.params_size()]), validate_shape=False)\r\ny_cudnn, state_cudnn = rnn_cudnn(tf.transpose(x, [1,0,2]), tf.zeros([n_layer, batch_size, n_cell]), param_cudnn)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    # NOTE: cudnn has more params because they use twice as many biases\r\n    print('y_tf: {}\\ny_cudnn: {}\\nparam_tf\\n{}\\n\\nparam_cudnn\\n{}\\n\\n'.format(*sess.run([y_tf, y_cudnn, param_tf, param_cudnn])))\r\n\r\n\r\n\r\n# Output:\r\n# y_tf: [[[ 0.]]]\r\n# y_cudnn: [[[ 0.20482421]]]\r\n# param_tf\r\n# [array([[ 0.,  0.],\r\n#        [ 0.,  0.]], dtype=float32), array([ 0.,  0.], dtype=float32), array([[ 0.],\r\n#        [ 0.]], dtype=float32), array([ 0.], dtype=float32)]\r\n\r\n# param_cudnn\r\n# [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n```\r\n"}