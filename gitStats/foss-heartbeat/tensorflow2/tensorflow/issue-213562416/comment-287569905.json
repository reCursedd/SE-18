{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/287569905", "html_url": "https://github.com/tensorflow/tensorflow/issues/8311#issuecomment-287569905", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8311", "id": 287569905, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NzU2OTkwNQ==", "user": {"login": "rewonc", "id": 3162580, "node_id": "MDQ6VXNlcjMxNjI1ODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/3162580?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rewonc", "html_url": "https://github.com/rewonc", "followers_url": "https://api.github.com/users/rewonc/followers", "following_url": "https://api.github.com/users/rewonc/following{/other_user}", "gists_url": "https://api.github.com/users/rewonc/gists{/gist_id}", "starred_url": "https://api.github.com/users/rewonc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rewonc/subscriptions", "organizations_url": "https://api.github.com/users/rewonc/orgs", "repos_url": "https://api.github.com/users/rewonc/repos", "events_url": "https://api.github.com/users/rewonc/events{/privacy}", "received_events_url": "https://api.github.com/users/rewonc/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-18T19:44:57Z", "updated_at": "2017-03-18T19:44:57Z", "author_association": "NONE", "body_html": "<p><code>skip_input</code>, as far as I can tell, skips the <code>Wx</code> linear transformation of the input and performs roughly half the computation as the other approach. This is probably in case you want to do batch norm through depth and so need flexibility to modify the outputs of <code>Wx</code>.  It does NOT appear to apply an extra linear transform \"outside\" of the GRU.</p>\n<p>As you're expecting to apply <code>Wx</code>, you should enable <code>linear_input</code>.</p>\n<p>As a side note, there still appears to be bug in <code>skip_input</code>, since for GRUs if you skip the initial linear transformation you need to pass in an input matrix that has dimensionality 3x that of the hidden dimension (each gate in the GRU takes a separate linear transformation). CuDNN still checks for equality in dimensionality, however, and appears to just copy the input to all three gates. This is likely incorrect and I can't make sense of it. Using <code>linear_input</code> will make the GRU behave as expected.</p>", "body_text": "skip_input, as far as I can tell, skips the Wx linear transformation of the input and performs roughly half the computation as the other approach. This is probably in case you want to do batch norm through depth and so need flexibility to modify the outputs of Wx.  It does NOT appear to apply an extra linear transform \"outside\" of the GRU.\nAs you're expecting to apply Wx, you should enable linear_input.\nAs a side note, there still appears to be bug in skip_input, since for GRUs if you skip the initial linear transformation you need to pass in an input matrix that has dimensionality 3x that of the hidden dimension (each gate in the GRU takes a separate linear transformation). CuDNN still checks for equality in dimensionality, however, and appears to just copy the input to all three gates. This is likely incorrect and I can't make sense of it. Using linear_input will make the GRU behave as expected.", "body": "`skip_input`, as far as I can tell, skips the `Wx` linear transformation of the input and performs roughly half the computation as the other approach. This is probably in case you want to do batch norm through depth and so need flexibility to modify the outputs of `Wx`.  It does NOT appear to apply an extra linear transform \"outside\" of the GRU.\r\n\r\nAs you're expecting to apply `Wx`, you should enable `linear_input`. \r\n\r\nAs a side note, there still appears to be bug in `skip_input`, since for GRUs if you skip the initial linear transformation you need to pass in an input matrix that has dimensionality 3x that of the hidden dimension (each gate in the GRU takes a separate linear transformation). CuDNN still checks for equality in dimensionality, however, and appears to just copy the input to all three gates. This is likely incorrect and I can't make sense of it. Using `linear_input` will make the GRU behave as expected."}