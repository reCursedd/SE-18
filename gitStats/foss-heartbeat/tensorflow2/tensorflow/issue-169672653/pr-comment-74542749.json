{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74542749", "pull_request_review_id": null, "id": 74542749, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc0NTQyNzQ5", "diff_hunk": "@@ -836,6 +837,92 @@ def fully_connected(inputs,\n \n \n @add_arg_scope\n+def layer_norm(inputs,\n+               center=True,\n+               scale=False,\n+               epsilon=0.001,\n+               activation_fn=None,\n+               reuse=None,\n+               variables_collections=None,\n+               outputs_collections=None,\n+               trainable=True,\n+               scope=None):\n+  \"\"\"Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450.\n+\n+    \"Layer Normalization\"\n+\n+    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n+\n+  Can be used as a normalizer function for conv2d and fully_connected.\n+\n+  Args:\n+    inputs: a tensor of size `[batch_size, height, width, channels]`\n+            or `[batch_size, channels]`.\n+    center: If True, subtract `beta`. If False, `beta` is ignored.\n+    scale: If True, multiply by `gamma`. If False, `gamma` is\n+      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n+      disabled since the scaling can be done by the next layer.\n+    epsilon: small float added to variance to avoid dividing by zero.", "path": "tensorflow/contrib/layers/python/layers/layers.py", "position": null, "original_position": 37, "commit_id": "1424ad7cacea96c4a4213f2a185ba061ac202079", "original_commit_id": "6791f48d801d80c39e7e9897ee07641f55bbfb84", "user": {"login": "mikowals", "id": 1331470, "node_id": "MDQ6VXNlcjEzMzE0NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1331470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikowals", "html_url": "https://github.com/mikowals", "followers_url": "https://api.github.com/users/mikowals/followers", "following_url": "https://api.github.com/users/mikowals/following{/other_user}", "gists_url": "https://api.github.com/users/mikowals/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikowals/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikowals/subscriptions", "organizations_url": "https://api.github.com/users/mikowals/orgs", "repos_url": "https://api.github.com/users/mikowals/repos", "events_url": "https://api.github.com/users/mikowals/events{/privacy}", "received_events_url": "https://api.github.com/users/mikowals/received_events", "type": "User", "site_admin": false}, "body": "I think adding epsilon before the square root and division is the correct approach.  If variance were 1 the division could be skipped but for variance near or at 0 the impact of division is  to change the values significantly.  \n", "created_at": "2016-08-12T05:24:37Z", "updated_at": "2016-08-24T17:57:53Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74542749", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74542749"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74542749"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671"}}, "body_html": "<p>I think adding epsilon before the square root and division is the correct approach.  If variance were 1 the division could be skipped but for variance near or at 0 the impact of division is  to change the values significantly.</p>", "body_text": "I think adding epsilon before the square root and division is the correct approach.  If variance were 1 the division could be skipped but for variance near or at 0 the impact of division is  to change the values significantly."}