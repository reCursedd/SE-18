{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74530015", "pull_request_review_id": null, "id": 74530015, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc0NTMwMDE1", "diff_hunk": "@@ -836,6 +837,92 @@ def fully_connected(inputs,\n \n \n @add_arg_scope\n+def layer_norm(inputs,\n+               center=True,\n+               scale=False,\n+               epsilon=0.001,\n+               activation_fn=None,\n+               reuse=None,\n+               variables_collections=None,\n+               outputs_collections=None,\n+               trainable=True,\n+               scope=None):\n+  \"\"\"Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450.\n+\n+    \"Layer Normalization\"\n+\n+    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n+\n+  Can be used as a normalizer function for conv2d and fully_connected.\n+\n+  Args:\n+    inputs: a tensor of size `[batch_size, height, width, channels]`\n+            or `[batch_size, channels]`.\n+    center: If True, subtract `beta`. If False, `beta` is ignored.\n+    scale: If True, multiply by `gamma`. If False, `gamma` is\n+      not used. When the next layer is linear (also e.g. `nn.relu`), this can be", "path": "tensorflow/contrib/layers/python/layers/layers.py", "position": null, "original_position": 35, "commit_id": "1424ad7cacea96c4a4213f2a185ba061ac202079", "original_commit_id": "6791f48d801d80c39e7e9897ee07641f55bbfb84", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "body": "Is this only true for linear and relu? Any fully connected layer learn the scale, right?\n", "created_at": "2016-08-12T01:09:28Z", "updated_at": "2016-08-24T17:57:53Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74530015", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74530015"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74530015"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671"}}, "body_html": "<p>Is this only true for linear and relu? Any fully connected layer learn the scale, right?</p>", "body_text": "Is this only true for linear and relu? Any fully connected layer learn the scale, right?"}