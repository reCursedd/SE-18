{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/344370698", "html_url": "https://github.com/tensorflow/tensorflow/pull/3671#issuecomment-344370698", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3671", "id": 344370698, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NDM3MDY5OA==", "user": {"login": "xcyan", "id": 8760875, "node_id": "MDQ6VXNlcjg3NjA4NzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/8760875?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xcyan", "html_url": "https://github.com/xcyan", "followers_url": "https://api.github.com/users/xcyan/followers", "following_url": "https://api.github.com/users/xcyan/following{/other_user}", "gists_url": "https://api.github.com/users/xcyan/gists{/gist_id}", "starred_url": "https://api.github.com/users/xcyan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xcyan/subscriptions", "organizations_url": "https://api.github.com/users/xcyan/orgs", "repos_url": "https://api.github.com/users/xcyan/repos", "events_url": "https://api.github.com/users/xcyan/events{/privacy}", "received_events_url": "https://api.github.com/users/xcyan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-14T19:29:56Z", "updated_at": "2017-11-14T20:19:15Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20853485\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xodus7\">@xodus7</a> One question about the layer_normalization op in this PR:<br>\nDid you really implement the layer_norm based on Ba et al 2016?</p>\n<p>As proposed in  Ba et al 2016, each instance in a mini-batch has its individual beta/gamma, but there is no such option in your implementation. Instead, you implemented the beta/gamma following batch normalization-style.<br>\n<code>params_shape = inputs_shape[-1:]</code><br>\n<code>gamma = variables.model_variable('gamma', shape=params_shape,dtype=dtype)</code><br>\n<code>outputs = nn.batch_normalization(inputs, mean, variance, offset=beta, scale=gamma, variance_epsilon=variance_epsilon)</code></p>\n<p>The authors of this paper should be able to comment on that: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7976463\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jimmylba\">@jimmylba</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10283090\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ryankiros\">@ryankiros</a></p>", "body_text": "@xodus7 One question about the layer_normalization op in this PR:\nDid you really implement the layer_norm based on Ba et al 2016?\nAs proposed in  Ba et al 2016, each instance in a mini-batch has its individual beta/gamma, but there is no such option in your implementation. Instead, you implemented the beta/gamma following batch normalization-style.\nparams_shape = inputs_shape[-1:]\ngamma = variables.model_variable('gamma', shape=params_shape,dtype=dtype)\noutputs = nn.batch_normalization(inputs, mean, variance, offset=beta, scale=gamma, variance_epsilon=variance_epsilon)\nThe authors of this paper should be able to comment on that: @jimmylba @ryankiros", "body": "@xodus7 One question about the layer_normalization op in this PR:\r\nDid you really implement the layer_norm based on Ba et al 2016?\r\n\r\nAs proposed in  Ba et al 2016, each instance in a mini-batch has its individual beta/gamma, but there is no such option in your implementation. Instead, you implemented the beta/gamma following batch normalization-style.\r\n`params_shape = inputs_shape[-1:]`\r\n`gamma = variables.model_variable('gamma', shape=params_shape,dtype=dtype)`\r\n`outputs = nn.batch_normalization(inputs, mean, variance, offset=beta, scale=gamma, variance_epsilon=variance_epsilon)`\r\n\r\nThe authors of this paper should be able to comment on that: @jimmylba @ryankiros"}