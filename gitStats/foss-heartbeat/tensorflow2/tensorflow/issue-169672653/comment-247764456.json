{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/247764456", "html_url": "https://github.com/tensorflow/tensorflow/pull/3671#issuecomment-247764456", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3671", "id": 247764456, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Nzc2NDQ1Ng==", "user": {"login": "MycChiu", "id": 6672514, "node_id": "MDQ6VXNlcjY2NzI1MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6672514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MycChiu", "html_url": "https://github.com/MycChiu", "followers_url": "https://api.github.com/users/MycChiu/followers", "following_url": "https://api.github.com/users/MycChiu/following{/other_user}", "gists_url": "https://api.github.com/users/MycChiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/MycChiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MycChiu/subscriptions", "organizations_url": "https://api.github.com/users/MycChiu/orgs", "repos_url": "https://api.github.com/users/MycChiu/repos", "events_url": "https://api.github.com/users/MycChiu/events{/privacy}", "received_events_url": "https://api.github.com/users/MycChiu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-17T11:32:23Z", "updated_at": "2016-09-17T11:32:23Z", "author_association": "NONE", "body_html": "<p>I found that this slowdown is not limited to rnn, it also happens to fully connected layers. Here's a quick benchmark:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> tensorflow.contrib.layers <span class=\"pl-k\">import</span> layer_norm\n<span class=\"pl-k\">from</span> timeit <span class=\"pl-k\">import</span> default_timer\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\nslim <span class=\"pl-k\">=</span> tf.contrib.slim\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\nnb_units <span class=\"pl-k\">=</span> <span class=\"pl-c1\">512</span>\nnb_layers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\nnb_epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\nitrs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">benchmark</span>(<span class=\"pl-smi\">norm_fn</span>):\n    epo_times <span class=\"pl-k\">=</span> np.zeros([nb_epoch])\n    <span class=\"pl-k\">with</span> tf.Graph().as_default():\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> fake data</span>\n        x <span class=\"pl-k\">=</span> tf.random_uniform([batch_size, <span class=\"pl-c1\">30</span>])\n        y <span class=\"pl-k\">=</span> tf.random_uniform([batch_size], <span class=\"pl-v\">maxval</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\n        labels <span class=\"pl-k\">=</span> tf.to_int32(y)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> define graph, need to pass scale argument since batch_norm's default</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> option for scale is False while that of layer_norm is True.</span>\n        out <span class=\"pl-k\">=</span> slim.repeat(x, nb_layers, slim.fully_connected, nb_units,\n                          <span class=\"pl-c\"><span class=\"pl-c\">#</span> normalizer_fn=layer_norm,</span>\n                          <span class=\"pl-v\">normalizer_fn</span><span class=\"pl-k\">=</span>norm_fn,\n                          <span class=\"pl-v\">normalizer_params</span><span class=\"pl-k\">=</span>{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>scale<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">True</span>},\n                          <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fc<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> calculate loss</span>\n        loss <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(out, labels)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> define optimizer</span>\n        optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-3</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> create train_op</span>\n        train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> initialization op</span>\n        init <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n\n        <span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> sess:\n            sess.run(init)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> warm up</span>\n            <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(itrs):\n                sess.run(train_op)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> start benchmark</span>\n            <span class=\"pl-k\">for</span> e <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(nb_epoch):\n                epo_start <span class=\"pl-k\">=</span> default_timer()\n                <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(itrs):\n                    sess.run(train_op)\n                eT <span class=\"pl-k\">=</span> (default_timer() <span class=\"pl-k\">-</span> epo_start)\n                epo_times[e] <span class=\"pl-k\">=</span> eT\n            <span class=\"pl-k\">return</span> epo_times\n\n\n\nvanilla_t <span class=\"pl-k\">=</span> benchmark(<span class=\"pl-c1\">None</span>)\nbatchNorm_t <span class=\"pl-k\">=</span> benchmark(slim.batch_norm)\nlayerNorm_t <span class=\"pl-k\">=</span> benchmark(layer_norm)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>vanilla times in seconds:<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-c1\">print</span>(vanilla_t)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_norm times in seconds:<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-c1\">print</span>(batchNorm_t)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>layer_norm times in seconds:<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-c1\">print</span>(layerNorm_t)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_norm is <span class=\"pl-c1\">%1.2f</span>X slower<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span>\n          (batchNorm_t.sum() <span class=\"pl-k\">/</span> vanilla_t.sum()))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>layer_norm is <span class=\"pl-c1\">%1.2f</span>X slower<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span>\n          (layerNorm_t.sum() <span class=\"pl-k\">/</span> vanilla_t.sum()))</pre></div>\n<p>Here's my output:</p>\n<pre><code>vanilla times in seconds:\n[ 0.42624067  0.42290944  0.43818542]\nbatch_norm times in seconds:\n[ 0.74915753  0.74786358  0.75310942]\nlayer_norm times in seconds:\n[ 1.13074295  1.12952367  1.13170891]\nbatch_norm is 1.75X slower\nlayer_norm is 2.63X slower\n</code></pre>\n<p>I just built tensorflow from source using the current master branch, so it's probably not the infamous batch_norm regression from few weeks ago. I wonder if this slowdown is inevitable for layer normalization or there are actually ways to improve the speed.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20853485\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xodus7\">@xodus7</a> I too can't think of a better way to implement this, I tried a naive approach using lower level ops, but it was even slower than yours.</p>", "body_text": "I found that this slowdown is not limited to rnn, it also happens to fully connected layers. Here's a quick benchmark:\nfrom tensorflow.contrib.layers import layer_norm\nfrom timeit import default_timer\n\nimport tensorflow as tf\nimport numpy as np\nslim = tf.contrib.slim\n\nbatch_size = 64\nnb_units = 512\nnb_layers = 20\nnb_epoch = 3\nitrs = 100\n\n\ndef benchmark(norm_fn):\n    epo_times = np.zeros([nb_epoch])\n    with tf.Graph().as_default():\n        # fake data\n        x = tf.random_uniform([batch_size, 30])\n        y = tf.random_uniform([batch_size], maxval=10)\n        labels = tf.to_int32(y)\n        # define graph, need to pass scale argument since batch_norm's default\n        # option for scale is False while that of layer_norm is True.\n        out = slim.repeat(x, nb_layers, slim.fully_connected, nb_units,\n                          # normalizer_fn=layer_norm,\n                          normalizer_fn=norm_fn,\n                          normalizer_params={\"scale\": True},\n                          scope=\"fc\")\n        # calculate loss\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(out, labels)\n        # define optimizer\n        optimizer = tf.train.AdamOptimizer(1e-3)\n        # create train_op\n        train_op = optimizer.minimize(loss)\n        # initialization op\n        init = tf.initialize_all_variables()\n\n        with tf.Session(\"\") as sess:\n            sess.run(init)\n            # warm up\n            for i in range(itrs):\n                sess.run(train_op)\n            # start benchmark\n            for e in range(nb_epoch):\n                epo_start = default_timer()\n                for i in range(itrs):\n                    sess.run(train_op)\n                eT = (default_timer() - epo_start)\n                epo_times[e] = eT\n            return epo_times\n\n\n\nvanilla_t = benchmark(None)\nbatchNorm_t = benchmark(slim.batch_norm)\nlayerNorm_t = benchmark(layer_norm)\nprint(\"vanilla times in seconds:\")\nprint(vanilla_t)\nprint(\"batch_norm times in seconds:\")\nprint(batchNorm_t)\nprint(\"layer_norm times in seconds:\")\nprint(layerNorm_t)\nprint(\"batch_norm is %1.2fX slower\" %\n          (batchNorm_t.sum() / vanilla_t.sum()))\nprint(\"layer_norm is %1.2fX slower\" %\n          (layerNorm_t.sum() / vanilla_t.sum()))\nHere's my output:\nvanilla times in seconds:\n[ 0.42624067  0.42290944  0.43818542]\nbatch_norm times in seconds:\n[ 0.74915753  0.74786358  0.75310942]\nlayer_norm times in seconds:\n[ 1.13074295  1.12952367  1.13170891]\nbatch_norm is 1.75X slower\nlayer_norm is 2.63X slower\n\nI just built tensorflow from source using the current master branch, so it's probably not the infamous batch_norm regression from few weeks ago. I wonder if this slowdown is inevitable for layer normalization or there are actually ways to improve the speed.\n@xodus7 I too can't think of a better way to implement this, I tried a naive approach using lower level ops, but it was even slower than yours.", "body": "I found that this slowdown is not limited to rnn, it also happens to fully connected layers. Here's a quick benchmark:\n\n``` python\nfrom tensorflow.contrib.layers import layer_norm\nfrom timeit import default_timer\n\nimport tensorflow as tf\nimport numpy as np\nslim = tf.contrib.slim\n\nbatch_size = 64\nnb_units = 512\nnb_layers = 20\nnb_epoch = 3\nitrs = 100\n\n\ndef benchmark(norm_fn):\n    epo_times = np.zeros([nb_epoch])\n    with tf.Graph().as_default():\n        # fake data\n        x = tf.random_uniform([batch_size, 30])\n        y = tf.random_uniform([batch_size], maxval=10)\n        labels = tf.to_int32(y)\n        # define graph, need to pass scale argument since batch_norm's default\n        # option for scale is False while that of layer_norm is True.\n        out = slim.repeat(x, nb_layers, slim.fully_connected, nb_units,\n                          # normalizer_fn=layer_norm,\n                          normalizer_fn=norm_fn,\n                          normalizer_params={\"scale\": True},\n                          scope=\"fc\")\n        # calculate loss\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(out, labels)\n        # define optimizer\n        optimizer = tf.train.AdamOptimizer(1e-3)\n        # create train_op\n        train_op = optimizer.minimize(loss)\n        # initialization op\n        init = tf.initialize_all_variables()\n\n        with tf.Session(\"\") as sess:\n            sess.run(init)\n            # warm up\n            for i in range(itrs):\n                sess.run(train_op)\n            # start benchmark\n            for e in range(nb_epoch):\n                epo_start = default_timer()\n                for i in range(itrs):\n                    sess.run(train_op)\n                eT = (default_timer() - epo_start)\n                epo_times[e] = eT\n            return epo_times\n\n\n\nvanilla_t = benchmark(None)\nbatchNorm_t = benchmark(slim.batch_norm)\nlayerNorm_t = benchmark(layer_norm)\nprint(\"vanilla times in seconds:\")\nprint(vanilla_t)\nprint(\"batch_norm times in seconds:\")\nprint(batchNorm_t)\nprint(\"layer_norm times in seconds:\")\nprint(layerNorm_t)\nprint(\"batch_norm is %1.2fX slower\" %\n          (batchNorm_t.sum() / vanilla_t.sum()))\nprint(\"layer_norm is %1.2fX slower\" %\n          (layerNorm_t.sum() / vanilla_t.sum()))\n```\n\nHere's my output:\n\n```\nvanilla times in seconds:\n[ 0.42624067  0.42290944  0.43818542]\nbatch_norm times in seconds:\n[ 0.74915753  0.74786358  0.75310942]\nlayer_norm times in seconds:\n[ 1.13074295  1.12952367  1.13170891]\nbatch_norm is 1.75X slower\nlayer_norm is 2.63X slower\n```\n\nI just built tensorflow from source using the current master branch, so it's probably not the infamous batch_norm regression from few weeks ago. I wonder if this slowdown is inevitable for layer normalization or there are actually ways to improve the speed.\n\n@xodus7 I too can't think of a better way to implement this, I tried a naive approach using lower level ops, but it was even slower than yours.\n"}