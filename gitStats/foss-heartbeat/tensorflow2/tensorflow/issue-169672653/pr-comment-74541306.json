{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74541306", "pull_request_review_id": null, "id": 74541306, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc0NTQxMzA2", "diff_hunk": "@@ -836,6 +837,92 @@ def fully_connected(inputs,\n \n \n @add_arg_scope\n+def layer_norm(inputs,\n+               center=True,\n+               scale=False,\n+               epsilon=0.001,\n+               activation_fn=None,\n+               reuse=None,\n+               variables_collections=None,\n+               outputs_collections=None,\n+               trainable=True,\n+               scope=None):\n+  \"\"\"Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450.\n+\n+    \"Layer Normalization\"\n+\n+    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n+\n+  Can be used as a normalizer function for conv2d and fully_connected.\n+\n+  Args:\n+    inputs: a tensor of size `[batch_size, height, width, channels]`\n+            or `[batch_size, channels]`.\n+    center: If True, subtract `beta`. If False, `beta` is ignored.\n+    scale: If True, multiply by `gamma`. If False, `gamma` is\n+      not used. When the next layer is linear (also e.g. `nn.relu`), this can be", "path": "tensorflow/contrib/layers/python/layers/layers.py", "position": null, "original_position": 35, "commit_id": "1424ad7cacea96c4a4213f2a185ba061ac202079", "original_commit_id": "6791f48d801d80c39e7e9897ee07641f55bbfb84", "user": {"login": "xodus7", "id": 20853485, "node_id": "MDQ6VXNlcjIwODUzNDg1", "avatar_url": "https://avatars1.githubusercontent.com/u/20853485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xodus7", "html_url": "https://github.com/xodus7", "followers_url": "https://api.github.com/users/xodus7/followers", "following_url": "https://api.github.com/users/xodus7/following{/other_user}", "gists_url": "https://api.github.com/users/xodus7/gists{/gist_id}", "starred_url": "https://api.github.com/users/xodus7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xodus7/subscriptions", "organizations_url": "https://api.github.com/users/xodus7/orgs", "repos_url": "https://api.github.com/users/xodus7/repos", "events_url": "https://api.github.com/users/xodus7/events{/privacy}", "received_events_url": "https://api.github.com/users/xodus7/received_events", "type": "User", "site_admin": false}, "body": "@martinwicke  I think the general intuition is that normalizing values with a variance of 1.0 before non-linearities such as tanh limits most activations to the linear portion of the function. This causes the network to lose it's expressive power, which the learned gamma and beta terms counteract. I think the idea is if you are applying a \"linear\" activation function, such as the identity transformation or relu, then these terms are not strictly necessary (though still beneficial?) and can be optimized out. Perhaps the default for this parameter should be True?\n", "created_at": "2016-08-12T04:50:28Z", "updated_at": "2016-08-24T17:57:53Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74541306", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74541306"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74541306"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a>  I think the general intuition is that normalizing values with a variance of 1.0 before non-linearities such as tanh limits most activations to the linear portion of the function. This causes the network to lose it's expressive power, which the learned gamma and beta terms counteract. I think the idea is if you are applying a \"linear\" activation function, such as the identity transformation or relu, then these terms are not strictly necessary (though still beneficial?) and can be optimized out. Perhaps the default for this parameter should be True?</p>", "body_text": "@martinwicke  I think the general intuition is that normalizing values with a variance of 1.0 before non-linearities such as tanh limits most activations to the linear portion of the function. This causes the network to lose it's expressive power, which the learned gamma and beta terms counteract. I think the idea is if you are applying a \"linear\" activation function, such as the identity transformation or relu, then these terms are not strictly necessary (though still beneficial?) and can be optimized out. Perhaps the default for this parameter should be True?"}