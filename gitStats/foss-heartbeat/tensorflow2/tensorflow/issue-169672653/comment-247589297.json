{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/247589297", "html_url": "https://github.com/tensorflow/tensorflow/pull/3671#issuecomment-247589297", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3671", "id": 247589297, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NzU4OTI5Nw==", "user": {"login": "MycChiu", "id": 6672514, "node_id": "MDQ6VXNlcjY2NzI1MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6672514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MycChiu", "html_url": "https://github.com/MycChiu", "followers_url": "https://api.github.com/users/MycChiu/followers", "following_url": "https://api.github.com/users/MycChiu/following{/other_user}", "gists_url": "https://api.github.com/users/MycChiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/MycChiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MycChiu/subscriptions", "organizations_url": "https://api.github.com/users/MycChiu/orgs", "repos_url": "https://api.github.com/users/MycChiu/repos", "events_url": "https://api.github.com/users/MycChiu/events{/privacy}", "received_events_url": "https://api.github.com/users/MycChiu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-16T12:34:17Z", "updated_at": "2016-09-16T12:34:50Z", "author_association": "NONE", "body_html": "<p>Great stuff, Thank you! Just one question though, has anyone used this layer on recurrent networks and found some major slow down in terms of wall clock time? (about 3-5x slower compared to vanilla rnn without any form of normalization.)</p>\n<p>Although I think this is not the problem of this particular implementation, since I found <a href=\"https://www.reddit.com/r/MachineLearning/comments/4ufmxy/layer_normalization_implemented_in_tensorflow/\" rel=\"nofollow\">a thread on reddit </a>, where other people have observed same slowdown with a different implementation. However, one of the co-authors from the original layer norm paper <a href=\"https://www.reddit.com/r/MachineLearning/comments/4ufmxy/layer_normalization_implemented_in_tensorflow/d5q95oe\" rel=\"nofollow\">commented in the same thread </a> stating that the implementation in Theano does not cause any slowdown with the help of CNMeM.</p>\n<p>I am really eager to solve this, since layer norm does help the model to converge faster in terms of iteration number, and if there is a way to incorporate it into rnn without any slowdown, I would love to add it to my future experiments more often.</p>", "body_text": "Great stuff, Thank you! Just one question though, has anyone used this layer on recurrent networks and found some major slow down in terms of wall clock time? (about 3-5x slower compared to vanilla rnn without any form of normalization.)\nAlthough I think this is not the problem of this particular implementation, since I found a thread on reddit , where other people have observed same slowdown with a different implementation. However, one of the co-authors from the original layer norm paper commented in the same thread  stating that the implementation in Theano does not cause any slowdown with the help of CNMeM.\nI am really eager to solve this, since layer norm does help the model to converge faster in terms of iteration number, and if there is a way to incorporate it into rnn without any slowdown, I would love to add it to my future experiments more often.", "body": "Great stuff, Thank you! Just one question though, has anyone used this layer on recurrent networks and found some major slow down in terms of wall clock time? (about 3-5x slower compared to vanilla rnn without any form of normalization.) \n\nAlthough I think this is not the problem of this particular implementation, since I found [a thread on reddit ](https://www.reddit.com/r/MachineLearning/comments/4ufmxy/layer_normalization_implemented_in_tensorflow/), where other people have observed same slowdown with a different implementation. However, one of the co-authors from the original layer norm paper [commented in the same thread ](https://www.reddit.com/r/MachineLearning/comments/4ufmxy/layer_normalization_implemented_in_tensorflow/d5q95oe) stating that the implementation in Theano does not cause any slowdown with the help of CNMeM.\n\nI am really eager to solve this, since layer norm does help the model to converge faster in terms of iteration number, and if there is a way to incorporate it into rnn without any slowdown, I would love to add it to my future experiments more often. \n"}