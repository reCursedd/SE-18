{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74547239", "pull_request_review_id": null, "id": 74547239, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc0NTQ3MjM5", "diff_hunk": "@@ -836,6 +837,92 @@ def fully_connected(inputs,\n \n \n @add_arg_scope\n+def layer_norm(inputs,\n+               center=True,\n+               scale=False,\n+               epsilon=0.001,\n+               activation_fn=None,\n+               reuse=None,\n+               variables_collections=None,\n+               outputs_collections=None,\n+               trainable=True,\n+               scope=None):\n+  \"\"\"Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450.\n+\n+    \"Layer Normalization\"\n+\n+    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n+\n+  Can be used as a normalizer function for conv2d and fully_connected.\n+\n+  Args:\n+    inputs: a tensor of size `[batch_size, height, width, channels]`\n+            or `[batch_size, channels]`.\n+    center: If True, subtract `beta`. If False, `beta` is ignored.\n+    scale: If True, multiply by `gamma`. If False, `gamma` is\n+      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n+      disabled since the scaling can be done by the next layer.\n+    epsilon: small float added to variance to avoid dividing by zero.", "path": "tensorflow/contrib/layers/python/layers/layers.py", "position": null, "original_position": 37, "commit_id": "1424ad7cacea96c4a4213f2a185ba061ac202079", "original_commit_id": "6791f48d801d80c39e7e9897ee07641f55bbfb84", "user": {"login": "xodus7", "id": 20853485, "node_id": "MDQ6VXNlcjIwODUzNDg1", "avatar_url": "https://avatars1.githubusercontent.com/u/20853485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xodus7", "html_url": "https://github.com/xodus7", "followers_url": "https://api.github.com/users/xodus7/followers", "following_url": "https://api.github.com/users/xodus7/following{/other_user}", "gists_url": "https://api.github.com/users/xodus7/gists{/gist_id}", "starred_url": "https://api.github.com/users/xodus7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xodus7/subscriptions", "organizations_url": "https://api.github.com/users/xodus7/orgs", "repos_url": "https://api.github.com/users/xodus7/repos", "events_url": "https://api.github.com/users/xodus7/events{/privacy}", "received_events_url": "https://api.github.com/users/xodus7/received_events", "type": "User", "site_admin": false}, "body": "@mikowals That's what the tf.batch_normalization() op does (which I use here). Thinking this through a bit more:\n- This can only occur if all values are identical.\n- If all values are identical then the variance is zero since we first subtract the mean.\n- Therefore the value of epsilon is unimportant since the end result of division will be zero.\n\nIs there a flaw in my logic or some corner case on certain hardware I'm not considering? If not then I think the correct thing to do here is to remove the parameter and pass in a sane default to the batch_normalization op.\n", "created_at": "2016-08-12T06:42:30Z", "updated_at": "2016-08-24T17:57:53Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74547239", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74547239"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74547239"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1331470\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mikowals\">@mikowals</a> That's what the tf.batch_normalization() op does (which I use here). Thinking this through a bit more:</p>\n<ul>\n<li>This can only occur if all values are identical.</li>\n<li>If all values are identical then the variance is zero since we first subtract the mean.</li>\n<li>Therefore the value of epsilon is unimportant since the end result of division will be zero.</li>\n</ul>\n<p>Is there a flaw in my logic or some corner case on certain hardware I'm not considering? If not then I think the correct thing to do here is to remove the parameter and pass in a sane default to the batch_normalization op.</p>", "body_text": "@mikowals That's what the tf.batch_normalization() op does (which I use here). Thinking this through a bit more:\n\nThis can only occur if all values are identical.\nIf all values are identical then the variance is zero since we first subtract the mean.\nTherefore the value of epsilon is unimportant since the end result of division will be zero.\n\nIs there a flaw in my logic or some corner case on certain hardware I'm not considering? If not then I think the correct thing to do here is to remove the parameter and pass in a sane default to the batch_normalization op."}