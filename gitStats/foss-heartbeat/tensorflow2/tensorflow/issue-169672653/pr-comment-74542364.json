{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74542364", "pull_request_review_id": null, "id": 74542364, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc0NTQyMzY0", "diff_hunk": "@@ -836,6 +837,92 @@ def fully_connected(inputs,\n \n \n @add_arg_scope\n+def layer_norm(inputs,\n+               center=True,\n+               scale=False,\n+               epsilon=0.001,\n+               activation_fn=None,\n+               reuse=None,\n+               variables_collections=None,\n+               outputs_collections=None,\n+               trainable=True,\n+               scope=None):\n+  \"\"\"Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450.\n+\n+    \"Layer Normalization\"\n+\n+    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n+\n+  Can be used as a normalizer function for conv2d and fully_connected.\n+\n+  Args:\n+    inputs: a tensor of size `[batch_size, height, width, channels]`\n+            or `[batch_size, channels]`.\n+    center: If True, subtract `beta`. If False, `beta` is ignored.\n+    scale: If True, multiply by `gamma`. If False, `gamma` is\n+      not used. When the next layer is linear (also e.g. `nn.relu`), this can be", "path": "tensorflow/contrib/layers/python/layers/layers.py", "position": null, "original_position": 35, "commit_id": "1424ad7cacea96c4a4213f2a185ba061ac202079", "original_commit_id": "6791f48d801d80c39e7e9897ee07641f55bbfb84", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "body": "I don't quite understand: The data flow goes\n\nlayer -> layer_norm -> activation_fn -> next layer -> activation_fn or another layer_norm or whatnot\n\nSo maybe if the activation_fn given to this is a linear one (None, or relu, or reluX), you may want this to be false?\n\nAnyway, I think a default of True makes more sense -- it seems to me you're not losing much even in terms of performance if it's on.\n", "created_at": "2016-08-12T05:16:25Z", "updated_at": "2016-08-24T17:57:53Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74542364", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/74542364"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/3671#discussion_r74542364"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3671"}}, "body_html": "<p>I don't quite understand: The data flow goes</p>\n<p>layer -&gt; layer_norm -&gt; activation_fn -&gt; next layer -&gt; activation_fn or another layer_norm or whatnot</p>\n<p>So maybe if the activation_fn given to this is a linear one (None, or relu, or reluX), you may want this to be false?</p>\n<p>Anyway, I think a default of True makes more sense -- it seems to me you're not losing much even in terms of performance if it's on.</p>", "body_text": "I don't quite understand: The data flow goes\nlayer -> layer_norm -> activation_fn -> next layer -> activation_fn or another layer_norm or whatnot\nSo maybe if the activation_fn given to this is a linear one (None, or relu, or reluX), you may want this to be false?\nAnyway, I think a default of True makes more sense -- it seems to me you're not losing much even in terms of performance if it's on."}