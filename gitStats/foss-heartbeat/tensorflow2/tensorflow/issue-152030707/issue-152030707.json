{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2177", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2177/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2177/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2177/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2177", "id": 152030707, "node_id": "MDU6SXNzdWUxNTIwMzA3MDc=", "number": 2177, "title": "tf.argmax Should Not Permit Backprop Through It", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-04-30T13:39:00Z", "updated_at": "2016-04-30T15:01:39Z", "closed_at": "2016-04-30T15:01:39Z", "author_association": "NONE", "body_html": "<p>Hey Tensorflow,</p>\n<p>Lately, I have been using the argmax function but I have always placed a tf.stop_gradient before using it. However, when I remove the stop_gradient, tensorflow still works fine.</p>\n<p>Maybe I'm misunderstanding something, but argmax is not a differentiable function. How is backprop still working when you remove it? Shouldn't an error be thrown when you pass argmax without any stop_gradient?</p>\n<p>If it is possible to differentiate argmax, then I would greatly appreciate any resource showing how this is done. Thanks TF!</p>", "body_text": "Hey Tensorflow,\nLately, I have been using the argmax function but I have always placed a tf.stop_gradient before using it. However, when I remove the stop_gradient, tensorflow still works fine.\nMaybe I'm misunderstanding something, but argmax is not a differentiable function. How is backprop still working when you remove it? Shouldn't an error be thrown when you pass argmax without any stop_gradient?\nIf it is possible to differentiate argmax, then I would greatly appreciate any resource showing how this is done. Thanks TF!", "body": "Hey Tensorflow,\n\nLately, I have been using the argmax function but I have always placed a tf.stop_gradient before using it. However, when I remove the stop_gradient, tensorflow still works fine. \n\nMaybe I'm misunderstanding something, but argmax is not a differentiable function. How is backprop still working when you remove it? Shouldn't an error be thrown when you pass argmax without any stop_gradient?\n\nIf it is possible to differentiate argmax, then I would greatly appreciate any resource showing how this is done. Thanks TF!\n"}