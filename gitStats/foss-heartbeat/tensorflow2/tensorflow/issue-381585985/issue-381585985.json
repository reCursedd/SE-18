{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23802", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23802/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23802/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23802/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23802", "id": 381585985, "node_id": "MDU6SXNzdWUzODE1ODU5ODU=", "number": 23802, "title": "Reduced redundancy in Estimator api", "user": {"login": "SumNeuron", "id": 22868585, "node_id": "MDQ6VXNlcjIyODY4NTg1", "avatar_url": "https://avatars3.githubusercontent.com/u/22868585?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SumNeuron", "html_url": "https://github.com/SumNeuron", "followers_url": "https://api.github.com/users/SumNeuron/followers", "following_url": "https://api.github.com/users/SumNeuron/following{/other_user}", "gists_url": "https://api.github.com/users/SumNeuron/gists{/gist_id}", "starred_url": "https://api.github.com/users/SumNeuron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SumNeuron/subscriptions", "organizations_url": "https://api.github.com/users/SumNeuron/orgs", "repos_url": "https://api.github.com/users/SumNeuron/repos", "events_url": "https://api.github.com/users/SumNeuron/events{/privacy}", "received_events_url": "https://api.github.com/users/SumNeuron/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-11-16T12:48:36Z", "updated_at": "2018-11-22T08:51:49Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>TensorFlow version (you are using): 1.10+</li>\n<li>Are you willing to contribute it (Yes/No): yes</li>\n</ul>\n<p><strong>Describe the feature and the current behavior/state.</strong></p>\n<p>The TensorFlow estimator api, while a welcomed addition, seems to add a lot of redundancy to the workflow as well as unnecessary complexity. While a prominent goal of the estimator API is to decouple the input pipelines from the model, unfortunately this is not fully realized due to this redundancy and complexity.</p>\n<h1>Example</h1>\n<p>Suppose we wish to make a custom estimator that has an <code>input_fn</code> which reads from TF <code>Record</code> files via <code>tf.data.TFRecordDataset(files)</code> and further, we wish to save our trained model so we can use it later (i.e. we need a <code>serving_input_reciever_fn</code>).</p>\n<p>For simplicity lets assume for a single \"example\" that after our input pipeline we are left with a tensor with shape [2,3], perhaps:</p>\n<pre><code>[[0,0,1],[1,0,0]]\n</code></pre>\n<p>and let us say that contextually we will refer to this feature as \"main_input\".</p>\n<p>Then we now need to wrap this into a <code>(Sequence)Example</code> (<em>NOTE</em>: see this <a href=\"https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel\" rel=\"nofollow\">S.O. post</a> for the lack of clarity the documentation provides on how to best do this)</p>\n<pre><code># all of input pipeline here\n\n# processed data\nmain_input = np.array([\n    [0, 0, 1],\n    [1, 0, 0],\n])\n\n# wrap each channel in Feature then wrap all in FeatureList\nmain_input_feat_list = tf.train.FeatureList(\n    feature=[\n        tf.train.Feature(float_list=tf.train.FloatList(value=value)) \n        for value in main_input\n    ]\n)\n\n# wrap in SequenceExample\nexample = tf.train.SequenceExample(\n    context={\n        # other stuff could go here\n    }, \n    feature_lists=tf.train.FeatureLists(feature_list={\n        'main_input': main_input_feat_list,\n        # other stuff could go here\n    })\n)\n\n\n# write to record\nwith tf.python_io.TFRecordWriter('demo_example.tfrecord') as writer:\n    writer.write(example.SerializeToString())\n</code></pre>\n<p>from here, we now need to unwrap this from TF <code>Record</code> in our <code>Estimator</code>'s <code>input_fn</code> using a <em>different</em> api:</p>\n<pre><code>\ndef parse_record(record):\n    return tf.parse_single_sequence_example(\n        record, \n        context_features={\n            # get context features here\n        }, \n        sequence_features={\n            'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32), # &lt;---- REDUNDANT\n            # get other sequence features here\n        }\n    )\n\n\ndef input_fn(files:list=['demo_example.tfrecord'], params:dict):\n    dataset = tf.data.TFRecordDataset(files).map(lambda record: parse_record(record))\n    # reformat dataset to return feature, label pairs\n    return dataset\n\n\n</code></pre>\n<p>Now if we want to use <code>tf.estimator.BestExporter</code> we need a <code>serving_input_receiver_fn</code> so lets go ahead and get that written:</p>\n<pre><code>def serving_input_receiver_fn():\n    batch_size = None\n    main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # &lt;---- REDUNDANT\n\n   \n    features = {'main_input': main_input}\n    \n    # unclear what the difference is between features and receiver tensors\n    receiver_tensors = features\n    return  tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\n\nexporter = tf.estimator.BestExporter(\n    name='best_exporter', \n    serving_input_receiver_fn= serving_input_receiver_fn,\n    exports_to_keep=3\n)\n</code></pre>\n<p>So while the input pipeline should be decoupled from the model, the model's first layer at the very least is defined by the input. Therefore I should not have to three different times, in three different ways tell TensorFlow what my input features are.</p>\n<pre><code>\n        # 1st to write to TF Records \n        feature_lists=tf.train.FeatureLists(feature_list={\n            'main_input':  tf.train.FeatureList(\n                feature=[\n                    tf.train.Feature(float_list=tf.train.FloatList(value=value)) \n                    for value in main_input\n                ]\n            ),\n            # other stuff could go here\n        })\n\n\n        # then to get it out of TF Records with an asymmetric api\n        #...\n        'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32)\n        #...\n\n        # and once more I have to define placeholders for the features in the serving_input_receiver_fn\n        main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # &lt;---- REDUNDANT\n        \n\n</code></pre>\n<h1>Proposal to solve</h1>\n<p>In this <a href=\"https://colab.research.google.com/drive/1HrSYF1I7rBGaNQ7388Ss3epWPLTloEC6\" rel=\"nofollow\">colab</a> I demonstrate the fledgling idea of how to solve this, named <a href=\"https://pypi.org/project/fio/\" rel=\"nofollow\">\"FIO\" (Feature Input / Output)</a> whereby instead of having to redefine everything many times over with different apis depending on the case, you can define everything once in a data \"schema\" e.g.</p>\n<pre><code># features here are of an actual example (not batched)\nfeatures = {\n    'my-feature': 'hi',\n    'seq': np.array([\n        # ch1, ch2, ch3\n        [   1,   1,  1], # element 1\n        [   2,   2,  2], # element 2\n        [   3,   3,  3], # element 3\n        [   4,   5,  6]  # element 4\n    ])\n}\n\n\n# here we specify what is needed to encode and decode from `(Sequence)Example` and `TF Records`\n\nSCHEMA = {\n    'my-feature': {'length': 'fixed', 'dtype': tf.string,  'shape': []},\n    'seq': {\n        'length': 'fixed',\n        'dtype': tf.int64,\n        'shape': [4, 3],\n        'encode': 'channels',\n        'channel_names': ['A', 'B', 'C'],\n        'data_format': 'channels_last'\n    }\n}\n</code></pre>\n<p>then</p>\n<pre><code>\n# define our converter\nfio = FIO(SCHEMA, etype='example')\n\n# write to example\nfile = 'example.tfrecord'\n\nexample = fio.to_example(features)\nwith tf.python_io.TFRecordWriter(file) as writer:\n    writer.write(example.SerializeToString())\n\n# use a sequence example instead\nfio = FIO(SCHEMA, etype='sequence_example', sequence_features=['seq'])\nfile = 'sequence_example.tfrecord'\nwith tf.python_io.TFRecordWriter(file) as writer:\n    writer.write(example.SerializeToString())\n\n</code></pre>\n<p>where in both cases we can read from records with</p>\n<pre><code>tf.data.TFRecordDataset(DATASET_FILENAMES).map(lambda r: fio.from_record(r))\n</code></pre>\n<p>Thus I propose that TensorFlow add's a new component to the estimator class, which would be the bride between the input pipelines / serving input functions and the <code>model_fn</code>. This would be something like the data schema.</p>\n<p>A user can define a data schema instance once and import it into the input pipeline to write their inputs to TF Record, import the same schema into their <code>Estimator</code>'s <code>input_fn</code> to read from TF <code>Record</code>s (or change the TF Record file format slightly so that you do not have define how it should be read, like how I do not have to define the keys in a json file to read a json file) and then once more it can be passed to (or automatically used by the <code>Estimator</code>) for the <code>serving_input_receiver_fn</code>.</p>\n<p>I believe letting users declaratively, upfront, define what the <code>Estimator</code> accepts in terms of data and then not forcing them to, at least trice, re-write what the data is, would be a great simplifier and clean up the <code>Estimator</code> api</p>\n<p><strong>Will this change the current api? How?</strong></p>\n<p>Technically, everything \"under the hood\" could remain the same. The changes would be made to <code>tf.data</code> to introduce a <code>Schema</code> class which would work as a unifier across the multiple ways of encoding / decoding <code>tf</code>/<code>np</code> tensors to / from TF Records. In addition, the <code>Estimator</code> class would be updated to accept a <code>tf.data.Schema</code> instance, from which it would automate the redundancy described above.</p>\n<p><strong>Who will benefit with this feature?</strong><br>\nAnyone who uses the <code>Estimator</code> api, anyone who uses <code>TF Records</code>, anyone who acknowledges that TensorFlow's documentation is far from complete and examples are needed to bridge how the \"high\" level apis quite often interface directly with lower (or even the lowest) level apis (e.g. <code>Estimators</code> which deal with <code>Records</code> or users who want to have an exporter which requires a <code>ServingInputReceiver</code>)</p>\n<p><strong>Any Other info.</strong><br>\nPlease look at <a href=\"https://pypi.org/project/fio/\" rel=\"nofollow\">FIO</a> and generalize this to work as described above.</p>\n<p>See these S.O. Posts and their linked Colab's to follow my journey in trying to decipher what should be a high level api, but it turns out to be fairly tethered to lower features.</p>\n<p><a href=\"https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel\" rel=\"nofollow\">optimal way to store tensors at TF Records</a></p>\n<p><a href=\"https://stackoverflow.com/questions/52064866/tensorflow-1-10-tfrecorddataset-recovering-tfrecords\" rel=\"nofollow\">recovering TF Records</a></p>\n<p><a href=\"https://stackoverflow.com/questions/52641737/tensorflow-1-10-custom-estimator-early-stopping-with-train-and-evaluate\" rel=\"nofollow\">early stopping</a></p>\n<p><a href=\"https://stackoverflow.com/questions/52874647/tensorflow-v1-10-why-is-an-input-serving-receiver-function-needed-when-checkpoi\" rel=\"nofollow\">what are serving_input_receiver_fn</a></p>\n<p><a href=\"https://stackoverflow.com/questions/53307954/tensorflow-custom-estimator-predict-throwing-value-error\" rel=\"nofollow\">using an estimator after training</a></p>\n<p><a href=\"https://stackoverflow.com/questions/53317235/tensorflow-custom-estimators-defining-estimator-spec-triggers-error\" rel=\"nofollow\">defining estimator spec throws error</a></p>", "body_text": "System information\n\nTensorFlow version (you are using): 1.10+\nAre you willing to contribute it (Yes/No): yes\n\nDescribe the feature and the current behavior/state.\nThe TensorFlow estimator api, while a welcomed addition, seems to add a lot of redundancy to the workflow as well as unnecessary complexity. While a prominent goal of the estimator API is to decouple the input pipelines from the model, unfortunately this is not fully realized due to this redundancy and complexity.\nExample\nSuppose we wish to make a custom estimator that has an input_fn which reads from TF Record files via tf.data.TFRecordDataset(files) and further, we wish to save our trained model so we can use it later (i.e. we need a serving_input_reciever_fn).\nFor simplicity lets assume for a single \"example\" that after our input pipeline we are left with a tensor with shape [2,3], perhaps:\n[[0,0,1],[1,0,0]]\n\nand let us say that contextually we will refer to this feature as \"main_input\".\nThen we now need to wrap this into a (Sequence)Example (NOTE: see this S.O. post for the lack of clarity the documentation provides on how to best do this)\n# all of input pipeline here\n\n# processed data\nmain_input = np.array([\n    [0, 0, 1],\n    [1, 0, 0],\n])\n\n# wrap each channel in Feature then wrap all in FeatureList\nmain_input_feat_list = tf.train.FeatureList(\n    feature=[\n        tf.train.Feature(float_list=tf.train.FloatList(value=value)) \n        for value in main_input\n    ]\n)\n\n# wrap in SequenceExample\nexample = tf.train.SequenceExample(\n    context={\n        # other stuff could go here\n    }, \n    feature_lists=tf.train.FeatureLists(feature_list={\n        'main_input': main_input_feat_list,\n        # other stuff could go here\n    })\n)\n\n\n# write to record\nwith tf.python_io.TFRecordWriter('demo_example.tfrecord') as writer:\n    writer.write(example.SerializeToString())\n\nfrom here, we now need to unwrap this from TF Record in our Estimator's input_fn using a different api:\n\ndef parse_record(record):\n    return tf.parse_single_sequence_example(\n        record, \n        context_features={\n            # get context features here\n        }, \n        sequence_features={\n            'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32), # <---- REDUNDANT\n            # get other sequence features here\n        }\n    )\n\n\ndef input_fn(files:list=['demo_example.tfrecord'], params:dict):\n    dataset = tf.data.TFRecordDataset(files).map(lambda record: parse_record(record))\n    # reformat dataset to return feature, label pairs\n    return dataset\n\n\n\nNow if we want to use tf.estimator.BestExporter we need a serving_input_receiver_fn so lets go ahead and get that written:\ndef serving_input_receiver_fn():\n    batch_size = None\n    main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # <---- REDUNDANT\n\n   \n    features = {'main_input': main_input}\n    \n    # unclear what the difference is between features and receiver tensors\n    receiver_tensors = features\n    return  tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\n\nexporter = tf.estimator.BestExporter(\n    name='best_exporter', \n    serving_input_receiver_fn= serving_input_receiver_fn,\n    exports_to_keep=3\n)\n\nSo while the input pipeline should be decoupled from the model, the model's first layer at the very least is defined by the input. Therefore I should not have to three different times, in three different ways tell TensorFlow what my input features are.\n\n        # 1st to write to TF Records \n        feature_lists=tf.train.FeatureLists(feature_list={\n            'main_input':  tf.train.FeatureList(\n                feature=[\n                    tf.train.Feature(float_list=tf.train.FloatList(value=value)) \n                    for value in main_input\n                ]\n            ),\n            # other stuff could go here\n        })\n\n\n        # then to get it out of TF Records with an asymmetric api\n        #...\n        'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32)\n        #...\n\n        # and once more I have to define placeholders for the features in the serving_input_receiver_fn\n        main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # <---- REDUNDANT\n        \n\n\nProposal to solve\nIn this colab I demonstrate the fledgling idea of how to solve this, named \"FIO\" (Feature Input / Output) whereby instead of having to redefine everything many times over with different apis depending on the case, you can define everything once in a data \"schema\" e.g.\n# features here are of an actual example (not batched)\nfeatures = {\n    'my-feature': 'hi',\n    'seq': np.array([\n        # ch1, ch2, ch3\n        [   1,   1,  1], # element 1\n        [   2,   2,  2], # element 2\n        [   3,   3,  3], # element 3\n        [   4,   5,  6]  # element 4\n    ])\n}\n\n\n# here we specify what is needed to encode and decode from `(Sequence)Example` and `TF Records`\n\nSCHEMA = {\n    'my-feature': {'length': 'fixed', 'dtype': tf.string,  'shape': []},\n    'seq': {\n        'length': 'fixed',\n        'dtype': tf.int64,\n        'shape': [4, 3],\n        'encode': 'channels',\n        'channel_names': ['A', 'B', 'C'],\n        'data_format': 'channels_last'\n    }\n}\n\nthen\n\n# define our converter\nfio = FIO(SCHEMA, etype='example')\n\n# write to example\nfile = 'example.tfrecord'\n\nexample = fio.to_example(features)\nwith tf.python_io.TFRecordWriter(file) as writer:\n    writer.write(example.SerializeToString())\n\n# use a sequence example instead\nfio = FIO(SCHEMA, etype='sequence_example', sequence_features=['seq'])\nfile = 'sequence_example.tfrecord'\nwith tf.python_io.TFRecordWriter(file) as writer:\n    writer.write(example.SerializeToString())\n\n\nwhere in both cases we can read from records with\ntf.data.TFRecordDataset(DATASET_FILENAMES).map(lambda r: fio.from_record(r))\n\nThus I propose that TensorFlow add's a new component to the estimator class, which would be the bride between the input pipelines / serving input functions and the model_fn. This would be something like the data schema.\nA user can define a data schema instance once and import it into the input pipeline to write their inputs to TF Record, import the same schema into their Estimator's input_fn to read from TF Records (or change the TF Record file format slightly so that you do not have define how it should be read, like how I do not have to define the keys in a json file to read a json file) and then once more it can be passed to (or automatically used by the Estimator) for the serving_input_receiver_fn.\nI believe letting users declaratively, upfront, define what the Estimator accepts in terms of data and then not forcing them to, at least trice, re-write what the data is, would be a great simplifier and clean up the Estimator api\nWill this change the current api? How?\nTechnically, everything \"under the hood\" could remain the same. The changes would be made to tf.data to introduce a Schema class which would work as a unifier across the multiple ways of encoding / decoding tf/np tensors to / from TF Records. In addition, the Estimator class would be updated to accept a tf.data.Schema instance, from which it would automate the redundancy described above.\nWho will benefit with this feature?\nAnyone who uses the Estimator api, anyone who uses TF Records, anyone who acknowledges that TensorFlow's documentation is far from complete and examples are needed to bridge how the \"high\" level apis quite often interface directly with lower (or even the lowest) level apis (e.g. Estimators which deal with Records or users who want to have an exporter which requires a ServingInputReceiver)\nAny Other info.\nPlease look at FIO and generalize this to work as described above.\nSee these S.O. Posts and their linked Colab's to follow my journey in trying to decipher what should be a high level api, but it turns out to be fairly tethered to lower features.\noptimal way to store tensors at TF Records\nrecovering TF Records\nearly stopping\nwhat are serving_input_receiver_fn\nusing an estimator after training\ndefining estimator spec throws error", "body": "\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.10+\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe TensorFlow estimator api, while a welcomed addition, seems to add a lot of redundancy to the workflow as well as unnecessary complexity. While a prominent goal of the estimator API is to decouple the input pipelines from the model, unfortunately this is not fully realized due to this redundancy and complexity.\r\n\r\n# Example\r\n\r\nSuppose we wish to make a custom estimator that has an `input_fn` which reads from TF `Record` files via `tf.data.TFRecordDataset(files)` and further, we wish to save our trained model so we can use it later (i.e. we need a `serving_input_reciever_fn`).  \r\n\r\nFor simplicity lets assume for a single \"example\" that after our input pipeline we are left with a tensor with shape [2,3], perhaps:\r\n\r\n    [[0,0,1],[1,0,0]]\r\n\r\nand let us say that contextually we will refer to this feature as \"main_input\".\r\n\r\nThen we now need to wrap this into a `(Sequence)Example` (*NOTE*: see this [S.O. post](https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel) for the lack of clarity the documentation provides on how to best do this)\r\n\r\n\r\n\r\n```\r\n# all of input pipeline here\r\n\r\n# processed data\r\nmain_input = np.array([\r\n    [0, 0, 1],\r\n    [1, 0, 0],\r\n])\r\n\r\n# wrap each channel in Feature then wrap all in FeatureList\r\nmain_input_feat_list = tf.train.FeatureList(\r\n    feature=[\r\n        tf.train.Feature(float_list=tf.train.FloatList(value=value)) \r\n        for value in main_input\r\n    ]\r\n)\r\n\r\n# wrap in SequenceExample\r\nexample = tf.train.SequenceExample(\r\n    context={\r\n        # other stuff could go here\r\n    }, \r\n    feature_lists=tf.train.FeatureLists(feature_list={\r\n        'main_input': main_input_feat_list,\r\n        # other stuff could go here\r\n    })\r\n)\r\n\r\n\r\n# write to record\r\nwith tf.python_io.TFRecordWriter('demo_example.tfrecord') as writer:\r\n    writer.write(example.SerializeToString())\r\n```\r\n\r\nfrom here, we now need to unwrap this from TF `Record` in our `Estimator`'s `input_fn` using a _different_ api:\r\n```\r\n\r\ndef parse_record(record):\r\n    return tf.parse_single_sequence_example(\r\n        record, \r\n        context_features={\r\n            # get context features here\r\n        }, \r\n        sequence_features={\r\n            'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32), # <---- REDUNDANT\r\n            # get other sequence features here\r\n        }\r\n    )\r\n\r\n\r\ndef input_fn(files:list=['demo_example.tfrecord'], params:dict):\r\n    dataset = tf.data.TFRecordDataset(files).map(lambda record: parse_record(record))\r\n    # reformat dataset to return feature, label pairs\r\n    return dataset\r\n\r\n\r\n```\r\n\r\nNow if we want to use `tf.estimator.BestExporter` we need a `serving_input_receiver_fn` so lets go ahead and get that written:\r\n\r\n\r\n```\r\ndef serving_input_receiver_fn():\r\n    batch_size = None\r\n    main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # <---- REDUNDANT\r\n\r\n   \r\n    features = {'main_input': main_input}\r\n    \r\n    # unclear what the difference is between features and receiver tensors\r\n    receiver_tensors = features\r\n    return  tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\r\n\r\n\r\nexporter = tf.estimator.BestExporter(\r\n    name='best_exporter', \r\n    serving_input_receiver_fn= serving_input_receiver_fn,\r\n    exports_to_keep=3\r\n)\r\n```\r\n\r\nSo while the input pipeline should be decoupled from the model, the model's first layer at the very least is defined by the input. Therefore I should not have to three different times, in three different ways tell TensorFlow what my input features are.\r\n\r\n```\r\n\r\n        # 1st to write to TF Records \r\n        feature_lists=tf.train.FeatureLists(feature_list={\r\n            'main_input':  tf.train.FeatureList(\r\n                feature=[\r\n                    tf.train.Feature(float_list=tf.train.FloatList(value=value)) \r\n                    for value in main_input\r\n                ]\r\n            ),\r\n            # other stuff could go here\r\n        })\r\n\r\n\r\n        # then to get it out of TF Records with an asymmetric api\r\n        #...\r\n        'main_input': tf.FixedLenSequenceFeature((3, ), tf.float32)\r\n        #...\r\n\r\n        # and once more I have to define placeholders for the features in the serving_input_receiver_fn\r\n        main_input = tf.placeholder(tf.float32, (batch_size, 2, 3), name='main_input') # <---- REDUNDANT\r\n        \r\n\r\n```\r\n\r\n# Proposal to solve\r\n\r\nIn this [colab](https://colab.research.google.com/drive/1HrSYF1I7rBGaNQ7388Ss3epWPLTloEC6) I demonstrate the fledgling idea of how to solve this, named [\"FIO\" (Feature Input / Output)](https://pypi.org/project/fio/) whereby instead of having to redefine everything many times over with different apis depending on the case, you can define everything once in a data \"schema\" e.g.\r\n\r\n\r\n```\r\n# features here are of an actual example (not batched)\r\nfeatures = {\r\n    'my-feature': 'hi',\r\n    'seq': np.array([\r\n        # ch1, ch2, ch3\r\n        [   1,   1,  1], # element 1\r\n        [   2,   2,  2], # element 2\r\n        [   3,   3,  3], # element 3\r\n        [   4,   5,  6]  # element 4\r\n    ])\r\n}\r\n\r\n\r\n# here we specify what is needed to encode and decode from `(Sequence)Example` and `TF Records`\r\n\r\nSCHEMA = {\r\n    'my-feature': {'length': 'fixed', 'dtype': tf.string,  'shape': []},\r\n    'seq': {\r\n        'length': 'fixed',\r\n        'dtype': tf.int64,\r\n        'shape': [4, 3],\r\n        'encode': 'channels',\r\n        'channel_names': ['A', 'B', 'C'],\r\n        'data_format': 'channels_last'\r\n    }\r\n}\r\n```\r\n\r\nthen\r\n```\r\n\r\n# define our converter\r\nfio = FIO(SCHEMA, etype='example')\r\n\r\n# write to example\r\nfile = 'example.tfrecord'\r\n\r\nexample = fio.to_example(features)\r\nwith tf.python_io.TFRecordWriter(file) as writer:\r\n    writer.write(example.SerializeToString())\r\n\r\n# use a sequence example instead\r\nfio = FIO(SCHEMA, etype='sequence_example', sequence_features=['seq'])\r\nfile = 'sequence_example.tfrecord'\r\nwith tf.python_io.TFRecordWriter(file) as writer:\r\n    writer.write(example.SerializeToString())\r\n\r\n```\r\n\r\nwhere in both cases we can read from records with\r\n\r\n```\r\ntf.data.TFRecordDataset(DATASET_FILENAMES).map(lambda r: fio.from_record(r))\r\n```\r\n\r\nThus I propose that TensorFlow add's a new component to the estimator class, which would be the bride between the input pipelines / serving input functions and the `model_fn`. This would be something like the data schema. \r\n\r\nA user can define a data schema instance once and import it into the input pipeline to write their inputs to TF Record, import the same schema into their `Estimator`'s `input_fn` to read from TF `Record`s (or change the TF Record file format slightly so that you do not have define how it should be read, like how I do not have to define the keys in a json file to read a json file) and then once more it can be passed to (or automatically used by the `Estimator`) for the `serving_input_receiver_fn`.  \r\n\r\nI believe letting users declaratively, upfront, define what the `Estimator` accepts in terms of data and then not forcing them to, at least trice, re-write what the data is, would be a great simplifier and clean up the `Estimator` api\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nTechnically, everything \"under the hood\" could remain the same. The changes would be made to `tf.data` to introduce a `Schema` class which would work as a unifier across the multiple ways of encoding / decoding `tf`/`np` tensors to / from TF Records. In addition, the `Estimator` class would be updated to accept a `tf.data.Schema` instance, from which it would automate the redundancy described above.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who uses the `Estimator` api, anyone who uses `TF Records`, anyone who acknowledges that TensorFlow's documentation is far from complete and examples are needed to bridge how the \"high\" level apis quite often interface directly with lower (or even the lowest) level apis (e.g. `Estimators` which deal with `Records` or users who want to have an exporter which requires a `ServingInputReceiver`)\r\n\r\n**Any Other info.**\r\nPlease look at [FIO](https://pypi.org/project/fio/) and generalize this to work as described above.\r\n\r\n\r\nSee these S.O. Posts and their linked Colab's to follow my journey in trying to decipher what should be a high level api, but it turns out to be fairly tethered to lower features.\r\n\r\n\r\n[optimal way to store tensors at TF Records](https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel)\r\n\r\n[recovering TF Records](https://stackoverflow.com/questions/52064866/tensorflow-1-10-tfrecorddataset-recovering-tfrecords)\r\n\r\n[early stopping](https://stackoverflow.com/questions/52641737/tensorflow-1-10-custom-estimator-early-stopping-with-train-and-evaluate)\r\n\r\n[what are serving_input_receiver_fn](https://stackoverflow.com/questions/52874647/tensorflow-v1-10-why-is-an-input-serving-receiver-function-needed-when-checkpoi)\r\n\r\n[using an estimator after training](https://stackoverflow.com/questions/53307954/tensorflow-custom-estimator-predict-throwing-value-error)\r\n\r\n[defining estimator spec throws error](https://stackoverflow.com/questions/53317235/tensorflow-custom-estimators-defining-estimator-spec-triggers-error)"}