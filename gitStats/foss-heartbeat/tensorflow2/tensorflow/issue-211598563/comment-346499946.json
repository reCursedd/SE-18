{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/346499946", "html_url": "https://github.com/tensorflow/tensorflow/issues/8043#issuecomment-346499946", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8043", "id": 346499946, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NjQ5OTk0Ng==", "user": {"login": "brightbytes-dude", "id": 6979675, "node_id": "MDQ6VXNlcjY5Nzk2NzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/6979675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brightbytes-dude", "html_url": "https://github.com/brightbytes-dude", "followers_url": "https://api.github.com/users/brightbytes-dude/followers", "following_url": "https://api.github.com/users/brightbytes-dude/following{/other_user}", "gists_url": "https://api.github.com/users/brightbytes-dude/gists{/gist_id}", "starred_url": "https://api.github.com/users/brightbytes-dude/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brightbytes-dude/subscriptions", "organizations_url": "https://api.github.com/users/brightbytes-dude/orgs", "repos_url": "https://api.github.com/users/brightbytes-dude/repos", "events_url": "https://api.github.com/users/brightbytes-dude/events{/privacy}", "received_events_url": "https://api.github.com/users/brightbytes-dude/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-22T23:27:32Z", "updated_at": "2017-11-22T23:27:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13877264\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ednaruiz\">@ednaruiz</a> - I still owe the thread some failing <code>pytest</code>s to demonstrate the issues with various attempted workarounds. (I haven't found the time to yet, and sadly am now working on another project for a few weeks.)</p>\n<p>Ultimately though, the only solutions I could come up with are:</p>\n<ol>\n<li>\n<p>Over-sample the under-represented class's examples.</p>\n</li>\n<li>\n<p>Use the low-level API to define the loss function by hand such that it uses the contents of the weight column when determining loss, and do not define the weight column as a feature column.  (I'm not sure how to do this yet, but there seems enough data on the web that I think I could figure it out.)</p>\n</li>\n</ol>\n<p>To reiterate the problem: if one attempts to solve the Class Imbalance problem by defining a weight column as a feature column when the class weights correspond to the class, then the network can trivially attain perfect accuracy simply by 100% correlating the weight to the outcome.</p>\n<p>For example, say I'm building a dog classifier and I have 100 pictures, 95 of which are <strong>not</strong> dogs, and the remaining 5 are dogs.</p>\n<p>I then add a weight column to my dataset, where the not-dog pictures have a class weight of 1/95 and the dog pictures have a weight of 1/5, and I specify the weight column when constructing my model.</p>\n<p>By doing so, the weight column becomes a feature column.  Because it's a feature column, my input function needs to supply values for it, and my model will use it to \"guess\" the correct class.</p>\n<p>I then train my model, and find that no matter how bad the dog pictures are, every training run always achieves 100% accuracy.</p>\n<p>That is, because the weight column is a feature column, the model trivially learns that if the class weight is 1/5, it's a dog, and if the class weight is 1/95, it's not a dog. Viva la ML!! :-)</p>\n<p>Furthermore, since my test runs and prediction runs require that the column be present (because it's a feature column of the model), I'm stuck specifying <strong>something</strong> when testing and predicting.</p>\n<p>For my actual model, hoping I could work around this, I tried setting the weight column value for testing and predicting to first the mean and then the midrange, but in both cases the trained models had worse accuracy than if I'd never used the column weights param.  So, I gave up.</p>\n<p>My conclusion is that one can only use the <code>weight_column_name</code> parameter when the values in that column are <strong>not</strong> correlated with the outcome.</p>\n<p>However, to solve the Class Imbalance problem, the <code>weight_column_name</code> must be correlated with the outcome.  So, I can't use this param in my modeling.</p>", "body_text": "@ednaruiz - I still owe the thread some failing pytests to demonstrate the issues with various attempted workarounds. (I haven't found the time to yet, and sadly am now working on another project for a few weeks.)\nUltimately though, the only solutions I could come up with are:\n\n\nOver-sample the under-represented class's examples.\n\n\nUse the low-level API to define the loss function by hand such that it uses the contents of the weight column when determining loss, and do not define the weight column as a feature column.  (I'm not sure how to do this yet, but there seems enough data on the web that I think I could figure it out.)\n\n\nTo reiterate the problem: if one attempts to solve the Class Imbalance problem by defining a weight column as a feature column when the class weights correspond to the class, then the network can trivially attain perfect accuracy simply by 100% correlating the weight to the outcome.\nFor example, say I'm building a dog classifier and I have 100 pictures, 95 of which are not dogs, and the remaining 5 are dogs.\nI then add a weight column to my dataset, where the not-dog pictures have a class weight of 1/95 and the dog pictures have a weight of 1/5, and I specify the weight column when constructing my model.\nBy doing so, the weight column becomes a feature column.  Because it's a feature column, my input function needs to supply values for it, and my model will use it to \"guess\" the correct class.\nI then train my model, and find that no matter how bad the dog pictures are, every training run always achieves 100% accuracy.\nThat is, because the weight column is a feature column, the model trivially learns that if the class weight is 1/5, it's a dog, and if the class weight is 1/95, it's not a dog. Viva la ML!! :-)\nFurthermore, since my test runs and prediction runs require that the column be present (because it's a feature column of the model), I'm stuck specifying something when testing and predicting.\nFor my actual model, hoping I could work around this, I tried setting the weight column value for testing and predicting to first the mean and then the midrange, but in both cases the trained models had worse accuracy than if I'd never used the column weights param.  So, I gave up.\nMy conclusion is that one can only use the weight_column_name parameter when the values in that column are not correlated with the outcome.\nHowever, to solve the Class Imbalance problem, the weight_column_name must be correlated with the outcome.  So, I can't use this param in my modeling.", "body": "@ednaruiz - I still owe the thread some failing `pytest`s to demonstrate the issues with various attempted workarounds. (I haven't found the time to yet, and sadly am now working on another project for a few weeks.)\r\n\r\nUltimately though, the only solutions I could come up with are:\r\n\r\n1. Over-sample the under-represented class's examples.  \r\n\r\n2. Use the low-level API to define the loss function by hand such that it uses the contents of the weight column when determining loss, and do not define the weight column as a feature column.  (I'm not sure how to do this yet, but there seems enough data on the web that I think I could figure it out.)\r\n\r\nTo reiterate the problem: if one attempts to solve the Class Imbalance problem by defining a weight column as a feature column when the class weights correspond to the class, then the network can trivially attain perfect accuracy simply by 100% correlating the weight to the outcome.\r\n\r\nFor example, say I'm building a dog classifier and I have 100 pictures, 95 of which are **not** dogs, and the remaining 5 are dogs.\r\n\r\nI then add a weight column to my dataset, where the not-dog pictures have a class weight of 1/95 and the dog pictures have a weight of 1/5, and I specify the weight column when constructing my model.\r\n\r\nBy doing so, the weight column becomes a feature column.  Because it's a feature column, my input function needs to supply values for it, and my model will use it to \"guess\" the correct class.  \r\n\r\nI then train my model, and find that no matter how bad the dog pictures are, every training run always achieves 100% accuracy.  \r\n\r\nThat is, because the weight column is a feature column, the model trivially learns that if the class weight is 1/5, it's a dog, and if the class weight is 1/95, it's not a dog. Viva la ML!! :-)\r\n\r\nFurthermore, since my test runs and prediction runs require that the column be present (because it's a feature column of the model), I'm stuck specifying **something** when testing and predicting.  \r\n\r\nFor my actual model, hoping I could work around this, I tried setting the weight column value for testing and predicting to first the mean and then the midrange, but in both cases the trained models had worse accuracy than if I'd never used the column weights param.  So, I gave up.\r\n\r\nMy conclusion is that one can only use the `weight_column_name` parameter when the values in that column are **not** correlated with the outcome.  \r\n\r\nHowever, to solve the Class Imbalance problem, the `weight_column_name` must be correlated with the outcome.  So, I can't use this param in my modeling."}