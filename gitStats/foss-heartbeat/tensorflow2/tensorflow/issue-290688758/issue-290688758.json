{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16306", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16306/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16306/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16306/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/16306", "id": 290688758, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY0NDY5MTYz", "number": 16306, "title": "Add LINM (Loop Invariant Node Motion) optimization pass in GraphOptim\u2026", "user": {"login": "minminsun", "id": 17308199, "node_id": "MDQ6VXNlcjE3MzA4MTk5", "avatar_url": "https://avatars3.githubusercontent.com/u/17308199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/minminsun", "html_url": "https://github.com/minminsun", "followers_url": "https://api.github.com/users/minminsun/followers", "following_url": "https://api.github.com/users/minminsun/following{/other_user}", "gists_url": "https://api.github.com/users/minminsun/gists{/gist_id}", "starred_url": "https://api.github.com/users/minminsun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/minminsun/subscriptions", "organizations_url": "https://api.github.com/users/minminsun/orgs", "repos_url": "https://api.github.com/users/minminsun/repos", "events_url": "https://api.github.com/users/minminsun/events{/privacy}", "received_events_url": "https://api.github.com/users/minminsun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 419840263, "node_id": "MDU6TGFiZWw0MTk4NDAyNjM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20testing%20(then%20merge)", "name": "awaiting testing (then merge)", "color": "c2e0c6", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2018-01-23T02:37:45Z", "updated_at": "2018-03-02T16:26:32Z", "closed_at": "2018-03-02T16:26:32Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16306", "html_url": "https://github.com/tensorflow/tensorflow/pull/16306", "diff_url": "https://github.com/tensorflow/tensorflow/pull/16306.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/16306.patch"}, "body_html": "<p>\u2026izer<br>\nThis change was inspired by LICM (Loop Invariant Code Motion) of compilers. We observed from some public models, e.g. seq2seq (<a href=\"https://github.com/google/seq2seq\">https://github.com/google/seq2seq</a>) and tensor2tensor (<a href=\"https://github.com/tensorflow/tensor2tensor\">https://github.com/tensorflow/tensor2tensor</a>), as well as some of our in-house models that there are many invariant nodes, including expensive MatMul nodes, inside the loop body.<br>\nThis optimization pass is to apply on Tensorflow computational graph to detect these invariant nodes and move them out of the loop body, that's why we call it LINM (Loop Invariant Node Motion).</p>\n<p>Although there's already a LICM pass in XLA (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/51895fe67434b6e9f5419872f69c7e6092ed69e9/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/51895fe67434b6e9f5419872f69c7e6092ed69e9\"><tt>51895fe</tt></a>), we still feel necessary to add this LINM pass in GraphOptimizer because:</p>\n<ol>\n<li>The XLA LICM pass is based on XlaWhile instruction, but the conversion from loop nodes (Enter/Exit/Switch/Merge/LoopCond) of tf.while to XlaWhile instruction is not hooked up yet (<a href=\"https://groups.google.com/forum/#!topic/xla-dev/IqLyL67cemI\" rel=\"nofollow\">https://groups.google.com/forum/#!topic/xla-dev/IqLyL67cemI</a>)</li>\n<li>We further found out that even if the conversion is hooked up, it works only when all nodes inside the loop has XLA kernel registered. It's a long way to go to get all operators supported by XLA.</li>\n<li>The LINM pass in GraphOptimizer is expected to work no matter whether XLA is on or off.</li>\n</ol>", "body_text": "\u2026izer\nThis change was inspired by LICM (Loop Invariant Code Motion) of compilers. We observed from some public models, e.g. seq2seq (https://github.com/google/seq2seq) and tensor2tensor (https://github.com/tensorflow/tensor2tensor), as well as some of our in-house models that there are many invariant nodes, including expensive MatMul nodes, inside the loop body.\nThis optimization pass is to apply on Tensorflow computational graph to detect these invariant nodes and move them out of the loop body, that's why we call it LINM (Loop Invariant Node Motion).\nAlthough there's already a LICM pass in XLA (51895fe), we still feel necessary to add this LINM pass in GraphOptimizer because:\n\nThe XLA LICM pass is based on XlaWhile instruction, but the conversion from loop nodes (Enter/Exit/Switch/Merge/LoopCond) of tf.while to XlaWhile instruction is not hooked up yet (https://groups.google.com/forum/#!topic/xla-dev/IqLyL67cemI)\nWe further found out that even if the conversion is hooked up, it works only when all nodes inside the loop has XLA kernel registered. It's a long way to go to get all operators supported by XLA.\nThe LINM pass in GraphOptimizer is expected to work no matter whether XLA is on or off.", "body": "\u2026izer\r\nThis change was inspired by LICM (Loop Invariant Code Motion) of compilers. We observed from some public models, e.g. seq2seq (https://github.com/google/seq2seq) and tensor2tensor (https://github.com/tensorflow/tensor2tensor), as well as some of our in-house models that there are many invariant nodes, including expensive MatMul nodes, inside the loop body. \r\nThis optimization pass is to apply on Tensorflow computational graph to detect these invariant nodes and move them out of the loop body, that's why we call it LINM (Loop Invariant Node Motion).\r\n\r\nAlthough there's already a LICM pass in XLA (https://github.com/tensorflow/tensorflow/commit/51895fe67434b6e9f5419872f69c7e6092ed69e9), we still feel necessary to add this LINM pass in GraphOptimizer because:\r\n1. The XLA LICM pass is based on XlaWhile instruction, but the conversion from loop nodes (Enter/Exit/Switch/Merge/LoopCond) of tf.while to XlaWhile instruction is not hooked up yet (https://groups.google.com/forum/#!topic/xla-dev/IqLyL67cemI)\r\n2. We further found out that even if the conversion is hooked up, it works only when all nodes inside the loop has XLA kernel registered. It's a long way to go to get all operators supported by XLA.\r\n3. The LINM pass in GraphOptimizer is expected to work no matter whether XLA is on or off. "}