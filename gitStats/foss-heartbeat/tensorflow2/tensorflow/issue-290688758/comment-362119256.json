{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362119256", "html_url": "https://github.com/tensorflow/tensorflow/pull/16306#issuecomment-362119256", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16306", "id": 362119256, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjExOTI1Ng==", "user": {"login": "yangjunpro", "id": 407784, "node_id": "MDQ6VXNlcjQwNzc4NA==", "avatar_url": "https://avatars3.githubusercontent.com/u/407784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yangjunpro", "html_url": "https://github.com/yangjunpro", "followers_url": "https://api.github.com/users/yangjunpro/followers", "following_url": "https://api.github.com/users/yangjunpro/following{/other_user}", "gists_url": "https://api.github.com/users/yangjunpro/gists{/gist_id}", "starred_url": "https://api.github.com/users/yangjunpro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yangjunpro/subscriptions", "organizations_url": "https://api.github.com/users/yangjunpro/orgs", "repos_url": "https://api.github.com/users/yangjunpro/repos", "events_url": "https://api.github.com/users/yangjunpro/events{/privacy}", "received_events_url": "https://api.github.com/users/yangjunpro/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T00:38:32Z", "updated_at": "2018-02-01T00:45:18Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6969686\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benoitsteiner\">@benoitsteiner</a><br>\nAs to the loop unrolling optimization related to memory saving, it looks like the similar idea from DeepMind's paper \"Memory-Efficient Backpropagation Through Time\". Also there is a related work from OpenAI's <a href=\"https://github.com/openai/gradient-checkpointing\">https://github.com/openai/gradient-checkpointing</a>.</p>\n<p>I agree with the memory optimization idea with loop unrolling. However, I think maybe it will more graceful to be implemented in the grappler MemoryOptimizer? Since in MemoryOptimizer it already has the swap2host and re-computation support, I think this loop unrolling memory optimization could be integrated into the MemoryOptimizer. Thus all memory related optimization code could be placed in the same optimization pass.<br>\nPreviously, we have use the same alike idea of re-computation for implementing a memory-efficient attention-operator(since it is a specific operator, so we haven't pushed it to the community TF repo). And it has to be admit that by implementing memory optimization in a specific way(such as a dedicated memory-efficient operator or within the customization of a special graph pattern such as loop construct) would require less effort. To generalize all those memory optimization behavior in a principal way, much much more effort may be required. But<br>\nas a graph-optimization tool, maybe it would be better to put this loop-unrolling based memory optimization work into the MemoryOptimizer? Please correct me if I am wrong. I think you guys must have deeper thinking about this design.</p>\n<p>As to the \"automated graph placement\" optimization, is it applicable for local execution(I mean single worker) or both for distributed execution? For local execution, I personally feel that the optimization room may not be quite big.  For distributed execution, the story is different, since based on the profiling data, we may choose how to allocate the workload computation among different devices(CPU, GPU, FPGA or other NPUs), also based on the profiling data, we may choose a optimal(or perhaps sub-optimal) distributed solution(worker number, ps number, communication strategy, shard strategy, etc.). Actually, the more complex the execution scenario is, the more room we could have with automatic placement.  We are also started working on this job but it is not easy and before figuring out a general solution, we need to do a lot of task analysis to ensure the abstracted general solution is good enough.</p>\n<p>Thanks</p>", "body_text": "@benoitsteiner\nAs to the loop unrolling optimization related to memory saving, it looks like the similar idea from DeepMind's paper \"Memory-Efficient Backpropagation Through Time\". Also there is a related work from OpenAI's https://github.com/openai/gradient-checkpointing.\nI agree with the memory optimization idea with loop unrolling. However, I think maybe it will more graceful to be implemented in the grappler MemoryOptimizer? Since in MemoryOptimizer it already has the swap2host and re-computation support, I think this loop unrolling memory optimization could be integrated into the MemoryOptimizer. Thus all memory related optimization code could be placed in the same optimization pass.\nPreviously, we have use the same alike idea of re-computation for implementing a memory-efficient attention-operator(since it is a specific operator, so we haven't pushed it to the community TF repo). And it has to be admit that by implementing memory optimization in a specific way(such as a dedicated memory-efficient operator or within the customization of a special graph pattern such as loop construct) would require less effort. To generalize all those memory optimization behavior in a principal way, much much more effort may be required. But\nas a graph-optimization tool, maybe it would be better to put this loop-unrolling based memory optimization work into the MemoryOptimizer? Please correct me if I am wrong. I think you guys must have deeper thinking about this design.\nAs to the \"automated graph placement\" optimization, is it applicable for local execution(I mean single worker) or both for distributed execution? For local execution, I personally feel that the optimization room may not be quite big.  For distributed execution, the story is different, since based on the profiling data, we may choose how to allocate the workload computation among different devices(CPU, GPU, FPGA or other NPUs), also based on the profiling data, we may choose a optimal(or perhaps sub-optimal) distributed solution(worker number, ps number, communication strategy, shard strategy, etc.). Actually, the more complex the execution scenario is, the more room we could have with automatic placement.  We are also started working on this job but it is not easy and before figuring out a general solution, we need to do a lot of task analysis to ensure the abstracted general solution is good enough.\nThanks", "body": "@benoitsteiner \r\nAs to the loop unrolling optimization related to memory saving, it looks like the similar idea from DeepMind's paper \"Memory-Efficient Backpropagation Through Time\". Also there is a related work from OpenAI's https://github.com/openai/gradient-checkpointing. \r\n\r\nI agree with the memory optimization idea with loop unrolling. However, I think maybe it will more graceful to be implemented in the grappler MemoryOptimizer? Since in MemoryOptimizer it already has the swap2host and re-computation support, I think this loop unrolling memory optimization could be integrated into the MemoryOptimizer. Thus all memory related optimization code could be placed in the same optimization pass. \r\nPreviously, we have use the same alike idea of re-computation for implementing a memory-efficient attention-operator(since it is a specific operator, so we haven't pushed it to the community TF repo). And it has to be admit that by implementing memory optimization in a specific way(such as a dedicated memory-efficient operator or within the customization of a special graph pattern such as loop construct) would require less effort. To generalize all those memory optimization behavior in a principal way, much much more effort may be required. But \r\nas a graph-optimization tool, maybe it would be better to put this loop-unrolling based memory optimization work into the MemoryOptimizer? Please correct me if I am wrong. I think you guys must have deeper thinking about this design.\r\n\r\nAs to the \"automated graph placement\" optimization, is it applicable for local execution(I mean single worker) or both for distributed execution? For local execution, I personally feel that the optimization room may not be quite big.  For distributed execution, the story is different, since based on the profiling data, we may choose how to allocate the workload computation among different devices(CPU, GPU, FPGA or other NPUs), also based on the profiling data, we may choose a optimal(or perhaps sub-optimal) distributed solution(worker number, ps number, communication strategy, shard strategy, etc.). Actually, the more complex the execution scenario is, the more room we could have with automatic placement.  We are also started working on this job but it is not easy and before figuring out a general solution, we need to do a lot of task analysis to ensure the abstracted general solution is good enough. \r\n\r\nThanks"}