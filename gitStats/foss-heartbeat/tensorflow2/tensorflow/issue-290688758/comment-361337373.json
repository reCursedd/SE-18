{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361337373", "html_url": "https://github.com/tensorflow/tensorflow/pull/16306#issuecomment-361337373", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16306", "id": 361337373, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTMzNzM3Mw==", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-29T18:20:21Z", "updated_at": "2018-01-31T18:33:34Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17308199\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/minminsun\">@minminsun</a> Most current work on graph-level optimizations is done in the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler\">Grappler subdirectory</a>. Grappler does graph optimizations (constant folding &amp; materialization, CSE, arithmetic optimizations, graph pruning, layout optimization etc.), and is on by default in the TF runtime. It operates on the same high-level graph representation (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto#L14\">GraphDef</a>) as TensorFlow.</p>\n<p>Grappler has a number of advantages over the older graph optimizer framework:</p>\n<ul>\n<li>It is actively maintained by a team at Google.</li>\n<li>Test coverage is formidable - optimizers are tested against a Google-scale collection of model graphs.  Also, by being on by default, new optimizations are run through every TensorFlow unit test.</li>\n<li>It is backend agnostic: The optimizations will apply whether you use the TensorFlow runtime, XLA, TensorRT, or any other backend, including mobile and web backends like deeplearn.js.</li>\n<li>It can be applied offline to stored graphs, which can be useful for mobile or serving from stored graphs.</li>\n<li>It is applied at the full graph level, before partitioning, which allows full graph optimizations, which may not be possible after partitioning.</li>\n</ul>\n<p>The individual passes are here:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler/optimizers\">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler/optimizers</a></p>\n<p>We would be happy to help you migrate your code to the Grappler framework, and change it to operate as another Grappler optimizer pass run by the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L81\">meta-optimizer</a>. The main difference is that Grappler works at the GraphDef level, but the underlying algorithms for your pass should be the same.</p>", "body_text": "@minminsun Most current work on graph-level optimizations is done in the Grappler subdirectory. Grappler does graph optimizations (constant folding & materialization, CSE, arithmetic optimizations, graph pruning, layout optimization etc.), and is on by default in the TF runtime. It operates on the same high-level graph representation (GraphDef) as TensorFlow.\nGrappler has a number of advantages over the older graph optimizer framework:\n\nIt is actively maintained by a team at Google.\nTest coverage is formidable - optimizers are tested against a Google-scale collection of model graphs.  Also, by being on by default, new optimizations are run through every TensorFlow unit test.\nIt is backend agnostic: The optimizations will apply whether you use the TensorFlow runtime, XLA, TensorRT, or any other backend, including mobile and web backends like deeplearn.js.\nIt can be applied offline to stored graphs, which can be useful for mobile or serving from stored graphs.\nIt is applied at the full graph level, before partitioning, which allows full graph optimizations, which may not be possible after partitioning.\n\nThe individual passes are here:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler/optimizers\nWe would be happy to help you migrate your code to the Grappler framework, and change it to operate as another Grappler optimizer pass run by the meta-optimizer. The main difference is that Grappler works at the GraphDef level, but the underlying algorithms for your pass should be the same.", "body": "@minminsun Most current work on graph-level optimizations is done in the [Grappler subdirectory](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler). Grappler does graph optimizations (constant folding & materialization, CSE, arithmetic optimizations, graph pruning, layout optimization etc.), and is on by default in the TF runtime. It operates on the same high-level graph representation ([GraphDef](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto#L14)) as TensorFlow. \r\n\r\nGrappler has a number of advantages over the older graph optimizer framework:\r\n\r\n* It is actively maintained by a team at Google.\r\n* Test coverage is formidable - optimizers are tested against a Google-scale collection of model graphs.  Also, by being on by default, new optimizations are run through every TensorFlow unit test.\r\n* It is backend agnostic: The optimizations will apply whether you use the TensorFlow runtime, XLA, TensorRT, or any other backend, including mobile and web backends like deeplearn.js. \r\n* It can be applied offline to stored graphs, which can be useful for mobile or serving from stored graphs.\r\n* It is applied at the full graph level, before partitioning, which allows full graph optimizations, which may not be possible after partitioning.\r\n\r\nThe individual passes are here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler/optimizers\r\n\r\nWe would be happy to help you migrate your code to the Grappler framework, and change it to operate as another Grappler optimizer pass run by the [meta-optimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L81). The main difference is that Grappler works at the GraphDef level, but the underlying algorithms for your pass should be the same.\r\n\r\n"}