{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361901504", "html_url": "https://github.com/tensorflow/tensorflow/pull/16306#issuecomment-361901504", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16306", "id": 361901504, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTkwMTUwNA==", "user": {"login": "yangjunpro", "id": 407784, "node_id": "MDQ6VXNlcjQwNzc4NA==", "avatar_url": "https://avatars3.githubusercontent.com/u/407784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yangjunpro", "html_url": "https://github.com/yangjunpro", "followers_url": "https://api.github.com/users/yangjunpro/followers", "following_url": "https://api.github.com/users/yangjunpro/following{/other_user}", "gists_url": "https://api.github.com/users/yangjunpro/gists{/gist_id}", "starred_url": "https://api.github.com/users/yangjunpro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yangjunpro/subscriptions", "organizations_url": "https://api.github.com/users/yangjunpro/orgs", "repos_url": "https://api.github.com/users/yangjunpro/repos", "events_url": "https://api.github.com/users/yangjunpro/events{/privacy}", "received_events_url": "https://api.github.com/users/yangjunpro/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T11:14:26Z", "updated_at": "2018-01-31T11:14:26Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6969686\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benoitsteiner\">@benoitsteiner</a></p>\n<p>Thanks for sharing the detailed information as to your plan about loop optimizations.<br>\nWe have also made a discussion about refactoring LINM into grappler passes. Currently we plan to release the grappler pass based LINM PR by the end of February since there will be a long traditional Chinese sprint festivals:) and also at present there is an ongoing project which is close to its release date and we don't want to make context switch too frequently.<br>\nLet us know whether this time slot is suitable for you.</p>\n<p>Before the official PR to be submit, we would keep update with the community to ensure what we are doing is at good pace. For example, in your replies, it is mentioned that \"We implemented utilities that are helpful when optimizing loops (frame identification, ...)\", actually in LINM we have already implemented the same \"frame identification\" utility function, and if your implementation is already ready, we may base on your utility functions within our LINM implementation. If it is not ready, we would also like to contribute to its implementation since frame identification actually is a somewhat tricky and complex functionality.</p>\n<p>As to those loop optimization plans mentioned in your previous reply, they are really interesting work. Internally we had some discussions about loop unrolling but we are not sure about the performance benefit with it since in our understanding loop unrolling in TF graph level may not bring as much performance gain as in traditional IR/low-level language level because the condition check overhead will be mitigated by the long execution of loop body itself. Also another potential performance improvement with loop unrolling is the interleave execution of loop iterations, however with our analysis, it looks that not too many DL workloads will trigger such optimization behavior.<br>\nActually an interesting question we keep asking ourselves is that \"which kind of traditional compiler optimization techniques may be suitable for Deep Learning graph level optimization?\".  For XLA, it could re-implement a lots of traditional  compiler optimization passes since HLO IR is quite like traditional programming language(XLA has its LLVLM IR emitter and LLVM backend for different targets, GPU  or CPU,etc.). But also I am wondering whether it is more productive by leveraging existing compiler for those mature targets, such as NVIDIA's nvcc or Intel's ICC. I don't know the exact answer.Also I have started a discussion here(<a href=\"https://groups.google.com/forum/#!topic/xla-dev/doFohtEAoLU\" rel=\"nofollow\">https://groups.google.com/forum/#!topic/xla-dev/doFohtEAoLU</a>), and it is still a open discussion thread.</p>\n<p>As to our graph-level optimization plan, I can provide a list as following:</p>\n<ol>\n<li>We are currently implementing a template-based op fusion engine(somewhat like TensorRT's catalog based behavior), since we found that there are some op fusion patterns with high usage frequency, for those \"high frequent\" op fusion patterns, we think it deserves to write a macro-op for it then use template-based op fusion to replace the origin subgraph with the macro-op within the graph optimization passes. We have already made some improvement with this op fusion engine, and significant performance improvement is observed. Actually in TensorFlow, we have found there are some offline post-processing tools like graph_transform which will transform the original graph into another one with better performance(such as replace conv+bias+relu with a single op, and micro-op based BN with fused BN, etc.), this is a cheaper way. However, by introducing a post-processing phase, it will require some behavior change of end-users, which may bring some overhead, especially for existing systems. So we choose to add this template-based op fusion engine as a new graph optimization pass. Fortunately, we have caught up with TF team on time, so we will ensure most of the logic will be implemented into grappler passes:).</li>\n<li>Another optimization plan wandering in our mind is that we are also wondering whether the \"non-mutable\" property of model weights at inference phase could be further leveraged for performance boost. For example, in traditional EDA compiler<br>\ncommunity, a specific logic optimization passes will be introduced for doing some logic optimizations to replace the original logic with a new one with better performance(such as mux logic optimization). We are curious whether such optimization could be imported into TF. I know grappler already has its arithmetic_optimization pass which does some strength reduction optimization, but I am wondering whether we could do the optimization in a more principal way, such as formulating it as  SAT problem. This is not a easy problem, so after doing some initial investigation, we temporarily switch to other threads. If Google guys have interest, we may collaboratively discuss the possibility of this optimization direction.</li>\n<li>Profiling-guided optimization. Since in Alibaba, we had both in-house and public cloud-based environment, and in those environments, various workloads are keeping running. For each workloads, lot of optimization passes may be triggered.  We believe that by collecting those online runtime behavior as profiling data, we may guide the optimization of  subsequent workloads execution. Of course, based on Google's paper in Micro 2010, it looks that Google already has the company-level online profiling tool earlier, maybe you have already have such optimization enabled within your in-house TF environment.</li>\n</ol>\n<p>Thanks</p>", "body_text": "@benoitsteiner\nThanks for sharing the detailed information as to your plan about loop optimizations.\nWe have also made a discussion about refactoring LINM into grappler passes. Currently we plan to release the grappler pass based LINM PR by the end of February since there will be a long traditional Chinese sprint festivals:) and also at present there is an ongoing project which is close to its release date and we don't want to make context switch too frequently.\nLet us know whether this time slot is suitable for you.\nBefore the official PR to be submit, we would keep update with the community to ensure what we are doing is at good pace. For example, in your replies, it is mentioned that \"We implemented utilities that are helpful when optimizing loops (frame identification, ...)\", actually in LINM we have already implemented the same \"frame identification\" utility function, and if your implementation is already ready, we may base on your utility functions within our LINM implementation. If it is not ready, we would also like to contribute to its implementation since frame identification actually is a somewhat tricky and complex functionality.\nAs to those loop optimization plans mentioned in your previous reply, they are really interesting work. Internally we had some discussions about loop unrolling but we are not sure about the performance benefit with it since in our understanding loop unrolling in TF graph level may not bring as much performance gain as in traditional IR/low-level language level because the condition check overhead will be mitigated by the long execution of loop body itself. Also another potential performance improvement with loop unrolling is the interleave execution of loop iterations, however with our analysis, it looks that not too many DL workloads will trigger such optimization behavior.\nActually an interesting question we keep asking ourselves is that \"which kind of traditional compiler optimization techniques may be suitable for Deep Learning graph level optimization?\".  For XLA, it could re-implement a lots of traditional  compiler optimization passes since HLO IR is quite like traditional programming language(XLA has its LLVLM IR emitter and LLVM backend for different targets, GPU  or CPU,etc.). But also I am wondering whether it is more productive by leveraging existing compiler for those mature targets, such as NVIDIA's nvcc or Intel's ICC. I don't know the exact answer.Also I have started a discussion here(https://groups.google.com/forum/#!topic/xla-dev/doFohtEAoLU), and it is still a open discussion thread.\nAs to our graph-level optimization plan, I can provide a list as following:\n\nWe are currently implementing a template-based op fusion engine(somewhat like TensorRT's catalog based behavior), since we found that there are some op fusion patterns with high usage frequency, for those \"high frequent\" op fusion patterns, we think it deserves to write a macro-op for it then use template-based op fusion to replace the origin subgraph with the macro-op within the graph optimization passes. We have already made some improvement with this op fusion engine, and significant performance improvement is observed. Actually in TensorFlow, we have found there are some offline post-processing tools like graph_transform which will transform the original graph into another one with better performance(such as replace conv+bias+relu with a single op, and micro-op based BN with fused BN, etc.), this is a cheaper way. However, by introducing a post-processing phase, it will require some behavior change of end-users, which may bring some overhead, especially for existing systems. So we choose to add this template-based op fusion engine as a new graph optimization pass. Fortunately, we have caught up with TF team on time, so we will ensure most of the logic will be implemented into grappler passes:).\nAnother optimization plan wandering in our mind is that we are also wondering whether the \"non-mutable\" property of model weights at inference phase could be further leveraged for performance boost. For example, in traditional EDA compiler\ncommunity, a specific logic optimization passes will be introduced for doing some logic optimizations to replace the original logic with a new one with better performance(such as mux logic optimization). We are curious whether such optimization could be imported into TF. I know grappler already has its arithmetic_optimization pass which does some strength reduction optimization, but I am wondering whether we could do the optimization in a more principal way, such as formulating it as  SAT problem. This is not a easy problem, so after doing some initial investigation, we temporarily switch to other threads. If Google guys have interest, we may collaboratively discuss the possibility of this optimization direction.\nProfiling-guided optimization. Since in Alibaba, we had both in-house and public cloud-based environment, and in those environments, various workloads are keeping running. For each workloads, lot of optimization passes may be triggered.  We believe that by collecting those online runtime behavior as profiling data, we may guide the optimization of  subsequent workloads execution. Of course, based on Google's paper in Micro 2010, it looks that Google already has the company-level online profiling tool earlier, maybe you have already have such optimization enabled within your in-house TF environment.\n\nThanks", "body": "@benoitsteiner \r\n\r\nThanks for sharing the detailed information as to your plan about loop optimizations.\r\nWe have also made a discussion about refactoring LINM into grappler passes. Currently we plan to release the grappler pass based LINM PR by the end of February since there will be a long traditional Chinese sprint festivals:) and also at present there is an ongoing project which is close to its release date and we don't want to make context switch too frequently. \r\nLet us know whether this time slot is suitable for you.\r\n\r\nBefore the official PR to be submit, we would keep update with the community to ensure what we are doing is at good pace. For example, in your replies, it is mentioned that \"We implemented utilities that are helpful when optimizing loops (frame identification, ...)\", actually in LINM we have already implemented the same \"frame identification\" utility function, and if your implementation is already ready, we may base on your utility functions within our LINM implementation. If it is not ready, we would also like to contribute to its implementation since frame identification actually is a somewhat tricky and complex functionality. \r\n\r\nAs to those loop optimization plans mentioned in your previous reply, they are really interesting work. Internally we had some discussions about loop unrolling but we are not sure about the performance benefit with it since in our understanding loop unrolling in TF graph level may not bring as much performance gain as in traditional IR/low-level language level because the condition check overhead will be mitigated by the long execution of loop body itself. Also another potential performance improvement with loop unrolling is the interleave execution of loop iterations, however with our analysis, it looks that not too many DL workloads will trigger such optimization behavior. \r\nActually an interesting question we keep asking ourselves is that \"which kind of traditional compiler optimization techniques may be suitable for Deep Learning graph level optimization?\".  For XLA, it could re-implement a lots of traditional  compiler optimization passes since HLO IR is quite like traditional programming language(XLA has its LLVLM IR emitter and LLVM backend for different targets, GPU  or CPU,etc.). But also I am wondering whether it is more productive by leveraging existing compiler for those mature targets, such as NVIDIA's nvcc or Intel's ICC. I don't know the exact answer.Also I have started a discussion here(https://groups.google.com/forum/#!topic/xla-dev/doFohtEAoLU), and it is still a open discussion thread. \r\n\r\nAs to our graph-level optimization plan, I can provide a list as following:\r\n1. We are currently implementing a template-based op fusion engine(somewhat like TensorRT's catalog based behavior), since we found that there are some op fusion patterns with high usage frequency, for those \"high frequent\" op fusion patterns, we think it deserves to write a macro-op for it then use template-based op fusion to replace the origin subgraph with the macro-op within the graph optimization passes. We have already made some improvement with this op fusion engine, and significant performance improvement is observed. Actually in TensorFlow, we have found there are some offline post-processing tools like graph_transform which will transform the original graph into another one with better performance(such as replace conv+bias+relu with a single op, and micro-op based BN with fused BN, etc.), this is a cheaper way. However, by introducing a post-processing phase, it will require some behavior change of end-users, which may bring some overhead, especially for existing systems. So we choose to add this template-based op fusion engine as a new graph optimization pass. Fortunately, we have caught up with TF team on time, so we will ensure most of the logic will be implemented into grappler passes:). \r\n2. Another optimization plan wandering in our mind is that we are also wondering whether the \"non-mutable\" property of model weights at inference phase could be further leveraged for performance boost. For example, in traditional EDA compiler \r\ncommunity, a specific logic optimization passes will be introduced for doing some logic optimizations to replace the original logic with a new one with better performance(such as mux logic optimization). We are curious whether such optimization could be imported into TF. I know grappler already has its arithmetic_optimization pass which does some strength reduction optimization, but I am wondering whether we could do the optimization in a more principal way, such as formulating it as  SAT problem. This is not a easy problem, so after doing some initial investigation, we temporarily switch to other threads. If Google guys have interest, we may collaboratively discuss the possibility of this optimization direction. \r\n3. Profiling-guided optimization. Since in Alibaba, we had both in-house and public cloud-based environment, and in those environments, various workloads are keeping running. For each workloads, lot of optimization passes may be triggered.  We believe that by collecting those online runtime behavior as profiling data, we may guide the optimization of  subsequent workloads execution. Of course, based on Google's paper in Micro 2010, it looks that Google already has the company-level online profiling tool earlier, maybe you have already have such optimization enabled within your in-house TF environment. \r\n\r\nThanks"}