{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362025272", "html_url": "https://github.com/tensorflow/tensorflow/pull/16306#issuecomment-362025272", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16306", "id": 362025272, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjAyNTI3Mg==", "user": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T18:27:09Z", "updated_at": "2018-01-31T18:27:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=407784\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yangjunpro\">@yangjunpro</a><br>\nReleasing the loop invariant code by the end of February would be great. That would leave plenty of time to make it part of the 1.7 release of TensorFlow.<br>\nWe put common grappler utilities in the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler/utils\">grappler/utils</a> directory. In particular, our frame inference api resides in the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler/utils/frame.h\">frame.h</a> header file. Another commonly used tools is our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_properties.h\">shape and type inference</a> code.</p>\n<p>We believe that loop unrolling can help the performance of models during training since it would cut down some of the overhead needed to feed the activations generated during the forward pass to the corresponding gradient computations during the backward pass. We could also take advantage of unrolling to reduce the memory usage: The idea is to unroll the loop k times, and only keep the activation generated at the end of each unrolled forward mini sequence of k iterations. The would reduce the amount of memory needed to keep activations around by a factor of k at the expense of having to recompute the activations at the beginning of each unrolled backward sequence of k iterations.</p>\n<p>I've worked on formal verification of ASIC circuits in the past, so I've pondered the use of SAT solvers to drive some of the optimizations we're doing. I believe there is a lot of potential in doing this, but given the initial investment needed to get this off the ground I doubt we will get to this before next year. In any case, we'll get in touch with you before we start.</p>\n<p>We've invested a lot of time in making it possible to collect performance data either by <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/measuring_cost_estimator.h\">running the graph</a>, or by <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/analytical_cost_estimator.h\">simulating</a> the execution of the graph. We're in the process of releasing the first optimizer that takes advantage these predictions (the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/memory_optimizer.h\">memory optimizer</a>). We will start releasing the second one (automated graph placement) later this month. There are many more graph level optimizations that could benefit from this though. The first one that comes to mind is to automatically choose the between the sparse, dense, or semi sparse implementations of common TF operations depending on the input workload.</p>", "body_text": "@yangjunpro\nReleasing the loop invariant code by the end of February would be great. That would leave plenty of time to make it part of the 1.7 release of TensorFlow.\nWe put common grappler utilities in the grappler/utils directory. In particular, our frame inference api resides in the frame.h header file. Another commonly used tools is our shape and type inference code.\nWe believe that loop unrolling can help the performance of models during training since it would cut down some of the overhead needed to feed the activations generated during the forward pass to the corresponding gradient computations during the backward pass. We could also take advantage of unrolling to reduce the memory usage: The idea is to unroll the loop k times, and only keep the activation generated at the end of each unrolled forward mini sequence of k iterations. The would reduce the amount of memory needed to keep activations around by a factor of k at the expense of having to recompute the activations at the beginning of each unrolled backward sequence of k iterations.\nI've worked on formal verification of ASIC circuits in the past, so I've pondered the use of SAT solvers to drive some of the optimizations we're doing. I believe there is a lot of potential in doing this, but given the initial investment needed to get this off the ground I doubt we will get to this before next year. In any case, we'll get in touch with you before we start.\nWe've invested a lot of time in making it possible to collect performance data either by running the graph, or by simulating the execution of the graph. We're in the process of releasing the first optimizer that takes advantage these predictions (the memory optimizer). We will start releasing the second one (automated graph placement) later this month. There are many more graph level optimizations that could benefit from this though. The first one that comes to mind is to automatically choose the between the sparse, dense, or semi sparse implementations of common TF operations depending on the input workload.", "body": "@yangjunpro\r\nReleasing the loop invariant code by the end of February would be great. That would leave plenty of time to make it part of the 1.7 release of TensorFlow.\r\n We put common grappler utilities in the [grappler/utils](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler/utils) directory. In particular, our frame inference api resides in the [frame.h](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/grappler/utils/frame.h) header file. Another commonly used tools is our [shape and type inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_properties.h) code.\r\n\r\nWe believe that loop unrolling can help the performance of models during training since it would cut down some of the overhead needed to feed the activations generated during the forward pass to the corresponding gradient computations during the backward pass. We could also take advantage of unrolling to reduce the memory usage: The idea is to unroll the loop k times, and only keep the activation generated at the end of each unrolled forward mini sequence of k iterations. The would reduce the amount of memory needed to keep activations around by a factor of k at the expense of having to recompute the activations at the beginning of each unrolled backward sequence of k iterations.\r\n\r\nI've worked on formal verification of ASIC circuits in the past, so I've pondered the use of SAT solvers to drive some of the optimizations we're doing. I believe there is a lot of potential in doing this, but given the initial investment needed to get this off the ground I doubt we will get to this before next year. In any case, we'll get in touch with you before we start.\r\n\r\nWe've invested a lot of time in making it possible to collect performance data either by [running the graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/measuring_cost_estimator.h), or by [simulating](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/analytical_cost_estimator.h) the execution of the graph. We're in the process of releasing the first optimizer that takes advantage these predictions (the [memory optimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/memory_optimizer.h)). We will start releasing the second one (automated graph placement) later this month. There are many more graph level optimizations that could benefit from this though. The first one that comes to mind is to automatically choose the between the sparse, dense, or semi sparse implementations of common TF operations depending on the input workload. "}