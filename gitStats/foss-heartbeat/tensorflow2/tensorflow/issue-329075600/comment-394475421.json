{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/394475421", "html_url": "https://github.com/tensorflow/tensorflow/issues/19750#issuecomment-394475421", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19750", "id": 394475421, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDQ3NTQyMQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-04T19:45:14Z", "updated_at": "2018-06-04T19:45:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It's a known issue that floating point computations can produce small errors on modern hardware, and that small errors can blow up into big errors over time of training. For instance, SSE optimized-code can lead to <a href=\"http://blog.nag.com/2011/02/wandering-precision.html\" rel=\"nofollow\">slightly different results</a> coming out of the same computation, depending on how full the memory is.</p>\n<p>The ultimate solution is to make your computations robust to small errors/non-determinism. But also, if you narrow down the problem, it might be possible the fix the errors. IE, non-determinism in reduce_sum has been fixed on GPU recently.</p>\n<p>In your case, loss of symmetry implies that result of <code>a b'</code> and <code>a' b</code> are not the same. Can isolate a specific <code>a</code> and <code>b</code> that produce this?</p>", "body_text": "It's a known issue that floating point computations can produce small errors on modern hardware, and that small errors can blow up into big errors over time of training. For instance, SSE optimized-code can lead to slightly different results coming out of the same computation, depending on how full the memory is.\nThe ultimate solution is to make your computations robust to small errors/non-determinism. But also, if you narrow down the problem, it might be possible the fix the errors. IE, non-determinism in reduce_sum has been fixed on GPU recently.\nIn your case, loss of symmetry implies that result of a b' and a' b are not the same. Can isolate a specific a and b that produce this?", "body": "It's a known issue that floating point computations can produce small errors on modern hardware, and that small errors can blow up into big errors over time of training. For instance, SSE optimized-code can lead to [slightly different results](http://blog.nag.com/2011/02/wandering-precision.html) coming out of the same computation, depending on how full the memory is.\r\n\r\nThe ultimate solution is to make your computations robust to small errors/non-determinism. But also, if you narrow down the problem, it might be possible the fix the errors. IE, non-determinism in reduce_sum has been fixed on GPU recently.\r\n\r\nIn your case, loss of symmetry implies that result of `a b'` and `a' b` are not the same. Can isolate a specific `a` and `b` that produce this?\r\n"}