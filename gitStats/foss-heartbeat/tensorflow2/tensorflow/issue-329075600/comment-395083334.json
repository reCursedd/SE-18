{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/395083334", "html_url": "https://github.com/tensorflow/tensorflow/issues/19750#issuecomment-395083334", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19750", "id": 395083334, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NTA4MzMzNA==", "user": {"login": "Zenome84", "id": 39647569, "node_id": "MDQ6VXNlcjM5NjQ3NTY5", "avatar_url": "https://avatars0.githubusercontent.com/u/39647569?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zenome84", "html_url": "https://github.com/Zenome84", "followers_url": "https://api.github.com/users/Zenome84/followers", "following_url": "https://api.github.com/users/Zenome84/following{/other_user}", "gists_url": "https://api.github.com/users/Zenome84/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zenome84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zenome84/subscriptions", "organizations_url": "https://api.github.com/users/Zenome84/orgs", "repos_url": "https://api.github.com/users/Zenome84/repos", "events_url": "https://api.github.com/users/Zenome84/events{/privacy}", "received_events_url": "https://api.github.com/users/Zenome84/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-06T14:13:11Z", "updated_at": "2018-06-06T14:13:11Z", "author_association": "NONE", "body_html": "<p>Also, the following variant using a custom <code>tensor_product</code> and using <code>tf.tensordot</code> suffers the same problem (this version would be necessary to work with higher moments):</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n#  Tensor Product for Ease\ndef tensor_product(*e):\n    \"\"\" Tensor product of elements \"\"\"\n    if len(e) == 1:\n        return e\n    elif len(e) == 2:\n        a, b = e\n        r_a = len(a.get_shape().as_list())\n        r_b = len(b.get_shape().as_list())\n        s_a = tf.concat([tf.shape(a), tf.constant([1] * r_b)], axis=0)\n        s_b = tf.concat([tf.constant([1] * r_a), tf.shape(b)], axis=0)\n        a_reshaped = tf.reshape(a, s_a)\n        b_reshaped = tf.reshape(b, s_b)\n        return a_reshaped * b_reshaped\n    prod = e[0]\n    for tensor in e[1:]:\n        prod = tensor_product(prod, tensor)\n    return prod\n\ndim = 3\ndimN = 5\n\n# We will load a positive definite matrix for K\nK_init = tf.placeholder(tf.float32, [dim, dim])\nK_ = tf.Variable(K_init, trainable=False, collections=[])\n\nC = tf.Variable(tf.ones([dimN, dim]), dtype=tf.float32)\na = tf.Variable(tf.ones([dimN]), dtype=tf.float32)\n\nCC = tensor_product(C, C)\naa = tensor_product(a, a)\n# We build a Recurrent Network for K\nK = [K_]\n# Kca should be symmetric: C*K*C' is symmetric, and A is diagonal; so their sum is symmetric \nKca = tf.expand_dims(tf.tensordot(CC, K[0], [[1, 3], [0, 1]]) + tf.diag(a), 0)\n# invKca should be symmetric as the inverse of a Kca\ninvKca = tf.expand_dims(tf.linalg.inv(Kca[0]), 0)\n# K[1] should be symmetric since, again C'*invKca*C is symmetric.\nK = tf.concat([K, tf.expand_dims(tf.tensordot(CC, invKca[0], [[0, 2], [0, 1]]), 0)], 0)\n\n# Some step cost function\ncostN = tf.expand_dims(tf.linalg.logdet(Kca[0]) + tf.matmul(tf.matmul([a], invKca[0]), [a], transpose_b=True)[0][0], 0)\n\n# The recursion\nfor n in range(1, 10):\n    Kca = tf.concat([Kca, tf.expand_dims(tf.tensordot(CC, K[n], [[1, 3], [0, 1]]) + tf.diag(a), 0)], 0)\n    invKca = tf.concat([invKca, tf.expand_dims(tf.linalg.inv(Kca[n]), 0)], 0)\n    K = tf.concat([K, tf.expand_dims(tf.tensordot(CC, invKca[n], [[0, 2], [0, 1]]), 0)], 0)\n\n    costN = tf.concat([costN, tf.expand_dims(tf.linalg.logdet(Kca[n]) + tf.matmul(tf.matmul([a], invKca[n]), [a], transpose_b=True)[0][0], 0)], 0)\n\ncost = tf.reduce_sum(costN)\n# If this example blows up, decrease the learning rate.\nstep = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Positive definite matrix K0\nL0 = np.array([[1.0, 0.0, 0.0], \\\n    [3.5, 2.5, 0.0], \\\n    [-2.0, -1.2, 3.0]])\nK0 = np.matmul(L0, L0.transpose())\n\nsess.run(K_.initializer, feed_dict={K_init:K0})\nfor n in range(100):\n    sess.run(step)\n    print(\"Cost: \" + str(sess.run(cost)))\n\n# Lo and Behold, K[10] is not symmetric\nprint(sess.run(K[10]) == sess.run(K[10]).transpose())\nprint(sess.run(K[10]))\nprint(\"Done\")\n</code></pre>", "body_text": "Also, the following variant using a custom tensor_product and using tf.tensordot suffers the same problem (this version would be necessary to work with higher moments):\nimport tensorflow as tf\nimport numpy as np\n\n#  Tensor Product for Ease\ndef tensor_product(*e):\n    \"\"\" Tensor product of elements \"\"\"\n    if len(e) == 1:\n        return e\n    elif len(e) == 2:\n        a, b = e\n        r_a = len(a.get_shape().as_list())\n        r_b = len(b.get_shape().as_list())\n        s_a = tf.concat([tf.shape(a), tf.constant([1] * r_b)], axis=0)\n        s_b = tf.concat([tf.constant([1] * r_a), tf.shape(b)], axis=0)\n        a_reshaped = tf.reshape(a, s_a)\n        b_reshaped = tf.reshape(b, s_b)\n        return a_reshaped * b_reshaped\n    prod = e[0]\n    for tensor in e[1:]:\n        prod = tensor_product(prod, tensor)\n    return prod\n\ndim = 3\ndimN = 5\n\n# We will load a positive definite matrix for K\nK_init = tf.placeholder(tf.float32, [dim, dim])\nK_ = tf.Variable(K_init, trainable=False, collections=[])\n\nC = tf.Variable(tf.ones([dimN, dim]), dtype=tf.float32)\na = tf.Variable(tf.ones([dimN]), dtype=tf.float32)\n\nCC = tensor_product(C, C)\naa = tensor_product(a, a)\n# We build a Recurrent Network for K\nK = [K_]\n# Kca should be symmetric: C*K*C' is symmetric, and A is diagonal; so their sum is symmetric \nKca = tf.expand_dims(tf.tensordot(CC, K[0], [[1, 3], [0, 1]]) + tf.diag(a), 0)\n# invKca should be symmetric as the inverse of a Kca\ninvKca = tf.expand_dims(tf.linalg.inv(Kca[0]), 0)\n# K[1] should be symmetric since, again C'*invKca*C is symmetric.\nK = tf.concat([K, tf.expand_dims(tf.tensordot(CC, invKca[0], [[0, 2], [0, 1]]), 0)], 0)\n\n# Some step cost function\ncostN = tf.expand_dims(tf.linalg.logdet(Kca[0]) + tf.matmul(tf.matmul([a], invKca[0]), [a], transpose_b=True)[0][0], 0)\n\n# The recursion\nfor n in range(1, 10):\n    Kca = tf.concat([Kca, tf.expand_dims(tf.tensordot(CC, K[n], [[1, 3], [0, 1]]) + tf.diag(a), 0)], 0)\n    invKca = tf.concat([invKca, tf.expand_dims(tf.linalg.inv(Kca[n]), 0)], 0)\n    K = tf.concat([K, tf.expand_dims(tf.tensordot(CC, invKca[n], [[0, 2], [0, 1]]), 0)], 0)\n\n    costN = tf.concat([costN, tf.expand_dims(tf.linalg.logdet(Kca[n]) + tf.matmul(tf.matmul([a], invKca[n]), [a], transpose_b=True)[0][0], 0)], 0)\n\ncost = tf.reduce_sum(costN)\n# If this example blows up, decrease the learning rate.\nstep = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Positive definite matrix K0\nL0 = np.array([[1.0, 0.0, 0.0], \\\n    [3.5, 2.5, 0.0], \\\n    [-2.0, -1.2, 3.0]])\nK0 = np.matmul(L0, L0.transpose())\n\nsess.run(K_.initializer, feed_dict={K_init:K0})\nfor n in range(100):\n    sess.run(step)\n    print(\"Cost: \" + str(sess.run(cost)))\n\n# Lo and Behold, K[10] is not symmetric\nprint(sess.run(K[10]) == sess.run(K[10]).transpose())\nprint(sess.run(K[10]))\nprint(\"Done\")", "body": "Also, the following variant using a custom `tensor_product` and using `tf.tensordot` suffers the same problem (this version would be necessary to work with higher moments):\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    #  Tensor Product for Ease\r\n    def tensor_product(*e):\r\n        \"\"\" Tensor product of elements \"\"\"\r\n        if len(e) == 1:\r\n            return e\r\n        elif len(e) == 2:\r\n            a, b = e\r\n            r_a = len(a.get_shape().as_list())\r\n            r_b = len(b.get_shape().as_list())\r\n            s_a = tf.concat([tf.shape(a), tf.constant([1] * r_b)], axis=0)\r\n            s_b = tf.concat([tf.constant([1] * r_a), tf.shape(b)], axis=0)\r\n            a_reshaped = tf.reshape(a, s_a)\r\n            b_reshaped = tf.reshape(b, s_b)\r\n            return a_reshaped * b_reshaped\r\n        prod = e[0]\r\n        for tensor in e[1:]:\r\n            prod = tensor_product(prod, tensor)\r\n        return prod\r\n\r\n    dim = 3\r\n    dimN = 5\r\n\r\n    # We will load a positive definite matrix for K\r\n    K_init = tf.placeholder(tf.float32, [dim, dim])\r\n    K_ = tf.Variable(K_init, trainable=False, collections=[])\r\n\r\n    C = tf.Variable(tf.ones([dimN, dim]), dtype=tf.float32)\r\n    a = tf.Variable(tf.ones([dimN]), dtype=tf.float32)\r\n\r\n    CC = tensor_product(C, C)\r\n    aa = tensor_product(a, a)\r\n    # We build a Recurrent Network for K\r\n    K = [K_]\r\n    # Kca should be symmetric: C*K*C' is symmetric, and A is diagonal; so their sum is symmetric \r\n    Kca = tf.expand_dims(tf.tensordot(CC, K[0], [[1, 3], [0, 1]]) + tf.diag(a), 0)\r\n    # invKca should be symmetric as the inverse of a Kca\r\n    invKca = tf.expand_dims(tf.linalg.inv(Kca[0]), 0)\r\n    # K[1] should be symmetric since, again C'*invKca*C is symmetric.\r\n    K = tf.concat([K, tf.expand_dims(tf.tensordot(CC, invKca[0], [[0, 2], [0, 1]]), 0)], 0)\r\n\r\n    # Some step cost function\r\n    costN = tf.expand_dims(tf.linalg.logdet(Kca[0]) + tf.matmul(tf.matmul([a], invKca[0]), [a], transpose_b=True)[0][0], 0)\r\n\r\n    # The recursion\r\n    for n in range(1, 10):\r\n        Kca = tf.concat([Kca, tf.expand_dims(tf.tensordot(CC, K[n], [[1, 3], [0, 1]]) + tf.diag(a), 0)], 0)\r\n        invKca = tf.concat([invKca, tf.expand_dims(tf.linalg.inv(Kca[n]), 0)], 0)\r\n        K = tf.concat([K, tf.expand_dims(tf.tensordot(CC, invKca[n], [[0, 2], [0, 1]]), 0)], 0)\r\n\r\n        costN = tf.concat([costN, tf.expand_dims(tf.linalg.logdet(Kca[n]) + tf.matmul(tf.matmul([a], invKca[n]), [a], transpose_b=True)[0][0], 0)], 0)\r\n\r\n    cost = tf.reduce_sum(costN)\r\n    # If this example blows up, decrease the learning rate.\r\n    step = tf.train.AdamOptimizer(0.001).minimize(cost)\r\n\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n    # Positive definite matrix K0\r\n    L0 = np.array([[1.0, 0.0, 0.0], \\\r\n        [3.5, 2.5, 0.0], \\\r\n        [-2.0, -1.2, 3.0]])\r\n    K0 = np.matmul(L0, L0.transpose())\r\n\r\n    sess.run(K_.initializer, feed_dict={K_init:K0})\r\n    for n in range(100):\r\n        sess.run(step)\r\n        print(\"Cost: \" + str(sess.run(cost)))\r\n\r\n    # Lo and Behold, K[10] is not symmetric\r\n    print(sess.run(K[10]) == sess.run(K[10]).transpose())\r\n    print(sess.run(K[10]))\r\n    print(\"Done\")"}