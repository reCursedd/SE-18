{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/394882371", "html_url": "https://github.com/tensorflow/tensorflow/issues/19750#issuecomment-394882371", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19750", "id": 394882371, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDg4MjM3MQ==", "user": {"login": "Zenome84", "id": 39647569, "node_id": "MDQ6VXNlcjM5NjQ3NTY5", "avatar_url": "https://avatars0.githubusercontent.com/u/39647569?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zenome84", "html_url": "https://github.com/Zenome84", "followers_url": "https://api.github.com/users/Zenome84/followers", "following_url": "https://api.github.com/users/Zenome84/following{/other_user}", "gists_url": "https://api.github.com/users/Zenome84/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zenome84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zenome84/subscriptions", "organizations_url": "https://api.github.com/users/Zenome84/orgs", "repos_url": "https://api.github.com/users/Zenome84/repos", "events_url": "https://api.github.com/users/Zenome84/events{/privacy}", "received_events_url": "https://api.github.com/users/Zenome84/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-05T22:36:04Z", "updated_at": "2018-06-05T22:36:04Z", "author_association": "NONE", "body_html": "<p>OK, so I created a dummy script to reproduce the issue. And I apologize, I don't think the issue ever arises with <code>tf.matmul(A, A, transpose_b=True)</code> applied as such. Rather, it happens when <code>tf.matmul</code> is applied iteratively on a positive-definite matrix - i.e. <code>tf.matmul(tf.matmul(c, A), c, transpose_b=True)</code>.</p>\n<p>Here is an example:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndim = 3\ndimN = 5\n\n# We will load a positive definite matrix for K\nK_init = tf.placeholder(tf.float32, [dim, dim])\nK_ = tf.Variable(K_init, trainable=False, collections=[])\n\nC = tf.Variable(tf.ones([dimN, dim]), dtype=tf.float32)\na = tf.Variable(tf.ones([dimN]), dtype=tf.float32)\n\n# We build a Recurrent Network for K\nK = [K_]\n# Kca should be symmetric: C*K*C' is symmetric, and A is diagonal; so their sum is symmetric \nKca = tf.expand_dims(tf.matmul(tf.matmul(C, K[0]), C, transpose_b=True) + tf.diag(a), 0)\n# invKca should be symmetric as the inverse of a Kca\ninvKca = tf.expand_dims(tf.linalg.inv(Kca[0]), 0)\n# K[1] should be symmetric since, again C'*invKca*C is symmetric.\nK = tf.concat([K, tf.expand_dims(tf.matmul(tf.matmul(C, invKca[0], transpose_a=True), C), 0)], 0)\n\n# Some step cost function\ncostN = tf.expand_dims(tf.linalg.logdet(Kca[0]) + tf.matmul(tf.matmul([a], invKca[0]), [a], transpose_b=True)[0][0], 0)\n\n# The recursion\nfor n in range(1, 10):\n    Kca = tf.concat([Kca, tf.expand_dims(tf.matmul(tf.matmul(C, K[n]), C, transpose_b=True) + tf.diag(a), 0)], 0)\n    invKca = tf.concat([invKca, tf.expand_dims(tf.linalg.inv(Kca[n]), 0)], 0)\n    K = tf.concat([K, tf.expand_dims(tf.matmul(tf.matmul(C, invKca[n], transpose_a=True), C), 0)], 0)\n\n    costN = tf.concat([costN, tf.expand_dims(tf.linalg.logdet(Kca[n]) + tf.matmul(tf.matmul([a], invKca[n]), [a], transpose_b=True)[0][0], 0)], 0)\n\ncost = tf.reduce_sum(costN)\n# If this example blows up, decrease the learning rate.\nstep = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Positive definite matrix K0\nL0 = np.array([[1.0, 0.0, 0.0], \\\n    [3.5, 2.5, 0.0], \\\n    [-2.0, -1.2, 3.0]])\nK0 = np.matmul(L0, L0.transpose())\n\nsess.run(K_.initializer, feed_dict={K_init:K0})\nfor n in range(100):\n    sess.run(step)\n    print(\"Cost: \" + str(sess.run(cost)))\n\n# Lo and Behold, K[10] is not symmetric\nprint(sess.run(K[10]) == sess.run(K[10]).transpose())\nprint(sess.run(K[10]))\nprint(\"Done\")\n</code></pre>\n<p>Now I can apply a manual fix using an intermediate step:<br>\n<code>B = tf.matmul(tf.matmul(c, A), c, transpose_b=True)</code><br>\n<code>C = 0.5(B + tf.transpose(B))</code></p>\n<p>But this type of process is so common - i.e. estimates of the second moments - that I would expect a more direct implementation.  Suffice it to say that for long recursions and many dimensions, having to add this intermediate step before every <code>logdet</code> and <code>inv</code> adds a significantly noticeable difference in step time.</p>", "body_text": "OK, so I created a dummy script to reproduce the issue. And I apologize, I don't think the issue ever arises with tf.matmul(A, A, transpose_b=True) applied as such. Rather, it happens when tf.matmul is applied iteratively on a positive-definite matrix - i.e. tf.matmul(tf.matmul(c, A), c, transpose_b=True).\nHere is an example:\nimport tensorflow as tf\nimport numpy as np\n\ndim = 3\ndimN = 5\n\n# We will load a positive definite matrix for K\nK_init = tf.placeholder(tf.float32, [dim, dim])\nK_ = tf.Variable(K_init, trainable=False, collections=[])\n\nC = tf.Variable(tf.ones([dimN, dim]), dtype=tf.float32)\na = tf.Variable(tf.ones([dimN]), dtype=tf.float32)\n\n# We build a Recurrent Network for K\nK = [K_]\n# Kca should be symmetric: C*K*C' is symmetric, and A is diagonal; so their sum is symmetric \nKca = tf.expand_dims(tf.matmul(tf.matmul(C, K[0]), C, transpose_b=True) + tf.diag(a), 0)\n# invKca should be symmetric as the inverse of a Kca\ninvKca = tf.expand_dims(tf.linalg.inv(Kca[0]), 0)\n# K[1] should be symmetric since, again C'*invKca*C is symmetric.\nK = tf.concat([K, tf.expand_dims(tf.matmul(tf.matmul(C, invKca[0], transpose_a=True), C), 0)], 0)\n\n# Some step cost function\ncostN = tf.expand_dims(tf.linalg.logdet(Kca[0]) + tf.matmul(tf.matmul([a], invKca[0]), [a], transpose_b=True)[0][0], 0)\n\n# The recursion\nfor n in range(1, 10):\n    Kca = tf.concat([Kca, tf.expand_dims(tf.matmul(tf.matmul(C, K[n]), C, transpose_b=True) + tf.diag(a), 0)], 0)\n    invKca = tf.concat([invKca, tf.expand_dims(tf.linalg.inv(Kca[n]), 0)], 0)\n    K = tf.concat([K, tf.expand_dims(tf.matmul(tf.matmul(C, invKca[n], transpose_a=True), C), 0)], 0)\n\n    costN = tf.concat([costN, tf.expand_dims(tf.linalg.logdet(Kca[n]) + tf.matmul(tf.matmul([a], invKca[n]), [a], transpose_b=True)[0][0], 0)], 0)\n\ncost = tf.reduce_sum(costN)\n# If this example blows up, decrease the learning rate.\nstep = tf.train.AdamOptimizer(0.001).minimize(cost)\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n# Positive definite matrix K0\nL0 = np.array([[1.0, 0.0, 0.0], \\\n    [3.5, 2.5, 0.0], \\\n    [-2.0, -1.2, 3.0]])\nK0 = np.matmul(L0, L0.transpose())\n\nsess.run(K_.initializer, feed_dict={K_init:K0})\nfor n in range(100):\n    sess.run(step)\n    print(\"Cost: \" + str(sess.run(cost)))\n\n# Lo and Behold, K[10] is not symmetric\nprint(sess.run(K[10]) == sess.run(K[10]).transpose())\nprint(sess.run(K[10]))\nprint(\"Done\")\n\nNow I can apply a manual fix using an intermediate step:\nB = tf.matmul(tf.matmul(c, A), c, transpose_b=True)\nC = 0.5(B + tf.transpose(B))\nBut this type of process is so common - i.e. estimates of the second moments - that I would expect a more direct implementation.  Suffice it to say that for long recursions and many dimensions, having to add this intermediate step before every logdet and inv adds a significantly noticeable difference in step time.", "body": "OK, so I created a dummy script to reproduce the issue. And I apologize, I don't think the issue ever arises with `tf.matmul(A, A, transpose_b=True)` applied as such. Rather, it happens when `tf.matmul` is applied iteratively on a positive-definite matrix - i.e. `tf.matmul(tf.matmul(c, A), c, transpose_b=True)`.\r\n\r\nHere is an example:\r\n\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    dim = 3\r\n    dimN = 5\r\n\r\n    # We will load a positive definite matrix for K\r\n    K_init = tf.placeholder(tf.float32, [dim, dim])\r\n    K_ = tf.Variable(K_init, trainable=False, collections=[])\r\n\r\n    C = tf.Variable(tf.ones([dimN, dim]), dtype=tf.float32)\r\n    a = tf.Variable(tf.ones([dimN]), dtype=tf.float32)\r\n\r\n    # We build a Recurrent Network for K\r\n    K = [K_]\r\n    # Kca should be symmetric: C*K*C' is symmetric, and A is diagonal; so their sum is symmetric \r\n    Kca = tf.expand_dims(tf.matmul(tf.matmul(C, K[0]), C, transpose_b=True) + tf.diag(a), 0)\r\n    # invKca should be symmetric as the inverse of a Kca\r\n    invKca = tf.expand_dims(tf.linalg.inv(Kca[0]), 0)\r\n    # K[1] should be symmetric since, again C'*invKca*C is symmetric.\r\n    K = tf.concat([K, tf.expand_dims(tf.matmul(tf.matmul(C, invKca[0], transpose_a=True), C), 0)], 0)\r\n\r\n    # Some step cost function\r\n    costN = tf.expand_dims(tf.linalg.logdet(Kca[0]) + tf.matmul(tf.matmul([a], invKca[0]), [a], transpose_b=True)[0][0], 0)\r\n\r\n    # The recursion\r\n    for n in range(1, 10):\r\n        Kca = tf.concat([Kca, tf.expand_dims(tf.matmul(tf.matmul(C, K[n]), C, transpose_b=True) + tf.diag(a), 0)], 0)\r\n        invKca = tf.concat([invKca, tf.expand_dims(tf.linalg.inv(Kca[n]), 0)], 0)\r\n        K = tf.concat([K, tf.expand_dims(tf.matmul(tf.matmul(C, invKca[n], transpose_a=True), C), 0)], 0)\r\n\r\n        costN = tf.concat([costN, tf.expand_dims(tf.linalg.logdet(Kca[n]) + tf.matmul(tf.matmul([a], invKca[n]), [a], transpose_b=True)[0][0], 0)], 0)\r\n\r\n    cost = tf.reduce_sum(costN)\r\n    # If this example blows up, decrease the learning rate.\r\n    step = tf.train.AdamOptimizer(0.001).minimize(cost)\r\n\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n\r\n    # Positive definite matrix K0\r\n    L0 = np.array([[1.0, 0.0, 0.0], \\\r\n        [3.5, 2.5, 0.0], \\\r\n        [-2.0, -1.2, 3.0]])\r\n    K0 = np.matmul(L0, L0.transpose())\r\n\r\n    sess.run(K_.initializer, feed_dict={K_init:K0})\r\n    for n in range(100):\r\n        sess.run(step)\r\n        print(\"Cost: \" + str(sess.run(cost)))\r\n\r\n    # Lo and Behold, K[10] is not symmetric\r\n    print(sess.run(K[10]) == sess.run(K[10]).transpose())\r\n    print(sess.run(K[10]))\r\n    print(\"Done\")\r\n\r\nNow I can apply a manual fix using an intermediate step:\r\n`B = tf.matmul(tf.matmul(c, A), c, transpose_b=True)`\r\n`C = 0.5(B + tf.transpose(B))`\r\n\r\nBut this type of process is so common - i.e. estimates of the second moments - that I would expect a more direct implementation.  Suffice it to say that for long recursions and many dimensions, having to add this intermediate step before every `logdet` and `inv` adds a significantly noticeable difference in step time."}