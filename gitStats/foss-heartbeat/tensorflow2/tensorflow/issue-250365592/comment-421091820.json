{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/421091820", "html_url": "https://github.com/tensorflow/tensorflow/pull/12299#issuecomment-421091820", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12299", "id": 421091820, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTA5MTgyMA==", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-13T17:42:35Z", "updated_at": "2018-09-13T17:42:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8589292\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/camaclean\">@camaclean</a> That's really cool! That said, getting a model to train well on that giant set of nodes seems like a pretty big challenge... With regards to the intermediate reduction: since we were operating within a single infiniband switch, a ring allreduce meant that we were just bandwidth limited between adjacent nodes in the allreduce. We would do 8-10 GPUs on a single machine and then have up to 10-30 nodes at most. So the only thing we tried besides the ring was a few special cases for when reducing small tensors (where latency dominates bandwidth).</p>\n<p>(Getting an efficient synchronous gradient update on a setup that big seems very tricky. Additionally, even if you do, your batch size will be <em>enormous</em>... if you're training standard deep learning models you'll have to throw a bunch of tricks at it to get it to optimize well with that many elements in the batch. Maybe a (partially?) asynchronous training strategy is more feasible?)</p>", "body_text": "@camaclean That's really cool! That said, getting a model to train well on that giant set of nodes seems like a pretty big challenge... With regards to the intermediate reduction: since we were operating within a single infiniband switch, a ring allreduce meant that we were just bandwidth limited between adjacent nodes in the allreduce. We would do 8-10 GPUs on a single machine and then have up to 10-30 nodes at most. So the only thing we tried besides the ring was a few special cases for when reducing small tensors (where latency dominates bandwidth).\n(Getting an efficient synchronous gradient update on a setup that big seems very tricky. Additionally, even if you do, your batch size will be enormous... if you're training standard deep learning models you'll have to throw a bunch of tricks at it to get it to optimize well with that many elements in the batch. Maybe a (partially?) asynchronous training strategy is more feasible?)", "body": "@camaclean That's really cool! That said, getting a model to train well on that giant set of nodes seems like a pretty big challenge... With regards to the intermediate reduction: since we were operating within a single infiniband switch, a ring allreduce meant that we were just bandwidth limited between adjacent nodes in the allreduce. We would do 8-10 GPUs on a single machine and then have up to 10-30 nodes at most. So the only thing we tried besides the ring was a few special cases for when reducing small tensors (where latency dominates bandwidth). \r\n\r\n(Getting an efficient synchronous gradient update on a setup that big seems very tricky. Additionally, even if you do, your batch size will be *enormous*... if you're training standard deep learning models you'll have to throw a bunch of tricks at it to get it to optimize well with that many elements in the batch. Maybe a (partially?) asynchronous training strategy is more feasible?)"}