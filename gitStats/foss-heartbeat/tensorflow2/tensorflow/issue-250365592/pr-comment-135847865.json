{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/135847865", "pull_request_review_id": 59307025, "id": 135847865, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzNTg0Nzg2NQ==", "diff_hunk": "@@ -0,0 +1,273 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+# pylint: disable=g-short-docstring-punctuation\n+\"\"\"## Communicating Between Processes with MPI\n+\n+TensorFlow natively provides inter-device communication through send and\n+receive ops and inter-node communication through Distributed TensorFlow, based\n+on the same send and receive abstractions. On HPC clusters where Infiniband or\n+other high-speed node interconnects are available, these can end up being\n+insufficient for synchronous data-parallel training (without asynchronous\n+gradient descent). This module implements a variety of MPI ops which can take\n+advantage of hardware-specific MPI libraries for efficient communication.\n+\n+In order to use this module, TensorFlow must be built with an MPI library,\n+which can be provided to the `./configure` script at build time. As a user of\n+TensorFlow, you will need to build TensorFlow yourself to select the MPI\n+library to use; to do so, follow the [instructions for building TensorFlow from\n+source](https://www.tensorflow.org/get_started/os_setup#installing_from_sources).\n+\n+### Utility Ops\n+\n+In addition to reductions and gathers, this module provides utility operations\n+for detecting the running MPI configuration.\n+\n+Example:\n+\n+```python\n+from tensorflow.contrib import mpi\n+\n+# Use `mpi.Session` instead of `tf.Session`\n+with mpi.Session() as session:\n+    rank = session.run(mpi.rank())\n+    print(\"My MPI Rank:\", rank)\n+\n+    if rank == 0:\n+        print(\"MPI Size:\", session.run(mpi.size()))\n+```\n+\n+@@rank\n+@@size\n+\n+### Ring Allreduce and Allgather\n+\n+When summing or averaging tensors across many processes, communication can\n+easily become a bottleneck. A naive implementation will send all the tensor\n+values to the same process, perform the reduction, and then broadcast the\n+values back to all other processes, effectively creating a synchronous\n+parameter server in one process. However, the process responsible for\n+performing the reduction will have to receive and send a massive amount of data\n+which scales with the number of processes *and* the number of parameters in the\n+model.\n+\n+Instead of centralizing the reduction and having one primary reducer, we can\n+implement a distributed allreduce or allgather. A bandwidth-optimal allreduce\n+will end up sending 2(N - 1) values for every value in the input tensor,\n+and can be implemented with a ring allreduce [1]. (Intuitively, a linear reduce\n+requires at least (N - 1) sends between the different nodes, and a broadcast of\n+the result also requires (N - 1) sends, for a total of 2 (N - 1); these two\n+steps cannot be combined in a clever way to reduce the number of required\n+sends.) This module implements bandwidth-optimal ring allreduce and ring\n+allgather operations using MPI; by choosing a hardware-appropriate MPI\n+implementation (such as OpenMPI with CUDA-IPC support), you can train large\n+models with synchronous gradient descent with minimal communication overhead.\n+\n+In addition to the `allreduce` and `allgather` functions, a convenience\n+`DistributedOptimizer` wrapper is provided to simplify using these functions\n+for reducing model gradients.\n+\n+Example:\n+\n+```python\n+import tensorflow as tf\n+from tensorflow.contrib import mpi_collectives as mpi\n+\n+# Construct a simple linear regression model to optimize\n+W = tf.get_variable(\"W\", shape=[20, 1], dtype=tf.float32)\n+B = tf.get_variable(\"B\", shape=[1, 1], dtype=tf.float32)\n+inputs = tf.placeholder(\"Inputs\", shape=[None, 20])\n+outputs = tf.placeholder(\"Outputs\", shape=[None, 1])\n+loss = tf.nn.l2_loss(tf.matmul(inputs, W) + B - outputs)\n+\n+# Training using MPI allreduce with DistributedOptimizer\n+optimizer = mpi.DistributedOptimizer(tf.train.AdamOptimizer())\n+train = optimizer.minimize(loss)\n+\n+# Average loss over all ranks, for printing.\n+# Do not pass this to an optimizer!\n+avg_loss = mpi.allreduce(loss)\n+\n+# On different ranks, feed different input data.\n+with mpi.Session() as session:\n+    rank = session.run(mpi.rank())\n+    batch_inputs, batch_outputs = construct_batch_for_rank(rank)\n+    feed_dict = {inputs: batch_inputs, outputs: batch_outputs}\n+    _, l = session.run([train, avg_loss], feed_dict=feed_dict)\n+    print(\"Average Loss:\", l)\n+```\n+\n+[1] Patarasuk, Pitch and Yuan, Xin. \"Bandwidth Optimal All-reduce Algorithms\n+for Clusters of Workstations\".\n+\n+@@Session\n+@@DistributedOptimizer\n+@@allreduce\n+@@allgather\n+\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow as tf\n+\n+from tensorflow.contrib.mpi_collectives.mpi_ops import size\n+from tensorflow.contrib.mpi_collectives.mpi_ops import rank\n+from tensorflow.contrib.mpi_collectives.mpi_ops import local_rank\n+from tensorflow.contrib.mpi_collectives.mpi_ops import allgather\n+from tensorflow.contrib.mpi_collectives.mpi_ops import _allreduce\n+from tensorflow.contrib.mpi_collectives.mpi_ops import init\n+\n+\n+def allreduce(tensor, average=True):\n+  \"\"\"Perform an MPI allreduce on a tf.Tensor or tf.IndexedSlices.\n+\n+  Arguments:\n+  tensor: tf.Tensor, tf.Variable, or tf.IndexedSlices to reduce.\n+          The shape of the input must be identical across all ranks.\n+  average: If True, computes the average over all ranks.\n+           Otherwise, computes the sum over all ranks.\n+\n+  This function performs a bandwidth-optimal ring allreduce on the input\n+  tensor. If the input is an tf.IndexedSlices, the function instead does an\n+  allgather on the values and the indices, effectively doing an allreduce on\n+  the represented tensor.\n+  \"\"\"\n+  if isinstance(tensor, tf.IndexedSlices):\n+    # For IndexedSlices, do two allgathers intead of an allreduce.\n+    mpi_size = tf.cast(size(), tensor.values.dtype)\n+    values = allgather(tensor.values)", "path": "tensorflow/contrib/mpi_collectives/__init__.py", "position": 151, "original_position": 151, "commit_id": "5518005998c073c27bb6389b4108fed063b344c9", "original_commit_id": "477d98da1d320fc7466cbba6c4d2400befabdeb2", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "body": "If the IndexedSlices are index-disjoint, this looks correct, but what happens in case of a common index?", "created_at": "2017-08-29T16:46:56Z", "updated_at": "2017-09-17T19:12:07Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/12299#discussion_r135847865", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12299", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/135847865"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/12299#discussion_r135847865"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12299"}}, "body_html": "<p>If the IndexedSlices are index-disjoint, this looks correct, but what happens in case of a common index?</p>", "body_text": "If the IndexedSlices are index-disjoint, this looks correct, but what happens in case of a common index?"}