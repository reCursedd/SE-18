{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/421068624", "html_url": "https://github.com/tensorflow/tensorflow/pull/12299#issuecomment-421068624", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12299", "id": 421068624, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTA2ODYyNA==", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-13T16:28:06Z", "updated_at": "2018-09-13T16:28:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8589292\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/camaclean\">@camaclean</a></p>\n<p>Short version: The allreduce present in MPI -- that is, non-vendor MPI implementations -- is not CUDA-aware, and thus copies memory to CPU before doing the collective. The ring allreduce is much more efficient when doing multi-node multi-GPU training than the default collective.</p>\n<p>When writing this package, we were optimizing for commodity hardware (possibly with Infiniband for multi-node connections) on CPUs and GPUs, which describes the majority of large DL applications. At the time of writing, most MPI implementations (that is, all that I checked) did not support direct GPU-to-GPU transfers in their collectives; however, at least Open MPI supported the simpler operations using direct GPU-to-GPU transfers and CUDA IPC. If you used <code>MPI_Iallreduce</code>, it would first transfer GPU memory to CPU, then perform the allreduce, then transfer it back. For a model with 1 GB of weights with several GPUs connected to the CPU via a shared PCIe connection, this would introduce a significant performance penalty. Implementing our own ring allreduce allowed us to take advantage of GPU-to-GPU transfers without crossing into CPU memory, so that the slowest point was (on multi-node setups) the network or Infiniband connection. The difference in performance between using <code>MPI_Iallreduce</code> and our ring allreduce was easily an order of magnitude.</p>\n<p>However, as you mention, this code is certainly not optimal for all MPI implementations and network topologies. Additionally, nowadays NCCL supports (or claims to support) multi-node communication, although I haven't had the time to check how well it works or how it performs its collectives (and whether they correctly handle GPU to GPU transfers over infiniband).</p>\n<p>If you have access to a thousand node cluster and want to switch out the allreduce for the default vendor one, would be really cool to see what sort of things you could do :)</p>", "body_text": "@camaclean\nShort version: The allreduce present in MPI -- that is, non-vendor MPI implementations -- is not CUDA-aware, and thus copies memory to CPU before doing the collective. The ring allreduce is much more efficient when doing multi-node multi-GPU training than the default collective.\nWhen writing this package, we were optimizing for commodity hardware (possibly with Infiniband for multi-node connections) on CPUs and GPUs, which describes the majority of large DL applications. At the time of writing, most MPI implementations (that is, all that I checked) did not support direct GPU-to-GPU transfers in their collectives; however, at least Open MPI supported the simpler operations using direct GPU-to-GPU transfers and CUDA IPC. If you used MPI_Iallreduce, it would first transfer GPU memory to CPU, then perform the allreduce, then transfer it back. For a model with 1 GB of weights with several GPUs connected to the CPU via a shared PCIe connection, this would introduce a significant performance penalty. Implementing our own ring allreduce allowed us to take advantage of GPU-to-GPU transfers without crossing into CPU memory, so that the slowest point was (on multi-node setups) the network or Infiniband connection. The difference in performance between using MPI_Iallreduce and our ring allreduce was easily an order of magnitude.\nHowever, as you mention, this code is certainly not optimal for all MPI implementations and network topologies. Additionally, nowadays NCCL supports (or claims to support) multi-node communication, although I haven't had the time to check how well it works or how it performs its collectives (and whether they correctly handle GPU to GPU transfers over infiniband).\nIf you have access to a thousand node cluster and want to switch out the allreduce for the default vendor one, would be really cool to see what sort of things you could do :)", "body": "@camaclean \r\n\r\nShort version: The allreduce present in MPI -- that is, non-vendor MPI implementations -- is not CUDA-aware, and thus copies memory to CPU before doing the collective. The ring allreduce is much more efficient when doing multi-node multi-GPU training than the default collective.\r\n\r\nWhen writing this package, we were optimizing for commodity hardware (possibly with Infiniband for multi-node connections) on CPUs and GPUs, which describes the majority of large DL applications. At the time of writing, most MPI implementations (that is, all that I checked) did not support direct GPU-to-GPU transfers in their collectives; however, at least Open MPI supported the simpler operations using direct GPU-to-GPU transfers and CUDA IPC. If you used `MPI_Iallreduce`, it would first transfer GPU memory to CPU, then perform the allreduce, then transfer it back. For a model with 1 GB of weights with several GPUs connected to the CPU via a shared PCIe connection, this would introduce a significant performance penalty. Implementing our own ring allreduce allowed us to take advantage of GPU-to-GPU transfers without crossing into CPU memory, so that the slowest point was (on multi-node setups) the network or Infiniband connection. The difference in performance between using `MPI_Iallreduce` and our ring allreduce was easily an order of magnitude.\r\n\r\nHowever, as you mention, this code is certainly not optimal for all MPI implementations and network topologies. Additionally, nowadays NCCL supports (or claims to support) multi-node communication, although I haven't had the time to check how well it works or how it performs its collectives (and whether they correctly handle GPU to GPU transfers over infiniband).\r\n\r\nIf you have access to a thousand node cluster and want to switch out the allreduce for the default vendor one, would be really cool to see what sort of things you could do :)"}