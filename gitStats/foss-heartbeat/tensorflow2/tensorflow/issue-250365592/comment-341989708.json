{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341989708", "html_url": "https://github.com/tensorflow/tensorflow/pull/12299#issuecomment-341989708", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12299", "id": 341989708, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTk4OTcwOA==", "user": {"login": "jthestness", "id": 28744304, "node_id": "MDQ6VXNlcjI4NzQ0MzA0", "avatar_url": "https://avatars0.githubusercontent.com/u/28744304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jthestness", "html_url": "https://github.com/jthestness", "followers_url": "https://api.github.com/users/jthestness/followers", "following_url": "https://api.github.com/users/jthestness/following{/other_user}", "gists_url": "https://api.github.com/users/jthestness/gists{/gist_id}", "starred_url": "https://api.github.com/users/jthestness/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jthestness/subscriptions", "organizations_url": "https://api.github.com/users/jthestness/orgs", "repos_url": "https://api.github.com/users/jthestness/repos", "events_url": "https://api.github.com/users/jthestness/events{/privacy}", "received_events_url": "https://api.github.com/users/jthestness/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-05T17:24:43Z", "updated_at": "2017-11-05T17:24:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11289001\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alexwal\">@alexwal</a> : We haven't tested Horovod, but the Uber team claims it performs better than this MPI allreduce. We have recently tested NCCL 2.0 (i.e. what Horovod uses) for intra-node communication outside of Tensorflow, and it performs a bit better than the MPI ring allreduce. We have some analysis about the differences that we will be releasing in the near future.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12964049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Zhaojp-Frank\">@Zhaojp-Frank</a> : (1) Tensorflow supports intra-node communication, and you can use that for data parallelism if you'd like. It cannot currently scale to inter-node communication without a collectives package such as this MPI collectives. Tensorflow's intra-node communication requires that you use a single process to manage all GPUs, while inter-node communication requires at least one process on each separate node. Because of these differences, the programming model can be very different. If you plan to scale to multiple nodes, we recommend that you start with collectives that will scale to multiple nodes rather than trying to retrofit that code later. (2) This MPI collectives package doesn't use NCCL.</p>", "body_text": "@alexwal : We haven't tested Horovod, but the Uber team claims it performs better than this MPI allreduce. We have recently tested NCCL 2.0 (i.e. what Horovod uses) for intra-node communication outside of Tensorflow, and it performs a bit better than the MPI ring allreduce. We have some analysis about the differences that we will be releasing in the near future.\n@Zhaojp-Frank : (1) Tensorflow supports intra-node communication, and you can use that for data parallelism if you'd like. It cannot currently scale to inter-node communication without a collectives package such as this MPI collectives. Tensorflow's intra-node communication requires that you use a single process to manage all GPUs, while inter-node communication requires at least one process on each separate node. Because of these differences, the programming model can be very different. If you plan to scale to multiple nodes, we recommend that you start with collectives that will scale to multiple nodes rather than trying to retrofit that code later. (2) This MPI collectives package doesn't use NCCL.", "body": "@alexwal : We haven't tested Horovod, but the Uber team claims it performs better than this MPI allreduce. We have recently tested NCCL 2.0 (i.e. what Horovod uses) for intra-node communication outside of Tensorflow, and it performs a bit better than the MPI ring allreduce. We have some analysis about the differences that we will be releasing in the near future.\r\n\r\n@Zhaojp-Frank : (1) Tensorflow supports intra-node communication, and you can use that for data parallelism if you'd like. It cannot currently scale to inter-node communication without a collectives package such as this MPI collectives. Tensorflow's intra-node communication requires that you use a single process to manage all GPUs, while inter-node communication requires at least one process on each separate node. Because of these differences, the programming model can be very different. If you plan to scale to multiple nodes, we recommend that you start with collectives that will scale to multiple nodes rather than trying to retrofit that code later. (2) This MPI collectives package doesn't use NCCL."}