{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3174", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3174/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3174/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3174/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3174", "id": 163577697, "node_id": "MDU6SXNzdWUxNjM1Nzc2OTc=", "number": 3174, "title": "C Stack smashing from inside python when using tensorflow", "user": {"login": "vasusharma", "id": 4756808, "node_id": "MDQ6VXNlcjQ3NTY4MDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/4756808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vasusharma", "html_url": "https://github.com/vasusharma", "followers_url": "https://api.github.com/users/vasusharma/followers", "following_url": "https://api.github.com/users/vasusharma/following{/other_user}", "gists_url": "https://api.github.com/users/vasusharma/gists{/gist_id}", "starred_url": "https://api.github.com/users/vasusharma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vasusharma/subscriptions", "organizations_url": "https://api.github.com/users/vasusharma/orgs", "repos_url": "https://api.github.com/users/vasusharma/repos", "events_url": "https://api.github.com/users/vasusharma/events{/privacy}", "received_events_url": "https://api.github.com/users/vasusharma/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-07-03T20:06:25Z", "updated_at": "2017-02-09T22:02:12Z", "closed_at": "2016-08-17T16:48:44Z", "author_association": "NONE", "body_html": "<p>I am trying to implement Fully Connected Convolutional Network in python using tensorflow and I am getting a Stack Smashing error in one of the Convolution operators. I have checked and all the filter sizes match up and are perfectly in sync with the layer shapes. I did my research on the error and found that this error is caused when you overflow the alloted buffer but I cant see any place where I exceed this buffer and the most weird part is that it's running for 10 layers (7 convolutions and 3 max pool ones of the VGG network) and crashing on the next convolution.<br>\n(Posted this on Stack Overflow and was told that this error is due to a bug in tensorflow and hence should be posted here. The reason I was told was that the stack smashing is happening in the C++ code and not the python code and I havent written any C++ code myself so the most likely place for the error to have happened is inside tensorflow)</p>\n<p>The error I get is:<br>\n<code>-&gt; _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict) (Pdb) c I tensorflow/core/kernels/logging_ops.cc:79] Shape of input image: [1 375 1242 3] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv1_1[1 375 1242 64] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv1_2[1 375 1242 64] I tensorflow/core/kernels/logging_ops.cc:79] Shape of pool1[1 188 621 64] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv2_1[1 188 621 128] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv2_2[1 188 621 128] I tensorflow/core/kernels/logging_ops.cc:79] Shape of pool2[1 94 311 128] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_1[1 94 311 256] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_2[1 94 311 256] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_3[1 94 311 256] I tensorflow/core/kernels/logging_ops.cc:79] Shape of pool3[1 47 156 256] *** stack smashing detected ***: python terminated Aborted (core dumped)</code></p>\n<p>Here is the relevant part of my code:</p>\n<pre><code>`class FCN8VGG:\ndef __init__(self, vgg16_npy_path=None):\n    if vgg16_npy_path is None:\n        path = sys.modules[self.__class__.__module__].__file__\n        # print path\n        path = os.path.abspath(os.path.join(path, os.pardir))\n        # print path\n        path = os.path.join(path, \"vgg16.npy\")\n        print(path)\n        vgg16_npy_path = path\n\n    self.data_dict = np.load(vgg16_npy_path).item()\n    self.wd = 5e-4\n    print(\"npy file loaded\")\ndef build(self, rgb, train=True, num_classes=14, random_init_fc8=True,\n          debug=True):\n    \"\"\"\n    Build the VGG model using loaded weights\n    Parameters\n    ----------\n    rgb: image batch tensor\n        Image in rgb shap. Scaled to Intervall [0, 255]\n    train: bool\n        Whether to build train or inference graph\n    num_classes: int\n        How many classes should be predicted (by fc8)\n    random_init_fc8 : bool\n        Whether to initialize fc8 layer randomly.\n        Finetuning is required in this case.\n    debug: bool\n        Whether to print additional Debug Information.\n    \"\"\"\n    # Convert RGB to BGR\n\n    with tf.name_scope('Processing'):\n        #import pdb;pdb.set_trace()\n        red, green, blue = tf.split(3, 3, rgb)\n        # assert red.get_shape().as_list()[1:] == [224, 224, 1]\n        # assert green.get_shape().as_list()[1:] == [224, 224, 1]\n        # assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n        bgr = tf.concat(3, [\n            blue - VGG_MEAN[0],\n            green - VGG_MEAN[1],\n            red - VGG_MEAN[2],\n        ])\n\n        if debug:\n            bgr = tf.Print(bgr, [tf.shape(bgr)],\n                           message='Shape of input image: ',\n                           summarize=4, first_n=1)\n\n    self.conv1_1 = self._conv_layer(bgr, \"conv1_1\")\n    self.conv1_2 = self._conv_layer(self.conv1_1, \"conv1_2\")\n    self.pool1 = self._max_pool(self.conv1_2, 'pool1', debug)\n\n    print(\"Pool1 layer ready\")\n\n    self.conv2_1 = self._conv_layer(self.pool1, \"conv2_1\")\n    self.conv2_2 = self._conv_layer(self.conv2_1, \"conv2_2\")\n    self.pool2 = self._max_pool(self.conv2_2, 'pool2', debug)\n    self.conv3_1 = self._conv_layer(self.pool2, \"conv3_1\")\n    self.conv3_2 = self._conv_layer(self.conv3_1, \"conv3_2\")\n    self.conv3_3 = self._conv_layer(self.conv3_2, \"conv3_3\")\n    self.pool3 = self._max_pool(self.conv3_3, 'pool3', debug)\n\n    print(\"Pool 3 layer ready\")\n    #pdb.set_trace()\n    self.conv4_1 = self._conv_layer(self.pool3, \"conv4_1\")\n    self.conv4_2 = self._conv_layer(self.conv4_1, \"conv4_2\")\n    self.conv4_3 = self._conv_layer(self.conv4_2, \"conv4_3\")\n    self.pool4 = self._max_pool(self.conv4_3, 'pool4', debug)\n\n    print(\"Pool 4 layer ready\")\n    #pdb.set_trace()\n    self.conv5_1 = self._conv_layer(self.pool4, \"conv5_1\")\n    self.conv5_2 = self._conv_layer(self.conv5_1, \"conv5_2\")\n    self.conv5_3 = self._conv_layer(self.conv5_2, \"conv5_3\")\n    self.pool5 = self._max_pool(self.conv5_3, 'pool5', debug)\n\n    print(\"Pool 5 layer ready\")\n    #pdb.set_trace()\n    self.fc6 = self._fc_layer(self.pool5, \"fc6\")\n\n    if train:\n        self.fc6 = tf.nn.dropout(self.fc6, 0.5)\n\n    self.fc7 = self._fc_layer(self.fc6, \"fc7\")\n    if train:\n        self.fc7 = tf.nn.dropout(self.fc7, 0.5)\n\n    if random_init_fc8:\n        self.score_fr = self._score_layer(self.fc7, \"score_fr\",\n                                          num_classes)\n    else:\n        self.score_fr = self._fc_layer(self.fc7, \"score_fr\",\n                                       num_classes=num_classes,\n                                       relu=False)\n\n    self.pred = tf.argmax(self.score_fr, dimension=3)\n\n    self.upscore2 = self._upscore_layer(self.score_fr,\n                                        shape=tf.shape(self.pool4),\n                                        num_classes=num_classes,\n                                        debug=debug, name='upscore2',\n                                        ksize=4, stride=2)\n    self.score_pool4 = self._score_layer(self.pool4, \"score_pool4\",\n                                         num_classes=num_classes)\n    self.fuse_pool4 = tf.add(self.upscore2, self.score_pool4)\n    self.upscore4 = self._upscore_layer(self.fuse_pool4,\n                                        shape=tf.shape(self.pool3),\n                                        num_classes=num_classes,\n                                        debug=debug, name='upscore4',\n                                        ksize=4, stride=2)\n    self.score_pool3 = self._score_layer(self.pool3, \"score_pool3\",\n                                         num_classes=num_classes)\n    self.fuse_pool3 = tf.add(self.upscore4, self.score_pool3)\n\n    self.upscore32 = self._upscore_layer(self.fuse_pool3,\n                                         shape=tf.shape(bgr),\n                                         num_classes=num_classes,\n                                         debug=debug, name='upscore32',\n                                         ksize=16, stride=8)\n\n    self.pred_up = tf.argmax(self.upscore32, dimension=3)\n\ndef _max_pool(self, bottom, name, debug):\n    pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                          padding='SAME', name=name)\n\n    if debug:\n        pool = tf.Print(pool, [tf.shape(pool)],\n                        message='Shape of %s' % name,\n                        summarize=4, first_n=1)\n    return pool\n\ndef _conv_layer(self, bottom, name):\n    with tf.variable_scope(name) as scope:\n        filt = self.get_conv_filter(name)\n        conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n\n        conv = tf.Print(conv, [tf.shape(conv)], message='Shape of %s' % name, summarize=4, first_n=1)\n\n        conv_biases = self.get_bias(name)\n        bias = tf.nn.bias_add(conv, conv_biases)\n        if relu:\n            bias = tf.nn.relu(bias)\n        _activation_summary(bias)\n\n        if debug:\n            bias = tf.Print(bias, [tf.shape(bias)],\n                            message='Shape of %s' % name,\n                            summarize=4, first_n=1)\n        return bias`\n\nThe driver tensorflow code has this command which produces the error:\n\n` _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict)`\n\nWhere loss is the tensorflow op for:\n\n`def loss(logits, labels):\n\"\"\"Calculate the loss from the logits and the labels.\n\nArgs:\n  logits: Logits tensor, float - [batch_size, height, width, NUM_CLASSES].  Must be unsclaed and unsoftmaxed.\n  labels: Labels tensor, int32 - [batch_size, height, width].\n\nReturns:\n  loss: Loss tensor of type float.\n\"\"\"\nwith tf.name_scope('loss'):\n    reshaped_logits = tf.reshape(logits, [-1, 14])  # shape [batch_size*height*width, 14]\n    reshaped_labels = tf.reshape(labels, [-1])  # shape [batch_size*height*width]   #Need fix size images for this\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(reshaped_logits, reshaped_labels)\nreturn loss   `\n\nAnd training is the driver training function:\n\n`def training(loss, learning_rate):\n  \"\"\"Sets up the training Ops.\n\n  Creates a summarizer to track the loss over time in TensorBoard.\n\n  Creates an optimizer and applies the gradients to all trainable variables.\n\n  The Op returned by this function is what must be passed to the\n  `sess.run()` call to cause the model to train.\n\n  Args:\n    loss: Loss tensor, from loss().\n    learning_rate: The learning rate to use for gradient descent.\n\n  Returns:\n    train_op: The Op for training.\n  \"\"\"\n  # Add a scalar summary for the snapshot loss.\n  tf.scalar_summary(loss.op.name, loss)\n  # Create the gradient descent optimizer with the given learning rate.\n  #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  optimizer = tf.train.AdamOptimizer(1e-6)\n  # Create a variable to track the global step.\n  print(\"Setting step size to 1e-6 for Adam Optimizer\")\n  global_step = tf.Variable(0, name='global_step', trainable=False)\n  # Use the optimizer to apply the gradients that minimize the loss\n  # (and also increment the global step counter) as a single training step.\n  print(\"Running optimizer\")\n  train_op = optimizer.minimize(loss, global_step=global_step)\n  return train_op `\n</code></pre>\n<p>The debug log from the building of the network is: This contains details of filter and layer shapes etc.</p>\n<pre><code>`Layer name: conv1_1\nLayer shape: (3, 3, 3, 64)\nINFO:tensorflow:Created variable conv1_1/filter:0 with shape (3, 3, 3, 64) and init &lt;function _initializer at 0x7fcc27c85ed8&gt;\n2016-06-30 14:15:53,939 INFO Created variable conv1_1/filter:0 with shape (3, 3, 3, 64) and init &lt;function _initializer at 0x7fcc27c85ed8&gt;\n\nINFO:tensorflow:Created variable conv1_1/biases:0 with shape (64,) and init &lt;function _initializer at 0x7fcc27c85ed8&gt;\n2016-06-30 14:15:53,944 INFO Created variable conv1_1/biases:0 with shape (64,) and init &lt;function _initializer at 0x7fcc27c85ed8&gt;\nLayer name: conv1_2\nLayer shape: (3, 3, 64, 64)\n\nINFO:tensorflow:Created variable conv1_2/filter:0 with shape (3, 3, 64, 64) and init &lt;function _initializer at 0x7fcc27c96488&gt;\n2016-06-30 14:15:53,952 INFO Created variable conv1_2/filter:0 with shape (3, 3, 64, 64) and init &lt;function _initializer at 0x7fcc27c96488&gt;\n\nINFO:tensorflow:Created variable conv1_2/biases:0 with shape (64,) and init &lt;function _initializer at 0x7fcc27c96488&gt;\n2016-06-30 14:15:53,956 INFO Created variable conv1_2/biases:0 with shape (64,) and init &lt;function _initializer at 0x7fcc27c96488&gt;\nPool1 layer ready\nLayer name: conv2_1\nLayer shape: (3, 3, 64, 128)\n\nINFO:tensorflow:Created variable conv2_1/filter:0 with shape (3, 3, 64, 128) and init &lt;function _initializer at 0x7fcc27c9dc08&gt;\n2016-06-30 14:15:53,967 INFO Created variable conv2_1/filter:0 with shape (3, 3, 64, 128) and init &lt;function _initializer at 0x7fcc27c9dc08&gt;\n\nINFO:tensorflow:Created variable conv2_1/biases:0 with shape (128,) and init &lt;function _initializer at 0x7fcc27c9dc08&gt;\n2016-06-30 14:15:53,972 INFO Created variable conv2_1/biases:0 with shape (128,) and init &lt;function _initializer at 0x7fcc27c9dc08&gt;\nLayer name: conv2_2\nLayer shape: (3, 3, 128, 128)\n\nINFO:tensorflow:Created variable conv2_2/filter:0 with shape (3, 3, 128, 128) and init &lt;function _initializer at 0x7fcbb3012488&gt;\n2016-06-30 14:15:53,980 INFO Created variable conv2_2/filter:0 with shape (3, 3, 128, 128) and init &lt;function _initializer at 0x7fcbb3012488&gt;\n\nINFO:tensorflow:Created variable conv2_2/biases:0 with shape (128,) and init &lt;function _initializer at 0x7fcbb2f8af50&gt;\n2016-06-30 14:15:53,985 INFO Created variable conv2_2/biases:0 with shape (128,) and init &lt;function _initializer at 0x7fcbb2f8af50&gt;\nPool 2 layer ready\nLayer name: conv3_1\nLayer shape: (3, 3, 128, 256)\n\nINFO:tensorflow:Created variable conv3_1/filter:0 with shape (3, 3, 128, 256) and init &lt;function _initializer at 0x7fcbb3012488&gt;\n2016-06-30 14:15:53,995 INFO Created variable conv3_1/filter:0 with shape (3, 3, 128, 256) and init &lt;function _initializer at 0x7fcbb3012488&gt;\n\nINFO:tensorflow:Created variable conv3_1/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb1a7cb90&gt;\n2016-06-30 14:15:54,000 INFO Created variable conv3_1/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb1a7cb90&gt;\nLayer name: conv3_2\nLayer shape: (3, 3, 256, 256)\n\nINFO:tensorflow:Created variable conv3_2/filter:0 with shape (3, 3, 256, 256) and init &lt;function _initializer at 0x7fcbb3012488&gt;\n2016-06-30 14:15:54,010 INFO Created variable conv3_2/filter:0 with shape (3, 3, 256, 256) and init &lt;function _initializer at 0x7fcbb3012488&gt;\n\nINFO:tensorflow:Created variable conv3_2/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb3012488&gt;\n2016-06-30 14:15:54,015 INFO Created variable conv3_2/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb3012488&gt;\nLayer name: conv3_3\nLayer shape: (3, 3, 256, 256)\n\nINFO:tensorflow:Created variable conv3_3/filter:0 with shape (3, 3, 256, 256) and init &lt;function _initializer at 0x7fcbb1a8aaa0&gt;\n2016-06-30 14:15:54,024 INFO Created variable conv3_3/filter:0 with shape (3, 3, 256, 256) and init &lt;function _initializer at 0x7fcbb1a8aaa0&gt;\n\nINFO:tensorflow:Created variable conv3_3/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb1a8aaa0&gt;\n2016-06-30 14:15:54,029 INFO Created variable conv3_3/biases:0 with shape (256,) and init &lt;function _initializer at 0x7fcbb1a8aaa0&gt;\nPool 3 layer ready\nLayer name: conv4_1\nLayer shape: (3, 3, 256, 512)\n\nINFO:tensorflow:Created variable conv4_1/filter:0 with shape (3, 3, 256, 512) and init &lt;function _initializer at 0x7fcbb1a4c7d0&gt;\n2016-06-30 14:15:54,043 INFO Created variable conv4_1/filter:0 with shape (3, 3, 256, 512) and init &lt;function _initializer at 0x7fcbb1a4c7d0&gt;\n\nINFO:tensorflow:Created variable conv4_1/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1a4c7d0&gt;\n2016-06-30 14:15:54,048 INFO Created variable conv4_1/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1a4c7d0&gt;\nLayer name: conv4_2\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv4_2/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1a12500&gt;\n2016-06-30 14:15:54,063 INFO Created variable conv4_2/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1a12500&gt;\n\nINFO:tensorflow:Created variable conv4_2/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1a12500&gt;\n2016-06-30 14:15:54,068 INFO Created variable conv4_2/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1a12500&gt;\nLayer name: conv4_3\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv4_3/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb19d89b0&gt;\n2016-06-30 14:15:54,083 INFO Created variable conv4_3/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb19d89b0&gt;\n\nINFO:tensorflow:Created variable conv4_3/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb19d89b0&gt;\n2016-06-30 14:15:54,088 INFO Created variable conv4_3/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb19d89b0&gt;\nPool 4 layer ready\nLayer name: conv5_1\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv5_1/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb199f6e0&gt;\n2016-06-30 14:15:54,103 INFO Created variable conv5_1/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb199f6e0&gt;\n\nINFO:tensorflow:Created variable conv5_1/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb199f6e0&gt;\n2016-06-30 14:15:54,108 INFO Created variable conv5_1/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb199f6e0&gt;\nLayer name: conv5_2\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv5_2/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1961488&gt;\n2016-06-30 14:15:54,122 INFO Created variable conv5_2/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1961488&gt;\n\nINFO:tensorflow:Created variable conv5_2/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb18daf50&gt;\n2016-06-30 14:15:54,127 INFO Created variable conv5_2/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb18daf50&gt;\nLayer name: conv5_3\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv5_3/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1961488&gt;\n2016-06-30 14:15:54,141 INFO Created variable conv5_3/filter:0 with shape (3, 3, 512, 512) and init &lt;function _initializer at 0x7fcbb1961488&gt;\n\nINFO:tensorflow:Created variable conv5_3/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1961488&gt;\n2016-06-30 14:15:54,146 INFO Created variable conv5_3/biases:0 with shape (512,) and init &lt;function _initializer at 0x7fcbb1961488&gt;\nPool 5 layer ready\nLayer name: fc6\nLayer shape: [7, 7, 512, 4096]\n\nINFO:tensorflow:Created variable fc6/weights:0 with shape (7, 7, 512, 4096) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;\n2016-06-30 14:15:54,435 INFO Created variable fc6/weights:0 with shape (7, 7, 512, 4096) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;\n\nINFO:tensorflow:Created variable fc6/biases:0 with shape (4096,) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;\n2016-06-30 14:15:54,438 INFO Created variable fc6/biases:0 with shape (4096,) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;\nLayer name: fc7\nLayer shape: [1, 1, 4096, 4096]\n\nINFO:tensorflow:Created variable fc7/weights:0 with shape (1, 1, 4096, 4096) and init &lt;function _initializer at 0x7fcbb189fed8&gt;\n2016-06-30 14:15:54,492 INFO Created variable fc7/weights:0 with shape (1, 1, 4096, 4096) and init &lt;function _initializer at 0x7fcbb189fed8&gt;\n\nINFO:tensorflow:Created variable fc7/biases:0 with shape (4096,) and init &lt;function _initializer at 0x7fcbb189fed8&gt;\n2016-06-30 14:15:54,495 INFO Created variable fc7/biases:0 with shape (4096,) and init &lt;function _initializer at 0x7fcbb189fed8&gt;\n\nINFO:tensorflow:Created variable score_fr/weights:0 with shape (1, 1, 4096, 14) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;\n2016-06-30 14:15:54,506 INFO Created variable score_fr/weights:0 with shape (1, 1, 4096, 14) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;\n\nINFO:tensorflow:Created variable score_fr/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;\n2016-06-30 14:15:54,510 INFO Created variable score_fr/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb186b5f0&gt;\n\nINFO:tensorflow:Created variable upscore2/up_filter:0 with shape (4, 4, 14, 14) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;\n2016-06-30 14:15:54,525 INFO Created variable upscore2/up_filter:0 with shape (4, 4, 14, 14) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;\n\nINFO:tensorflow:Created variable score_pool4/weights:0 with shape (1, 1, 512, 14) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;\n2016-06-30 14:15:54,536 INFO Created variable score_pool4/weights:0 with shape (1, 1, 512, 14) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;\n\nINFO:tensorflow:Created variable score_pool4/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;\n2016-06-30 14:15:54,540 INFO Created variable score_pool4/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb17a48c0&gt;\n\nINFO:tensorflow:Created variable upscore4/up_filter:0 with shape (4, 4, 14, 14) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;\n2016-06-30 14:15:54,555 INFO Created variable upscore4/up_filter:0 with shape (4, 4, 14, 14) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;\n\nINFO:tensorflow:Created variable score_pool3/weights:0 with shape (1, 1, 256, 14) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;\n2016-06-30 14:15:54,566 INFO Created variable score_pool3/weights:0 with shape (1, 1, 256, 14) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;\n\nINFO:tensorflow:Created variable score_pool3/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;\n2016-06-30 14:15:54,570 INFO Created variable score_pool3/biases:0 with shape (14,) and init &lt;function _initializer at 0x7fcbb16c0aa0&gt;\n\nINFO:tensorflow:Created variable upscore32/up_filter:0 with shape (16, 16, 14, 14) and init &lt;function _initializer at 0x7fcbb15eb758&gt;\n2016-06-30 14:15:54,585 INFO Created variable upscore32/up_filter:0 with shape (16, 16, 14, 14) and init &lt;function _initializer at 0x7fcbb15eb758&gt;`\n</code></pre>\n<p>Been stuck on this for quite some time and cant figure out whats wrong... Any help will be appreciated :)</p>", "body_text": "I am trying to implement Fully Connected Convolutional Network in python using tensorflow and I am getting a Stack Smashing error in one of the Convolution operators. I have checked and all the filter sizes match up and are perfectly in sync with the layer shapes. I did my research on the error and found that this error is caused when you overflow the alloted buffer but I cant see any place where I exceed this buffer and the most weird part is that it's running for 10 layers (7 convolutions and 3 max pool ones of the VGG network) and crashing on the next convolution.\n(Posted this on Stack Overflow and was told that this error is due to a bug in tensorflow and hence should be posted here. The reason I was told was that the stack smashing is happening in the C++ code and not the python code and I havent written any C++ code myself so the most likely place for the error to have happened is inside tensorflow)\nThe error I get is:\n-> _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict) (Pdb) c I tensorflow/core/kernels/logging_ops.cc:79] Shape of input image: [1 375 1242 3] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv1_1[1 375 1242 64] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv1_2[1 375 1242 64] I tensorflow/core/kernels/logging_ops.cc:79] Shape of pool1[1 188 621 64] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv2_1[1 188 621 128] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv2_2[1 188 621 128] I tensorflow/core/kernels/logging_ops.cc:79] Shape of pool2[1 94 311 128] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_1[1 94 311 256] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_2[1 94 311 256] I tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_3[1 94 311 256] I tensorflow/core/kernels/logging_ops.cc:79] Shape of pool3[1 47 156 256] *** stack smashing detected ***: python terminated Aborted (core dumped)\nHere is the relevant part of my code:\n`class FCN8VGG:\ndef __init__(self, vgg16_npy_path=None):\n    if vgg16_npy_path is None:\n        path = sys.modules[self.__class__.__module__].__file__\n        # print path\n        path = os.path.abspath(os.path.join(path, os.pardir))\n        # print path\n        path = os.path.join(path, \"vgg16.npy\")\n        print(path)\n        vgg16_npy_path = path\n\n    self.data_dict = np.load(vgg16_npy_path).item()\n    self.wd = 5e-4\n    print(\"npy file loaded\")\ndef build(self, rgb, train=True, num_classes=14, random_init_fc8=True,\n          debug=True):\n    \"\"\"\n    Build the VGG model using loaded weights\n    Parameters\n    ----------\n    rgb: image batch tensor\n        Image in rgb shap. Scaled to Intervall [0, 255]\n    train: bool\n        Whether to build train or inference graph\n    num_classes: int\n        How many classes should be predicted (by fc8)\n    random_init_fc8 : bool\n        Whether to initialize fc8 layer randomly.\n        Finetuning is required in this case.\n    debug: bool\n        Whether to print additional Debug Information.\n    \"\"\"\n    # Convert RGB to BGR\n\n    with tf.name_scope('Processing'):\n        #import pdb;pdb.set_trace()\n        red, green, blue = tf.split(3, 3, rgb)\n        # assert red.get_shape().as_list()[1:] == [224, 224, 1]\n        # assert green.get_shape().as_list()[1:] == [224, 224, 1]\n        # assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n        bgr = tf.concat(3, [\n            blue - VGG_MEAN[0],\n            green - VGG_MEAN[1],\n            red - VGG_MEAN[2],\n        ])\n\n        if debug:\n            bgr = tf.Print(bgr, [tf.shape(bgr)],\n                           message='Shape of input image: ',\n                           summarize=4, first_n=1)\n\n    self.conv1_1 = self._conv_layer(bgr, \"conv1_1\")\n    self.conv1_2 = self._conv_layer(self.conv1_1, \"conv1_2\")\n    self.pool1 = self._max_pool(self.conv1_2, 'pool1', debug)\n\n    print(\"Pool1 layer ready\")\n\n    self.conv2_1 = self._conv_layer(self.pool1, \"conv2_1\")\n    self.conv2_2 = self._conv_layer(self.conv2_1, \"conv2_2\")\n    self.pool2 = self._max_pool(self.conv2_2, 'pool2', debug)\n    self.conv3_1 = self._conv_layer(self.pool2, \"conv3_1\")\n    self.conv3_2 = self._conv_layer(self.conv3_1, \"conv3_2\")\n    self.conv3_3 = self._conv_layer(self.conv3_2, \"conv3_3\")\n    self.pool3 = self._max_pool(self.conv3_3, 'pool3', debug)\n\n    print(\"Pool 3 layer ready\")\n    #pdb.set_trace()\n    self.conv4_1 = self._conv_layer(self.pool3, \"conv4_1\")\n    self.conv4_2 = self._conv_layer(self.conv4_1, \"conv4_2\")\n    self.conv4_3 = self._conv_layer(self.conv4_2, \"conv4_3\")\n    self.pool4 = self._max_pool(self.conv4_3, 'pool4', debug)\n\n    print(\"Pool 4 layer ready\")\n    #pdb.set_trace()\n    self.conv5_1 = self._conv_layer(self.pool4, \"conv5_1\")\n    self.conv5_2 = self._conv_layer(self.conv5_1, \"conv5_2\")\n    self.conv5_3 = self._conv_layer(self.conv5_2, \"conv5_3\")\n    self.pool5 = self._max_pool(self.conv5_3, 'pool5', debug)\n\n    print(\"Pool 5 layer ready\")\n    #pdb.set_trace()\n    self.fc6 = self._fc_layer(self.pool5, \"fc6\")\n\n    if train:\n        self.fc6 = tf.nn.dropout(self.fc6, 0.5)\n\n    self.fc7 = self._fc_layer(self.fc6, \"fc7\")\n    if train:\n        self.fc7 = tf.nn.dropout(self.fc7, 0.5)\n\n    if random_init_fc8:\n        self.score_fr = self._score_layer(self.fc7, \"score_fr\",\n                                          num_classes)\n    else:\n        self.score_fr = self._fc_layer(self.fc7, \"score_fr\",\n                                       num_classes=num_classes,\n                                       relu=False)\n\n    self.pred = tf.argmax(self.score_fr, dimension=3)\n\n    self.upscore2 = self._upscore_layer(self.score_fr,\n                                        shape=tf.shape(self.pool4),\n                                        num_classes=num_classes,\n                                        debug=debug, name='upscore2',\n                                        ksize=4, stride=2)\n    self.score_pool4 = self._score_layer(self.pool4, \"score_pool4\",\n                                         num_classes=num_classes)\n    self.fuse_pool4 = tf.add(self.upscore2, self.score_pool4)\n    self.upscore4 = self._upscore_layer(self.fuse_pool4,\n                                        shape=tf.shape(self.pool3),\n                                        num_classes=num_classes,\n                                        debug=debug, name='upscore4',\n                                        ksize=4, stride=2)\n    self.score_pool3 = self._score_layer(self.pool3, \"score_pool3\",\n                                         num_classes=num_classes)\n    self.fuse_pool3 = tf.add(self.upscore4, self.score_pool3)\n\n    self.upscore32 = self._upscore_layer(self.fuse_pool3,\n                                         shape=tf.shape(bgr),\n                                         num_classes=num_classes,\n                                         debug=debug, name='upscore32',\n                                         ksize=16, stride=8)\n\n    self.pred_up = tf.argmax(self.upscore32, dimension=3)\n\ndef _max_pool(self, bottom, name, debug):\n    pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                          padding='SAME', name=name)\n\n    if debug:\n        pool = tf.Print(pool, [tf.shape(pool)],\n                        message='Shape of %s' % name,\n                        summarize=4, first_n=1)\n    return pool\n\ndef _conv_layer(self, bottom, name):\n    with tf.variable_scope(name) as scope:\n        filt = self.get_conv_filter(name)\n        conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n\n        conv = tf.Print(conv, [tf.shape(conv)], message='Shape of %s' % name, summarize=4, first_n=1)\n\n        conv_biases = self.get_bias(name)\n        bias = tf.nn.bias_add(conv, conv_biases)\n        if relu:\n            bias = tf.nn.relu(bias)\n        _activation_summary(bias)\n\n        if debug:\n            bias = tf.Print(bias, [tf.shape(bias)],\n                            message='Shape of %s' % name,\n                            summarize=4, first_n=1)\n        return bias`\n\nThe driver tensorflow code has this command which produces the error:\n\n` _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict)`\n\nWhere loss is the tensorflow op for:\n\n`def loss(logits, labels):\n\"\"\"Calculate the loss from the logits and the labels.\n\nArgs:\n  logits: Logits tensor, float - [batch_size, height, width, NUM_CLASSES].  Must be unsclaed and unsoftmaxed.\n  labels: Labels tensor, int32 - [batch_size, height, width].\n\nReturns:\n  loss: Loss tensor of type float.\n\"\"\"\nwith tf.name_scope('loss'):\n    reshaped_logits = tf.reshape(logits, [-1, 14])  # shape [batch_size*height*width, 14]\n    reshaped_labels = tf.reshape(labels, [-1])  # shape [batch_size*height*width]   #Need fix size images for this\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(reshaped_logits, reshaped_labels)\nreturn loss   `\n\nAnd training is the driver training function:\n\n`def training(loss, learning_rate):\n  \"\"\"Sets up the training Ops.\n\n  Creates a summarizer to track the loss over time in TensorBoard.\n\n  Creates an optimizer and applies the gradients to all trainable variables.\n\n  The Op returned by this function is what must be passed to the\n  `sess.run()` call to cause the model to train.\n\n  Args:\n    loss: Loss tensor, from loss().\n    learning_rate: The learning rate to use for gradient descent.\n\n  Returns:\n    train_op: The Op for training.\n  \"\"\"\n  # Add a scalar summary for the snapshot loss.\n  tf.scalar_summary(loss.op.name, loss)\n  # Create the gradient descent optimizer with the given learning rate.\n  #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  optimizer = tf.train.AdamOptimizer(1e-6)\n  # Create a variable to track the global step.\n  print(\"Setting step size to 1e-6 for Adam Optimizer\")\n  global_step = tf.Variable(0, name='global_step', trainable=False)\n  # Use the optimizer to apply the gradients that minimize the loss\n  # (and also increment the global step counter) as a single training step.\n  print(\"Running optimizer\")\n  train_op = optimizer.minimize(loss, global_step=global_step)\n  return train_op `\n\nThe debug log from the building of the network is: This contains details of filter and layer shapes etc.\n`Layer name: conv1_1\nLayer shape: (3, 3, 3, 64)\nINFO:tensorflow:Created variable conv1_1/filter:0 with shape (3, 3, 3, 64) and init <function _initializer at 0x7fcc27c85ed8>\n2016-06-30 14:15:53,939 INFO Created variable conv1_1/filter:0 with shape (3, 3, 3, 64) and init <function _initializer at 0x7fcc27c85ed8>\n\nINFO:tensorflow:Created variable conv1_1/biases:0 with shape (64,) and init <function _initializer at 0x7fcc27c85ed8>\n2016-06-30 14:15:53,944 INFO Created variable conv1_1/biases:0 with shape (64,) and init <function _initializer at 0x7fcc27c85ed8>\nLayer name: conv1_2\nLayer shape: (3, 3, 64, 64)\n\nINFO:tensorflow:Created variable conv1_2/filter:0 with shape (3, 3, 64, 64) and init <function _initializer at 0x7fcc27c96488>\n2016-06-30 14:15:53,952 INFO Created variable conv1_2/filter:0 with shape (3, 3, 64, 64) and init <function _initializer at 0x7fcc27c96488>\n\nINFO:tensorflow:Created variable conv1_2/biases:0 with shape (64,) and init <function _initializer at 0x7fcc27c96488>\n2016-06-30 14:15:53,956 INFO Created variable conv1_2/biases:0 with shape (64,) and init <function _initializer at 0x7fcc27c96488>\nPool1 layer ready\nLayer name: conv2_1\nLayer shape: (3, 3, 64, 128)\n\nINFO:tensorflow:Created variable conv2_1/filter:0 with shape (3, 3, 64, 128) and init <function _initializer at 0x7fcc27c9dc08>\n2016-06-30 14:15:53,967 INFO Created variable conv2_1/filter:0 with shape (3, 3, 64, 128) and init <function _initializer at 0x7fcc27c9dc08>\n\nINFO:tensorflow:Created variable conv2_1/biases:0 with shape (128,) and init <function _initializer at 0x7fcc27c9dc08>\n2016-06-30 14:15:53,972 INFO Created variable conv2_1/biases:0 with shape (128,) and init <function _initializer at 0x7fcc27c9dc08>\nLayer name: conv2_2\nLayer shape: (3, 3, 128, 128)\n\nINFO:tensorflow:Created variable conv2_2/filter:0 with shape (3, 3, 128, 128) and init <function _initializer at 0x7fcbb3012488>\n2016-06-30 14:15:53,980 INFO Created variable conv2_2/filter:0 with shape (3, 3, 128, 128) and init <function _initializer at 0x7fcbb3012488>\n\nINFO:tensorflow:Created variable conv2_2/biases:0 with shape (128,) and init <function _initializer at 0x7fcbb2f8af50>\n2016-06-30 14:15:53,985 INFO Created variable conv2_2/biases:0 with shape (128,) and init <function _initializer at 0x7fcbb2f8af50>\nPool 2 layer ready\nLayer name: conv3_1\nLayer shape: (3, 3, 128, 256)\n\nINFO:tensorflow:Created variable conv3_1/filter:0 with shape (3, 3, 128, 256) and init <function _initializer at 0x7fcbb3012488>\n2016-06-30 14:15:53,995 INFO Created variable conv3_1/filter:0 with shape (3, 3, 128, 256) and init <function _initializer at 0x7fcbb3012488>\n\nINFO:tensorflow:Created variable conv3_1/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb1a7cb90>\n2016-06-30 14:15:54,000 INFO Created variable conv3_1/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb1a7cb90>\nLayer name: conv3_2\nLayer shape: (3, 3, 256, 256)\n\nINFO:tensorflow:Created variable conv3_2/filter:0 with shape (3, 3, 256, 256) and init <function _initializer at 0x7fcbb3012488>\n2016-06-30 14:15:54,010 INFO Created variable conv3_2/filter:0 with shape (3, 3, 256, 256) and init <function _initializer at 0x7fcbb3012488>\n\nINFO:tensorflow:Created variable conv3_2/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb3012488>\n2016-06-30 14:15:54,015 INFO Created variable conv3_2/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb3012488>\nLayer name: conv3_3\nLayer shape: (3, 3, 256, 256)\n\nINFO:tensorflow:Created variable conv3_3/filter:0 with shape (3, 3, 256, 256) and init <function _initializer at 0x7fcbb1a8aaa0>\n2016-06-30 14:15:54,024 INFO Created variable conv3_3/filter:0 with shape (3, 3, 256, 256) and init <function _initializer at 0x7fcbb1a8aaa0>\n\nINFO:tensorflow:Created variable conv3_3/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb1a8aaa0>\n2016-06-30 14:15:54,029 INFO Created variable conv3_3/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb1a8aaa0>\nPool 3 layer ready\nLayer name: conv4_1\nLayer shape: (3, 3, 256, 512)\n\nINFO:tensorflow:Created variable conv4_1/filter:0 with shape (3, 3, 256, 512) and init <function _initializer at 0x7fcbb1a4c7d0>\n2016-06-30 14:15:54,043 INFO Created variable conv4_1/filter:0 with shape (3, 3, 256, 512) and init <function _initializer at 0x7fcbb1a4c7d0>\n\nINFO:tensorflow:Created variable conv4_1/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1a4c7d0>\n2016-06-30 14:15:54,048 INFO Created variable conv4_1/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1a4c7d0>\nLayer name: conv4_2\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv4_2/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1a12500>\n2016-06-30 14:15:54,063 INFO Created variable conv4_2/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1a12500>\n\nINFO:tensorflow:Created variable conv4_2/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1a12500>\n2016-06-30 14:15:54,068 INFO Created variable conv4_2/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1a12500>\nLayer name: conv4_3\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv4_3/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb19d89b0>\n2016-06-30 14:15:54,083 INFO Created variable conv4_3/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb19d89b0>\n\nINFO:tensorflow:Created variable conv4_3/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb19d89b0>\n2016-06-30 14:15:54,088 INFO Created variable conv4_3/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb19d89b0>\nPool 4 layer ready\nLayer name: conv5_1\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv5_1/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb199f6e0>\n2016-06-30 14:15:54,103 INFO Created variable conv5_1/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb199f6e0>\n\nINFO:tensorflow:Created variable conv5_1/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb199f6e0>\n2016-06-30 14:15:54,108 INFO Created variable conv5_1/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb199f6e0>\nLayer name: conv5_2\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv5_2/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1961488>\n2016-06-30 14:15:54,122 INFO Created variable conv5_2/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1961488>\n\nINFO:tensorflow:Created variable conv5_2/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb18daf50>\n2016-06-30 14:15:54,127 INFO Created variable conv5_2/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb18daf50>\nLayer name: conv5_3\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv5_3/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1961488>\n2016-06-30 14:15:54,141 INFO Created variable conv5_3/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1961488>\n\nINFO:tensorflow:Created variable conv5_3/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1961488>\n2016-06-30 14:15:54,146 INFO Created variable conv5_3/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1961488>\nPool 5 layer ready\nLayer name: fc6\nLayer shape: [7, 7, 512, 4096]\n\nINFO:tensorflow:Created variable fc6/weights:0 with shape (7, 7, 512, 4096) and init <function _initializer at 0x7fcbb186b5f0>\n2016-06-30 14:15:54,435 INFO Created variable fc6/weights:0 with shape (7, 7, 512, 4096) and init <function _initializer at 0x7fcbb186b5f0>\n\nINFO:tensorflow:Created variable fc6/biases:0 with shape (4096,) and init <function _initializer at 0x7fcbb186b5f0>\n2016-06-30 14:15:54,438 INFO Created variable fc6/biases:0 with shape (4096,) and init <function _initializer at 0x7fcbb186b5f0>\nLayer name: fc7\nLayer shape: [1, 1, 4096, 4096]\n\nINFO:tensorflow:Created variable fc7/weights:0 with shape (1, 1, 4096, 4096) and init <function _initializer at 0x7fcbb189fed8>\n2016-06-30 14:15:54,492 INFO Created variable fc7/weights:0 with shape (1, 1, 4096, 4096) and init <function _initializer at 0x7fcbb189fed8>\n\nINFO:tensorflow:Created variable fc7/biases:0 with shape (4096,) and init <function _initializer at 0x7fcbb189fed8>\n2016-06-30 14:15:54,495 INFO Created variable fc7/biases:0 with shape (4096,) and init <function _initializer at 0x7fcbb189fed8>\n\nINFO:tensorflow:Created variable score_fr/weights:0 with shape (1, 1, 4096, 14) and init <function _initializer at 0x7fcbb186b5f0>\n2016-06-30 14:15:54,506 INFO Created variable score_fr/weights:0 with shape (1, 1, 4096, 14) and init <function _initializer at 0x7fcbb186b5f0>\n\nINFO:tensorflow:Created variable score_fr/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb186b5f0>\n2016-06-30 14:15:54,510 INFO Created variable score_fr/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb186b5f0>\n\nINFO:tensorflow:Created variable upscore2/up_filter:0 with shape (4, 4, 14, 14) and init <function _initializer at 0x7fcbb17a48c0>\n2016-06-30 14:15:54,525 INFO Created variable upscore2/up_filter:0 with shape (4, 4, 14, 14) and init <function _initializer at 0x7fcbb17a48c0>\n\nINFO:tensorflow:Created variable score_pool4/weights:0 with shape (1, 1, 512, 14) and init <function _initializer at 0x7fcbb17a48c0>\n2016-06-30 14:15:54,536 INFO Created variable score_pool4/weights:0 with shape (1, 1, 512, 14) and init <function _initializer at 0x7fcbb17a48c0>\n\nINFO:tensorflow:Created variable score_pool4/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb17a48c0>\n2016-06-30 14:15:54,540 INFO Created variable score_pool4/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb17a48c0>\n\nINFO:tensorflow:Created variable upscore4/up_filter:0 with shape (4, 4, 14, 14) and init <function _initializer at 0x7fcbb16c0aa0>\n2016-06-30 14:15:54,555 INFO Created variable upscore4/up_filter:0 with shape (4, 4, 14, 14) and init <function _initializer at 0x7fcbb16c0aa0>\n\nINFO:tensorflow:Created variable score_pool3/weights:0 with shape (1, 1, 256, 14) and init <function _initializer at 0x7fcbb16c0aa0>\n2016-06-30 14:15:54,566 INFO Created variable score_pool3/weights:0 with shape (1, 1, 256, 14) and init <function _initializer at 0x7fcbb16c0aa0>\n\nINFO:tensorflow:Created variable score_pool3/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb16c0aa0>\n2016-06-30 14:15:54,570 INFO Created variable score_pool3/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb16c0aa0>\n\nINFO:tensorflow:Created variable upscore32/up_filter:0 with shape (16, 16, 14, 14) and init <function _initializer at 0x7fcbb15eb758>\n2016-06-30 14:15:54,585 INFO Created variable upscore32/up_filter:0 with shape (16, 16, 14, 14) and init <function _initializer at 0x7fcbb15eb758>`\n\nBeen stuck on this for quite some time and cant figure out whats wrong... Any help will be appreciated :)", "body": "I am trying to implement Fully Connected Convolutional Network in python using tensorflow and I am getting a Stack Smashing error in one of the Convolution operators. I have checked and all the filter sizes match up and are perfectly in sync with the layer shapes. I did my research on the error and found that this error is caused when you overflow the alloted buffer but I cant see any place where I exceed this buffer and the most weird part is that it's running for 10 layers (7 convolutions and 3 max pool ones of the VGG network) and crashing on the next convolution.\n(Posted this on Stack Overflow and was told that this error is due to a bug in tensorflow and hence should be posted here. The reason I was told was that the stack smashing is happening in the C++ code and not the python code and I havent written any C++ code myself so the most likely place for the error to have happened is inside tensorflow)\n\nThe error I get is:\n`-> _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict)\n(Pdb) c\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of input image: [1 375 1242 3]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of conv1_1[1 375 1242 64]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of conv1_2[1 375 1242 64]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of pool1[1 188 621 64]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of conv2_1[1 188 621 128]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of conv2_2[1 188 621 128]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of pool2[1 94 311 128]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_1[1 94 311 256]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_2[1 94 311 256]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of conv3_3[1 94 311 256]\nI tensorflow/core/kernels/logging_ops.cc:79] Shape of pool3[1 47 156 256]\n*** stack smashing detected ***: python terminated\nAborted (core dumped)`\n\nHere is the relevant part of my code:\n\n```\n`class FCN8VGG:\ndef __init__(self, vgg16_npy_path=None):\n    if vgg16_npy_path is None:\n        path = sys.modules[self.__class__.__module__].__file__\n        # print path\n        path = os.path.abspath(os.path.join(path, os.pardir))\n        # print path\n        path = os.path.join(path, \"vgg16.npy\")\n        print(path)\n        vgg16_npy_path = path\n\n    self.data_dict = np.load(vgg16_npy_path).item()\n    self.wd = 5e-4\n    print(\"npy file loaded\")\ndef build(self, rgb, train=True, num_classes=14, random_init_fc8=True,\n          debug=True):\n    \"\"\"\n    Build the VGG model using loaded weights\n    Parameters\n    ----------\n    rgb: image batch tensor\n        Image in rgb shap. Scaled to Intervall [0, 255]\n    train: bool\n        Whether to build train or inference graph\n    num_classes: int\n        How many classes should be predicted (by fc8)\n    random_init_fc8 : bool\n        Whether to initialize fc8 layer randomly.\n        Finetuning is required in this case.\n    debug: bool\n        Whether to print additional Debug Information.\n    \"\"\"\n    # Convert RGB to BGR\n\n    with tf.name_scope('Processing'):\n        #import pdb;pdb.set_trace()\n        red, green, blue = tf.split(3, 3, rgb)\n        # assert red.get_shape().as_list()[1:] == [224, 224, 1]\n        # assert green.get_shape().as_list()[1:] == [224, 224, 1]\n        # assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n        bgr = tf.concat(3, [\n            blue - VGG_MEAN[0],\n            green - VGG_MEAN[1],\n            red - VGG_MEAN[2],\n        ])\n\n        if debug:\n            bgr = tf.Print(bgr, [tf.shape(bgr)],\n                           message='Shape of input image: ',\n                           summarize=4, first_n=1)\n\n    self.conv1_1 = self._conv_layer(bgr, \"conv1_1\")\n    self.conv1_2 = self._conv_layer(self.conv1_1, \"conv1_2\")\n    self.pool1 = self._max_pool(self.conv1_2, 'pool1', debug)\n\n    print(\"Pool1 layer ready\")\n\n    self.conv2_1 = self._conv_layer(self.pool1, \"conv2_1\")\n    self.conv2_2 = self._conv_layer(self.conv2_1, \"conv2_2\")\n    self.pool2 = self._max_pool(self.conv2_2, 'pool2', debug)\n    self.conv3_1 = self._conv_layer(self.pool2, \"conv3_1\")\n    self.conv3_2 = self._conv_layer(self.conv3_1, \"conv3_2\")\n    self.conv3_3 = self._conv_layer(self.conv3_2, \"conv3_3\")\n    self.pool3 = self._max_pool(self.conv3_3, 'pool3', debug)\n\n    print(\"Pool 3 layer ready\")\n    #pdb.set_trace()\n    self.conv4_1 = self._conv_layer(self.pool3, \"conv4_1\")\n    self.conv4_2 = self._conv_layer(self.conv4_1, \"conv4_2\")\n    self.conv4_3 = self._conv_layer(self.conv4_2, \"conv4_3\")\n    self.pool4 = self._max_pool(self.conv4_3, 'pool4', debug)\n\n    print(\"Pool 4 layer ready\")\n    #pdb.set_trace()\n    self.conv5_1 = self._conv_layer(self.pool4, \"conv5_1\")\n    self.conv5_2 = self._conv_layer(self.conv5_1, \"conv5_2\")\n    self.conv5_3 = self._conv_layer(self.conv5_2, \"conv5_3\")\n    self.pool5 = self._max_pool(self.conv5_3, 'pool5', debug)\n\n    print(\"Pool 5 layer ready\")\n    #pdb.set_trace()\n    self.fc6 = self._fc_layer(self.pool5, \"fc6\")\n\n    if train:\n        self.fc6 = tf.nn.dropout(self.fc6, 0.5)\n\n    self.fc7 = self._fc_layer(self.fc6, \"fc7\")\n    if train:\n        self.fc7 = tf.nn.dropout(self.fc7, 0.5)\n\n    if random_init_fc8:\n        self.score_fr = self._score_layer(self.fc7, \"score_fr\",\n                                          num_classes)\n    else:\n        self.score_fr = self._fc_layer(self.fc7, \"score_fr\",\n                                       num_classes=num_classes,\n                                       relu=False)\n\n    self.pred = tf.argmax(self.score_fr, dimension=3)\n\n    self.upscore2 = self._upscore_layer(self.score_fr,\n                                        shape=tf.shape(self.pool4),\n                                        num_classes=num_classes,\n                                        debug=debug, name='upscore2',\n                                        ksize=4, stride=2)\n    self.score_pool4 = self._score_layer(self.pool4, \"score_pool4\",\n                                         num_classes=num_classes)\n    self.fuse_pool4 = tf.add(self.upscore2, self.score_pool4)\n    self.upscore4 = self._upscore_layer(self.fuse_pool4,\n                                        shape=tf.shape(self.pool3),\n                                        num_classes=num_classes,\n                                        debug=debug, name='upscore4',\n                                        ksize=4, stride=2)\n    self.score_pool3 = self._score_layer(self.pool3, \"score_pool3\",\n                                         num_classes=num_classes)\n    self.fuse_pool3 = tf.add(self.upscore4, self.score_pool3)\n\n    self.upscore32 = self._upscore_layer(self.fuse_pool3,\n                                         shape=tf.shape(bgr),\n                                         num_classes=num_classes,\n                                         debug=debug, name='upscore32',\n                                         ksize=16, stride=8)\n\n    self.pred_up = tf.argmax(self.upscore32, dimension=3)\n\ndef _max_pool(self, bottom, name, debug):\n    pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                          padding='SAME', name=name)\n\n    if debug:\n        pool = tf.Print(pool, [tf.shape(pool)],\n                        message='Shape of %s' % name,\n                        summarize=4, first_n=1)\n    return pool\n\ndef _conv_layer(self, bottom, name):\n    with tf.variable_scope(name) as scope:\n        filt = self.get_conv_filter(name)\n        conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n\n        conv = tf.Print(conv, [tf.shape(conv)], message='Shape of %s' % name, summarize=4, first_n=1)\n\n        conv_biases = self.get_bias(name)\n        bias = tf.nn.bias_add(conv, conv_biases)\n        if relu:\n            bias = tf.nn.relu(bias)\n        _activation_summary(bias)\n\n        if debug:\n            bias = tf.Print(bias, [tf.shape(bias)],\n                            message='Shape of %s' % name,\n                            summarize=4, first_n=1)\n        return bias`\n\nThe driver tensorflow code has this command which produces the error:\n\n` _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict)`\n\nWhere loss is the tensorflow op for:\n\n`def loss(logits, labels):\n\"\"\"Calculate the loss from the logits and the labels.\n\nArgs:\n  logits: Logits tensor, float - [batch_size, height, width, NUM_CLASSES].  Must be unsclaed and unsoftmaxed.\n  labels: Labels tensor, int32 - [batch_size, height, width].\n\nReturns:\n  loss: Loss tensor of type float.\n\"\"\"\nwith tf.name_scope('loss'):\n    reshaped_logits = tf.reshape(logits, [-1, 14])  # shape [batch_size*height*width, 14]\n    reshaped_labels = tf.reshape(labels, [-1])  # shape [batch_size*height*width]   #Need fix size images for this\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(reshaped_logits, reshaped_labels)\nreturn loss   `\n\nAnd training is the driver training function:\n\n`def training(loss, learning_rate):\n  \"\"\"Sets up the training Ops.\n\n  Creates a summarizer to track the loss over time in TensorBoard.\n\n  Creates an optimizer and applies the gradients to all trainable variables.\n\n  The Op returned by this function is what must be passed to the\n  `sess.run()` call to cause the model to train.\n\n  Args:\n    loss: Loss tensor, from loss().\n    learning_rate: The learning rate to use for gradient descent.\n\n  Returns:\n    train_op: The Op for training.\n  \"\"\"\n  # Add a scalar summary for the snapshot loss.\n  tf.scalar_summary(loss.op.name, loss)\n  # Create the gradient descent optimizer with the given learning rate.\n  #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  optimizer = tf.train.AdamOptimizer(1e-6)\n  # Create a variable to track the global step.\n  print(\"Setting step size to 1e-6 for Adam Optimizer\")\n  global_step = tf.Variable(0, name='global_step', trainable=False)\n  # Use the optimizer to apply the gradients that minimize the loss\n  # (and also increment the global step counter) as a single training step.\n  print(\"Running optimizer\")\n  train_op = optimizer.minimize(loss, global_step=global_step)\n  return train_op `\n```\n\nThe debug log from the building of the network is: This contains details of filter and layer shapes etc.\n\n```\n`Layer name: conv1_1\nLayer shape: (3, 3, 3, 64)\nINFO:tensorflow:Created variable conv1_1/filter:0 with shape (3, 3, 3, 64) and init <function _initializer at 0x7fcc27c85ed8>\n2016-06-30 14:15:53,939 INFO Created variable conv1_1/filter:0 with shape (3, 3, 3, 64) and init <function _initializer at 0x7fcc27c85ed8>\n\nINFO:tensorflow:Created variable conv1_1/biases:0 with shape (64,) and init <function _initializer at 0x7fcc27c85ed8>\n2016-06-30 14:15:53,944 INFO Created variable conv1_1/biases:0 with shape (64,) and init <function _initializer at 0x7fcc27c85ed8>\nLayer name: conv1_2\nLayer shape: (3, 3, 64, 64)\n\nINFO:tensorflow:Created variable conv1_2/filter:0 with shape (3, 3, 64, 64) and init <function _initializer at 0x7fcc27c96488>\n2016-06-30 14:15:53,952 INFO Created variable conv1_2/filter:0 with shape (3, 3, 64, 64) and init <function _initializer at 0x7fcc27c96488>\n\nINFO:tensorflow:Created variable conv1_2/biases:0 with shape (64,) and init <function _initializer at 0x7fcc27c96488>\n2016-06-30 14:15:53,956 INFO Created variable conv1_2/biases:0 with shape (64,) and init <function _initializer at 0x7fcc27c96488>\nPool1 layer ready\nLayer name: conv2_1\nLayer shape: (3, 3, 64, 128)\n\nINFO:tensorflow:Created variable conv2_1/filter:0 with shape (3, 3, 64, 128) and init <function _initializer at 0x7fcc27c9dc08>\n2016-06-30 14:15:53,967 INFO Created variable conv2_1/filter:0 with shape (3, 3, 64, 128) and init <function _initializer at 0x7fcc27c9dc08>\n\nINFO:tensorflow:Created variable conv2_1/biases:0 with shape (128,) and init <function _initializer at 0x7fcc27c9dc08>\n2016-06-30 14:15:53,972 INFO Created variable conv2_1/biases:0 with shape (128,) and init <function _initializer at 0x7fcc27c9dc08>\nLayer name: conv2_2\nLayer shape: (3, 3, 128, 128)\n\nINFO:tensorflow:Created variable conv2_2/filter:0 with shape (3, 3, 128, 128) and init <function _initializer at 0x7fcbb3012488>\n2016-06-30 14:15:53,980 INFO Created variable conv2_2/filter:0 with shape (3, 3, 128, 128) and init <function _initializer at 0x7fcbb3012488>\n\nINFO:tensorflow:Created variable conv2_2/biases:0 with shape (128,) and init <function _initializer at 0x7fcbb2f8af50>\n2016-06-30 14:15:53,985 INFO Created variable conv2_2/biases:0 with shape (128,) and init <function _initializer at 0x7fcbb2f8af50>\nPool 2 layer ready\nLayer name: conv3_1\nLayer shape: (3, 3, 128, 256)\n\nINFO:tensorflow:Created variable conv3_1/filter:0 with shape (3, 3, 128, 256) and init <function _initializer at 0x7fcbb3012488>\n2016-06-30 14:15:53,995 INFO Created variable conv3_1/filter:0 with shape (3, 3, 128, 256) and init <function _initializer at 0x7fcbb3012488>\n\nINFO:tensorflow:Created variable conv3_1/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb1a7cb90>\n2016-06-30 14:15:54,000 INFO Created variable conv3_1/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb1a7cb90>\nLayer name: conv3_2\nLayer shape: (3, 3, 256, 256)\n\nINFO:tensorflow:Created variable conv3_2/filter:0 with shape (3, 3, 256, 256) and init <function _initializer at 0x7fcbb3012488>\n2016-06-30 14:15:54,010 INFO Created variable conv3_2/filter:0 with shape (3, 3, 256, 256) and init <function _initializer at 0x7fcbb3012488>\n\nINFO:tensorflow:Created variable conv3_2/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb3012488>\n2016-06-30 14:15:54,015 INFO Created variable conv3_2/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb3012488>\nLayer name: conv3_3\nLayer shape: (3, 3, 256, 256)\n\nINFO:tensorflow:Created variable conv3_3/filter:0 with shape (3, 3, 256, 256) and init <function _initializer at 0x7fcbb1a8aaa0>\n2016-06-30 14:15:54,024 INFO Created variable conv3_3/filter:0 with shape (3, 3, 256, 256) and init <function _initializer at 0x7fcbb1a8aaa0>\n\nINFO:tensorflow:Created variable conv3_3/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb1a8aaa0>\n2016-06-30 14:15:54,029 INFO Created variable conv3_3/biases:0 with shape (256,) and init <function _initializer at 0x7fcbb1a8aaa0>\nPool 3 layer ready\nLayer name: conv4_1\nLayer shape: (3, 3, 256, 512)\n\nINFO:tensorflow:Created variable conv4_1/filter:0 with shape (3, 3, 256, 512) and init <function _initializer at 0x7fcbb1a4c7d0>\n2016-06-30 14:15:54,043 INFO Created variable conv4_1/filter:0 with shape (3, 3, 256, 512) and init <function _initializer at 0x7fcbb1a4c7d0>\n\nINFO:tensorflow:Created variable conv4_1/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1a4c7d0>\n2016-06-30 14:15:54,048 INFO Created variable conv4_1/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1a4c7d0>\nLayer name: conv4_2\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv4_2/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1a12500>\n2016-06-30 14:15:54,063 INFO Created variable conv4_2/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1a12500>\n\nINFO:tensorflow:Created variable conv4_2/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1a12500>\n2016-06-30 14:15:54,068 INFO Created variable conv4_2/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1a12500>\nLayer name: conv4_3\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv4_3/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb19d89b0>\n2016-06-30 14:15:54,083 INFO Created variable conv4_3/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb19d89b0>\n\nINFO:tensorflow:Created variable conv4_3/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb19d89b0>\n2016-06-30 14:15:54,088 INFO Created variable conv4_3/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb19d89b0>\nPool 4 layer ready\nLayer name: conv5_1\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv5_1/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb199f6e0>\n2016-06-30 14:15:54,103 INFO Created variable conv5_1/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb199f6e0>\n\nINFO:tensorflow:Created variable conv5_1/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb199f6e0>\n2016-06-30 14:15:54,108 INFO Created variable conv5_1/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb199f6e0>\nLayer name: conv5_2\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv5_2/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1961488>\n2016-06-30 14:15:54,122 INFO Created variable conv5_2/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1961488>\n\nINFO:tensorflow:Created variable conv5_2/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb18daf50>\n2016-06-30 14:15:54,127 INFO Created variable conv5_2/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb18daf50>\nLayer name: conv5_3\nLayer shape: (3, 3, 512, 512)\n\nINFO:tensorflow:Created variable conv5_3/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1961488>\n2016-06-30 14:15:54,141 INFO Created variable conv5_3/filter:0 with shape (3, 3, 512, 512) and init <function _initializer at 0x7fcbb1961488>\n\nINFO:tensorflow:Created variable conv5_3/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1961488>\n2016-06-30 14:15:54,146 INFO Created variable conv5_3/biases:0 with shape (512,) and init <function _initializer at 0x7fcbb1961488>\nPool 5 layer ready\nLayer name: fc6\nLayer shape: [7, 7, 512, 4096]\n\nINFO:tensorflow:Created variable fc6/weights:0 with shape (7, 7, 512, 4096) and init <function _initializer at 0x7fcbb186b5f0>\n2016-06-30 14:15:54,435 INFO Created variable fc6/weights:0 with shape (7, 7, 512, 4096) and init <function _initializer at 0x7fcbb186b5f0>\n\nINFO:tensorflow:Created variable fc6/biases:0 with shape (4096,) and init <function _initializer at 0x7fcbb186b5f0>\n2016-06-30 14:15:54,438 INFO Created variable fc6/biases:0 with shape (4096,) and init <function _initializer at 0x7fcbb186b5f0>\nLayer name: fc7\nLayer shape: [1, 1, 4096, 4096]\n\nINFO:tensorflow:Created variable fc7/weights:0 with shape (1, 1, 4096, 4096) and init <function _initializer at 0x7fcbb189fed8>\n2016-06-30 14:15:54,492 INFO Created variable fc7/weights:0 with shape (1, 1, 4096, 4096) and init <function _initializer at 0x7fcbb189fed8>\n\nINFO:tensorflow:Created variable fc7/biases:0 with shape (4096,) and init <function _initializer at 0x7fcbb189fed8>\n2016-06-30 14:15:54,495 INFO Created variable fc7/biases:0 with shape (4096,) and init <function _initializer at 0x7fcbb189fed8>\n\nINFO:tensorflow:Created variable score_fr/weights:0 with shape (1, 1, 4096, 14) and init <function _initializer at 0x7fcbb186b5f0>\n2016-06-30 14:15:54,506 INFO Created variable score_fr/weights:0 with shape (1, 1, 4096, 14) and init <function _initializer at 0x7fcbb186b5f0>\n\nINFO:tensorflow:Created variable score_fr/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb186b5f0>\n2016-06-30 14:15:54,510 INFO Created variable score_fr/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb186b5f0>\n\nINFO:tensorflow:Created variable upscore2/up_filter:0 with shape (4, 4, 14, 14) and init <function _initializer at 0x7fcbb17a48c0>\n2016-06-30 14:15:54,525 INFO Created variable upscore2/up_filter:0 with shape (4, 4, 14, 14) and init <function _initializer at 0x7fcbb17a48c0>\n\nINFO:tensorflow:Created variable score_pool4/weights:0 with shape (1, 1, 512, 14) and init <function _initializer at 0x7fcbb17a48c0>\n2016-06-30 14:15:54,536 INFO Created variable score_pool4/weights:0 with shape (1, 1, 512, 14) and init <function _initializer at 0x7fcbb17a48c0>\n\nINFO:tensorflow:Created variable score_pool4/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb17a48c0>\n2016-06-30 14:15:54,540 INFO Created variable score_pool4/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb17a48c0>\n\nINFO:tensorflow:Created variable upscore4/up_filter:0 with shape (4, 4, 14, 14) and init <function _initializer at 0x7fcbb16c0aa0>\n2016-06-30 14:15:54,555 INFO Created variable upscore4/up_filter:0 with shape (4, 4, 14, 14) and init <function _initializer at 0x7fcbb16c0aa0>\n\nINFO:tensorflow:Created variable score_pool3/weights:0 with shape (1, 1, 256, 14) and init <function _initializer at 0x7fcbb16c0aa0>\n2016-06-30 14:15:54,566 INFO Created variable score_pool3/weights:0 with shape (1, 1, 256, 14) and init <function _initializer at 0x7fcbb16c0aa0>\n\nINFO:tensorflow:Created variable score_pool3/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb16c0aa0>\n2016-06-30 14:15:54,570 INFO Created variable score_pool3/biases:0 with shape (14,) and init <function _initializer at 0x7fcbb16c0aa0>\n\nINFO:tensorflow:Created variable upscore32/up_filter:0 with shape (16, 16, 14, 14) and init <function _initializer at 0x7fcbb15eb758>\n2016-06-30 14:15:54,585 INFO Created variable upscore32/up_filter:0 with shape (16, 16, 14, 14) and init <function _initializer at 0x7fcbb15eb758>`\n```\n\nBeen stuck on this for quite some time and cant figure out whats wrong... Any help will be appreciated :)\n"}