{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/340619673", "html_url": "https://github.com/tensorflow/tensorflow/issues/14075#issuecomment-340619673", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14075", "id": 340619673, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDYxOTY3Mw==", "user": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-30T23:57:53Z", "updated_at": "2017-10-30T23:57:53Z", "author_association": "MEMBER", "body_html": "<p>Oh, right, Adam's sparse updates are in terms of other ops rather than being fused. Looks like none of the other optimizers have GPU kernels implemented for the sparse updates (they do have GPU kernels for dense updates).</p>\n<p>One approach would be to re-implement <a href=\"https://github.com/tensorflow/tensorflow/blob/1f582aa3b24d59400492cc74876f0e075e7c106c/tensorflow/python/training/momentum.py#L97\">MomentumOptimizer's sparse updates</a> in terms of the <a href=\"https://github.com/tensorflow/tensorflow/blob/1f582aa3b24d59400492cc74876f0e075e7c106c/tensorflow/python/training/adam.py#L171\">same ops Adam is using</a> rather than using the custom fused kernel. May be difficult without also using Adam's dense update behavior (which is more correct, but would upset lots of people).</p>\n<p>Or it may be possible to implement the sparse fused GPU kernel efficiently using CUB.</p>\n<p>Or you could make a simple copy of MomentumOptimizer which always does dense updates, since those will run on the GPU and be faster in your case. This is more of a hack, but certainly contributions welcome for either of the other options.</p>", "body_text": "Oh, right, Adam's sparse updates are in terms of other ops rather than being fused. Looks like none of the other optimizers have GPU kernels implemented for the sparse updates (they do have GPU kernels for dense updates).\nOne approach would be to re-implement MomentumOptimizer's sparse updates in terms of the same ops Adam is using rather than using the custom fused kernel. May be difficult without also using Adam's dense update behavior (which is more correct, but would upset lots of people).\nOr it may be possible to implement the sparse fused GPU kernel efficiently using CUB.\nOr you could make a simple copy of MomentumOptimizer which always does dense updates, since those will run on the GPU and be faster in your case. This is more of a hack, but certainly contributions welcome for either of the other options.", "body": "Oh, right, Adam's sparse updates are in terms of other ops rather than being fused. Looks like none of the other optimizers have GPU kernels implemented for the sparse updates (they do have GPU kernels for dense updates).\r\n\r\nOne approach would be to re-implement [MomentumOptimizer's sparse updates](https://github.com/tensorflow/tensorflow/blob/1f582aa3b24d59400492cc74876f0e075e7c106c/tensorflow/python/training/momentum.py#L97) in terms of the [same ops Adam is using](https://github.com/tensorflow/tensorflow/blob/1f582aa3b24d59400492cc74876f0e075e7c106c/tensorflow/python/training/adam.py#L171) rather than using the custom fused kernel. May be difficult without also using Adam's dense update behavior (which is more correct, but would upset lots of people).\r\n\r\nOr it may be possible to implement the sparse fused GPU kernel efficiently using CUB.\r\n\r\nOr you could make a simple copy of MomentumOptimizer which always does dense updates, since those will run on the GPU and be faster in your case. This is more of a hack, but certainly contributions welcome for either of the other options."}