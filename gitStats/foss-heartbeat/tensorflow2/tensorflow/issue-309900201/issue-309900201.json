{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18103", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18103/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18103/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18103/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18103", "id": 309900201, "node_id": "MDU6SXNzdWUzMDk5MDAyMDE=", "number": 18103, "title": "Keras TimeDistributed wrapper around GlobalMaxPooling2D error with TPU", "user": {"login": "droidicus", "id": 6981482, "node_id": "MDQ6VXNlcjY5ODE0ODI=", "avatar_url": "https://avatars2.githubusercontent.com/u/6981482?v=4", "gravatar_id": "", "url": "https://api.github.com/users/droidicus", "html_url": "https://github.com/droidicus", "followers_url": "https://api.github.com/users/droidicus/followers", "following_url": "https://api.github.com/users/droidicus/following{/other_user}", "gists_url": "https://api.github.com/users/droidicus/gists{/gist_id}", "starred_url": "https://api.github.com/users/droidicus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/droidicus/subscriptions", "organizations_url": "https://api.github.com/users/droidicus/orgs", "repos_url": "https://api.github.com/users/droidicus/repos", "events_url": "https://api.github.com/users/droidicus/events{/privacy}", "received_events_url": "https://api.github.com/users/droidicus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-03-29T20:53:37Z", "updated_at": "2018-06-09T02:48:23Z", "closed_at": "2018-06-09T02:48:23Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux 4.9.0-6-amd64 <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a> SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.7.0-rc1</li>\n<li><strong>Python version</strong>: 2.7.13</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: Google Cloud TPU v2-8 running TF 1.7</li>\n<li><strong>Exact command to reproduce</strong>: See below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Using a Keras TimeDistributed wrapper to wrap a Keras GlobalMaxPooling2D layer, and processing on a Google Cloud TPU results in a <code>ValueError</code>. The layer behaves as expected if the TPUEstimator is configured to use CPU. Error raised by the TPU failure case:<br>\n<code>ValueError: Cannot create a gradient accumulator for tensor 'TPUReplicate/loop/time_distributed/while/Identity:0' inside XLA while_loop because maximum_iterations was not passed to the tf.while_loop call ('TPUReplicate/loop/time_distributed/while/while_context').</code></p>\n<h3>Source code / logs</h3>\n<p>Test code minimal example keras_td_test.py:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Test for Keras model TimeDistributed wrapper on TPU.</span>\n<span class=\"pl-s\">    Based on https://github.com/tensorflow/tpu/blob/master/models/official/resnet/</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n\n<span class=\"pl-k\">from</span> absl <span class=\"pl-k\">import</span> flags\n<span class=\"pl-k\">import</span> absl.logging <span class=\"pl-k\">as</span> _logging  <span class=\"pl-c\"><span class=\"pl-c\">#</span> pylint: disable=unused-import</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">from</span> tensorflow.contrib.tpu.python.tpu <span class=\"pl-k\">import</span> tpu_config\n<span class=\"pl-k\">from</span> tensorflow.contrib.tpu.python.tpu <span class=\"pl-k\">import</span> tpu_estimator\n<span class=\"pl-k\">from</span> tensorflow.contrib.tpu.python.tpu <span class=\"pl-k\">import</span> tpu_optimizer\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Define flags for the system</span>\n<span class=\"pl-c1\">FLAGS</span> <span class=\"pl-k\">=</span> flags.<span class=\"pl-c1\">FLAGS</span>\nflags.DEFINE_bool(\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>use_tpu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Use TPU to execute the model for training and evaluation. If<span class=\"pl-pds\">'</span></span>\n          <span class=\"pl-s\"><span class=\"pl-pds\">'</span> --use_tpu=false, will use whatever devices are available to<span class=\"pl-pds\">'</span></span>\n          <span class=\"pl-s\"><span class=\"pl-pds\">'</span> TensorFlow by default (e.g. CPU and GPU)<span class=\"pl-pds\">'</span></span>))\nflags.DEFINE_string(\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tpu_name<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Name of the Cloud TPU for Cluster Resolvers.<span class=\"pl-pds\">'</span></span>)\nflags.DEFINE_string(\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>model_dir<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n    <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>The directory where the model and training/evaluation summaries are<span class=\"pl-pds\">'</span></span>\n          <span class=\"pl-s\"><span class=\"pl-pds\">'</span> stored.<span class=\"pl-pds\">'</span></span>))\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">unused_argv</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get the TPU GRPC URL if it is needed</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.use_tpu:\n        tpu_cluster_resolver <span class=\"pl-k\">=</span> (\n            tf.contrib.cluster_resolver.TPUClusterResolver(\n                <span class=\"pl-c1\">FLAGS</span>.tpu_name))\n        tpu_grpc_url <span class=\"pl-k\">=</span> tpu_cluster_resolver.get_master()\n    <span class=\"pl-k\">else</span>:\n        tpu_grpc_url <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set the configuration for the TPU</span>\n    config <span class=\"pl-k\">=</span> tpu_config.RunConfig(\n        <span class=\"pl-v\">master</span><span class=\"pl-k\">=</span>tpu_grpc_url,\n        <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.model_dir)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the TPUEstimator</span>\n    test_estimator <span class=\"pl-k\">=</span> tpu_estimator.TPUEstimator(\n            <span class=\"pl-v\">use_tpu</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.use_tpu,\n            <span class=\"pl-v\">model_fn</span><span class=\"pl-k\">=</span>test_model_fn,\n            <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config,\n            <span class=\"pl-v\">train_batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1024</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Train the estimator for 10 steps</span>\n    test_estimator.train(input_fn, <span class=\"pl-v\">max_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">input_fn</span>(<span class=\"pl-smi\">params</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Generate a random dataset of the correct shape</span>\n    data <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>).astype(np.float32)\n    label <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">1024</span>,<span class=\"pl-c1\">10</span>).astype(np.float32)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Repeat and batch</span>\n    rand_dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices((data, label)).repeat()\n    rand_dataset <span class=\"pl-k\">=</span> rand_dataset.apply(tf.contrib.data.batch_and_drop_remainder(params[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_size<span class=\"pl-pds\">'</span></span>]))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Make input_fn for the TPUEstimator train step</span>\n    rand_dataset_fn <span class=\"pl-k\">=</span> rand_dataset.make_one_shot_iterator().get_next()\n    <span class=\"pl-k\">return</span> rand_dataset_fn\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_model_fn</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">mode</span>, <span class=\"pl-smi\">params</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Dense layer to give the system something to train</span>\n    dense_out <span class=\"pl-k\">=</span> tf.keras.layers.Dense(<span class=\"pl-c1\">10</span>)(features)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>#########################################</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> The Keras wrapper that causes an error #</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>#########################################</span>\n    predictions <span class=\"pl-k\">=</span> tf.keras.layers.TimeDistributed(\n        tf.keras.layers.GlobalMaxPooling2D()\n    )(dense_out)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create ops for the TPUEstimatorSpec</span>\n    loss <span class=\"pl-k\">=</span> tf.losses.mean_squared_error(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions)\n    optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.001</span>)\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.use_tpu:\n        optimizer <span class=\"pl-k\">=</span> tpu_optimizer.CrossShardOptimizer(optimizer)\n    global_step <span class=\"pl-k\">=</span> tf.train.get_global_step()\n    train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss, global_step)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Return the TPUEstimatorSpec</span>\n    <span class=\"pl-k\">return</span> tpu_estimator.TPUEstimatorSpec(\n        <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode,\n        <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss,\n        <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do the thing</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>main wrapper<span class=\"pl-pds\">'</span></span>)\n    tf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)\n    tf.app.run()</pre></div>\n<p>Error on TPU, and sanitized log file:</p>\n<pre><code>$ python keras_td_test.py --model_dir=gs://my_bucket/keras_td_test --tpu_name=tpu-name\nWARNING: Logging before flag parsing goes to stderr.\nW0329 19:53:32.386956 139824142264064 tf_logging.py:126] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the retry module or similar alternatives.\nmain wrapper\nW0329 19:53:32.582829 139824142264064 __init__.py:44] file_cache is unavailable when using oauth2client &gt;= 4.0.0\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n    from . import file_cache\n  File \"/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in &lt;module&gt;\n    'file_cache is unavailable when using oauth2client &gt;= 4.0.0')\nImportError: file_cache is unavailable when using oauth2client &gt;= 4.0.0\n2018-03-29 19:53:32.619736: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-03-29 19:53:32.622444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job local -&gt; {0 -&gt; localhost:44849}\n2018-03-29 19:53:32.623808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:44849\nW0329 19:53:32.704463 139824142264064 tf_logging.py:126] Estimator's model_fn (&lt;function test_model_fn at 0x7f2b1fc4d7d0&gt;) includes params argument, but params are not passed to Estimator.\nI0329 19:53:32.705055 139824142264064 tf_logging.py:116] Using config: {'_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, computation_shape=None, per_host_input_for_training=True, tpu_job_name=None, initial_infeed_sleep_secs=None), '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2b1fc4bc90&gt;, '_cluster': None, '_model_dir': 'gs://my_bucket/keras_td_test', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': u'grpc://10.240.1.2:8470', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': u'grpc://10.240.1.2:8470', '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\nI0329 19:53:32.772562 139824142264064 tf_logging.py:116] Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n2018-03-29 19:53:32.773276: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:351] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\nI0329 19:53:32.776926 139824142264064 tf_logging.py:116] Found TPU system:\nI0329 19:53:32.777101 139824142264064 tf_logging.py:116] *** Num TPU Cores: 8\nI0329 19:53:32.777353 139824142264064 tf_logging.py:116] *** Num TPU Workers: 1\nI0329 19:53:32.777436 139824142264064 tf_logging.py:116] *** Num TPU Cores Per Worker: 8\nI0329 19:53:32.777509 139824142264064 tf_logging.py:116] *** Available Devices: [_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184)]\nI0329 19:53:32.789855 139824142264064 tf_logging.py:116] Calling model_fn.\nTraceback (most recent call last):\n  File \"keras_td_test.py\", line 101, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\n    _sys.exit(main(argv))\n  File \"keras_td_test.py\", line 57, in main\n    test_estimator.train(input_fn, max_steps=10)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 824, in _train_model\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 805, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1827, in _model_fn\n    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2016, in _train_on_tpu_system\n    device_assignment=ctx.device_assignment)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 491, in shard\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 323, in replicate\n    outputs = computation(*computation_inputs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2009, in multi_tpu_train_steps_on_single_shard\n    name=b'loop')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 207, in repeat\n    cond, body_wrapper, inputs=inputs, infeed_queue=infeed_queue, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 169, in while_loop\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3202, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2940, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2877, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 120, in body_wrapper\n    outputs = body(*(inputs + dequeue_ops))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 203, in body_wrapper\n    return [i + 1] + _convert_to_list(body(*args))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1076, in train_step\n    self._call_model_fn(features, labels))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1230, in _call_model_fn\n    estimator_spec = self._model_fn(features=features, **kwargs)\n  File \"keras_td_test.py\", line 89, in test_model_fn\n    train_op = optimizer.minimize(loss, global_step)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 399, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py\", line 85, in compute_gradients\n    return self._opt.compute_gradients(loss, var_list=var_list, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 492, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 488, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 379, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in &lt;lambda&gt;\n    lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py\", line 131, in _TensorArrayWriteGrad\n    grad = g.read(index)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 861, in read\n    return self._implementation.read(index, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 260, in read\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6419, in tensor_array_read_v3\n    dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1696, in __init__\n    self._control_flow_post_processing()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1707, in _control_flow_post_processing\n    self._control_flow_context.AddOp(self)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2429, in AddOp\n    self._AddOpInternal(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2450, in _AddOpInternal\n    real_x = self.AddValue(x)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2382, in AddValue\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1152, in GetRealValue\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1017, in AddForwardAccumulator\n    value, self.forward_context)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 752, in GetMaxSizeFromNestedMaximumIterations\n    \"the tf.while_loop call ('%s').\" % (value_name, while_ctxt.name))\nValueError: Cannot create a gradient accumulator for tensor 'TPUReplicate/loop/time_distributed/while/Identity:0' inside XLA while_loop because maximum_iterations was not passed to the tf.while_loop call ('TPUReplicate/loop/time_distributed/while/while_context').\n</code></pre>\n<p>Working as expected on CPU:</p>\n<pre><code>$ python keras_td_test.py --model_dir=gs://my_bucket/keras_td_test --use_tpu=False\nWARNING: Logging before flag parsing goes to stderr.\nW0329 19:52:51.631102 140253725816576 tf_logging.py:126] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the retry module or similar alternatives.\nmain wrapper\nW0329 19:52:51.829307 140253725816576 tf_logging.py:126] Estimator's model_fn (&lt;function test_model_fn at 0x7f8f24f2c7d0&gt;) includes params argument, but params are not passed to Estimator.\nI0329 19:52:51.829844 140253725816576 tf_logging.py:116] Using config: {'_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, computation_shape=None, per_host_input_for_training=True, tpu_job_name=None, initial_infeed_sleep_secs=None), '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f24f2a550&gt;, '_cluster': None, '_model_dir': 'gs://my_bucket/keras_td_test', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\nI0329 19:52:51.967509 140253725816576 tf_logging.py:116] Calling model_fn.\nI0329 19:52:51.967808 140253725816576 tf_logging.py:116] Running train on CPU\nI0329 19:52:52.141927 140253725816576 tf_logging.py:116] Done calling model_fn.\nI0329 19:52:52.143158 140253725816576 tf_logging.py:116] Create CheckpointSaverHook.\nI0329 19:52:53.315506 140253725816576 tf_logging.py:116] Graph was finalized.\n2018-03-29 19:52:53.315908: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nI0329 19:52:53.452600 140253725816576 tf_logging.py:116] Running local_init_op.\nI0329 19:52:53.456309 140253725816576 tf_logging.py:116] Done running local_init_op.\nI0329 19:52:55.594362 140253725816576 tf_logging.py:116] Saving checkpoints for 1 into gs://my_bucket/keras_td_test/model.ckpt.\nI0329 19:52:58.770798 140253725816576 tf_logging.py:116] loss = 0.62052673, step = 0\nI0329 19:52:59.185467 140253725816576 tf_logging.py:116] Saving checkpoints for 10 into gs://my_bucket/keras_td_test/model.ckpt.\nI0329 19:53:03.196044 140253725816576 tf_logging.py:116] Loss for final step: 0.5509924.\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.7.0-rc1\nPython version: 2.7.13\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: Google Cloud TPU v2-8 running TF 1.7\nExact command to reproduce: See below\n\nDescribe the problem\nUsing a Keras TimeDistributed wrapper to wrap a Keras GlobalMaxPooling2D layer, and processing on a Google Cloud TPU results in a ValueError. The layer behaves as expected if the TPUEstimator is configured to use CPU. Error raised by the TPU failure case:\nValueError: Cannot create a gradient accumulator for tensor 'TPUReplicate/loop/time_distributed/while/Identity:0' inside XLA while_loop because maximum_iterations was not passed to the tf.while_loop call ('TPUReplicate/loop/time_distributed/while/while_context').\nSource code / logs\nTest code minimal example keras_td_test.py:\n\"\"\" Test for Keras model TimeDistributed wrapper on TPU.\n    Based on https://github.com/tensorflow/tpu/blob/master/models/official/resnet/\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport absl.logging as _logging  # pylint: disable=unused-import\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.contrib.tpu.python.tpu import tpu_config\nfrom tensorflow.contrib.tpu.python.tpu import tpu_estimator\nfrom tensorflow.contrib.tpu.python.tpu import tpu_optimizer\n\n# Define flags for the system\nFLAGS = flags.FLAGS\nflags.DEFINE_bool(\n    'use_tpu', True,\n    help=('Use TPU to execute the model for training and evaluation. If'\n          ' --use_tpu=false, will use whatever devices are available to'\n          ' TensorFlow by default (e.g. CPU and GPU)'))\nflags.DEFINE_string(\n    'tpu_name', default=None,\n    help='Name of the Cloud TPU for Cluster Resolvers.')\nflags.DEFINE_string(\n    'model_dir', default=None,\n    help=('The directory where the model and training/evaluation summaries are'\n          ' stored.'))\n\ndef main(unused_argv):\n    # Get the TPU GRPC URL if it is needed\n    if FLAGS.use_tpu:\n        tpu_cluster_resolver = (\n            tf.contrib.cluster_resolver.TPUClusterResolver(\n                FLAGS.tpu_name))\n        tpu_grpc_url = tpu_cluster_resolver.get_master()\n    else:\n        tpu_grpc_url = None\n\n    # Set the configuration for the TPU\n    config = tpu_config.RunConfig(\n        master=tpu_grpc_url,\n        model_dir=FLAGS.model_dir)\n\n    # Create the TPUEstimator\n    test_estimator = tpu_estimator.TPUEstimator(\n            use_tpu=FLAGS.use_tpu,\n            model_fn=test_model_fn,\n            config=config,\n            train_batch_size=1024)\n\n    # Train the estimator for 10 steps\n    test_estimator.train(input_fn, max_steps=10)\n\ndef input_fn(params):\n    # Generate a random dataset of the correct shape\n    data = np.random.rand(1024, 10, 10, 10).astype(np.float32)\n    label = np.random.rand(1024,10).astype(np.float32)\n\n    # Repeat and batch\n    rand_dataset = tf.data.Dataset.from_tensor_slices((data, label)).repeat()\n    rand_dataset = rand_dataset.apply(tf.contrib.data.batch_and_drop_remainder(params['batch_size']))\n\n    # Make input_fn for the TPUEstimator train step\n    rand_dataset_fn = rand_dataset.make_one_shot_iterator().get_next()\n    return rand_dataset_fn\n\ndef test_model_fn(features, labels, mode, params):\n    # Dense layer to give the system something to train\n    dense_out = tf.keras.layers.Dense(10)(features)\n\n    ##########################################\n    # The Keras wrapper that causes an error #\n    ##########################################\n    predictions = tf.keras.layers.TimeDistributed(\n        tf.keras.layers.GlobalMaxPooling2D()\n    )(dense_out)\n\n    # Create ops for the TPUEstimatorSpec\n    loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n    optimizer = tf.train.GradientDescentOptimizer(0.001)\n    if FLAGS.use_tpu:\n        optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)\n    global_step = tf.train.get_global_step()\n    train_op = optimizer.minimize(loss, global_step)\n\n    # Return the TPUEstimatorSpec\n    return tpu_estimator.TPUEstimatorSpec(\n        mode=mode,\n        loss=loss,\n        train_op=train_op)\n\nif __name__ == '__main__':\n    # Do the thing\n    print('main wrapper')\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\nError on TPU, and sanitized log file:\n$ python keras_td_test.py --model_dir=gs://my_bucket/keras_td_test --tpu_name=tpu-name\nWARNING: Logging before flag parsing goes to stderr.\nW0329 19:53:32.386956 139824142264064 tf_logging.py:126] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the retry module or similar alternatives.\nmain wrapper\nW0329 19:53:32.582829 139824142264064 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n    from . import file_cache\n  File \"/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n    'file_cache is unavailable when using oauth2client >= 4.0.0')\nImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n2018-03-29 19:53:32.619736: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-03-29 19:53:32.622444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job local -> {0 -> localhost:44849}\n2018-03-29 19:53:32.623808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:44849\nW0329 19:53:32.704463 139824142264064 tf_logging.py:126] Estimator's model_fn (<function test_model_fn at 0x7f2b1fc4d7d0>) includes params argument, but params are not passed to Estimator.\nI0329 19:53:32.705055 139824142264064 tf_logging.py:116] Using config: {'_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, computation_shape=None, per_host_input_for_training=True, tpu_job_name=None, initial_infeed_sleep_secs=None), '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2b1fc4bc90>, '_cluster': None, '_model_dir': 'gs://my_bucket/keras_td_test', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': u'grpc://10.240.1.2:8470', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': u'grpc://10.240.1.2:8470', '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\nI0329 19:53:32.772562 139824142264064 tf_logging.py:116] Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n2018-03-29 19:53:32.773276: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:351] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\nI0329 19:53:32.776926 139824142264064 tf_logging.py:116] Found TPU system:\nI0329 19:53:32.777101 139824142264064 tf_logging.py:116] *** Num TPU Cores: 8\nI0329 19:53:32.777353 139824142264064 tf_logging.py:116] *** Num TPU Workers: 1\nI0329 19:53:32.777436 139824142264064 tf_logging.py:116] *** Num TPU Cores Per Worker: 8\nI0329 19:53:32.777509 139824142264064 tf_logging.py:116] *** Available Devices: [_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184)]\nI0329 19:53:32.789855 139824142264064 tf_logging.py:116] Calling model_fn.\nTraceback (most recent call last):\n  File \"keras_td_test.py\", line 101, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\n    _sys.exit(main(argv))\n  File \"keras_td_test.py\", line 57, in main\n    test_estimator.train(input_fn, max_steps=10)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 824, in _train_model\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 805, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1827, in _model_fn\n    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2016, in _train_on_tpu_system\n    device_assignment=ctx.device_assignment)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 491, in shard\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 323, in replicate\n    outputs = computation(*computation_inputs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2009, in multi_tpu_train_steps_on_single_shard\n    name=b'loop')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 207, in repeat\n    cond, body_wrapper, inputs=inputs, infeed_queue=infeed_queue, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 169, in while_loop\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3202, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2940, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2877, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 120, in body_wrapper\n    outputs = body(*(inputs + dequeue_ops))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 203, in body_wrapper\n    return [i + 1] + _convert_to_list(body(*args))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1076, in train_step\n    self._call_model_fn(features, labels))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1230, in _call_model_fn\n    estimator_spec = self._model_fn(features=features, **kwargs)\n  File \"keras_td_test.py\", line 89, in test_model_fn\n    train_op = optimizer.minimize(loss, global_step)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 399, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py\", line 85, in compute_gradients\n    return self._opt.compute_gradients(loss, var_list=var_list, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 492, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 488, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 379, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py\", line 131, in _TensorArrayWriteGrad\n    grad = g.read(index)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 861, in read\n    return self._implementation.read(index, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 260, in read\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6419, in tensor_array_read_v3\n    dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1696, in __init__\n    self._control_flow_post_processing()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1707, in _control_flow_post_processing\n    self._control_flow_context.AddOp(self)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2429, in AddOp\n    self._AddOpInternal(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2450, in _AddOpInternal\n    real_x = self.AddValue(x)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2382, in AddValue\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1152, in GetRealValue\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1017, in AddForwardAccumulator\n    value, self.forward_context)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 752, in GetMaxSizeFromNestedMaximumIterations\n    \"the tf.while_loop call ('%s').\" % (value_name, while_ctxt.name))\nValueError: Cannot create a gradient accumulator for tensor 'TPUReplicate/loop/time_distributed/while/Identity:0' inside XLA while_loop because maximum_iterations was not passed to the tf.while_loop call ('TPUReplicate/loop/time_distributed/while/while_context').\n\nWorking as expected on CPU:\n$ python keras_td_test.py --model_dir=gs://my_bucket/keras_td_test --use_tpu=False\nWARNING: Logging before flag parsing goes to stderr.\nW0329 19:52:51.631102 140253725816576 tf_logging.py:126] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the retry module or similar alternatives.\nmain wrapper\nW0329 19:52:51.829307 140253725816576 tf_logging.py:126] Estimator's model_fn (<function test_model_fn at 0x7f8f24f2c7d0>) includes params argument, but params are not passed to Estimator.\nI0329 19:52:51.829844 140253725816576 tf_logging.py:116] Using config: {'_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, computation_shape=None, per_host_input_for_training=True, tpu_job_name=None, initial_infeed_sleep_secs=None), '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f24f2a550>, '_cluster': None, '_model_dir': 'gs://my_bucket/keras_td_test', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\nI0329 19:52:51.967509 140253725816576 tf_logging.py:116] Calling model_fn.\nI0329 19:52:51.967808 140253725816576 tf_logging.py:116] Running train on CPU\nI0329 19:52:52.141927 140253725816576 tf_logging.py:116] Done calling model_fn.\nI0329 19:52:52.143158 140253725816576 tf_logging.py:116] Create CheckpointSaverHook.\nI0329 19:52:53.315506 140253725816576 tf_logging.py:116] Graph was finalized.\n2018-03-29 19:52:53.315908: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nI0329 19:52:53.452600 140253725816576 tf_logging.py:116] Running local_init_op.\nI0329 19:52:53.456309 140253725816576 tf_logging.py:116] Done running local_init_op.\nI0329 19:52:55.594362 140253725816576 tf_logging.py:116] Saving checkpoints for 1 into gs://my_bucket/keras_td_test/model.ckpt.\nI0329 19:52:58.770798 140253725816576 tf_logging.py:116] loss = 0.62052673, step = 0\nI0329 19:52:59.185467 140253725816576 tf_logging.py:116] Saving checkpoints for 10 into gs://my_bucket/keras_td_test/model.ckpt.\nI0329 19:53:03.196044 140253725816576 tf_logging.py:116] Loss for final step: 0.5509924.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 4.9.0-6-amd64 #1 SMP Debian 4.9.82-1+deb9u3 (2018-03-02) x86_64 GNU/Linux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0-rc1\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: Google Cloud TPU v2-8 running TF 1.7\r\n- **Exact command to reproduce**: See below\r\n\r\n\r\n### Describe the problem\r\nUsing a Keras TimeDistributed wrapper to wrap a Keras GlobalMaxPooling2D layer, and processing on a Google Cloud TPU results in a ```ValueError```. The layer behaves as expected if the TPUEstimator is configured to use CPU. Error raised by the TPU failure case:\r\n```ValueError: Cannot create a gradient accumulator for tensor 'TPUReplicate/loop/time_distributed/while/Identity:0' inside XLA while_loop because maximum_iterations was not passed to the tf.while_loop call ('TPUReplicate/loop/time_distributed/while/while_context').```\r\n\r\n### Source code / logs\r\nTest code minimal example keras_td_test.py:\r\n```python\r\n\"\"\" Test for Keras model TimeDistributed wrapper on TPU.\r\n    Based on https://github.com/tensorflow/tpu/blob/master/models/official/resnet/\r\n\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom absl import flags\r\nimport absl.logging as _logging  # pylint: disable=unused-import\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.tpu.python.tpu import tpu_config\r\nfrom tensorflow.contrib.tpu.python.tpu import tpu_estimator\r\nfrom tensorflow.contrib.tpu.python.tpu import tpu_optimizer\r\n\r\n# Define flags for the system\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_bool(\r\n    'use_tpu', True,\r\n    help=('Use TPU to execute the model for training and evaluation. If'\r\n          ' --use_tpu=false, will use whatever devices are available to'\r\n          ' TensorFlow by default (e.g. CPU and GPU)'))\r\nflags.DEFINE_string(\r\n    'tpu_name', default=None,\r\n    help='Name of the Cloud TPU for Cluster Resolvers.')\r\nflags.DEFINE_string(\r\n    'model_dir', default=None,\r\n    help=('The directory where the model and training/evaluation summaries are'\r\n          ' stored.'))\r\n\r\ndef main(unused_argv):\r\n    # Get the TPU GRPC URL if it is needed\r\n    if FLAGS.use_tpu:\r\n        tpu_cluster_resolver = (\r\n            tf.contrib.cluster_resolver.TPUClusterResolver(\r\n                FLAGS.tpu_name))\r\n        tpu_grpc_url = tpu_cluster_resolver.get_master()\r\n    else:\r\n        tpu_grpc_url = None\r\n\r\n    # Set the configuration for the TPU\r\n    config = tpu_config.RunConfig(\r\n        master=tpu_grpc_url,\r\n        model_dir=FLAGS.model_dir)\r\n\r\n    # Create the TPUEstimator\r\n    test_estimator = tpu_estimator.TPUEstimator(\r\n            use_tpu=FLAGS.use_tpu,\r\n            model_fn=test_model_fn,\r\n            config=config,\r\n            train_batch_size=1024)\r\n\r\n    # Train the estimator for 10 steps\r\n    test_estimator.train(input_fn, max_steps=10)\r\n\r\ndef input_fn(params):\r\n    # Generate a random dataset of the correct shape\r\n    data = np.random.rand(1024, 10, 10, 10).astype(np.float32)\r\n    label = np.random.rand(1024,10).astype(np.float32)\r\n\r\n    # Repeat and batch\r\n    rand_dataset = tf.data.Dataset.from_tensor_slices((data, label)).repeat()\r\n    rand_dataset = rand_dataset.apply(tf.contrib.data.batch_and_drop_remainder(params['batch_size']))\r\n\r\n    # Make input_fn for the TPUEstimator train step\r\n    rand_dataset_fn = rand_dataset.make_one_shot_iterator().get_next()\r\n    return rand_dataset_fn\r\n\r\ndef test_model_fn(features, labels, mode, params):\r\n    # Dense layer to give the system something to train\r\n    dense_out = tf.keras.layers.Dense(10)(features)\r\n\r\n    ##########################################\r\n    # The Keras wrapper that causes an error #\r\n    ##########################################\r\n    predictions = tf.keras.layers.TimeDistributed(\r\n        tf.keras.layers.GlobalMaxPooling2D()\r\n    )(dense_out)\r\n\r\n    # Create ops for the TPUEstimatorSpec\r\n    loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\r\n    optimizer = tf.train.GradientDescentOptimizer(0.001)\r\n    if FLAGS.use_tpu:\r\n        optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)\r\n    global_step = tf.train.get_global_step()\r\n    train_op = optimizer.minimize(loss, global_step)\r\n\r\n    # Return the TPUEstimatorSpec\r\n    return tpu_estimator.TPUEstimatorSpec(\r\n        mode=mode,\r\n        loss=loss,\r\n        train_op=train_op)\r\n\r\nif __name__ == '__main__':\r\n    # Do the thing\r\n    print('main wrapper')\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run()\r\n```\r\n\r\nError on TPU, and sanitized log file:\r\n```\r\n$ python keras_td_test.py --model_dir=gs://my_bucket/keras_td_test --tpu_name=tpu-name\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0329 19:53:32.386956 139824142264064 tf_logging.py:126] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\nmain wrapper\r\nW0329 19:53:32.582829 139824142264064 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\r\n    from . import file_cache\r\n  File \"/usr/local/lib/python2.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\r\n    'file_cache is unavailable when using oauth2client >= 4.0.0')\r\nImportError: file_cache is unavailable when using oauth2client >= 4.0.0\r\n2018-03-29 19:53:32.619736: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-03-29 19:53:32.622444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job local -> {0 -> localhost:44849}\r\n2018-03-29 19:53:32.623808: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:44849\r\nW0329 19:53:32.704463 139824142264064 tf_logging.py:126] Estimator's model_fn (<function test_model_fn at 0x7f2b1fc4d7d0>) includes params argument, but params are not passed to Estimator.\r\nI0329 19:53:32.705055 139824142264064 tf_logging.py:116] Using config: {'_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, computation_shape=None, per_host_input_for_training=True, tpu_job_name=None, initial_infeed_sleep_secs=None), '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2b1fc4bc90>, '_cluster': None, '_model_dir': 'gs://my_bucket/keras_td_test', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': u'grpc://10.240.1.2:8470', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': u'grpc://10.240.1.2:8470', '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\r\nI0329 19:53:32.772562 139824142264064 tf_logging.py:116] Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\r\n2018-03-29 19:53:32.773276: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:351] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\r\nI0329 19:53:32.776926 139824142264064 tf_logging.py:116] Found TPU system:\r\nI0329 19:53:32.777101 139824142264064 tf_logging.py:116] *** Num TPU Cores: 8\r\nI0329 19:53:32.777353 139824142264064 tf_logging.py:116] *** Num TPU Workers: 1\r\nI0329 19:53:32.777436 139824142264064 tf_logging.py:116] *** Num TPU Cores Per Worker: 8\r\nI0329 19:53:32.777509 139824142264064 tf_logging.py:116] *** Available Devices: [_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184)]\r\nI0329 19:53:32.789855 139824142264064 tf_logging.py:116] Calling model_fn.\r\nTraceback (most recent call last):\r\n  File \"keras_td_test.py\", line 101, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"keras_td_test.py\", line 57, in main\r\n    test_estimator.train(input_fn, max_steps=10)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 824, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 805, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1827, in _model_fn\r\n    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2016, in _train_on_tpu_system\r\n    device_assignment=ctx.device_assignment)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 491, in shard\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 323, in replicate\r\n    outputs = computation(*computation_inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2009, in multi_tpu_train_steps_on_single_shard\r\n    name=b'loop')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 207, in repeat\r\n    cond, body_wrapper, inputs=inputs, infeed_queue=infeed_queue, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 169, in while_loop\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3202, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2940, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2877, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 120, in body_wrapper\r\n    outputs = body(*(inputs + dequeue_ops))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/training_loop.py\", line 203, in body_wrapper\r\n    return [i + 1] + _convert_to_list(body(*args))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1076, in train_step\r\n    self._call_model_fn(features, labels))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1230, in _call_model_fn\r\n    estimator_spec = self._model_fn(features=features, **kwargs)\r\n  File \"keras_td_test.py\", line 89, in test_model_fn\r\n    train_op = optimizer.minimize(loss, global_step)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 399, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_optimizer.py\", line 85, in compute_gradients\r\n    return self._opt.compute_gradients(loss, var_list=var_list, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 492, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 488, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 379, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 625, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py\", line 131, in _TensorArrayWriteGrad\r\n    grad = g.read(index)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 861, in read\r\n    return self._implementation.read(index, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 260, in read\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6419, in tensor_array_read_v3\r\n    dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1696, in __init__\r\n    self._control_flow_post_processing()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1707, in _control_flow_post_processing\r\n    self._control_flow_context.AddOp(self)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2429, in AddOp\r\n    self._AddOpInternal(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2450, in _AddOpInternal\r\n    real_x = self.AddValue(x)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2382, in AddValue\r\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1152, in GetRealValue\r\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1017, in AddForwardAccumulator\r\n    value, self.forward_context)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 752, in GetMaxSizeFromNestedMaximumIterations\r\n    \"the tf.while_loop call ('%s').\" % (value_name, while_ctxt.name))\r\nValueError: Cannot create a gradient accumulator for tensor 'TPUReplicate/loop/time_distributed/while/Identity:0' inside XLA while_loop because maximum_iterations was not passed to the tf.while_loop call ('TPUReplicate/loop/time_distributed/while/while_context').\r\n```\r\n\r\nWorking as expected on CPU:\r\n```\r\n$ python keras_td_test.py --model_dir=gs://my_bucket/keras_td_test --use_tpu=False\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0329 19:52:51.631102 140253725816576 tf_logging.py:126] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\nmain wrapper\r\nW0329 19:52:51.829307 140253725816576 tf_logging.py:126] Estimator's model_fn (<function test_model_fn at 0x7f8f24f2c7d0>) includes params argument, but params are not passed to Estimator.\r\nI0329 19:52:51.829844 140253725816576 tf_logging.py:116] Using config: {'_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, computation_shape=None, per_host_input_for_training=True, tpu_job_name=None, initial_infeed_sleep_secs=None), '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f24f2a550>, '_cluster': None, '_model_dir': 'gs://my_bucket/keras_td_test', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\r\nI0329 19:52:51.967509 140253725816576 tf_logging.py:116] Calling model_fn.\r\nI0329 19:52:51.967808 140253725816576 tf_logging.py:116] Running train on CPU\r\nI0329 19:52:52.141927 140253725816576 tf_logging.py:116] Done calling model_fn.\r\nI0329 19:52:52.143158 140253725816576 tf_logging.py:116] Create CheckpointSaverHook.\r\nI0329 19:52:53.315506 140253725816576 tf_logging.py:116] Graph was finalized.\r\n2018-03-29 19:52:53.315908: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nI0329 19:52:53.452600 140253725816576 tf_logging.py:116] Running local_init_op.\r\nI0329 19:52:53.456309 140253725816576 tf_logging.py:116] Done running local_init_op.\r\nI0329 19:52:55.594362 140253725816576 tf_logging.py:116] Saving checkpoints for 1 into gs://my_bucket/keras_td_test/model.ckpt.\r\nI0329 19:52:58.770798 140253725816576 tf_logging.py:116] loss = 0.62052673, step = 0\r\nI0329 19:52:59.185467 140253725816576 tf_logging.py:116] Saving checkpoints for 10 into gs://my_bucket/keras_td_test/model.ckpt.\r\nI0329 19:53:03.196044 140253725816576 tf_logging.py:116] Loss for final step: 0.5509924.\r\n```"}