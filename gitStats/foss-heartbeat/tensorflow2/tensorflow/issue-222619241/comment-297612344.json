{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/297612344", "html_url": "https://github.com/tensorflow/tensorflow/issues/9301#issuecomment-297612344", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9301", "id": 297612344, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NzYxMjM0NA==", "user": {"login": "kwotsin", "id": 11178344, "node_id": "MDQ6VXNlcjExMTc4MzQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/11178344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwotsin", "html_url": "https://github.com/kwotsin", "followers_url": "https://api.github.com/users/kwotsin/followers", "following_url": "https://api.github.com/users/kwotsin/following{/other_user}", "gists_url": "https://api.github.com/users/kwotsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwotsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwotsin/subscriptions", "organizations_url": "https://api.github.com/users/kwotsin/orgs", "repos_url": "https://api.github.com/users/kwotsin/repos", "events_url": "https://api.github.com/users/kwotsin/events{/privacy}", "received_events_url": "https://api.github.com/users/kwotsin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-27T05:01:26Z", "updated_at": "2017-04-27T05:44:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=25754898\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrehentz\">@andrehentz</a> I have tried building TF from source but with not very much success. The furthest version of TF I've gotten to is the one available here: <a href=\"https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack\">https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack</a>. Would this be a cause of the issue?</p>\n<p>However, for the quantization, I run it on my host machine. Regarding the <code>android:libtensorflow</code>_inference file you mentioned to build, do I run the graph_transform tool or quantize_graph tool after building that with bazel? I have tried to use <code>graph_transform</code> and <code>quantize_graph</code> both after building the tensorflow inference on my host machine (is this correct), but they gave me the same results as before.</p>\n<p>On the other hand, on my fine-tuned model for predicting flowers, the model actually predicted very well (nearly identical accuracy at 99% for both quantized and frozen models) for <strong>some images</strong>. Upon testing on images that are slightly larger, the quantized version is no better than a random guess. Could this be attributed to the accuracy of quantization? In any case, the frozen model seems to be much more robust (correct predictions for everything).</p>\n<p>Regarding the speed: I find that quantization could speed up my predictions by at least 30%, and the fastest time it took for some runs was half that required of the frozen models. I am very much looking forward to using quantization if the robustness issue can be solved. Meanwhile, thank you for helping me in this matter! I look forward to your fix! :D</p>", "body_text": "@andrehentz I have tried building TF from source but with not very much success. The furthest version of TF I've gotten to is the one available here: https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack. Would this be a cause of the issue?\nHowever, for the quantization, I run it on my host machine. Regarding the android:libtensorflow_inference file you mentioned to build, do I run the graph_transform tool or quantize_graph tool after building that with bazel? I have tried to use graph_transform and quantize_graph both after building the tensorflow inference on my host machine (is this correct), but they gave me the same results as before.\nOn the other hand, on my fine-tuned model for predicting flowers, the model actually predicted very well (nearly identical accuracy at 99% for both quantized and frozen models) for some images. Upon testing on images that are slightly larger, the quantized version is no better than a random guess. Could this be attributed to the accuracy of quantization? In any case, the frozen model seems to be much more robust (correct predictions for everything).\nRegarding the speed: I find that quantization could speed up my predictions by at least 30%, and the fastest time it took for some runs was half that required of the frozen models. I am very much looking forward to using quantization if the robustness issue can be solved. Meanwhile, thank you for helping me in this matter! I look forward to your fix! :D", "body": "@andrehentz I have tried building TF from source but with not very much success. The furthest version of TF I've gotten to is the one available here: https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack. Would this be a cause of the issue? \r\n\r\nHowever, for the quantization, I run it on my host machine. Regarding the `android:libtensorflow`_inference file you mentioned to build, do I run the graph_transform tool or quantize_graph tool after building that with bazel? I have tried to use `graph_transform` and `quantize_graph` both after building the tensorflow inference on my host machine (is this correct), but they gave me the same results as before.\r\n\r\nOn the other hand, on my fine-tuned model for predicting flowers, the model actually predicted very well (nearly identical accuracy at 99% for both quantized and frozen models) for **some images**. Upon testing on images that are slightly larger, the quantized version is no better than a random guess. Could this be attributed to the accuracy of quantization? In any case, the frozen model seems to be much more robust (correct predictions for everything).\r\n\r\nRegarding the speed: I find that quantization could speed up my predictions by at least 30%, and the fastest time it took for some runs was half that required of the frozen models. I am very much looking forward to using quantization if the robustness issue can be solved. Meanwhile, thank you for helping me in this matter! I look forward to your fix! :D "}