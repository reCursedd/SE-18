{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/295565578", "html_url": "https://github.com/tensorflow/tensorflow/issues/9301#issuecomment-295565578", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9301", "id": 295565578, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTU2NTU3OA==", "user": {"login": "kwotsin", "id": 11178344, "node_id": "MDQ6VXNlcjExMTc4MzQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/11178344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwotsin", "html_url": "https://github.com/kwotsin", "followers_url": "https://api.github.com/users/kwotsin/followers", "following_url": "https://api.github.com/users/kwotsin/following{/other_user}", "gists_url": "https://api.github.com/users/kwotsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwotsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwotsin/subscriptions", "organizations_url": "https://api.github.com/users/kwotsin/orgs", "repos_url": "https://api.github.com/users/kwotsin/repos", "events_url": "https://api.github.com/users/kwotsin/events{/privacy}", "received_events_url": "https://api.github.com/users/kwotsin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-20T03:27:07Z", "updated_at": "2017-04-20T08:57:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for your reply! Do you have an example of how this could be done? I have always thought there was only one quantization function available (for the weights). What is the difference between quantizing the weights vs the nodes?</p>\n<p>The current code I'm using for the quantization is this:</p>\n<pre><code>/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph \\\n--input=./frozen_model_inception_v3.pb \\\n--output_node_names=\"InceptionV3/Predictions/Softmax\" \\\n--output=./quantized_graph_inception_v3.pb \\\n--mode=eightbit\n\n</code></pre>\n<p>EDIT: I found out the quantization of the nodes come from the graph_transform tool, and I tried to quantize an inception v4 model both weights and the nodes. However, the quantization of the node give me an <code>IndexError</code>. This is the error stack:</p>\n<pre><code>Traceback (most recent call last):\n  File \"evaluate_IR2_pb.py\", line 32, in &lt;module&gt;\n    tf.import_graph_def(graph_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 404, in import_graph_def\n    ops.set_shapes_for_outputs(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1719, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1669, in call_with_requiring\n    return call_cpp_shape_fn(op, require_shape_fn=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\n    debug_python_shape_fn, require_shape_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 653, in _call_cpp_shape_fn_impl\n    v = tensor_util.constant_value(op.inputs[idx])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 703, in constant_value\n    ret = _ConstantValue(tensor)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 651, in _ConstantValue\n    dim = constant_value(tensor.op.inputs[-1])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1439, in __getitem__\n    return self._op._inputs[i]\nIndexError: list index out of range\n</code></pre>\n<p>Here is the bazel command for quantizing weights and nodes:</p>\n<pre><code>/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=./frozen_model_inception_resnet_v2.pb \\\n--out_graph=./quantized_weights_and_nodes_inception_resnet_v2.pb \\\n--inputs='Placeholder_only:0' \\\n--outputs='InceptionResnetV2/Logits/Predictions:0' \\\n--transforms='quantize_weights quantize_nodes'\n</code></pre>\n<p>Once I removed <code>quantize_nodes</code>, the error is gone. But I am left with a quantization (<code>quantize_weights</code> only) that gives me the 100% inaccuracy I saw on the Jetson. On the other hand, using the quantization method (also weights only) I first used still gave me a high accuracy I expected.</p>\n<p>However, when I tried this on the inception_v3 model, the quantization of weights and nodes work as expected, but not on the Jetson, which gives me a <code>ValueError: No op named QuantizedMul in defined operations\". Upon removing </code>quantize_nodes<code>and using the</code>quantize_weights` only, the model runs, but again, it gives me the highly inaccurate answers.</p>\n<p>My conclusion is that <code>quantize_nodes</code> isn't stable for the Jetson TX1, but on the other hand, <code>quantize_weights</code> (from the graph_transform tool) is highly inaccurate. Note that the above predictions are all on the custom imagenet model.</p>\n<p>Discounting the use of the tools from <code>graph_transform</code> and following what I did initially with the quantization, both the quantization (not from <code>graph_transform</code> tool) and freezing graph (by manually exporting the graph after restoring from a checkpoint) works very well for another non-imagenet problem. The model used for this problem came from fine-tuning the imagenet problem. This is a very strange behavior.</p>\n<p>So from what I can conclude, I think it is that quantization tools from <code>transform_graphs</code> is rather buggy, and for the quantization from <code>quantize_graph</code>, it works quite well but one can get very bad results on a Jetson TX probably due to how the architecture handles the calculations (This is just my conjecture). The calculations could be done differently in the Jetson because of the quantization to 8 bits, but I am not experienced with the Jetson so I can't say anything for sure. I have kept the imagenet prediction model and the fine-tuned model codes almost exactly the same, and the only things differing between them are the checkpoint files and the number of classes to predict in the end(1001 for the imagenet, and 5 for the fine-tuned one). Could this have caused the error?</p>\n<p>Is there a way to get consistent results from quantization (especially on a Jetson)?</p>", "body_text": "Thanks for your reply! Do you have an example of how this could be done? I have always thought there was only one quantization function available (for the weights). What is the difference between quantizing the weights vs the nodes?\nThe current code I'm using for the quantization is this:\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph \\\n--input=./frozen_model_inception_v3.pb \\\n--output_node_names=\"InceptionV3/Predictions/Softmax\" \\\n--output=./quantized_graph_inception_v3.pb \\\n--mode=eightbit\n\n\nEDIT: I found out the quantization of the nodes come from the graph_transform tool, and I tried to quantize an inception v4 model both weights and the nodes. However, the quantization of the node give me an IndexError. This is the error stack:\nTraceback (most recent call last):\n  File \"evaluate_IR2_pb.py\", line 32, in <module>\n    tf.import_graph_def(graph_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 404, in import_graph_def\n    ops.set_shapes_for_outputs(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1719, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1669, in call_with_requiring\n    return call_cpp_shape_fn(op, require_shape_fn=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\n    debug_python_shape_fn, require_shape_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 653, in _call_cpp_shape_fn_impl\n    v = tensor_util.constant_value(op.inputs[idx])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 703, in constant_value\n    ret = _ConstantValue(tensor)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 651, in _ConstantValue\n    dim = constant_value(tensor.op.inputs[-1])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1439, in __getitem__\n    return self._op._inputs[i]\nIndexError: list index out of range\n\nHere is the bazel command for quantizing weights and nodes:\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=./frozen_model_inception_resnet_v2.pb \\\n--out_graph=./quantized_weights_and_nodes_inception_resnet_v2.pb \\\n--inputs='Placeholder_only:0' \\\n--outputs='InceptionResnetV2/Logits/Predictions:0' \\\n--transforms='quantize_weights quantize_nodes'\n\nOnce I removed quantize_nodes, the error is gone. But I am left with a quantization (quantize_weights only) that gives me the 100% inaccuracy I saw on the Jetson. On the other hand, using the quantization method (also weights only) I first used still gave me a high accuracy I expected.\nHowever, when I tried this on the inception_v3 model, the quantization of weights and nodes work as expected, but not on the Jetson, which gives me a ValueError: No op named QuantizedMul in defined operations\". Upon removing quantize_nodesand using thequantize_weights` only, the model runs, but again, it gives me the highly inaccurate answers.\nMy conclusion is that quantize_nodes isn't stable for the Jetson TX1, but on the other hand, quantize_weights (from the graph_transform tool) is highly inaccurate. Note that the above predictions are all on the custom imagenet model.\nDiscounting the use of the tools from graph_transform and following what I did initially with the quantization, both the quantization (not from graph_transform tool) and freezing graph (by manually exporting the graph after restoring from a checkpoint) works very well for another non-imagenet problem. The model used for this problem came from fine-tuning the imagenet problem. This is a very strange behavior.\nSo from what I can conclude, I think it is that quantization tools from transform_graphs is rather buggy, and for the quantization from quantize_graph, it works quite well but one can get very bad results on a Jetson TX probably due to how the architecture handles the calculations (This is just my conjecture). The calculations could be done differently in the Jetson because of the quantization to 8 bits, but I am not experienced with the Jetson so I can't say anything for sure. I have kept the imagenet prediction model and the fine-tuned model codes almost exactly the same, and the only things differing between them are the checkpoint files and the number of classes to predict in the end(1001 for the imagenet, and 5 for the fine-tuned one). Could this have caused the error?\nIs there a way to get consistent results from quantization (especially on a Jetson)?", "body": "Thanks for your reply! Do you have an example of how this could be done? I have always thought there was only one quantization function available (for the weights). What is the difference between quantizing the weights vs the nodes?\r\n\r\nThe current code I'm using for the quantization is this:\r\n\r\n```\r\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph \\\r\n--input=./frozen_model_inception_v3.pb \\\r\n--output_node_names=\"InceptionV3/Predictions/Softmax\" \\\r\n--output=./quantized_graph_inception_v3.pb \\\r\n--mode=eightbit\r\n\r\n```\r\n\r\nEDIT: I found out the quantization of the nodes come from the graph_transform tool, and I tried to quantize an inception v4 model both weights and the nodes. However, the quantization of the node give me an `IndexError`. This is the error stack:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"evaluate_IR2_pb.py\", line 32, in <module>\r\n    tf.import_graph_def(graph_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 404, in import_graph_def\r\n    ops.set_shapes_for_outputs(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1719, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1669, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 653, in _call_cpp_shape_fn_impl\r\n    v = tensor_util.constant_value(op.inputs[idx])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 703, in constant_value\r\n    ret = _ConstantValue(tensor)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 651, in _ConstantValue\r\n    dim = constant_value(tensor.op.inputs[-1])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1439, in __getitem__\r\n    return self._op._inputs[i]\r\nIndexError: list index out of range\r\n```\r\nHere is the bazel command for quantizing weights and nodes:\r\n```\r\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_model_inception_resnet_v2.pb \\\r\n--out_graph=./quantized_weights_and_nodes_inception_resnet_v2.pb \\\r\n--inputs='Placeholder_only:0' \\\r\n--outputs='InceptionResnetV2/Logits/Predictions:0' \\\r\n--transforms='quantize_weights quantize_nodes'\r\n```\r\nOnce I removed `quantize_nodes`, the error is gone. But I am left with a quantization (`quantize_weights` only) that gives me the 100% inaccuracy I saw on the Jetson. On the other hand, using the quantization method (also weights only) I first used still gave me a high accuracy I expected.\r\n\r\nHowever, when I tried this on the inception_v3 model, the quantization of weights and nodes work as expected, but not on the Jetson, which gives me a `ValueError: No op named QuantizedMul in defined operations\". Upon removing `quantize_nodes` and using the `quantize_weights` only, the model runs, but again, it gives me the highly inaccurate answers.\r\n\r\nMy conclusion is that `quantize_nodes` isn't stable for the Jetson TX1, but on the other hand, `quantize_weights` (from the graph_transform tool) is highly inaccurate. Note that the above predictions are all on the custom imagenet model.\r\n\r\nDiscounting the use of the tools from `graph_transform` and following what I did initially with the quantization, both the quantization (not from `graph_transform` tool) and freezing graph (by manually exporting the graph after restoring from a checkpoint) works very well for another non-imagenet problem. The model used for this problem came from fine-tuning the imagenet problem. This is a very strange behavior.\r\n\r\nSo from what I can conclude, I think it is that quantization tools from `transform_graphs` is rather buggy, and for the quantization from `quantize_graph`, it works quite well but one can get very bad results on a Jetson TX probably due to how the architecture handles the calculations (This is just my conjecture). The calculations could be done differently in the Jetson because of the quantization to 8 bits, but I am not experienced with the Jetson so I can't say anything for sure. I have kept the imagenet prediction model and the fine-tuned model codes almost exactly the same, and the only things differing between them are the checkpoint files and the number of classes to predict in the end(1001 for the imagenet, and 5 for the fine-tuned one). Could this have caused the error? \r\n\r\nIs there a way to get consistent results from quantization (especially on a Jetson)?"}