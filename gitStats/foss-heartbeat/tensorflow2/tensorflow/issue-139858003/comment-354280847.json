{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354280847", "html_url": "https://github.com/tensorflow/tensorflow/issues/1453#issuecomment-354280847", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1453", "id": 354280847, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDI4MDg0Nw==", "user": {"login": "selcouthlyBlue", "id": 13268675, "node_id": "MDQ6VXNlcjEzMjY4Njc1", "avatar_url": "https://avatars2.githubusercontent.com/u/13268675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selcouthlyBlue", "html_url": "https://github.com/selcouthlyBlue", "followers_url": "https://api.github.com/users/selcouthlyBlue/followers", "following_url": "https://api.github.com/users/selcouthlyBlue/following{/other_user}", "gists_url": "https://api.github.com/users/selcouthlyBlue/gists{/gist_id}", "starred_url": "https://api.github.com/users/selcouthlyBlue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selcouthlyBlue/subscriptions", "organizations_url": "https://api.github.com/users/selcouthlyBlue/orgs", "repos_url": "https://api.github.com/users/selcouthlyBlue/repos", "events_url": "https://api.github.com/users/selcouthlyBlue/events{/privacy}", "received_events_url": "https://api.github.com/users/selcouthlyBlue/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-28T12:26:27Z", "updated_at": "2017-12-28T12:26:27Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN. etc.</p>\n</blockquote>\n<p>How exactly do I transpose the output? Say I have this code:</p>\n<pre><code>input = tf.placeholder(tf.float32, [batch_size, time_steps, num_features]\ncell = rnn.BasicLSTMCell(num_units=1)\nlstm1 = tf.nn.dynamic_rnn(cell, input, dtype=tf.float32)\ntranposed_lstm_output = some_transpose_function(lstm1) # how does this work?\nlstm2 = tf.nn.dynamic_rnn(cell, transposed_lstm_output, dtype=tf.float32)\n</code></pre>\n<p>As for the other one:</p>\n<blockquote>\n<p>Alternatively feed your data &amp; its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results.</p>\n</blockquote>\n<p>How do I \"depth-concatenate\" the results?</p>\n<pre><code>input = tf.placeholder(tf.float32, [batch_size, time_steps, num_features])\ntransposed_input = tf.placeholder(tf.float32, [batch_size, num_features, time_steps])\ncell = rnn.BasicLSTMCell(num_units=1)\nlstm1 = tf.nn.dynamic_rnn(cell, input, dtype=tf.float32)\nlstm2 = tf.nn.dynamic_rnn(cell, transposed_input, dtype=tf.float32)\nmdlstm1 = some_depth_concatenation_function([lstm1, lstm2]) # how does this work?\n</code></pre>", "body_text": "Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN. etc.\n\nHow exactly do I transpose the output? Say I have this code:\ninput = tf.placeholder(tf.float32, [batch_size, time_steps, num_features]\ncell = rnn.BasicLSTMCell(num_units=1)\nlstm1 = tf.nn.dynamic_rnn(cell, input, dtype=tf.float32)\ntranposed_lstm_output = some_transpose_function(lstm1) # how does this work?\nlstm2 = tf.nn.dynamic_rnn(cell, transposed_lstm_output, dtype=tf.float32)\n\nAs for the other one:\n\nAlternatively feed your data & its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results.\n\nHow do I \"depth-concatenate\" the results?\ninput = tf.placeholder(tf.float32, [batch_size, time_steps, num_features])\ntransposed_input = tf.placeholder(tf.float32, [batch_size, num_features, time_steps])\ncell = rnn.BasicLSTMCell(num_units=1)\nlstm1 = tf.nn.dynamic_rnn(cell, input, dtype=tf.float32)\nlstm2 = tf.nn.dynamic_rnn(cell, transposed_input, dtype=tf.float32)\nmdlstm1 = some_depth_concatenation_function([lstm1, lstm2]) # how does this work?", "body": "> Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN. etc.\r\n\r\nHow exactly do I transpose the output? Say I have this code:\r\n\r\n```\r\ninput = tf.placeholder(tf.float32, [batch_size, time_steps, num_features]\r\ncell = rnn.BasicLSTMCell(num_units=1)\r\nlstm1 = tf.nn.dynamic_rnn(cell, input, dtype=tf.float32)\r\ntranposed_lstm_output = some_transpose_function(lstm1) # how does this work?\r\nlstm2 = tf.nn.dynamic_rnn(cell, transposed_lstm_output, dtype=tf.float32)\r\n```\r\n\r\nAs for the other one:\r\n\r\n> Alternatively feed your data & its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results.\r\n\r\nHow do I \"depth-concatenate\" the results?\r\n\r\n```\r\ninput = tf.placeholder(tf.float32, [batch_size, time_steps, num_features])\r\ntransposed_input = tf.placeholder(tf.float32, [batch_size, num_features, time_steps])\r\ncell = rnn.BasicLSTMCell(num_units=1)\r\nlstm1 = tf.nn.dynamic_rnn(cell, input, dtype=tf.float32)\r\nlstm2 = tf.nn.dynamic_rnn(cell, transposed_input, dtype=tf.float32)\r\nmdlstm1 = some_depth_concatenation_function([lstm1, lstm2]) # how does this work?\r\n```"}