{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/194976468", "html_url": "https://github.com/tensorflow/tensorflow/issues/1453#issuecomment-194976468", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1453", "id": 194976468, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NDk3NjQ2OA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-10T17:55:19Z", "updated_at": "2016-03-10T17:55:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN.  etc.  Alternatively feed your data &amp; its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results.  And maybe feed the result into another RNN.</p>\n<p>This allows one to implement both separable and non-separable multidimensional RNNs.  Not sure we need the sugar until people show it's superior to convolutional networks for something, and there's lots of use cases.</p>\n<p><strong>That said</strong>, it would be really nice to implement GridLSTM (also by Alex Graves, and according to their paper showing a lot of promise with fewer parameters).  GridLSTM uses standard LSTM cells, so it would be a function that uses RNNCell objects.  It connects them in interesting, convolutional, ways.</p>\n<p>GridLSTM would probably go well in contrib, until enough people use it and it shows to be a very successful technique.</p>", "body_text": "Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN.  etc.  Alternatively feed your data & its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results.  And maybe feed the result into another RNN.\nThis allows one to implement both separable and non-separable multidimensional RNNs.  Not sure we need the sugar until people show it's superior to convolutional networks for something, and there's lots of use cases.\nThat said, it would be really nice to implement GridLSTM (also by Alex Graves, and according to their paper showing a lot of promise with fewer parameters).  GridLSTM uses standard LSTM cells, so it would be a function that uses RNNCell objects.  It connects them in interesting, convolutional, ways.\nGridLSTM would probably go well in contrib, until enough people use it and it shows to be a very successful technique.", "body": "Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN.  etc.  Alternatively feed your data & its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results.  And maybe feed the result into another RNN.\n\nThis allows one to implement both separable and non-separable multidimensional RNNs.  Not sure we need the sugar until people show it's superior to convolutional networks for something, and there's lots of use cases.\n\n**That said**, it would be really nice to implement GridLSTM (also by Alex Graves, and according to their paper showing a lot of promise with fewer parameters).  GridLSTM uses standard LSTM cells, so it would be a function that uses RNNCell objects.  It connects them in interesting, convolutional, ways.\n\nGridLSTM would probably go well in contrib, until enough people use it and it shows to be a very successful technique.\n"}