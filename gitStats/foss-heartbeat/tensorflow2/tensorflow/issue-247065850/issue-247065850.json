{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11942", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11942/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11942/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11942/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11942", "id": 247065850, "node_id": "MDU6SXNzdWUyNDcwNjU4NTA=", "number": 11942, "title": "Got `ValueError: Both labels and logits must be provided` while both labels and logits have been provided. ", "user": {"login": "kindlychung", "id": 995761, "node_id": "MDQ6VXNlcjk5NTc2MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/995761?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kindlychung", "html_url": "https://github.com/kindlychung", "followers_url": "https://api.github.com/users/kindlychung/followers", "following_url": "https://api.github.com/users/kindlychung/following{/other_user}", "gists_url": "https://api.github.com/users/kindlychung/gists{/gist_id}", "starred_url": "https://api.github.com/users/kindlychung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kindlychung/subscriptions", "organizations_url": "https://api.github.com/users/kindlychung/orgs", "repos_url": "https://api.github.com/users/kindlychung/repos", "events_url": "https://api.github.com/users/kindlychung/events{/privacy}", "received_events_url": "https://api.github.com/users/kindlychung/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-01T13:18:29Z", "updated_at": "2017-08-02T21:58:55Z", "closed_at": "2017-08-02T21:58:55Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.2.0-5-g435cdfc 1.2.1</li>\n<li><strong>Python version</strong>:  3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32)  GCC 4.4.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 5.1</li>\n<li><strong>GPU model and memory</strong>: GTX-1050 4GB</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Got <code>ValueError: Both labels and logits must be provided</code> while both labels and logits have been provided.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Softmax example in TF using the classical Iris dataset</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Download iris.data from https://archive.ics.uci.edu/ml/datasets/Iris</span>\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-c1\">print</span>(tf.<span class=\"pl-c1\">__version__</span>)\n<span class=\"pl-k\">import</span> os\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">combine_inputs</span>(<span class=\"pl-smi\">X</span>):\n    res <span class=\"pl-k\">=</span>  tf.matmul(X, W) <span class=\"pl-k\">+</span> b\n    res <span class=\"pl-k\">=</span> tf.identity(res, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>linear_out<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">inference</span>(<span class=\"pl-smi\">X</span>):\n    <span class=\"pl-k\">return</span> tf.nn.softmax(combine_inputs(X), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax_out<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">loss</span>(<span class=\"pl-smi\">X</span>, <span class=\"pl-smi\">Y</span>):\n    <span class=\"pl-k\">return</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>Y, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>combine_inputs(X), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>softmax_entropy<span class=\"pl-pds\">\"</span></span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loss<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">read_csv</span>(<span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">file_name</span>, <span class=\"pl-smi\">record_defaults</span>):\n    filename_queue <span class=\"pl-k\">=</span> tf.train.string_input_producer([os.path.dirname(<span class=\"pl-c1\">__file__</span>) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> file_name])\n    reader <span class=\"pl-k\">=</span> tf.TextLineReader(<span class=\"pl-v\">skip_header_lines</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    key, value <span class=\"pl-k\">=</span> reader.read(filename_queue)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> decode_csv will convert a Tensor from type string (the text line) in</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> a tuple of tensor columns with the specified defaults, which also</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> sets the data type for each column</span>\n    decoded <span class=\"pl-k\">=</span> tf.decode_csv(value, <span class=\"pl-v\">record_defaults</span><span class=\"pl-k\">=</span>record_defaults)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> batch actually reads the file and loads \"batch_size\" rows in a single tensor</span>\n    <span class=\"pl-k\">return</span> tf.train.shuffle_batch(decoded,\n                                  <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n                                  <span class=\"pl-v\">capacity</span><span class=\"pl-k\">=</span>batch_size <span class=\"pl-k\">*</span> <span class=\"pl-c1\">50</span>,\n                                  <span class=\"pl-v\">min_after_dequeue</span><span class=\"pl-k\">=</span>batch_size)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">inputs</span>():\n    sepal_length, sepal_width, petal_length, petal_width, label <span class=\"pl-k\">=</span>\\\n        read_csv(<span class=\"pl-c1\">100</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>iris.csv<span class=\"pl-pds\">\"</span></span>, [[<span class=\"pl-c1\">0.0</span>], [<span class=\"pl-c1\">0.0</span>], [<span class=\"pl-c1\">0.0</span>], [<span class=\"pl-c1\">0.0</span>], [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>]])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> convert class names to a 0 based class index.</span>\n    label_number <span class=\"pl-k\">=</span> tf.to_int32(tf.argmax(tf.to_int32(tf.stack([\n        tf.equal(label, [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Iris-setosa<span class=\"pl-pds\">\"</span></span>]),\n        tf.equal(label, [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Iris-versicolor<span class=\"pl-pds\">\"</span></span>]),\n        tf.equal(label, [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Iris-virginica<span class=\"pl-pds\">\"</span></span>])\n    ])), <span class=\"pl-c1\">0</span>))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Pack all the features that we care about in a single matrix;</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> We then transpose to have a matrix with one example per row and one feature per column.</span>\n    features <span class=\"pl-k\">=</span> tf.transpose(tf.stack([sepal_length, sepal_width, petal_length, petal_width]))\n    <span class=\"pl-k\">return</span> features, label_number\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">total_loss</span>):\n    learning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.01</span>\n    <span class=\"pl-k\">return</span> tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">evaluate</span>(<span class=\"pl-smi\">sess</span>, <span class=\"pl-smi\">X</span>, <span class=\"pl-smi\">Y</span>):\n    predicted <span class=\"pl-k\">=</span> tf.cast(tf.arg_max(inference(X), <span class=\"pl-c1\">1</span>), tf.int32)\n    <span class=\"pl-c1\">print</span>(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Launch the graph in a session, setup boilerplate</span>\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> this time weights form a matrix, not a column vector, one \"weight vector\" per class.</span>\n        W <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>]), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weights<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> so do the biases, one per class.</span>\n        b <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">3</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bias<span class=\"pl-pds\">\"</span></span>))\n        tf.global_variables_initializer().run()\n        X, Y <span class=\"pl-k\">=</span> inputs()\n        total_loss <span class=\"pl-k\">=</span> loss(X, Y)\n        train_op <span class=\"pl-k\">=</span> train(total_loss)\n        summary_out <span class=\"pl-k\">=</span> os.path.join(os.path.dirname(<span class=\"pl-c1\">__file__</span>), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tf_summary<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-k\">import</span> shutil\n        shutil.rmtree(summary_out)\n        writer <span class=\"pl-k\">=</span> tf.summary.FileWriter(summary_out, <span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>sess.graph)\n        coord <span class=\"pl-k\">=</span> tf.train.Coordinator()\n        threads <span class=\"pl-k\">=</span> tf.train.start_queue_runners(<span class=\"pl-v\">sess</span><span class=\"pl-k\">=</span>sess, <span class=\"pl-v\">coord</span><span class=\"pl-k\">=</span>coord)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> actual training loop</span>\n        training_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n        <span class=\"pl-k\">for</span> step <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(training_steps):\n            sess.run([train_op])\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> for debugging and learning purposes, see how the loss gets decremented thru training steps</span>\n            <span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loss: <span class=\"pl-pds\">\"</span></span>, sess.run([total_loss]))\n        evaluate(sess, X, Y)\n        coord.request_stop()\n        coord.join(threads)\n        writer.close()</pre></div>\n<p>Error:</p>\n<pre><code>/home/kaiyin/miniconda3/envs/tf/bin/python3.6 /home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\n2017-08-01 15:12:57.469597: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.469610: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.469613: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.469616: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.469618: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.554651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-08-01 15:12:57.554839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \nname: GeForce GTX 1050\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\npciBusID 0000:01:00.0\nTotal memory: 3.95GiB\nFree memory: 1.97GiB\n2017-08-01 15:12:57.554848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \n2017-08-01 15:12:57.554852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \n2017-08-01 15:12:57.554859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\nTraceback (most recent call last):\n  File \"/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\", line 69, in &lt;module&gt;\n    total_loss = loss(X, Y)\n  File \"/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\", line 17, in loss\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(None, Y, combine_inputs(X)), name=\"loss\")\n  File \"/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1555, in softmax_cross_entropy_with_logits\n    labels, logits)\n  File \"/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1512, in _ensure_xent_args\n    raise ValueError(\"Both labels and logits must be provided.\")\nValueError: Both labels and logits must be provided.\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.2.0-5-g435cdfc 1.2.1\nPython version:  3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32)  GCC 4.4.7\nBazel version (if compiling from source):\nCUDA/cuDNN version: 5.1\nGPU model and memory: GTX-1050 4GB\n\nDescribe the problem\nGot ValueError: Both labels and logits must be provided while both labels and logits have been provided.\nSource code / logs\n# Softmax example in TF using the classical Iris dataset\n# Download iris.data from https://archive.ics.uci.edu/ml/datasets/Iris\n\nimport tensorflow as tf\nprint(tf.__version__)\nimport os\n\ndef combine_inputs(X):\n    res =  tf.matmul(X, W) + b\n    res = tf.identity(res, name=\"linear_out\")\n\n\ndef inference(X):\n    return tf.nn.softmax(combine_inputs(X), \"softmax_out\")\n\n\ndef loss(X, Y):\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=combine_inputs(X), name=\"softmax_entropy\"), name=\"loss\")\n\n\ndef read_csv(batch_size, file_name, record_defaults):\n    filename_queue = tf.train.string_input_producer([os.path.dirname(__file__) + \"/\" + file_name])\n    reader = tf.TextLineReader(skip_header_lines=1)\n    key, value = reader.read(filename_queue)\n    # decode_csv will convert a Tensor from type string (the text line) in\n    # a tuple of tensor columns with the specified defaults, which also\n    # sets the data type for each column\n    decoded = tf.decode_csv(value, record_defaults=record_defaults)\n    # batch actually reads the file and loads \"batch_size\" rows in a single tensor\n    return tf.train.shuffle_batch(decoded,\n                                  batch_size=batch_size,\n                                  capacity=batch_size * 50,\n                                  min_after_dequeue=batch_size)\n\n\ndef inputs():\n    sepal_length, sepal_width, petal_length, petal_width, label =\\\n        read_csv(100, \"iris.csv\", [[0.0], [0.0], [0.0], [0.0], [\"\"]])\n    # convert class names to a 0 based class index.\n    label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.stack([\n        tf.equal(label, [\"Iris-setosa\"]),\n        tf.equal(label, [\"Iris-versicolor\"]),\n        tf.equal(label, [\"Iris-virginica\"])\n    ])), 0))\n    # Pack all the features that we care about in a single matrix;\n    # We then transpose to have a matrix with one example per row and one feature per column.\n    features = tf.transpose(tf.stack([sepal_length, sepal_width, petal_length, petal_width]))\n    return features, label_number\n\n\ndef train(total_loss):\n    learning_rate = 0.01\n    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n\n\ndef evaluate(sess, X, Y):\n    predicted = tf.cast(tf.arg_max(inference(X), 1), tf.int32)\n    print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))\n\n\n# Launch the graph in a session, setup boilerplate\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        # this time weights form a matrix, not a column vector, one \"weight vector\" per class.\n        W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\n        # so do the biases, one per class.\n        b = tf.Variable(tf.zeros([3], name=\"bias\"))\n        tf.global_variables_initializer().run()\n        X, Y = inputs()\n        total_loss = loss(X, Y)\n        train_op = train(total_loss)\n        summary_out = os.path.join(os.path.dirname(__file__), \"tf_summary\")\n        import shutil\n        shutil.rmtree(summary_out)\n        writer = tf.summary.FileWriter(summary_out, graph=sess.graph)\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n        # actual training loop\n        training_steps = 1000\n        for step in range(training_steps):\n            sess.run([train_op])\n            # for debugging and learning purposes, see how the loss gets decremented thru training steps\n            if step % 10 == 0:\n                print(\"loss: \", sess.run([total_loss]))\n        evaluate(sess, X, Y)\n        coord.request_stop()\n        coord.join(threads)\n        writer.close()\nError:\n/home/kaiyin/miniconda3/envs/tf/bin/python3.6 /home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\n2017-08-01 15:12:57.469597: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.469610: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.469613: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.469616: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.469618: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-01 15:12:57.554651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-08-01 15:12:57.554839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \nname: GeForce GTX 1050\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\npciBusID 0000:01:00.0\nTotal memory: 3.95GiB\nFree memory: 1.97GiB\n2017-08-01 15:12:57.554848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \n2017-08-01 15:12:57.554852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \n2017-08-01 15:12:57.554859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\nTraceback (most recent call last):\n  File \"/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\", line 69, in <module>\n    total_loss = loss(X, Y)\n  File \"/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\", line 17, in loss\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(None, Y, combine_inputs(X)), name=\"loss\")\n  File \"/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1555, in softmax_cross_entropy_with_logits\n    labels, logits)\n  File \"/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1512, in _ensure_xent_args\n    raise ValueError(\"Both labels and logits must be provided.\")\nValueError: Both labels and logits must be provided.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**:  3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32)  GCC 4.4.7 \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: GTX-1050 4GB\r\n\r\n\r\n### Describe the problem\r\n\r\nGot `ValueError: Both labels and logits must be provided` while both labels and logits have been provided. \r\n\r\n### Source code / logs\r\n\r\n```python\r\n# Softmax example in TF using the classical Iris dataset\r\n# Download iris.data from https://archive.ics.uci.edu/ml/datasets/Iris\r\n\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nimport os\r\n\r\ndef combine_inputs(X):\r\n    res =  tf.matmul(X, W) + b\r\n    res = tf.identity(res, name=\"linear_out\")\r\n\r\n\r\ndef inference(X):\r\n    return tf.nn.softmax(combine_inputs(X), \"softmax_out\")\r\n\r\n\r\ndef loss(X, Y):\r\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=combine_inputs(X), name=\"softmax_entropy\"), name=\"loss\")\r\n\r\n\r\ndef read_csv(batch_size, file_name, record_defaults):\r\n    filename_queue = tf.train.string_input_producer([os.path.dirname(__file__) + \"/\" + file_name])\r\n    reader = tf.TextLineReader(skip_header_lines=1)\r\n    key, value = reader.read(filename_queue)\r\n    # decode_csv will convert a Tensor from type string (the text line) in\r\n    # a tuple of tensor columns with the specified defaults, which also\r\n    # sets the data type for each column\r\n    decoded = tf.decode_csv(value, record_defaults=record_defaults)\r\n    # batch actually reads the file and loads \"batch_size\" rows in a single tensor\r\n    return tf.train.shuffle_batch(decoded,\r\n                                  batch_size=batch_size,\r\n                                  capacity=batch_size * 50,\r\n                                  min_after_dequeue=batch_size)\r\n\r\n\r\ndef inputs():\r\n    sepal_length, sepal_width, petal_length, petal_width, label =\\\r\n        read_csv(100, \"iris.csv\", [[0.0], [0.0], [0.0], [0.0], [\"\"]])\r\n    # convert class names to a 0 based class index.\r\n    label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.stack([\r\n        tf.equal(label, [\"Iris-setosa\"]),\r\n        tf.equal(label, [\"Iris-versicolor\"]),\r\n        tf.equal(label, [\"Iris-virginica\"])\r\n    ])), 0))\r\n    # Pack all the features that we care about in a single matrix;\r\n    # We then transpose to have a matrix with one example per row and one feature per column.\r\n    features = tf.transpose(tf.stack([sepal_length, sepal_width, petal_length, petal_width]))\r\n    return features, label_number\r\n\r\n\r\ndef train(total_loss):\r\n    learning_rate = 0.01\r\n    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\r\n\r\n\r\ndef evaluate(sess, X, Y):\r\n    predicted = tf.cast(tf.arg_max(inference(X), 1), tf.int32)\r\n    print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))\r\n\r\n\r\n# Launch the graph in a session, setup boilerplate\r\nwith tf.Graph().as_default():\r\n    with tf.Session() as sess:\r\n        # this time weights form a matrix, not a column vector, one \"weight vector\" per class.\r\n        W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\r\n        # so do the biases, one per class.\r\n        b = tf.Variable(tf.zeros([3], name=\"bias\"))\r\n        tf.global_variables_initializer().run()\r\n        X, Y = inputs()\r\n        total_loss = loss(X, Y)\r\n        train_op = train(total_loss)\r\n        summary_out = os.path.join(os.path.dirname(__file__), \"tf_summary\")\r\n        import shutil\r\n        shutil.rmtree(summary_out)\r\n        writer = tf.summary.FileWriter(summary_out, graph=sess.graph)\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n        # actual training loop\r\n        training_steps = 1000\r\n        for step in range(training_steps):\r\n            sess.run([train_op])\r\n            # for debugging and learning purposes, see how the loss gets decremented thru training steps\r\n            if step % 10 == 0:\r\n                print(\"loss: \", sess.run([total_loss]))\r\n        evaluate(sess, X, Y)\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n        writer.close()\r\n```\r\n\r\nError:\r\n\r\n```\r\n/home/kaiyin/miniconda3/envs/tf/bin/python3.6 /home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\r\n2017-08-01 15:12:57.469597: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.469610: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.469613: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.469616: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.469618: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-01 15:12:57.554651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-08-01 15:12:57.554839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 1050\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 1.97GiB\r\n2017-08-01 15:12:57.554848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-08-01 15:12:57.554852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-08-01 15:12:57.554859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\r\nTraceback (most recent call last):\r\n  File \"/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\", line 69, in <module>\r\n    total_loss = loss(X, Y)\r\n  File \"/home/kaiyin/PycharmProjects/tensorflow-mnist/learntf_008_softmax.py\", line 17, in loss\r\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(None, Y, combine_inputs(X)), name=\"loss\")\r\n  File \"/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1555, in softmax_cross_entropy_with_logits\r\n    labels, logits)\r\n  File \"/home/kaiyin/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1512, in _ensure_xent_args\r\n    raise ValueError(\"Both labels and logits must be provided.\")\r\nValueError: Both labels and logits must be provided.\r\n```\r\n"}