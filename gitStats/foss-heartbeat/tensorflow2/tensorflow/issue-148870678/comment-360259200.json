{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/360259200", "html_url": "https://github.com/tensorflow/tensorflow/issues/1984#issuecomment-360259200", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1984", "id": 360259200, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDI1OTIwMA==", "user": {"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-24T20:13:53Z", "updated_at": "2018-01-24T20:13:53Z", "author_association": "MEMBER", "body_html": "<p>Let's take the limiting case where parallel_iterations is infinity. Now, if you imagine unrolling the whole loop as a dataflow graph, then the graph will just execute according to its data dependencies.</p>\n<p>So in your example, there can't be two instances of <code>y</code> running in parallel, because <code>y</code>'s input in step <code>i+1</code> is the output of <code>y</code> in step <code>i</code>. Likewise, there can't be two instances of <code>i+1</code> running in parallel for the same reason.</p>\n<p>However, if <code>f(.)</code> is slow, all the <code>i+1</code> ops will run ahead to completion without waiting for even the <code>f(.)</code> in the first iteration to complete, because there's no dependency of <code>f</code> that's needed to compute the next <code>i</code>.</p>\n<p>If you had said</p>\n<pre><code>i = tf.constant(0) \nx = ... \ncond_ix = lambda i, x: tf.less(i, 10) \n\ndef body1(i, x):\n       y = g(i)\n       return i+1, y  \ntf.while_loop(cond_ix, body1, [i, x])\n</code></pre>\n<p>then if <code>g</code> were slow you could easily end up with <code>g(0)</code>, <code>g(1)</code>, ..., <code>g(9)</code> all running in parallel because there's no dependency on the output of <code>g(0)</code> that is needed to start <code>g(1)</code>.</p>", "body_text": "Let's take the limiting case where parallel_iterations is infinity. Now, if you imagine unrolling the whole loop as a dataflow graph, then the graph will just execute according to its data dependencies.\nSo in your example, there can't be two instances of y running in parallel, because y's input in step i+1 is the output of y in step i. Likewise, there can't be two instances of i+1 running in parallel for the same reason.\nHowever, if f(.) is slow, all the i+1 ops will run ahead to completion without waiting for even the f(.) in the first iteration to complete, because there's no dependency of f that's needed to compute the next i.\nIf you had said\ni = tf.constant(0) \nx = ... \ncond_ix = lambda i, x: tf.less(i, 10) \n\ndef body1(i, x):\n       y = g(i)\n       return i+1, y  \ntf.while_loop(cond_ix, body1, [i, x])\n\nthen if g were slow you could easily end up with g(0), g(1), ..., g(9) all running in parallel because there's no dependency on the output of g(0) that is needed to start g(1).", "body": "Let's take the limiting case where parallel_iterations is infinity. Now, if you imagine unrolling the whole loop as a dataflow graph, then the graph will just execute according to its data dependencies.\r\n\r\nSo in your example, there can't be two instances of `y` running in parallel, because `y`'s input in step `i+1` is the output of `y` in step `i`. Likewise, there can't be two instances of `i+1` running in parallel for the same reason.\r\n\r\nHowever, if `f(.)` is slow, all the `i+1` ops will run ahead to completion without waiting for even the `f(.)` in the first iteration to complete, because there's no dependency of `f` that's needed to compute the next `i`.\r\n\r\nIf you had said\r\n```\r\ni = tf.constant(0) \r\nx = ... \r\ncond_ix = lambda i, x: tf.less(i, 10) \r\n\r\ndef body1(i, x):\r\n       y = g(i)\r\n       return i+1, y  \r\ntf.while_loop(cond_ix, body1, [i, x])\r\n```\r\nthen if `g` were slow you could easily end up with `g(0)`, `g(1)`, ..., `g(9)` all running in parallel because there's no dependency on the output of `g(0)` that is needed to start `g(1)`."}