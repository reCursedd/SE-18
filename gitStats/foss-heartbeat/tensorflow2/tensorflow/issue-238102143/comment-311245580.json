{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/311245580", "html_url": "https://github.com/tensorflow/tensorflow/pull/11009#issuecomment-311245580", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11009", "id": 311245580, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTI0NTU4MA==", "user": {"login": "Yiran-wu", "id": 6999199, "node_id": "MDQ6VXNlcjY5OTkxOTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/6999199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yiran-wu", "html_url": "https://github.com/Yiran-wu", "followers_url": "https://api.github.com/users/Yiran-wu/followers", "following_url": "https://api.github.com/users/Yiran-wu/following{/other_user}", "gists_url": "https://api.github.com/users/Yiran-wu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yiran-wu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yiran-wu/subscriptions", "organizations_url": "https://api.github.com/users/Yiran-wu/orgs", "repos_url": "https://api.github.com/users/Yiran-wu/repos", "events_url": "https://api.github.com/users/Yiran-wu/events{/privacy}", "received_events_url": "https://api.github.com/users/Yiran-wu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-27T03:51:42Z", "updated_at": "2017-06-27T06:30:12Z", "author_association": "NONE", "body_html": "<p>I use the following script to test it, HADOOP_USER_NAME can be work,  close this issue.<br>\nthinks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=170179\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jhseu\">@jhseu</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a></p>\n<pre><code># import MNIST\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\nimport tensorflow as tf\nimport os\n\nos.environ['HADOOP_USER_NAME'] = 'test001'\n# set parameters\nlearning_rate = 0.01\ntraining_iteration = 30\nbatch_size = 100\ndisplay_step = 2\n\n\n# TF graph input\nx = tf.placeholder(\"float\", [None, 784])\ny = tf.placeholder(\"float\", [None, 10])\n\n# create a model\n\n# set model weights\n# 784 is the dimension of a flattened MNIST image\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\nwith tf.name_scope(\"Wx_b\") as scope:\n    # construct linear model\n    model = tf.nn.softmax(tf.matmul(x, W) + b) #softmax\n\n# add summary ops to collect data\nw_h = tf.summary.histogram(\"weights\", W)\nb_h = tf.summary.histogram(\"biases\", b)\n\nwith tf.name_scope(\"cost_function\") as scope:\n    # minimize error using cross entropy\n    cost_function = -tf.reduce_sum(y*tf.log(model))\n    # create a summary to monitor the cost function\n    tf.summary.scalar(\"cost_function\", cost_function)\n\nwith tf.name_scope(\"train\") as scope:\n    # gradient descent\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n\ninit = tf.global_variables_initializer()\n\n# merge all summaries into a single operator\nmerged_summary_op = tf.summary.merge_all()\n\n# launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # set the logs writer to the folder /tmp/tensorflow_logs\n    summary_writer = tf.summary.FileWriter('hdfs://172.16.141.90:8020/tmp/tensorflow_logs', graph=sess.graph)\n\n    # training cycle\n    for iteration in range(training_iteration):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # loop over all batches\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # fit training using batch data\n            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n            # compute the average loss\n            avg_cost += sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n            # write logs for each iteration\n            summary_str = sess.run(merged_summary_op, feed_dict={x: batch_xs, y: batch_ys})\n            summary_writer.add_summary(summary_str, iteration*total_batch + i)\n        # display logs per iteration step\n        if iteration % display_step == 0:\n            print(\"Iteration:\", '%04d' % (iteration + 1), \"cost= \", \"{:.9f}\".format(avg_cost))\n\n    print(\"Tuning completed!\")\n\n    # test the model\n    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n    # calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n\n\nprint(\"Success!\")\n</code></pre>", "body_text": "I use the following script to test it, HADOOP_USER_NAME can be work,  close this issue.\nthinks @jhseu @drpngx\n# import MNIST\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\nimport tensorflow as tf\nimport os\n\nos.environ['HADOOP_USER_NAME'] = 'test001'\n# set parameters\nlearning_rate = 0.01\ntraining_iteration = 30\nbatch_size = 100\ndisplay_step = 2\n\n\n# TF graph input\nx = tf.placeholder(\"float\", [None, 784])\ny = tf.placeholder(\"float\", [None, 10])\n\n# create a model\n\n# set model weights\n# 784 is the dimension of a flattened MNIST image\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\nwith tf.name_scope(\"Wx_b\") as scope:\n    # construct linear model\n    model = tf.nn.softmax(tf.matmul(x, W) + b) #softmax\n\n# add summary ops to collect data\nw_h = tf.summary.histogram(\"weights\", W)\nb_h = tf.summary.histogram(\"biases\", b)\n\nwith tf.name_scope(\"cost_function\") as scope:\n    # minimize error using cross entropy\n    cost_function = -tf.reduce_sum(y*tf.log(model))\n    # create a summary to monitor the cost function\n    tf.summary.scalar(\"cost_function\", cost_function)\n\nwith tf.name_scope(\"train\") as scope:\n    # gradient descent\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n\ninit = tf.global_variables_initializer()\n\n# merge all summaries into a single operator\nmerged_summary_op = tf.summary.merge_all()\n\n# launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # set the logs writer to the folder /tmp/tensorflow_logs\n    summary_writer = tf.summary.FileWriter('hdfs://172.16.141.90:8020/tmp/tensorflow_logs', graph=sess.graph)\n\n    # training cycle\n    for iteration in range(training_iteration):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # loop over all batches\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # fit training using batch data\n            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n            # compute the average loss\n            avg_cost += sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n            # write logs for each iteration\n            summary_str = sess.run(merged_summary_op, feed_dict={x: batch_xs, y: batch_ys})\n            summary_writer.add_summary(summary_str, iteration*total_batch + i)\n        # display logs per iteration step\n        if iteration % display_step == 0:\n            print(\"Iteration:\", '%04d' % (iteration + 1), \"cost= \", \"{:.9f}\".format(avg_cost))\n\n    print(\"Tuning completed!\")\n\n    # test the model\n    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n    # calculate accuracy\n    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n\n\nprint(\"Success!\")", "body": "I use the following script to test it, HADOOP_USER_NAME can be work,  close this issue.\r\nthinks @jhseu @drpngx \r\n```\r\n# import MNIST\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n\r\nimport tensorflow as tf\r\nimport os\r\n\r\nos.environ['HADOOP_USER_NAME'] = 'test001'\r\n# set parameters\r\nlearning_rate = 0.01\r\ntraining_iteration = 30\r\nbatch_size = 100\r\ndisplay_step = 2\r\n\r\n\r\n# TF graph input\r\nx = tf.placeholder(\"float\", [None, 784])\r\ny = tf.placeholder(\"float\", [None, 10])\r\n\r\n# create a model\r\n\r\n# set model weights\r\n# 784 is the dimension of a flattened MNIST image\r\nW = tf.Variable(tf.zeros([784, 10]))\r\nb = tf.Variable(tf.zeros([10]))\r\n\r\nwith tf.name_scope(\"Wx_b\") as scope:\r\n    # construct linear model\r\n    model = tf.nn.softmax(tf.matmul(x, W) + b) #softmax\r\n\r\n# add summary ops to collect data\r\nw_h = tf.summary.histogram(\"weights\", W)\r\nb_h = tf.summary.histogram(\"biases\", b)\r\n\r\nwith tf.name_scope(\"cost_function\") as scope:\r\n    # minimize error using cross entropy\r\n    cost_function = -tf.reduce_sum(y*tf.log(model))\r\n    # create a summary to monitor the cost function\r\n    tf.summary.scalar(\"cost_function\", cost_function)\r\n\r\nwith tf.name_scope(\"train\") as scope:\r\n    # gradient descent\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n# merge all summaries into a single operator\r\nmerged_summary_op = tf.summary.merge_all()\r\n\r\n# launch the graph\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n\r\n    # set the logs writer to the folder /tmp/tensorflow_logs\r\n    summary_writer = tf.summary.FileWriter('hdfs://172.16.141.90:8020/tmp/tensorflow_logs', graph=sess.graph)\r\n\r\n    # training cycle\r\n    for iteration in range(training_iteration):\r\n        avg_cost = 0.\r\n        total_batch = int(mnist.train.num_examples/batch_size)\r\n        # loop over all batches\r\n        for i in range(total_batch):\r\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\r\n            # fit training using batch data\r\n            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\r\n            # compute the average loss\r\n            avg_cost += sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})/total_batch\r\n            # write logs for each iteration\r\n            summary_str = sess.run(merged_summary_op, feed_dict={x: batch_xs, y: batch_ys})\r\n            summary_writer.add_summary(summary_str, iteration*total_batch + i)\r\n        # display logs per iteration step\r\n        if iteration % display_step == 0:\r\n            print(\"Iteration:\", '%04d' % (iteration + 1), \"cost= \", \"{:.9f}\".format(avg_cost))\r\n\r\n    print(\"Tuning completed!\")\r\n\r\n    # test the model\r\n    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\r\n    # calculate accuracy\r\n    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\r\n    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\r\n\r\n\r\nprint(\"Success!\")\r\n```"}