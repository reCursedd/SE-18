{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316600605", "html_url": "https://github.com/tensorflow/tensorflow/pull/9909#issuecomment-316600605", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9909", "id": 316600605, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjYwMDYwNQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-20T05:28:08Z", "updated_at": "2017-07-20T05:28:08Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">My reading of the equations is that the input and hidden state should be\nmixed in this way at each time step\n\n<a href=\"https://en.m.wikipedia.org/wiki/Long_short-term_memory\">https://en.m.wikipedia.org/wiki/Long_short-term_memory</a>\n\nWe also compare to cudnn implementation here and ensure we have identical\nvalues within machine precision:\n\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py</a>\n\nFunction testCudnnCompatibleRnnCells (the lstm compatible cell is just a\nsubclass of LSTMCell with forget_bias=0).</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Jul 19, 2017 10:08 PM, \"Kristian Holsheimer\" ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; The docstring isn't the thing\n that's wrong here. The reason is that it's used by e.g. LSTMCell, which\n requires the behavior mentioned in the docsting, *not* the implementation\n that you suggest. If we keep the implementation like this, we are diverting\n from the standard LSTM architecture in two ways:\n\n    1. We use len(args) more weights\n    2. We allow for mixing between input and hidden state at each time step\n\n I suspect that the real reason we're doing this concatenation thing is\n (time) optimization. So, if we're willing to have a non-standard definition\n of LSTM in tensorflow, we could decide to leave it like this, but *we\n need to be honest and make this choice consciously*, not as an unintended\n by-product of a low-level optimization trick.\n\n For the record, my choice would be to use the definition of LSTM that is\n used by the rest of the community (as per this pull request).\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"228629023\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9909\" href=\"https://github.com/tensorflow/tensorflow/pull/9909#issuecomment-316598058\">#9909 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_3FB6PvGvJL8-ks5sPuDGgaJpZM4Naw8b\">https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_3FB6PvGvJL8-ks5sPuDGgaJpZM4Naw8b</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "My reading of the equations is that the input and hidden state should be\nmixed in this way at each time step\n\nhttps://en.m.wikipedia.org/wiki/Long_short-term_memory\n\nWe also compare to cudnn implementation here and ensure we have identical\nvalues within machine precision:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py\n\nFunction testCudnnCompatibleRnnCells (the lstm compatible cell is just a\nsubclass of LSTMCell with forget_bias=0).\n\u2026\nOn Jul 19, 2017 10:08 PM, \"Kristian Holsheimer\" ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo> The docstring isn't the thing\n that's wrong here. The reason is that it's used by e.g. LSTMCell, which\n requires the behavior mentioned in the docsting, *not* the implementation\n that you suggest. If we keep the implementation like this, we are diverting\n from the standard LSTM architecture in two ways:\n\n    1. We use len(args) more weights\n    2. We allow for mixing between input and hidden state at each time step\n\n I suspect that the real reason we're doing this concatenation thing is\n (time) optimization. So, if we're willing to have a non-standard definition\n of LSTM in tensorflow, we could decide to leave it like this, but *we\n need to be honest and make this choice consciously*, not as an unintended\n by-product of a low-level optimization trick.\n\n For the record, my choice would be to use the definition of LSTM that is\n used by the rest of the community (as per this pull request).\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#9909 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_3FB6PvGvJL8-ks5sPuDGgaJpZM4Naw8b>\n .", "body": "My reading of the equations is that the input and hidden state should be\nmixed in this way at each time step\n\nhttps://en.m.wikipedia.org/wiki/Long_short-term_memory\n\nWe also compare to cudnn implementation here and ensure we have identical\nvalues within machine precision:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py\n\nFunction testCudnnCompatibleRnnCells (the lstm compatible cell is just a\nsubclass of LSTMCell with forget_bias=0).\n\nOn Jul 19, 2017 10:08 PM, \"Kristian Holsheimer\" <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> The docstring isn't the thing\n> that's wrong here. The reason is that it's used by e.g. LSTMCell, which\n> requires the behavior mentioned in the docsting, *not* the implementation\n> that you suggest. If we keep the implementation like this, we are diverting\n> from the standard LSTM architecture in two ways:\n>\n>    1. We use len(args) more weights\n>    2. We allow for mixing between input and hidden state at each time step\n>\n> I suspect that the real reason we're doing this concatenation thing is\n> (time) optimization. So, if we're willing to have a non-standard definition\n> of LSTM in tensorflow, we could decide to leave it like this, but *we\n> need to be honest and make this choice consciously*, not as an unintended\n> by-product of a low-level optimization trick.\n>\n> For the record, my choice would be to use the definition of LSTM that is\n> used by the rest of the community (as per this pull request).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9909#issuecomment-316598058>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_3FB6PvGvJL8-ks5sPuDGgaJpZM4Naw8b>\n> .\n>\n"}