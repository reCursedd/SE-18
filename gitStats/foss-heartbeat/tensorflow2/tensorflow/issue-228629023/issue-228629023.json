{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9909", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9909/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9909/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9909/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/9909", "id": 228629023, "node_id": "MDExOlB1bGxSZXF1ZXN0MTIwNTQwNDAz", "number": 9909, "title": "Fixed _linear(.) to use *batch* matrix multiplication.", "user": {"login": "KristianHolsheimer", "id": 8200332, "node_id": "MDQ6VXNlcjgyMDAzMzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/8200332?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KristianHolsheimer", "html_url": "https://github.com/KristianHolsheimer", "followers_url": "https://api.github.com/users/KristianHolsheimer/followers", "following_url": "https://api.github.com/users/KristianHolsheimer/following{/other_user}", "gists_url": "https://api.github.com/users/KristianHolsheimer/gists{/gist_id}", "starred_url": "https://api.github.com/users/KristianHolsheimer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KristianHolsheimer/subscriptions", "organizations_url": "https://api.github.com/users/KristianHolsheimer/orgs", "repos_url": "https://api.github.com/users/KristianHolsheimer/repos", "events_url": "https://api.github.com/users/KristianHolsheimer/events{/privacy}", "received_events_url": "https://api.github.com/users/KristianHolsheimer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 390482148, "node_id": "MDU6TGFiZWwzOTA0ODIxNDg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20review", "name": "awaiting review", "color": "fef2c0", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2017-05-15T07:35:54Z", "updated_at": "2017-07-20T16:55:39Z", "closed_at": "2017-07-20T01:55:43Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9909", "html_url": "https://github.com/tensorflow/tensorflow/pull/9909", "diff_url": "https://github.com/tensorflow/tensorflow/pull/9909.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/9909.patch"}, "body_html": "<p>In the <code>_linear</code> function (from the <code>tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl</code> module), there's relatively subtle bug. The reason that this bug is subtle is that it only affects tensors used by <code>_linear</code> internally.</p>\n<p>For future reference, the signature and docstring of <code>_linear</code> are:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_linear</span>(<span class=\"pl-smi\">args</span>, <span class=\"pl-smi\">output_size</span>, <span class=\"pl-smi\">bias</span>, <span class=\"pl-smi\">bias_start</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Args:</span>\n<span class=\"pl-s\">    args: a 2D Tensor or a list of 2D, batch x n, Tensors.</span>\n<span class=\"pl-s\">    output_size: int, second dimension of W[i].</span>\n<span class=\"pl-s\">    bias: boolean, whether to add a bias term or not.</span>\n<span class=\"pl-s\">    bias_start: starting value to initialize the bias; 0 by default.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Returns:</span>\n<span class=\"pl-s\">    A 2D Tensor with shape [batch x output_size] equal to</span>\n<span class=\"pl-s\">    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Raises:</span>\n<span class=\"pl-s\">    ValueError: if some of the arguments has unspecified or wrong shape.</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span></pre></div>\n<p><strong>The Problem.</strong><br>\nFrom the docsting of <code>_linear</code>, the expected behavior of the function is that it should return an output <code>y</code> such that <code>y = sum_i(args[i] * W[i])</code>, indicating a batch-wise <code>matmul</code> (formerly known as <code>batch_matmul</code>). This wasn't actually how <code>_linear</code> was implemented, though. Instead, all <code>args</code> were concatenated and the weights were chosen to have rank 2, i.e.</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> concat(args, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> x.shape == (batch_size, total_arg_size)</span>\nw <span class=\"pl-k\">=</span> Variable(<span class=\"pl-c1\">...</span>)         <span class=\"pl-c\"><span class=\"pl-c\">#</span> w.shape == (total_arg_size, output_size)</span>\ny <span class=\"pl-k\">=</span> matmul(x, w)          <span class=\"pl-c\"><span class=\"pl-c\">#</span> y.shape == (batch_size, output_size)</span></pre></div>\n<p>Now, the main problem here is that <strong><code>w</code> is not block-diagonal</strong>, which means that it contains more entries than it should. There are two problems with this:</p>\n<ol>\n<li>The <strong>number of weights is too large</strong> by a factor of <code>num_args</code>.</li>\n<li>The extra \"off-diagonal\" weights yield <strong>spurious connections</strong> in the net architecture of e.g. LSTMCell.</li>\n</ol>\n<p><strong>The Solution.</strong><br>\nThe solution is to use batch-wise <code>matmul</code> instead. In order to do this, we need <code>x</code> and <code>w</code> to be rank-3 (rather than rank-2) tensors, e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> stack(args, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> x.shape == (num_args, batch_size, input_size)</span>\nw <span class=\"pl-k\">=</span> Variable(<span class=\"pl-c1\">...</span>)                          <span class=\"pl-c\"><span class=\"pl-c\">#</span> w.shape == (num_args, input_size, output_size / num_args)</span>\ny <span class=\"pl-k\">=</span> matmul(x, w)                           <span class=\"pl-c\"><span class=\"pl-c\">#</span> y.shape == (num_args, batch_size, output_size / num_args)</span>\ny <span class=\"pl-k\">=</span> transpose(y, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])                <span class=\"pl-c\"><span class=\"pl-c\">#</span> y.shape == (batch_size, num_args, output_size / num_args)</span>\ny <span class=\"pl-k\">=</span> reshape(y, [batch_size, output_size])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> y.shape == (batch_size, output_size)</span></pre></div>\n<p><strong>Tests.</strong><br>\nI ran <code>tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_impl_test.py</code>, but it was failing all over the place. I expected this, because the <code>_linear</code> is used in many of the rnn cell classes. The tests were failing in more places than just the place that covered <code>_linear</code>. Let me know if I need to update the unit tests too. Also, the change in the PR put somewhat more stringent constraints on the shapes of the input <code>args</code>, i.e. they must all be the same. In the previous implementation, only the axis-0 sizes (<code>batch_size</code>) needed to be the same. We could use padding and slicing if we need to be able to handle different axis-1 sizes (<code>input_size</code>).</p>", "body_text": "In the _linear function (from the tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl module), there's relatively subtle bug. The reason that this bug is subtle is that it only affects tensors used by _linear internally.\nFor future reference, the signature and docstring of _linear are:\ndef _linear(args, output_size, bias, bias_start=0.0):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"\nThe Problem.\nFrom the docsting of _linear, the expected behavior of the function is that it should return an output y such that y = sum_i(args[i] * W[i]), indicating a batch-wise matmul (formerly known as batch_matmul). This wasn't actually how _linear was implemented, though. Instead, all args were concatenated and the weights were chosen to have rank 2, i.e.\nx = concat(args, axis=1)  # x.shape == (batch_size, total_arg_size)\nw = Variable(...)         # w.shape == (total_arg_size, output_size)\ny = matmul(x, w)          # y.shape == (batch_size, output_size)\nNow, the main problem here is that w is not block-diagonal, which means that it contains more entries than it should. There are two problems with this:\n\nThe number of weights is too large by a factor of num_args.\nThe extra \"off-diagonal\" weights yield spurious connections in the net architecture of e.g. LSTMCell.\n\nThe Solution.\nThe solution is to use batch-wise matmul instead. In order to do this, we need x and w to be rank-3 (rather than rank-2) tensors, e.g.\nx = stack(args, axis=0)                    # x.shape == (num_args, batch_size, input_size)\nw = Variable(...)                          # w.shape == (num_args, input_size, output_size / num_args)\ny = matmul(x, w)                           # y.shape == (num_args, batch_size, output_size / num_args)\ny = transpose(y, [1, 0, 2])                # y.shape == (batch_size, num_args, output_size / num_args)\ny = reshape(y, [batch_size, output_size])  # y.shape == (batch_size, output_size)\nTests.\nI ran tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_impl_test.py, but it was failing all over the place. I expected this, because the _linear is used in many of the rnn cell classes. The tests were failing in more places than just the place that covered _linear. Let me know if I need to update the unit tests too. Also, the change in the PR put somewhat more stringent constraints on the shapes of the input args, i.e. they must all be the same. In the previous implementation, only the axis-0 sizes (batch_size) needed to be the same. We could use padding and slicing if we need to be able to handle different axis-1 sizes (input_size).", "body": "In the `_linear` function (from the `tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl` module), there's relatively subtle bug. The reason that this bug is subtle is that it only affects tensors used by `_linear` internally.\r\n\r\nFor future reference, the signature and docstring of `_linear` are:\r\n```python\r\ndef _linear(args, output_size, bias, bias_start=0.0):\r\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\r\n\r\n  Args:\r\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\r\n    output_size: int, second dimension of W[i].\r\n    bias: boolean, whether to add a bias term or not.\r\n    bias_start: starting value to initialize the bias; 0 by default.\r\n\r\n  Returns:\r\n    A 2D Tensor with shape [batch x output_size] equal to\r\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\r\n\r\n  Raises:\r\n    ValueError: if some of the arguments has unspecified or wrong shape.\r\n  \"\"\"\r\n```\r\n\r\n\r\n**The Problem.**\r\nFrom the docsting of `_linear`, the expected behavior of the function is that it should return an output `y` such that `y = sum_i(args[i] * W[i])`, indicating a batch-wise `matmul` (formerly known as `batch_matmul`). This wasn't actually how `_linear` was implemented, though. Instead, all `args` were concatenated and the weights were chosen to have rank 2, i.e.\r\n```python\r\nx = concat(args, axis=1)  # x.shape == (batch_size, total_arg_size)\r\nw = Variable(...)         # w.shape == (total_arg_size, output_size)\r\ny = matmul(x, w)          # y.shape == (batch_size, output_size)\r\n```\r\nNow, the main problem here is that **`w` is not block-diagonal**, which means that it contains more entries than it should. There are two problems with this:\r\n1. The **number of weights is too large** by a factor of `num_args`.\r\n2. The extra \"off-diagonal\" weights yield **spurious connections** in the net architecture of e.g. LSTMCell.\r\n\r\n**The Solution.**\r\nThe solution is to use batch-wise `matmul` instead. In order to do this, we need `x` and `w` to be rank-3 (rather than rank-2) tensors, e.g.\r\n```python\r\nx = stack(args, axis=0)                    # x.shape == (num_args, batch_size, input_size)\r\nw = Variable(...)                          # w.shape == (num_args, input_size, output_size / num_args)\r\ny = matmul(x, w)                           # y.shape == (num_args, batch_size, output_size / num_args)\r\ny = transpose(y, [1, 0, 2])                # y.shape == (batch_size, num_args, output_size / num_args)\r\ny = reshape(y, [batch_size, output_size])  # y.shape == (batch_size, output_size)\r\n```\r\n\r\n**Tests.**\r\nI ran `tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_impl_test.py`, but it was failing all over the place. I expected this, because the `_linear` is used in many of the rnn cell classes. The tests were failing in more places than just the place that covered `_linear`. Let me know if I need to update the unit tests too. Also, the change in the PR put somewhat more stringent constraints on the shapes of the input `args`, i.e. they must all be the same. In the previous implementation, only the axis-0 sizes (`batch_size`) needed to be the same. We could use padding and slicing if we need to be able to handle different axis-1 sizes (`input_size`)."}