{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316601503", "html_url": "https://github.com/tensorflow/tensorflow/pull/9909#issuecomment-316601503", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9909", "id": 316601503, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjYwMTUwMw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-20T05:34:23Z", "updated_at": "2017-07-20T05:34:23Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Perhaps it's confusing because we are right multiplying by the parameters,\nunlike the typical format?  Tf data is row major and batch is the major\ndimension, which is why everything is transposed.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Jul 19, 2017 10:28 PM, \"ebrevdo\" ***@***.***&gt; wrote:\n My reading of the equations is that the input and hidden state should be\n mixed in this way at each time step\n\n <a href=\"https://en.m.wikipedia.org/wiki/Long_short-term_memory\">https://en.m.wikipedia.org/wiki/Long_short-term_memory</a>\n\n We also compare to cudnn implementation here and ensure we have identical\n values within machine precision:\n\n <a href=\"https://github.com/tensorflow/tensorflow/blob/master/\">https://github.com/tensorflow/tensorflow/blob/master/</a>\n tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py\n\n Function testCudnnCompatibleRnnCells (the lstm compatible cell is just a\n subclass of LSTMCell with forget_bias=0).\n\n On Jul 19, 2017 10:08 PM, \"Kristian Holsheimer\" ***@***.***&gt;\n wrote:\n\n &gt; <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; The docstring isn't the thing\n &gt; that's wrong here. The reason is that it's used by e.g. LSTMCell, which\n &gt; requires the behavior mentioned in the docsting, *not* the implementation\n &gt; that you suggest. If we keep the implementation like this, we are\n diverting\n &gt; from the standard LSTM architecture in two ways:\n &gt;\n &gt; 1. We use len(args) more weights\n &gt; 2. We allow for mixing between input and hidden state at each time step\n &gt;\n &gt; I suspect that the real reason we're doing this concatenation thing is\n &gt; (time) optimization. So, if we're willing to have a non-standard\n definition\n &gt; of LSTM in tensorflow, we could decide to leave it like this, but *we\n &gt; need to be honest and make this choice consciously*, not as an unintended\n &gt; by-product of a low-level optimization trick.\n &gt;\n &gt; For the record, my choice would be to use the definition of LSTM that is\n &gt; used by the rest of the community (as per this pull request).\n &gt;\n &gt; \u2014\n &gt; You are receiving this because you were mentioned.\n &gt; Reply to this email directly, view it on GitHub\n &gt; &lt;<a href=\"https://github.com/tensorflow/tensorflow/pull/\">https://github.com/tensorflow/tensorflow/pull/</a>\n 9909#issuecomment-316598058&gt;,\n &gt; or mute the thread\n &gt; &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_\">https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_</a>\n 3FB6PvGvJL8-ks5sPuDGgaJpZM4Naw8b&gt;\n &gt; .\n &gt;\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"228629023\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9909\" href=\"https://github.com/tensorflow/tensorflow/pull/9909#issuecomment-316600605\">#9909 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim5norvEi8o3RDemFE3pwShzFiKC4ks5sPuWZgaJpZM4Naw8b\">https://github.com/notifications/unsubscribe-auth/ABtim5norvEi8o3RDemFE3pwShzFiKC4ks5sPuWZgaJpZM4Naw8b</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Perhaps it's confusing because we are right multiplying by the parameters,\nunlike the typical format?  Tf data is row major and batch is the major\ndimension, which is why everything is transposed.\n\u2026\nOn Jul 19, 2017 10:28 PM, \"ebrevdo\" ***@***.***> wrote:\n My reading of the equations is that the input and hidden state should be\n mixed in this way at each time step\n\n https://en.m.wikipedia.org/wiki/Long_short-term_memory\n\n We also compare to cudnn implementation here and ensure we have identical\n values within machine precision:\n\n https://github.com/tensorflow/tensorflow/blob/master/\n tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py\n\n Function testCudnnCompatibleRnnCells (the lstm compatible cell is just a\n subclass of LSTMCell with forget_bias=0).\n\n On Jul 19, 2017 10:08 PM, \"Kristian Holsheimer\" ***@***.***>\n wrote:\n\n > @ebrevdo <https://github.com/ebrevdo> The docstring isn't the thing\n > that's wrong here. The reason is that it's used by e.g. LSTMCell, which\n > requires the behavior mentioned in the docsting, *not* the implementation\n > that you suggest. If we keep the implementation like this, we are\n diverting\n > from the standard LSTM architecture in two ways:\n >\n > 1. We use len(args) more weights\n > 2. We allow for mixing between input and hidden state at each time step\n >\n > I suspect that the real reason we're doing this concatenation thing is\n > (time) optimization. So, if we're willing to have a non-standard\n definition\n > of LSTM in tensorflow, we could decide to leave it like this, but *we\n > need to be honest and make this choice consciously*, not as an unintended\n > by-product of a low-level optimization trick.\n >\n > For the record, my choice would be to use the definition of LSTM that is\n > used by the rest of the community (as per this pull request).\n >\n > \u2014\n > You are receiving this because you were mentioned.\n > Reply to this email directly, view it on GitHub\n > <https://github.com/tensorflow/tensorflow/pull/\n 9909#issuecomment-316598058>,\n > or mute the thread\n > <https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_\n 3FB6PvGvJL8-ks5sPuDGgaJpZM4Naw8b>\n > .\n >\n\n \u2014\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n <#9909 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim5norvEi8o3RDemFE3pwShzFiKC4ks5sPuWZgaJpZM4Naw8b>\n .", "body": "Perhaps it's confusing because we are right multiplying by the parameters,\nunlike the typical format?  Tf data is row major and batch is the major\ndimension, which is why everything is transposed.\n\nOn Jul 19, 2017 10:28 PM, \"ebrevdo\" <notifications@github.com> wrote:\n\n> My reading of the equations is that the input and hidden state should be\n> mixed in this way at each time step\n>\n> https://en.m.wikipedia.org/wiki/Long_short-term_memory\n>\n> We also compare to cudnn implementation here and ensure we have identical\n> values within machine precision:\n>\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/contrib/cudnn_rnn/python/kernel_tests/cudnn_rnn_ops_test.py\n>\n> Function testCudnnCompatibleRnnCells (the lstm compatible cell is just a\n> subclass of LSTMCell with forget_bias=0).\n>\n> On Jul 19, 2017 10:08 PM, \"Kristian Holsheimer\" <notifications@github.com>\n> wrote:\n>\n> > @ebrevdo <https://github.com/ebrevdo> The docstring isn't the thing\n> > that's wrong here. The reason is that it's used by e.g. LSTMCell, which\n> > requires the behavior mentioned in the docsting, *not* the implementation\n> > that you suggest. If we keep the implementation like this, we are\n> diverting\n> > from the standard LSTM architecture in two ways:\n> >\n> > 1. We use len(args) more weights\n> > 2. We allow for mixing between input and hidden state at each time step\n> >\n> > I suspect that the real reason we're doing this concatenation thing is\n> > (time) optimization. So, if we're willing to have a non-standard\n> definition\n> > of LSTM in tensorflow, we could decide to leave it like this, but *we\n> > need to be honest and make this choice consciously*, not as an unintended\n> > by-product of a low-level optimization trick.\n> >\n> > For the record, my choice would be to use the definition of LSTM that is\n> > used by the rest of the community (as per this pull request).\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/pull/\n> 9909#issuecomment-316598058>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/ABtim8lt8GbL5EQcLSY_\n> 3FB6PvGvJL8-ks5sPuDGgaJpZM4Naw8b>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9909#issuecomment-316600605>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5norvEi8o3RDemFE3pwShzFiKC4ks5sPuWZgaJpZM4Naw8b>\n> .\n>\n"}