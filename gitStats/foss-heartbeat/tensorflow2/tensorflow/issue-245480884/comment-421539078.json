{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/421539078", "html_url": "https://github.com/tensorflow/tensorflow/issues/11756#issuecomment-421539078", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11756", "id": 421539078, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTUzOTA3OA==", "user": {"login": "SijanC147", "id": 13831460, "node_id": "MDQ6VXNlcjEzODMxNDYw", "avatar_url": "https://avatars3.githubusercontent.com/u/13831460?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SijanC147", "html_url": "https://github.com/SijanC147", "followers_url": "https://api.github.com/users/SijanC147/followers", "following_url": "https://api.github.com/users/SijanC147/following{/other_user}", "gists_url": "https://api.github.com/users/SijanC147/gists{/gist_id}", "starred_url": "https://api.github.com/users/SijanC147/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SijanC147/subscriptions", "organizations_url": "https://api.github.com/users/SijanC147/orgs", "repos_url": "https://api.github.com/users/SijanC147/repos", "events_url": "https://api.github.com/users/SijanC147/events{/privacy}", "received_events_url": "https://api.github.com/users/SijanC147/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-15T07:48:41Z", "updated_at": "2018-09-15T07:49:00Z", "author_association": "NONE", "body_html": "<p>Unfortunately, as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> mentioned the <code>masked_softmax</code> implementation by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12462956\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bstriner\">@bstriner</a> broke for me when computing gradients, producing <code>NaN</code>s in computing the loss.</p>\n<p>A simple workaround that got it working for me was replacing <code>np.inf</code> with <code>tf.float32.max</code>. This, of course, incurs some penalty as the padded values will not be <em>completely</em> negligible, but I think it is the most numerically stable approach.</p>\n<p>I'm also asking if there are any other downsides to this approach as I'm only just starting out with Tensorflow and Machine Learning in general, so I'd appreciate knowing if this approach is actually breaking anything.</p>", "body_text": "Unfortunately, as @yaroslavvb mentioned the masked_softmax implementation by @bstriner broke for me when computing gradients, producing NaNs in computing the loss.\nA simple workaround that got it working for me was replacing np.inf with tf.float32.max. This, of course, incurs some penalty as the padded values will not be completely negligible, but I think it is the most numerically stable approach.\nI'm also asking if there are any other downsides to this approach as I'm only just starting out with Tensorflow and Machine Learning in general, so I'd appreciate knowing if this approach is actually breaking anything.", "body": "Unfortunately, as @yaroslavvb mentioned the `masked_softmax` implementation by @bstriner broke for me when computing gradients, producing `NaN`s in computing the loss. \r\n\r\nA simple workaround that got it working for me was replacing `np.inf` with `tf.float32.max`. This, of course, incurs some penalty as the padded values will not be _completely_ negligible, but I think it is the most numerically stable approach. \r\n\r\nI'm also asking if there are any other downsides to this approach as I'm only just starting out with Tensorflow and Machine Learning in general, so I'd appreciate knowing if this approach is actually breaking anything. "}