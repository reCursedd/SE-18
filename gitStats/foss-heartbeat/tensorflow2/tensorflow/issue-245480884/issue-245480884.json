{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11756", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11756/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11756/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11756/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11756", "id": 245480884, "node_id": "MDU6SXNzdWUyNDU0ODA4ODQ=", "number": 11756, "title": "Infinity mask breaks gradient", "user": {"login": "hongzimao", "id": 5906100, "node_id": "MDQ6VXNlcjU5MDYxMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/5906100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hongzimao", "html_url": "https://github.com/hongzimao", "followers_url": "https://api.github.com/users/hongzimao/followers", "following_url": "https://api.github.com/users/hongzimao/following{/other_user}", "gists_url": "https://api.github.com/users/hongzimao/gists{/gist_id}", "starred_url": "https://api.github.com/users/hongzimao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hongzimao/subscriptions", "organizations_url": "https://api.github.com/users/hongzimao/orgs", "repos_url": "https://api.github.com/users/hongzimao/repos", "events_url": "https://api.github.com/users/hongzimao/events{/privacy}", "received_events_url": "https://api.github.com/users/hongzimao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-07-25T17:46:52Z", "updated_at": "2018-10-29T16:10:03Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm trying to do softmax over selected indices, using infinity mask to silent out the unwanted ones. However, the gradient of those unwanted entires become nan as opposed to 0.</p>\n<p>The reason I didn't use boolean mask is that the mask indices are different in my batch, which can't end up with a nice matrix form. If there's workaround here I'll be more than happy to adopt.</p>\n<p>The code I tested the infinity mask is</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\na = tf.placeholder(tf.float32, [5])\ninf_mask = tf.placeholder(tf.float32, [5])\n\nb = tf.multiply(a, inf_mask)\nsf = tf.nn.softmax(b)\n\nloss = (sf[2] - 0)\ngrad = tf.gradients(loss, a)\n\nsess = tf.Session()\n\na_np = np.ones([5])\nnp_mask = np.ones([5]) * 4\nnp_mask[1] = -np.inf\n\nprint sess.run([sf, grad], feed_dict={\n    a: a_np,\n    inf_mask: np_mask\n})\n\nsess.close()\n</code></pre>\n<p>The output is <code>[array([ 0.25,  0.  ,  0.25,  0.25,  0.25], dtype=float32), [array([-0.25,   nan,  0.75, -0.25, -0.25], dtype=float32)]]</code></p>\n<p>The mask is working but the gradient has a nan, which should have been 0 I think.</p>", "body_text": "I'm trying to do softmax over selected indices, using infinity mask to silent out the unwanted ones. However, the gradient of those unwanted entires become nan as opposed to 0.\nThe reason I didn't use boolean mask is that the mask indices are different in my batch, which can't end up with a nice matrix form. If there's workaround here I'll be more than happy to adopt.\nThe code I tested the infinity mask is\nimport numpy as np\nimport tensorflow as tf\n\na = tf.placeholder(tf.float32, [5])\ninf_mask = tf.placeholder(tf.float32, [5])\n\nb = tf.multiply(a, inf_mask)\nsf = tf.nn.softmax(b)\n\nloss = (sf[2] - 0)\ngrad = tf.gradients(loss, a)\n\nsess = tf.Session()\n\na_np = np.ones([5])\nnp_mask = np.ones([5]) * 4\nnp_mask[1] = -np.inf\n\nprint sess.run([sf, grad], feed_dict={\n    a: a_np,\n    inf_mask: np_mask\n})\n\nsess.close()\n\nThe output is [array([ 0.25,  0.  ,  0.25,  0.25,  0.25], dtype=float32), [array([-0.25,   nan,  0.75, -0.25, -0.25], dtype=float32)]]\nThe mask is working but the gradient has a nan, which should have been 0 I think.", "body": "I'm trying to do softmax over selected indices, using infinity mask to silent out the unwanted ones. However, the gradient of those unwanted entires become nan as opposed to 0.\r\n\r\nThe reason I didn't use boolean mask is that the mask indices are different in my batch, which can't end up with a nice matrix form. If there's workaround here I'll be more than happy to adopt.\r\n\r\nThe code I tested the infinity mask is\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = tf.placeholder(tf.float32, [5])\r\ninf_mask = tf.placeholder(tf.float32, [5])\r\n\r\nb = tf.multiply(a, inf_mask)\r\nsf = tf.nn.softmax(b)\r\n\r\nloss = (sf[2] - 0)\r\ngrad = tf.gradients(loss, a)\r\n\r\nsess = tf.Session()\r\n\r\na_np = np.ones([5])\r\nnp_mask = np.ones([5]) * 4\r\nnp_mask[1] = -np.inf\r\n\r\nprint sess.run([sf, grad], feed_dict={\r\n    a: a_np,\r\n    inf_mask: np_mask\r\n})\r\n\r\nsess.close()\r\n```\r\n\r\nThe output is `[array([ 0.25,  0.  ,  0.25,  0.25,  0.25], dtype=float32), [array([-0.25,   nan,  0.75, -0.25, -0.25], dtype=float32)]]`\r\n\r\nThe mask is working but the gradient has a nan, which should have been 0 I think."}