{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/433971494", "html_url": "https://github.com/tensorflow/tensorflow/issues/11756#issuecomment-433971494", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11756", "id": 433971494, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzk3MTQ5NA==", "user": {"login": "bstriner", "id": 12462956, "node_id": "MDQ6VXNlcjEyNDYyOTU2", "avatar_url": "https://avatars3.githubusercontent.com/u/12462956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bstriner", "html_url": "https://github.com/bstriner", "followers_url": "https://api.github.com/users/bstriner/followers", "following_url": "https://api.github.com/users/bstriner/following{/other_user}", "gists_url": "https://api.github.com/users/bstriner/gists{/gist_id}", "starred_url": "https://api.github.com/users/bstriner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bstriner/subscriptions", "organizations_url": "https://api.github.com/users/bstriner/orgs", "repos_url": "https://api.github.com/users/bstriner/repos", "events_url": "https://api.github.com/users/bstriner/events{/privacy}", "received_events_url": "https://api.github.com/users/bstriner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-29T16:10:03Z", "updated_at": "2018-10-29T16:10:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=30319064\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/NickRyder\">@NickRyder</a> You can adapt the sparse_logsoftmax below. Inputs are dense logits and sparse indices. It gives you the normalized logits in a dense matrix. You can then use the sparse_crossentropy_loss below to get the logits at the labels.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">sparse_logsoftmax</span>(<span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">idx</span>):\n    dense_shape <span class=\"pl-k\">=</span> tf.cast(tf.shape(logits), tf.int64)\n    logits_values <span class=\"pl-k\">=</span> tf.gather_nd(<span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>logits, <span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>idx)\n    sparse_logits <span class=\"pl-k\">=</span> tf.SparseTensor(<span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>idx, <span class=\"pl-v\">values</span><span class=\"pl-k\">=</span>logits_values, <span class=\"pl-v\">dense_shape</span><span class=\"pl-k\">=</span>dense_shape)\n    lmax <span class=\"pl-k\">=</span> tf.sparse_reduce_max(<span class=\"pl-v\">sp_input</span><span class=\"pl-k\">=</span>sparse_logits, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">keep_dims</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    lmax <span class=\"pl-k\">=</span> tf.stop_gradient(lmax)\n    normed_logits <span class=\"pl-k\">=</span> logits <span class=\"pl-k\">-</span> lmax\n    normed_exp_values <span class=\"pl-k\">=</span> tf.exp(tf.gather_nd(<span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>normed_logits, <span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>idx))\n    sparse_normed_exp <span class=\"pl-k\">=</span> tf.SparseTensor(<span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>idx, <span class=\"pl-v\">values</span><span class=\"pl-k\">=</span>normed_exp_values, <span class=\"pl-v\">dense_shape</span><span class=\"pl-k\">=</span>dense_shape)\n    normed_sum <span class=\"pl-k\">=</span> tf.log(tf.sparse_reduce_sum(<span class=\"pl-v\">sp_input</span><span class=\"pl-k\">=</span>sparse_normed_exp, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">keep_dims</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)) <span class=\"pl-k\">+</span> lmax\n    lsm <span class=\"pl-k\">=</span> logits <span class=\"pl-k\">-</span> normed_sum\n    <span class=\"pl-k\">return</span> lsm\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">sparse_crossentropy_loss</span>(<span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">labels</span>):\n    n <span class=\"pl-k\">=</span> tf.shape(labels)[<span class=\"pl-c1\">0</span>]\n    idx <span class=\"pl-k\">=</span> tf.stack((tf.range(n), labels), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    nll <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span> tf.reduce_mean(tf.gather_nd(<span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>logits, <span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>idx))\n    <span class=\"pl-k\">return</span> nll</pre></div>", "body_text": "@NickRyder You can adapt the sparse_logsoftmax below. Inputs are dense logits and sparse indices. It gives you the normalized logits in a dense matrix. You can then use the sparse_crossentropy_loss below to get the logits at the labels.\ndef sparse_logsoftmax(logits, idx):\n    dense_shape = tf.cast(tf.shape(logits), tf.int64)\n    logits_values = tf.gather_nd(params=logits, indices=idx)\n    sparse_logits = tf.SparseTensor(indices=idx, values=logits_values, dense_shape=dense_shape)\n    lmax = tf.sparse_reduce_max(sp_input=sparse_logits, axis=-1, keep_dims=True)\n    lmax = tf.stop_gradient(lmax)\n    normed_logits = logits - lmax\n    normed_exp_values = tf.exp(tf.gather_nd(params=normed_logits, indices=idx))\n    sparse_normed_exp = tf.SparseTensor(indices=idx, values=normed_exp_values, dense_shape=dense_shape)\n    normed_sum = tf.log(tf.sparse_reduce_sum(sp_input=sparse_normed_exp, axis=-1, keep_dims=True)) + lmax\n    lsm = logits - normed_sum\n    return lsm\n\n\ndef sparse_crossentropy_loss(logits, labels):\n    n = tf.shape(labels)[0]\n    idx = tf.stack((tf.range(n), labels), axis=-1)\n    nll = - tf.reduce_mean(tf.gather_nd(params=logits, indices=idx))\n    return nll", "body": "@NickRyder You can adapt the sparse_logsoftmax below. Inputs are dense logits and sparse indices. It gives you the normalized logits in a dense matrix. You can then use the sparse_crossentropy_loss below to get the logits at the labels.\r\n\r\n```python\r\ndef sparse_logsoftmax(logits, idx):\r\n    dense_shape = tf.cast(tf.shape(logits), tf.int64)\r\n    logits_values = tf.gather_nd(params=logits, indices=idx)\r\n    sparse_logits = tf.SparseTensor(indices=idx, values=logits_values, dense_shape=dense_shape)\r\n    lmax = tf.sparse_reduce_max(sp_input=sparse_logits, axis=-1, keep_dims=True)\r\n    lmax = tf.stop_gradient(lmax)\r\n    normed_logits = logits - lmax\r\n    normed_exp_values = tf.exp(tf.gather_nd(params=normed_logits, indices=idx))\r\n    sparse_normed_exp = tf.SparseTensor(indices=idx, values=normed_exp_values, dense_shape=dense_shape)\r\n    normed_sum = tf.log(tf.sparse_reduce_sum(sp_input=sparse_normed_exp, axis=-1, keep_dims=True)) + lmax\r\n    lsm = logits - normed_sum\r\n    return lsm\r\n\r\n\r\ndef sparse_crossentropy_loss(logits, labels):\r\n    n = tf.shape(labels)[0]\r\n    idx = tf.stack((tf.range(n), labels), axis=-1)\r\n    nll = - tf.reduce_mean(tf.gather_nd(params=logits, indices=idx))\r\n    return nll\r\n```"}