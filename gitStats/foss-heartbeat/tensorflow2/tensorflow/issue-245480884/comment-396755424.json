{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/396755424", "html_url": "https://github.com/tensorflow/tensorflow/issues/11756#issuecomment-396755424", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11756", "id": 396755424, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Njc1NTQyNA==", "user": {"login": "bstriner", "id": 12462956, "node_id": "MDQ6VXNlcjEyNDYyOTU2", "avatar_url": "https://avatars3.githubusercontent.com/u/12462956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bstriner", "html_url": "https://github.com/bstriner", "followers_url": "https://api.github.com/users/bstriner/followers", "following_url": "https://api.github.com/users/bstriner/following{/other_user}", "gists_url": "https://api.github.com/users/bstriner/gists{/gist_id}", "starred_url": "https://api.github.com/users/bstriner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bstriner/subscriptions", "organizations_url": "https://api.github.com/users/bstriner/orgs", "repos_url": "https://api.github.com/users/bstriner/repos", "events_url": "https://api.github.com/users/bstriner/events{/privacy}", "received_events_url": "https://api.github.com/users/bstriner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-12T22:31:24Z", "updated_at": "2018-06-12T22:31:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5906100\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hongzimao\">@hongzimao</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=36339764\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sy2737\">@sy2737</a> I think you guys were on the right track originally, just didn't debug things quite correctly. You wanted <code>a-inf_mask</code>, not multiply. The second solution posted above is still dangerous. stable softmax should be <code>e^(a-max(a))</code>.</p>\n<p>The key is that <code>exp(-inf)==0</code>, <code>max(a, -inf)==a</code> and <code>a-inf==-inf</code>. Unfortunately, <code>0*inf==nan</code>, so making the mask correctly is tricky.</p>\n<p>Two most numerically stable options would be either -inf mask or just using a sparse softmax (which might be better depending on what you are doing).</p>\n<p>Below is an example of using -inf mask. It has some specifics because of broadcasting but you should be able to make it into whatever you need. Note that if your intention is to use this for loss calculations, you should be doing something else. Should only be using softmax itself for things like attention.</p>\n<ul>\n<li>Use <code>tf.sequence_mask</code> to create a mask from sequence lengths</li>\n<li>Create an infinity mask (this is the ugly part)<br>\n-- <code>tf.where</code> to get the indices<br>\n-- <code>tf.tile</code> to make as many infs as required (broadcasting doesn't seem to work)<br>\n-- <code>tf.scatter_nd</code> to make the mask using the indices and the infs</li>\n<li>Then just <code>tf.nn.softmax(logits - infmask, axis=1)</code></li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">masked_softmax</span>(<span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">mask</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Masked softmax over dim 1, mask broadcasts over dim 2</span>\n<span class=\"pl-s\">    :param logits: (N, L, T)</span>\n<span class=\"pl-s\">    :param mask: (N, L)</span>\n<span class=\"pl-s\">    :return: probabilities (N, L, T)</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    v <span class=\"pl-k\">=</span> tf.shape(logits)[<span class=\"pl-c1\">2</span>]\n    indices <span class=\"pl-k\">=</span> tf.cast(tf.where(tf.logical_not(mask)), tf.int32)\n    inf <span class=\"pl-k\">=</span> tf.constant(np.array([[np.inf]], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    infs <span class=\"pl-k\">=</span> tf.tile(inf, [tf.shape(indices)[<span class=\"pl-c1\">0</span>], v])\n    infmask <span class=\"pl-k\">=</span> tf.scatter_nd(\n        <span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>indices,\n        <span class=\"pl-v\">updates</span><span class=\"pl-k\">=</span>infs,\n        <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>tf.shape(logits))\n    _p <span class=\"pl-k\">=</span> tf.nn.softmax(logits <span class=\"pl-k\">-</span> infmask, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">return</span> _p</pre></div>", "body_text": "@hongzimao @sy2737 I think you guys were on the right track originally, just didn't debug things quite correctly. You wanted a-inf_mask, not multiply. The second solution posted above is still dangerous. stable softmax should be e^(a-max(a)).\nThe key is that exp(-inf)==0, max(a, -inf)==a and a-inf==-inf. Unfortunately, 0*inf==nan, so making the mask correctly is tricky.\nTwo most numerically stable options would be either -inf mask or just using a sparse softmax (which might be better depending on what you are doing).\nBelow is an example of using -inf mask. It has some specifics because of broadcasting but you should be able to make it into whatever you need. Note that if your intention is to use this for loss calculations, you should be doing something else. Should only be using softmax itself for things like attention.\n\nUse tf.sequence_mask to create a mask from sequence lengths\nCreate an infinity mask (this is the ugly part)\n-- tf.where to get the indices\n-- tf.tile to make as many infs as required (broadcasting doesn't seem to work)\n-- tf.scatter_nd to make the mask using the indices and the infs\nThen just tf.nn.softmax(logits - infmask, axis=1)\n\ndef masked_softmax(logits, mask):\n    \"\"\"\n    Masked softmax over dim 1, mask broadcasts over dim 2\n    :param logits: (N, L, T)\n    :param mask: (N, L)\n    :return: probabilities (N, L, T)\n    \"\"\"\n    v = tf.shape(logits)[2]\n    indices = tf.cast(tf.where(tf.logical_not(mask)), tf.int32)\n    inf = tf.constant(np.array([[np.inf]], dtype=np.float32), dtype=tf.float32)\n    infs = tf.tile(inf, [tf.shape(indices)[0], v])\n    infmask = tf.scatter_nd(\n        indices=indices,\n        updates=infs,\n        shape=tf.shape(logits))\n    _p = tf.nn.softmax(logits - infmask, axis=1)\n    return _p", "body": "@hongzimao @sy2737 I think you guys were on the right track originally, just didn't debug things quite correctly. You wanted `a-inf_mask`, not multiply. The second solution posted above is still dangerous. stable softmax should be `e^(a-max(a))`.\r\n\r\nThe key is that `exp(-inf)==0`, `max(a, -inf)==a` and `a-inf==-inf`. Unfortunately, `0*inf==nan`, so making the mask correctly is tricky.\r\n\r\nTwo most numerically stable options would be either -inf mask or just using a sparse softmax (which might be better depending on what you are doing). \r\n\r\nBelow is an example of using -inf mask. It has some specifics because of broadcasting but you should be able to make it into whatever you need. Note that if your intention is to use this for loss calculations, you should be doing something else. Should only be using softmax itself for things like attention.\r\n- Use `tf.sequence_mask` to create a mask from sequence lengths\r\n- Create an infinity mask (this is the ugly part)\r\n-- `tf.where` to get the indices\r\n-- `tf.tile` to make as many infs as required (broadcasting doesn't seem to work)\r\n-- `tf.scatter_nd` to make the mask using the indices and the infs\r\n- Then just `tf.nn.softmax(logits - infmask, axis=1)`\r\n\r\n```python\r\ndef masked_softmax(logits, mask):\r\n    \"\"\"\r\n    Masked softmax over dim 1, mask broadcasts over dim 2\r\n    :param logits: (N, L, T)\r\n    :param mask: (N, L)\r\n    :return: probabilities (N, L, T)\r\n    \"\"\"\r\n    v = tf.shape(logits)[2]\r\n    indices = tf.cast(tf.where(tf.logical_not(mask)), tf.int32)\r\n    inf = tf.constant(np.array([[np.inf]], dtype=np.float32), dtype=tf.float32)\r\n    infs = tf.tile(inf, [tf.shape(indices)[0], v])\r\n    infmask = tf.scatter_nd(\r\n        indices=indices,\r\n        updates=infs,\r\n        shape=tf.shape(logits))\r\n    _p = tf.nn.softmax(logits - infmask, axis=1)\r\n    return _p\r\n```\r\n\r\n"}