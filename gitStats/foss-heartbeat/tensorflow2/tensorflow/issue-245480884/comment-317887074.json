{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317887074", "html_url": "https://github.com/tensorflow/tensorflow/issues/11756#issuecomment-317887074", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11756", "id": 317887074, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzg4NzA3NA==", "user": {"login": "hongzimao", "id": 5906100, "node_id": "MDQ6VXNlcjU5MDYxMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/5906100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hongzimao", "html_url": "https://github.com/hongzimao", "followers_url": "https://api.github.com/users/hongzimao/followers", "following_url": "https://api.github.com/users/hongzimao/following{/other_user}", "gists_url": "https://api.github.com/users/hongzimao/gists{/gist_id}", "starred_url": "https://api.github.com/users/hongzimao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hongzimao/subscriptions", "organizations_url": "https://api.github.com/users/hongzimao/orgs", "repos_url": "https://api.github.com/users/hongzimao/repos", "events_url": "https://api.github.com/users/hongzimao/events{/privacy}", "received_events_url": "https://api.github.com/users/hongzimao/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-25T22:06:13Z", "updated_at": "2017-07-25T22:08:57Z", "author_association": "NONE", "body_html": "<p>Yes, there's a solution on StackOverflow that uses <code>np.finfo(np.float32).min</code> instead of <code>-np.inf</code> <a href=\"https://stackoverflow.com/questions/45310221/tensorflow-infinity-mask-breaks-gradient/\" rel=\"nofollow\">https://stackoverflow.com/questions/45310221/tensorflow-infinity-mask-breaks-gradient/</a>.</p>\n<p>Also, it seems a mask (a 0-1 mask instead of negative infinity mask) applied <em>after</em> exponential also works.</p>\n<p>I'm not sure how robust these methods are though. Hope someone can clarify. Thanks!</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\na = tf.placeholder(tf.float32, [5])\ninf_mask = tf.placeholder(tf.float32, [5])\nzero_one_mask = tf.placeholder(tf.float32, [5])\nexp_a = tf.exp(a)\n\nb = tf.multiply(exp_a, zero_one_mask)\nsf = b / tf.reduce_sum(b)\n\n# b = tf.multiply(a, inf_mask)\n# sf = tf.nn.softmax(b)\n\nloss = (sf[2] - 0)\ngrad = tf.gradients(loss, a)\n\nsess = tf.Session()\n\na_np = np.ones([5])\nnp_mask = np.ones([5])\nnp_mask[1] = 0\n\nprint sess.run([sf, grad], feed_dict={\n    a: a_np,\n    zero_one_mask: np_mask\n})\n\nsess.close()\n</code></pre>\n<p>The output is <code>[array([ 0.25,  0.  ,  0.25,  0.25,  0.25], dtype=float32), [array([-0.0625    , -0.        ,  0.18750001, -0.0625    , -0.0625    ], dtype=float32)]]</code>.</p>", "body_text": "Yes, there's a solution on StackOverflow that uses np.finfo(np.float32).min instead of -np.inf https://stackoverflow.com/questions/45310221/tensorflow-infinity-mask-breaks-gradient/.\nAlso, it seems a mask (a 0-1 mask instead of negative infinity mask) applied after exponential also works.\nI'm not sure how robust these methods are though. Hope someone can clarify. Thanks!\nimport numpy as np\nimport tensorflow as tf\n\na = tf.placeholder(tf.float32, [5])\ninf_mask = tf.placeholder(tf.float32, [5])\nzero_one_mask = tf.placeholder(tf.float32, [5])\nexp_a = tf.exp(a)\n\nb = tf.multiply(exp_a, zero_one_mask)\nsf = b / tf.reduce_sum(b)\n\n# b = tf.multiply(a, inf_mask)\n# sf = tf.nn.softmax(b)\n\nloss = (sf[2] - 0)\ngrad = tf.gradients(loss, a)\n\nsess = tf.Session()\n\na_np = np.ones([5])\nnp_mask = np.ones([5])\nnp_mask[1] = 0\n\nprint sess.run([sf, grad], feed_dict={\n    a: a_np,\n    zero_one_mask: np_mask\n})\n\nsess.close()\n\nThe output is [array([ 0.25,  0.  ,  0.25,  0.25,  0.25], dtype=float32), [array([-0.0625    , -0.        ,  0.18750001, -0.0625    , -0.0625    ], dtype=float32)]].", "body": "Yes, there's a solution on StackOverflow that uses `np.finfo(np.float32).min` instead of `-np.inf` https://stackoverflow.com/questions/45310221/tensorflow-infinity-mask-breaks-gradient/.\r\n\r\nAlso, it seems a mask (a 0-1 mask instead of negative infinity mask) applied _after_ exponential also works. \r\n\r\nI'm not sure how robust these methods are though. Hope someone can clarify. Thanks!\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = tf.placeholder(tf.float32, [5])\r\ninf_mask = tf.placeholder(tf.float32, [5])\r\nzero_one_mask = tf.placeholder(tf.float32, [5])\r\nexp_a = tf.exp(a)\r\n\r\nb = tf.multiply(exp_a, zero_one_mask)\r\nsf = b / tf.reduce_sum(b)\r\n\r\n# b = tf.multiply(a, inf_mask)\r\n# sf = tf.nn.softmax(b)\r\n\r\nloss = (sf[2] - 0)\r\ngrad = tf.gradients(loss, a)\r\n\r\nsess = tf.Session()\r\n\r\na_np = np.ones([5])\r\nnp_mask = np.ones([5])\r\nnp_mask[1] = 0\r\n\r\nprint sess.run([sf, grad], feed_dict={\r\n    a: a_np,\r\n    zero_one_mask: np_mask\r\n})\r\n\r\nsess.close()\r\n```\r\n\r\nThe output is `[array([ 0.25,  0.  ,  0.25,  0.25,  0.25], dtype=float32), [array([-0.0625    , -0.        ,  0.18750001, -0.0625    , -0.0625    ], dtype=float32)]]`."}