{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318498606", "html_url": "https://github.com/tensorflow/tensorflow/issues/11756#issuecomment-318498606", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11756", "id": 318498606, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODQ5ODYwNg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-27T21:59:59Z", "updated_at": "2017-07-27T22:00:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Once you start feeding in infinities, then I expect there'll be NaN's in gradients. Fundamental reason is that gradients are obtained using simple algebraic transformations which only make sense for finite numbers. The robust way of handling things would be to use boolean masks.</p>\n<p>It looks like \"really large number\" works in this case, although in general things can overflow and give unexpected nan. For instance, the example below produces <code>nan</code> gradient.</p>\n<pre><code>x = tf.placeholder(tf.float32)\ny = tf.exp(x)\nz = tf.exp(-y)\ngrad = tf.gradients(z,[x])[0]\nprint sess.run(grad, feed_dict={x: 1e10})    # =&gt; nan\n</code></pre>", "body_text": "Once you start feeding in infinities, then I expect there'll be NaN's in gradients. Fundamental reason is that gradients are obtained using simple algebraic transformations which only make sense for finite numbers. The robust way of handling things would be to use boolean masks.\nIt looks like \"really large number\" works in this case, although in general things can overflow and give unexpected nan. For instance, the example below produces nan gradient.\nx = tf.placeholder(tf.float32)\ny = tf.exp(x)\nz = tf.exp(-y)\ngrad = tf.gradients(z,[x])[0]\nprint sess.run(grad, feed_dict={x: 1e10})    # => nan", "body": "Once you start feeding in infinities, then I expect there'll be NaN's in gradients. Fundamental reason is that gradients are obtained using simple algebraic transformations which only make sense for finite numbers. The robust way of handling things would be to use boolean masks.\r\n\r\nIt looks like \"really large number\" works in this case, although in general things can overflow and give unexpected nan. For instance, the example below produces `nan` gradient.\r\n\r\n```\r\nx = tf.placeholder(tf.float32)\r\ny = tf.exp(x)\r\nz = tf.exp(-y)\r\ngrad = tf.gradients(z,[x])[0]\r\nprint sess.run(grad, feed_dict={x: 1e10})    # => nan\r\n```\r\n"}