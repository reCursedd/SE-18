{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/415598588", "html_url": "https://github.com/tensorflow/tensorflow/issues/21582#issuecomment-415598588", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21582", "id": 415598588, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNTU5ODU4OA==", "user": {"login": "m3bm3b", "id": 10168793, "node_id": "MDQ6VXNlcjEwMTY4Nzkz", "avatar_url": "https://avatars0.githubusercontent.com/u/10168793?v=4", "gravatar_id": "", "url": "https://api.github.com/users/m3bm3b", "html_url": "https://github.com/m3bm3b", "followers_url": "https://api.github.com/users/m3bm3b/followers", "following_url": "https://api.github.com/users/m3bm3b/following{/other_user}", "gists_url": "https://api.github.com/users/m3bm3b/gists{/gist_id}", "starred_url": "https://api.github.com/users/m3bm3b/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/m3bm3b/subscriptions", "organizations_url": "https://api.github.com/users/m3bm3b/orgs", "repos_url": "https://api.github.com/users/m3bm3b/repos", "events_url": "https://api.github.com/users/m3bm3b/events{/privacy}", "received_events_url": "https://api.github.com/users/m3bm3b/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-23T22:59:54Z", "updated_at": "2018-08-23T22:59:54Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>memory is being allocated in nsync::nsync_waiter_new_(). As I mentioned in my<br>\nfirst post, I can see by inspection of<br>\n<a href=\"https://github.com/google/nsync/blob/master/internal/common.c\">https://github.com/google/nsync/blob/master/internal/common.c</a><br>\nthat the memory is  not deallocated in this case.</p>\n</blockquote>\n<p>Summary:</p>\n<p>valgrind's leak detector is complaining not because there's a leak,<br>\nbut because it doesn't like interior  pointers, which point to places within a<br>\nmalloced region, rather than the start.<br>\nThat's what  \"possibly lost\" in vagrind's report means<br>\n(see <a href=\"http://valgrind.org/docs/manual/valgrind_manual.pdf\" rel=\"nofollow\">http://valgrind.org/docs/manual/valgrind_manual.pdf</a><br>\nand search for \"possibly lost\").<br>\nThe memory is actually still reachable.</p>\n<p>Those blocks are not supposed to be deallocated, but when no longer in use<br>\nare instead put into a free pool and reused, for reasons that I give below.</p>\n<p>nsync uses interior pointers because it puts the same struct on <em>multiple</em><br>\ndoubly-linked lists simultaneously.   It uses pointers that are embedded in the<br>\nstruct (rather than separate list objects) for efficiency:<br>\nit would be bad to touch twice<br>\nas many cache lines while scanning a list.<br>\nThe embedded pointers  for the multiple doubly-linked lists couldn't all be<br>\nto the start of the objects unless the code for linked list manipulation<br>\nwere duplicated (one copy of the code for each type of linked list),<br>\nor took an offset argument to tell the linked-list code<br>\nwhere to find the links within the object.   An alternative would be a<br>\nfake linked list just for valgrind, linking through every such struct.<br>\nAs you can imagine, to add such complication to the code<br>\nto mollify one checker seems a little ugly,<br>\nespecially because the concern it evokes is illusory: there is no leak.<br>\nNotice also valgrind memcheck is being fairly zealous here, because<br>\ninterior pointers are not that uncommon in normal code.  For example,<br>\nnotice this text in  <a href=\"http://valgrind.org/docs/manual/valgrind_manual.pdf\" rel=\"nofollow\">http://valgrind.org/docs/manual/valgrind_manual.pdf</a><br>\n\"The available heuristics provide detection of valid interior pointers to<br>\nstd::stdstring, to new[] allocated arrays with elements having destructors<br>\nand to interior pointers pointing to an inner part of a C++ object using<br>\nmultiple inheritance.\"</p>\n<p>Below are more details than you probably want about<br>\nwhy nsync does thing the way it does.</p>\n<p>The memory involved is a \"waiter\" struct, which is used<br>\nto enqueue a thread for later wakeup by another thread.<br>\nRather than freeing the waiter struct after use, the struct<br>\nis placed into a free pool  either by waiter_destroy() (which is invoked<br>\nby thread exit), or by  nsync_waiter_free_().      The memory in the<br>\npool is reused by calls to nsync_waiter_new_(); no new allocations are done unless<br>\nthe free pool is empty.  No waiter struct is allowed to become unreachable.</p>\n<p>(Aside: Normally, a thread gets one struct and uses it until the thread exits.<br>\nThe complication of having both waiter_destroy() and nsync_waiter_free_()<br>\noccurs because when debugging nsync itself, a single thread could<br>\nuse more than one waiter struct simultaneously.  But we can ignore that here.)</p>\n<p>Thus, the number of waiter structs<br>\nshould be at most the maximum number of concurrent threads.<br>\nThere would be cause for concern only if the leak detector reported<br>\nsubstantially more waiter structs than that<br>\n(e.g., if the programme had used at most tens of<br>\nthreads concurrently, but the leak detector reported hundreds of waiter<br>\nstructs).    Such an eventuality would have suggested that waiter structs<br>\nwere not being reused effectively.   But here, you list a report for only one<br>\nsuch struct, so nothing bad seems to be happening.</p>\n<p>You might wonder why the library doesn't just free the structs,<br>\nrather than putting them on an internal free list.<br>\nThat would be simpler, but to free the memory would slow down the library<br>\nwhen under heavy load.  The reasons are somewhat subtle.</p>\n<p>First, I'll explain how waiter structs are used in the code as it stands,<br>\nand why that requires that the waiter structs are never freed.<br>\nThen I'll explain why it would be slower to have it work in a way that<br>\ndid allow the waiter structs to be freed.</p>\n<p>When nsync needs to block a thread (to wait on a condition variable or a mutex)<br>\nit uses a waiter struct W that has an atomic word \"W.nw.waiting\", primary<br>\nlink list pointers \"W.nw.q\",  and a platform-implemented semaphore \"W.sem\".</p>\n<p>A thread X wishing to wait on some wait queue Q acts as follows:<br>\nX0) finds its a waiter struct W (typically via a pointer in<br>\nthread-local storage),<br>\nX1) sets \"W.nw.waiting\" to non-zero,<br>\nX2) locks Q's spinlock,<br>\nX3) inserts W.nw.q  onto Q,<br>\nX4) unlocks Q spinlock, and<br>\nX5) while (\"W.nw.waiting\" is non-zero)<br>\nX5a) waits on \"W.sem\"</p>\n<p>A thread Y wishing to wake a thread from a queue Q:<br>\nY0) locks Q's spinlock,<br>\nY1) removes a waiter struct W from Q,<br>\nY2) unlocks Q spinlock,<br>\nY3) zeroes \"W.nw.waiting\", and<br>\nY4) wakes \"W.sem\".</p>\n<p>Notice that these actions might interleave legally in the following way:<br>\nX0; X1; X2; X3; X4; Y0; Y1; Y2; Y3; X5<br>\nAt this point thread X is no longer waiting, and does not need to execute X5a.<br>\nNow suppose that thread X returns from the nsync library, and exits,<br>\nand only then does thread Y run Y4, which touches W.sem.</p>\n<p>Clearly, W.sem needs to exist when Y does Y4, or the programme will crash.<br>\nBut W (and thus W.sem) was assigned for the use of thread X, which has exited.<br>\nThis means that with this protocol, thread X must <em>not</em> deallocate the<br>\nwaiter struct it was assigned, even when thread X exits.<br>\nThis is achieved by making waiter structs immortal in the way<br>\ndescribed above; they are never freed, but instead reused.<br>\nThus, it is guaranteed that \"W.sem\" will exist when Y4 happens, even if W is been<br>\nput on the free list, or even reused by another thread X'.   This reuse is still correct,<br>\neven if Y4 happens after X' has started using W: the worst that can happen is that<br>\n(rarely) X' will go once more harmlessly around the loop (X5,X5a).</p>\n<p>The obvious way to avoid this \"immortality\" of W would be to perform the wakeup of<br>\n\"X.sem\" under Q's spinlocklock.<br>\nThat is, Y could use the order Y0;Y1;Y3;Y4;Y2.   This would<br>\nbe correct, but performs poorly under high load.  That's because the critical section<br>\nof Q's spinlock would then be substantially longer,<br>\nas it would include a full thread wakeup<br>\n(typically involving a scheduling decision) in addition to the simple<br>\nqueue insertions and removals currently performed under Q's spinlock.<br>\nDepending on load, the poor performance can be bad enough in practice<br>\nthat the immortality of the waiter struct is a small price to pay.</p>\n<p>(Aside:It's important to maintain high performance at high load because high load<br>\nusually occurs during system overload.   That's the time when you want<br>\nthroughput to be as high as possible, because that's the best way to get out of<br>\noverload again.  Systems that get a lot slower when pushed into overload<br>\ntend to have unstable performance characteristics.)</p>\n<p>For the case of a platform that has no thread-specific data,<br>\nthere is a less important reason for using a free queue to reuse waiter structs.<br>\nIn this case, if the memory were given back to the malloc() allocator,<br>\nthen the cost of the malloc()/free() calls and the cost of re-initializing the<br>\nwaiter struct (including the semaphore) every time a thread blocked<br>\nwould slow things somewhat down too.</p>\n<p>You might also wonder why the waiter struct needs to be on more than<br>\nlist simultaneously.   That's because a single mutex waiter queue actually has<br>\ntwo doubly-linked lists.  One is linked though all the threads' waiter structs<br>\nin the usual way.<br>\nThe other uses the \"same_condition\" link to connect all adjacent elements<br>\non the waiter queue that have the same non-trivial waiter condition<br>\n(again, see conditional critical sections in nsync's README file).<br>\nThis allows the implementation to test a single condition, and skip over<br>\nmany adjacent  threads waiting on that condition.<br>\nThis is a surprisingly common case.  For example, consider a<br>\nproducer-consumer queue, in which either all waiting threads will be<br>\nwaiting for \"queue non-empty\" or all waiting threads<br>\nwill be waiting for \"queue non-full\".  Thus, this is a fairly potent optimization.</p>", "body_text": "memory is being allocated in nsync::nsync_waiter_new_(). As I mentioned in my\nfirst post, I can see by inspection of\nhttps://github.com/google/nsync/blob/master/internal/common.c\nthat the memory is  not deallocated in this case.\n\nSummary:\nvalgrind's leak detector is complaining not because there's a leak,\nbut because it doesn't like interior  pointers, which point to places within a\nmalloced region, rather than the start.\nThat's what  \"possibly lost\" in vagrind's report means\n(see http://valgrind.org/docs/manual/valgrind_manual.pdf\nand search for \"possibly lost\").\nThe memory is actually still reachable.\nThose blocks are not supposed to be deallocated, but when no longer in use\nare instead put into a free pool and reused, for reasons that I give below.\nnsync uses interior pointers because it puts the same struct on multiple\ndoubly-linked lists simultaneously.   It uses pointers that are embedded in the\nstruct (rather than separate list objects) for efficiency:\nit would be bad to touch twice\nas many cache lines while scanning a list.\nThe embedded pointers  for the multiple doubly-linked lists couldn't all be\nto the start of the objects unless the code for linked list manipulation\nwere duplicated (one copy of the code for each type of linked list),\nor took an offset argument to tell the linked-list code\nwhere to find the links within the object.   An alternative would be a\nfake linked list just for valgrind, linking through every such struct.\nAs you can imagine, to add such complication to the code\nto mollify one checker seems a little ugly,\nespecially because the concern it evokes is illusory: there is no leak.\nNotice also valgrind memcheck is being fairly zealous here, because\ninterior pointers are not that uncommon in normal code.  For example,\nnotice this text in  http://valgrind.org/docs/manual/valgrind_manual.pdf\n\"The available heuristics provide detection of valid interior pointers to\nstd::stdstring, to new[] allocated arrays with elements having destructors\nand to interior pointers pointing to an inner part of a C++ object using\nmultiple inheritance.\"\nBelow are more details than you probably want about\nwhy nsync does thing the way it does.\nThe memory involved is a \"waiter\" struct, which is used\nto enqueue a thread for later wakeup by another thread.\nRather than freeing the waiter struct after use, the struct\nis placed into a free pool  either by waiter_destroy() (which is invoked\nby thread exit), or by  nsync_waiter_free_().      The memory in the\npool is reused by calls to nsync_waiter_new_(); no new allocations are done unless\nthe free pool is empty.  No waiter struct is allowed to become unreachable.\n(Aside: Normally, a thread gets one struct and uses it until the thread exits.\nThe complication of having both waiter_destroy() and nsync_waiter_free_()\noccurs because when debugging nsync itself, a single thread could\nuse more than one waiter struct simultaneously.  But we can ignore that here.)\nThus, the number of waiter structs\nshould be at most the maximum number of concurrent threads.\nThere would be cause for concern only if the leak detector reported\nsubstantially more waiter structs than that\n(e.g., if the programme had used at most tens of\nthreads concurrently, but the leak detector reported hundreds of waiter\nstructs).    Such an eventuality would have suggested that waiter structs\nwere not being reused effectively.   But here, you list a report for only one\nsuch struct, so nothing bad seems to be happening.\nYou might wonder why the library doesn't just free the structs,\nrather than putting them on an internal free list.\nThat would be simpler, but to free the memory would slow down the library\nwhen under heavy load.  The reasons are somewhat subtle.\nFirst, I'll explain how waiter structs are used in the code as it stands,\nand why that requires that the waiter structs are never freed.\nThen I'll explain why it would be slower to have it work in a way that\ndid allow the waiter structs to be freed.\nWhen nsync needs to block a thread (to wait on a condition variable or a mutex)\nit uses a waiter struct W that has an atomic word \"W.nw.waiting\", primary\nlink list pointers \"W.nw.q\",  and a platform-implemented semaphore \"W.sem\".\nA thread X wishing to wait on some wait queue Q acts as follows:\nX0) finds its a waiter struct W (typically via a pointer in\nthread-local storage),\nX1) sets \"W.nw.waiting\" to non-zero,\nX2) locks Q's spinlock,\nX3) inserts W.nw.q  onto Q,\nX4) unlocks Q spinlock, and\nX5) while (\"W.nw.waiting\" is non-zero)\nX5a) waits on \"W.sem\"\nA thread Y wishing to wake a thread from a queue Q:\nY0) locks Q's spinlock,\nY1) removes a waiter struct W from Q,\nY2) unlocks Q spinlock,\nY3) zeroes \"W.nw.waiting\", and\nY4) wakes \"W.sem\".\nNotice that these actions might interleave legally in the following way:\nX0; X1; X2; X3; X4; Y0; Y1; Y2; Y3; X5\nAt this point thread X is no longer waiting, and does not need to execute X5a.\nNow suppose that thread X returns from the nsync library, and exits,\nand only then does thread Y run Y4, which touches W.sem.\nClearly, W.sem needs to exist when Y does Y4, or the programme will crash.\nBut W (and thus W.sem) was assigned for the use of thread X, which has exited.\nThis means that with this protocol, thread X must not deallocate the\nwaiter struct it was assigned, even when thread X exits.\nThis is achieved by making waiter structs immortal in the way\ndescribed above; they are never freed, but instead reused.\nThus, it is guaranteed that \"W.sem\" will exist when Y4 happens, even if W is been\nput on the free list, or even reused by another thread X'.   This reuse is still correct,\neven if Y4 happens after X' has started using W: the worst that can happen is that\n(rarely) X' will go once more harmlessly around the loop (X5,X5a).\nThe obvious way to avoid this \"immortality\" of W would be to perform the wakeup of\n\"X.sem\" under Q's spinlocklock.\nThat is, Y could use the order Y0;Y1;Y3;Y4;Y2.   This would\nbe correct, but performs poorly under high load.  That's because the critical section\nof Q's spinlock would then be substantially longer,\nas it would include a full thread wakeup\n(typically involving a scheduling decision) in addition to the simple\nqueue insertions and removals currently performed under Q's spinlock.\nDepending on load, the poor performance can be bad enough in practice\nthat the immortality of the waiter struct is a small price to pay.\n(Aside:It's important to maintain high performance at high load because high load\nusually occurs during system overload.   That's the time when you want\nthroughput to be as high as possible, because that's the best way to get out of\noverload again.  Systems that get a lot slower when pushed into overload\ntend to have unstable performance characteristics.)\nFor the case of a platform that has no thread-specific data,\nthere is a less important reason for using a free queue to reuse waiter structs.\nIn this case, if the memory were given back to the malloc() allocator,\nthen the cost of the malloc()/free() calls and the cost of re-initializing the\nwaiter struct (including the semaphore) every time a thread blocked\nwould slow things somewhat down too.\nYou might also wonder why the waiter struct needs to be on more than\nlist simultaneously.   That's because a single mutex waiter queue actually has\ntwo doubly-linked lists.  One is linked though all the threads' waiter structs\nin the usual way.\nThe other uses the \"same_condition\" link to connect all adjacent elements\non the waiter queue that have the same non-trivial waiter condition\n(again, see conditional critical sections in nsync's README file).\nThis allows the implementation to test a single condition, and skip over\nmany adjacent  threads waiting on that condition.\nThis is a surprisingly common case.  For example, consider a\nproducer-consumer queue, in which either all waiting threads will be\nwaiting for \"queue non-empty\" or all waiting threads\nwill be waiting for \"queue non-full\".  Thus, this is a fairly potent optimization.", "body": "> memory is being allocated in nsync::nsync_waiter_new_(). As I mentioned in my \r\n> first post, I can see by inspection of \r\n> https://github.com/google/nsync/blob/master/internal/common.c \r\n> that the memory is  not deallocated in this case.\r\n\r\nSummary:\r\n\r\nvalgrind's leak detector is complaining not because there's a leak,\r\nbut because it doesn't like interior  pointers, which point to places within a \r\nmalloced region, rather than the start.  \r\nThat's what  \"possibly lost\" in vagrind's report means \r\n(see http://valgrind.org/docs/manual/valgrind_manual.pdf \r\nand search for \"possibly lost\").\r\nThe memory is actually still reachable.\r\n\r\nThose blocks are not supposed to be deallocated, but when no longer in use \r\nare instead put into a free pool and reused, for reasons that I give below.\r\n\r\nnsync uses interior pointers because it puts the same struct on _multiple_\r\ndoubly-linked lists simultaneously.   It uses pointers that are embedded in the \r\nstruct (rather than separate list objects) for efficiency: \r\nit would be bad to touch twice\r\nas many cache lines while scanning a list.\r\nThe embedded pointers  for the multiple doubly-linked lists couldn't all be \r\nto the start of the objects unless the code for linked list manipulation \r\nwere duplicated (one copy of the code for each type of linked list), \r\nor took an offset argument to tell the linked-list code\r\nwhere to find the links within the object.   An alternative would be a\r\nfake linked list just for valgrind, linking through every such struct.\r\nAs you can imagine, to add such complication to the code\r\nto mollify one checker seems a little ugly, \r\nespecially because the concern it evokes is illusory: there is no leak.\r\nNotice also valgrind memcheck is being fairly zealous here, because\r\ninterior pointers are not that uncommon in normal code.  For example,\r\nnotice this text in  http://valgrind.org/docs/manual/valgrind_manual.pdf\r\n    \"The available heuristics provide detection of valid interior pointers to \r\n    std::stdstring, to new[] allocated arrays with elements having destructors \r\n    and to interior pointers pointing to an inner part of a C++ object using \r\n    multiple inheritance.\"\r\n\r\n\r\nBelow are more details than you probably want about \r\nwhy nsync does thing the way it does.\r\n\r\nThe memory involved is a \"waiter\" struct, which is used\r\nto enqueue a thread for later wakeup by another thread.\r\nRather than freeing the waiter struct after use, the struct\r\nis placed into a free pool  either by waiter_destroy() (which is invoked \r\nby thread exit), or by  nsync_waiter_free_().      The memory in the \r\npool is reused by calls to nsync_waiter_new_(); no new allocations are done unless\r\nthe free pool is empty.  No waiter struct is allowed to become unreachable.\r\n\r\n(Aside: Normally, a thread gets one struct and uses it until the thread exits.\r\nThe complication of having both waiter_destroy() and nsync_waiter_free_()\r\noccurs because when debugging nsync itself, a single thread could\r\nuse more than one waiter struct simultaneously.  But we can ignore that here.)\r\n\r\nThus, the number of waiter structs\r\nshould be at most the maximum number of concurrent threads.\r\nThere would be cause for concern only if the leak detector reported\r\nsubstantially more waiter structs than that\r\n(e.g., if the programme had used at most tens of  \r\nthreads concurrently, but the leak detector reported hundreds of waiter\r\nstructs).    Such an eventuality would have suggested that waiter structs\r\nwere not being reused effectively.   But here, you list a report for only one \r\nsuch struct, so nothing bad seems to be happening.\r\n\r\nYou might wonder why the library doesn't just free the structs,\r\nrather than putting them on an internal free list.\r\nThat would be simpler, but to free the memory would slow down the library\r\nwhen under heavy load.  The reasons are somewhat subtle.\r\n\r\n\r\nFirst, I'll explain how waiter structs are used in the code as it stands, \r\nand why that requires that the waiter structs are never freed.\r\nThen I'll explain why it would be slower to have it work in a way that\r\ndid allow the waiter structs to be freed.\r\n\r\nWhen nsync needs to block a thread (to wait on a condition variable or a mutex)\r\nit uses a waiter struct W that has an atomic word \"W.nw.waiting\", primary\r\nlink list pointers \"W.nw.q\",  and a platform-implemented semaphore \"W.sem\".\r\n\r\nA thread X wishing to wait on some wait queue Q acts as follows:\r\nX0) finds its a waiter struct W (typically via a pointer in \r\nthread-local storage),\r\nX1) sets \"W.nw.waiting\" to non-zero, \r\nX2) locks Q's spinlock,\r\nX3) inserts W.nw.q  onto Q,\r\nX4) unlocks Q spinlock, and\r\nX5) while (\"W.nw.waiting\" is non-zero)\r\n      X5a) waits on \"W.sem\"\r\n\r\nA thread Y wishing to wake a thread from a queue Q:\r\nY0) locks Q's spinlock,\r\nY1) removes a waiter struct W from Q,\r\nY2) unlocks Q spinlock, \r\nY3) zeroes \"W.nw.waiting\", and \r\nY4) wakes \"W.sem\".\r\n\r\nNotice that these actions might interleave legally in the following way:\r\n    X0; X1; X2; X3; X4; Y0; Y1; Y2; Y3; X5\r\nAt this point thread X is no longer waiting, and does not need to execute X5a.  \r\nNow suppose that thread X returns from the nsync library, and exits, \r\nand only then does thread Y run Y4, which touches W.sem.\r\n\r\nClearly, W.sem needs to exist when Y does Y4, or the programme will crash.\r\nBut W (and thus W.sem) was assigned for the use of thread X, which has exited.\r\nThis means that with this protocol, thread X must _not_ deallocate the \r\nwaiter struct it was assigned, even when thread X exits. \r\nThis is achieved by making waiter structs immortal in the way \r\ndescribed above; they are never freed, but instead reused.\r\nThus, it is guaranteed that \"W.sem\" will exist when Y4 happens, even if W is been\r\nput on the free list, or even reused by another thread X'.   This reuse is still correct,\r\neven if Y4 happens after X' has started using W: the worst that can happen is that\r\n(rarely) X' will go once more harmlessly around the loop (X5,X5a).\r\n\r\nThe obvious way to avoid this \"immortality\" of W would be to perform the wakeup of\r\n\"X.sem\" under Q's spinlocklock.  \r\nThat is, Y could use the order Y0;Y1;Y3;Y4;Y2.   This would \r\nbe correct, but performs poorly under high load.  That's because the critical section\r\nof Q's spinlock would then be substantially longer, \r\nas it would include a full thread wakeup\r\n(typically involving a scheduling decision) in addition to the simple\r\nqueue insertions and removals currently performed under Q's spinlock.\r\nDepending on load, the poor performance can be bad enough in practice\r\nthat the immortality of the waiter struct is a small price to pay.\r\n\r\n(Aside:It's important to maintain high performance at high load because high load\r\nusually occurs during system overload.   That's the time when you want \r\nthroughput to be as high as possible, because that's the best way to get out of \r\noverload again.  Systems that get a lot slower when pushed into overload \r\ntend to have unstable performance characteristics.)\r\n\r\nFor the case of a platform that has no thread-specific data,\r\nthere is a less important reason for using a free queue to reuse waiter structs.\r\nIn this case, if the memory were given back to the malloc() allocator, \r\nthen the cost of the malloc()/free() calls and the cost of re-initializing the \r\nwaiter struct (including the semaphore) every time a thread blocked\r\nwould slow things somewhat down too.\r\n\r\n\r\nYou might also wonder why the waiter struct needs to be on more than \r\nlist simultaneously.   That's because a single mutex waiter queue actually has\r\ntwo doubly-linked lists.  One is linked though all the threads' waiter structs \r\nin the usual way.\r\nThe other uses the \"same_condition\" link to connect all adjacent elements \r\non the waiter queue that have the same non-trivial waiter condition \r\n(again, see conditional critical sections in nsync's README file).\r\nThis allows the implementation to test a single condition, and skip over \r\nmany adjacent  threads waiting on that condition. \r\nThis is a surprisingly common case.  For example, consider a \r\nproducer-consumer queue, in which either all waiting threads will be \r\nwaiting for \"queue non-empty\" or all waiting threads\r\nwill be waiting for \"queue non-full\".  Thus, this is a fairly potent optimization.\r\n\r\n\r\n"}