{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15660", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15660/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15660/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15660/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15660", "id": 284751425, "node_id": "MDU6SXNzdWUyODQ3NTE0MjU=", "number": 15660, "title": "Distributed training fault tolerance", "user": {"login": "erfannoury", "id": 1921165, "node_id": "MDQ6VXNlcjE5MjExNjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1921165?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erfannoury", "html_url": "https://github.com/erfannoury", "followers_url": "https://api.github.com/users/erfannoury/followers", "following_url": "https://api.github.com/users/erfannoury/following{/other_user}", "gists_url": "https://api.github.com/users/erfannoury/gists{/gist_id}", "starred_url": "https://api.github.com/users/erfannoury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erfannoury/subscriptions", "organizations_url": "https://api.github.com/users/erfannoury/orgs", "repos_url": "https://api.github.com/users/erfannoury/repos", "events_url": "https://api.github.com/users/erfannoury/events{/privacy}", "received_events_url": "https://api.github.com/users/erfannoury/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-27T16:10:38Z", "updated_at": "2018-01-17T19:54:38Z", "closed_at": "2018-01-17T19:54:38Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 14.04.5 LTS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.4.0-19-ga52c8d9 1.4.1</li>\n<li><strong>Python version</strong>: Python 3.6.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: release 8.0, V8.0.44/ libcudnn v6.0.21</li>\n<li><strong>GPU model and memory</strong>: Titan X (Pascal) 12 GB</li>\n<li><strong>Exact command to reproduce</strong>: In the description</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am using the parameter server/master/worker paradigm to run model training in distributed mode. The master node does the training and evaluation, while the worker nodes only do the training (on their shard of the data). I should also note that I am using the <code>tf.contrib.learn.Experiment</code> interface.</p>\n<p>This model of distributed training works as expected, however, sometimes one or more of the worker nodes fail. While they have failed, the master node and other worker nodes continue the training. The problem is that when this occurs (even when only one of the worker nodes have failed), the loss suddenly becomes zero, and as a result gradients as well become zero, while the metrics suddenly change to the value of a random model, as can be seen in the figures below.</p>\n<ul>\n<li>\n<p>Loss curve. As can be seen one or more of the workers have failed three times during the training.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1921165/34386146-3ed3d652-eaf5-11e7-99de-7923862089e6.jpg\"><img src=\"https://user-images.githubusercontent.com/1921165/34386146-3ed3d652-eaf5-11e7-99de-7923862089e6.jpg\" alt=\"loss_fail\" style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p>Gradient norm curve<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1921165/34386155-48760dba-eaf5-11e7-86f9-d653cda03a3e.PNG\"><img src=\"https://user-images.githubusercontent.com/1921165/34386155-48760dba-eaf5-11e7-86f9-d653cda03a3e.PNG\" alt=\"gradient_fail\" style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p>Accuracy (on the validation set)<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1921165/34386161-51b1f114-eaf5-11e7-8a6c-303d8972b2b8.PNG\"><img src=\"https://user-images.githubusercontent.com/1921165/34386161-51b1f114-eaf5-11e7-8a6c-303d8972b2b8.PNG\" alt=\"accuracy_fail\" style=\"max-width:100%;\"></a></p>\n</li>\n</ul>\n<p><strong>Is there a way to prevent this behavior, either by stopping the training when one of the workers fails, or pausing the training until the failed worker comes back online again (like in the beginning of the training when training only starts when all of the workers come online)?</strong></p>\n<h3>Source code / logs</h3>\n<p>As indicated above, I use the experiment interface. This is the configuration for distributed training:</p>\n<div class=\"highlight highlight-source-python\"><pre>dist_config <span class=\"pl-k\">=</span> {}\ndist_config[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cluster<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> {\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>master<span class=\"pl-pds\">'</span></span>: [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>127.0.0.1:<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(dist_start_port <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>)],\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ps<span class=\"pl-pds\">'</span></span>: [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>127.0.0.1:<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(dist_start_port)],\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>: [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>127.0.0.1:<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(dist_start_port <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">+</span> i)\n            <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(worker_count)]\n}\ndist_config[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>task<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> {\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>type<span class=\"pl-pds\">'</span></span>: dist_type,\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>index<span class=\"pl-pds\">'</span></span>: (worker_index <span class=\"pl-k\">if</span> args.dist_type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">0</span>)\n}\ndist_config[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>environment<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cloud<span class=\"pl-pds\">'</span></span>\nos.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TF_CONFIG<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> json.dumps(dist_config)</pre></div>\n<p>And this is how I start each node:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> dist_type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>master<span class=\"pl-pds\">'</span></span>:\n    experiment.train_and_evaluate()\n<span class=\"pl-k\">elif</span> dist_type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ps<span class=\"pl-pds\">'</span></span>:\n    experiment.run_std_server()\n<span class=\"pl-k\">else</span>:\n    experiment.train()</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04.5 LTS\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): v1.4.0-19-ga52c8d9 1.4.1\nPython version: Python 3.6.3\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: release 8.0, V8.0.44/ libcudnn v6.0.21\nGPU model and memory: Titan X (Pascal) 12 GB\nExact command to reproduce: In the description\n\nDescribe the problem\nI am using the parameter server/master/worker paradigm to run model training in distributed mode. The master node does the training and evaluation, while the worker nodes only do the training (on their shard of the data). I should also note that I am using the tf.contrib.learn.Experiment interface.\nThis model of distributed training works as expected, however, sometimes one or more of the worker nodes fail. While they have failed, the master node and other worker nodes continue the training. The problem is that when this occurs (even when only one of the worker nodes have failed), the loss suddenly becomes zero, and as a result gradients as well become zero, while the metrics suddenly change to the value of a random model, as can be seen in the figures below.\n\n\nLoss curve. As can be seen one or more of the workers have failed three times during the training.\n\n\n\nGradient norm curve\n\n\n\nAccuracy (on the validation set)\n\n\n\nIs there a way to prevent this behavior, either by stopping the training when one of the workers fails, or pausing the training until the failed worker comes back online again (like in the beginning of the training when training only starts when all of the workers come online)?\nSource code / logs\nAs indicated above, I use the experiment interface. This is the configuration for distributed training:\ndist_config = {}\ndist_config['cluster'] = {\n    'master': ['127.0.0.1:{}'.format(dist_start_port + 1)],\n    'ps': ['127.0.0.1:{}'.format(dist_start_port)],\n    'worker': ['127.0.0.1:{}'.format(dist_start_port + 2 + i)\n            for i in range(worker_count)]\n}\ndist_config['task'] = {\n    'type': dist_type,\n    'index': (worker_index if args.dist_type == 'worker' else 0)\n}\ndist_config['environment'] = 'cloud'\nos.environ['TF_CONFIG'] = json.dumps(dist_config)\nAnd this is how I start each node:\nif dist_type == 'master':\n    experiment.train_and_evaluate()\nelif dist_type == 'ps':\n    experiment.run_std_server()\nelse:\n    experiment.train()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.5 LTS\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: Python 3.6.3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: release 8.0, V8.0.44/ libcudnn v6.0.21\r\n- **GPU model and memory**: Titan X (Pascal) 12 GB\r\n- **Exact command to reproduce**: In the description\r\n\r\n### Describe the problem\r\nI am using the parameter server/master/worker paradigm to run model training in distributed mode. The master node does the training and evaluation, while the worker nodes only do the training (on their shard of the data). I should also note that I am using the `tf.contrib.learn.Experiment` interface.\r\n\r\nThis model of distributed training works as expected, however, sometimes one or more of the worker nodes fail. While they have failed, the master node and other worker nodes continue the training. The problem is that when this occurs (even when only one of the worker nodes have failed), the loss suddenly becomes zero, and as a result gradients as well become zero, while the metrics suddenly change to the value of a random model, as can be seen in the figures below.\r\n\r\n* Loss curve. As can be seen one or more of the workers have failed three times during the training.\r\n![loss_fail](https://user-images.githubusercontent.com/1921165/34386146-3ed3d652-eaf5-11e7-99de-7923862089e6.jpg)\r\n\r\n* Gradient norm curve\r\n![gradient_fail](https://user-images.githubusercontent.com/1921165/34386155-48760dba-eaf5-11e7-86f9-d653cda03a3e.PNG)\r\n\r\n* Accuracy (on the validation set)\r\n![accuracy_fail](https://user-images.githubusercontent.com/1921165/34386161-51b1f114-eaf5-11e7-8a6c-303d8972b2b8.PNG)\r\n\r\n\r\n**Is there a way to prevent this behavior, either by stopping the training when one of the workers fails, or pausing the training until the failed worker comes back online again (like in the beginning of the training when training only starts when all of the workers come online)?**\r\n\r\n### Source code / logs\r\nAs indicated above, I use the experiment interface. This is the configuration for distributed training:\r\n```python\r\ndist_config = {}\r\ndist_config['cluster'] = {\r\n    'master': ['127.0.0.1:{}'.format(dist_start_port + 1)],\r\n    'ps': ['127.0.0.1:{}'.format(dist_start_port)],\r\n    'worker': ['127.0.0.1:{}'.format(dist_start_port + 2 + i)\r\n            for i in range(worker_count)]\r\n}\r\ndist_config['task'] = {\r\n    'type': dist_type,\r\n    'index': (worker_index if args.dist_type == 'worker' else 0)\r\n}\r\ndist_config['environment'] = 'cloud'\r\nos.environ['TF_CONFIG'] = json.dumps(dist_config)\r\n```\r\nAnd this is how I start each node:\r\n```python\r\nif dist_type == 'master':\r\n    experiment.train_and_evaluate()\r\nelif dist_type == 'ps':\r\n    experiment.run_std_server()\r\nelse:\r\n    experiment.train()\r\n```\r\n"}