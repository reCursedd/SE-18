{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422287875", "html_url": "https://github.com/tensorflow/tensorflow/issues/22274#issuecomment-422287875", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22274", "id": 422287875, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjI4Nzg3NQ==", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-18T07:38:18Z", "updated_at": "2018-09-18T07:38:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1647833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuefengz\">@yuefengz</a> I just tried MirroredStrategy but it seems to require NCCL even if it is a CPU-only build.</p>\n<p>Is this going to be fixed soon? If not, I will open a separate issue for this.</p>\n<pre><code>INFO:tensorflow:Initializing RunConfig with distribution strategies.\nINFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpSuQoQy\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': {'ps': ['localhost:5000'], 'worker': ['localhost:5001']}, '_model_dir': '/var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpSuQoQy', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': DistributeConfig(train_distribute=&lt;tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x12316fc90&gt;, eval_distribute=None, remote_cluster={'ps': ['localhost:5000'], 'worker': ['localhost:5001']}), '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': &lt;tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x12316fc90&gt;, '_master': '', '_distribute_coordinator_mode': 'standalone_client'}\nINFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.\nINFO:tensorflow:Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'ps': ['localhost:5000'], 'worker': ['localhost:5001']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).So auto-sharding is not done. Please verify correctness of auto-sharding for your input.\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).So auto-sharding is not done. Please verify correctness of auto-sharding for your input.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:batch_all_reduce invoked for batches size = 2 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Creating chief session creator with config: allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\nisolate_session_state: true\n\nINFO:tensorflow:Graph was finalized.\n---------------------------------------------------------------------------\nNotFoundError                             Traceback (most recent call last)\n&lt;ipython-input-6-77bd9e8610db&gt; in &lt;module&gt;()\n     17 train_spec = tf.estimator.TrainSpec(input_fn=input_fn)\n     18 eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\n---&gt; 19 tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.pyc in train_and_evaluate(estimator, train_spec, eval_spec)\n    460     logging.info('Running `train_and_evaluate` with Distribute Coordinator.')\n    461     distribute_coordinator_training.train_and_evaluate(\n--&gt; 462         estimator, train_spec, eval_spec, _TrainingExecutor)\n    463     return\n    464 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.pyc in train_and_evaluate(estimator, train_spec, eval_spec, executor_cls)\n    262       mode=run_config._distribute_coordinator_mode,\n    263       cluster_spec=cluster_spec,\n--&gt; 264       session_config=run_config.session_config)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in run_distribute_coordinator(worker_fn, strategy, eval_fn, eval_strategy, mode, cluster_spec, task_type, task_id, session_config, rpc_layer)\n    747       else:\n    748         _run_in_graph_client(worker_fn, strategy, eval_fn, eval_strategy,\n--&gt; 749                              cluster_spec, session_config, rpc_layer)\n    750     else:\n    751       # If not a client job, run the standard server.\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in _run_in_graph_client(worker_fn, strategy, eval_fn, eval_strategy, cluster_spec, session_config, rpc_layer)\n    471       None,\n    472       session_config,\n--&gt; 473       rpc_layer=rpc_layer)\n    474   if eval_thread:\n    475     eval_thread.join()\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in _run_single_worker(worker_fn, strategy, cluster_spec, task_type, task_id, session_config, rpc_layer, worker_barrier)\n    342       worker_barrier=worker_barrier)\n    343   with context:\n--&gt; 344     worker_fn(strategy)\n    345 \n    346 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.pyc in _worker_fn(strategy)\n    230         input_fn=train_spec.input_fn,\n    231         max_steps=train_spec.max_steps,\n--&gt; 232         hooks=list(train_spec.hooks))\n    233 \n    234   def _eval_fn(strategy):\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n    353 \n    354       saving_listeners = _check_listeners_type(saving_listeners)\n--&gt; 355       loss = self._train_model(input_fn, hooks, saving_listeners)\n    356       logging.info('Loss for final step: %s.', loss)\n    357       return self\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\n   1176   def _train_model(self, input_fn, hooks, saving_listeners):\n   1177     if self._train_distribution:\n-&gt; 1178       return self._train_model_distributed(input_fn, hooks, saving_listeners)\n   1179     else:\n   1180       return self._train_model_default(input_fn, hooks, saving_listeners)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model_distributed(self, input_fn, hooks, saving_listeners)\n   1323         return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n   1324                                                hooks, global_step_tensor,\n-&gt; 1325                                                saving_listeners)\n   1326 \n   1327   def _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks,\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\n   1403         save_summaries_steps=self._config.save_summary_steps,\n   1404         config=self._session_config,\n-&gt; 1405         log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\n   1406       loss = None\n   1407       while not mon_sess.should_stop():\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in MonitoredTrainingSession(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\n    449         max_wait_secs=max_wait_secs,\n    450         save_checkpoint_steps=save_checkpoint_steps,\n--&gt; 451         summary_dir=summary_dir)\n    452 \n    453   if not is_chief:\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in _create_monitored_session_with_worker_context(worker_context, scaffold, checkpoint_dir, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\n    340       session_creator=session_creator,\n    341       hooks=all_hooks,\n--&gt; 342       stop_grace_period_secs=stop_grace_period_secs)\n    343 \n    344 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, session_creator, hooks, stop_grace_period_secs)\n    919     super(MonitoredSession, self).__init__(\n    920         session_creator, hooks, should_recover=True,\n--&gt; 921         stop_grace_period_secs=stop_grace_period_secs)\n    922 \n    923 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, session_creator, hooks, should_recover, stop_grace_period_secs)\n    641         stop_grace_period_secs=stop_grace_period_secs)\n    642     if should_recover:\n--&gt; 643       self._sess = _RecoverableSession(self._coordinated_creator)\n    644     else:\n    645       self._sess = self._coordinated_creator.create_session()\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, sess_creator)\n   1105     \"\"\"\n   1106     self._sess_creator = sess_creator\n-&gt; 1107     _WrappedSession.__init__(self, self._create_session())\n   1108 \n   1109   def _create_session(self):\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in _create_session(self)\n   1110     while True:\n   1111       try:\n-&gt; 1112         return self._sess_creator.create_session()\n   1113       except _PREEMPTION_ERRORS as e:\n   1114         logging.info('An error was raised while a session was being created. '\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in create_session(self)\n    798       \"\"\"Creates a coordinated session.\"\"\"\n    799       # Keep the tf_sess for unit testing.\n--&gt; 800       self.tf_sess = self._session_creator.create_session()\n    801       # We don't want coordinator to suppress any exception.\n    802       self.coord = coordinator.Coordinator(clean_stop_exception_types=[])\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in create_session(self)\n    564         init_op=self._scaffold.init_op,\n    565         init_feed_dict=self._scaffold.init_feed_dict,\n--&gt; 566         init_fn=self._scaffold.init_fn)\n    567 \n    568 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.pyc in prepare_session(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\n    285                            \"init_fn or local_init_op was given\")\n    286       if init_op is not None:\n--&gt; 287         sess.run(init_op, feed_dict=init_feed_dict)\n    288       if init_fn:\n    289         init_fn(sess)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    885     try:\n    886       result = self._run(None, fetches, feed_dict, options_ptr,\n--&gt; 887                          run_metadata_ptr)\n    888       if run_metadata:\n    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):\n   1109       results = self._do_run(handle, final_targets, final_fetches,\n-&gt; 1110                              feed_dict_tensor, options, run_metadata)\n   1111     else:\n   1112       results = []\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n   1284     if handle is None:\n   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,\n-&gt; 1286                            run_metadata)\n   1287     else:\n   1288       return self._do_call(_prun_fn, handle, feeds, fetches)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\n   1304           pass\n   1305       message = error_interpolation.interpolate(message, self._graph)\n-&gt; 1306       raise type(e)(node_def, op, message)\n   1307 \n   1308   def _extend_graph(self):\n\nNotFoundError: Op type not registered 'NcclAllReduce' in binary running on Bairens-MacBook.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n</code></pre>", "body_text": "@yuefengz I just tried MirroredStrategy but it seems to require NCCL even if it is a CPU-only build.\nIs this going to be fixed soon? If not, I will open a separate issue for this.\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\nINFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpSuQoQy\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': {'ps': ['localhost:5000'], 'worker': ['localhost:5001']}, '_model_dir': '/var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpSuQoQy', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x12316fc90>, eval_distribute=None, remote_cluster={'ps': ['localhost:5000'], 'worker': ['localhost:5001']}), '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x12316fc90>, '_master': '', '_distribute_coordinator_mode': 'standalone_client'}\nINFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.\nINFO:tensorflow:Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'ps': ['localhost:5000'], 'worker': ['localhost:5001']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).So auto-sharding is not done. Please verify correctness of auto-sharding for your input.\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).So auto-sharding is not done. Please verify correctness of auto-sharding for your input.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:batch_all_reduce invoked for batches size = 2 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Creating chief session creator with config: allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\nisolate_session_state: true\n\nINFO:tensorflow:Graph was finalized.\n---------------------------------------------------------------------------\nNotFoundError                             Traceback (most recent call last)\n<ipython-input-6-77bd9e8610db> in <module>()\n     17 train_spec = tf.estimator.TrainSpec(input_fn=input_fn)\n     18 eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\n---> 19 tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.pyc in train_and_evaluate(estimator, train_spec, eval_spec)\n    460     logging.info('Running `train_and_evaluate` with Distribute Coordinator.')\n    461     distribute_coordinator_training.train_and_evaluate(\n--> 462         estimator, train_spec, eval_spec, _TrainingExecutor)\n    463     return\n    464 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.pyc in train_and_evaluate(estimator, train_spec, eval_spec, executor_cls)\n    262       mode=run_config._distribute_coordinator_mode,\n    263       cluster_spec=cluster_spec,\n--> 264       session_config=run_config.session_config)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in run_distribute_coordinator(worker_fn, strategy, eval_fn, eval_strategy, mode, cluster_spec, task_type, task_id, session_config, rpc_layer)\n    747       else:\n    748         _run_in_graph_client(worker_fn, strategy, eval_fn, eval_strategy,\n--> 749                              cluster_spec, session_config, rpc_layer)\n    750     else:\n    751       # If not a client job, run the standard server.\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in _run_in_graph_client(worker_fn, strategy, eval_fn, eval_strategy, cluster_spec, session_config, rpc_layer)\n    471       None,\n    472       session_config,\n--> 473       rpc_layer=rpc_layer)\n    474   if eval_thread:\n    475     eval_thread.join()\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in _run_single_worker(worker_fn, strategy, cluster_spec, task_type, task_id, session_config, rpc_layer, worker_barrier)\n    342       worker_barrier=worker_barrier)\n    343   with context:\n--> 344     worker_fn(strategy)\n    345 \n    346 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.pyc in _worker_fn(strategy)\n    230         input_fn=train_spec.input_fn,\n    231         max_steps=train_spec.max_steps,\n--> 232         hooks=list(train_spec.hooks))\n    233 \n    234   def _eval_fn(strategy):\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n    353 \n    354       saving_listeners = _check_listeners_type(saving_listeners)\n--> 355       loss = self._train_model(input_fn, hooks, saving_listeners)\n    356       logging.info('Loss for final step: %s.', loss)\n    357       return self\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\n   1176   def _train_model(self, input_fn, hooks, saving_listeners):\n   1177     if self._train_distribution:\n-> 1178       return self._train_model_distributed(input_fn, hooks, saving_listeners)\n   1179     else:\n   1180       return self._train_model_default(input_fn, hooks, saving_listeners)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model_distributed(self, input_fn, hooks, saving_listeners)\n   1323         return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n   1324                                                hooks, global_step_tensor,\n-> 1325                                                saving_listeners)\n   1326 \n   1327   def _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks,\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\n   1403         save_summaries_steps=self._config.save_summary_steps,\n   1404         config=self._session_config,\n-> 1405         log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\n   1406       loss = None\n   1407       while not mon_sess.should_stop():\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in MonitoredTrainingSession(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\n    449         max_wait_secs=max_wait_secs,\n    450         save_checkpoint_steps=save_checkpoint_steps,\n--> 451         summary_dir=summary_dir)\n    452 \n    453   if not is_chief:\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in _create_monitored_session_with_worker_context(worker_context, scaffold, checkpoint_dir, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\n    340       session_creator=session_creator,\n    341       hooks=all_hooks,\n--> 342       stop_grace_period_secs=stop_grace_period_secs)\n    343 \n    344 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, session_creator, hooks, stop_grace_period_secs)\n    919     super(MonitoredSession, self).__init__(\n    920         session_creator, hooks, should_recover=True,\n--> 921         stop_grace_period_secs=stop_grace_period_secs)\n    922 \n    923 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, session_creator, hooks, should_recover, stop_grace_period_secs)\n    641         stop_grace_period_secs=stop_grace_period_secs)\n    642     if should_recover:\n--> 643       self._sess = _RecoverableSession(self._coordinated_creator)\n    644     else:\n    645       self._sess = self._coordinated_creator.create_session()\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, sess_creator)\n   1105     \"\"\"\n   1106     self._sess_creator = sess_creator\n-> 1107     _WrappedSession.__init__(self, self._create_session())\n   1108 \n   1109   def _create_session(self):\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in _create_session(self)\n   1110     while True:\n   1111       try:\n-> 1112         return self._sess_creator.create_session()\n   1113       except _PREEMPTION_ERRORS as e:\n   1114         logging.info('An error was raised while a session was being created. '\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in create_session(self)\n    798       \"\"\"Creates a coordinated session.\"\"\"\n    799       # Keep the tf_sess for unit testing.\n--> 800       self.tf_sess = self._session_creator.create_session()\n    801       # We don't want coordinator to suppress any exception.\n    802       self.coord = coordinator.Coordinator(clean_stop_exception_types=[])\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in create_session(self)\n    564         init_op=self._scaffold.init_op,\n    565         init_feed_dict=self._scaffold.init_feed_dict,\n--> 566         init_fn=self._scaffold.init_fn)\n    567 \n    568 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.pyc in prepare_session(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\n    285                            \"init_fn or local_init_op was given\")\n    286       if init_op is not None:\n--> 287         sess.run(init_op, feed_dict=init_feed_dict)\n    288       if init_fn:\n    289         init_fn(sess)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    885     try:\n    886       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 887                          run_metadata_ptr)\n    888       if run_metadata:\n    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):\n   1109       results = self._do_run(handle, final_targets, final_fetches,\n-> 1110                              feed_dict_tensor, options, run_metadata)\n   1111     else:\n   1112       results = []\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n   1284     if handle is None:\n   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,\n-> 1286                            run_metadata)\n   1287     else:\n   1288       return self._do_call(_prun_fn, handle, feeds, fetches)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\n   1304           pass\n   1305       message = error_interpolation.interpolate(message, self._graph)\n-> 1306       raise type(e)(node_def, op, message)\n   1307 \n   1308   def _extend_graph(self):\n\nNotFoundError: Op type not registered 'NcclAllReduce' in binary running on Bairens-MacBook.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.", "body": "@yuefengz I just tried MirroredStrategy but it seems to require NCCL even if it is a CPU-only build.\r\n\r\nIs this going to be fixed soon? If not, I will open a separate issue for this.\r\n\r\n```\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpSuQoQy\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': {'ps': ['localhost:5000'], 'worker': ['localhost:5001']}, '_model_dir': '/var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpSuQoQy', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x12316fc90>, eval_distribute=None, remote_cluster={'ps': ['localhost:5000'], 'worker': ['localhost:5001']}), '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x12316fc90>, '_master': '', '_distribute_coordinator_mode': 'standalone_client'}\r\nINFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.\r\nINFO:tensorflow:Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'ps': ['localhost:5000'], 'worker': ['localhost:5001']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).So auto-sharding is not done. Please verify correctness of auto-sharding for your input.\r\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).So auto-sharding is not done. Please verify correctness of auto-sharding for your input.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 2 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Creating chief session creator with config: allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\nisolate_session_state: true\r\n\r\nINFO:tensorflow:Graph was finalized.\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-6-77bd9e8610db> in <module>()\r\n     17 train_spec = tf.estimator.TrainSpec(input_fn=input_fn)\r\n     18 eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\r\n---> 19 tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.pyc in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    460     logging.info('Running `train_and_evaluate` with Distribute Coordinator.')\r\n    461     distribute_coordinator_training.train_and_evaluate(\r\n--> 462         estimator, train_spec, eval_spec, _TrainingExecutor)\r\n    463     return\r\n    464 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.pyc in train_and_evaluate(estimator, train_spec, eval_spec, executor_cls)\r\n    262       mode=run_config._distribute_coordinator_mode,\r\n    263       cluster_spec=cluster_spec,\r\n--> 264       session_config=run_config.session_config)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in run_distribute_coordinator(worker_fn, strategy, eval_fn, eval_strategy, mode, cluster_spec, task_type, task_id, session_config, rpc_layer)\r\n    747       else:\r\n    748         _run_in_graph_client(worker_fn, strategy, eval_fn, eval_strategy,\r\n--> 749                              cluster_spec, session_config, rpc_layer)\r\n    750     else:\r\n    751       # If not a client job, run the standard server.\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in _run_in_graph_client(worker_fn, strategy, eval_fn, eval_strategy, cluster_spec, session_config, rpc_layer)\r\n    471       None,\r\n    472       session_config,\r\n--> 473       rpc_layer=rpc_layer)\r\n    474   if eval_thread:\r\n    475     eval_thread.join()\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in _run_single_worker(worker_fn, strategy, cluster_spec, task_type, task_id, session_config, rpc_layer, worker_barrier)\r\n    342       worker_barrier=worker_barrier)\r\n    343   with context:\r\n--> 344     worker_fn(strategy)\r\n    345 \r\n    346 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.pyc in _worker_fn(strategy)\r\n    230         input_fn=train_spec.input_fn,\r\n    231         max_steps=train_spec.max_steps,\r\n--> 232         hooks=list(train_spec.hooks))\r\n    233 \r\n    234   def _eval_fn(strategy):\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    353 \r\n    354       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 355       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    356       logging.info('Loss for final step: %s.', loss)\r\n    357       return self\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1176   def _train_model(self, input_fn, hooks, saving_listeners):\r\n   1177     if self._train_distribution:\r\n-> 1178       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1179     else:\r\n   1180       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n   1323         return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n   1324                                                hooks, global_step_tensor,\r\n-> 1325                                                saving_listeners)\r\n   1326 \r\n   1327   def _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks,\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\r\n   1403         save_summaries_steps=self._config.save_summary_steps,\r\n   1404         config=self._session_config,\r\n-> 1405         log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n   1406       loss = None\r\n   1407       while not mon_sess.should_stop():\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in MonitoredTrainingSession(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\r\n    449         max_wait_secs=max_wait_secs,\r\n    450         save_checkpoint_steps=save_checkpoint_steps,\r\n--> 451         summary_dir=summary_dir)\r\n    452 \r\n    453   if not is_chief:\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in _create_monitored_session_with_worker_context(worker_context, scaffold, checkpoint_dir, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\r\n    340       session_creator=session_creator,\r\n    341       hooks=all_hooks,\r\n--> 342       stop_grace_period_secs=stop_grace_period_secs)\r\n    343 \r\n    344 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, session_creator, hooks, stop_grace_period_secs)\r\n    919     super(MonitoredSession, self).__init__(\r\n    920         session_creator, hooks, should_recover=True,\r\n--> 921         stop_grace_period_secs=stop_grace_period_secs)\r\n    922 \r\n    923 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, session_creator, hooks, should_recover, stop_grace_period_secs)\r\n    641         stop_grace_period_secs=stop_grace_period_secs)\r\n    642     if should_recover:\r\n--> 643       self._sess = _RecoverableSession(self._coordinated_creator)\r\n    644     else:\r\n    645       self._sess = self._coordinated_creator.create_session()\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, sess_creator)\r\n   1105     \"\"\"\r\n   1106     self._sess_creator = sess_creator\r\n-> 1107     _WrappedSession.__init__(self, self._create_session())\r\n   1108 \r\n   1109   def _create_session(self):\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in _create_session(self)\r\n   1110     while True:\r\n   1111       try:\r\n-> 1112         return self._sess_creator.create_session()\r\n   1113       except _PREEMPTION_ERRORS as e:\r\n   1114         logging.info('An error was raised while a session was being created. '\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in create_session(self)\r\n    798       \"\"\"Creates a coordinated session.\"\"\"\r\n    799       # Keep the tf_sess for unit testing.\r\n--> 800       self.tf_sess = self._session_creator.create_session()\r\n    801       # We don't want coordinator to suppress any exception.\r\n    802       self.coord = coordinator.Coordinator(clean_stop_exception_types=[])\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in create_session(self)\r\n    564         init_op=self._scaffold.init_op,\r\n    565         init_feed_dict=self._scaffold.init_feed_dict,\r\n--> 566         init_fn=self._scaffold.init_fn)\r\n    567 \r\n    568 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.pyc in prepare_session(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\r\n    285                            \"init_fn or local_init_op was given\")\r\n    286       if init_op is not None:\r\n--> 287         sess.run(init_op, feed_dict=init_feed_dict)\r\n    288       if init_fn:\r\n    289         init_fn(sess)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    885     try:\r\n    886       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 887                          run_metadata_ptr)\r\n    888       if run_metadata:\r\n    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1109       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1110                              feed_dict_tensor, options, run_metadata)\r\n   1111     else:\r\n   1112       results = []\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1284     if handle is None:\r\n   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1286                            run_metadata)\r\n   1287     else:\r\n   1288       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1304           pass\r\n   1305       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1306       raise type(e)(node_def, op, message)\r\n   1307 \r\n   1308   def _extend_graph(self):\r\n\r\nNotFoundError: Op type not registered 'NcclAllReduce' in binary running on Bairens-MacBook.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```"}