{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16386", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16386/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16386/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16386/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16386", "id": 291435540, "node_id": "MDU6SXNzdWUyOTE0MzU1NDA=", "number": 16386, "title": "Using keras layers within an Estimator either causes training where it shouldn't or corrupts weights", "user": {"login": "zmjjmz", "id": 1694612, "node_id": "MDQ6VXNlcjE2OTQ2MTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1694612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zmjjmz", "html_url": "https://github.com/zmjjmz", "followers_url": "https://api.github.com/users/zmjjmz/followers", "following_url": "https://api.github.com/users/zmjjmz/following{/other_user}", "gists_url": "https://api.github.com/users/zmjjmz/gists{/gist_id}", "starred_url": "https://api.github.com/users/zmjjmz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zmjjmz/subscriptions", "organizations_url": "https://api.github.com/users/zmjjmz/orgs", "repos_url": "https://api.github.com/users/zmjjmz/repos", "events_url": "https://api.github.com/users/zmjjmz/events{/privacy}", "received_events_url": "https://api.github.com/users/zmjjmz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-01-25T03:06:57Z", "updated_at": "2018-01-25T19:21:38Z", "closed_at": "2018-01-25T17:23:21Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Debian 3.16.36</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.4.0-19-ga52c8d9', '1.4.1')</li>\n<li><strong>Python version</strong>: 2.7.9</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: See gist</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Hey there,</p>\n<p>So I have a <code>tf.keras</code> model that for use on Amazon SageMaker I'm trying to convert into an Estimator. I know there's <code>tf.keras.estimator.model_to_estimator</code> but, I'm having separate <a href=\"https://github.com/tensorflow/tensorflow/issues/16385\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/16385/hovercard\">issues with that</a>.</p>\n<p>In the <a href=\"https://gist.github.com/zmjjmz/667d0d7e6b6c97c49b3aaf4d67b03d2c\">easily run reproduction here</a> I have (as a demonstration) a <code>tf.keras.layers.Embedding</code> which is initialized with all zeros and has <code>trainable=False</code>. Followed by that is a <code>Dense</code> layer with <code>use_bias=False</code> because I couldn't figure out how to get predictions out of an Estimator without training it first (and I can't train nothing apparently). Since all of the embeddings are zero however and can't be trained, the <code>Dense</code> layer should always produce a zero, even after training it. Instead, it produces garbage!</p>\n<p>In fact, I've taken a few steps to ensure that no training takes place, although ideally I'd be able to just run the estimator without training:</p>\n<ol>\n<li>I've set the loss to be 0 initially (l2_norm of what should start out as 0)</li>\n<li>Optimize with SGD using a learning rate of 0</li>\n<li>One training example that should have zero loss...</li>\n</ol>\n<p>The output I actually get is very much non-zero. If I inspect the <code>embed/embeddings:0</code> tensor in <code>tfbdg</code>, I see this:</p>\n<pre><code>array([[ 0.14387012,  0.83495581,  0.44025695,  0.25154734,  0.7214781 ,  0.40229702,  0.82108581,  0.12210274,  0.43861651,  0.39615464],                \n       [ 0.81636655,  0.48157215,  0.48987687,  0.48775947,  0.62187696,  0.25421095,  0.64555049,  0.97305572,  0.53352964,  0.34286666],                \n       [ 0.82881641,  0.80365777,  0.4596678 ,  0.21614265,  0.22256434,  0.07986271,  0.92880177,  0.64946997,  0.89239001,  0.13793337],                \n       [ 0.98491704,  0.15281868,  0.77106941,  0.30048406,  0.86042607,  0.88010466,  0.64362776,  0.70185173,  0.49912012,  0.61521161]], dtype=float32)\n</code></pre>\n<p>Even though it shouldn't have budged from all zeroes!</p>\n<p>So, something about how I'm doing this is fundamentally broken. I suspect that the issue is in line <code>61</code> where I start a new Session -- however this appears to be necessary, since I need to ensure that the <code>keras</code> backend is using the same <code>Graph</code> as <code>tensorflow.get_default_graph()</code> due to the peculiarities of how the Estimator calls the <code>model_fn</code>.</p>\n<p>Notably those values hold between:</p>\n<ol>\n<li>Runs of the estimator</li>\n<li>Successive runs with the same <code>tf.set_random_seed</code> value</li>\n</ol>\n<p>The latter makes me think that somehow the <code>Embedding</code> layer is receiving <em>a</em> gradient despite my best efforts, although it's hard to test this versus some sort of memory corruption.</p>\n<p>If you need me to provide any more information let me know -- I'm sure I've left something out.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 3.16.36\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): ('v1.4.0-19-ga52c8d9', '1.4.1')\nPython version: 2.7.9\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: See gist\n\nDescribe the problem\nHey there,\nSo I have a tf.keras model that for use on Amazon SageMaker I'm trying to convert into an Estimator. I know there's tf.keras.estimator.model_to_estimator but, I'm having separate issues with that.\nIn the easily run reproduction here I have (as a demonstration) a tf.keras.layers.Embedding which is initialized with all zeros and has trainable=False. Followed by that is a Dense layer with use_bias=False because I couldn't figure out how to get predictions out of an Estimator without training it first (and I can't train nothing apparently). Since all of the embeddings are zero however and can't be trained, the Dense layer should always produce a zero, even after training it. Instead, it produces garbage!\nIn fact, I've taken a few steps to ensure that no training takes place, although ideally I'd be able to just run the estimator without training:\n\nI've set the loss to be 0 initially (l2_norm of what should start out as 0)\nOptimize with SGD using a learning rate of 0\nOne training example that should have zero loss...\n\nThe output I actually get is very much non-zero. If I inspect the embed/embeddings:0 tensor in tfbdg, I see this:\narray([[ 0.14387012,  0.83495581,  0.44025695,  0.25154734,  0.7214781 ,  0.40229702,  0.82108581,  0.12210274,  0.43861651,  0.39615464],                \n       [ 0.81636655,  0.48157215,  0.48987687,  0.48775947,  0.62187696,  0.25421095,  0.64555049,  0.97305572,  0.53352964,  0.34286666],                \n       [ 0.82881641,  0.80365777,  0.4596678 ,  0.21614265,  0.22256434,  0.07986271,  0.92880177,  0.64946997,  0.89239001,  0.13793337],                \n       [ 0.98491704,  0.15281868,  0.77106941,  0.30048406,  0.86042607,  0.88010466,  0.64362776,  0.70185173,  0.49912012,  0.61521161]], dtype=float32)\n\nEven though it shouldn't have budged from all zeroes!\nSo, something about how I'm doing this is fundamentally broken. I suspect that the issue is in line 61 where I start a new Session -- however this appears to be necessary, since I need to ensure that the keras backend is using the same Graph as tensorflow.get_default_graph() due to the peculiarities of how the Estimator calls the model_fn.\nNotably those values hold between:\n\nRuns of the estimator\nSuccessive runs with the same tf.set_random_seed value\n\nThe latter makes me think that somehow the Embedding layer is receiving a gradient despite my best efforts, although it's hard to test this versus some sort of memory corruption.\nIf you need me to provide any more information let me know -- I'm sure I've left something out.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')\r\n- **Python version**: 2.7.9\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See gist\r\n\r\n### Describe the problem\r\n\r\nHey there,\r\n\r\nSo I have a `tf.keras` model that for use on Amazon SageMaker I'm trying to convert into an Estimator. I know there's `tf.keras.estimator.model_to_estimator` but, I'm having separate [issues with that](https://github.com/tensorflow/tensorflow/issues/16385).\r\n\r\nIn the [easily run reproduction here](https://gist.github.com/zmjjmz/667d0d7e6b6c97c49b3aaf4d67b03d2c) I have (as a demonstration) a `tf.keras.layers.Embedding` which is initialized with all zeros and has `trainable=False`. Followed by that is a `Dense` layer with `use_bias=False` because I couldn't figure out how to get predictions out of an Estimator without training it first (and I can't train nothing apparently). Since all of the embeddings are zero however and can't be trained, the `Dense` layer should always produce a zero, even after training it. Instead, it produces garbage!\r\n\r\nIn fact, I've taken a few steps to ensure that no training takes place, although ideally I'd be able to just run the estimator without training:\r\n1) I've set the loss to be 0 initially (l2_norm of what should start out as 0)\r\n2) Optimize with SGD using a learning rate of 0\r\n3) One training example that should have zero loss...\r\n\r\nThe output I actually get is very much non-zero. If I inspect the `embed/embeddings:0` tensor in `tfbdg`, I see this:\r\n```\r\narray([[ 0.14387012,  0.83495581,  0.44025695,  0.25154734,  0.7214781 ,  0.40229702,  0.82108581,  0.12210274,  0.43861651,  0.39615464],                \r\n       [ 0.81636655,  0.48157215,  0.48987687,  0.48775947,  0.62187696,  0.25421095,  0.64555049,  0.97305572,  0.53352964,  0.34286666],                \r\n       [ 0.82881641,  0.80365777,  0.4596678 ,  0.21614265,  0.22256434,  0.07986271,  0.92880177,  0.64946997,  0.89239001,  0.13793337],                \r\n       [ 0.98491704,  0.15281868,  0.77106941,  0.30048406,  0.86042607,  0.88010466,  0.64362776,  0.70185173,  0.49912012,  0.61521161]], dtype=float32)\r\n```\r\n\r\nEven though it shouldn't have budged from all zeroes! \r\n\r\nSo, something about how I'm doing this is fundamentally broken. I suspect that the issue is in line `61` where I start a new Session -- however this appears to be necessary, since I need to ensure that the `keras` backend is using the same `Graph` as `tensorflow.get_default_graph()` due to the peculiarities of how the Estimator calls the `model_fn`.\r\n\r\nNotably those values hold between:\r\n1) Runs of the estimator\r\n2) Successive runs with the same `tf.set_random_seed` value\r\n\r\nThe latter makes me think that somehow the `Embedding` layer is receiving *a* gradient despite my best efforts, although it's hard to test this versus some sort of memory corruption. \r\n\r\nIf you need me to provide any more information let me know -- I'm sure I've left something out.\r\n\r\n"}