{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/428846133", "html_url": "https://github.com/tensorflow/tensorflow/issues/15835#issuecomment-428846133", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15835", "id": 428846133, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODg0NjEzMw==", "user": {"login": "u10116032", "id": 20497002, "node_id": "MDQ6VXNlcjIwNDk3MDAy", "avatar_url": "https://avatars3.githubusercontent.com/u/20497002?v=4", "gravatar_id": "", "url": "https://api.github.com/users/u10116032", "html_url": "https://github.com/u10116032", "followers_url": "https://api.github.com/users/u10116032/followers", "following_url": "https://api.github.com/users/u10116032/following{/other_user}", "gists_url": "https://api.github.com/users/u10116032/gists{/gist_id}", "starred_url": "https://api.github.com/users/u10116032/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/u10116032/subscriptions", "organizations_url": "https://api.github.com/users/u10116032/orgs", "repos_url": "https://api.github.com/users/u10116032/repos", "events_url": "https://api.github.com/users/u10116032/events{/privacy}", "received_events_url": "https://api.github.com/users/u10116032/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-11T07:12:06Z", "updated_at": "2018-10-11T07:23:05Z", "author_association": "NONE", "body_html": "<p>I ran Tensorflow Object Detection project with fasterrcnn_resnet50 on distributed training. All I modified is to add a convLSTM layer after feature extractor.<br>\nRecently I want to run on distributed machines through legacy/train.py, and I faced same problem like this issue.</p>\n<p>My cluster spec is as below:</p>\n<pre><code>{\n  \"cluster\": {\n    \"ps\": [\n      \"localhost:3000\"\n    ],\n    \"master\": [\n      \"localhost:3001\"\n    ],\n    \"worker\": [\n      \"localhost:3002\"\n    ]\n  },\n  \"task\": {\n    \"type\": \"ps/master/worker\", \n    \"index\": 0\n  }\n}\n</code></pre>\n<p>For <strong>ps</strong> machine, I set task.type in cluster spec as <strong>ps</strong>, and it runs well.<br>\nFor <strong>master</strong> machine, I set task.type in cluster spec as <strong>master</strong>, and it runs well too.<br>\nIt starts training since the default setting of synchronize training in config file is false.</p>\n<p>However, for <strong>worker</strong> machine, I set task.type as <strong>worker</strong>, and it repeatedly shows:</p>\n<pre><code>INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: rnn/convlstm_cell/kernel, rnn/convlstm_cell/biases, rnn/convlstm_cell/kernel/Momentum, rnn/convlstm_cell/biases/Momentum\n</code></pre>\n<p>I'm wondering if it's a bug for slim.learning.train / supervisor in legacy/trainer.py or not,<br>\nshould I change to MonitoredTrainingSession ?</p>", "body_text": "I ran Tensorflow Object Detection project with fasterrcnn_resnet50 on distributed training. All I modified is to add a convLSTM layer after feature extractor.\nRecently I want to run on distributed machines through legacy/train.py, and I faced same problem like this issue.\nMy cluster spec is as below:\n{\n  \"cluster\": {\n    \"ps\": [\n      \"localhost:3000\"\n    ],\n    \"master\": [\n      \"localhost:3001\"\n    ],\n    \"worker\": [\n      \"localhost:3002\"\n    ]\n  },\n  \"task\": {\n    \"type\": \"ps/master/worker\", \n    \"index\": 0\n  }\n}\n\nFor ps machine, I set task.type in cluster spec as ps, and it runs well.\nFor master machine, I set task.type in cluster spec as master, and it runs well too.\nIt starts training since the default setting of synchronize training in config file is false.\nHowever, for worker machine, I set task.type as worker, and it repeatedly shows:\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: rnn/convlstm_cell/kernel, rnn/convlstm_cell/biases, rnn/convlstm_cell/kernel/Momentum, rnn/convlstm_cell/biases/Momentum\n\nI'm wondering if it's a bug for slim.learning.train / supervisor in legacy/trainer.py or not,\nshould I change to MonitoredTrainingSession ?", "body": "I ran Tensorflow Object Detection project with fasterrcnn_resnet50 on distributed training. All I modified is to add a convLSTM layer after feature extractor.\r\nRecently I want to run on distributed machines through legacy/train.py, and I faced same problem like this issue.\r\n\r\nMy cluster spec is as below:\r\n```\r\n{\r\n  \"cluster\": {\r\n    \"ps\": [\r\n      \"localhost:3000\"\r\n    ],\r\n    \"master\": [\r\n      \"localhost:3001\"\r\n    ],\r\n    \"worker\": [\r\n      \"localhost:3002\"\r\n    ]\r\n  },\r\n  \"task\": {\r\n    \"type\": \"ps/master/worker\", \r\n    \"index\": 0\r\n  }\r\n}\r\n``` \r\n\r\nFor **ps** machine, I set task.type in cluster spec as **ps**, and it runs well.\r\nFor **master** machine, I set task.type in cluster spec as **master**, and it runs well too.\r\nIt starts training since the default setting of synchronize training in config file is false.\r\n\r\nHowever, for **worker** machine, I set task.type as **worker**, and it repeatedly shows:\r\n```\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: rnn/convlstm_cell/kernel, rnn/convlstm_cell/biases, rnn/convlstm_cell/kernel/Momentum, rnn/convlstm_cell/biases/Momentum\r\n```\r\n\r\nI'm wondering if it's a bug for slim.learning.train / supervisor in legacy/trainer.py or not,\r\nshould I change to MonitoredTrainingSession ?"}