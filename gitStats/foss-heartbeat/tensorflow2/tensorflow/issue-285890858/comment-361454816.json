{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361454816", "html_url": "https://github.com/tensorflow/tensorflow/issues/15835#issuecomment-361454816", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15835", "id": 361454816, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTQ1NDgxNg==", "user": {"login": "weidong8405347", "id": 12580804, "node_id": "MDQ6VXNlcjEyNTgwODA0", "avatar_url": "https://avatars1.githubusercontent.com/u/12580804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weidong8405347", "html_url": "https://github.com/weidong8405347", "followers_url": "https://api.github.com/users/weidong8405347/followers", "following_url": "https://api.github.com/users/weidong8405347/following{/other_user}", "gists_url": "https://api.github.com/users/weidong8405347/gists{/gist_id}", "starred_url": "https://api.github.com/users/weidong8405347/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weidong8405347/subscriptions", "organizations_url": "https://api.github.com/users/weidong8405347/orgs", "repos_url": "https://api.github.com/users/weidong8405347/repos", "events_url": "https://api.github.com/users/weidong8405347/events{/privacy}", "received_events_url": "https://api.github.com/users/weidong8405347/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T02:18:32Z", "updated_at": "2018-01-30T02:18:32Z", "author_association": "NONE", "body_html": "<pre><code>with tf.device(tf.train.replica_device_setter(\n    worker_device=\"/job:worker/task:%d\" % task_id,\n    cluster=cluster_spec)):\n   \n  feat_info = tf.get_variable(\"feature_info\", (len(id_map),FLAGS.features_column), tf.float32, trainable=False, partitioner=tf.                                    \n</code></pre>\n<p>fixed_size_partitioner(num_workers * 100))<br>\nadj_info = tf.get_variable(\"adj_info\", (len(id_map),FLAGS.max_degree), tf.int64, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers * 100))</p>\n<pre><code>  with tf.device('/job:worker/task:%d' %task_id):\n      adj_local = tf.Variable(tf.constant(minibatch.adj, dtype=tf.int64), trainable=False, name=\"adj_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\n      feat_local = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False, name=\"feat_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\n   \n  length, begin, end = split_node_by_task(len(id_map), task_id, num_workers)\n  adj = tf.nn.embedding_lookup(adj_info, [x for x in range(begin, end)])\n  adj = adj_local\n   \n  feat = tf.nn.embedding_lookup(feat_info, [x for x in range(begin, end)])\n  feat = feat_local            \n  grads_and_vars = model.optimizer.compute_gradients(model.loss)\n  clipped_grads_and_vars = [(tf.clip_by_value(grad, -5.0, 5.0) if grad is not None else None, var)\n                            for grad, var in grads_and_vars]\n  rep_op = tf.train.SyncReplicasOptimizer(model.optimizer,\n                                          replicas_to_aggregate=num_workers,\n                                          total_num_replicas=num_workers,\n                                          use_locking=True)\n  tf.logging.info('[%s] Build train op ...' % datetime.now())\n  summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\n                         \n  apply_gradient_op = rep_op.apply_gradients(clipped_grads_and_vars, global_step=global_step)                                                                      \n  init_token_op = rep_op.get_init_tokens_op()\n  chief_queue_runner = rep_op.get_chief_queue_runner()\n                         \n  saver = tf.train.Saver(max_to_keep=FLAGS.max_to_keep)\n                         \n  summary_op = tf.summary.merge(summaries)\n  init_op = tf.global_variables_initializer()\n  local_init_op = tf.local_variables_initializer()\n  table_op = tf.tables_initializer()\n  merge_op = tf.group(init_op, table_op)\ntf.logging.info('[%s] Build Supervisor ...' % datetime.now())\nsv = tf.train.Supervisor(is_chief=is_chief,\n                         logdir=FLAGS.checkpoint,\n                         init_op=merge_op,\n                         local_init_op=local_init_op,\n                         summary_op=None,\n                         saver=saver,\n                         global_step=global_step,\n                         recovery_wait_secs=FLAGS.recovery_wait_secs,\n                         save_summaries_secs=None,\n                         save_model_secs=None)                                                                                                                     \n                          \nsess_config = tf.ConfigProto(\n    allow_soft_placement=False,\n    log_device_placement=FLAGS.log_device_placement)\nif is_chief:              \n    tf.logging.info('[%s] Worker %d, Initialize session...' %\n                    (datetime.now(), task_id))\nelse:                     \n    tf.logging.info('[%s] Worker %d, Wait for session to be initialized..' %\n                    (datetime.now(), task_id))\n                          \nwith sv.prepare_or_wait_for_session(target, config=sess_config, max_wait_secs=FLAGS.max_wait_secs) as sess:\n</code></pre>", "body_text": "with tf.device(tf.train.replica_device_setter(\n    worker_device=\"/job:worker/task:%d\" % task_id,\n    cluster=cluster_spec)):\n   \n  feat_info = tf.get_variable(\"feature_info\", (len(id_map),FLAGS.features_column), tf.float32, trainable=False, partitioner=tf.                                    \n\nfixed_size_partitioner(num_workers * 100))\nadj_info = tf.get_variable(\"adj_info\", (len(id_map),FLAGS.max_degree), tf.int64, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers * 100))\n  with tf.device('/job:worker/task:%d' %task_id):\n      adj_local = tf.Variable(tf.constant(minibatch.adj, dtype=tf.int64), trainable=False, name=\"adj_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\n      feat_local = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False, name=\"feat_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\n   \n  length, begin, end = split_node_by_task(len(id_map), task_id, num_workers)\n  adj = tf.nn.embedding_lookup(adj_info, [x for x in range(begin, end)])\n  adj = adj_local\n   \n  feat = tf.nn.embedding_lookup(feat_info, [x for x in range(begin, end)])\n  feat = feat_local            \n  grads_and_vars = model.optimizer.compute_gradients(model.loss)\n  clipped_grads_and_vars = [(tf.clip_by_value(grad, -5.0, 5.0) if grad is not None else None, var)\n                            for grad, var in grads_and_vars]\n  rep_op = tf.train.SyncReplicasOptimizer(model.optimizer,\n                                          replicas_to_aggregate=num_workers,\n                                          total_num_replicas=num_workers,\n                                          use_locking=True)\n  tf.logging.info('[%s] Build train op ...' % datetime.now())\n  summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\n                         \n  apply_gradient_op = rep_op.apply_gradients(clipped_grads_and_vars, global_step=global_step)                                                                      \n  init_token_op = rep_op.get_init_tokens_op()\n  chief_queue_runner = rep_op.get_chief_queue_runner()\n                         \n  saver = tf.train.Saver(max_to_keep=FLAGS.max_to_keep)\n                         \n  summary_op = tf.summary.merge(summaries)\n  init_op = tf.global_variables_initializer()\n  local_init_op = tf.local_variables_initializer()\n  table_op = tf.tables_initializer()\n  merge_op = tf.group(init_op, table_op)\ntf.logging.info('[%s] Build Supervisor ...' % datetime.now())\nsv = tf.train.Supervisor(is_chief=is_chief,\n                         logdir=FLAGS.checkpoint,\n                         init_op=merge_op,\n                         local_init_op=local_init_op,\n                         summary_op=None,\n                         saver=saver,\n                         global_step=global_step,\n                         recovery_wait_secs=FLAGS.recovery_wait_secs,\n                         save_summaries_secs=None,\n                         save_model_secs=None)                                                                                                                     \n                          \nsess_config = tf.ConfigProto(\n    allow_soft_placement=False,\n    log_device_placement=FLAGS.log_device_placement)\nif is_chief:              \n    tf.logging.info('[%s] Worker %d, Initialize session...' %\n                    (datetime.now(), task_id))\nelse:                     \n    tf.logging.info('[%s] Worker %d, Wait for session to be initialized..' %\n                    (datetime.now(), task_id))\n                          \nwith sv.prepare_or_wait_for_session(target, config=sess_config, max_wait_secs=FLAGS.max_wait_secs) as sess:", "body": "    with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % task_id,\r\n        cluster=cluster_spec)):\r\n       \r\n      feat_info = tf.get_variable(\"feature_info\", (len(id_map),FLAGS.features_column), tf.float32, trainable=False, partitioner=tf.                                    \r\nfixed_size_partitioner(num_workers * 100))\r\n      adj_info = tf.get_variable(\"adj_info\", (len(id_map),FLAGS.max_degree), tf.int64, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers * 100))\r\n       \r\n      with tf.device('/job:worker/task:%d' %task_id):\r\n          adj_local = tf.Variable(tf.constant(minibatch.adj, dtype=tf.int64), trainable=False, name=\"adj_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n          feat_local = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False, name=\"feat_local\", collections=[tf.GraphKeys.LOCAL_VARIABLES])\r\n       \r\n      length, begin, end = split_node_by_task(len(id_map), task_id, num_workers)\r\n      adj = tf.nn.embedding_lookup(adj_info, [x for x in range(begin, end)])\r\n      adj = adj_local\r\n       \r\n      feat = tf.nn.embedding_lookup(feat_info, [x for x in range(begin, end)])\r\n      feat = feat_local            \r\n      grads_and_vars = model.optimizer.compute_gradients(model.loss)\r\n      clipped_grads_and_vars = [(tf.clip_by_value(grad, -5.0, 5.0) if grad is not None else None, var)\r\n                                for grad, var in grads_and_vars]\r\n      rep_op = tf.train.SyncReplicasOptimizer(model.optimizer,\r\n                                              replicas_to_aggregate=num_workers,\r\n                                              total_num_replicas=num_workers,\r\n                                              use_locking=True)\r\n      tf.logging.info('[%s] Build train op ...' % datetime.now())\r\n      summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\r\n                             \r\n      apply_gradient_op = rep_op.apply_gradients(clipped_grads_and_vars, global_step=global_step)                                                                      \r\n      init_token_op = rep_op.get_init_tokens_op()\r\n      chief_queue_runner = rep_op.get_chief_queue_runner()\r\n                             \r\n      saver = tf.train.Saver(max_to_keep=FLAGS.max_to_keep)\r\n                             \r\n      summary_op = tf.summary.merge(summaries)\r\n      init_op = tf.global_variables_initializer()\r\n      local_init_op = tf.local_variables_initializer()\r\n      table_op = tf.tables_initializer()\r\n      merge_op = tf.group(init_op, table_op)\r\n    tf.logging.info('[%s] Build Supervisor ...' % datetime.now())\r\n    sv = tf.train.Supervisor(is_chief=is_chief,\r\n                             logdir=FLAGS.checkpoint,\r\n                             init_op=merge_op,\r\n                             local_init_op=local_init_op,\r\n                             summary_op=None,\r\n                             saver=saver,\r\n                             global_step=global_step,\r\n                             recovery_wait_secs=FLAGS.recovery_wait_secs,\r\n                             save_summaries_secs=None,\r\n                             save_model_secs=None)                                                                                                                     \r\n                              \r\n    sess_config = tf.ConfigProto(\r\n        allow_soft_placement=False,\r\n        log_device_placement=FLAGS.log_device_placement)\r\n    if is_chief:              \r\n        tf.logging.info('[%s] Worker %d, Initialize session...' %\r\n                        (datetime.now(), task_id))\r\n    else:                     \r\n        tf.logging.info('[%s] Worker %d, Wait for session to be initialized..' %\r\n                        (datetime.now(), task_id))\r\n                              \r\n    with sv.prepare_or_wait_for_session(target, config=sess_config, max_wait_secs=FLAGS.max_wait_secs) as sess:"}