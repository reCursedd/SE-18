{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324834761", "html_url": "https://github.com/tensorflow/tensorflow/issues/12556#issuecomment-324834761", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12556", "id": 324834761, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDgzNDc2MQ==", "user": {"login": "luvwinnie", "id": 13714992, "node_id": "MDQ6VXNlcjEzNzE0OTky", "avatar_url": "https://avatars1.githubusercontent.com/u/13714992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luvwinnie", "html_url": "https://github.com/luvwinnie", "followers_url": "https://api.github.com/users/luvwinnie/followers", "following_url": "https://api.github.com/users/luvwinnie/following{/other_user}", "gists_url": "https://api.github.com/users/luvwinnie/gists{/gist_id}", "starred_url": "https://api.github.com/users/luvwinnie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luvwinnie/subscriptions", "organizations_url": "https://api.github.com/users/luvwinnie/orgs", "repos_url": "https://api.github.com/users/luvwinnie/repos", "events_url": "https://api.github.com/users/luvwinnie/events{/privacy}", "received_events_url": "https://api.github.com/users/luvwinnie/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-25T06:27:54Z", "updated_at": "2017-08-25T06:38:56Z", "author_association": "NONE", "body_html": "<p>My code is fine in non-distributed version, which why I'm trying to improve the training speed , I'm converting my code to distributed version</p>\n<p>I'm actually trying to create 2 queues, one for training another one for testing after the test_timing for the file. which I found in <a href=\"https://stackoverflow.com/questions/41162955/tensorflow-queues-switching-between-train-and-validation-data\" rel=\"nofollow\">stackoverflow</a></p>\n<p>How does the string_input_producer really work when two files is given?<br>\nWhat I'm trying to do is :<br>\n[Queue 1(Train)  from csv file 1 ]   ,    [Queue 2(Test)   from csv file 2]<br>\nusing these two Queue for training and testing within a program.</p>\n<p>I tried your code with below</p>\n<pre>`'''\npc-01$ python example.py --job-name=\"ps\" --task_index=0 \npc-02$ python example.py --job-name=\"worker\" --task_index=0 \npc-03$ python example.py --job-name=\"worker\" --task_index=1 \npc-04$ python example.py --job-name=\"worker\" --task_index=2 \n\nreference from ischlag.github.io\n'''\nfrom __future__ import print_function\nimport os\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom os import path\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport sys\nimport gzip\n\n#cluster specification\nparameter_servers = [\"192.168.1.7:2223\"]\nworkers = [\"192.168.1.2:2223\",\n           \"192.168.1.8:2223\",\n           \"192.168.1.9:2223\"]\n\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\n# input flags\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_string(\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\nFLAGS = tf.app.flags.FLAGS\n\n# start a server for a specific task\nif FLAGS.job_name == \"ps\":\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\nelse:\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\n#input config\nnum_input = 13\nnum_classes = 3\nh_layer_units = 512\nnumber_of_h_layer = 6\ntest_timing_point = 512\nlearning_rate = 1E-4\n\nif FLAGS.job_name == \"ps\":\n    print(\"calling join to serve\")\n    server.join()\nelif FLAGS.job_name == \"worker\":\n    \n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\n    reader = tf.TextLineReader()\n    key, value = reader.read(filename_queue)\n    record_default = [[1.0] for _ in range(14)]\n    record_default[0] = [1]\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\n    # labels = tf.one_hot(labels,depth=3,dtype=tf.int32)\n    tf.summary.scalar(\"label\",labels)\n    global_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32)\n    \n    # server = tf.train.Server.create_local_server()\n    summary_op = tf.summary.merge_all()\n    summary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='./tmp_test',summary_op=summary_op)\n    previous=[]\n    with tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\n        tf.train.start_queue_runners(sess=sess)\n        for i in range(1024):\n            example,label,step = sess.run([mfcc_features,labels,global_step])\n            if set(previous) == set(example):\n                print(\"it's same!\")\n                sys.exit()\n            else:\n                previous = example\n                print(example,label,step)\n`\n</pre>\n<p>this code work fine when the <code>tf.one_hot(labels,depth=3,dtype=tf.int32)</code> is not used.<br>\nhowever if I include the tf.one_hot transformation it show the error that I'm giving wrong op u'label'</p>\n<pre>`Caused by op u'label', defined at:\n  File \"sound_classifier_distributed_test_input.py\", line 74, in \n    tf.summary.scalar(\"label\",labels_hot)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py\", line 129, in scalar\n    tags=scope.rstrip('/'), values=tensor, name=scope)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 265, in _scalar_summary\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [3] (tag 'label')\n         [[Node: label = ScalarSummary[T=DT_INT32, _device=\"/job:ps/replica:0/task:0/cpu:0\"](label/tags, one_hot)]]\n         [[Node: Merge/MergeSummary_S13 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:2/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=-9152570865690878554, tensor_name=\"edge_72_Merge/MergeSummary\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:2/cpu:0\"]()]]`\n</pre>\n<p>But I'm trying to make the label to be one_hot for classification.<br>\nHow am I suppose to:</p>\n<ol>\n<li>Read data from csv file ---&gt; data structure [label , feature , .... feature]</li>\n<li>Convert label to one_hot</li>\n<li>Batch up the one_hot labels and features to 1024 Batch size?</li>\n</ol>", "body_text": "My code is fine in non-distributed version, which why I'm trying to improve the training speed , I'm converting my code to distributed version\nI'm actually trying to create 2 queues, one for training another one for testing after the test_timing for the file. which I found in stackoverflow\nHow does the string_input_producer really work when two files is given?\nWhat I'm trying to do is :\n[Queue 1(Train)  from csv file 1 ]   ,    [Queue 2(Test)   from csv file 2]\nusing these two Queue for training and testing within a program.\nI tried your code with below\n`'''\npc-01$ python example.py --job-name=\"ps\" --task_index=0 \npc-02$ python example.py --job-name=\"worker\" --task_index=0 \npc-03$ python example.py --job-name=\"worker\" --task_index=1 \npc-04$ python example.py --job-name=\"worker\" --task_index=2 \n\nreference from ischlag.github.io\n'''\nfrom __future__ import print_function\nimport os\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom os import path\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport sys\nimport gzip\n\n#cluster specification\nparameter_servers = [\"192.168.1.7:2223\"]\nworkers = [\"192.168.1.2:2223\",\n           \"192.168.1.8:2223\",\n           \"192.168.1.9:2223\"]\n\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\n# input flags\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_string(\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\nFLAGS = tf.app.flags.FLAGS\n\n# start a server for a specific task\nif FLAGS.job_name == \"ps\":\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\nelse:\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\n#input config\nnum_input = 13\nnum_classes = 3\nh_layer_units = 512\nnumber_of_h_layer = 6\ntest_timing_point = 512\nlearning_rate = 1E-4\n\nif FLAGS.job_name == \"ps\":\n    print(\"calling join to serve\")\n    server.join()\nelif FLAGS.job_name == \"worker\":\n    \n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\n    reader = tf.TextLineReader()\n    key, value = reader.read(filename_queue)\n    record_default = [[1.0] for _ in range(14)]\n    record_default[0] = [1]\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\n    # labels = tf.one_hot(labels,depth=3,dtype=tf.int32)\n    tf.summary.scalar(\"label\",labels)\n    global_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32)\n    \n    # server = tf.train.Server.create_local_server()\n    summary_op = tf.summary.merge_all()\n    summary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='./tmp_test',summary_op=summary_op)\n    previous=[]\n    with tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\n        tf.train.start_queue_runners(sess=sess)\n        for i in range(1024):\n            example,label,step = sess.run([mfcc_features,labels,global_step])\n            if set(previous) == set(example):\n                print(\"it's same!\")\n                sys.exit()\n            else:\n                previous = example\n                print(example,label,step)\n`\n\nthis code work fine when the tf.one_hot(labels,depth=3,dtype=tf.int32) is not used.\nhowever if I include the tf.one_hot transformation it show the error that I'm giving wrong op u'label'\n`Caused by op u'label', defined at:\n  File \"sound_classifier_distributed_test_input.py\", line 74, in \n    tf.summary.scalar(\"label\",labels_hot)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py\", line 129, in scalar\n    tags=scope.rstrip('/'), values=tensor, name=scope)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 265, in _scalar_summary\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [3] (tag 'label')\n         [[Node: label = ScalarSummary[T=DT_INT32, _device=\"/job:ps/replica:0/task:0/cpu:0\"](label/tags, one_hot)]]\n         [[Node: Merge/MergeSummary_S13 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:2/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=-9152570865690878554, tensor_name=\"edge_72_Merge/MergeSummary\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:2/cpu:0\"]()]]`\n\nBut I'm trying to make the label to be one_hot for classification.\nHow am I suppose to:\n\nRead data from csv file ---> data structure [label , feature , .... feature]\nConvert label to one_hot\nBatch up the one_hot labels and features to 1024 Batch size?", "body": "My code is fine in non-distributed version, which why I'm trying to improve the training speed , I'm converting my code to distributed version\r\n\r\nI'm actually trying to create 2 queues, one for training another one for testing after the test_timing for the file. which I found in [stackoverflow](https://stackoverflow.com/questions/41162955/tensorflow-queues-switching-between-train-and-validation-data)\r\n\r\nHow does the string_input_producer really work when two files is given?\r\nWhat I'm trying to do is :\r\n[Queue 1(Train)  from csv file 1 ]   ,    [Queue 2(Test)   from csv file 2]\r\nusing these two Queue for training and testing within a program.\r\n\r\n\r\nI tried your code with below\r\n<pre>\r\n`'''\r\npc-01$ python example.py --job-name=\"ps\" --task_index=0 \r\npc-02$ python example.py --job-name=\"worker\" --task_index=0 \r\npc-03$ python example.py --job-name=\"worker\" --task_index=1 \r\npc-04$ python example.py --job-name=\"worker\" --task_index=2 \r\n\r\nreference from ischlag.github.io\r\n'''\r\nfrom __future__ import print_function\r\nimport os\r\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nfrom os import path\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport time\r\nimport sys\r\nimport gzip\r\n\r\n#cluster specification\r\nparameter_servers = [\"192.168.1.7:2223\"]\r\nworkers = [\"192.168.1.2:2223\",\r\n           \"192.168.1.8:2223\",\r\n           \"192.168.1.9:2223\"]\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n\r\n# input flags\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_string(\r\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\r\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n# start a server for a specific task\r\nif FLAGS.job_name == \"ps\":\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\nelse:\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\n#input config\r\nnum_input = 13\r\nnum_classes = 3\r\nh_layer_units = 512\r\nnumber_of_h_layer = 6\r\ntest_timing_point = 512\r\nlearning_rate = 1E-4\r\n\r\nif FLAGS.job_name == \"ps\":\r\n    print(\"calling join to serve\")\r\n    server.join()\r\nelif FLAGS.job_name == \"worker\":\r\n    \r\n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\r\n    reader = tf.TextLineReader()\r\n    key, value = reader.read(filename_queue)\r\n    record_default = [[1.0] for _ in range(14)]\r\n    record_default[0] = [1]\r\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\r\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\r\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\r\n    # labels = tf.one_hot(labels,depth=3,dtype=tf.int32)\r\n    tf.summary.scalar(\"label\",labels)\r\n    global_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32)\r\n    \r\n    # server = tf.train.Server.create_local_server()\r\n    summary_op = tf.summary.merge_all()\r\n    summary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='./tmp_test',summary_op=summary_op)\r\n    previous=[]\r\n    with tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\r\n        tf.train.start_queue_runners(sess=sess)\r\n        for i in range(1024):\r\n            example,label,step = sess.run([mfcc_features,labels,global_step])\r\n            if set(previous) == set(example):\r\n                print(\"it's same!\")\r\n                sys.exit()\r\n            else:\r\n                previous = example\r\n                print(example,label,step)\r\n`\r\n</pre>\r\n\r\nthis code work fine when the `tf.one_hot(labels,depth=3,dtype=tf.int32)` is not used.\r\nhowever if I include the tf.one_hot transformation it show the error that I'm giving wrong op u'label'\r\n<pre>\r\n`Caused by op u'label', defined at:\r\n  File \"sound_classifier_distributed_test_input.py\", line 74, in <module>\r\n    tf.summary.scalar(\"label\",labels_hot)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py\", line 129, in scalar\r\n    tags=scope.rstrip('/'), values=tensor, name=scope)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 265, in _scalar_summary\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [3] (tag 'label')\r\n         [[Node: label = ScalarSummary[T=DT_INT32, _device=\"/job:ps/replica:0/task:0/cpu:0\"](label/tags, one_hot)]]\r\n         [[Node: Merge/MergeSummary_S13 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:2/cpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=-9152570865690878554, tensor_name=\"edge_72_Merge/MergeSummary\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:2/cpu:0\"]()]]`\r\n</pre>\r\n\r\nBut I'm trying to make the label to be one_hot for classification.\r\nHow am I suppose to:\r\n1) Read data from csv file ---> data structure [label , feature , .... feature]\r\n2) Convert label to one_hot\r\n3) Batch up the one_hot labels and features to 1024 Batch size?"}