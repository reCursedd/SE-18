{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/325005731", "html_url": "https://github.com/tensorflow/tensorflow/issues/12556#issuecomment-325005731", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12556", "id": 325005731, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNTAwNTczMQ==", "user": {"login": "kinsumliu", "id": 8632201, "node_id": "MDQ6VXNlcjg2MzIyMDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/8632201?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kinsumliu", "html_url": "https://github.com/kinsumliu", "followers_url": "https://api.github.com/users/kinsumliu/followers", "following_url": "https://api.github.com/users/kinsumliu/following{/other_user}", "gists_url": "https://api.github.com/users/kinsumliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/kinsumliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kinsumliu/subscriptions", "organizations_url": "https://api.github.com/users/kinsumliu/orgs", "repos_url": "https://api.github.com/users/kinsumliu/repos", "events_url": "https://api.github.com/users/kinsumliu/events{/privacy}", "received_events_url": "https://api.github.com/users/kinsumliu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-25T18:44:00Z", "updated_at": "2017-08-25T22:37:19Z", "author_association": "NONE", "body_html": "<p>There are a few things in your code that need to be changed, especially for your data feeding mechanism. So your first session run will fetch the data but <code>xent</code> cannot be computed so <code>tf.summary.scalar(\"x-entropy\",xent)</code> causes problem.</p>\n<p>Check out my code below. It should work. Note that I use <code>train_op = tf.train.GradientDescentOptimizer(1.0).minimize(xent, global_step=global_step)</code> because I am doing it on a single machine. After you change to use <code>SyncReplicasOptimizer</code>, first turn off training to see the data feeding works by using <code>loss,glob_step= sess.run([xent,global_step])</code>.</p>\n<pre><code>from __future__ import print_function\nimport os\nfrom os import path\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport sys\nimport gzip\nimport math\n\n# input flags\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_string(\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\nFLAGS = tf.app.flags.FLAGS\n\n#input config\nnum_input = 13\nnum_classes = 3\nh_layer_units = 512\nnumber_of_h_layer = 6\ntest_timing_point = 512\nlearning_rate = 1E-4\nbatch_size = 1\n\n#-----function definition-------\ndef readData():\n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\n    reader = tf.TextLineReader()\n    key, value = reader.read(filename_queue)\n    record_default = [[1.0] for _ in range(14)]\n    record_default[0] = [1]\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\n    labels1hot = tf.one_hot(labels,depth=3,dtype=tf.int32)\n    labels_batch, mfcc_features_batch = tf.train.batch(\n            [labels1hot, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\n    return (mfcc_features_batch, labels_batch)\n\nis_chief = True\nglobal_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32,trainable=False)\n(x,y) = readData()\nw1 = tf.Variable(tf.truncated_normal([num_input,512],stddev=0.1),name=\"W1\")\nb1 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B1\")\nact1 = tf.nn.tanh(tf.add(tf.matmul(tf.cast(x, tf.float32),w1),b1))\n\nw2 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W2\")\nb2 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B2\")\nact2 = tf.nn.tanh(tf.add(tf.matmul(act1,w2),b2))\n\nw3 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W3\")\nb3 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B3\")\nact3 = tf.nn.tanh(tf.add(tf.matmul(act2,w3),b3))\n\nw4 = tf.Variable(tf.truncated_normal(shape=[512,3],stddev=0.1),name=\"W4\")\nb4 = tf.Variable(tf.truncated_normal(shape=[3],stddev=0.1),name=\"B4\")\nact4 = tf.add(tf.matmul(act3,w4),b4)\n\n# #----loss----\nxent = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(\n        logits=act4,labels=y),name=\"x-entropy\")\n\n# # ----train----\n# grad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\n# rep_op = tf.train.SyncReplicasOptimizer(grad_op,\n#                                         replicas_to_aggregate=3,\n#                                         total_num_replicas=3)\n# train_op = rep_op.minimize(xent,global_step=global_step)\n\ntrain_op = tf.train.GradientDescentOptimizer(1.0).minimize(xent, global_step=global_step)\n\n\n# sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\n# init_token_op = rep_op.get_init_tokens_op()\ntf.summary.scalar(\"x-entropy\",xent)\n\nserver = tf.train.Server.create_local_server()\nsummary_op = tf.summary.merge_all()\nsummary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='.',summary_op=summary_op)\nwith tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\n    tf.train.start_queue_runners(sess=sess)\n    for i in range(2):\n        _,loss,glob_step= sess.run([train_op,xent,global_step])\n        print (loss, glob_step)\n</code></pre>", "body_text": "There are a few things in your code that need to be changed, especially for your data feeding mechanism. So your first session run will fetch the data but xent cannot be computed so tf.summary.scalar(\"x-entropy\",xent) causes problem.\nCheck out my code below. It should work. Note that I use train_op = tf.train.GradientDescentOptimizer(1.0).minimize(xent, global_step=global_step) because I am doing it on a single machine. After you change to use SyncReplicasOptimizer, first turn off training to see the data feeding works by using loss,glob_step= sess.run([xent,global_step]).\nfrom __future__ import print_function\nimport os\nfrom os import path\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport sys\nimport gzip\nimport math\n\n# input flags\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_string(\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\nFLAGS = tf.app.flags.FLAGS\n\n#input config\nnum_input = 13\nnum_classes = 3\nh_layer_units = 512\nnumber_of_h_layer = 6\ntest_timing_point = 512\nlearning_rate = 1E-4\nbatch_size = 1\n\n#-----function definition-------\ndef readData():\n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\n    reader = tf.TextLineReader()\n    key, value = reader.read(filename_queue)\n    record_default = [[1.0] for _ in range(14)]\n    record_default[0] = [1]\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\n    labels1hot = tf.one_hot(labels,depth=3,dtype=tf.int32)\n    labels_batch, mfcc_features_batch = tf.train.batch(\n            [labels1hot, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\n    return (mfcc_features_batch, labels_batch)\n\nis_chief = True\nglobal_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32,trainable=False)\n(x,y) = readData()\nw1 = tf.Variable(tf.truncated_normal([num_input,512],stddev=0.1),name=\"W1\")\nb1 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B1\")\nact1 = tf.nn.tanh(tf.add(tf.matmul(tf.cast(x, tf.float32),w1),b1))\n\nw2 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W2\")\nb2 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B2\")\nact2 = tf.nn.tanh(tf.add(tf.matmul(act1,w2),b2))\n\nw3 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W3\")\nb3 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B3\")\nact3 = tf.nn.tanh(tf.add(tf.matmul(act2,w3),b3))\n\nw4 = tf.Variable(tf.truncated_normal(shape=[512,3],stddev=0.1),name=\"W4\")\nb4 = tf.Variable(tf.truncated_normal(shape=[3],stddev=0.1),name=\"B4\")\nact4 = tf.add(tf.matmul(act3,w4),b4)\n\n# #----loss----\nxent = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(\n        logits=act4,labels=y),name=\"x-entropy\")\n\n# # ----train----\n# grad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\n# rep_op = tf.train.SyncReplicasOptimizer(grad_op,\n#                                         replicas_to_aggregate=3,\n#                                         total_num_replicas=3)\n# train_op = rep_op.minimize(xent,global_step=global_step)\n\ntrain_op = tf.train.GradientDescentOptimizer(1.0).minimize(xent, global_step=global_step)\n\n\n# sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\n# init_token_op = rep_op.get_init_tokens_op()\ntf.summary.scalar(\"x-entropy\",xent)\n\nserver = tf.train.Server.create_local_server()\nsummary_op = tf.summary.merge_all()\nsummary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='.',summary_op=summary_op)\nwith tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\n    tf.train.start_queue_runners(sess=sess)\n    for i in range(2):\n        _,loss,glob_step= sess.run([train_op,xent,global_step])\n        print (loss, glob_step)", "body": "There are a few things in your code that need to be changed, especially for your data feeding mechanism. So your first session run will fetch the data but `xent` cannot be computed so `tf.summary.scalar(\"x-entropy\",xent)` causes problem. \r\n\r\nCheck out my code below. It should work. Note that I use `train_op = tf.train.GradientDescentOptimizer(1.0).minimize(xent, global_step=global_step)` because I am doing it on a single machine. After you change to use `SyncReplicasOptimizer`, first turn off training to see the data feeding works by using `loss,glob_step= sess.run([xent,global_step])`.\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport os\r\nfrom os import path\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport time\r\nimport sys\r\nimport gzip\r\nimport math\r\n\r\n# input flags\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_string(\r\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\r\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n#input config\r\nnum_input = 13\r\nnum_classes = 3\r\nh_layer_units = 512\r\nnumber_of_h_layer = 6\r\ntest_timing_point = 512\r\nlearning_rate = 1E-4\r\nbatch_size = 1\r\n\r\n#-----function definition-------\r\ndef readData():\r\n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\r\n    reader = tf.TextLineReader()\r\n    key, value = reader.read(filename_queue)\r\n    record_default = [[1.0] for _ in range(14)]\r\n    record_default[0] = [1]\r\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\r\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\r\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\r\n    labels1hot = tf.one_hot(labels,depth=3,dtype=tf.int32)\r\n    labels_batch, mfcc_features_batch = tf.train.batch(\r\n            [labels1hot, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\r\n    return (mfcc_features_batch, labels_batch)\r\n\r\nis_chief = True\r\nglobal_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32,trainable=False)\r\n(x,y) = readData()\r\nw1 = tf.Variable(tf.truncated_normal([num_input,512],stddev=0.1),name=\"W1\")\r\nb1 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B1\")\r\nact1 = tf.nn.tanh(tf.add(tf.matmul(tf.cast(x, tf.float32),w1),b1))\r\n\r\nw2 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W2\")\r\nb2 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B2\")\r\nact2 = tf.nn.tanh(tf.add(tf.matmul(act1,w2),b2))\r\n\r\nw3 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W3\")\r\nb3 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B3\")\r\nact3 = tf.nn.tanh(tf.add(tf.matmul(act2,w3),b3))\r\n\r\nw4 = tf.Variable(tf.truncated_normal(shape=[512,3],stddev=0.1),name=\"W4\")\r\nb4 = tf.Variable(tf.truncated_normal(shape=[3],stddev=0.1),name=\"B4\")\r\nact4 = tf.add(tf.matmul(act3,w4),b4)\r\n\r\n# #----loss----\r\nxent = tf.reduce_mean(\r\n    tf.nn.softmax_cross_entropy_with_logits(\r\n        logits=act4,labels=y),name=\"x-entropy\")\r\n\r\n# # ----train----\r\n# grad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n# rep_op = tf.train.SyncReplicasOptimizer(grad_op,\r\n#                                         replicas_to_aggregate=3,\r\n#                                         total_num_replicas=3)\r\n# train_op = rep_op.minimize(xent,global_step=global_step)\r\n\r\ntrain_op = tf.train.GradientDescentOptimizer(1.0).minimize(xent, global_step=global_step)\r\n\r\n\r\n# sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\r\n# init_token_op = rep_op.get_init_tokens_op()\r\ntf.summary.scalar(\"x-entropy\",xent)\r\n\r\nserver = tf.train.Server.create_local_server()\r\nsummary_op = tf.summary.merge_all()\r\nsummary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='.',summary_op=summary_op)\r\nwith tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\r\n    tf.train.start_queue_runners(sess=sess)\r\n    for i in range(2):\r\n        _,loss,glob_step= sess.run([train_op,xent,global_step])\r\n        print (loss, glob_step)\r\n```"}