{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324863225", "html_url": "https://github.com/tensorflow/tensorflow/issues/12556#issuecomment-324863225", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12556", "id": 324863225, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDg2MzIyNQ==", "user": {"login": "luvwinnie", "id": 13714992, "node_id": "MDQ6VXNlcjEzNzE0OTky", "avatar_url": "https://avatars1.githubusercontent.com/u/13714992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luvwinnie", "html_url": "https://github.com/luvwinnie", "followers_url": "https://api.github.com/users/luvwinnie/followers", "following_url": "https://api.github.com/users/luvwinnie/following{/other_user}", "gists_url": "https://api.github.com/users/luvwinnie/gists{/gist_id}", "starred_url": "https://api.github.com/users/luvwinnie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luvwinnie/subscriptions", "organizations_url": "https://api.github.com/users/luvwinnie/orgs", "repos_url": "https://api.github.com/users/luvwinnie/repos", "events_url": "https://api.github.com/users/luvwinnie/events{/privacy}", "received_events_url": "https://api.github.com/users/luvwinnie/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-25T09:01:28Z", "updated_at": "2017-08-25T09:03:34Z", "author_association": "NONE", "body_html": "<p>I build up my code step by step. While the code to the <code>xent</code> calculation it still works fine.<br>\nBut when I added the definition of the train_op as below:</p>\n<pre>grad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\nrep_op = tf.train.SyncReplicasOptimizer(grad_op,\n                                                replicas_to_aggregate=3,\n                                                total_num_replicas=3)\ntrain_op = rep_op.minimize(xent,global_step=global_step)\nsync_replicas_hook = rep_op.make_session_run_hook(is_chief)\ninit_token_op = rep_op.get_init_tokens_op()\nchief_queue_runner = rep_op.get_chief_queue_runner()\n</pre>\n<p>It's start show the error message getting the -1 negative dimension<br>\nlooks like this issue have been open at  <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"246125815\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11823\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/11823/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/11823\">#11823</a><br>\nIs this the tensorflow version bugs?</p>\n<p>Here is my new full source code:</p>\n<pre>'''\npc-01$ python example.py --job-name=\"ps\" --task_index=0 \npc-02$ python example.py --job-name=\"worker\" --task_index=0 \npc-03$ python example.py --job-name=\"worker\" --task_index=1 \npc-04$ python example.py --job-name=\"worker\" --task_index=2 \n\nreference from ischlag.github.io\n'''\nfrom __future__ import print_function\nimport os\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom os import path\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport sys\nimport gzip\nimport math\n#cluster specification\nparameter_servers = [\"192.168.1.7:2223\"]\nworkers = [\"192.168.1.2:2223\",\n           \"192.168.1.8:2223\",\n           \"192.168.1.9:2223\"]\n\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\n# input flags\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_string(\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\nFLAGS = tf.app.flags.FLAGS\n\n# start a server for a specific task\nif FLAGS.job_name == \"ps\":\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\nelse:\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\n#input config\nnum_input = 13\nnum_classes = 3\nh_layer_units = 512\nnumber_of_h_layer = 6\ntest_timing_point = 512\nlearning_rate = 1E-4\nbatch_size = 1024\n\nif FLAGS.job_name == \"ps\":\n    print(\"calling join to serve\")\n    server.join()\nelif FLAGS.job_name == \"worker\":\n\n#-----function definition-------\n    \n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\n    reader = tf.TextLineReader()\n    key, value = reader.read(filename_queue)\n    record_default = [[1.0] for _ in range(14)]\n    record_default[0] = [1]\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\n    labels = tf.one_hot(labels,depth=3,dtype=tf.int32)\n    labels_batch, mfcc_features_batch = tf.train.batch(\n            [labels, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\n    tf.summary.tensor_summary(\"label\",labels)\n    \n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" %FLAGS.task_index,cluster=cluster)):\n        is_chief = (FLAGS.task_index == 0)\n        global_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32,trainable=False) \n        x = tf.placeholder(tf.float32,shape=[None,num_input],name=\"mfcc_input\")\n        y = tf.placeholder(tf.int32,shape=[None,num_classes],name=\"labels\")\n        w1 = tf.Variable(tf.truncated_normal([num_input,512],stddev=0.1),name=\"W\")\n        b1 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\n        act1 = tf.nn.tanh(tf.add(tf.matmul(x,w1),b1))\n\n        w2 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W\")\n        b2 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\n        act2 = tf.nn.tanh(tf.add(tf.matmul(act1,w2),b2))\n\n        w3 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W\")\n        b3 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\n        act3 = tf.nn.tanh(tf.add(tf.matmul(act2,w3),b3))\n\n\n\n        w4 = tf.Variable(tf.truncated_normal(shape=[512,3],stddev=0.1),name=\"W\")\n        b4 = tf.Variable(tf.truncated_normal(shape=[3],stddev=0.1),name=\"B\")\n        act4 = tf.add(tf.matmul(act3,w4),b4)\n\n        # #----loss----\n        xent = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(\n                logits=act4,labels=y),name=\"x-entropy\")\n\n\n        # ----train----\n        grad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\n        rep_op = tf.train.SyncReplicasOptimizer(grad_op,\n                                                replicas_to_aggregate=3,\n                                                total_num_replicas=3)\n        # # rep_op.compute_gradients()\n        train_op = rep_op.minimize(xent,global_step=global_step)\n        sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\n        init_token_op = rep_op.get_init_tokens_op()\n        chief_queue_runner = rep_op.get_chief_queue_runner()\n        #----- accuracy\n        # correct_prediction = tf.equal(tf.argmax(act4,1),tf.argmax(y,1))\n        # accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n    \n    tf.summary.scalar(\"x-entropy\",xent)\n    # tf.summary.scalar(\"accuracy\",accuracy)\n\n \n\n    # server = tf.train.Server.create_local_server()\n    # session_run_hook = tf.train.SessionRunHook(chief_queue_runner)\n    summary_op = tf.summary.merge_all()\n    summary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='./tmp_test3',summary_op=summary_op)\n    previous=[]\n    with tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\n        tf.train.start_queue_runners(sess=sess)\n        # sess.run(init_token_op\n        \n        for i in range(1024):\n            example,label_to_train = sess.run([mfcc_features_batch,labels_batch])\n            print(example.shape,label_to_train.shape)\n            # print(sess.run([act4],feed_dict={x:example}))\n            loss,glob_step= sess.run([xent,global_step],feed_dict={x:example,y:label_to_train})\n            \n            print(loss,glob_step)\n            if np.array_equiv(previous,example):\n                print(\"it's same!\")\n            else:\n                previous = example\n                print(\"not same!\")\n</pre>", "body_text": "I build up my code step by step. While the code to the xent calculation it still works fine.\nBut when I added the definition of the train_op as below:\ngrad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\nrep_op = tf.train.SyncReplicasOptimizer(grad_op,\n                                                replicas_to_aggregate=3,\n                                                total_num_replicas=3)\ntrain_op = rep_op.minimize(xent,global_step=global_step)\nsync_replicas_hook = rep_op.make_session_run_hook(is_chief)\ninit_token_op = rep_op.get_init_tokens_op()\nchief_queue_runner = rep_op.get_chief_queue_runner()\n\nIt's start show the error message getting the -1 negative dimension\nlooks like this issue have been open at  #11823\nIs this the tensorflow version bugs?\nHere is my new full source code:\n'''\npc-01$ python example.py --job-name=\"ps\" --task_index=0 \npc-02$ python example.py --job-name=\"worker\" --task_index=0 \npc-03$ python example.py --job-name=\"worker\" --task_index=1 \npc-04$ python example.py --job-name=\"worker\" --task_index=2 \n\nreference from ischlag.github.io\n'''\nfrom __future__ import print_function\nimport os\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom os import path\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport sys\nimport gzip\nimport math\n#cluster specification\nparameter_servers = [\"192.168.1.7:2223\"]\nworkers = [\"192.168.1.2:2223\",\n           \"192.168.1.8:2223\",\n           \"192.168.1.9:2223\"]\n\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\n# input flags\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_string(\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\nFLAGS = tf.app.flags.FLAGS\n\n# start a server for a specific task\nif FLAGS.job_name == \"ps\":\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\nelse:\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\n#input config\nnum_input = 13\nnum_classes = 3\nh_layer_units = 512\nnumber_of_h_layer = 6\ntest_timing_point = 512\nlearning_rate = 1E-4\nbatch_size = 1024\n\nif FLAGS.job_name == \"ps\":\n    print(\"calling join to serve\")\n    server.join()\nelif FLAGS.job_name == \"worker\":\n\n#-----function definition-------\n    \n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\n    reader = tf.TextLineReader()\n    key, value = reader.read(filename_queue)\n    record_default = [[1.0] for _ in range(14)]\n    record_default[0] = [1]\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\n    labels = tf.one_hot(labels,depth=3,dtype=tf.int32)\n    labels_batch, mfcc_features_batch = tf.train.batch(\n            [labels, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\n    tf.summary.tensor_summary(\"label\",labels)\n    \n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" %FLAGS.task_index,cluster=cluster)):\n        is_chief = (FLAGS.task_index == 0)\n        global_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32,trainable=False) \n        x = tf.placeholder(tf.float32,shape=[None,num_input],name=\"mfcc_input\")\n        y = tf.placeholder(tf.int32,shape=[None,num_classes],name=\"labels\")\n        w1 = tf.Variable(tf.truncated_normal([num_input,512],stddev=0.1),name=\"W\")\n        b1 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\n        act1 = tf.nn.tanh(tf.add(tf.matmul(x,w1),b1))\n\n        w2 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W\")\n        b2 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\n        act2 = tf.nn.tanh(tf.add(tf.matmul(act1,w2),b2))\n\n        w3 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W\")\n        b3 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\n        act3 = tf.nn.tanh(tf.add(tf.matmul(act2,w3),b3))\n\n\n\n        w4 = tf.Variable(tf.truncated_normal(shape=[512,3],stddev=0.1),name=\"W\")\n        b4 = tf.Variable(tf.truncated_normal(shape=[3],stddev=0.1),name=\"B\")\n        act4 = tf.add(tf.matmul(act3,w4),b4)\n\n        # #----loss----\n        xent = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(\n                logits=act4,labels=y),name=\"x-entropy\")\n\n\n        # ----train----\n        grad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\n        rep_op = tf.train.SyncReplicasOptimizer(grad_op,\n                                                replicas_to_aggregate=3,\n                                                total_num_replicas=3)\n        # # rep_op.compute_gradients()\n        train_op = rep_op.minimize(xent,global_step=global_step)\n        sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\n        init_token_op = rep_op.get_init_tokens_op()\n        chief_queue_runner = rep_op.get_chief_queue_runner()\n        #----- accuracy\n        # correct_prediction = tf.equal(tf.argmax(act4,1),tf.argmax(y,1))\n        # accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n    \n    tf.summary.scalar(\"x-entropy\",xent)\n    # tf.summary.scalar(\"accuracy\",accuracy)\n\n \n\n    # server = tf.train.Server.create_local_server()\n    # session_run_hook = tf.train.SessionRunHook(chief_queue_runner)\n    summary_op = tf.summary.merge_all()\n    summary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='./tmp_test3',summary_op=summary_op)\n    previous=[]\n    with tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\n        tf.train.start_queue_runners(sess=sess)\n        # sess.run(init_token_op\n        \n        for i in range(1024):\n            example,label_to_train = sess.run([mfcc_features_batch,labels_batch])\n            print(example.shape,label_to_train.shape)\n            # print(sess.run([act4],feed_dict={x:example}))\n            loss,glob_step= sess.run([xent,global_step],feed_dict={x:example,y:label_to_train})\n            \n            print(loss,glob_step)\n            if np.array_equiv(previous,example):\n                print(\"it's same!\")\n            else:\n                previous = example\n                print(\"not same!\")", "body": "I build up my code step by step. While the code to the `xent` calculation it still works fine.\r\nBut when I added the definition of the train_op as below:\r\n<pre>\r\ngrad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\nrep_op = tf.train.SyncReplicasOptimizer(grad_op,\r\n                                                replicas_to_aggregate=3,\r\n                                                total_num_replicas=3)\r\ntrain_op = rep_op.minimize(xent,global_step=global_step)\r\nsync_replicas_hook = rep_op.make_session_run_hook(is_chief)\r\ninit_token_op = rep_op.get_init_tokens_op()\r\nchief_queue_runner = rep_op.get_chief_queue_runner()\r\n</pre>\r\nIt's start show the error message getting the -1 negative dimension\r\nlooks like this issue have been open at  #11823\r\nIs this the tensorflow version bugs?\r\n\r\nHere is my new full source code:\r\n<pre>\r\n'''\r\npc-01$ python example.py --job-name=\"ps\" --task_index=0 \r\npc-02$ python example.py --job-name=\"worker\" --task_index=0 \r\npc-03$ python example.py --job-name=\"worker\" --task_index=1 \r\npc-04$ python example.py --job-name=\"worker\" --task_index=2 \r\n\r\nreference from ischlag.github.io\r\n'''\r\nfrom __future__ import print_function\r\nimport os\r\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nfrom os import path\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport time\r\nimport sys\r\nimport gzip\r\nimport math\r\n#cluster specification\r\nparameter_servers = [\"192.168.1.7:2223\"]\r\nworkers = [\"192.168.1.2:2223\",\r\n           \"192.168.1.8:2223\",\r\n           \"192.168.1.9:2223\"]\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n\r\n# input flags\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_string(\r\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\r\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n# start a server for a specific task\r\nif FLAGS.job_name == \"ps\":\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\nelse:\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\n#input config\r\nnum_input = 13\r\nnum_classes = 3\r\nh_layer_units = 512\r\nnumber_of_h_layer = 6\r\ntest_timing_point = 512\r\nlearning_rate = 1E-4\r\nbatch_size = 1024\r\n\r\nif FLAGS.job_name == \"ps\":\r\n    print(\"calling join to serve\")\r\n    server.join()\r\nelif FLAGS.job_name == \"worker\":\r\n\r\n#-----function definition-------\r\n    \r\n    filename_queue = tf.train.string_input_producer([\"csv_files/mfcc_combine_train_shuffled.csv\",\"csv_files/mfcc_combine_test_shuffled.csv\"])\r\n    reader = tf.TextLineReader()\r\n    key, value = reader.read(filename_queue)\r\n    record_default = [[1.0] for _ in range(14)]\r\n    record_default[0] = [1]\r\n    labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(value, record_defaults=record_default)\r\n    mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\r\n                            mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\r\n    labels = tf.one_hot(labels,depth=3,dtype=tf.int32)\r\n    labels_batch, mfcc_features_batch = tf.train.batch(\r\n            [labels, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\r\n    tf.summary.tensor_summary(\"label\",labels)\r\n    \r\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" %FLAGS.task_index,cluster=cluster)):\r\n        is_chief = (FLAGS.task_index == 0)\r\n        global_step = tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),dtype=tf.int32,trainable=False) \r\n        x = tf.placeholder(tf.float32,shape=[None,num_input],name=\"mfcc_input\")\r\n        y = tf.placeholder(tf.int32,shape=[None,num_classes],name=\"labels\")\r\n        w1 = tf.Variable(tf.truncated_normal([num_input,512],stddev=0.1),name=\"W\")\r\n        b1 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\r\n        act1 = tf.nn.tanh(tf.add(tf.matmul(x,w1),b1))\r\n\r\n        w2 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W\")\r\n        b2 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\r\n        act2 = tf.nn.tanh(tf.add(tf.matmul(act1,w2),b2))\r\n\r\n        w3 = tf.Variable(tf.truncated_normal([512,512],stddev=0.1),name=\"W\")\r\n        b3 = tf.Variable(tf.constant(0.1,shape=[512]),name=\"B\")\r\n        act3 = tf.nn.tanh(tf.add(tf.matmul(act2,w3),b3))\r\n\r\n\r\n\r\n        w4 = tf.Variable(tf.truncated_normal(shape=[512,3],stddev=0.1),name=\"W\")\r\n        b4 = tf.Variable(tf.truncated_normal(shape=[3],stddev=0.1),name=\"B\")\r\n        act4 = tf.add(tf.matmul(act3,w4),b4)\r\n\r\n        # #----loss----\r\n        xent = tf.reduce_mean(\r\n            tf.nn.softmax_cross_entropy_with_logits(\r\n                logits=act4,labels=y),name=\"x-entropy\")\r\n\r\n\r\n        # ----train----\r\n        grad_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n        rep_op = tf.train.SyncReplicasOptimizer(grad_op,\r\n                                                replicas_to_aggregate=3,\r\n                                                total_num_replicas=3)\r\n        # # rep_op.compute_gradients()\r\n        train_op = rep_op.minimize(xent,global_step=global_step)\r\n        sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\r\n        init_token_op = rep_op.get_init_tokens_op()\r\n        chief_queue_runner = rep_op.get_chief_queue_runner()\r\n        #----- accuracy\r\n        # correct_prediction = tf.equal(tf.argmax(act4,1),tf.argmax(y,1))\r\n        # accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n    \r\n    tf.summary.scalar(\"x-entropy\",xent)\r\n    # tf.summary.scalar(\"accuracy\",accuracy)\r\n\r\n \r\n\r\n    # server = tf.train.Server.create_local_server()\r\n    # session_run_hook = tf.train.SessionRunHook(chief_queue_runner)\r\n    summary_op = tf.summary.merge_all()\r\n    summary_hook = tf.train.SummarySaverHook(save_secs=1,output_dir='./tmp_test3',summary_op=summary_op)\r\n    previous=[]\r\n    with tf.train.MonitoredTrainingSession(master=server.target,hooks=[summary_hook],is_chief=True) as sess:\r\n        tf.train.start_queue_runners(sess=sess)\r\n        # sess.run(init_token_op\r\n        \r\n        for i in range(1024):\r\n            example,label_to_train = sess.run([mfcc_features_batch,labels_batch])\r\n            print(example.shape,label_to_train.shape)\r\n            # print(sess.run([act4],feed_dict={x:example}))\r\n            loss,glob_step= sess.run([xent,global_step],feed_dict={x:example,y:label_to_train})\r\n            \r\n            print(loss,glob_step)\r\n            if np.array_equiv(previous,example):\r\n                print(\"it's same!\")\r\n            else:\r\n                previous = example\r\n                print(\"not same!\")\r\n</pre>\r\n"}