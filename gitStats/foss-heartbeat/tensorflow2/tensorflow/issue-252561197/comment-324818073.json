{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324818073", "html_url": "https://github.com/tensorflow/tensorflow/issues/12556#issuecomment-324818073", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12556", "id": 324818073, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDgxODA3Mw==", "user": {"login": "luvwinnie", "id": 13714992, "node_id": "MDQ6VXNlcjEzNzE0OTky", "avatar_url": "https://avatars1.githubusercontent.com/u/13714992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luvwinnie", "html_url": "https://github.com/luvwinnie", "followers_url": "https://api.github.com/users/luvwinnie/followers", "following_url": "https://api.github.com/users/luvwinnie/following{/other_user}", "gists_url": "https://api.github.com/users/luvwinnie/gists{/gist_id}", "starred_url": "https://api.github.com/users/luvwinnie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luvwinnie/subscriptions", "organizations_url": "https://api.github.com/users/luvwinnie/orgs", "repos_url": "https://api.github.com/users/luvwinnie/repos", "events_url": "https://api.github.com/users/luvwinnie/events{/privacy}", "received_events_url": "https://api.github.com/users/luvwinnie/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-25T03:59:35Z", "updated_at": "2017-08-25T04:02:57Z", "author_association": "NONE", "body_html": "<p>here is my full source:<br>\ninfo:<br>\n4 Nodes PC with Ubuntu 16.04LTS<br>\n1 parameter server and 3 worker<br>\nParameter server specs:<br>\n2 GPUs<br>\nworkers specs:<br>\n1: 1GPU,   2: 2 GPUs, 3: 1 GPU<br>\nThe reason i use the 2Gpus for parameters server is because it has faster CPU for my code to process the queue input pipeline faster.</p>\n<pre>`'''\n192.168.1.7$ python example.py --job-name=\"ps\" --task_index=0 \n192.168.1.2$ python example.py --job-name=\"worker\" --task_index=0 \n192.168.1.8$ python example.py --job-name=\"worker\" --task_index=1 \n192.168.1.9$ python example.py --job-name=\"worker\" --task_index=2 \n\nreference from ischlag.github.io\n'''\nfrom __future__ import print_function\nimport os\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom os import path\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport sys\nimport gzip\n\n#cluster specification\nparameter_servers = [\"192.168.1.7:2223\"]\nworkers = [\"192.168.1.2:2223\",\n           \"192.168.1.8:2223\",\n           \"192.168.1.9:2223\"]\n\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\n# input flags\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_string(\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\nFLAGS = tf.app.flags.FLAGS\n\n# start a server for a specific task\nif FLAGS.job_name == \"ps\":\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\nelse:\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\n#input config\nnum_input = 13\nnum_classes = 3\nh_layer_units = 512\nnumber_of_h_layer = 6\ntest_timing_point = 512\nlearning_rate = 1E-4\n\nif FLAGS.job_name == \"ps\":\n    print(\"calling join to serve\")\n    server.join()\nelif FLAGS.job_name == \"worker\":\n\n#-----function definition-------\n    def extract_data_set(filename, batch_size, shuffle_batch=False,name=None):\n        filename_queue = tf.train.string_input_producer([filename])\n        reader = tf.TextLineReader()\n        key, value = reader.read(filename_queue)\n        record_default = [[1], [1.0], [1.0], [1.0], [1.0], [1.0], [\n            1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]]\n        labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(\n            value, record_defaults=record_default)\n        mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\n                                mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\n        labels = tf.one_hot(labels,depth=3,dtype=tf.int32,name=\"labels_hot\")\n        labels_batch, mfcc_features_batch = tf.train.batch(\n            [labels, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\n        return labels_batch, mfcc_features_batch\n\n    def hidden_layer(input, size_in, size_out, name=\"hidden_layer\",reuse=False):\n        with tf.name_scope(name):\n            w = tf.Variable(tf.truncated_normal([size_in,size_out],stddev=0.1),name=\"W\")\n            b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=\"B\")\n            act = tf.nn.tanh(tf.add(tf.matmul(input, w), b))\n            tf.summary.histogram(\"weights\", w)\n            tf.summary.histogram(\"bias\", b)\n            tf.summary.histogram(\"activations\", act)\n            return act\n\n\n    def fully_connected_layer(input, size_in, size_out, name=\"fc\",reuse=False):\n        with tf.name_scope(name):\n            w = tf.Variable(tf.truncated_normal(shape=[size_in,size_out],stddev=0.1),name=\"W\")\n            b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=\"B\")\n            act = tf.add(tf.matmul(input, w), b)\n            tf.summary.histogram(\"weights\", w)\n            tf.summary.histogram(\"bias\", b)\n            tf.summary.histogram(\"activations\", act)\n            return act\n#-------start program------------\n    print(\"calling worker\")\n    with tf.device(\"/job:ps/task:0\"):\n        print(\"Calculating data size...\")\n        with gzip.open('csv_files/mfcc_combine_train_shuffled.gz') as f:\n            text = f.readlines()\n            size1 = len(text)\n            print(\"train:{}\".format(size1))\n        with gzip.open('csv_files/mfcc_combine_test_shuffled.gz') as f:\n            text = f.readlines()\n            size2 = len(text)\n            print(\"train:{}\".format(size2))\n\n\n        del text\n        test_timing = (size2 / FLAGS.batch_size) / \\\n            (size1 / FLAGS.batch_size / FLAGS.test_timing_point)\n        print(\"test timing:{}\".format(test_timing))\n        steps = size1 / FLAGS.batch_size\n        print(\"steps:{}\".format(steps))\n    with tf.device(\"/job:ps/task:0\"):\n        train_labels_batch, train_mfcc_batch = extract_data_set(\n            \"csv_files/mfcc_combine_train_shuffled.csv\", FLAGS.batch_size,\"train\")\n        test_labels_batch, test_mfcc_batch = extract_data_set(\n            \"csv_files/mfcc_combine_test_shuffled.csv\", FLAGS.batch_size,\"test\")\n\n    # def sound_classifier_model(learning_rate, number_of_h_layer, h_layer_units, hparam):\n        # tf.reset_default_graph()\n    #saver = tf.train.Saver(sharded=True)\n    \n    # Between-graph replication model\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" %FLAGS.task_index,cluster=cluster)):\n        is_chief = (FLAGS.task_index == 0)\n        global_step = tf.get_variable('global_step', [], \n                                      initializer = tf.constant_initializer(0),dtype=tf.int32)\n        # global_step = tf.Variable(0,name='global_step')\n        #global_step=tf.get_variable('global_step', shape=[], initializer=tf.zeros_initializer(), dtype=tf.int32, trainable=False)\n        x = tf.placeholder(tf.float32,shape=[None,num_input],name=\"mfcc_input\")\n        y = tf.placeholder(tf.int32,shape=[None,num_classes],name=\"labels\")\n        is_training = tf.placeholder(dtype=bool, shape=(), name=\"is_training\")\n        q_selector = tf.cond(is_training,lambda:[train_mfcc_batch,train_labels_batch],lambda:[test_mfcc_batch,test_labels_batch])\n\n        h = []\n        h.append(hidden_layer(x,num_input,h_layer_units))\n        for i in range(number_of_h_layer):\n            h.append(hidden_layer(h[i],h_layer_units,h_layer_units))\n\n        output = fully_connected_layer(h[number_of_h_layer],h_layer_units,num_classes)\n        saver = tf.train.Saver()\n        with tf.name_scope(\"x-entropy\"):\n            xent = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(\n                    logits=output, labels=y), name=\"x-entropy\")\n\n        with tf.name_scope(\"train\"):\n            grad_op = tf.train.AdamOptimizer(learning_rate)\n            rep_op = tf.train.SyncReplicasOptimizer(grad_op,\n                                                    replicas_to_aggregate=len(workers),\n                                                    total_num_replicas=len(workers))\n            train_op = rep_op.minimize(xent,global_step=global_step)\n            sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\n            # grads = rep_op.compute_gradients(xent)\n            # apply_gradients_op = rep_op.apply_gradients(grads,global_step=global_step)\n            # with tf.control_dependencies([apply_gradients_op]):\n                # train_op=tf.identity(xent,name='train_op')\n            # train_step = rep_op.minimize(xent,global_step=global_step,aggregation_method=tf.AggregationMethod.ADD_N)\n        init_token_op = rep_op.get_init_tokens_op()\n        chief_queue_runner = rep_op.get_chief_queue_runner()\n        with tf.name_scope(\"accuracy\"):\n            correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(y,1))\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\n        tf.summary.scalar(\"x-entropy\", xent)\n        tf.summary.scalar(\"accuracy\",accuracy)\n        \n        summary_op = tf.summary.merge_all()\n        global_init_op = tf.global_variables_initializer()\n        local_init_op = tf.local_variables_initializer()\n        \n    #------------prepare for session------------\n    # coord = tf.train.Coordinator()\n    print(\"creating Supervisor...\")\n    # sv = tf.train.Supervisor(is_chief=is_cheif,\n    #                         global_step=global_step,\n    #                         saver = saver,\n    #                         logdir=FLAGS.log_dir)\n    print(\"making config...\")\n    sess_config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=True,device_filters=[\"/job:ps\",\"/job:worker/task:%d\"%FLAGS.task_index])\n    begin_time = time.time()\n    summary_hook = tf.train.SummarySaverHook(save_secs=600,output_dir=FLAGS.log_dir,summary_op=summary_op)\n    checkpoint_hook = tf.train.CheckpointSaverHook(save_steps=test_timing,checkpoint_dir=FLAGS.log_dir,saver=saver)\n\n    #---------------training session--------------\n    print(\"waiting for session prepare....\")\n    with tf.train.MonitoredTrainingSession(server.target,is_chief=is_chief,hooks=[sync_replicas_hook,summary_hook,checkpoint_hook],config=sess_config) as sess:\n        while not sess.should_stop():\n            '''\n            # is cheif\n            if FLAGS.task_index == 0:\n                sv.start_queue_runners(sess,[cheif_queue_runner])\n                sess.run(init_token_op)\n            '''\n            print(\"in session\")\n            # while not sv.should_stop():\n                # coord = tf.train.Coordinator()\n            # sess.run(tf.global_variables_initializer())\n            # sess.run(tf.local_variables_initializer())\n            # coord = tf.train.Coordinator()\n            # if FLAGS.task_index == 0:\n            tf.train.start_queue_runners(sess=sess)\n             sess.run(init_token_op)\n            # sv.start_queue_runners(sess=sess)\n            with tf.device(\"/job:ps/task:0/cpu:0\"):\n                f = open(FLAGS.log_dir + \"accuracy_\" + str(learning_rate) +\n                        \"_with_\" + str(number_of_h_layer) + \"layers\", \"w+\")\n                print(\"loading data to queue....\")\n            \n            print(\"done loading to queue\")\n            print(\"start adding summary\")\n            with tf.device(\"/job:ps/task:0/cpu:0\"):\n                f = open(FLAGS.log_dir+\"accuracy_\"+str(learning_rate)+\"_with_\"+str(number_of_h_layer)+\"layers\",\"w+\")\n\n                # writer = tf.summary.FileWriter(FLAGS.log_dir,graph=tf.get_default_graph())\n                # writer = tf.summary.FileWriter(FLAGS.log_dir + \"lr_%.0E layers:%d\"%(learning_rate,6))\n                # writer.add_graph(sess.graph)\n                #perform training cycles\n                start_time = time.time()\n\n            print(\"start training\")\n            for i in range(steps):\n                print(\"read data\")\n                mfcc_batch,label_batch = sess.run(q_selector,feed_dict={is_training:True})\n                # label_batch = sess.run(tf.one_hot(y_batch,depth=3,dtype=tf.int32,name=\"labels\"))\n                [_,loss,glob_step,train_accuracy] = sess.run([train_op,xent,global_step,accuracy],feed_dict={x:mfcc_batch,y:label_batch})\n                # [train_accuracy,s] = sess.run([accuracy,summary_op],feed_dict={x:mfcc_batch,y:label_batch})\n                print(\"done\")\n                elapsed = round(time.time()-start_time,2)\n                sys.stdout.write(\"\\r\")\n                sys.stdout.write(\"learning_rate:{0},{1} layers , training:{2}/{3} , train_accuracy:{4} [elapsed_time:{5:.2f}] \".format(learning_rate,number_of_h_layer,i,steps,train_accuracy,elapsed))\n                sys.stdout.flush()\n                # writer.add_summary(summary,i)\n                print(\"before train\")\n                # sess.run(global_step,init_token_op,chief_queue_runner)\n                # sess.run(train_op,xent)\n                print(\"after train\")\n                if i != 0 and i % test_timing == 0:\n                    # saver.save(sess, os.path.join(FLAGS.log_dir, \"model.ckpt\"), i)\n                    test_time += 1\n                    for j in range(test_timing):\n                        mfcc_batch,y_batch = sess.run(q_selector,feed_dict={is_training:False})\n                        test_label = sess.run(tf.one_hot(y_batch,depth=3,dtype=tf.int32,name=\"test_label\"))\n                        test_accuracy = accuracy.eval(feed_dict={x:mfcc_batch,y:test_label})\n                        sys.stdout.write(\"\\r\")\n                        sys.stdout.write(\"{}layers: test_num:{} testing:{} test_accuracy:{}\".format(number_of_h_layer,test_time,j,test_accuracy))\n                        sys.stdout.flush()\n                        with tf.device(\"/job:ps/task:0/cpu:0\"):\n                            f.write(\"{}layers: test_num:{} testing:{} test_accuracy:{}\\n\".format(number_of_h_layer,test_time,j,test_accuracy))\n                            f.flush()\n\n    f.write(\"total_training for lr {},{}layers time:{} minutes\".format(learning_rate,number_of_h_layer,((time.time()-start_time)/60.0)))\n    f.flush()\n    f.close()\n    print(\"-----%s minutes ------\"%((time.time()-start_time)/60.0))\n#    sv.stop()\n    print(\"Done!\")\n`\n</pre>\n<p>I run as the code provided comments in respective IP PC but it shows the error as previous<br>\nThe Worker with task index 0 just hangs at the error.</p>\n<p>Is this the problem of my Placeholder without explicitly define the size?The [None,13] part.</p>", "body_text": "here is my full source:\ninfo:\n4 Nodes PC with Ubuntu 16.04LTS\n1 parameter server and 3 worker\nParameter server specs:\n2 GPUs\nworkers specs:\n1: 1GPU,   2: 2 GPUs, 3: 1 GPU\nThe reason i use the 2Gpus for parameters server is because it has faster CPU for my code to process the queue input pipeline faster.\n`'''\n192.168.1.7$ python example.py --job-name=\"ps\" --task_index=0 \n192.168.1.2$ python example.py --job-name=\"worker\" --task_index=0 \n192.168.1.8$ python example.py --job-name=\"worker\" --task_index=1 \n192.168.1.9$ python example.py --job-name=\"worker\" --task_index=2 \n\nreference from ischlag.github.io\n'''\nfrom __future__ import print_function\nimport os\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom os import path\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport time\nimport sys\nimport gzip\n\n#cluster specification\nparameter_servers = [\"192.168.1.7:2223\"]\nworkers = [\"192.168.1.2:2223\",\n           \"192.168.1.8:2223\",\n           \"192.168.1.9:2223\"]\n\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n\n# input flags\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_string(\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\ntf.app.flags.DEFINE_integer(\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\nFLAGS = tf.app.flags.FLAGS\n\n# start a server for a specific task\nif FLAGS.job_name == \"ps\":\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\nelse:\n    server = tf.train.Server(cluster, \n                        job_name=FLAGS.job_name,\n                        task_index=FLAGS.task_index)\n#input config\nnum_input = 13\nnum_classes = 3\nh_layer_units = 512\nnumber_of_h_layer = 6\ntest_timing_point = 512\nlearning_rate = 1E-4\n\nif FLAGS.job_name == \"ps\":\n    print(\"calling join to serve\")\n    server.join()\nelif FLAGS.job_name == \"worker\":\n\n#-----function definition-------\n    def extract_data_set(filename, batch_size, shuffle_batch=False,name=None):\n        filename_queue = tf.train.string_input_producer([filename])\n        reader = tf.TextLineReader()\n        key, value = reader.read(filename_queue)\n        record_default = [[1], [1.0], [1.0], [1.0], [1.0], [1.0], [\n            1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]]\n        labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(\n            value, record_defaults=record_default)\n        mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\n                                mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\n        labels = tf.one_hot(labels,depth=3,dtype=tf.int32,name=\"labels_hot\")\n        labels_batch, mfcc_features_batch = tf.train.batch(\n            [labels, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\n        return labels_batch, mfcc_features_batch\n\n    def hidden_layer(input, size_in, size_out, name=\"hidden_layer\",reuse=False):\n        with tf.name_scope(name):\n            w = tf.Variable(tf.truncated_normal([size_in,size_out],stddev=0.1),name=\"W\")\n            b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=\"B\")\n            act = tf.nn.tanh(tf.add(tf.matmul(input, w), b))\n            tf.summary.histogram(\"weights\", w)\n            tf.summary.histogram(\"bias\", b)\n            tf.summary.histogram(\"activations\", act)\n            return act\n\n\n    def fully_connected_layer(input, size_in, size_out, name=\"fc\",reuse=False):\n        with tf.name_scope(name):\n            w = tf.Variable(tf.truncated_normal(shape=[size_in,size_out],stddev=0.1),name=\"W\")\n            b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=\"B\")\n            act = tf.add(tf.matmul(input, w), b)\n            tf.summary.histogram(\"weights\", w)\n            tf.summary.histogram(\"bias\", b)\n            tf.summary.histogram(\"activations\", act)\n            return act\n#-------start program------------\n    print(\"calling worker\")\n    with tf.device(\"/job:ps/task:0\"):\n        print(\"Calculating data size...\")\n        with gzip.open('csv_files/mfcc_combine_train_shuffled.gz') as f:\n            text = f.readlines()\n            size1 = len(text)\n            print(\"train:{}\".format(size1))\n        with gzip.open('csv_files/mfcc_combine_test_shuffled.gz') as f:\n            text = f.readlines()\n            size2 = len(text)\n            print(\"train:{}\".format(size2))\n\n\n        del text\n        test_timing = (size2 / FLAGS.batch_size) / \\\n            (size1 / FLAGS.batch_size / FLAGS.test_timing_point)\n        print(\"test timing:{}\".format(test_timing))\n        steps = size1 / FLAGS.batch_size\n        print(\"steps:{}\".format(steps))\n    with tf.device(\"/job:ps/task:0\"):\n        train_labels_batch, train_mfcc_batch = extract_data_set(\n            \"csv_files/mfcc_combine_train_shuffled.csv\", FLAGS.batch_size,\"train\")\n        test_labels_batch, test_mfcc_batch = extract_data_set(\n            \"csv_files/mfcc_combine_test_shuffled.csv\", FLAGS.batch_size,\"test\")\n\n    # def sound_classifier_model(learning_rate, number_of_h_layer, h_layer_units, hparam):\n        # tf.reset_default_graph()\n    #saver = tf.train.Saver(sharded=True)\n    \n    # Between-graph replication model\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" %FLAGS.task_index,cluster=cluster)):\n        is_chief = (FLAGS.task_index == 0)\n        global_step = tf.get_variable('global_step', [], \n                                      initializer = tf.constant_initializer(0),dtype=tf.int32)\n        # global_step = tf.Variable(0,name='global_step')\n        #global_step=tf.get_variable('global_step', shape=[], initializer=tf.zeros_initializer(), dtype=tf.int32, trainable=False)\n        x = tf.placeholder(tf.float32,shape=[None,num_input],name=\"mfcc_input\")\n        y = tf.placeholder(tf.int32,shape=[None,num_classes],name=\"labels\")\n        is_training = tf.placeholder(dtype=bool, shape=(), name=\"is_training\")\n        q_selector = tf.cond(is_training,lambda:[train_mfcc_batch,train_labels_batch],lambda:[test_mfcc_batch,test_labels_batch])\n\n        h = []\n        h.append(hidden_layer(x,num_input,h_layer_units))\n        for i in range(number_of_h_layer):\n            h.append(hidden_layer(h[i],h_layer_units,h_layer_units))\n\n        output = fully_connected_layer(h[number_of_h_layer],h_layer_units,num_classes)\n        saver = tf.train.Saver()\n        with tf.name_scope(\"x-entropy\"):\n            xent = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(\n                    logits=output, labels=y), name=\"x-entropy\")\n\n        with tf.name_scope(\"train\"):\n            grad_op = tf.train.AdamOptimizer(learning_rate)\n            rep_op = tf.train.SyncReplicasOptimizer(grad_op,\n                                                    replicas_to_aggregate=len(workers),\n                                                    total_num_replicas=len(workers))\n            train_op = rep_op.minimize(xent,global_step=global_step)\n            sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\n            # grads = rep_op.compute_gradients(xent)\n            # apply_gradients_op = rep_op.apply_gradients(grads,global_step=global_step)\n            # with tf.control_dependencies([apply_gradients_op]):\n                # train_op=tf.identity(xent,name='train_op')\n            # train_step = rep_op.minimize(xent,global_step=global_step,aggregation_method=tf.AggregationMethod.ADD_N)\n        init_token_op = rep_op.get_init_tokens_op()\n        chief_queue_runner = rep_op.get_chief_queue_runner()\n        with tf.name_scope(\"accuracy\"):\n            correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(y,1))\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\n        tf.summary.scalar(\"x-entropy\", xent)\n        tf.summary.scalar(\"accuracy\",accuracy)\n        \n        summary_op = tf.summary.merge_all()\n        global_init_op = tf.global_variables_initializer()\n        local_init_op = tf.local_variables_initializer()\n        \n    #------------prepare for session------------\n    # coord = tf.train.Coordinator()\n    print(\"creating Supervisor...\")\n    # sv = tf.train.Supervisor(is_chief=is_cheif,\n    #                         global_step=global_step,\n    #                         saver = saver,\n    #                         logdir=FLAGS.log_dir)\n    print(\"making config...\")\n    sess_config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=True,device_filters=[\"/job:ps\",\"/job:worker/task:%d\"%FLAGS.task_index])\n    begin_time = time.time()\n    summary_hook = tf.train.SummarySaverHook(save_secs=600,output_dir=FLAGS.log_dir,summary_op=summary_op)\n    checkpoint_hook = tf.train.CheckpointSaverHook(save_steps=test_timing,checkpoint_dir=FLAGS.log_dir,saver=saver)\n\n    #---------------training session--------------\n    print(\"waiting for session prepare....\")\n    with tf.train.MonitoredTrainingSession(server.target,is_chief=is_chief,hooks=[sync_replicas_hook,summary_hook,checkpoint_hook],config=sess_config) as sess:\n        while not sess.should_stop():\n            '''\n            # is cheif\n            if FLAGS.task_index == 0:\n                sv.start_queue_runners(sess,[cheif_queue_runner])\n                sess.run(init_token_op)\n            '''\n            print(\"in session\")\n            # while not sv.should_stop():\n                # coord = tf.train.Coordinator()\n            # sess.run(tf.global_variables_initializer())\n            # sess.run(tf.local_variables_initializer())\n            # coord = tf.train.Coordinator()\n            # if FLAGS.task_index == 0:\n            tf.train.start_queue_runners(sess=sess)\n             sess.run(init_token_op)\n            # sv.start_queue_runners(sess=sess)\n            with tf.device(\"/job:ps/task:0/cpu:0\"):\n                f = open(FLAGS.log_dir + \"accuracy_\" + str(learning_rate) +\n                        \"_with_\" + str(number_of_h_layer) + \"layers\", \"w+\")\n                print(\"loading data to queue....\")\n            \n            print(\"done loading to queue\")\n            print(\"start adding summary\")\n            with tf.device(\"/job:ps/task:0/cpu:0\"):\n                f = open(FLAGS.log_dir+\"accuracy_\"+str(learning_rate)+\"_with_\"+str(number_of_h_layer)+\"layers\",\"w+\")\n\n                # writer = tf.summary.FileWriter(FLAGS.log_dir,graph=tf.get_default_graph())\n                # writer = tf.summary.FileWriter(FLAGS.log_dir + \"lr_%.0E layers:%d\"%(learning_rate,6))\n                # writer.add_graph(sess.graph)\n                #perform training cycles\n                start_time = time.time()\n\n            print(\"start training\")\n            for i in range(steps):\n                print(\"read data\")\n                mfcc_batch,label_batch = sess.run(q_selector,feed_dict={is_training:True})\n                # label_batch = sess.run(tf.one_hot(y_batch,depth=3,dtype=tf.int32,name=\"labels\"))\n                [_,loss,glob_step,train_accuracy] = sess.run([train_op,xent,global_step,accuracy],feed_dict={x:mfcc_batch,y:label_batch})\n                # [train_accuracy,s] = sess.run([accuracy,summary_op],feed_dict={x:mfcc_batch,y:label_batch})\n                print(\"done\")\n                elapsed = round(time.time()-start_time,2)\n                sys.stdout.write(\"\\r\")\n                sys.stdout.write(\"learning_rate:{0},{1} layers , training:{2}/{3} , train_accuracy:{4} [elapsed_time:{5:.2f}] \".format(learning_rate,number_of_h_layer,i,steps,train_accuracy,elapsed))\n                sys.stdout.flush()\n                # writer.add_summary(summary,i)\n                print(\"before train\")\n                # sess.run(global_step,init_token_op,chief_queue_runner)\n                # sess.run(train_op,xent)\n                print(\"after train\")\n                if i != 0 and i % test_timing == 0:\n                    # saver.save(sess, os.path.join(FLAGS.log_dir, \"model.ckpt\"), i)\n                    test_time += 1\n                    for j in range(test_timing):\n                        mfcc_batch,y_batch = sess.run(q_selector,feed_dict={is_training:False})\n                        test_label = sess.run(tf.one_hot(y_batch,depth=3,dtype=tf.int32,name=\"test_label\"))\n                        test_accuracy = accuracy.eval(feed_dict={x:mfcc_batch,y:test_label})\n                        sys.stdout.write(\"\\r\")\n                        sys.stdout.write(\"{}layers: test_num:{} testing:{} test_accuracy:{}\".format(number_of_h_layer,test_time,j,test_accuracy))\n                        sys.stdout.flush()\n                        with tf.device(\"/job:ps/task:0/cpu:0\"):\n                            f.write(\"{}layers: test_num:{} testing:{} test_accuracy:{}\\n\".format(number_of_h_layer,test_time,j,test_accuracy))\n                            f.flush()\n\n    f.write(\"total_training for lr {},{}layers time:{} minutes\".format(learning_rate,number_of_h_layer,((time.time()-start_time)/60.0)))\n    f.flush()\n    f.close()\n    print(\"-----%s minutes ------\"%((time.time()-start_time)/60.0))\n#    sv.stop()\n    print(\"Done!\")\n`\n\nI run as the code provided comments in respective IP PC but it shows the error as previous\nThe Worker with task index 0 just hangs at the error.\nIs this the problem of my Placeholder without explicitly define the size?The [None,13] part.", "body": "here is my full source:\r\ninfo:\r\n4 Nodes PC with Ubuntu 16.04LTS\r\n1 parameter server and 3 worker\r\nParameter server specs:\r\n2 GPUs\r\nworkers specs:\r\n1: 1GPU,   2: 2 GPUs, 3: 1 GPU\r\nThe reason i use the 2Gpus for parameters server is because it has faster CPU for my code to process the queue input pipeline faster.\r\n<pre>\r\n`'''\r\n192.168.1.7$ python example.py --job-name=\"ps\" --task_index=0 \r\n192.168.1.2$ python example.py --job-name=\"worker\" --task_index=0 \r\n192.168.1.8$ python example.py --job-name=\"worker\" --task_index=1 \r\n192.168.1.9$ python example.py --job-name=\"worker\" --task_index=2 \r\n\r\nreference from ischlag.github.io\r\n'''\r\nfrom __future__ import print_function\r\nimport os\r\n#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nfrom os import path\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\nimport time\r\nimport sys\r\nimport gzip\r\n\r\n#cluster specification\r\nparameter_servers = [\"192.168.1.7:2223\"]\r\nworkers = [\"192.168.1.2:2223\",\r\n           \"192.168.1.8:2223\",\r\n           \"192.168.1.9:2223\"]\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n\r\n# input flags\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_string(\r\n    \"log_dir\", \"lastest_train_log/sound_classifier/\", \"\"\"Log directory to view tensorboard\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"batch_size\", 1024, \"\"\"batch size to train [default:1024]\"\"\")\r\ntf.app.flags.DEFINE_integer(\"epoch\", 10, \"\"\"epoch size [default:10]\"\"\")\r\ntf.app.flags.DEFINE_integer(\r\n    \"test_timing_point\", 512, \"\"\"set a testing interval [default:512]\"\"\")\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n# start a server for a specific task\r\nif FLAGS.job_name == \"ps\":\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\nelse:\r\n    server = tf.train.Server(cluster, \r\n                        job_name=FLAGS.job_name,\r\n                        task_index=FLAGS.task_index)\r\n#input config\r\nnum_input = 13\r\nnum_classes = 3\r\nh_layer_units = 512\r\nnumber_of_h_layer = 6\r\ntest_timing_point = 512\r\nlearning_rate = 1E-4\r\n\r\nif FLAGS.job_name == \"ps\":\r\n    print(\"calling join to serve\")\r\n    server.join()\r\nelif FLAGS.job_name == \"worker\":\r\n\r\n#-----function definition-------\r\n    def extract_data_set(filename, batch_size, shuffle_batch=False,name=None):\r\n        filename_queue = tf.train.string_input_producer([filename])\r\n        reader = tf.TextLineReader()\r\n        key, value = reader.read(filename_queue)\r\n        record_default = [[1], [1.0], [1.0], [1.0], [1.0], [1.0], [\r\n            1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]]\r\n        labels, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13 = tf.decode_csv(\r\n            value, record_defaults=record_default)\r\n        mfcc_features = tf.stack([mfcc1, mfcc2, mfcc3, mfcc4, mfcc5,\r\n                                mfcc6, mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13])\r\n        labels = tf.one_hot(labels,depth=3,dtype=tf.int32,name=\"labels_hot\")\r\n        labels_batch, mfcc_features_batch = tf.train.batch(\r\n            [labels, mfcc_features], batch_size=batch_size, allow_smaller_final_batch=True)\r\n        return labels_batch, mfcc_features_batch\r\n\r\n    def hidden_layer(input, size_in, size_out, name=\"hidden_layer\",reuse=False):\r\n        with tf.name_scope(name):\r\n            w = tf.Variable(tf.truncated_normal([size_in,size_out],stddev=0.1),name=\"W\")\r\n            b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=\"B\")\r\n            act = tf.nn.tanh(tf.add(tf.matmul(input, w), b))\r\n            tf.summary.histogram(\"weights\", w)\r\n            tf.summary.histogram(\"bias\", b)\r\n            tf.summary.histogram(\"activations\", act)\r\n            return act\r\n\r\n\r\n    def fully_connected_layer(input, size_in, size_out, name=\"fc\",reuse=False):\r\n        with tf.name_scope(name):\r\n            w = tf.Variable(tf.truncated_normal(shape=[size_in,size_out],stddev=0.1),name=\"W\")\r\n            b = tf.Variable(tf.constant(0.1,shape=[size_out]),name=\"B\")\r\n            act = tf.add(tf.matmul(input, w), b)\r\n            tf.summary.histogram(\"weights\", w)\r\n            tf.summary.histogram(\"bias\", b)\r\n            tf.summary.histogram(\"activations\", act)\r\n            return act\r\n#-------start program------------\r\n    print(\"calling worker\")\r\n    with tf.device(\"/job:ps/task:0\"):\r\n        print(\"Calculating data size...\")\r\n        with gzip.open('csv_files/mfcc_combine_train_shuffled.gz') as f:\r\n            text = f.readlines()\r\n            size1 = len(text)\r\n            print(\"train:{}\".format(size1))\r\n        with gzip.open('csv_files/mfcc_combine_test_shuffled.gz') as f:\r\n            text = f.readlines()\r\n            size2 = len(text)\r\n            print(\"train:{}\".format(size2))\r\n\r\n\r\n        del text\r\n        test_timing = (size2 / FLAGS.batch_size) / \\\r\n            (size1 / FLAGS.batch_size / FLAGS.test_timing_point)\r\n        print(\"test timing:{}\".format(test_timing))\r\n        steps = size1 / FLAGS.batch_size\r\n        print(\"steps:{}\".format(steps))\r\n    with tf.device(\"/job:ps/task:0\"):\r\n        train_labels_batch, train_mfcc_batch = extract_data_set(\r\n            \"csv_files/mfcc_combine_train_shuffled.csv\", FLAGS.batch_size,\"train\")\r\n        test_labels_batch, test_mfcc_batch = extract_data_set(\r\n            \"csv_files/mfcc_combine_test_shuffled.csv\", FLAGS.batch_size,\"test\")\r\n\r\n    # def sound_classifier_model(learning_rate, number_of_h_layer, h_layer_units, hparam):\r\n        # tf.reset_default_graph()\r\n    #saver = tf.train.Saver(sharded=True)\r\n    \r\n    # Between-graph replication model\r\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" %FLAGS.task_index,cluster=cluster)):\r\n        is_chief = (FLAGS.task_index == 0)\r\n        global_step = tf.get_variable('global_step', [], \r\n                                      initializer = tf.constant_initializer(0),dtype=tf.int32)\r\n        # global_step = tf.Variable(0,name='global_step')\r\n        #global_step=tf.get_variable('global_step', shape=[], initializer=tf.zeros_initializer(), dtype=tf.int32, trainable=False)\r\n        x = tf.placeholder(tf.float32,shape=[None,num_input],name=\"mfcc_input\")\r\n        y = tf.placeholder(tf.int32,shape=[None,num_classes],name=\"labels\")\r\n        is_training = tf.placeholder(dtype=bool, shape=(), name=\"is_training\")\r\n        q_selector = tf.cond(is_training,lambda:[train_mfcc_batch,train_labels_batch],lambda:[test_mfcc_batch,test_labels_batch])\r\n\r\n        h = []\r\n        h.append(hidden_layer(x,num_input,h_layer_units))\r\n        for i in range(number_of_h_layer):\r\n            h.append(hidden_layer(h[i],h_layer_units,h_layer_units))\r\n\r\n        output = fully_connected_layer(h[number_of_h_layer],h_layer_units,num_classes)\r\n        saver = tf.train.Saver()\r\n        with tf.name_scope(\"x-entropy\"):\r\n            xent = tf.reduce_mean(\r\n            tf.nn.softmax_cross_entropy_with_logits(\r\n                    logits=output, labels=y), name=\"x-entropy\")\r\n\r\n        with tf.name_scope(\"train\"):\r\n            grad_op = tf.train.AdamOptimizer(learning_rate)\r\n            rep_op = tf.train.SyncReplicasOptimizer(grad_op,\r\n                                                    replicas_to_aggregate=len(workers),\r\n                                                    total_num_replicas=len(workers))\r\n            train_op = rep_op.minimize(xent,global_step=global_step)\r\n            sync_replicas_hook = rep_op.make_session_run_hook(is_chief)\r\n            # grads = rep_op.compute_gradients(xent)\r\n            # apply_gradients_op = rep_op.apply_gradients(grads,global_step=global_step)\r\n            # with tf.control_dependencies([apply_gradients_op]):\r\n                # train_op=tf.identity(xent,name='train_op')\r\n            # train_step = rep_op.minimize(xent,global_step=global_step,aggregation_method=tf.AggregationMethod.ADD_N)\r\n        init_token_op = rep_op.get_init_tokens_op()\r\n        chief_queue_runner = rep_op.get_chief_queue_runner()\r\n        with tf.name_scope(\"accuracy\"):\r\n            correct_prediction = tf.equal(tf.argmax(output,1),tf.argmax(y,1))\r\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n\r\n        tf.summary.scalar(\"x-entropy\", xent)\r\n        tf.summary.scalar(\"accuracy\",accuracy)\r\n        \r\n        summary_op = tf.summary.merge_all()\r\n        global_init_op = tf.global_variables_initializer()\r\n        local_init_op = tf.local_variables_initializer()\r\n        \r\n    #------------prepare for session------------\r\n    # coord = tf.train.Coordinator()\r\n    print(\"creating Supervisor...\")\r\n    # sv = tf.train.Supervisor(is_chief=is_cheif,\r\n    #                         global_step=global_step,\r\n    #                         saver = saver,\r\n    #                         logdir=FLAGS.log_dir)\r\n    print(\"making config...\")\r\n    sess_config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=True,device_filters=[\"/job:ps\",\"/job:worker/task:%d\"%FLAGS.task_index])\r\n    begin_time = time.time()\r\n    summary_hook = tf.train.SummarySaverHook(save_secs=600,output_dir=FLAGS.log_dir,summary_op=summary_op)\r\n    checkpoint_hook = tf.train.CheckpointSaverHook(save_steps=test_timing,checkpoint_dir=FLAGS.log_dir,saver=saver)\r\n\r\n    #---------------training session--------------\r\n    print(\"waiting for session prepare....\")\r\n    with tf.train.MonitoredTrainingSession(server.target,is_chief=is_chief,hooks=[sync_replicas_hook,summary_hook,checkpoint_hook],config=sess_config) as sess:\r\n        while not sess.should_stop():\r\n            '''\r\n            # is cheif\r\n            if FLAGS.task_index == 0:\r\n                sv.start_queue_runners(sess,[cheif_queue_runner])\r\n                sess.run(init_token_op)\r\n            '''\r\n            print(\"in session\")\r\n            # while not sv.should_stop():\r\n                # coord = tf.train.Coordinator()\r\n            # sess.run(tf.global_variables_initializer())\r\n            # sess.run(tf.local_variables_initializer())\r\n            # coord = tf.train.Coordinator()\r\n            # if FLAGS.task_index == 0:\r\n            tf.train.start_queue_runners(sess=sess)\r\n             sess.run(init_token_op)\r\n            # sv.start_queue_runners(sess=sess)\r\n            with tf.device(\"/job:ps/task:0/cpu:0\"):\r\n                f = open(FLAGS.log_dir + \"accuracy_\" + str(learning_rate) +\r\n                        \"_with_\" + str(number_of_h_layer) + \"layers\", \"w+\")\r\n                print(\"loading data to queue....\")\r\n            \r\n            print(\"done loading to queue\")\r\n            print(\"start adding summary\")\r\n            with tf.device(\"/job:ps/task:0/cpu:0\"):\r\n                f = open(FLAGS.log_dir+\"accuracy_\"+str(learning_rate)+\"_with_\"+str(number_of_h_layer)+\"layers\",\"w+\")\r\n\r\n                # writer = tf.summary.FileWriter(FLAGS.log_dir,graph=tf.get_default_graph())\r\n                # writer = tf.summary.FileWriter(FLAGS.log_dir + \"lr_%.0E layers:%d\"%(learning_rate,6))\r\n                # writer.add_graph(sess.graph)\r\n                #perform training cycles\r\n                start_time = time.time()\r\n\r\n            print(\"start training\")\r\n            for i in range(steps):\r\n                print(\"read data\")\r\n                mfcc_batch,label_batch = sess.run(q_selector,feed_dict={is_training:True})\r\n                # label_batch = sess.run(tf.one_hot(y_batch,depth=3,dtype=tf.int32,name=\"labels\"))\r\n                [_,loss,glob_step,train_accuracy] = sess.run([train_op,xent,global_step,accuracy],feed_dict={x:mfcc_batch,y:label_batch})\r\n                # [train_accuracy,s] = sess.run([accuracy,summary_op],feed_dict={x:mfcc_batch,y:label_batch})\r\n                print(\"done\")\r\n                elapsed = round(time.time()-start_time,2)\r\n                sys.stdout.write(\"\\r\")\r\n                sys.stdout.write(\"learning_rate:{0},{1} layers , training:{2}/{3} , train_accuracy:{4} [elapsed_time:{5:.2f}] \".format(learning_rate,number_of_h_layer,i,steps,train_accuracy,elapsed))\r\n                sys.stdout.flush()\r\n                # writer.add_summary(summary,i)\r\n                print(\"before train\")\r\n                # sess.run(global_step,init_token_op,chief_queue_runner)\r\n                # sess.run(train_op,xent)\r\n                print(\"after train\")\r\n                if i != 0 and i % test_timing == 0:\r\n                    # saver.save(sess, os.path.join(FLAGS.log_dir, \"model.ckpt\"), i)\r\n                    test_time += 1\r\n                    for j in range(test_timing):\r\n                        mfcc_batch,y_batch = sess.run(q_selector,feed_dict={is_training:False})\r\n                        test_label = sess.run(tf.one_hot(y_batch,depth=3,dtype=tf.int32,name=\"test_label\"))\r\n                        test_accuracy = accuracy.eval(feed_dict={x:mfcc_batch,y:test_label})\r\n                        sys.stdout.write(\"\\r\")\r\n                        sys.stdout.write(\"{}layers: test_num:{} testing:{} test_accuracy:{}\".format(number_of_h_layer,test_time,j,test_accuracy))\r\n                        sys.stdout.flush()\r\n                        with tf.device(\"/job:ps/task:0/cpu:0\"):\r\n                            f.write(\"{}layers: test_num:{} testing:{} test_accuracy:{}\\n\".format(number_of_h_layer,test_time,j,test_accuracy))\r\n                            f.flush()\r\n\r\n    f.write(\"total_training for lr {},{}layers time:{} minutes\".format(learning_rate,number_of_h_layer,((time.time()-start_time)/60.0)))\r\n    f.flush()\r\n    f.close()\r\n    print(\"-----%s minutes ------\"%((time.time()-start_time)/60.0))\r\n#    sv.stop()\r\n    print(\"Done!\")\r\n`\r\n</pre>\r\n\r\nI run as the code provided comments in respective IP PC but it shows the error as previous\r\nThe Worker with task index 0 just hangs at the error.\r\n\r\nIs this the problem of my Placeholder without explicitly define the size?The [None,13] part."}