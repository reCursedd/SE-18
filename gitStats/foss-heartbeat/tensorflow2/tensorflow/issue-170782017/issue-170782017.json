{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3762", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3762/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3762/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3762/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3762", "id": 170782017, "node_id": "MDU6SXNzdWUxNzA3ODIwMTc=", "number": 3762, "title": "PS is bound in GPU:0 by default, we can't change!", "user": {"login": "DjangoPeng", "id": 16943353, "node_id": "MDQ6VXNlcjE2OTQzMzUz", "avatar_url": "https://avatars3.githubusercontent.com/u/16943353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DjangoPeng", "html_url": "https://github.com/DjangoPeng", "followers_url": "https://api.github.com/users/DjangoPeng/followers", "following_url": "https://api.github.com/users/DjangoPeng/following{/other_user}", "gists_url": "https://api.github.com/users/DjangoPeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/DjangoPeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DjangoPeng/subscriptions", "organizations_url": "https://api.github.com/users/DjangoPeng/orgs", "repos_url": "https://api.github.com/users/DjangoPeng/repos", "events_url": "https://api.github.com/users/DjangoPeng/events{/privacy}", "received_events_url": "https://api.github.com/users/DjangoPeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-08-12T01:22:48Z", "updated_at": "2016-08-26T20:42:40Z", "closed_at": "2016-08-26T20:42:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I modified the seq2seq , mnist_replica and ptb model into distributed training model. But all of them have the same problem, that is when I start the server and <code>server.join()</code> the ps. The ps was bound in GPU:0 by default, and I can't change the setting. I follow the tutorial of distributed tensorflow <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html#putting-it-all-together-example-trainer-program\" rel=\"nofollow\">here</a>.<br>\nThis is part of my translate.py :</p>\n<pre><code>def train():\n\n  # set distributed configs\n  ps_hosts = [\"9.91.9.130:2222\"]\n  worker_hosts = [\"9.91.9.130:2223\", \"9.91.9.130:2224\"]\n  #worker_hosts = [\"9.91.9.130:2223\"]\n\n  cluster = tf.train.ClusterSpec({\"ps\":ps_hosts, \"worker\":worker_hosts})\n  server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n        server.join()\n  elif FLAGS.job_name == \"worker\":\n      # Worker server \n      is_chief = (FLAGS.task_index == 0)      \n      gpu_num = FLAGS.task_index + 1\n      #with tf.Graph().as_default():\n      with tf.device(tf.train.replica_device_setter(cluster=cluster,\n          worker_device=\"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, gpu_num))):\n      #with tf.device(\"/gpu:%d\" % FLAGS.task_index):\n          \"\"\"Train a en-&gt;fr translation model using WMT data.\"\"\"\n</code></pre>\n<p>This is the GPU info:</p>\n<pre><code>Fri Aug 12 09:17:07 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 352.39     Driver Version: 352.39         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   67C    P0    63W / 149W |  11099MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\n| N/A   45C    P0    72W / 149W |  10986MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 0000:85:00.0     Off |                    0 |\n| N/A   76C    P0    66W / 149W |  11049MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 0000:86:00.0     Off |                    0 |\n| N/A   57C    P0    75W / 149W |    223MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      9198    C   python                                       10940MiB |\n|    0     29865    C   python                                          64MiB |\n|    0     29961    C   python                                          64MiB |\n|    1      9198    C   python                                          64MiB |\n|    1     29865    C   python                                          64MiB |\n|    1     29961    C   python                                       10827MiB |\n|    2      9198    C   python                                          64MiB |\n|    2     29865    C   python                                       10891MiB |\n|    2     29961    C   python                                          64MiB |\n|    3      9198    C   python                                          64MiB |\n|    3     29865    C   python                                          64MiB |\n|    3     29961    C   python                                          64MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>As you can see, the pid 9198 is the <code>ps server</code> process, cause it occupied the GPU:0, I have to bind the <code>worker server</code> in GPU:1 and GPU:2.<br>\nCould we add a function or API to set the <code>ps server device</code> by configuration or arguments?</p>", "body_text": "I modified the seq2seq , mnist_replica and ptb model into distributed training model. But all of them have the same problem, that is when I start the server and server.join() the ps. The ps was bound in GPU:0 by default, and I can't change the setting. I follow the tutorial of distributed tensorflow here.\nThis is part of my translate.py :\ndef train():\n\n  # set distributed configs\n  ps_hosts = [\"9.91.9.130:2222\"]\n  worker_hosts = [\"9.91.9.130:2223\", \"9.91.9.130:2224\"]\n  #worker_hosts = [\"9.91.9.130:2223\"]\n\n  cluster = tf.train.ClusterSpec({\"ps\":ps_hosts, \"worker\":worker_hosts})\n  server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n        server.join()\n  elif FLAGS.job_name == \"worker\":\n      # Worker server \n      is_chief = (FLAGS.task_index == 0)      \n      gpu_num = FLAGS.task_index + 1\n      #with tf.Graph().as_default():\n      with tf.device(tf.train.replica_device_setter(cluster=cluster,\n          worker_device=\"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, gpu_num))):\n      #with tf.device(\"/gpu:%d\" % FLAGS.task_index):\n          \"\"\"Train a en->fr translation model using WMT data.\"\"\"\n\nThis is the GPU info:\nFri Aug 12 09:17:07 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 352.39     Driver Version: 352.39         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   67C    P0    63W / 149W |  11099MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\n| N/A   45C    P0    72W / 149W |  10986MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 0000:85:00.0     Off |                    0 |\n| N/A   76C    P0    66W / 149W |  11049MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 0000:86:00.0     Off |                    0 |\n| N/A   57C    P0    75W / 149W |    223MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      9198    C   python                                       10940MiB |\n|    0     29865    C   python                                          64MiB |\n|    0     29961    C   python                                          64MiB |\n|    1      9198    C   python                                          64MiB |\n|    1     29865    C   python                                          64MiB |\n|    1     29961    C   python                                       10827MiB |\n|    2      9198    C   python                                          64MiB |\n|    2     29865    C   python                                       10891MiB |\n|    2     29961    C   python                                          64MiB |\n|    3      9198    C   python                                          64MiB |\n|    3     29865    C   python                                          64MiB |\n|    3     29961    C   python                                          64MiB |\n+-----------------------------------------------------------------------------+\n\nAs you can see, the pid 9198 is the ps server process, cause it occupied the GPU:0, I have to bind the worker server in GPU:1 and GPU:2.\nCould we add a function or API to set the ps server device by configuration or arguments?", "body": "I modified the seq2seq , mnist_replica and ptb model into distributed training model. But all of them have the same problem, that is when I start the server and `server.join()` the ps. The ps was bound in GPU:0 by default, and I can't change the setting. I follow the tutorial of distributed tensorflow [here](https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html#putting-it-all-together-example-trainer-program). \nThis is part of my translate.py :\n\n```\ndef train():\n\n  # set distributed configs\n  ps_hosts = [\"9.91.9.130:2222\"]\n  worker_hosts = [\"9.91.9.130:2223\", \"9.91.9.130:2224\"]\n  #worker_hosts = [\"9.91.9.130:2223\"]\n\n  cluster = tf.train.ClusterSpec({\"ps\":ps_hosts, \"worker\":worker_hosts})\n  server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n        server.join()\n  elif FLAGS.job_name == \"worker\":\n      # Worker server \n      is_chief = (FLAGS.task_index == 0)      \n      gpu_num = FLAGS.task_index + 1\n      #with tf.Graph().as_default():\n      with tf.device(tf.train.replica_device_setter(cluster=cluster,\n          worker_device=\"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, gpu_num))):\n      #with tf.device(\"/gpu:%d\" % FLAGS.task_index):\n          \"\"\"Train a en->fr translation model using WMT data.\"\"\"\n```\n\nThis is the GPU info:\n\n```\nFri Aug 12 09:17:07 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 352.39     Driver Version: 352.39         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   67C    P0    63W / 149W |  11099MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\n| N/A   45C    P0    72W / 149W |  10986MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 0000:85:00.0     Off |                    0 |\n| N/A   76C    P0    66W / 149W |  11049MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 0000:86:00.0     Off |                    0 |\n| N/A   57C    P0    75W / 149W |    223MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      9198    C   python                                       10940MiB |\n|    0     29865    C   python                                          64MiB |\n|    0     29961    C   python                                          64MiB |\n|    1      9198    C   python                                          64MiB |\n|    1     29865    C   python                                          64MiB |\n|    1     29961    C   python                                       10827MiB |\n|    2      9198    C   python                                          64MiB |\n|    2     29865    C   python                                       10891MiB |\n|    2     29961    C   python                                          64MiB |\n|    3      9198    C   python                                          64MiB |\n|    3     29865    C   python                                          64MiB |\n|    3     29961    C   python                                          64MiB |\n+-----------------------------------------------------------------------------+\n```\n\nAs you can see, the pid 9198 is the `ps server` process, cause it occupied the GPU:0, I have to bind the `worker server` in GPU:1 and GPU:2.\nCould we add a function or API to set the `ps server device` by configuration or arguments?\n"}