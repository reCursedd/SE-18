{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239602794", "html_url": "https://github.com/tensorflow/tensorflow/issues/3762#issuecomment-239602794", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3762", "id": 239602794, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTYwMjc5NA==", "user": {"login": "DjangoPeng", "id": 16943353, "node_id": "MDQ6VXNlcjE2OTQzMzUz", "avatar_url": "https://avatars3.githubusercontent.com/u/16943353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DjangoPeng", "html_url": "https://github.com/DjangoPeng", "followers_url": "https://api.github.com/users/DjangoPeng/followers", "following_url": "https://api.github.com/users/DjangoPeng/following{/other_user}", "gists_url": "https://api.github.com/users/DjangoPeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/DjangoPeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DjangoPeng/subscriptions", "organizations_url": "https://api.github.com/users/DjangoPeng/orgs", "repos_url": "https://api.github.com/users/DjangoPeng/repos", "events_url": "https://api.github.com/users/DjangoPeng/events{/privacy}", "received_events_url": "https://api.github.com/users/DjangoPeng/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-13T05:07:13Z", "updated_at": "2016-08-13T05:08:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> I set a cluster in one machine with 4 GPUs, and tried the <code>ps_tasks=1</code>. Then I run 4 scripts in the machine:</p>\n<pre><code>#PS0\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=ps --task_index=0\n\n#Worker0\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=0\n\n#Worker1\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=1\n\n#Worker2\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=2\n</code></pre>\n<p>And I changed the code:</p>\n<pre><code>  ps_hosts = [\"9.91.9.130:2222\"]\n  worker_hosts = [\"9.91.9.130:2223\", \"9.91.9.130:2224\", \"9.91.9.130:2225\"]\n  #worker_hosts = [\"9.91.9.130:2223\"]\n\n  cluster = tf.train.ClusterSpec({\"ps\":ps_hosts, \"worker\":worker_hosts})\n  server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n        server.join()\n  elif FLAGS.job_name == \"worker\":\n      # Worker server \n      is_chief = (FLAGS.task_index == 0)      \n      gpu_num = FLAGS.task_index\n      with tf.Graph().as_default():\n        with tf.device(tf.train.replica_device_setter(cluster=cluster,\n            worker_device=\"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, gpu_num),\n            #ps_device=\"/job:ps/task:0/cpu:0\",\n            ps_tasks=1)):\n</code></pre>\n<p>But the chief task(worker0) get errors <code>CUDA_ERROR_OUT_OF_MEMORY: failed to allocate 10.54G down to 370.68M</code>, cause the memory is allocated to the <code>ps task(ps0)</code></p>\n<pre><code>Creating 1 layers of 10 units.\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nI'm in chief task!\nfinish creating model\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 10.54G (11319703296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n...\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 370.68M (388683520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n...\nE tensorflow/stream_executor/cuda/cuda_blas.cc:361] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n...\ntensorflow.python.framework.errors.InternalError: Blas SGEMM launch failed : a.shape=(1, 20), b.shape=(20, 10), m=1, n=10, k=20\n</code></pre>\n<p>What's more, the other worker task hanging forever with the log info:</p>\n<pre><code>I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7616 get requests, put_count=3642 evicted_count=1000 eviction_rate=0.274574 and unsatisfied allocation rate=0.666229\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n</code></pre>\n<p>And even though I solve the <code>memory problem</code> by putting the worker task in other GPU, the <code>stuck problem</code> is still exist. I have no idea about what happend in the hanging process.</p>", "body_text": "@girving I set a cluster in one machine with 4 GPUs, and tried the ps_tasks=1. Then I run 4 scripts in the machine:\n#PS0\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=ps --task_index=0\n\n#Worker0\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=0\n\n#Worker1\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=1\n\n#Worker2\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=2\n\nAnd I changed the code:\n  ps_hosts = [\"9.91.9.130:2222\"]\n  worker_hosts = [\"9.91.9.130:2223\", \"9.91.9.130:2224\", \"9.91.9.130:2225\"]\n  #worker_hosts = [\"9.91.9.130:2223\"]\n\n  cluster = tf.train.ClusterSpec({\"ps\":ps_hosts, \"worker\":worker_hosts})\n  server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n        server.join()\n  elif FLAGS.job_name == \"worker\":\n      # Worker server \n      is_chief = (FLAGS.task_index == 0)      \n      gpu_num = FLAGS.task_index\n      with tf.Graph().as_default():\n        with tf.device(tf.train.replica_device_setter(cluster=cluster,\n            worker_device=\"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, gpu_num),\n            #ps_device=\"/job:ps/task:0/cpu:0\",\n            ps_tasks=1)):\n\nBut the chief task(worker0) get errors CUDA_ERROR_OUT_OF_MEMORY: failed to allocate 10.54G down to 370.68M, cause the memory is allocated to the ps task(ps0)\nCreating 1 layers of 10 units.\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nI'm in chief task!\nfinish creating model\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 10.54G (11319703296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n...\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 370.68M (388683520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n...\nE tensorflow/stream_executor/cuda/cuda_blas.cc:361] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n...\ntensorflow.python.framework.errors.InternalError: Blas SGEMM launch failed : a.shape=(1, 20), b.shape=(20, 10), m=1, n=10, k=20\n\nWhat's more, the other worker task hanging forever with the log info:\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7616 get requests, put_count=3642 evicted_count=1000 eviction_rate=0.274574 and unsatisfied allocation rate=0.666229\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n\nAnd even though I solve the memory problem by putting the worker task in other GPU, the stuck problem is still exist. I have no idea about what happend in the hanging process.", "body": "@girving I set a cluster in one machine with 4 GPUs, and tried the `ps_tasks=1`. Then I run 4 scripts in the machine:\n\n```\n#PS0\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=ps --task_index=0\n\n#Worker0\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=0\n\n#Worker1\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=1\n\n#Worker2\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=2\n```\n\nAnd I changed the code:\n\n```\n  ps_hosts = [\"9.91.9.130:2222\"]\n  worker_hosts = [\"9.91.9.130:2223\", \"9.91.9.130:2224\", \"9.91.9.130:2225\"]\n  #worker_hosts = [\"9.91.9.130:2223\"]\n\n  cluster = tf.train.ClusterSpec({\"ps\":ps_hosts, \"worker\":worker_hosts})\n  server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n        server.join()\n  elif FLAGS.job_name == \"worker\":\n      # Worker server \n      is_chief = (FLAGS.task_index == 0)      \n      gpu_num = FLAGS.task_index\n      with tf.Graph().as_default():\n        with tf.device(tf.train.replica_device_setter(cluster=cluster,\n            worker_device=\"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, gpu_num),\n            #ps_device=\"/job:ps/task:0/cpu:0\",\n            ps_tasks=1)):\n```\n\nBut the chief task(worker0) get errors `CUDA_ERROR_OUT_OF_MEMORY: failed to allocate 10.54G down to 370.68M`, cause the memory is allocated to the `ps task(ps0)`\n\n```\nCreating 1 layers of 10 units.\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nI'm in chief task!\nfinish creating model\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 10.54G (11319703296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n...\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 370.68M (388683520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n...\nE tensorflow/stream_executor/cuda/cuda_blas.cc:361] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n...\ntensorflow.python.framework.errors.InternalError: Blas SGEMM launch failed : a.shape=(1, 20), b.shape=(20, 10), m=1, n=10, k=20\n```\n\nWhat's more, the other worker task hanging forever with the log info:\n\n```\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7616 get requests, put_count=3642 evicted_count=1000 eviction_rate=0.274574 and unsatisfied allocation rate=0.666229\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n```\n\nAnd even though I solve the `memory problem` by putting the worker task in other GPU, the `stuck problem` is still exist. I have no idea about what happend in the hanging process.\n"}