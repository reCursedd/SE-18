{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239722001", "html_url": "https://github.com/tensorflow/tensorflow/issues/3762#issuecomment-239722001", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3762", "id": 239722001, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTcyMjAwMQ==", "user": {"login": "DjangoPeng", "id": 16943353, "node_id": "MDQ6VXNlcjE2OTQzMzUz", "avatar_url": "https://avatars3.githubusercontent.com/u/16943353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DjangoPeng", "html_url": "https://github.com/DjangoPeng", "followers_url": "https://api.github.com/users/DjangoPeng/followers", "following_url": "https://api.github.com/users/DjangoPeng/following{/other_user}", "gists_url": "https://api.github.com/users/DjangoPeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/DjangoPeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DjangoPeng/subscriptions", "organizations_url": "https://api.github.com/users/DjangoPeng/orgs", "repos_url": "https://api.github.com/users/DjangoPeng/repos", "events_url": "https://api.github.com/users/DjangoPeng/events{/privacy}", "received_events_url": "https://api.github.com/users/DjangoPeng/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-15T03:21:03Z", "updated_at": "2016-08-15T03:22:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20988542\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bixiongxu\">@bixiongxu</a> <code>conflicting memory allocation</code> means both ps0 and worker0 apply memory in GPU0, while I have fixed it by shifting one #GPU for workers. The device placement is as below:</p>\n<pre><code>ps0: GPU0\nworker0: GPU1\nworker1: GPU2\nworker2: GPU3\n</code></pre>\n<p>But now my problem is the <code>stuck worker process</code>, .I can't start training even though the first step. The  log for each worker is the same, except the chief worker.<br>\nChief worker has no output log, but just stuck at <code>session.run(output_feed, input_feed)</code>.</p>\n<pre><code>------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nstart running session\n</code></pre>\n<p>All other worker:</p>\n<pre><code>------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nstart running session\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7615 get requests, put_count=3642 evicted_count=1000 eviction_rate=0.274574 and unsatisfied allocation rate=0.666185\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n</code></pre>", "body_text": "@bixiongxu conflicting memory allocation means both ps0 and worker0 apply memory in GPU0, while I have fixed it by shifting one #GPU for workers. The device placement is as below:\nps0: GPU0\nworker0: GPU1\nworker1: GPU2\nworker2: GPU3\n\nBut now my problem is the stuck worker process, .I can't start training even though the first step. The  log for each worker is the same, except the chief worker.\nChief worker has no output log, but just stuck at session.run(output_feed, input_feed).\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nstart running session\n\nAll other worker:\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nstart running session\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7615 get requests, put_count=3642 evicted_count=1000 eviction_rate=0.274574 and unsatisfied allocation rate=0.666185\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110", "body": "@bixiongxu `conflicting memory allocation` means both ps0 and worker0 apply memory in GPU0, while I have fixed it by shifting one #GPU for workers. The device placement is as below:\n\n```\nps0: GPU0\nworker0: GPU1\nworker1: GPU2\nworker2: GPU3\n```\n\nBut now my problem is the `stuck worker process`, .I can't start training even though the first step. The  log for each worker is the same, except the chief worker.\nChief worker has no output log, but just stuck at `session.run(output_feed, input_feed)`.\n\n```\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nstart running session\n```\n\nAll other worker:\n\n```\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nstart running session\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7615 get requests, put_count=3642 evicted_count=1000 eviction_rate=0.274574 and unsatisfied allocation rate=0.666185\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n```\n"}