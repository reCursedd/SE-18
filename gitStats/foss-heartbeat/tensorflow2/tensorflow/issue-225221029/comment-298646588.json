{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/298646588", "html_url": "https://github.com/tensorflow/tensorflow/issues/9527#issuecomment-298646588", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9527", "id": 298646588, "node_id": "MDEyOklzc3VlQ29tbWVudDI5ODY0NjU4OA==", "user": {"login": "fxsuper", "id": 16196995, "node_id": "MDQ6VXNlcjE2MTk2OTk1", "avatar_url": "https://avatars1.githubusercontent.com/u/16196995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fxsuper", "html_url": "https://github.com/fxsuper", "followers_url": "https://api.github.com/users/fxsuper/followers", "following_url": "https://api.github.com/users/fxsuper/following{/other_user}", "gists_url": "https://api.github.com/users/fxsuper/gists{/gist_id}", "starred_url": "https://api.github.com/users/fxsuper/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fxsuper/subscriptions", "organizations_url": "https://api.github.com/users/fxsuper/orgs", "repos_url": "https://api.github.com/users/fxsuper/repos", "events_url": "https://api.github.com/users/fxsuper/events{/privacy}", "received_events_url": "https://api.github.com/users/fxsuper/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-02T14:09:27Z", "updated_at": "2017-05-03T00:48:26Z", "author_association": "NONE", "body_html": "<p>Sure, I've tried to simplify the code as much as possible below (sorry I realize it's still a bit long). This simpler version shows a 3x difference in training and ~500x difference in inference. And yes I'm aware that GitHub is not meant for help and debugging, but I thought the speed gap was large enough for this to almost be a bug or at least a feature request.</p>\n<p><strong>Static version</strong></p>\n<pre><code>import numpy as np\nimport numpy.random as npr\nimport tensorflow as tf\nfrom tensorflow.python.ops.nn import l2_normalize\nnpr.seed(0)\nbatch_size = 32\nnum_steps = 1000\ninputs = tf.constant(npr.rand(num_steps * 3, batch_size, 3), dtype=tf.float32)\nvar = tf.Variable([1, 1, 1], dtype=tf.float32)\n\nid_mat = np.identity(3, dtype='float32')\ninit = []\nfor row in id_mat:\n    r = tf.tile(row[np.newaxis], [batch_size, 1])\n    init.append(r)\n\nfor d in tf.unstack(inputs):\n    a, b, c = init[-3:]                                                 \n    m = tf.transpose(tf.stack([c, tf.cross(b, c), b]), perm=[1, 2, 0])\n    p = l2_normalize(tf.squeeze(tf.matmul(m, tf.expand_dims(d, 2))), 1)\n    init.append(p)\n\nfinal = tf.stack(init[2:-1])\n    \nloss = tf.reduce_sum(tf.reduce_sum(final, axis=[0, 1]) * var)\ntrain = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\nsess = tf.Session()\ntf.global_variables_initializer().run(session=sess)\n\nfinal.eval(session=sess) # ~0.0007 secs\ntrain.run(session=sess) # ~0.12 secs\n</code></pre>\n<p><strong>Dynamic version</strong></p>\n<pre><code>import collections\nimport numpy as np\nimport numpy.random as npr\nimport tensorflow as tf\nfrom tensorflow.python.ops.nn import l2_normalize\nnpr.seed(0)\nbatch_size = 32\nnum_steps = 1000\ninputs = tf.constant(npr.rand(num_steps * 3, batch_size, 3), dtype=tf.float32)\nvar = tf.Variable([1, 1, 1], dtype=tf.float32)\n\nTriplet = collections.namedtuple('Triplet', 'a, b, c')\nid_mat = np.identity(3, dtype='float32')\ninit = Triplet(*[tf.tile(row[np.newaxis], [batch_size, 1]) for row in id_mat])\n\ndef extend(tri, inputs):\n    m = tf.transpose(tf.stack([tri.c, tf.cross(tri.b, tri.c), tri.b]), perm=[1, 2, 0])\n    p = l2_normalize(tf.squeeze(tf.matmul(m, tf.expand_dims(inputs, 2))), 1)\n    return p\n\ni = tf.constant(1)\ns = inputs.get_shape().as_list()[0]\nta = tf.TensorArray(tf.float32, size=s)\n\ndef body(i, tri, ta):\n    p = extend(tri, inputs[i - 1])\n    return [i + 1, Triplet(tri.b, tri.c, p), ta.write(i, p)]\n\n_, _, final_ta = tf.while_loop(lambda i, _1, _2: i &lt; s, body, \n                               [i, init, ta.write(0, init.c)],\n                               parallel_iterations=1, swap_memory=False) \n\nfinal = final_ta.stack()\n\nloss = tf.reduce_sum(tf.reduce_sum(final, axis=[0, 1]) * var)\ntrain = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\nsess = tf.Session()\ntf.global_variables_initializer().run(session=sess)\n\nfinal.eval(session=sess) # ~0.37 secs\ntrain.run(session=sess) # ~0.37 secs\n</code></pre>\n<p>Note that increasing the number of <code>parallel_iterations</code> only makes things worse, likely because the computation is very sequential.</p>", "body_text": "Sure, I've tried to simplify the code as much as possible below (sorry I realize it's still a bit long). This simpler version shows a 3x difference in training and ~500x difference in inference. And yes I'm aware that GitHub is not meant for help and debugging, but I thought the speed gap was large enough for this to almost be a bug or at least a feature request.\nStatic version\nimport numpy as np\nimport numpy.random as npr\nimport tensorflow as tf\nfrom tensorflow.python.ops.nn import l2_normalize\nnpr.seed(0)\nbatch_size = 32\nnum_steps = 1000\ninputs = tf.constant(npr.rand(num_steps * 3, batch_size, 3), dtype=tf.float32)\nvar = tf.Variable([1, 1, 1], dtype=tf.float32)\n\nid_mat = np.identity(3, dtype='float32')\ninit = []\nfor row in id_mat:\n    r = tf.tile(row[np.newaxis], [batch_size, 1])\n    init.append(r)\n\nfor d in tf.unstack(inputs):\n    a, b, c = init[-3:]                                                 \n    m = tf.transpose(tf.stack([c, tf.cross(b, c), b]), perm=[1, 2, 0])\n    p = l2_normalize(tf.squeeze(tf.matmul(m, tf.expand_dims(d, 2))), 1)\n    init.append(p)\n\nfinal = tf.stack(init[2:-1])\n    \nloss = tf.reduce_sum(tf.reduce_sum(final, axis=[0, 1]) * var)\ntrain = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\nsess = tf.Session()\ntf.global_variables_initializer().run(session=sess)\n\nfinal.eval(session=sess) # ~0.0007 secs\ntrain.run(session=sess) # ~0.12 secs\n\nDynamic version\nimport collections\nimport numpy as np\nimport numpy.random as npr\nimport tensorflow as tf\nfrom tensorflow.python.ops.nn import l2_normalize\nnpr.seed(0)\nbatch_size = 32\nnum_steps = 1000\ninputs = tf.constant(npr.rand(num_steps * 3, batch_size, 3), dtype=tf.float32)\nvar = tf.Variable([1, 1, 1], dtype=tf.float32)\n\nTriplet = collections.namedtuple('Triplet', 'a, b, c')\nid_mat = np.identity(3, dtype='float32')\ninit = Triplet(*[tf.tile(row[np.newaxis], [batch_size, 1]) for row in id_mat])\n\ndef extend(tri, inputs):\n    m = tf.transpose(tf.stack([tri.c, tf.cross(tri.b, tri.c), tri.b]), perm=[1, 2, 0])\n    p = l2_normalize(tf.squeeze(tf.matmul(m, tf.expand_dims(inputs, 2))), 1)\n    return p\n\ni = tf.constant(1)\ns = inputs.get_shape().as_list()[0]\nta = tf.TensorArray(tf.float32, size=s)\n\ndef body(i, tri, ta):\n    p = extend(tri, inputs[i - 1])\n    return [i + 1, Triplet(tri.b, tri.c, p), ta.write(i, p)]\n\n_, _, final_ta = tf.while_loop(lambda i, _1, _2: i < s, body, \n                               [i, init, ta.write(0, init.c)],\n                               parallel_iterations=1, swap_memory=False) \n\nfinal = final_ta.stack()\n\nloss = tf.reduce_sum(tf.reduce_sum(final, axis=[0, 1]) * var)\ntrain = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\nsess = tf.Session()\ntf.global_variables_initializer().run(session=sess)\n\nfinal.eval(session=sess) # ~0.37 secs\ntrain.run(session=sess) # ~0.37 secs\n\nNote that increasing the number of parallel_iterations only makes things worse, likely because the computation is very sequential.", "body": "Sure, I've tried to simplify the code as much as possible below (sorry I realize it's still a bit long). This simpler version shows a 3x difference in training and ~500x difference in inference. And yes I'm aware that GitHub is not meant for help and debugging, but I thought the speed gap was large enough for this to almost be a bug or at least a feature request.\r\n\r\n**Static version**\r\n```\r\nimport numpy as np\r\nimport numpy.random as npr\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.nn import l2_normalize\r\nnpr.seed(0)\r\nbatch_size = 32\r\nnum_steps = 1000\r\ninputs = tf.constant(npr.rand(num_steps * 3, batch_size, 3), dtype=tf.float32)\r\nvar = tf.Variable([1, 1, 1], dtype=tf.float32)\r\n\r\nid_mat = np.identity(3, dtype='float32')\r\ninit = []\r\nfor row in id_mat:\r\n    r = tf.tile(row[np.newaxis], [batch_size, 1])\r\n    init.append(r)\r\n\r\nfor d in tf.unstack(inputs):\r\n    a, b, c = init[-3:]                                                 \r\n    m = tf.transpose(tf.stack([c, tf.cross(b, c), b]), perm=[1, 2, 0])\r\n    p = l2_normalize(tf.squeeze(tf.matmul(m, tf.expand_dims(d, 2))), 1)\r\n    init.append(p)\r\n\r\nfinal = tf.stack(init[2:-1])\r\n    \r\nloss = tf.reduce_sum(tf.reduce_sum(final, axis=[0, 1]) * var)\r\ntrain = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\n\r\nsess = tf.Session()\r\ntf.global_variables_initializer().run(session=sess)\r\n\r\nfinal.eval(session=sess) # ~0.0007 secs\r\ntrain.run(session=sess) # ~0.12 secs\r\n```\r\n\r\n**Dynamic version**\r\n```\r\nimport collections\r\nimport numpy as np\r\nimport numpy.random as npr\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.nn import l2_normalize\r\nnpr.seed(0)\r\nbatch_size = 32\r\nnum_steps = 1000\r\ninputs = tf.constant(npr.rand(num_steps * 3, batch_size, 3), dtype=tf.float32)\r\nvar = tf.Variable([1, 1, 1], dtype=tf.float32)\r\n\r\nTriplet = collections.namedtuple('Triplet', 'a, b, c')\r\nid_mat = np.identity(3, dtype='float32')\r\ninit = Triplet(*[tf.tile(row[np.newaxis], [batch_size, 1]) for row in id_mat])\r\n\r\ndef extend(tri, inputs):\r\n    m = tf.transpose(tf.stack([tri.c, tf.cross(tri.b, tri.c), tri.b]), perm=[1, 2, 0])\r\n    p = l2_normalize(tf.squeeze(tf.matmul(m, tf.expand_dims(inputs, 2))), 1)\r\n    return p\r\n\r\ni = tf.constant(1)\r\ns = inputs.get_shape().as_list()[0]\r\nta = tf.TensorArray(tf.float32, size=s)\r\n\r\ndef body(i, tri, ta):\r\n    p = extend(tri, inputs[i - 1])\r\n    return [i + 1, Triplet(tri.b, tri.c, p), ta.write(i, p)]\r\n\r\n_, _, final_ta = tf.while_loop(lambda i, _1, _2: i < s, body, \r\n                               [i, init, ta.write(0, init.c)],\r\n                               parallel_iterations=1, swap_memory=False) \r\n\r\nfinal = final_ta.stack()\r\n\r\nloss = tf.reduce_sum(tf.reduce_sum(final, axis=[0, 1]) * var)\r\ntrain = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\n\r\nsess = tf.Session()\r\ntf.global_variables_initializer().run(session=sess)\r\n\r\nfinal.eval(session=sess) # ~0.37 secs\r\ntrain.run(session=sess) # ~0.37 secs\r\n```\r\n\r\nNote that increasing the number of `parallel_iterations` only makes things worse, likely because the computation is very sequential."}