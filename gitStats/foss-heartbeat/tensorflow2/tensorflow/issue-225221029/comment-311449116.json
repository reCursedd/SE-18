{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/311449116", "html_url": "https://github.com/tensorflow/tensorflow/issues/9527#issuecomment-311449116", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9527", "id": 311449116, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTQ0OTExNg==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-27T18:44:44Z", "updated_at": "2017-06-27T18:44:44Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">static_rnn with sequence_length= passed in will generally outperform manual\nunroll if the unroll size is bigger than *all* of your sequence lengths.\n this is not the case for BPTT.  when using BPTT, do not pass the\nsequence_length argument to static_rnn since you usually  want to calculate\nall time steps.  that argument is more useful when doing static unroll for\nseq2seq or other nontrivial architectures.\n\ndynamic_rnn is as performant as static unroll for large batch sizes and\ndepths; but not for smaller batch sizes and depths.  what are your batch\nsize and depth?\n\nfurthermore, if you have very large sequence lengths, dynamic_rnn can copy\nactivations to CPU from the GPU to allow you to train much bigger graphs\n(if you pass swap_memory=True).  static unrolling would just lead to GPU\nOOM in these cases.  again this is more useful for large batch size, depth,\nand sequence length values.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Tue, Jun 27, 2017 at 7:22 AM, Ryan Butler ***@***.***&gt; wrote:\n Unsure if this is the place to write this info, but I have been working on\n a comparison of different unfolding methods with LSTMs, and I am getting\n about 2x slower training speeds when using dynamic_rnn versus manually\n unrolling the graph statically in python, and about 1.5x slower training\n speeds when using static_rnn versus unrolling the graph statically in\n python. Given that I am using dynamic_rnn in a traditional use case, I find\n this unexpected considering that most of the internet claims that\n dynamic_rnn is the better option of the three.\n\n `with tf.variable_scope('Unrolled') as scope:\n lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=cell_size) # This\n defines the cell structure\n initial_state = lstm_cell.zero_state(batch_size=batch_size,\n dtype=tf.float32) # Initial state\n\n             self._sequence_lengths = tf.random_uniform(\n                 shape=(batch_size,), minval=1, maxval=num_steps+1, dtype=tf.int32\n             )  # , trainable=False, validate_shape=False, collections=[], name='Sequence-Lengths')\n\n             def traditional_bptt():\n                 \"\"\"Calls the lstm cell with the state and output for each time until num_steps.\"\"\"\n                 state = initial_state\n                 # Unroll the graph num_steps back into the \"past\"\n                 for i in range(num_steps):\n                     if i &gt; 0: scope.reuse_variables()  # Reuse the variables created in the 1st LSTM cell\n                     output, state = lstm_cell(  # Step the LSTM through the sequence\n                         self._hot[i, ...] if time_major else self._hot[:, i, ...],\n                         state\n                     )\n                 return output\n\n             def dynamic_bptt():\n                 \"\"\"Uses dynamic_rnn to unroll the graph.\"\"\"\n                 outputs, states = tf.nn.dynamic_rnn(\n                     lstm_cell, self._hot,\n                     sequence_length=self._sequence_lengths,\n                     initial_state=initial_state,\n                     time_major=time_major,\n                     scope=scope\n                 )\n                 return outputs[-1, ...] if time_major else outputs[:, -1, ...]\n\n             def static_bptt():\n                 \"\"\"Uses static_rnn to unroll the graph\"\"\"\n                 inputs = tf.unstack(self._hot, axis=0 if time_major else 1)\n\n                 outputs, states = tf.nn.static_rnn(\n                     lstm_cell, inputs,\n                     sequence_length=self._sequence_lengths,\n                     initial_state=initial_state,\n                     scope=scope\n                 )\n                 return outputs[-1]\n\n             # Emulate a switch statement\n             final_output = {\n                 'traditional': traditional_bptt,\n                 'dynamic': dynamic_bptt,\n                 'static': static_bptt\n             }.get(bptt_method)()\n\n `\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"225221029\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9527\" href=\"https://github.com/tensorflow/tensorflow/issues/9527#issuecomment-311373632\">#9527 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim5MDn21ejaFTjEgzd_UUs5jQkZFUks5sIRAygaJpZM4NMF3P\">https://github.com/notifications/unsubscribe-auth/ABtim5MDn21ejaFTjEgzd_UUs5jQkZFUks5sIRAygaJpZM4NMF3P</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "static_rnn with sequence_length= passed in will generally outperform manual\nunroll if the unroll size is bigger than *all* of your sequence lengths.\n this is not the case for BPTT.  when using BPTT, do not pass the\nsequence_length argument to static_rnn since you usually  want to calculate\nall time steps.  that argument is more useful when doing static unroll for\nseq2seq or other nontrivial architectures.\n\ndynamic_rnn is as performant as static unroll for large batch sizes and\ndepths; but not for smaller batch sizes and depths.  what are your batch\nsize and depth?\n\nfurthermore, if you have very large sequence lengths, dynamic_rnn can copy\nactivations to CPU from the GPU to allow you to train much bigger graphs\n(if you pass swap_memory=True).  static unrolling would just lead to GPU\nOOM in these cases.  again this is more useful for large batch size, depth,\nand sequence length values.\n\u2026\nOn Tue, Jun 27, 2017 at 7:22 AM, Ryan Butler ***@***.***> wrote:\n Unsure if this is the place to write this info, but I have been working on\n a comparison of different unfolding methods with LSTMs, and I am getting\n about 2x slower training speeds when using dynamic_rnn versus manually\n unrolling the graph statically in python, and about 1.5x slower training\n speeds when using static_rnn versus unrolling the graph statically in\n python. Given that I am using dynamic_rnn in a traditional use case, I find\n this unexpected considering that most of the internet claims that\n dynamic_rnn is the better option of the three.\n\n `with tf.variable_scope('Unrolled') as scope:\n lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=cell_size) # This\n defines the cell structure\n initial_state = lstm_cell.zero_state(batch_size=batch_size,\n dtype=tf.float32) # Initial state\n\n             self._sequence_lengths = tf.random_uniform(\n                 shape=(batch_size,), minval=1, maxval=num_steps+1, dtype=tf.int32\n             )  # , trainable=False, validate_shape=False, collections=[], name='Sequence-Lengths')\n\n             def traditional_bptt():\n                 \"\"\"Calls the lstm cell with the state and output for each time until num_steps.\"\"\"\n                 state = initial_state\n                 # Unroll the graph num_steps back into the \"past\"\n                 for i in range(num_steps):\n                     if i > 0: scope.reuse_variables()  # Reuse the variables created in the 1st LSTM cell\n                     output, state = lstm_cell(  # Step the LSTM through the sequence\n                         self._hot[i, ...] if time_major else self._hot[:, i, ...],\n                         state\n                     )\n                 return output\n\n             def dynamic_bptt():\n                 \"\"\"Uses dynamic_rnn to unroll the graph.\"\"\"\n                 outputs, states = tf.nn.dynamic_rnn(\n                     lstm_cell, self._hot,\n                     sequence_length=self._sequence_lengths,\n                     initial_state=initial_state,\n                     time_major=time_major,\n                     scope=scope\n                 )\n                 return outputs[-1, ...] if time_major else outputs[:, -1, ...]\n\n             def static_bptt():\n                 \"\"\"Uses static_rnn to unroll the graph\"\"\"\n                 inputs = tf.unstack(self._hot, axis=0 if time_major else 1)\n\n                 outputs, states = tf.nn.static_rnn(\n                     lstm_cell, inputs,\n                     sequence_length=self._sequence_lengths,\n                     initial_state=initial_state,\n                     scope=scope\n                 )\n                 return outputs[-1]\n\n             # Emulate a switch statement\n             final_output = {\n                 'traditional': traditional_bptt,\n                 'dynamic': dynamic_bptt,\n                 'static': static_bptt\n             }.get(bptt_method)()\n\n `\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#9527 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim5MDn21ejaFTjEgzd_UUs5jQkZFUks5sIRAygaJpZM4NMF3P>\n .", "body": "static_rnn with sequence_length= passed in will generally outperform manual\nunroll if the unroll size is bigger than *all* of your sequence lengths.\n this is not the case for BPTT.  when using BPTT, do not pass the\nsequence_length argument to static_rnn since you usually  want to calculate\nall time steps.  that argument is more useful when doing static unroll for\nseq2seq or other nontrivial architectures.\n\ndynamic_rnn is as performant as static unroll for large batch sizes and\ndepths; but not for smaller batch sizes and depths.  what are your batch\nsize and depth?\n\nfurthermore, if you have very large sequence lengths, dynamic_rnn can copy\nactivations to CPU from the GPU to allow you to train much bigger graphs\n(if you pass swap_memory=True).  static unrolling would just lead to GPU\nOOM in these cases.  again this is more useful for large batch size, depth,\nand sequence length values.\n\n\n\nOn Tue, Jun 27, 2017 at 7:22 AM, Ryan Butler <notifications@github.com>\nwrote:\n\n> Unsure if this is the place to write this info, but I have been working on\n> a comparison of different unfolding methods with LSTMs, and I am getting\n> about 2x slower training speeds when using dynamic_rnn versus manually\n> unrolling the graph statically in python, and about 1.5x slower training\n> speeds when using static_rnn versus unrolling the graph statically in\n> python. Given that I am using dynamic_rnn in a traditional use case, I find\n> this unexpected considering that most of the internet claims that\n> dynamic_rnn is the better option of the three.\n>\n> `with tf.variable_scope('Unrolled') as scope:\n> lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=cell_size) # This\n> defines the cell structure\n> initial_state = lstm_cell.zero_state(batch_size=batch_size,\n> dtype=tf.float32) # Initial state\n>\n>             self._sequence_lengths = tf.random_uniform(\n>                 shape=(batch_size,), minval=1, maxval=num_steps+1, dtype=tf.int32\n>             )  # , trainable=False, validate_shape=False, collections=[], name='Sequence-Lengths')\n>\n>             def traditional_bptt():\n>                 \"\"\"Calls the lstm cell with the state and output for each time until num_steps.\"\"\"\n>                 state = initial_state\n>                 # Unroll the graph num_steps back into the \"past\"\n>                 for i in range(num_steps):\n>                     if i > 0: scope.reuse_variables()  # Reuse the variables created in the 1st LSTM cell\n>                     output, state = lstm_cell(  # Step the LSTM through the sequence\n>                         self._hot[i, ...] if time_major else self._hot[:, i, ...],\n>                         state\n>                     )\n>                 return output\n>\n>             def dynamic_bptt():\n>                 \"\"\"Uses dynamic_rnn to unroll the graph.\"\"\"\n>                 outputs, states = tf.nn.dynamic_rnn(\n>                     lstm_cell, self._hot,\n>                     sequence_length=self._sequence_lengths,\n>                     initial_state=initial_state,\n>                     time_major=time_major,\n>                     scope=scope\n>                 )\n>                 return outputs[-1, ...] if time_major else outputs[:, -1, ...]\n>\n>             def static_bptt():\n>                 \"\"\"Uses static_rnn to unroll the graph\"\"\"\n>                 inputs = tf.unstack(self._hot, axis=0 if time_major else 1)\n>\n>                 outputs, states = tf.nn.static_rnn(\n>                     lstm_cell, inputs,\n>                     sequence_length=self._sequence_lengths,\n>                     initial_state=initial_state,\n>                     scope=scope\n>                 )\n>                 return outputs[-1]\n>\n>             # Emulate a switch statement\n>             final_output = {\n>                 'traditional': traditional_bptt,\n>                 'dynamic': dynamic_bptt,\n>                 'static': static_bptt\n>             }.get(bptt_method)()\n>\n> `\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9527#issuecomment-311373632>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5MDn21ejaFTjEgzd_UUs5jQkZFUks5sIRAygaJpZM4NMF3P>\n> .\n>\n"}