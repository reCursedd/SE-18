{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/311783825", "html_url": "https://github.com/tensorflow/tensorflow/issues/9527#issuecomment-311783825", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9527", "id": 311783825, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTc4MzgyNQ==", "user": {"login": "TheButlah", "id": 6969415, "node_id": "MDQ6VXNlcjY5Njk0MTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/6969415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TheButlah", "html_url": "https://github.com/TheButlah", "followers_url": "https://api.github.com/users/TheButlah/followers", "following_url": "https://api.github.com/users/TheButlah/following{/other_user}", "gists_url": "https://api.github.com/users/TheButlah/gists{/gist_id}", "starred_url": "https://api.github.com/users/TheButlah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TheButlah/subscriptions", "organizations_url": "https://api.github.com/users/TheButlah/orgs", "repos_url": "https://api.github.com/users/TheButlah/repos", "events_url": "https://api.github.com/users/TheButlah/events{/privacy}", "received_events_url": "https://api.github.com/users/TheButlah/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-28T20:43:40Z", "updated_at": "2017-06-28T20:49:02Z", "author_association": "NONE", "body_html": "<p>I need to accept variable length sequences, so I am essentially comparing dyamic_rnn vs static_rnn vs using a python loop and then a per-timestep mask on the loss function. I have tested using batch sizes of 200 on both 10 and 100 long sequences of one-hot embedded vectors (10 'words' in dictionary). What do you define as larger batch sizes and timesteps? I get the logic behind the GPU swap thing, but I don't think that really warrants this 2x-2.5x slowdown even when swapping is set to be off. I also just wrote a custom, simpler version of dynamic_rnn that doesn't use TensorArrays or tf.where and it performs ~25% faster than dynamic_rnn; still not as fast as manual unrolls in python however. Also, according to my tests, static_rnn with sequence length passed in does not outperform manual unrolling because the tf.cond calls are tremendously expensive. With a batch size of 200 and 100 timesteps, supplying static_rnn with a sequence length vector filled with the value 10, dynamic_rnn takes ~90 seconds, static_rnn takes ~70 seconds, and manual unrolling takes ~30 seconds, even though manual unrolling strictly speaking is wasting 90/100 = 90% of its timesteps.</p>", "body_text": "I need to accept variable length sequences, so I am essentially comparing dyamic_rnn vs static_rnn vs using a python loop and then a per-timestep mask on the loss function. I have tested using batch sizes of 200 on both 10 and 100 long sequences of one-hot embedded vectors (10 'words' in dictionary). What do you define as larger batch sizes and timesteps? I get the logic behind the GPU swap thing, but I don't think that really warrants this 2x-2.5x slowdown even when swapping is set to be off. I also just wrote a custom, simpler version of dynamic_rnn that doesn't use TensorArrays or tf.where and it performs ~25% faster than dynamic_rnn; still not as fast as manual unrolls in python however. Also, according to my tests, static_rnn with sequence length passed in does not outperform manual unrolling because the tf.cond calls are tremendously expensive. With a batch size of 200 and 100 timesteps, supplying static_rnn with a sequence length vector filled with the value 10, dynamic_rnn takes ~90 seconds, static_rnn takes ~70 seconds, and manual unrolling takes ~30 seconds, even though manual unrolling strictly speaking is wasting 90/100 = 90% of its timesteps.", "body": "I need to accept variable length sequences, so I am essentially comparing dyamic_rnn vs static_rnn vs using a python loop and then a per-timestep mask on the loss function. I have tested using batch sizes of 200 on both 10 and 100 long sequences of one-hot embedded vectors (10 'words' in dictionary). What do you define as larger batch sizes and timesteps? I get the logic behind the GPU swap thing, but I don't think that really warrants this 2x-2.5x slowdown even when swapping is set to be off. I also just wrote a custom, simpler version of dynamic_rnn that doesn't use TensorArrays or tf.where and it performs ~25% faster than dynamic_rnn; still not as fast as manual unrolls in python however. Also, according to my tests, static_rnn with sequence length passed in does not outperform manual unrolling because the tf.cond calls are tremendously expensive. With a batch size of 200 and 100 timesteps, supplying static_rnn with a sequence length vector filled with the value 10, dynamic_rnn takes ~90 seconds, static_rnn takes ~70 seconds, and manual unrolling takes ~30 seconds, even though manual unrolling strictly speaking is wasting 90/100 = 90% of its timesteps."}