{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/311373632", "html_url": "https://github.com/tensorflow/tensorflow/issues/9527#issuecomment-311373632", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9527", "id": 311373632, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTM3MzYzMg==", "user": {"login": "TheButlah", "id": 6969415, "node_id": "MDQ6VXNlcjY5Njk0MTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/6969415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TheButlah", "html_url": "https://github.com/TheButlah", "followers_url": "https://api.github.com/users/TheButlah/followers", "following_url": "https://api.github.com/users/TheButlah/following{/other_user}", "gists_url": "https://api.github.com/users/TheButlah/gists{/gist_id}", "starred_url": "https://api.github.com/users/TheButlah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TheButlah/subscriptions", "organizations_url": "https://api.github.com/users/TheButlah/orgs", "repos_url": "https://api.github.com/users/TheButlah/repos", "events_url": "https://api.github.com/users/TheButlah/events{/privacy}", "received_events_url": "https://api.github.com/users/TheButlah/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-27T14:21:45Z", "updated_at": "2017-06-27T14:37:57Z", "author_association": "NONE", "body_html": "<p>Unsure if this is the place to write this info, but I have been working on a comparison of different unfolding methods with LSTMs, and I am getting about 2x slower training speeds when using dynamic_rnn versus manually unrolling the graph statically in python, and about 1.5x slower training speeds when using static_rnn versus unrolling the graph statically in python. Given that I am using dynamic_rnn in a traditional use case, I find this surprising considering that most of the internet claims that dynamic_rnn is the better option of the three. I am basing these results on both GPU and CPU testing, with a training set size of 200 and 2,000 epochs</p>\n<p>Having trouble pasting the code, so here is the file. Relevant code is from lines 83 to 131.<br>\n<a href=\"https://github.com/TheButlah/LSTM/blob/a9f54879640c75c32903e2331a5478044879ff34/model.py\">https://github.com/TheButlah/LSTM/blob/a9f54879640c75c32903e2331a5478044879ff34/model.py</a></p>", "body_text": "Unsure if this is the place to write this info, but I have been working on a comparison of different unfolding methods with LSTMs, and I am getting about 2x slower training speeds when using dynamic_rnn versus manually unrolling the graph statically in python, and about 1.5x slower training speeds when using static_rnn versus unrolling the graph statically in python. Given that I am using dynamic_rnn in a traditional use case, I find this surprising considering that most of the internet claims that dynamic_rnn is the better option of the three. I am basing these results on both GPU and CPU testing, with a training set size of 200 and 2,000 epochs\nHaving trouble pasting the code, so here is the file. Relevant code is from lines 83 to 131.\nhttps://github.com/TheButlah/LSTM/blob/a9f54879640c75c32903e2331a5478044879ff34/model.py", "body": "Unsure if this is the place to write this info, but I have been working on a comparison of different unfolding methods with LSTMs, and I am getting about 2x slower training speeds when using dynamic_rnn versus manually unrolling the graph statically in python, and about 1.5x slower training speeds when using static_rnn versus unrolling the graph statically in python. Given that I am using dynamic_rnn in a traditional use case, I find this surprising considering that most of the internet claims that dynamic_rnn is the better option of the three. I am basing these results on both GPU and CPU testing, with a training set size of 200 and 2,000 epochs\r\n\r\nHaving trouble pasting the code, so here is the file. Relevant code is from lines 83 to 131.\r\nhttps://github.com/TheButlah/LSTM/blob/a9f54879640c75c32903e2331a5478044879ff34/model.py"}