{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12630", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12630/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12630/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12630/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12630", "id": 253147041, "node_id": "MDU6SXNzdWUyNTMxNDcwNDE=", "number": 12630, "title": "It seems that tf.gfile.Copy can not support larget hdfs file", "user": {"login": "ericyue", "id": 918889, "node_id": "MDQ6VXNlcjkxODg4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/918889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericyue", "html_url": "https://github.com/ericyue", "followers_url": "https://api.github.com/users/ericyue/followers", "following_url": "https://api.github.com/users/ericyue/following{/other_user}", "gists_url": "https://api.github.com/users/ericyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericyue/subscriptions", "organizations_url": "https://api.github.com/users/ericyue/orgs", "repos_url": "https://api.github.com/users/ericyue/repos", "events_url": "https://api.github.com/users/ericyue/events{/privacy}", "received_events_url": "https://api.github.com/users/ericyue/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-08-27T07:33:29Z", "updated_at": "2017-12-01T07:07:23Z", "closed_at": "2017-12-01T07:04:40Z", "author_association": "NONE", "body_html": "<p>It seems that when using <code>tf.gfile.Copy(\"hdfs://default/some_hdfs_file\",\"./test\")</code>, if <code>some_hdfs_file</code> are large file such as 11GB in my case, it will report Exception.  Change to a 3GB file works fine.</p>\n<p>I'm using the latest version of tensorflow.</p>\n<pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 384, in copy\n    compat.as_bytes(oldpath), compat.as_bytes(newpath), overwrite, status)\n  File \"/home/x/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://default/tmp/x/x/dmcluster_predict_data_v3/20170817/023645/part-00000\n</code></pre>", "body_text": "It seems that when using tf.gfile.Copy(\"hdfs://default/some_hdfs_file\",\"./test\"), if some_hdfs_file are large file such as 11GB in my case, it will report Exception.  Change to a 3GB file works fine.\nI'm using the latest version of tensorflow.\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 384, in copy\n    compat.as_bytes(oldpath), compat.as_bytes(newpath), overwrite, status)\n  File \"/home/x/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://default/tmp/x/x/dmcluster_predict_data_v3/20170817/023645/part-00000", "body": "It seems that when using ``tf.gfile.Copy(\"hdfs://default/some_hdfs_file\",\"./test\")``, if `some_hdfs_file` are large file such as 11GB in my case, it will report Exception.  Change to a 3GB file works fine.\r\n\r\nI'm using the latest version of tensorflow.\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 384, in copy\r\n    compat.as_bytes(oldpath), compat.as_bytes(newpath), overwrite, status)\r\n  File \"/home/x/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/x/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://default/tmp/x/x/dmcluster_predict_data_v3/20170817/023645/part-00000\r\n```"}