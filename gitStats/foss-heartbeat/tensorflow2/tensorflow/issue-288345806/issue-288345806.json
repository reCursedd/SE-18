{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16102", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16102/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16102/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16102/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16102", "id": 288345806, "node_id": "MDU6SXNzdWUyODgzNDU4MDY=", "number": 16102, "title": "[Bug]: Unable to update the batch_normalization layer moving_mean/moving_variance of keras ", "user": {"login": "zizhaozhang", "id": 5761873, "node_id": "MDQ6VXNlcjU3NjE4NzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5761873?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zizhaozhang", "html_url": "https://github.com/zizhaozhang", "followers_url": "https://api.github.com/users/zizhaozhang/followers", "following_url": "https://api.github.com/users/zizhaozhang/following{/other_user}", "gists_url": "https://api.github.com/users/zizhaozhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/zizhaozhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zizhaozhang/subscriptions", "organizations_url": "https://api.github.com/users/zizhaozhang/orgs", "repos_url": "https://api.github.com/users/zizhaozhang/repos", "events_url": "https://api.github.com/users/zizhaozhang/events{/privacy}", "received_events_url": "https://api.github.com/users/zizhaozhang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2018-01-13T17:39:28Z", "updated_at": "2018-07-03T15:14:12Z", "closed_at": "2018-06-04T23:02:02Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: Tensorflow 1.4.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>Python version</strong>: Python 3.5</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0</li>\n<li><strong>GPU model and memory</strong>: 1080 Ti</li>\n<li><strong>Exact command to reproduce</strong>: see below</li>\n</ul>\n<p>It is following up this <a href=\"https://github.com/tensorflow/tensorflow/issues/15367#issuecomment-357088739\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/15367/hovercard\">issue</a> about training models by mixing Tensorflow with Keras using this style, exampled <a href=\"https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\" rel=\"nofollow\">here</a>. Basically, using Inception architectures in keras.applications.InceptionV3 and training using sess.run() with a placeholder K.learning_phase() to indicate the training/testing mode.</p>\n<p><em>At beginning, I found the evaluation accuracy in testing mode (loaded back from a checkpoint) is different from the results when evaluation on the fly during training. But when I evaluated using training mode, it can give me reasonable but not good results. So I knew the problem happened in batch normalization.</em></p>\n<p><strong>I tried either using tf.keras and keras independent version (2.1.2). But both not work.</strong></p>\n<p>After some investigations, I found there should an issue of Keras of using batch normalization. I track the moving_mean and moving_variance and found they never update (maintaining the initial values, zero and one). In tf. keras, I notice its batchnorm inherits tf.layers.BatchNormalization (see <a href=\"https://github.com/tensorflow/tensorflow/blob/ac8e67399d75edce6a9f94afaa2adb577035966e/tensorflow/python/keras/_impl/keras/layers/normalization.py#L26\">here</a>). However, by checking the tutorial of this tf.layers.BatchNormalization,<br>\n<em>It required to add a <strong>update_ops</strong> in optimizer. But it never mentioned in Keras (which should be<br>\nclarified).</em><br>\nI did not see Keras using update_ops anywhere. <strong>So i believe if you want to fine-tune a keras predefined model in applications, you never be able to update your moving_mean and moving_variance for your new data</strong>.</p>\n<p><strong>Note that I also tried keras independent version. It never worked. moving_mean and moving_variance are always not changed. I  tracked the value of batch_mean which is used to update moving_mean. batch_mean has values but not on moving_mean.</strong></p>\n<p>In addition, I found moving_mean and moving_variance are in tf.trainable_variables() when using keras but not in tf.keras. I am not sure if this matters.</p>\n<p>Here is an example code</p>\n<pre><code>image = keras.preprocessing.image\ndef preprocess_input(x):\n    # the same as keras.applications.inception_v3\n    with tf.name_scope('preprocess_input'):\n        x /= 255.\n        x -= 0.5\n        x *= 2.\n        return x\n\ndef get_main_network(name, input_tensor, use_weights=False):\n\n    processed_tensor = preprocess_input(input_tensor)\n\n    if name == 'inception':\n        base_model = keras.applications.InceptionV3(include_top=True,\n                                                    weights='imagenet' if use_weights else None,\n                                                    pooling='avg',\n                                                    input_tensor=processed_tensor)\n    \n    model = keras.models.Model(inputs=base_model.input, outputs=base_model.output)\n \n    return model\n\nimg_shape=[299,299]\nimg = tf.placeholder(tf.float32, shape=[None]+img_shape+[3])\n\nwith tf.name_scope('model'):\n    model = get_main_network('inception', input_tensor=img, use_weights=False)\n    output = model.output\n    logit = tf.cast(tf.argmax(output, axis=1), np.float32)\n\n# define loss\nwith tf.name_scope('cross_entropy'):\n    cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=output)  \n\n# define optimizer\nwith tf.name_scope('learning_rate'):\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    learning_rate = tf.train.exponential_decay(opt.learning_rate, global_step,\n                                        iter_epoch*opt.lr_decay_epoch, opt.lr_decay, staircase=True)\n    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) #.minimize(cross_entropy_loss, global_step=global_step)\n\n''' **This matters a lot but not working for independent keras version'**''\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    train_step = optimizer.minimize(cross_entropy_loss, global_step=global_step)\n\n''' Initialization '''\ninit_op = tf.variables_initializer([]) # a fake one, variables already initialized in keras\nsess.run(init_op)\n\nimg_path = 'elephant.jpg'\nimgs= image.load_img(img_path, target_size=img_shape)\nx = image.img_to_array(imgs)\nx = np.expand_dims(x, axis=0)\nsaver = tf.train.Saver(max_to_keep=20) # must be added in the end\nwith sess.as_default():\n    \n    feed_dict = {   \n                        img: x_batch,\n                        label: y_batch,\n                        K.learning_phase(): True\n                    }\n        _, loss = sess.run([train_step, \n                                    cross_entropy_loss, \n                                    ], feed_dict=feed_dict)\n\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=28546240\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tensorflowbutler\">@tensorflowbutler</a> responsed</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): Tensorflow 1.4.1\nBazel version (if compiling from source): N/A\nPython version: Python 3.5\nCUDA/cuDNN version: 8.0\nGPU model and memory: 1080 Ti\nExact command to reproduce: see below\n\nIt is following up this issue about training models by mixing Tensorflow with Keras using this style, exampled here. Basically, using Inception architectures in keras.applications.InceptionV3 and training using sess.run() with a placeholder K.learning_phase() to indicate the training/testing mode.\nAt beginning, I found the evaluation accuracy in testing mode (loaded back from a checkpoint) is different from the results when evaluation on the fly during training. But when I evaluated using training mode, it can give me reasonable but not good results. So I knew the problem happened in batch normalization.\nI tried either using tf.keras and keras independent version (2.1.2). But both not work.\nAfter some investigations, I found there should an issue of Keras of using batch normalization. I track the moving_mean and moving_variance and found they never update (maintaining the initial values, zero and one). In tf. keras, I notice its batchnorm inherits tf.layers.BatchNormalization (see here). However, by checking the tutorial of this tf.layers.BatchNormalization,\nIt required to add a update_ops in optimizer. But it never mentioned in Keras (which should be\nclarified).\nI did not see Keras using update_ops anywhere. So i believe if you want to fine-tune a keras predefined model in applications, you never be able to update your moving_mean and moving_variance for your new data.\nNote that I also tried keras independent version. It never worked. moving_mean and moving_variance are always not changed. I  tracked the value of batch_mean which is used to update moving_mean. batch_mean has values but not on moving_mean.\nIn addition, I found moving_mean and moving_variance are in tf.trainable_variables() when using keras but not in tf.keras. I am not sure if this matters.\nHere is an example code\nimage = keras.preprocessing.image\ndef preprocess_input(x):\n    # the same as keras.applications.inception_v3\n    with tf.name_scope('preprocess_input'):\n        x /= 255.\n        x -= 0.5\n        x *= 2.\n        return x\n\ndef get_main_network(name, input_tensor, use_weights=False):\n\n    processed_tensor = preprocess_input(input_tensor)\n\n    if name == 'inception':\n        base_model = keras.applications.InceptionV3(include_top=True,\n                                                    weights='imagenet' if use_weights else None,\n                                                    pooling='avg',\n                                                    input_tensor=processed_tensor)\n    \n    model = keras.models.Model(inputs=base_model.input, outputs=base_model.output)\n \n    return model\n\nimg_shape=[299,299]\nimg = tf.placeholder(tf.float32, shape=[None]+img_shape+[3])\n\nwith tf.name_scope('model'):\n    model = get_main_network('inception', input_tensor=img, use_weights=False)\n    output = model.output\n    logit = tf.cast(tf.argmax(output, axis=1), np.float32)\n\n# define loss\nwith tf.name_scope('cross_entropy'):\n    cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=output)  \n\n# define optimizer\nwith tf.name_scope('learning_rate'):\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    learning_rate = tf.train.exponential_decay(opt.learning_rate, global_step,\n                                        iter_epoch*opt.lr_decay_epoch, opt.lr_decay, staircase=True)\n    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) #.minimize(cross_entropy_loss, global_step=global_step)\n\n''' **This matters a lot but not working for independent keras version'**''\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    train_step = optimizer.minimize(cross_entropy_loss, global_step=global_step)\n\n''' Initialization '''\ninit_op = tf.variables_initializer([]) # a fake one, variables already initialized in keras\nsess.run(init_op)\n\nimg_path = 'elephant.jpg'\nimgs= image.load_img(img_path, target_size=img_shape)\nx = image.img_to_array(imgs)\nx = np.expand_dims(x, axis=0)\nsaver = tf.train.Saver(max_to_keep=20) # must be added in the end\nwith sess.as_default():\n    \n    feed_dict = {   \n                        img: x_batch,\n                        label: y_batch,\n                        K.learning_phase(): True\n                    }\n        _, loss = sess.run([train_step, \n                                    cross_entropy_loss, \n                                    ], feed_dict=feed_dict)\n\n\n@tensorflowbutler responsed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: Tensorflow 1.4.1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **Python version**: Python 3.5\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: 1080 Ti\r\n- **Exact command to reproduce**: see below\r\n\r\nIt is following up this [issue](https://github.com/tensorflow/tensorflow/issues/15367#issuecomment-357088739) about training models by mixing Tensorflow with Keras using this style, exampled [here](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html). Basically, using Inception architectures in keras.applications.InceptionV3 and training using sess.run() with a placeholder K.learning_phase() to indicate the training/testing mode.\r\n\r\n_At beginning, I found the evaluation accuracy in testing mode (loaded back from a checkpoint) is different from the results when evaluation on the fly during training. But when I evaluated using training mode, it can give me reasonable but not good results. So I knew the problem happened in batch normalization._\r\n\r\n**I tried either using tf.keras and keras independent version (2.1.2). But both not work.**\r\n\r\nAfter some investigations, I found there should an issue of Keras of using batch normalization. I track the moving_mean and moving_variance and found they never update (maintaining the initial values, zero and one). In tf. keras, I notice its batchnorm inherits tf.layers.BatchNormalization (see [here](https://github.com/tensorflow/tensorflow/blob/ac8e67399d75edce6a9f94afaa2adb577035966e/tensorflow/python/keras/_impl/keras/layers/normalization.py#L26)). However, by checking the tutorial of this tf.layers.BatchNormalization,\r\n  _It required to add a **update_ops** in optimizer. But it never mentioned in Keras (which should be \r\n  clarified)._ \r\nI did not see Keras using update_ops anywhere. **So i believe if you want to fine-tune a keras predefined model in applications, you never be able to update your moving_mean and moving_variance for your new data**.\r\n\r\n**Note that I also tried keras independent version. It never worked. moving_mean and moving_variance are always not changed. I  tracked the value of batch_mean which is used to update moving_mean. batch_mean has values but not on moving_mean.**\r\n\r\nIn addition, I found moving_mean and moving_variance are in tf.trainable_variables() when using keras but not in tf.keras. I am not sure if this matters.\r\n\r\nHere is an example code\r\n\r\n```\r\nimage = keras.preprocessing.image\r\ndef preprocess_input(x):\r\n    # the same as keras.applications.inception_v3\r\n    with tf.name_scope('preprocess_input'):\r\n        x /= 255.\r\n        x -= 0.5\r\n        x *= 2.\r\n        return x\r\n\r\ndef get_main_network(name, input_tensor, use_weights=False):\r\n\r\n    processed_tensor = preprocess_input(input_tensor)\r\n\r\n    if name == 'inception':\r\n        base_model = keras.applications.InceptionV3(include_top=True,\r\n                                                    weights='imagenet' if use_weights else None,\r\n                                                    pooling='avg',\r\n                                                    input_tensor=processed_tensor)\r\n    \r\n    model = keras.models.Model(inputs=base_model.input, outputs=base_model.output)\r\n \r\n    return model\r\n\r\nimg_shape=[299,299]\r\nimg = tf.placeholder(tf.float32, shape=[None]+img_shape+[3])\r\n\r\nwith tf.name_scope('model'):\r\n    model = get_main_network('inception', input_tensor=img, use_weights=False)\r\n    output = model.output\r\n    logit = tf.cast(tf.argmax(output, axis=1), np.float32)\r\n\r\n# define loss\r\nwith tf.name_scope('cross_entropy'):\r\n    cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=output)  \r\n\r\n# define optimizer\r\nwith tf.name_scope('learning_rate'):\r\n    global_step = tf.Variable(0, name='global_step', trainable=False)\r\n    learning_rate = tf.train.exponential_decay(opt.learning_rate, global_step,\r\n                                        iter_epoch*opt.lr_decay_epoch, opt.lr_decay, staircase=True)\r\n    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) #.minimize(cross_entropy_loss, global_step=global_step)\r\n\r\n''' **This matters a lot but not working for independent keras version'**''\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    train_step = optimizer.minimize(cross_entropy_loss, global_step=global_step)\r\n\r\n''' Initialization '''\r\ninit_op = tf.variables_initializer([]) # a fake one, variables already initialized in keras\r\nsess.run(init_op)\r\n\r\nimg_path = 'elephant.jpg'\r\nimgs= image.load_img(img_path, target_size=img_shape)\r\nx = image.img_to_array(imgs)\r\nx = np.expand_dims(x, axis=0)\r\nsaver = tf.train.Saver(max_to_keep=20) # must be added in the end\r\nwith sess.as_default():\r\n    \r\n    feed_dict = {   \r\n                        img: x_batch,\r\n                        label: y_batch,\r\n                        K.learning_phase(): True\r\n                    }\r\n        _, loss = sess.run([train_step, \r\n                                    cross_entropy_loss, \r\n                                    ], feed_dict=feed_dict)\r\n\r\n```\r\n@tensorflowbutler responsed"}