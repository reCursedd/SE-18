{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362895404", "html_url": "https://github.com/tensorflow/tensorflow/issues/16102#issuecomment-362895404", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16102", "id": 362895404, "node_id": "MDEyOklzc3VlQ29tbWVudDM2Mjg5NTQwNA==", "user": {"login": "ymcasky", "id": 6229000, "node_id": "MDQ6VXNlcjYyMjkwMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/6229000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymcasky", "html_url": "https://github.com/ymcasky", "followers_url": "https://api.github.com/users/ymcasky/followers", "following_url": "https://api.github.com/users/ymcasky/following{/other_user}", "gists_url": "https://api.github.com/users/ymcasky/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymcasky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymcasky/subscriptions", "organizations_url": "https://api.github.com/users/ymcasky/orgs", "repos_url": "https://api.github.com/users/ymcasky/repos", "events_url": "https://api.github.com/users/ymcasky/events{/privacy}", "received_events_url": "https://api.github.com/users/ymcasky/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-04T10:04:50Z", "updated_at": "2018-02-04T10:04:50Z", "author_association": "NONE", "body_html": "<p>Dear all,<br>\nI have other example code shows keras's BatchNormalization doesn't work as following.</p>\n<pre><code>\nimport tensorflow as tf\nimport numpy as np\n\n\n#from tensorflow.python.keras import  backend\nfrom tensorflow import keras\nK = keras.backend\n\nfrom tensorflow.python.keras.layers import  Input\n\nfrom tensorflow.python.keras.layers import MaxPooling2D\nfrom tensorflow.python.keras.layers import BatchNormalization\nfrom tensorflow.python.keras.layers import Conv2D, Dense, Flatten\nfrom tensorflow.python.keras.layers import Activation\nfrom tensorflow.python.keras.models import Model\n\nprint(tf.__version__)\n# Load Data\nimport cifar10\n\nimg_size = 32\nnum_channels = 3\nnum_classes = 10\ncifar10.maybe_download_and_extract()\n\n\nclass_names = cifar10.load_class_names()\n\n\nimages_train, cls_train, labels_train = cifar10.load_training_data()\n\nimages_test, cls_test, labels_test = cifar10.load_test_data()\n\nprint(\"Size of:\")\nprint(\"- Training-set:\\t\\t{}\".format(len(images_train)))\nprint(\"- Test-set:\\t\\t{}\".format(len(images_test)))\n\n\n\nimg_size_cropped = 24\n#%%\ndef pre_process_image(image, training):\n    # This function takes a single image as input,\n    # and a boolean whether to build the training or testing graph.\n    \n    if training:\n        # For training, add the following to the TensorFlow graph.\n\n        # Randomly crop the input image.\n        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\n\n        # Randomly flip the image horizontally.\n        image = tf.image.random_flip_left_right(image)\n        \n        # Randomly adjust hue, contrast and saturation.\n        image = tf.image.random_hue(image, max_delta=0.05)\n        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n        image = tf.image.random_brightness(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n\n        # Some of these functions may overflow and result in pixel\n        # values beyond the [0, 1] range. It is unclear from the\n        # documentation of TensorFlow 0.10.0rc0 whether this is\n        # intended. A simple solution is to limit the range.\n\n        # Limit the image pixels between [0, 1] in case of overflow.\n        image = tf.minimum(image, 1.0)\n        image = tf.maximum(image, 0.0)\n    else:\n        # For training, add the following to the TensorFlow graph.\n\n        # Crop the input image around the centre so it is the same\n        # size as images that are randomly cropped during training.\n        image = tf.image.resize_image_with_crop_or_pad(image,\n                                                       target_height=img_size_cropped,\n                                                       target_width=img_size_cropped)\n\n    return image\n\ndef pre_process(images, training):\n    # Use TensorFlow to loop over all the input images and call\n    # the function above which takes a single image as input.\n    images = tf.map_fn(lambda image: pre_process_image(image, training), images)\n\n    return images\n\n#%%\ndef _bn_relu(input_layer):\n    \"\"\"Helper to build a BN -&gt; relu block\n    \"\"\"\n    norm = BatchNormalization(axis=3)(input_layer)\n    return Activation(\"relu\")(norm)\n#%%\n\nx = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\n\ndistorted_images = pre_process(images=x, training=True)\n\ny_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n\ny_true_cls = tf.argmax(y_true, axis=1)\n\ndef create_network(training):\n    # Wrap the neural network in the scope named 'network'.\n    # Create new variables during training, and re-use during testing.\n    with tf.variable_scope('network', reuse=not training):\n\n        images = x        \n        images = pre_process(images=images, training=training)\n        \n        inputs = Input(tensor=images)\n\n        net_ks = Conv2D(kernel_size=5, strides=1, filters=64, padding='same',\n             activation='linear', name='layer_conv1')(inputs)\n\n        net_ks = _bn_relu(net_ks)\n        net_ks = MaxPooling2D(pool_size=2, strides=2)(net_ks)\n\n\n        net_ks = Conv2D(kernel_size=5, strides=1, filters=64, padding='same',\n        activation='relu', name='layer_conv2')(net_ks) \n        net_ks = MaxPooling2D(pool_size=2, strides=2)(net_ks)\n\n        \n        net_ks = Flatten()(net_ks)\n        \n        net_ks = Dense(256, activation='relu', name='layer_fc1')(net_ks)\n        net_ks = Dense(128, activation='relu', name='layer_fc2')(net_ks)\n        preds_ks = Dense(num_classes, activation='linear')(net_ks)\n        \n        preds_softmax = tf.nn.softmax(preds_ks)\n        step1 = tf.cast(y_true, tf.float32) * tf.log(preds_softmax)\n        step2 = -tf.reduce_sum(step1, reduction_indices=[1])\n        loss = tf.reduce_mean(step2)       # loss\n        \n        model = Model(inputs=inputs, outputs=preds_ks)\n\n    return preds_softmax, loss, model\n\n#%% Create Neural Network for Training Phase\nglobal_step = tf.Variable(initial_value=0,\n                          name='global_step', trainable=False)\n\ny_pred, loss, model = create_network(training=True)\n\nmodel.summary()\n\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)\n#%% Create Neural Network for Test Phase / Inference\n#y_pred, _ , _= create_network(training=False)\ny_pred_cls = tf.argmax(y_pred, axis=1)\ncorrect_prediction = tf.equal(y_pred_cls, y_true_cls)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsession = tf.Session()\nsession.run(tf.global_variables_initializer())\ntrain_batch_size = 64\n\ndef random_batch():\n    # Number of images in the training-set.\n    num_images = len(images_train)\n\n    # Create a random index.\n    idx = np.random.choice(num_images,\n                           size=train_batch_size,\n                           replace=False)\n\n    # Use the random index to select random images and labels.\n    x_batch = images_train[idx, :, :, :]\n    y_batch = labels_train[idx, :]\n\n    return x_batch, y_batch\n\ndef optimize(num_iterations):\n\n    for i in range(num_iterations):\n\n        x_batch, y_true_batch = random_batch()\n\n\n        feed_dict_train = {x: x_batch,\n                           y_true: y_true_batch, K.learning_phase(): 1}\n\n\n        i_global, _ = session.run([global_step, optimizer],\n                                  feed_dict=feed_dict_train)\n\n        # Print status to screen every 100 iterations (and last).\n        if (i_global % 100 == 0) or (i == num_iterations - 1):\n            # Calculate the accuracy on the training-batch.\n            feed_dict_train = {x: x_batch,\n                           y_true: y_true_batch, K.learning_phase(): 0}\n            batch_acc = session.run(accuracy,\n                                    feed_dict=feed_dict_train)\n\n            # Print status.\n            msg = \"Global Step: {0:&gt;6}, Training Batch Accuracy(phase 0): {1:&gt;6.1%}\"\n            print(msg.format(i_global, batch_acc))\n            \n            feed_dict_train = {x: x_batch,\n                           y_true: y_true_batch, K.learning_phase(): 1}\n            batch_acc = session.run(accuracy,\n                                    feed_dict=feed_dict_train)\n\n            # Print status.\n            msg = \"Global Step: {0:&gt;6}, Training Batch Accuracy(phase 1): {1:&gt;6.1%}\"\n            print(msg.format(i_global, batch_acc))  \n\n\n#%%\noptimize(num_iterations=10000)\n</code></pre>\n<p>Please check this bug. Thanks a lot!!</p>", "body_text": "Dear all,\nI have other example code shows keras's BatchNormalization doesn't work as following.\n\nimport tensorflow as tf\nimport numpy as np\n\n\n#from tensorflow.python.keras import  backend\nfrom tensorflow import keras\nK = keras.backend\n\nfrom tensorflow.python.keras.layers import  Input\n\nfrom tensorflow.python.keras.layers import MaxPooling2D\nfrom tensorflow.python.keras.layers import BatchNormalization\nfrom tensorflow.python.keras.layers import Conv2D, Dense, Flatten\nfrom tensorflow.python.keras.layers import Activation\nfrom tensorflow.python.keras.models import Model\n\nprint(tf.__version__)\n# Load Data\nimport cifar10\n\nimg_size = 32\nnum_channels = 3\nnum_classes = 10\ncifar10.maybe_download_and_extract()\n\n\nclass_names = cifar10.load_class_names()\n\n\nimages_train, cls_train, labels_train = cifar10.load_training_data()\n\nimages_test, cls_test, labels_test = cifar10.load_test_data()\n\nprint(\"Size of:\")\nprint(\"- Training-set:\\t\\t{}\".format(len(images_train)))\nprint(\"- Test-set:\\t\\t{}\".format(len(images_test)))\n\n\n\nimg_size_cropped = 24\n#%%\ndef pre_process_image(image, training):\n    # This function takes a single image as input,\n    # and a boolean whether to build the training or testing graph.\n    \n    if training:\n        # For training, add the following to the TensorFlow graph.\n\n        # Randomly crop the input image.\n        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\n\n        # Randomly flip the image horizontally.\n        image = tf.image.random_flip_left_right(image)\n        \n        # Randomly adjust hue, contrast and saturation.\n        image = tf.image.random_hue(image, max_delta=0.05)\n        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n        image = tf.image.random_brightness(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n\n        # Some of these functions may overflow and result in pixel\n        # values beyond the [0, 1] range. It is unclear from the\n        # documentation of TensorFlow 0.10.0rc0 whether this is\n        # intended. A simple solution is to limit the range.\n\n        # Limit the image pixels between [0, 1] in case of overflow.\n        image = tf.minimum(image, 1.0)\n        image = tf.maximum(image, 0.0)\n    else:\n        # For training, add the following to the TensorFlow graph.\n\n        # Crop the input image around the centre so it is the same\n        # size as images that are randomly cropped during training.\n        image = tf.image.resize_image_with_crop_or_pad(image,\n                                                       target_height=img_size_cropped,\n                                                       target_width=img_size_cropped)\n\n    return image\n\ndef pre_process(images, training):\n    # Use TensorFlow to loop over all the input images and call\n    # the function above which takes a single image as input.\n    images = tf.map_fn(lambda image: pre_process_image(image, training), images)\n\n    return images\n\n#%%\ndef _bn_relu(input_layer):\n    \"\"\"Helper to build a BN -> relu block\n    \"\"\"\n    norm = BatchNormalization(axis=3)(input_layer)\n    return Activation(\"relu\")(norm)\n#%%\n\nx = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\n\ndistorted_images = pre_process(images=x, training=True)\n\ny_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n\ny_true_cls = tf.argmax(y_true, axis=1)\n\ndef create_network(training):\n    # Wrap the neural network in the scope named 'network'.\n    # Create new variables during training, and re-use during testing.\n    with tf.variable_scope('network', reuse=not training):\n\n        images = x        \n        images = pre_process(images=images, training=training)\n        \n        inputs = Input(tensor=images)\n\n        net_ks = Conv2D(kernel_size=5, strides=1, filters=64, padding='same',\n             activation='linear', name='layer_conv1')(inputs)\n\n        net_ks = _bn_relu(net_ks)\n        net_ks = MaxPooling2D(pool_size=2, strides=2)(net_ks)\n\n\n        net_ks = Conv2D(kernel_size=5, strides=1, filters=64, padding='same',\n        activation='relu', name='layer_conv2')(net_ks) \n        net_ks = MaxPooling2D(pool_size=2, strides=2)(net_ks)\n\n        \n        net_ks = Flatten()(net_ks)\n        \n        net_ks = Dense(256, activation='relu', name='layer_fc1')(net_ks)\n        net_ks = Dense(128, activation='relu', name='layer_fc2')(net_ks)\n        preds_ks = Dense(num_classes, activation='linear')(net_ks)\n        \n        preds_softmax = tf.nn.softmax(preds_ks)\n        step1 = tf.cast(y_true, tf.float32) * tf.log(preds_softmax)\n        step2 = -tf.reduce_sum(step1, reduction_indices=[1])\n        loss = tf.reduce_mean(step2)       # loss\n        \n        model = Model(inputs=inputs, outputs=preds_ks)\n\n    return preds_softmax, loss, model\n\n#%% Create Neural Network for Training Phase\nglobal_step = tf.Variable(initial_value=0,\n                          name='global_step', trainable=False)\n\ny_pred, loss, model = create_network(training=True)\n\nmodel.summary()\n\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)\n#%% Create Neural Network for Test Phase / Inference\n#y_pred, _ , _= create_network(training=False)\ny_pred_cls = tf.argmax(y_pred, axis=1)\ncorrect_prediction = tf.equal(y_pred_cls, y_true_cls)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsession = tf.Session()\nsession.run(tf.global_variables_initializer())\ntrain_batch_size = 64\n\ndef random_batch():\n    # Number of images in the training-set.\n    num_images = len(images_train)\n\n    # Create a random index.\n    idx = np.random.choice(num_images,\n                           size=train_batch_size,\n                           replace=False)\n\n    # Use the random index to select random images and labels.\n    x_batch = images_train[idx, :, :, :]\n    y_batch = labels_train[idx, :]\n\n    return x_batch, y_batch\n\ndef optimize(num_iterations):\n\n    for i in range(num_iterations):\n\n        x_batch, y_true_batch = random_batch()\n\n\n        feed_dict_train = {x: x_batch,\n                           y_true: y_true_batch, K.learning_phase(): 1}\n\n\n        i_global, _ = session.run([global_step, optimizer],\n                                  feed_dict=feed_dict_train)\n\n        # Print status to screen every 100 iterations (and last).\n        if (i_global % 100 == 0) or (i == num_iterations - 1):\n            # Calculate the accuracy on the training-batch.\n            feed_dict_train = {x: x_batch,\n                           y_true: y_true_batch, K.learning_phase(): 0}\n            batch_acc = session.run(accuracy,\n                                    feed_dict=feed_dict_train)\n\n            # Print status.\n            msg = \"Global Step: {0:>6}, Training Batch Accuracy(phase 0): {1:>6.1%}\"\n            print(msg.format(i_global, batch_acc))\n            \n            feed_dict_train = {x: x_batch,\n                           y_true: y_true_batch, K.learning_phase(): 1}\n            batch_acc = session.run(accuracy,\n                                    feed_dict=feed_dict_train)\n\n            # Print status.\n            msg = \"Global Step: {0:>6}, Training Batch Accuracy(phase 1): {1:>6.1%}\"\n            print(msg.format(i_global, batch_acc))  \n\n\n#%%\noptimize(num_iterations=10000)\n\nPlease check this bug. Thanks a lot!!", "body": "Dear all, \r\nI have other example code shows keras's BatchNormalization doesn't work as following.\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\n#from tensorflow.python.keras import  backend\r\nfrom tensorflow import keras\r\nK = keras.backend\r\n\r\nfrom tensorflow.python.keras.layers import  Input\r\n\r\nfrom tensorflow.python.keras.layers import MaxPooling2D\r\nfrom tensorflow.python.keras.layers import BatchNormalization\r\nfrom tensorflow.python.keras.layers import Conv2D, Dense, Flatten\r\nfrom tensorflow.python.keras.layers import Activation\r\nfrom tensorflow.python.keras.models import Model\r\n\r\nprint(tf.__version__)\r\n# Load Data\r\nimport cifar10\r\n\r\nimg_size = 32\r\nnum_channels = 3\r\nnum_classes = 10\r\ncifar10.maybe_download_and_extract()\r\n\r\n\r\nclass_names = cifar10.load_class_names()\r\n\r\n\r\nimages_train, cls_train, labels_train = cifar10.load_training_data()\r\n\r\nimages_test, cls_test, labels_test = cifar10.load_test_data()\r\n\r\nprint(\"Size of:\")\r\nprint(\"- Training-set:\\t\\t{}\".format(len(images_train)))\r\nprint(\"- Test-set:\\t\\t{}\".format(len(images_test)))\r\n\r\n\r\n\r\nimg_size_cropped = 24\r\n#%%\r\ndef pre_process_image(image, training):\r\n    # This function takes a single image as input,\r\n    # and a boolean whether to build the training or testing graph.\r\n    \r\n    if training:\r\n        # For training, add the following to the TensorFlow graph.\r\n\r\n        # Randomly crop the input image.\r\n        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])\r\n\r\n        # Randomly flip the image horizontally.\r\n        image = tf.image.random_flip_left_right(image)\r\n        \r\n        # Randomly adjust hue, contrast and saturation.\r\n        image = tf.image.random_hue(image, max_delta=0.05)\r\n        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\r\n        image = tf.image.random_brightness(image, max_delta=0.2)\r\n        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\r\n\r\n        # Some of these functions may overflow and result in pixel\r\n        # values beyond the [0, 1] range. It is unclear from the\r\n        # documentation of TensorFlow 0.10.0rc0 whether this is\r\n        # intended. A simple solution is to limit the range.\r\n\r\n        # Limit the image pixels between [0, 1] in case of overflow.\r\n        image = tf.minimum(image, 1.0)\r\n        image = tf.maximum(image, 0.0)\r\n    else:\r\n        # For training, add the following to the TensorFlow graph.\r\n\r\n        # Crop the input image around the centre so it is the same\r\n        # size as images that are randomly cropped during training.\r\n        image = tf.image.resize_image_with_crop_or_pad(image,\r\n                                                       target_height=img_size_cropped,\r\n                                                       target_width=img_size_cropped)\r\n\r\n    return image\r\n\r\ndef pre_process(images, training):\r\n    # Use TensorFlow to loop over all the input images and call\r\n    # the function above which takes a single image as input.\r\n    images = tf.map_fn(lambda image: pre_process_image(image, training), images)\r\n\r\n    return images\r\n\r\n#%%\r\ndef _bn_relu(input_layer):\r\n    \"\"\"Helper to build a BN -> relu block\r\n    \"\"\"\r\n    norm = BatchNormalization(axis=3)(input_layer)\r\n    return Activation(\"relu\")(norm)\r\n#%%\r\n\r\nx = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\r\n\r\ndistorted_images = pre_process(images=x, training=True)\r\n\r\ny_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\r\n\r\ny_true_cls = tf.argmax(y_true, axis=1)\r\n\r\ndef create_network(training):\r\n    # Wrap the neural network in the scope named 'network'.\r\n    # Create new variables during training, and re-use during testing.\r\n    with tf.variable_scope('network', reuse=not training):\r\n\r\n        images = x        \r\n        images = pre_process(images=images, training=training)\r\n        \r\n        inputs = Input(tensor=images)\r\n\r\n        net_ks = Conv2D(kernel_size=5, strides=1, filters=64, padding='same',\r\n             activation='linear', name='layer_conv1')(inputs)\r\n\r\n        net_ks = _bn_relu(net_ks)\r\n        net_ks = MaxPooling2D(pool_size=2, strides=2)(net_ks)\r\n\r\n\r\n        net_ks = Conv2D(kernel_size=5, strides=1, filters=64, padding='same',\r\n        activation='relu', name='layer_conv2')(net_ks) \r\n        net_ks = MaxPooling2D(pool_size=2, strides=2)(net_ks)\r\n\r\n        \r\n        net_ks = Flatten()(net_ks)\r\n        \r\n        net_ks = Dense(256, activation='relu', name='layer_fc1')(net_ks)\r\n        net_ks = Dense(128, activation='relu', name='layer_fc2')(net_ks)\r\n        preds_ks = Dense(num_classes, activation='linear')(net_ks)\r\n        \r\n        preds_softmax = tf.nn.softmax(preds_ks)\r\n        step1 = tf.cast(y_true, tf.float32) * tf.log(preds_softmax)\r\n        step2 = -tf.reduce_sum(step1, reduction_indices=[1])\r\n        loss = tf.reduce_mean(step2)       # loss\r\n        \r\n        model = Model(inputs=inputs, outputs=preds_ks)\r\n\r\n    return preds_softmax, loss, model\r\n\r\n#%% Create Neural Network for Training Phase\r\nglobal_step = tf.Variable(initial_value=0,\r\n                          name='global_step', trainable=False)\r\n\r\ny_pred, loss, model = create_network(training=True)\r\n\r\nmodel.summary()\r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss, global_step=global_step)\r\n#%% Create Neural Network for Test Phase / Inference\r\n#y_pred, _ , _= create_network(training=False)\r\ny_pred_cls = tf.argmax(y_pred, axis=1)\r\ncorrect_prediction = tf.equal(y_pred_cls, y_true_cls)\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\nsession = tf.Session()\r\nsession.run(tf.global_variables_initializer())\r\ntrain_batch_size = 64\r\n\r\ndef random_batch():\r\n    # Number of images in the training-set.\r\n    num_images = len(images_train)\r\n\r\n    # Create a random index.\r\n    idx = np.random.choice(num_images,\r\n                           size=train_batch_size,\r\n                           replace=False)\r\n\r\n    # Use the random index to select random images and labels.\r\n    x_batch = images_train[idx, :, :, :]\r\n    y_batch = labels_train[idx, :]\r\n\r\n    return x_batch, y_batch\r\n\r\ndef optimize(num_iterations):\r\n\r\n    for i in range(num_iterations):\r\n\r\n        x_batch, y_true_batch = random_batch()\r\n\r\n\r\n        feed_dict_train = {x: x_batch,\r\n                           y_true: y_true_batch, K.learning_phase(): 1}\r\n\r\n\r\n        i_global, _ = session.run([global_step, optimizer],\r\n                                  feed_dict=feed_dict_train)\r\n\r\n        # Print status to screen every 100 iterations (and last).\r\n        if (i_global % 100 == 0) or (i == num_iterations - 1):\r\n            # Calculate the accuracy on the training-batch.\r\n            feed_dict_train = {x: x_batch,\r\n                           y_true: y_true_batch, K.learning_phase(): 0}\r\n            batch_acc = session.run(accuracy,\r\n                                    feed_dict=feed_dict_train)\r\n\r\n            # Print status.\r\n            msg = \"Global Step: {0:>6}, Training Batch Accuracy(phase 0): {1:>6.1%}\"\r\n            print(msg.format(i_global, batch_acc))\r\n            \r\n            feed_dict_train = {x: x_batch,\r\n                           y_true: y_true_batch, K.learning_phase(): 1}\r\n            batch_acc = session.run(accuracy,\r\n                                    feed_dict=feed_dict_train)\r\n\r\n            # Print status.\r\n            msg = \"Global Step: {0:>6}, Training Batch Accuracy(phase 1): {1:>6.1%}\"\r\n            print(msg.format(i_global, batch_acc))  \r\n\r\n\r\n#%%\r\noptimize(num_iterations=10000)\r\n```\r\nPlease check this bug. Thanks a lot!!\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}