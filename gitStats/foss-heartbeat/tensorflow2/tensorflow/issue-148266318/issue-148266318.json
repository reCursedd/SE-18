{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1934", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1934/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1934/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1934/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1934", "id": 148266318, "node_id": "MDU6SXNzdWUxNDgyNjYzMTg=", "number": 1934, "title": "Is there a way improve memory strategy with in-place & broadcasting & bsxfun?", "user": {"login": "myme5261314", "id": 1814831, "node_id": "MDQ6VXNlcjE4MTQ4MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1814831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myme5261314", "html_url": "https://github.com/myme5261314", "followers_url": "https://api.github.com/users/myme5261314/followers", "following_url": "https://api.github.com/users/myme5261314/following{/other_user}", "gists_url": "https://api.github.com/users/myme5261314/gists{/gist_id}", "starred_url": "https://api.github.com/users/myme5261314/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myme5261314/subscriptions", "organizations_url": "https://api.github.com/users/myme5261314/orgs", "repos_url": "https://api.github.com/users/myme5261314/repos", "events_url": "https://api.github.com/users/myme5261314/events{/privacy}", "received_events_url": "https://api.github.com/users/myme5261314/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2016-04-14T06:43:32Z", "updated_at": "2016-04-25T03:03:31Z", "closed_at": "2016-04-25T03:03:31Z", "author_association": "NONE", "body_html": "<p>Suppose I have 4GB gpu memory, see the following code.</p>\n<div class=\"highlight highlight-source-python\"><pre>A <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">5000</span>, <span class=\"pl-c1\">5000</span>, <span class=\"pl-c1\">25</span>]))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 5000*5000*25*4byte=2.5GB</span>\nB <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">5000</span>, <span class=\"pl-c1\">25</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 5000*25*4byte=500KB</span>\n<span class=\"pl-v\">sub_op</span> <span class=\"pl-k\">=</span> A.assign(A <span class=\"pl-k\">-</span> B)</pre></div>\n<p>So, if you run sub_op, apparently, it will lead to OOM error, since A-B will need 5GB peak memory because of the broadcasting repetition.</p>\n<p>Question 1: Is there a way to use functions like <code>bsxfun</code> in matlab which will not extend two object into the same big size and then calculate? i.e.</p>\n<div class=\"highlight highlight-source-python\"><pre>sub_op <span class=\"pl-k\">=</span> A.assign(tf.bsxfun(<span class=\"pl-k\">@</span>minus, A, B))</pre></div>\n<p>Question 2: So, there's another problem about in-place assignment. Since sub_op related to in-place assignment, if the tensorflow haven't implemented it reasonable, it will lead to OOM error two, since there's no 5GB gpu memory to handle two 2.5GB objects.</p>\n<p>Any ideas?</p>", "body_text": "Suppose I have 4GB gpu memory, see the following code.\nA = tf.Variable(tf.zeros([5000, 5000, 25]))  # 5000*5000*25*4byte=2.5GB\nB = tf.Variable(tf.zeros([5000, 25])  # 5000*25*4byte=500KB\nsub_op = A.assign(A - B)\nSo, if you run sub_op, apparently, it will lead to OOM error, since A-B will need 5GB peak memory because of the broadcasting repetition.\nQuestion 1: Is there a way to use functions like bsxfun in matlab which will not extend two object into the same big size and then calculate? i.e.\nsub_op = A.assign(tf.bsxfun(@minus, A, B))\nQuestion 2: So, there's another problem about in-place assignment. Since sub_op related to in-place assignment, if the tensorflow haven't implemented it reasonable, it will lead to OOM error two, since there's no 5GB gpu memory to handle two 2.5GB objects.\nAny ideas?", "body": "Suppose I have 4GB gpu memory, see the following code.\n\n``` python\nA = tf.Variable(tf.zeros([5000, 5000, 25]))  # 5000*5000*25*4byte=2.5GB\nB = tf.Variable(tf.zeros([5000, 25])  # 5000*25*4byte=500KB\nsub_op = A.assign(A - B)\n```\n\nSo, if you run sub_op, apparently, it will lead to OOM error, since A-B will need 5GB peak memory because of the broadcasting repetition.\n\nQuestion 1: Is there a way to use functions like `bsxfun` in matlab which will not extend two object into the same big size and then calculate? i.e.\n\n``` python\nsub_op = A.assign(tf.bsxfun(@minus, A, B))\n```\n\nQuestion 2: So, there's another problem about in-place assignment. Since sub_op related to in-place assignment, if the tensorflow haven't implemented it reasonable, it will lead to OOM error two, since there's no 5GB gpu memory to handle two 2.5GB objects.\n\nAny ideas?\n"}