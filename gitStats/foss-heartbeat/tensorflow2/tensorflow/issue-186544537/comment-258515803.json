{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/258515803", "html_url": "https://github.com/tensorflow/tensorflow/issues/5326#issuecomment-258515803", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5326", "id": 258515803, "node_id": "MDEyOklzc3VlQ29tbWVudDI1ODUxNTgwMw==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-04T18:44:06Z", "updated_at": "2016-11-04T18:44:06Z", "author_association": "MEMBER", "body_html": "<p>Currently optimizers in TF do sparse updates, as you pointed out, only if the variable access was made with a gather (as the gradient of gather is defined to be indexedslices which triggers sparse updates).</p>\n<p>That said what this paper implemented is not sparse updates as part of the optimization process but sparse updates as part of unrolling an RNN (wherein each state has a reference to a very large memory tensor and does scatter_add on it). Currently tensorflow has no support for efficient sparse mutations on arbitrary tensors.</p>\n<p>I see two ways of supporting this: adding a new type of mutable resource like tensorarray which allows for these operations, or adding a graph-rewriting pass which checks which sparse tensor mutations are valid and can be done with overwriting instead of copying.</p>", "body_text": "Currently optimizers in TF do sparse updates, as you pointed out, only if the variable access was made with a gather (as the gradient of gather is defined to be indexedslices which triggers sparse updates).\nThat said what this paper implemented is not sparse updates as part of the optimization process but sparse updates as part of unrolling an RNN (wherein each state has a reference to a very large memory tensor and does scatter_add on it). Currently tensorflow has no support for efficient sparse mutations on arbitrary tensors.\nI see two ways of supporting this: adding a new type of mutable resource like tensorarray which allows for these operations, or adding a graph-rewriting pass which checks which sparse tensor mutations are valid and can be done with overwriting instead of copying.", "body": "Currently optimizers in TF do sparse updates, as you pointed out, only if the variable access was made with a gather (as the gradient of gather is defined to be indexedslices which triggers sparse updates).\n\nThat said what this paper implemented is not sparse updates as part of the optimization process but sparse updates as part of unrolling an RNN (wherein each state has a reference to a very large memory tensor and does scatter_add on it). Currently tensorflow has no support for efficient sparse mutations on arbitrary tensors.\n\nI see two ways of supporting this: adding a new type of mutable resource like tensorarray which allows for these operations, or adding a graph-rewriting pass which checks which sparse tensor mutations are valid and can be done with overwriting instead of copying.\n"}