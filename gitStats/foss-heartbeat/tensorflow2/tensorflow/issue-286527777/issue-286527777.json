{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15919", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15919/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15919/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15919/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15919", "id": 286527777, "node_id": "MDU6SXNzdWUyODY1Mjc3Nzc=", "number": 15919, "title": "A bug of tf.layers.batch_normalization when training is not a constant", "user": {"login": "x10000year", "id": 22427780, "node_id": "MDQ6VXNlcjIyNDI3Nzgw", "avatar_url": "https://avatars0.githubusercontent.com/u/22427780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/x10000year", "html_url": "https://github.com/x10000year", "followers_url": "https://api.github.com/users/x10000year/followers", "following_url": "https://api.github.com/users/x10000year/following{/other_user}", "gists_url": "https://api.github.com/users/x10000year/gists{/gist_id}", "starred_url": "https://api.github.com/users/x10000year/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/x10000year/subscriptions", "organizations_url": "https://api.github.com/users/x10000year/orgs", "repos_url": "https://api.github.com/users/x10000year/repos", "events_url": "https://api.github.com/users/x10000year/events{/privacy}", "received_events_url": "https://api.github.com/users/x10000year/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2018-01-06T23:03:16Z", "updated_at": "2018-05-02T20:19:59Z", "closed_at": "2018-05-02T20:19:59Z", "author_association": "NONE", "body_html": "<p>I encountered a bug of tf.layers.batch_normalization when the training argument is not a constant. Usually, when the training argument evaluates to False, the moving mean and variance should not be updated. However, in certain cases, the moving mean and variance may become NaN.</p>\n<p>The bug occurs in the following code in python/layers/normalization.py:</p>\n<pre><code>    training_value = utils.constant_value(training)\n    if training_value is None:\n      one_minus_decay = utils.smart_cond(training,\n                                         lambda: self._one_minus_decay,\n                                         lambda: 0.)\n    else:\n      one_minus_decay = ops.convert_to_tensor(self._one_minus_decay)\n    if training_value or training_value is None:\n      mean_update = self._assign_moving_average(self.moving_mean, mean,\n                                                one_minus_decay)\n      variance_update = self._assign_moving_average(self.moving_variance,\n                                                    variance, one_minus_decay)\n</code></pre>\n<p>When training is not a constant but evaluates to False, one_minus_decay is set to 0, and it is expected that _assign_moving_average does not actually change the moving average. However, mean and variance are outputs of FusedBatchNormOp, which are not actually computed when training is False. So, the content of mean and variance are random, and it can contain NaN values in certain cases. The NaN values in mean and variance then lead to NaN values in the moving mean and moving variance, even if the one_minus_decay is 0 (NaN times 0 is still NaN). Once moving mean and variance contain NaN values, the network produces NaN outputs forever.</p>\n<p>I think a way to fix this issue is to modify _assign_moving_average. Just add following one line:<br>\n<code>update_delta = tf.cond(tf.equal(one_minus_decay, 0), 0, update_delta)</code><br>\nAnother way to fix is to let mean and variance output be zero if training is False. This way also helps preventing triggering the inf_or_nan_filter of tfdbg.</p>", "body_text": "I encountered a bug of tf.layers.batch_normalization when the training argument is not a constant. Usually, when the training argument evaluates to False, the moving mean and variance should not be updated. However, in certain cases, the moving mean and variance may become NaN.\nThe bug occurs in the following code in python/layers/normalization.py:\n    training_value = utils.constant_value(training)\n    if training_value is None:\n      one_minus_decay = utils.smart_cond(training,\n                                         lambda: self._one_minus_decay,\n                                         lambda: 0.)\n    else:\n      one_minus_decay = ops.convert_to_tensor(self._one_minus_decay)\n    if training_value or training_value is None:\n      mean_update = self._assign_moving_average(self.moving_mean, mean,\n                                                one_minus_decay)\n      variance_update = self._assign_moving_average(self.moving_variance,\n                                                    variance, one_minus_decay)\n\nWhen training is not a constant but evaluates to False, one_minus_decay is set to 0, and it is expected that _assign_moving_average does not actually change the moving average. However, mean and variance are outputs of FusedBatchNormOp, which are not actually computed when training is False. So, the content of mean and variance are random, and it can contain NaN values in certain cases. The NaN values in mean and variance then lead to NaN values in the moving mean and moving variance, even if the one_minus_decay is 0 (NaN times 0 is still NaN). Once moving mean and variance contain NaN values, the network produces NaN outputs forever.\nI think a way to fix this issue is to modify _assign_moving_average. Just add following one line:\nupdate_delta = tf.cond(tf.equal(one_minus_decay, 0), 0, update_delta)\nAnother way to fix is to let mean and variance output be zero if training is False. This way also helps preventing triggering the inf_or_nan_filter of tfdbg.", "body": "I encountered a bug of tf.layers.batch_normalization when the training argument is not a constant. Usually, when the training argument evaluates to False, the moving mean and variance should not be updated. However, in certain cases, the moving mean and variance may become NaN.\r\n\r\nThe bug occurs in the following code in python/layers/normalization.py:\r\n\r\n```\r\n    training_value = utils.constant_value(training)\r\n    if training_value is None:\r\n      one_minus_decay = utils.smart_cond(training,\r\n                                         lambda: self._one_minus_decay,\r\n                                         lambda: 0.)\r\n    else:\r\n      one_minus_decay = ops.convert_to_tensor(self._one_minus_decay)\r\n    if training_value or training_value is None:\r\n      mean_update = self._assign_moving_average(self.moving_mean, mean,\r\n                                                one_minus_decay)\r\n      variance_update = self._assign_moving_average(self.moving_variance,\r\n                                                    variance, one_minus_decay)\r\n```\r\n\r\nWhen training is not a constant but evaluates to False, one_minus_decay is set to 0, and it is expected that _assign_moving_average does not actually change the moving average. However, mean and variance are outputs of FusedBatchNormOp, which are not actually computed when training is False. So, the content of mean and variance are random, and it can contain NaN values in certain cases. The NaN values in mean and variance then lead to NaN values in the moving mean and moving variance, even if the one_minus_decay is 0 (NaN times 0 is still NaN). Once moving mean and variance contain NaN values, the network produces NaN outputs forever.\r\n\r\nI think a way to fix this issue is to modify _assign_moving_average. Just add following one line: \r\n`update_delta = tf.cond(tf.equal(one_minus_decay, 0), 0, update_delta)`\r\n Another way to fix is to let mean and variance output be zero if training is False. This way also helps preventing triggering the inf_or_nan_filter of tfdbg.\r\n  "}