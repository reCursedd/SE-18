{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5362", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5362/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5362/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5362/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5362", "id": 186925619, "node_id": "MDU6SXNzdWUxODY5MjU2MTk=", "number": 5362, "title": "CUDA_TOO_MANY_PEERS exception when launching on p2.16xlarge on aws", "user": {"login": "alexatknit", "id": 15474222, "node_id": "MDQ6VXNlcjE1NDc0MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/15474222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexatknit", "html_url": "https://github.com/alexatknit", "followers_url": "https://api.github.com/users/alexatknit/followers", "following_url": "https://api.github.com/users/alexatknit/following{/other_user}", "gists_url": "https://api.github.com/users/alexatknit/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexatknit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexatknit/subscriptions", "organizations_url": "https://api.github.com/users/alexatknit/orgs", "repos_url": "https://api.github.com/users/alexatknit/repos", "events_url": "https://api.github.com/users/alexatknit/events{/privacy}", "received_events_url": "https://api.github.com/users/alexatknit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173351, "node_id": "MDU6TGFiZWw0NzMxNzMzNTE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:build/install", "name": "type:build/install", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 17, "created_at": "2016-11-02T21:30:46Z", "updated_at": "2016-11-04T18:28:13Z", "closed_at": "2016-11-04T18:28:13Z", "author_association": "NONE", "body_html": "<p>This issue is identical to the issue discussed <a href=\"https://devtalk.nvidia.com/default/topic/970010/cuda-peer-resources-error-when-running-on-more-than-8-k80s-aws-p2-16xlarge-/\" rel=\"nofollow\">here</a>.</p>\n<p>When launching tensorflow to train on more than 8 gpu instances, tensorflow will cause a CUDA_TOO_MANY_PEERS error in the driver. The Tesla K80 shows up as 2 gpu instances internally so even though the p2.16xlarge technically only has 8 gpus it looks to the driver like it has 16. It seems that the cuda p2p system limits any gpu to only connect to a maximum 8 other gpus via p2p, but when the graph is launched, tensorflow (quite understandably) attempts to create a p2p connection between every gpu in the graph.</p>\n<p>Is there some way to disable p2p or is there a way via intelligent graph construction to limit the number of connections any given gpu requires, or even use a resource pool for gpu p2p connections?</p>\n<div class=\"highlight highlight-source-shell\"><pre>ubuntu@host:<span class=\"pl-k\">~</span>/workspace/nn$ nvidia-smi\nWed Nov  2 20:53:20 2016       \n+-----------------------------------------------------------------------------+\n<span class=\"pl-k\">|</span> NVIDIA-SMI 361.93.02              Driver Version: 361.93.02                 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span> GPU  Name        Persistence-M<span class=\"pl-k\">|</span> Bus-Id        Disp.A <span class=\"pl-k\">|</span> Volatile Uncorr. ECC <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> Fan  Temp  Perf  Pwr:Usage/Cap<span class=\"pl-k\">|</span>         Memory-Usage <span class=\"pl-k\">|</span> GPU-Util  Compute M. <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>===============================+======================+======================<span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>   0  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:0F.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   45C    P8    27W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   1  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:10.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   39C    P8    30W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   2  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:11.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   50C    P8    27W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   3  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:12.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   43C    P8    31W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   4  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:13.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   50C    P8    57W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   5  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:14.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   41C    P8    70W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   6  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:15.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   51C    P0    56W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   7  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:16.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   43C    P0    71W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   8  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:17.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   46C    P0    56W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   9  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:18.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   39C    P0    71W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>  10  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:19.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   49C    P0    58W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>  11  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:1A.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   40C    P0    73W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>  12  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:1B.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   49C    P0    58W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>  13  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:1C.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   40C    P0    70W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>  14  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:1D.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   49C    P0    60W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>  15  Tesla K80           Off  <span class=\"pl-k\">|</span> 0000:00:1E.0     Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   41C    P0    69W / 149W <span class=\"pl-k\">|</span>      0MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n<span class=\"pl-k\">|</span> Processes:                                                       GPU Memory <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>  GPU       PID  Type  Process name                               Usage      <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>=============================================================================<span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>  No running processes found                                                 <span class=\"pl-k\">|</span>\n+-----------------------------------------------------------------------------+\nubuntu@host:<span class=\"pl-k\">~</span>/workspace/nn$ python3 deploy_local.py \nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:0f.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x3bb3f120\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:10.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x3bf86790\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:11.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x3c3cf0c0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:12.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x4a7fe070\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:13.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x4ac4dd20\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:14.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x4b0a16a0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:15.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x4b4f9220\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:16.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x4b9545d0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 8 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:17.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x4bdb3920\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 9 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:18.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x4c216790\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 10 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:19.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x4c67d450\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 11 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1a.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x4cae8580\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 12 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1b.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x28999af0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 13 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1c.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x28e0bf00\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 14 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1d.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active<span class=\"pl-k\">;</span> existing: 0x29282290\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 15 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1e.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nE tensorflow/core/common_runtime/direct_session.cc:132] Internal: Internal: failed to <span class=\"pl-c1\">enable</span> peer access from 0x1c961680 to 0x18f1ac60: CUDA_ERROR_TOO_MANY_PEERS\nTraceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>deploy_local.py<span class=\"pl-pds\">\"</span></span>, line 39, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    n_test_batches=4,\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/ubuntu/workspace/nn/nn/model/network.py<span class=\"pl-pds\">\"</span></span>, line 271, <span class=\"pl-k\">in</span> train\n    test_steps=test_steps, save_steps=save_steps, load_all=load_all, debug=debug, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/home/ubuntu/workspace/nn/nn/train/train.py<span class=\"pl-pds\">\"</span></span>, line 86, <span class=\"pl-k\">in</span> train_model\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)) as sesh:\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line 1138, <span class=\"pl-k\">in</span> __init__\n    super(Session, self).__init__(target, graph, config=config)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py<span class=\"pl-pds\">\"</span></span>, line 502, <span class=\"pl-k\">in</span> __init__\n    self._session = tf_session.TF_NewSession(opts, status)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/lib/python3.4/contextlib.py<span class=\"pl-pds\">\"</span></span>, line 66, <span class=\"pl-k\">in</span> __exit__\n    next(self.gen)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py<span class=\"pl-pds\">\"</span></span>, line 463, <span class=\"pl-k\">in</span> raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors.InternalError: Failed to create session.</pre></div>", "body_text": "This issue is identical to the issue discussed here.\nWhen launching tensorflow to train on more than 8 gpu instances, tensorflow will cause a CUDA_TOO_MANY_PEERS error in the driver. The Tesla K80 shows up as 2 gpu instances internally so even though the p2.16xlarge technically only has 8 gpus it looks to the driver like it has 16. It seems that the cuda p2p system limits any gpu to only connect to a maximum 8 other gpus via p2p, but when the graph is launched, tensorflow (quite understandably) attempts to create a p2p connection between every gpu in the graph.\nIs there some way to disable p2p or is there a way via intelligent graph construction to limit the number of connections any given gpu requires, or even use a resource pool for gpu p2p connections?\nubuntu@host:~/workspace/nn$ nvidia-smi\nWed Nov  2 20:53:20 2016       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 361.93.02              Driver Version: 361.93.02                 |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 0000:00:0F.0     Off |                    0 |\n| N/A   45C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 0000:00:10.0     Off |                    0 |\n| N/A   39C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           Off  | 0000:00:11.0     Off |                    0 |\n| N/A   50C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           Off  | 0000:00:12.0     Off |                    0 |\n| N/A   43C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   4  Tesla K80           Off  | 0000:00:13.0     Off |                    0 |\n| N/A   50C    P8    57W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   5  Tesla K80           Off  | 0000:00:14.0     Off |                    0 |\n| N/A   41C    P8    70W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   6  Tesla K80           Off  | 0000:00:15.0     Off |                    0 |\n| N/A   51C    P0    56W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   7  Tesla K80           Off  | 0000:00:16.0     Off |                    0 |\n| N/A   43C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   8  Tesla K80           Off  | 0000:00:17.0     Off |                    0 |\n| N/A   46C    P0    56W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   9  Tesla K80           Off  | 0000:00:18.0     Off |                    0 |\n| N/A   39C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|  10  Tesla K80           Off  | 0000:00:19.0     Off |                    0 |\n| N/A   49C    P0    58W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|  11  Tesla K80           Off  | 0000:00:1A.0     Off |                    0 |\n| N/A   40C    P0    73W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|  12  Tesla K80           Off  | 0000:00:1B.0     Off |                    0 |\n| N/A   49C    P0    58W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|  13  Tesla K80           Off  | 0000:00:1C.0     Off |                    0 |\n| N/A   40C    P0    70W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|  14  Tesla K80           Off  | 0000:00:1D.0     Off |                    0 |\n| N/A   49C    P0    60W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|  15  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\n| N/A   41C    P0    69W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nubuntu@host:~/workspace/nn$ python3 deploy_local.py \nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:0f.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3bb3f120\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:10.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3bf86790\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:11.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3c3cf0c0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:12.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4a7fe070\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:13.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4ac4dd20\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:14.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4b0a16a0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:15.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4b4f9220\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:16.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4b9545d0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 8 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:17.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4bdb3920\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 9 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:18.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4c216790\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 10 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:19.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4c67d450\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 11 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1a.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4cae8580\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 12 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1b.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x28999af0\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 13 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1c.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x28e0bf00\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 14 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1d.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x29282290\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 15 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:1e.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nE tensorflow/core/common_runtime/direct_session.cc:132] Internal: Internal: failed to enable peer access from 0x1c961680 to 0x18f1ac60: CUDA_ERROR_TOO_MANY_PEERS\nTraceback (most recent call last):\n  File \"deploy_local.py\", line 39, in <module>\n    n_test_batches=4,\n  File \"/home/ubuntu/workspace/nn/nn/model/network.py\", line 271, in train\n    test_steps=test_steps, save_steps=save_steps, load_all=load_all, debug=debug, **kwargs)\n  File \"/home/ubuntu/workspace/nn/nn/train/train.py\", line 86, in train_model\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)) as sesh:\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 1138, in __init__\n    super(Session, self).__init__(target, graph, config=config)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 502, in __init__\n    self._session = tf_session.TF_NewSession(opts, status)\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors.InternalError: Failed to create session.", "body": "This issue is identical to the issue discussed [here](https://devtalk.nvidia.com/default/topic/970010/cuda-peer-resources-error-when-running-on-more-than-8-k80s-aws-p2-16xlarge-/).\r\n\r\nWhen launching tensorflow to train on more than 8 gpu instances, tensorflow will cause a CUDA_TOO_MANY_PEERS error in the driver. The Tesla K80 shows up as 2 gpu instances internally so even though the p2.16xlarge technically only has 8 gpus it looks to the driver like it has 16. It seems that the cuda p2p system limits any gpu to only connect to a maximum 8 other gpus via p2p, but when the graph is launched, tensorflow (quite understandably) attempts to create a p2p connection between every gpu in the graph.\r\n\r\nIs there some way to disable p2p or is there a way via intelligent graph construction to limit the number of connections any given gpu requires, or even use a resource pool for gpu p2p connections?\r\n\r\n```bash\r\nubuntu@host:~/workspace/nn$ nvidia-smi\r\nWed Nov  2 20:53:20 2016       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 361.93.02              Driver Version: 361.93.02                 |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:00:0F.0     Off |                    0 |\r\n| N/A   45C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:00:10.0     Off |                    0 |\r\n| N/A   39C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla K80           Off  | 0000:00:11.0     Off |                    0 |\r\n| N/A   50C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla K80           Off  | 0000:00:12.0     Off |                    0 |\r\n| N/A   43C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla K80           Off  | 0000:00:13.0     Off |                    0 |\r\n| N/A   50C    P8    57W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla K80           Off  | 0000:00:14.0     Off |                    0 |\r\n| N/A   41C    P8    70W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla K80           Off  | 0000:00:15.0     Off |                    0 |\r\n| N/A   51C    P0    56W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla K80           Off  | 0000:00:16.0     Off |                    0 |\r\n| N/A   43C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   8  Tesla K80           Off  | 0000:00:17.0     Off |                    0 |\r\n| N/A   46C    P0    56W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   9  Tesla K80           Off  | 0000:00:18.0     Off |                    0 |\r\n| N/A   39C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  10  Tesla K80           Off  | 0000:00:19.0     Off |                    0 |\r\n| N/A   49C    P0    58W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  11  Tesla K80           Off  | 0000:00:1A.0     Off |                    0 |\r\n| N/A   40C    P0    73W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  12  Tesla K80           Off  | 0000:00:1B.0     Off |                    0 |\r\n| N/A   49C    P0    58W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  13  Tesla K80           Off  | 0000:00:1C.0     Off |                    0 |\r\n| N/A   40C    P0    70W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  14  Tesla K80           Off  | 0000:00:1D.0     Off |                    0 |\r\n| N/A   49C    P0    60W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|  15  Tesla K80           Off  | 0000:00:1E.0     Off |                    0 |\r\n| N/A   41C    P0    69W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\nubuntu@host:~/workspace/nn$ python3 deploy_local.py \r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:0f.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3bb3f120\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:10.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3bf86790\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 2 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:11.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3c3cf0c0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 3 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:12.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4a7fe070\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 4 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:13.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4ac4dd20\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 5 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:14.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4b0a16a0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 6 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:15.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4b4f9220\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 7 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:16.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4b9545d0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 8 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:17.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4bdb3920\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 9 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:18.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4c216790\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 10 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:19.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4c67d450\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 11 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1a.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x4cae8580\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 12 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1b.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x28999af0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 13 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1c.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x28e0bf00\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 14 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1d.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x29282290\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 15 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:00:1e.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nE tensorflow/core/common_runtime/direct_session.cc:132] Internal: Internal: failed to enable peer access from 0x1c961680 to 0x18f1ac60: CUDA_ERROR_TOO_MANY_PEERS\r\nTraceback (most recent call last):\r\n  File \"deploy_local.py\", line 39, in <module>\r\n    n_test_batches=4,\r\n  File \"/home/ubuntu/workspace/nn/nn/model/network.py\", line 271, in train\r\n    test_steps=test_steps, save_steps=save_steps, load_all=load_all, debug=debug, **kwargs)\r\n  File \"/home/ubuntu/workspace/nn/nn/train/train.py\", line 86, in train_model\r\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)) as sesh:\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 1138, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 502, in __init__\r\n    self._session = tf_session.TF_NewSession(opts, status)\r\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors.InternalError: Failed to create session.\r\n```"}