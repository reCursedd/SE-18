{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23822", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23822/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23822/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23822/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23822", "id": 381889271, "node_id": "MDU6SXNzdWUzODE4ODkyNzE=", "number": 23822, "title": "Event files created by estimator are not closed after training/evaluation", "user": {"login": "phil510", "id": 30630542, "node_id": "MDQ6VXNlcjMwNjMwNTQy", "avatar_url": "https://avatars3.githubusercontent.com/u/30630542?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phil510", "html_url": "https://github.com/phil510", "followers_url": "https://api.github.com/users/phil510/followers", "following_url": "https://api.github.com/users/phil510/following{/other_user}", "gists_url": "https://api.github.com/users/phil510/gists{/gist_id}", "starred_url": "https://api.github.com/users/phil510/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phil510/subscriptions", "organizations_url": "https://api.github.com/users/phil510/orgs", "repos_url": "https://api.github.com/users/phil510/repos", "events_url": "https://api.github.com/users/phil510/events{/privacy}", "received_events_url": "https://api.github.com/users/phil510/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-17T18:43:16Z", "updated_at": "2018-11-17T18:43:16Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm using the estimator API and a custom implementation of several hyperparameter search methods (random search, hyperband, etc.) to train multiple models and select the best hyperparameters. At the end of the algorithm I would like to delete all the files created during the trainings/evaluations (e.g., the checkpoint files, event files, etc.). I'm trying this with shutil.rmtree; however, I am not able to delete the event files as it appears they are still in use.</p>\n<p>Here is an example code - mostly pulled from the Iris example scripts - along with the error to illustrate the issue. The actual code uses a custom estimator implementation instead of the DNNClassifier, but this has the same problem.</p>\n<pre><code>import tensorflow as tf\nimport pandas as pd\nimport os\nimport shutil\n\ndef load_data(y_name='Species'):\n    train_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n    test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n\n    train_data = pd.read_csv(train_url)\n    train_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n    test_data = pd.read_csv(test_url)\n    test_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n    \n    train_x = train_data.iloc[:, 0:4]\n    train_y = train_data.iloc[:, 4]\n    \n    test_x = test_data.iloc[:, 0:4]\n    test_y = test_data.iloc[:, 4]\n    \n    return (train_x, train_y), (test_x, test_y)\n\ndef train_input_fn(features, labels, batch_size):\n    \"\"\"An input function for training\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n    # Shuffle, repeat, and batch the examples.\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n\ndef eval_input_fn(features, labels, batch_size):\n    \"\"\"An input function for evaluation or prediction\"\"\"\n    features=dict(features)\n    if labels is None:\n        # No labels, use only features.\n        inputs = features\n    else:\n        inputs = (features, labels)\n\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n\n    # Batch the examples\n    assert batch_size is not None, \"batch_size must not be None\"\n    dataset = dataset.batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n\nmodel_dir = os.path.join(os.getcwd(), *['estimator_test', 'test_1'])\n\n(train_x, train_y), (test_x, test_y) = load_data()\n\n# Feature columns describe how to use the input.\nmy_feature_columns = []\nfor key in train_x.keys():\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n\n# Build 2 hidden layer DNN with 10, 10 units respectively.\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    # Two hidden layers of 10 nodes each.\n    hidden_units=[10, 10],\n    # The model must choose between 3 classes.\n    n_classes=3,\n    model_dir = model_dir)\n\n# Train the Model.\nclassifier.train(\n    input_fn=lambda: train_input_fn(train_x, train_y, 100),\n    steps=100)\n\n# Evaluate the model.\neval_result = classifier.evaluate(\n    input_fn=lambda: eval_input_fn(test_x, test_y, 100))\n\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n\nshutil.rmtree(model_dir)\n</code></pre>\n<p>Here is the error:</p>\n<pre lang=\"OSError\" data-meta=\"                                  Traceback (most recent call last)\"><code>&lt;ipython-input-40-beccd7cbb56e&gt; in &lt;module&gt;()\n----&gt; 1 shutil.rmtree(os.path.join(os.getcwd(), *['estimator_test', 'delete_test_1']))\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in rmtree(path, ignore_errors, onerror)\n    492             os.close(fd)\n    493     else:\n--&gt; 494         return _rmtree_unsafe(path, onerror)\n    495 \n    496 # Allow introspection of whether or not the hardening against symlink\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\n    391         os.rmdir(path)\n    392     except OSError:\n--&gt; 393         onerror(os.rmdir, path, sys.exc_info())\n    394 \n    395 # Version using fd-based APIs to protect against races\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\n    389                 onerror(os.unlink, fullname, sys.exc_info())\n    390     try:\n--&gt; 391         os.rmdir(path)\n    392     except OSError:\n    393         onerror(os.rmdir, path, sys.exc_info())\n\nOSError: [WinError 145] The directory is not empty:\n</code></pre>\n<p>I looked at all the files that were open by Python processes, and it's only the event files that are still in use.  I would expect that all files would be closed after training/evaluation are complete. Unfortunately, I cannot find a way to manually close these after training.</p>\n<p>On a related note (and this is more of a feature request), it would be great to be able to disable writing summaries in the estimator configuration as the files are pretty large. This would remove the need to delete them after training altogether.</p>\n<p>System information</p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows</li>\n<li>TensorFlow installed from (source or binary): Source</li>\n<li>TensorFlow version (use command below): 1.11 (CPU version)</li>\n<li>Python version: 3.6</li>\n</ul>", "body_text": "I'm using the estimator API and a custom implementation of several hyperparameter search methods (random search, hyperband, etc.) to train multiple models and select the best hyperparameters. At the end of the algorithm I would like to delete all the files created during the trainings/evaluations (e.g., the checkpoint files, event files, etc.). I'm trying this with shutil.rmtree; however, I am not able to delete the event files as it appears they are still in use.\nHere is an example code - mostly pulled from the Iris example scripts - along with the error to illustrate the issue. The actual code uses a custom estimator implementation instead of the DNNClassifier, but this has the same problem.\nimport tensorflow as tf\nimport pandas as pd\nimport os\nimport shutil\n\ndef load_data(y_name='Species'):\n    train_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n    test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n\n    train_data = pd.read_csv(train_url)\n    train_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n    test_data = pd.read_csv(test_url)\n    test_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n    \n    train_x = train_data.iloc[:, 0:4]\n    train_y = train_data.iloc[:, 4]\n    \n    test_x = test_data.iloc[:, 0:4]\n    test_y = test_data.iloc[:, 4]\n    \n    return (train_x, train_y), (test_x, test_y)\n\ndef train_input_fn(features, labels, batch_size):\n    \"\"\"An input function for training\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n    # Shuffle, repeat, and batch the examples.\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n\ndef eval_input_fn(features, labels, batch_size):\n    \"\"\"An input function for evaluation or prediction\"\"\"\n    features=dict(features)\n    if labels is None:\n        # No labels, use only features.\n        inputs = features\n    else:\n        inputs = (features, labels)\n\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n\n    # Batch the examples\n    assert batch_size is not None, \"batch_size must not be None\"\n    dataset = dataset.batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n\nmodel_dir = os.path.join(os.getcwd(), *['estimator_test', 'test_1'])\n\n(train_x, train_y), (test_x, test_y) = load_data()\n\n# Feature columns describe how to use the input.\nmy_feature_columns = []\nfor key in train_x.keys():\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n\n# Build 2 hidden layer DNN with 10, 10 units respectively.\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    # Two hidden layers of 10 nodes each.\n    hidden_units=[10, 10],\n    # The model must choose between 3 classes.\n    n_classes=3,\n    model_dir = model_dir)\n\n# Train the Model.\nclassifier.train(\n    input_fn=lambda: train_input_fn(train_x, train_y, 100),\n    steps=100)\n\n# Evaluate the model.\neval_result = classifier.evaluate(\n    input_fn=lambda: eval_input_fn(test_x, test_y, 100))\n\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n\nshutil.rmtree(model_dir)\n\nHere is the error:\n<ipython-input-40-beccd7cbb56e> in <module>()\n----> 1 shutil.rmtree(os.path.join(os.getcwd(), *['estimator_test', 'delete_test_1']))\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in rmtree(path, ignore_errors, onerror)\n    492             os.close(fd)\n    493     else:\n--> 494         return _rmtree_unsafe(path, onerror)\n    495 \n    496 # Allow introspection of whether or not the hardening against symlink\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\n    391         os.rmdir(path)\n    392     except OSError:\n--> 393         onerror(os.rmdir, path, sys.exc_info())\n    394 \n    395 # Version using fd-based APIs to protect against races\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\n    389                 onerror(os.unlink, fullname, sys.exc_info())\n    390     try:\n--> 391         os.rmdir(path)\n    392     except OSError:\n    393         onerror(os.rmdir, path, sys.exc_info())\n\nOSError: [WinError 145] The directory is not empty:\n\nI looked at all the files that were open by Python processes, and it's only the event files that are still in use.  I would expect that all files would be closed after training/evaluation are complete. Unfortunately, I cannot find a way to manually close these after training.\nOn a related note (and this is more of a feature request), it would be great to be able to disable writing summaries in the estimator configuration as the files are pretty large. This would remove the need to delete them after training altogether.\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.11 (CPU version)\nPython version: 3.6", "body": "I'm using the estimator API and a custom implementation of several hyperparameter search methods (random search, hyperband, etc.) to train multiple models and select the best hyperparameters. At the end of the algorithm I would like to delete all the files created during the trainings/evaluations (e.g., the checkpoint files, event files, etc.). I'm trying this with shutil.rmtree; however, I am not able to delete the event files as it appears they are still in use.\r\n\r\nHere is an example code - mostly pulled from the Iris example scripts - along with the error to illustrate the issue. The actual code uses a custom estimator implementation instead of the DNNClassifier, but this has the same problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport os\r\nimport shutil\r\n\r\ndef load_data(y_name='Species'):\r\n    train_url = \"http://download.tensorflow.org/data/iris_training.csv\"\r\n    test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\r\n\r\n    train_data = pd.read_csv(train_url)\r\n    train_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\r\n    test_data = pd.read_csv(test_url)\r\n    test_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\r\n    \r\n    train_x = train_data.iloc[:, 0:4]\r\n    train_y = train_data.iloc[:, 4]\r\n    \r\n    test_x = test_data.iloc[:, 0:4]\r\n    test_y = test_data.iloc[:, 4]\r\n    \r\n    return (train_x, train_y), (test_x, test_y)\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for training\"\"\"\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\ndef eval_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for evaluation or prediction\"\"\"\r\n    features=dict(features)\r\n    if labels is None:\r\n        # No labels, use only features.\r\n        inputs = features\r\n    else:\r\n        inputs = (features, labels)\r\n\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\r\n\r\n    # Batch the examples\r\n    assert batch_size is not None, \"batch_size must not be None\"\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\nmodel_dir = os.path.join(os.getcwd(), *['estimator_test', 'test_1'])\r\n\r\n(train_x, train_y), (test_x, test_y) = load_data()\r\n\r\n# Feature columns describe how to use the input.\r\nmy_feature_columns = []\r\nfor key in train_x.keys():\r\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\r\n\r\n# Build 2 hidden layer DNN with 10, 10 units respectively.\r\nclassifier = tf.estimator.DNNClassifier(\r\n    feature_columns=my_feature_columns,\r\n    # Two hidden layers of 10 nodes each.\r\n    hidden_units=[10, 10],\r\n    # The model must choose between 3 classes.\r\n    n_classes=3,\r\n    model_dir = model_dir)\r\n\r\n# Train the Model.\r\nclassifier.train(\r\n    input_fn=lambda: train_input_fn(train_x, train_y, 100),\r\n    steps=100)\r\n\r\n# Evaluate the model.\r\neval_result = classifier.evaluate(\r\n    input_fn=lambda: eval_input_fn(test_x, test_y, 100))\r\n\r\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\r\n\r\nshutil.rmtree(model_dir)\r\n```\r\nHere is the error:\r\n```OSError                                   Traceback (most recent call last)\r\n<ipython-input-40-beccd7cbb56e> in <module>()\r\n----> 1 shutil.rmtree(os.path.join(os.getcwd(), *['estimator_test', 'delete_test_1']))\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in rmtree(path, ignore_errors, onerror)\r\n    492             os.close(fd)\r\n    493     else:\r\n--> 494         return _rmtree_unsafe(path, onerror)\r\n    495 \r\n    496 # Allow introspection of whether or not the hardening against symlink\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\r\n    391         os.rmdir(path)\r\n    392     except OSError:\r\n--> 393         onerror(os.rmdir, path, sys.exc_info())\r\n    394 \r\n    395 # Version using fd-based APIs to protect against races\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\r\n    389                 onerror(os.unlink, fullname, sys.exc_info())\r\n    390     try:\r\n--> 391         os.rmdir(path)\r\n    392     except OSError:\r\n    393         onerror(os.rmdir, path, sys.exc_info())\r\n\r\nOSError: [WinError 145] The directory is not empty:\r\n```\r\nI looked at all the files that were open by Python processes, and it's only the event files that are still in use.  I would expect that all files would be closed after training/evaluation are complete. Unfortunately, I cannot find a way to manually close these after training.\r\n\r\nOn a related note (and this is more of a feature request), it would be great to be able to disable writing summaries in the estimator configuration as the files are pretty large. This would remove the need to delete them after training altogether. \r\n\r\nSystem information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.11 (CPU version)\r\n- Python version: 3.6\r\n"}