{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15045", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15045/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15045/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15045/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15045", "id": 278559116, "node_id": "MDU6SXNzdWUyNzg1NTkxMTY=", "number": 15045, "title": "Updated Tensor flow, incompatible function parameters. (Float32>int)", "user": {"login": "IB888", "id": 29187519, "node_id": "MDQ6VXNlcjI5MTg3NTE5", "avatar_url": "https://avatars2.githubusercontent.com/u/29187519?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IB888", "html_url": "https://github.com/IB888", "followers_url": "https://api.github.com/users/IB888/followers", "following_url": "https://api.github.com/users/IB888/following{/other_user}", "gists_url": "https://api.github.com/users/IB888/gists{/gist_id}", "starred_url": "https://api.github.com/users/IB888/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IB888/subscriptions", "organizations_url": "https://api.github.com/users/IB888/orgs", "repos_url": "https://api.github.com/users/IB888/repos", "events_url": "https://api.github.com/users/IB888/events{/privacy}", "received_events_url": "https://api.github.com/users/IB888/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2017-12-01T18:47:28Z", "updated_at": "2018-06-17T18:32:46Z", "closed_at": "2018-06-17T18:29:41Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<h3>System information</h3>\n<p>Windows 10<br>\nTensorFlow installed using pip.<br>\nI am using CDU<br>\nPython 3.6.3 (using Spyder)<br>\nTensorFlow version: 1.4.0</p>\n<p>Have I written custom code: No<br>\nOS Platform and Distribution:<br>\nTensorFlow installed from<br>\nBazel version:NA<br>\nCUDA/cuDNN version:NA<br>\nGPU model and memory: using cpu.<br>\nExact command to reproduce:<br>\nactivate myenvrionment<br>\nspyder</p>\n<p>This code was written on 0.x tensorFlow, it could be an update issue, but I can't find the outdated function.</p>\n<p>`</p>\n<pre><code>from keras import optimizers\nfrom keras.models import Model\nfrom keras.layers import Dropout, Lambda\nfrom keras.layers import Input, average\nfrom keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose\nfrom keras.layers import ZeroPadding2D, Cropping2D\nfrom keras import backend as K\n\n\ndef mvn(tensor):\n    '''Performs per-channel spatial mean-variance normalization.'''\n    epsilon = 1e-6\n    mean = K.mean(tensor, axis=(1,2), keepdims=True)\n    std = K.std(tensor, axis=(1,2), keepdims=True)\n    mvn = (tensor - mean) / (std + epsilon)\n    \n    return mvn\n\n\ndef crop(tensors):\n    '''\n    List of 2 tensors, the second tensor having larger spatial dimensions.\n    '''\n    h_dims, w_dims = [], []\n    for t in tensors:\n        b, h, w, d = K.get_variable_shape(t)\n        h_dims.append(h)\n        w_dims.append(w)\n    crop_h, crop_w = (h_dims[1] - h_dims[0]), (w_dims[1] - w_dims[0])\n    rem_h = crop_h % 2\n    rem_w = crop_w % 2\n    crop_h_dims = (crop_h / 2, crop_h / 2 + rem_h)\n    crop_w_dims = (crop_w / 2, crop_w / 2 + rem_w)\n    cropped = Cropping2D(cropping=(crop_h_dims, crop_w_dims))(tensors[1])\n    \n    return cropped\n\n\ndef dice_coef(y_true, y_pred, smooth=0.0):\n    '''Average dice coefficient per batch.'''\n    axes = (1,2,3)\n    intersection = K.sum(y_true * y_pred, axis=axes)\n    summation = K.sum(y_true, axis=axes) + K.sum(y_pred, axis=axes)\n    \n    return K.mean((2.0 * intersection + smooth) / (summation + smooth), axis=0)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred, smooth=10.0)\n\n\ndef jaccard_coef(y_true, y_pred, smooth=0.0):\n    '''Average jaccard coefficient per batch.'''\n    axes = (1,2,3)\n    intersection = K.sum(y_true * y_pred, axis=axes)\n    union = K.sum(y_true, axis=axes) + K.sum(y_pred, axis=axes) - intersection\n    return K.mean( (intersection + smooth) / (union + smooth), axis=0)\n\n\ndef fcn_model(input_shape, num_classes, weights=None):\n    ''' \"Skip\" FCN architecture similar to Long et al., 2015\n    https://arxiv.org/abs/1411.4038\n    '''\n    if num_classes == 2:\n        num_classes = 1\n        loss = dice_coef_loss\n        activation = 'sigmoid'\n    else:\n        loss = 'categorical_crossentropy'\n        activation = 'softmax'\n\n    kwargs = dict(\n        kernel_size=3,\n        strides=1,\n        activation='relu',\n        padding='same',\n        use_bias=True,\n        kernel_initializer='glorot_uniform',\n        bias_initializer='zeros',\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        trainable=True,\n    )\n    \n    data = Input(shape=input_shape, dtype='float', name='data')\n    mvn0 = Lambda(mvn, name='mvn0')(data)\n    pad = ZeroPadding2D(padding=5, name='pad')(mvn0)\n\n    conv1 = Conv2D(filters=64, name='conv1', **kwargs)(pad)\n    mvn1 = Lambda(mvn, name='mvn1')(conv1)\n    \n    conv2 = Conv2D(filters=64, name='conv2', **kwargs)(mvn1)\n    mvn2 = Lambda(mvn, name='mvn2')(conv2)\n\n    conv3 = Conv2D(filters=64, name='conv3', **kwargs)(mvn2)\n    mvn3 = Lambda(mvn, name='mvn3')(conv3)\n    pool1 = MaxPooling2D(pool_size=3, strides=2,\n                    padding='valid', name='pool1')(mvn3)\n\n    \n    conv4 = Conv2D(filters=128, name='conv4', **kwargs)(pool1)\n    mvn4 = Lambda(mvn, name='mvn4')(conv4)\n\n    conv5 = Conv2D(filters=128, name='conv5', **kwargs)(mvn4)\n    mvn5 = Lambda(mvn, name='mvn5')(conv5)\n\n    conv6 = Conv2D(filters=128, name='conv6', **kwargs)(mvn5)\n    mvn6 = Lambda(mvn, name='mvn6')(conv6)\n\n    conv7 = Conv2D(filters=128, name='conv7', **kwargs)(mvn6)\n    mvn7 = Lambda(mvn, name='mvn7')(conv7)\n    pool2 = MaxPooling2D(pool_size=3, strides=2,\n                    padding='valid', name='pool2')(mvn7)\n\n\n    conv8 = Conv2D(filters=256, name='conv8', **kwargs)(pool2)\n    mvn8 = Lambda(mvn, name='mvn8')(conv8)\n\n    conv9 = Conv2D(filters=256, name='conv9', **kwargs)(mvn8)\n    mvn9 = Lambda(mvn, name='mvn9')(conv9)\n\n    conv10 = Conv2D(filters=256, name='conv10', **kwargs)(mvn9)\n    mvn10 = Lambda(mvn, name='mvn10')(conv10)\n\n    conv11 = Conv2D(filters=256, name='conv11', **kwargs)(mvn10)\n    mvn11 = Lambda(mvn, name='mvn11')(conv11)\n    pool3 = MaxPooling2D(pool_size=3, strides=2,\n                    padding='valid', name='pool3')(mvn11)\n    drop1 = Dropout(rate=0.5, name='drop1')(pool3)\n\n\n    conv12 = Conv2D(filters=512, name='conv12', **kwargs)(drop1)\n    mvn12 = Lambda(mvn, name='mvn12')(conv12)\n\n    conv13 = Conv2D(filters=512, name='conv13', **kwargs)(mvn12)\n    mvn13 = Lambda(mvn, name='mvn13')(conv13)\n\n    conv14 = Conv2D(filters=512, name='conv14', **kwargs)(mvn13)\n    mvn14 = Lambda(mvn, name='mvn14')(conv14)\n\n    conv15 = Conv2D(filters=512, name='conv15', **kwargs)(mvn14)\n    mvn15 = Lambda(mvn, name='mvn15')(conv15)\n    drop2 = Dropout(rate=0.5, name='drop2')(mvn15)\n\n\n    score_conv15 = Conv2D(filters=num_classes, kernel_size=1,\n                        strides=1, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=True,\n                        name='score_conv15')(drop2)\n    upsample1 = Conv2DTranspose(filters=num_classes, kernel_size=3,\n                        strides=2, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=False,\n                        name='upsample1')(score_conv15)\n    score_conv11 = Conv2D(filters=num_classes, kernel_size=1,\n                        strides=1, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=True,\n                        name='score_conv11')(mvn11)\n    crop1 = Lambda(crop, name='crop1')([upsample1, score_conv11])\n    fuse_scores1 = average([crop1, upsample1], name='fuse_scores1')\n    \n    upsample2 = Conv2DTranspose(filters=num_classes, kernel_size=3,\n                        strides=2, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=False,\n                        name='upsample2')(fuse_scores1)\n    score_conv7 = Conv2D(filters=num_classes, kernel_size=1,\n                        strides=1, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=True,\n                        name='score_conv7')(mvn7)\n    crop2 = Lambda(crop, name='crop2')([upsample2, score_conv7])\n    fuse_scores2 = average([crop2, upsample2], name='fuse_scores2')\n    \n    upsample3 = Conv2DTranspose(filters=num_classes, kernel_size=3,\n                        strides=2, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=False,\n                        name='upsample3')(fuse_scores2)\n    crop3 = Lambda(crop, name='crop3')([data, upsample3])\n    predictions = Conv2D(filters=num_classes, kernel_size=1,\n                        strides=1, activation=activation, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=True,\n                        name='predictions')(crop3)\n    \n    model = Model(inputs=data, outputs=predictions)\n    if weights is not None:\n        model.load_weights(weights)\n    sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n    model.compile(optimizer=sgd, loss=loss,\n                  metrics=['accuracy', dice_coef, jaccard_coef])\n\n    return model\n\n\nif __name__ == '__main__':\n    model = fcn_model((100, 100, 1), 2, weights=None)\n</code></pre>\n<p>`</p>\n<h1>Spyder execution log:</h1>\n<p>runfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')<br>\nUsing TensorFlow backend.<br>\nTraceback (most recent call last):</p>\n<p>File \"\", line 1, in <br>\nrunfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 710, in runfile<br>\nexecfile(filename, namespace)</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 101, in execfile<br>\nexec(compile(f.read(), filename, 'exec'), namespace)</p>\n<p>File \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 197, in <br>\nmodel = fcn_model((100, 100, 1), 2, weights=None)</p>\n<p>File \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 162, in fcn_model<br>\ncrop1 = Lambda(crop, name='crop1')([upsample1, score_conv11])</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in <strong>call</strong><br>\noutput = self.call(inputs, **kwargs)</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 651, in call<br>\nreturn self.function(inputs, **arguments)</p>\n<p>File \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 36, in crop<br>\ncropped = Cropping2D(cropping=(crop_h_dims, crop_w_dims))(tensors[1])</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in <strong>call</strong><br>\noutput = self.call(inputs, **kwargs)</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 1874, in call<br>\nself.cropping[1][0]: -self.cropping[1][1],</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper<br>\nname=name)</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 706, in strided_slice<br>\nshrink_axis_mask=shrink_axis_mask)</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5429, in strided_slice<br>\nname=name)</p>\n<p>File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 609, in _apply_op_helper<br>\nparam_name=input_name)</p>\n<p>**File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 60, in _SatisfiesTypeConstraint<br>\n\", \".join(dtypes.as_dtype(x).name for x in allowed_list)))</p>\n<p><strong>TypeError: Value passed to parameter 'begin' has DataType float32 not in list of allowed values: int32, int64</strong>**</p>\n<p>Please help.</p>", "body_text": "Hi,\nSystem information\nWindows 10\nTensorFlow installed using pip.\nI am using CDU\nPython 3.6.3 (using Spyder)\nTensorFlow version: 1.4.0\nHave I written custom code: No\nOS Platform and Distribution:\nTensorFlow installed from\nBazel version:NA\nCUDA/cuDNN version:NA\nGPU model and memory: using cpu.\nExact command to reproduce:\nactivate myenvrionment\nspyder\nThis code was written on 0.x tensorFlow, it could be an update issue, but I can't find the outdated function.\n`\nfrom keras import optimizers\nfrom keras.models import Model\nfrom keras.layers import Dropout, Lambda\nfrom keras.layers import Input, average\nfrom keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose\nfrom keras.layers import ZeroPadding2D, Cropping2D\nfrom keras import backend as K\n\n\ndef mvn(tensor):\n    '''Performs per-channel spatial mean-variance normalization.'''\n    epsilon = 1e-6\n    mean = K.mean(tensor, axis=(1,2), keepdims=True)\n    std = K.std(tensor, axis=(1,2), keepdims=True)\n    mvn = (tensor - mean) / (std + epsilon)\n    \n    return mvn\n\n\ndef crop(tensors):\n    '''\n    List of 2 tensors, the second tensor having larger spatial dimensions.\n    '''\n    h_dims, w_dims = [], []\n    for t in tensors:\n        b, h, w, d = K.get_variable_shape(t)\n        h_dims.append(h)\n        w_dims.append(w)\n    crop_h, crop_w = (h_dims[1] - h_dims[0]), (w_dims[1] - w_dims[0])\n    rem_h = crop_h % 2\n    rem_w = crop_w % 2\n    crop_h_dims = (crop_h / 2, crop_h / 2 + rem_h)\n    crop_w_dims = (crop_w / 2, crop_w / 2 + rem_w)\n    cropped = Cropping2D(cropping=(crop_h_dims, crop_w_dims))(tensors[1])\n    \n    return cropped\n\n\ndef dice_coef(y_true, y_pred, smooth=0.0):\n    '''Average dice coefficient per batch.'''\n    axes = (1,2,3)\n    intersection = K.sum(y_true * y_pred, axis=axes)\n    summation = K.sum(y_true, axis=axes) + K.sum(y_pred, axis=axes)\n    \n    return K.mean((2.0 * intersection + smooth) / (summation + smooth), axis=0)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred, smooth=10.0)\n\n\ndef jaccard_coef(y_true, y_pred, smooth=0.0):\n    '''Average jaccard coefficient per batch.'''\n    axes = (1,2,3)\n    intersection = K.sum(y_true * y_pred, axis=axes)\n    union = K.sum(y_true, axis=axes) + K.sum(y_pred, axis=axes) - intersection\n    return K.mean( (intersection + smooth) / (union + smooth), axis=0)\n\n\ndef fcn_model(input_shape, num_classes, weights=None):\n    ''' \"Skip\" FCN architecture similar to Long et al., 2015\n    https://arxiv.org/abs/1411.4038\n    '''\n    if num_classes == 2:\n        num_classes = 1\n        loss = dice_coef_loss\n        activation = 'sigmoid'\n    else:\n        loss = 'categorical_crossentropy'\n        activation = 'softmax'\n\n    kwargs = dict(\n        kernel_size=3,\n        strides=1,\n        activation='relu',\n        padding='same',\n        use_bias=True,\n        kernel_initializer='glorot_uniform',\n        bias_initializer='zeros',\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        trainable=True,\n    )\n    \n    data = Input(shape=input_shape, dtype='float', name='data')\n    mvn0 = Lambda(mvn, name='mvn0')(data)\n    pad = ZeroPadding2D(padding=5, name='pad')(mvn0)\n\n    conv1 = Conv2D(filters=64, name='conv1', **kwargs)(pad)\n    mvn1 = Lambda(mvn, name='mvn1')(conv1)\n    \n    conv2 = Conv2D(filters=64, name='conv2', **kwargs)(mvn1)\n    mvn2 = Lambda(mvn, name='mvn2')(conv2)\n\n    conv3 = Conv2D(filters=64, name='conv3', **kwargs)(mvn2)\n    mvn3 = Lambda(mvn, name='mvn3')(conv3)\n    pool1 = MaxPooling2D(pool_size=3, strides=2,\n                    padding='valid', name='pool1')(mvn3)\n\n    \n    conv4 = Conv2D(filters=128, name='conv4', **kwargs)(pool1)\n    mvn4 = Lambda(mvn, name='mvn4')(conv4)\n\n    conv5 = Conv2D(filters=128, name='conv5', **kwargs)(mvn4)\n    mvn5 = Lambda(mvn, name='mvn5')(conv5)\n\n    conv6 = Conv2D(filters=128, name='conv6', **kwargs)(mvn5)\n    mvn6 = Lambda(mvn, name='mvn6')(conv6)\n\n    conv7 = Conv2D(filters=128, name='conv7', **kwargs)(mvn6)\n    mvn7 = Lambda(mvn, name='mvn7')(conv7)\n    pool2 = MaxPooling2D(pool_size=3, strides=2,\n                    padding='valid', name='pool2')(mvn7)\n\n\n    conv8 = Conv2D(filters=256, name='conv8', **kwargs)(pool2)\n    mvn8 = Lambda(mvn, name='mvn8')(conv8)\n\n    conv9 = Conv2D(filters=256, name='conv9', **kwargs)(mvn8)\n    mvn9 = Lambda(mvn, name='mvn9')(conv9)\n\n    conv10 = Conv2D(filters=256, name='conv10', **kwargs)(mvn9)\n    mvn10 = Lambda(mvn, name='mvn10')(conv10)\n\n    conv11 = Conv2D(filters=256, name='conv11', **kwargs)(mvn10)\n    mvn11 = Lambda(mvn, name='mvn11')(conv11)\n    pool3 = MaxPooling2D(pool_size=3, strides=2,\n                    padding='valid', name='pool3')(mvn11)\n    drop1 = Dropout(rate=0.5, name='drop1')(pool3)\n\n\n    conv12 = Conv2D(filters=512, name='conv12', **kwargs)(drop1)\n    mvn12 = Lambda(mvn, name='mvn12')(conv12)\n\n    conv13 = Conv2D(filters=512, name='conv13', **kwargs)(mvn12)\n    mvn13 = Lambda(mvn, name='mvn13')(conv13)\n\n    conv14 = Conv2D(filters=512, name='conv14', **kwargs)(mvn13)\n    mvn14 = Lambda(mvn, name='mvn14')(conv14)\n\n    conv15 = Conv2D(filters=512, name='conv15', **kwargs)(mvn14)\n    mvn15 = Lambda(mvn, name='mvn15')(conv15)\n    drop2 = Dropout(rate=0.5, name='drop2')(mvn15)\n\n\n    score_conv15 = Conv2D(filters=num_classes, kernel_size=1,\n                        strides=1, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=True,\n                        name='score_conv15')(drop2)\n    upsample1 = Conv2DTranspose(filters=num_classes, kernel_size=3,\n                        strides=2, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=False,\n                        name='upsample1')(score_conv15)\n    score_conv11 = Conv2D(filters=num_classes, kernel_size=1,\n                        strides=1, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=True,\n                        name='score_conv11')(mvn11)\n    crop1 = Lambda(crop, name='crop1')([upsample1, score_conv11])\n    fuse_scores1 = average([crop1, upsample1], name='fuse_scores1')\n    \n    upsample2 = Conv2DTranspose(filters=num_classes, kernel_size=3,\n                        strides=2, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=False,\n                        name='upsample2')(fuse_scores1)\n    score_conv7 = Conv2D(filters=num_classes, kernel_size=1,\n                        strides=1, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=True,\n                        name='score_conv7')(mvn7)\n    crop2 = Lambda(crop, name='crop2')([upsample2, score_conv7])\n    fuse_scores2 = average([crop2, upsample2], name='fuse_scores2')\n    \n    upsample3 = Conv2DTranspose(filters=num_classes, kernel_size=3,\n                        strides=2, activation=None, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=False,\n                        name='upsample3')(fuse_scores2)\n    crop3 = Lambda(crop, name='crop3')([data, upsample3])\n    predictions = Conv2D(filters=num_classes, kernel_size=1,\n                        strides=1, activation=activation, padding='valid',\n                        kernel_initializer='glorot_uniform', use_bias=True,\n                        name='predictions')(crop3)\n    \n    model = Model(inputs=data, outputs=predictions)\n    if weights is not None:\n        model.load_weights(weights)\n    sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n    model.compile(optimizer=sgd, loss=loss,\n                  metrics=['accuracy', dice_coef, jaccard_coef])\n\n    return model\n\n\nif __name__ == '__main__':\n    model = fcn_model((100, 100, 1), 2, weights=None)\n\n`\nSpyder execution log:\nrunfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')\nUsing TensorFlow backend.\nTraceback (most recent call last):\nFile \"\", line 1, in \nrunfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 710, in runfile\nexecfile(filename, namespace)\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 101, in execfile\nexec(compile(f.read(), filename, 'exec'), namespace)\nFile \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 197, in \nmodel = fcn_model((100, 100, 1), 2, weights=None)\nFile \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 162, in fcn_model\ncrop1 = Lambda(crop, name='crop1')([upsample1, score_conv11])\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in call\noutput = self.call(inputs, **kwargs)\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 651, in call\nreturn self.function(inputs, **arguments)\nFile \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 36, in crop\ncropped = Cropping2D(cropping=(crop_h_dims, crop_w_dims))(tensors[1])\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in call\noutput = self.call(inputs, **kwargs)\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 1874, in call\nself.cropping[1][0]: -self.cropping[1][1],\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\nname=name)\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 706, in strided_slice\nshrink_axis_mask=shrink_axis_mask)\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5429, in strided_slice\nname=name)\nFile \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 609, in _apply_op_helper\nparam_name=input_name)\n**File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 60, in _SatisfiesTypeConstraint\n\", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\nTypeError: Value passed to parameter 'begin' has DataType float32 not in list of allowed values: int32, int64**\nPlease help.", "body": "Hi, \r\n\r\n### System information\r\nWindows 10\r\nTensorFlow installed using pip. \r\nI am using CDU \r\nPython 3.6.3 (using Spyder)\r\nTensorFlow version: 1.4.0\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution:\r\nTensorFlow installed from\r\nBazel version:NA\r\nCUDA/cuDNN version:NA\r\nGPU model and memory: using cpu.\r\nExact command to reproduce: \r\nactivate myenvrionment\r\nspyder \r\n\r\n\r\nThis code was written on 0.x tensorFlow, it could be an update issue, but I can't find the outdated function. \r\n\r\n`\r\n    \r\n    from keras import optimizers\r\n    from keras.models import Model\r\n    from keras.layers import Dropout, Lambda\r\n    from keras.layers import Input, average\r\n    from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose\r\n    from keras.layers import ZeroPadding2D, Cropping2D\r\n    from keras import backend as K\r\n    \r\n    \r\n    def mvn(tensor):\r\n        '''Performs per-channel spatial mean-variance normalization.'''\r\n        epsilon = 1e-6\r\n        mean = K.mean(tensor, axis=(1,2), keepdims=True)\r\n        std = K.std(tensor, axis=(1,2), keepdims=True)\r\n        mvn = (tensor - mean) / (std + epsilon)\r\n        \r\n        return mvn\r\n    \r\n    \r\n    def crop(tensors):\r\n        '''\r\n        List of 2 tensors, the second tensor having larger spatial dimensions.\r\n        '''\r\n        h_dims, w_dims = [], []\r\n        for t in tensors:\r\n            b, h, w, d = K.get_variable_shape(t)\r\n            h_dims.append(h)\r\n            w_dims.append(w)\r\n        crop_h, crop_w = (h_dims[1] - h_dims[0]), (w_dims[1] - w_dims[0])\r\n        rem_h = crop_h % 2\r\n        rem_w = crop_w % 2\r\n        crop_h_dims = (crop_h / 2, crop_h / 2 + rem_h)\r\n        crop_w_dims = (crop_w / 2, crop_w / 2 + rem_w)\r\n        cropped = Cropping2D(cropping=(crop_h_dims, crop_w_dims))(tensors[1])\r\n        \r\n        return cropped\r\n    \r\n    \r\n    def dice_coef(y_true, y_pred, smooth=0.0):\r\n        '''Average dice coefficient per batch.'''\r\n        axes = (1,2,3)\r\n        intersection = K.sum(y_true * y_pred, axis=axes)\r\n        summation = K.sum(y_true, axis=axes) + K.sum(y_pred, axis=axes)\r\n        \r\n        return K.mean((2.0 * intersection + smooth) / (summation + smooth), axis=0)\r\n    \r\n    \r\n    def dice_coef_loss(y_true, y_pred):\r\n        return 1.0 - dice_coef(y_true, y_pred, smooth=10.0)\r\n    \r\n    \r\n    def jaccard_coef(y_true, y_pred, smooth=0.0):\r\n        '''Average jaccard coefficient per batch.'''\r\n        axes = (1,2,3)\r\n        intersection = K.sum(y_true * y_pred, axis=axes)\r\n        union = K.sum(y_true, axis=axes) + K.sum(y_pred, axis=axes) - intersection\r\n        return K.mean( (intersection + smooth) / (union + smooth), axis=0)\r\n    \r\n    \r\n    def fcn_model(input_shape, num_classes, weights=None):\r\n        ''' \"Skip\" FCN architecture similar to Long et al., 2015\r\n        https://arxiv.org/abs/1411.4038\r\n        '''\r\n        if num_classes == 2:\r\n            num_classes = 1\r\n            loss = dice_coef_loss\r\n            activation = 'sigmoid'\r\n        else:\r\n            loss = 'categorical_crossentropy'\r\n            activation = 'softmax'\r\n    \r\n        kwargs = dict(\r\n            kernel_size=3,\r\n            strides=1,\r\n            activation='relu',\r\n            padding='same',\r\n            use_bias=True,\r\n            kernel_initializer='glorot_uniform',\r\n            bias_initializer='zeros',\r\n            bias_regularizer=None,\r\n            activity_regularizer=None,\r\n            kernel_constraint=None,\r\n            bias_constraint=None,\r\n            trainable=True,\r\n        )\r\n        \r\n        data = Input(shape=input_shape, dtype='float', name='data')\r\n        mvn0 = Lambda(mvn, name='mvn0')(data)\r\n        pad = ZeroPadding2D(padding=5, name='pad')(mvn0)\r\n    \r\n        conv1 = Conv2D(filters=64, name='conv1', **kwargs)(pad)\r\n        mvn1 = Lambda(mvn, name='mvn1')(conv1)\r\n        \r\n        conv2 = Conv2D(filters=64, name='conv2', **kwargs)(mvn1)\r\n        mvn2 = Lambda(mvn, name='mvn2')(conv2)\r\n    \r\n        conv3 = Conv2D(filters=64, name='conv3', **kwargs)(mvn2)\r\n        mvn3 = Lambda(mvn, name='mvn3')(conv3)\r\n        pool1 = MaxPooling2D(pool_size=3, strides=2,\r\n                        padding='valid', name='pool1')(mvn3)\r\n    \r\n        \r\n        conv4 = Conv2D(filters=128, name='conv4', **kwargs)(pool1)\r\n        mvn4 = Lambda(mvn, name='mvn4')(conv4)\r\n    \r\n        conv5 = Conv2D(filters=128, name='conv5', **kwargs)(mvn4)\r\n        mvn5 = Lambda(mvn, name='mvn5')(conv5)\r\n    \r\n        conv6 = Conv2D(filters=128, name='conv6', **kwargs)(mvn5)\r\n        mvn6 = Lambda(mvn, name='mvn6')(conv6)\r\n    \r\n        conv7 = Conv2D(filters=128, name='conv7', **kwargs)(mvn6)\r\n        mvn7 = Lambda(mvn, name='mvn7')(conv7)\r\n        pool2 = MaxPooling2D(pool_size=3, strides=2,\r\n                        padding='valid', name='pool2')(mvn7)\r\n    \r\n    \r\n        conv8 = Conv2D(filters=256, name='conv8', **kwargs)(pool2)\r\n        mvn8 = Lambda(mvn, name='mvn8')(conv8)\r\n    \r\n        conv9 = Conv2D(filters=256, name='conv9', **kwargs)(mvn8)\r\n        mvn9 = Lambda(mvn, name='mvn9')(conv9)\r\n    \r\n        conv10 = Conv2D(filters=256, name='conv10', **kwargs)(mvn9)\r\n        mvn10 = Lambda(mvn, name='mvn10')(conv10)\r\n    \r\n        conv11 = Conv2D(filters=256, name='conv11', **kwargs)(mvn10)\r\n        mvn11 = Lambda(mvn, name='mvn11')(conv11)\r\n        pool3 = MaxPooling2D(pool_size=3, strides=2,\r\n                        padding='valid', name='pool3')(mvn11)\r\n        drop1 = Dropout(rate=0.5, name='drop1')(pool3)\r\n    \r\n    \r\n        conv12 = Conv2D(filters=512, name='conv12', **kwargs)(drop1)\r\n        mvn12 = Lambda(mvn, name='mvn12')(conv12)\r\n    \r\n        conv13 = Conv2D(filters=512, name='conv13', **kwargs)(mvn12)\r\n        mvn13 = Lambda(mvn, name='mvn13')(conv13)\r\n    \r\n        conv14 = Conv2D(filters=512, name='conv14', **kwargs)(mvn13)\r\n        mvn14 = Lambda(mvn, name='mvn14')(conv14)\r\n    \r\n        conv15 = Conv2D(filters=512, name='conv15', **kwargs)(mvn14)\r\n        mvn15 = Lambda(mvn, name='mvn15')(conv15)\r\n        drop2 = Dropout(rate=0.5, name='drop2')(mvn15)\r\n    \r\n    \r\n        score_conv15 = Conv2D(filters=num_classes, kernel_size=1,\r\n                            strides=1, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=True,\r\n                            name='score_conv15')(drop2)\r\n        upsample1 = Conv2DTranspose(filters=num_classes, kernel_size=3,\r\n                            strides=2, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=False,\r\n                            name='upsample1')(score_conv15)\r\n        score_conv11 = Conv2D(filters=num_classes, kernel_size=1,\r\n                            strides=1, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=True,\r\n                            name='score_conv11')(mvn11)\r\n        crop1 = Lambda(crop, name='crop1')([upsample1, score_conv11])\r\n        fuse_scores1 = average([crop1, upsample1], name='fuse_scores1')\r\n        \r\n        upsample2 = Conv2DTranspose(filters=num_classes, kernel_size=3,\r\n                            strides=2, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=False,\r\n                            name='upsample2')(fuse_scores1)\r\n        score_conv7 = Conv2D(filters=num_classes, kernel_size=1,\r\n                            strides=1, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=True,\r\n                            name='score_conv7')(mvn7)\r\n        crop2 = Lambda(crop, name='crop2')([upsample2, score_conv7])\r\n        fuse_scores2 = average([crop2, upsample2], name='fuse_scores2')\r\n        \r\n        upsample3 = Conv2DTranspose(filters=num_classes, kernel_size=3,\r\n                            strides=2, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=False,\r\n                            name='upsample3')(fuse_scores2)\r\n        crop3 = Lambda(crop, name='crop3')([data, upsample3])\r\n        predictions = Conv2D(filters=num_classes, kernel_size=1,\r\n                            strides=1, activation=activation, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=True,\r\n                            name='predictions')(crop3)\r\n        \r\n        model = Model(inputs=data, outputs=predictions)\r\n        if weights is not None:\r\n            model.load_weights(weights)\r\n        sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\r\n        model.compile(optimizer=sgd, loss=loss,\r\n                      metrics=['accuracy', dice_coef, jaccard_coef])\r\n    \r\n        return model\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        model = fcn_model((100, 100, 1), 2, weights=None)\r\n    \r\n\r\n`\r\n\r\n# Spyder execution log:\r\nrunfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-3-d1b60b53383b>\", line 1, in <module>\r\n    runfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 710, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 101, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 197, in <module>\r\n    model = fcn_model((100, 100, 1), 2, weights=None)\r\n\r\n  File \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 162, in fcn_model\r\n    crop1 = Lambda(crop, name='crop1')([upsample1, score_conv11])\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 651, in call\r\n    return self.function(inputs, **arguments)\r\n\r\n  File \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 36, in crop\r\n    cropped = Cropping2D(cropping=(crop_h_dims, crop_w_dims))(tensors[1])\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 1874, in call\r\n    self.cropping[1][0]: -self.cropping[1][1],\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\r\n    name=name)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 706, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5429, in strided_slice\r\n    name=name)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n\r\n  **File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n\r\n**TypeError: Value passed to parameter 'begin' has DataType float32 not in list of allowed values: int32, int64****\r\n\r\n\r\nPlease help."}