{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/368129230", "html_url": "https://github.com/tensorflow/tensorflow/issues/12659#issuecomment-368129230", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12659", "id": 368129230, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODEyOTIzMA==", "user": {"login": "smatzek", "id": 13350259, "node_id": "MDQ6VXNlcjEzMzUwMjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/13350259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smatzek", "html_url": "https://github.com/smatzek", "followers_url": "https://api.github.com/users/smatzek/followers", "following_url": "https://api.github.com/users/smatzek/following{/other_user}", "gists_url": "https://api.github.com/users/smatzek/gists{/gist_id}", "starred_url": "https://api.github.com/users/smatzek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smatzek/subscriptions", "organizations_url": "https://api.github.com/users/smatzek/orgs", "repos_url": "https://api.github.com/users/smatzek/repos", "events_url": "https://api.github.com/users/smatzek/events{/privacy}", "received_events_url": "https://api.github.com/users/smatzek/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-23T20:26:28Z", "updated_at": "2018-02-23T20:27:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"244011182\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11603\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/11603/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/11603\">#11603</a> is similar to this and might well be a dup.<br>\nThe differences seen in 11603 and here likely come down to the architecture specific implementations of Eigen's PacketMath pmax function.</p>\n<p>Per <a href=\"https://msdn.microsoft.com/en-us/library/windows/desktop/jj218760(v=vs.85).aspx#alpha_754_Deviations\" rel=\"nofollow\">IEEE-754R</a> the behavior should be:</p>\n<p>min(x,QNaN) == min(QNaN,x) == x (same for max)</p>\n<p>In practice with std::max we see:<br>\nstd::max(nan, x) = nan<br>\nstd::max(x, nan) = x</p>\n<p>In Eigen's pmax functions it differs per architecture:</p>\n<p><strong>x86 Eigen PacketMath pmax</strong><br>\nThrough Eigen <a href=\"http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1373\" rel=\"nofollow\">bug 1373</a> the x86 archs under Eigen/src/Core/arch were changed to match std::max's behavior of always returning the first argument when comparing NaNs.  Previous to this it followed the Intel SIMD intrinsics' behavior of always returning the second argument.</p>\n<p><strong>CUDA Eigen PacketMath pmax</strong><br>\nFloat and double are implemented using <a href=\"http://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__SINGLE.html#group__CUDA__MATH__SINGLE_1g6e7516db46be25c33fb26e203287f2a3\" rel=\"nofollow\">fmaxf</a> and <a href=\"http://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__DOUBLE.html#group__CUDA__MATH__DOUBLE_1g8f5b0627e6706e432728bd16cb326754\" rel=\"nofollow\">fmax</a> respectively.  Per CUDA's API doc these handle NaNs like this: \"If one argument is NaN, returns the numeric argument.\"</p>\n<p>So the Eigen CUDA impl is following IEEE-754R in this respect.</p>\n<p>Half math is a bit different and is using &gt; and &lt; operators on the respective halves.</p>\n<p><strong>ALTIVEC (used by ppc64le) Eigen PacketMath pmax</strong></p>\n<p>The pmax functions are implemented using the vec_max intrinsic which follows IEEE-754R and thus returns numeric for numeric vs NaN comparisons.</p>\n<p>So to resolve the issue of differing functionality between std::max, x86 Eigen, and CUDA/ALTIVEC Eigen a decision has to be made which behavior is the desired behavior.</p>\n<p>The changes under Eigen <a href=\"http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1373\" rel=\"nofollow\">bug 1373</a> were done because it \"makes the Eigen Tensor library properly propagate NaNs for the Relu operator, which is performance critical for neural net models.\"</p>", "body_text": "Issue #11603 is similar to this and might well be a dup.\nThe differences seen in 11603 and here likely come down to the architecture specific implementations of Eigen's PacketMath pmax function.\nPer IEEE-754R the behavior should be:\nmin(x,QNaN) == min(QNaN,x) == x (same for max)\nIn practice with std::max we see:\nstd::max(nan, x) = nan\nstd::max(x, nan) = x\nIn Eigen's pmax functions it differs per architecture:\nx86 Eigen PacketMath pmax\nThrough Eigen bug 1373 the x86 archs under Eigen/src/Core/arch were changed to match std::max's behavior of always returning the first argument when comparing NaNs.  Previous to this it followed the Intel SIMD intrinsics' behavior of always returning the second argument.\nCUDA Eigen PacketMath pmax\nFloat and double are implemented using fmaxf and fmax respectively.  Per CUDA's API doc these handle NaNs like this: \"If one argument is NaN, returns the numeric argument.\"\nSo the Eigen CUDA impl is following IEEE-754R in this respect.\nHalf math is a bit different and is using > and < operators on the respective halves.\nALTIVEC (used by ppc64le) Eigen PacketMath pmax\nThe pmax functions are implemented using the vec_max intrinsic which follows IEEE-754R and thus returns numeric for numeric vs NaN comparisons.\nSo to resolve the issue of differing functionality between std::max, x86 Eigen, and CUDA/ALTIVEC Eigen a decision has to be made which behavior is the desired behavior.\nThe changes under Eigen bug 1373 were done because it \"makes the Eigen Tensor library properly propagate NaNs for the Relu operator, which is performance critical for neural net models.\"", "body": "Issue #11603 is similar to this and might well be a dup.\r\nThe differences seen in 11603 and here likely come down to the architecture specific implementations of Eigen's PacketMath pmax function.\r\n\r\nPer [IEEE-754R](https://msdn.microsoft.com/en-us/library/windows/desktop/jj218760(v=vs.85).aspx#alpha_754_Deviations) the behavior should be:\r\n\r\nmin(x,QNaN) == min(QNaN,x) == x (same for max)\r\n\r\nIn practice with std::max we see:\r\nstd::max(nan, x) = nan\r\nstd::max(x, nan) = x\r\n\r\nIn Eigen's pmax functions it differs per architecture:\r\n\r\n**x86 Eigen PacketMath pmax**\r\nThrough Eigen [bug 1373](http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1373) the x86 archs under Eigen/src/Core/arch were changed to match std::max's behavior of always returning the first argument when comparing NaNs.  Previous to this it followed the Intel SIMD intrinsics' behavior of always returning the second argument.\r\n\r\n**CUDA Eigen PacketMath pmax**\r\nFloat and double are implemented using [fmaxf](http://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__SINGLE.html#group__CUDA__MATH__SINGLE_1g6e7516db46be25c33fb26e203287f2a3) and [fmax](http://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__DOUBLE.html#group__CUDA__MATH__DOUBLE_1g8f5b0627e6706e432728bd16cb326754) respectively.  Per CUDA's API doc these handle NaNs like this: \"If one argument is NaN, returns the numeric argument.\"\r\n\r\nSo the Eigen CUDA impl is following IEEE-754R in this respect.\r\n\r\nHalf math is a bit different and is using > and < operators on the respective halves.\r\n\r\n**ALTIVEC (used by ppc64le) Eigen PacketMath pmax**\r\n\r\nThe pmax functions are implemented using the vec_max intrinsic which follows IEEE-754R and thus returns numeric for numeric vs NaN comparisons.\r\n\r\nSo to resolve the issue of differing functionality between std::max, x86 Eigen, and CUDA/ALTIVEC Eigen a decision has to be made which behavior is the desired behavior.\r\n\r\nThe changes under Eigen [bug 1373](http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1373) were done because it \"makes the Eigen Tensor library properly propagate NaNs for the Relu operator, which is performance critical for neural net models.\""}