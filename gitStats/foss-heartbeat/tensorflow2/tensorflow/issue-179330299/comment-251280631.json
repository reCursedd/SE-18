{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/251280631", "html_url": "https://github.com/tensorflow/tensorflow/issues/4590#issuecomment-251280631", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4590", "id": 251280631, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MTI4MDYzMQ==", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-04T02:34:14Z", "updated_at": "2016-10-04T02:34:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Sure. What motivated filing this was a confusing error message and non-obvious solution with regard to <code>dynamic_rnn</code>. I was slicing a part of my input sequence (for example I needed to train on times T_0 through T_end, where T_end is computed as part of the graph) using <code>tf.slice</code>. However <code>dynamic_rnn</code> requires that the input has the last dimension defined (since that dimension is the dimension of the network input). As a result of the bug above, the output of <code>tf.slice</code> is of completely unknown shape, and so <code>dynamic_rnn</code> fails. It's possible to fix this with a <code>tf.reshape</code> with a constant dimension, but this is definitely a suboptimal situation (and non-obvious). Does that make sense?</p>", "body_text": "Sure. What motivated filing this was a confusing error message and non-obvious solution with regard to dynamic_rnn. I was slicing a part of my input sequence (for example I needed to train on times T_0 through T_end, where T_end is computed as part of the graph) using tf.slice. However dynamic_rnn requires that the input has the last dimension defined (since that dimension is the dimension of the network input). As a result of the bug above, the output of tf.slice is of completely unknown shape, and so dynamic_rnn fails. It's possible to fix this with a tf.reshape with a constant dimension, but this is definitely a suboptimal situation (and non-obvious). Does that make sense?", "body": "Sure. What motivated filing this was a confusing error message and non-obvious solution with regard to `dynamic_rnn`. I was slicing a part of my input sequence (for example I needed to train on times T_0 through T_end, where T_end is computed as part of the graph) using `tf.slice`. However `dynamic_rnn` requires that the input has the last dimension defined (since that dimension is the dimension of the network input). As a result of the bug above, the output of `tf.slice` is of completely unknown shape, and so `dynamic_rnn` fails. It's possible to fix this with a `tf.reshape` with a constant dimension, but this is definitely a suboptimal situation (and non-obvious). Does that make sense?\n"}