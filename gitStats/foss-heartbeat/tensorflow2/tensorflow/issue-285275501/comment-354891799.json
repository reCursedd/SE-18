{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354891799", "html_url": "https://github.com/tensorflow/tensorflow/issues/15760#issuecomment-354891799", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15760", "id": 354891799, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDg5MTc5OQ==", "user": {"login": "act65", "id": 6046380, "node_id": "MDQ6VXNlcjYwNDYzODA=", "avatar_url": "https://avatars3.githubusercontent.com/u/6046380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/act65", "html_url": "https://github.com/act65", "followers_url": "https://api.github.com/users/act65/followers", "following_url": "https://api.github.com/users/act65/following{/other_user}", "gists_url": "https://api.github.com/users/act65/gists{/gist_id}", "starred_url": "https://api.github.com/users/act65/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/act65/subscriptions", "organizations_url": "https://api.github.com/users/act65/orgs", "repos_url": "https://api.github.com/users/act65/repos", "events_url": "https://api.github.com/users/act65/events{/privacy}", "received_events_url": "https://api.github.com/users/act65/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-02T22:26:10Z", "updated_at": "2018-01-02T22:26:25Z", "author_association": "NONE", "body_html": "<p>I am not sure it is necessary to add another registration mechanism to allow me to write some custom aggregation ops? It also seems like a lot of work...?<br>\nCan we not just think of aggregation as some fn that takes gradients (batch x vectorised parameters) and returns the aggregated grads (1 x vectorised parameters)? So a sum is one instance of this? And we can just drop in some other fn in??</p>\n<p>Replace code from <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/gradients_impl.py\">source</a>.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> i, out_grad <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(out_grads):\n    out_grads[i] <span class=\"pl-k\">=</span> math_ops.accumulate_n(out_grad)</pre></div>\n<p>with</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> i, out_grad <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(out_grads):\n    mat <span class=\"pl-k\">=</span> tf.stack(out_grad)\n    u, s ,v <span class=\"pl-k\">=</span> tf.svd(mat)\n    out_grads[i] <span class=\"pl-k\">=</span> u[:, <span class=\"pl-c1\">0</span>]<span class=\"pl-k\">*</span>s[<span class=\"pl-c1\">0</span>]  <span class=\"pl-c\"><span class=\"pl-c\">#</span> the principle component (rather than the sum).</span></pre></div>\n<p>But, I feel like I might be missing something? I still dont understand your comment</p>\n<blockquote>\n<p>because there's no explicit aggregation step. The \"aggregation\" happens as part of backprop</p>\n</blockquote>\n<p>I though aggregation over the batch dimension only occurs at the end of the backward propagation of the gradients?!</p>", "body_text": "I am not sure it is necessary to add another registration mechanism to allow me to write some custom aggregation ops? It also seems like a lot of work...?\nCan we not just think of aggregation as some fn that takes gradients (batch x vectorised parameters) and returns the aggregated grads (1 x vectorised parameters)? So a sum is one instance of this? And we can just drop in some other fn in??\nReplace code from source.\nfor i, out_grad in enumerate(out_grads):\n    out_grads[i] = math_ops.accumulate_n(out_grad)\nwith\nfor i, out_grad in enumerate(out_grads):\n    mat = tf.stack(out_grad)\n    u, s ,v = tf.svd(mat)\n    out_grads[i] = u[:, 0]*s[0]  # the principle component (rather than the sum).\nBut, I feel like I might be missing something? I still dont understand your comment\n\nbecause there's no explicit aggregation step. The \"aggregation\" happens as part of backprop\n\nI though aggregation over the batch dimension only occurs at the end of the backward propagation of the gradients?!", "body": "I am not sure it is necessary to add another registration mechanism to allow me to write some custom aggregation ops? It also seems like a lot of work...?\r\nCan we not just think of aggregation as some fn that takes gradients (batch x vectorised parameters) and returns the aggregated grads (1 x vectorised parameters)? So a sum is one instance of this? And we can just drop in some other fn in??\r\n\r\nReplace code from [source](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/gradients_impl.py).\r\n```python\r\nfor i, out_grad in enumerate(out_grads):\r\n    out_grads[i] = math_ops.accumulate_n(out_grad)\r\n```\r\nwith \r\n```python\r\nfor i, out_grad in enumerate(out_grads):\r\n    mat = tf.stack(out_grad)\r\n    u, s ,v = tf.svd(mat)\r\n    out_grads[i] = u[:, 0]*s[0]  # the principle component (rather than the sum).\r\n```\r\n\r\nBut, I feel like I might be missing something? I still dont understand your comment\r\n\r\n>  because there's no explicit aggregation step. The \"aggregation\" happens as part of backprop\r\n\r\nI though aggregation over the batch dimension only occurs at the end of the backward propagation of the gradients?! "}