{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/258097738", "html_url": "https://github.com/tensorflow/tensorflow/issues/5372#issuecomment-258097738", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5372", "id": 258097738, "node_id": "MDEyOklzc3VlQ29tbWVudDI1ODA5NzczOA==", "user": {"login": "llhe", "id": 192829, "node_id": "MDQ6VXNlcjE5MjgyOQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/192829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/llhe", "html_url": "https://github.com/llhe", "followers_url": "https://api.github.com/users/llhe/followers", "following_url": "https://api.github.com/users/llhe/following{/other_user}", "gists_url": "https://api.github.com/users/llhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/llhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/llhe/subscriptions", "organizations_url": "https://api.github.com/users/llhe/orgs", "repos_url": "https://api.github.com/users/llhe/repos", "events_url": "https://api.github.com/users/llhe/events{/privacy}", "received_events_url": "https://api.github.com/users/llhe/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-03T09:39:23Z", "updated_at": "2016-11-03T09:39:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6997460\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jkschin\">@jkschin</a> You mentioned you have run twice, so did you use the same TFRecord file or recreate it for each run?</p>\n<p>If it's the first case, I guess you are using local disk. It might be just corruption in the IO path, like <strong>broken disk</strong>, bugs in IO controllers, non-ECC memory and bit rot etc. But this should be rare. Using checksum-ed auto recovery (by either duplication or EC code) file system like GFS, HDFS (or simply RAID) can avoid most of the problems. I think <strong>check every write to prevent such cases of corrupted data happening</strong> is not the responsibility of tensorflow.</p>\n<p>If it's the second case, it may be a bug.</p>", "body_text": "@jkschin You mentioned you have run twice, so did you use the same TFRecord file or recreate it for each run?\nIf it's the first case, I guess you are using local disk. It might be just corruption in the IO path, like broken disk, bugs in IO controllers, non-ECC memory and bit rot etc. But this should be rare. Using checksum-ed auto recovery (by either duplication or EC code) file system like GFS, HDFS (or simply RAID) can avoid most of the problems. I think check every write to prevent such cases of corrupted data happening is not the responsibility of tensorflow.\nIf it's the second case, it may be a bug.", "body": "@jkschin You mentioned you have run twice, so did you use the same TFRecord file or recreate it for each run?\n\nIf it's the first case, I guess you are using local disk. It might be just corruption in the IO path, like **broken disk**, bugs in IO controllers, non-ECC memory and bit rot etc. But this should be rare. Using checksum-ed auto recovery (by either duplication or EC code) file system like GFS, HDFS (or simply RAID) can avoid most of the problems. I think **check every write to prevent such cases of corrupted data happening** is not the responsibility of tensorflow.\n\nIf it's the second case, it may be a bug.\n"}