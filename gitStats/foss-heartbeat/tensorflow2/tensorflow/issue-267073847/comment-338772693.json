{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/338772693", "html_url": "https://github.com/tensorflow/tensorflow/issues/13847#issuecomment-338772693", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13847", "id": 338772693, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODc3MjY5Mw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-23T19:37:30Z", "updated_at": "2017-10-23T19:37:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There are a couple of ways to switch between different datasets with a single <code>Iterator</code> (search for \"reinitializable iterator\" and \"feedable iterator\" in <a href=\"https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator\" rel=\"nofollow\">this guide</a>), but most people find it easier to construct multiple graphs for these cases.</p>\n<p>The code example I showed does not create additional threads, but you can add them wherever you'd like by adding a <code>.prefetch(buffer_size)</code> transformation. Although it doesn't make much sense for the concrete example, here's how you'd add prefetching threads to the <code>a_dataset</code> and <code>b_dataset</code> computations:</p>\n<div class=\"highlight highlight-source-python\"><pre>a_dataset <span class=\"pl-k\">=</span> tf.data.Dataset.range(<span class=\"pl-c1\">30</span>).repeat().prefetch(<span class=\"pl-c1\">10</span>)\nb_dataset <span class=\"pl-k\">=</span> tf.data.Dataset.range(<span class=\"pl-c1\">30</span>).repeat().prefetch(<span class=\"pl-c1\">10</span>)\nbatched_dataset <span class=\"pl-k\">=</span> tf.data.Dataset.zip((a_dataset, b_dataset)).batch(<span class=\"pl-c1\">5</span>)</pre></div>\n<p>If you have an expensive <code>Dataset.map()</code> in your program, you can also parallelize that by adding <code>num_parallel_calls=N</code> to process <code>N</code> elements at once.</p>\n<p>All these options add parallelism without comprising on determinism. There is an explicitly \"sloppy\" transformation called <code>tf.contrib.data.sloppy_interleave()</code> that can produce non-determinstic results (but can be useful to make I/O blockages), and we've considered adding a non-deterministic parallel map (to deal with stragglers), but it's generally been more pleasant to work with deterministic APIs.</p>\n<p>By contrast, the <code>num_threads &gt; 1</code> versions of the queue-based APIs are inherently racy, because they issue multiple <code>Session.run()</code> calls in parallel, and mutate queue state (dequeuing from the source queue, enqueuing to the destination queue) without any coordination. Setting <code>num_threads = 1</code> ought to help, but it conflicts with <code>tf.train.shuffle_batch()</code>, because there can still be a race between a single producer thread and a single consumer thread when <code>capacity != min_after_dequeue</code>, leading to unrepeatable results.</p>", "body_text": "There are a couple of ways to switch between different datasets with a single Iterator (search for \"reinitializable iterator\" and \"feedable iterator\" in this guide), but most people find it easier to construct multiple graphs for these cases.\nThe code example I showed does not create additional threads, but you can add them wherever you'd like by adding a .prefetch(buffer_size) transformation. Although it doesn't make much sense for the concrete example, here's how you'd add prefetching threads to the a_dataset and b_dataset computations:\na_dataset = tf.data.Dataset.range(30).repeat().prefetch(10)\nb_dataset = tf.data.Dataset.range(30).repeat().prefetch(10)\nbatched_dataset = tf.data.Dataset.zip((a_dataset, b_dataset)).batch(5)\nIf you have an expensive Dataset.map() in your program, you can also parallelize that by adding num_parallel_calls=N to process N elements at once.\nAll these options add parallelism without comprising on determinism. There is an explicitly \"sloppy\" transformation called tf.contrib.data.sloppy_interleave() that can produce non-determinstic results (but can be useful to make I/O blockages), and we've considered adding a non-deterministic parallel map (to deal with stragglers), but it's generally been more pleasant to work with deterministic APIs.\nBy contrast, the num_threads > 1 versions of the queue-based APIs are inherently racy, because they issue multiple Session.run() calls in parallel, and mutate queue state (dequeuing from the source queue, enqueuing to the destination queue) without any coordination. Setting num_threads = 1 ought to help, but it conflicts with tf.train.shuffle_batch(), because there can still be a race between a single producer thread and a single consumer thread when capacity != min_after_dequeue, leading to unrepeatable results.", "body": "There are a couple of ways to switch between different datasets with a single `Iterator` (search for \"reinitializable iterator\" and \"feedable iterator\" in [this guide](https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator)), but most people find it easier to construct multiple graphs for these cases.\r\n\r\nThe code example I showed does not create additional threads, but you can add them wherever you'd like by adding a `.prefetch(buffer_size)` transformation. Although it doesn't make much sense for the concrete example, here's how you'd add prefetching threads to the `a_dataset` and `b_dataset` computations:\r\n\r\n```python\r\na_dataset = tf.data.Dataset.range(30).repeat().prefetch(10)\r\nb_dataset = tf.data.Dataset.range(30).repeat().prefetch(10)\r\nbatched_dataset = tf.data.Dataset.zip((a_dataset, b_dataset)).batch(5)\r\n```\r\n\r\nIf you have an expensive `Dataset.map()` in your program, you can also parallelize that by adding `num_parallel_calls=N` to process `N` elements at once.\r\n\r\nAll these options add parallelism without comprising on determinism. There is an explicitly \"sloppy\" transformation called `tf.contrib.data.sloppy_interleave()` that can produce non-determinstic results (but can be useful to make I/O blockages), and we've considered adding a non-deterministic parallel map (to deal with stragglers), but it's generally been more pleasant to work with deterministic APIs.\r\n\r\nBy contrast, the `num_threads > 1` versions of the queue-based APIs are inherently racy, because they issue multiple `Session.run()` calls in parallel, and mutate queue state (dequeuing from the source queue, enqueuing to the destination queue) without any coordination. Setting `num_threads = 1` ought to help, but it conflicts with `tf.train.shuffle_batch()`, because there can still be a race between a single producer thread and a single consumer thread when `capacity != min_after_dequeue`, leading to unrepeatable results."}