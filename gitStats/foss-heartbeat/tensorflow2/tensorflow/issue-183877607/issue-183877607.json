{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5066", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5066/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5066/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5066/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5066", "id": 183877607, "node_id": "MDU6SXNzdWUxODM4Nzc2MDc=", "number": 5066, "title": "Multiple GPU Memory Being Allocated for single device script", "user": {"login": "acrosson", "id": 4795661, "node_id": "MDQ6VXNlcjQ3OTU2NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4795661?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acrosson", "html_url": "https://github.com/acrosson", "followers_url": "https://api.github.com/users/acrosson/followers", "following_url": "https://api.github.com/users/acrosson/following{/other_user}", "gists_url": "https://api.github.com/users/acrosson/gists{/gist_id}", "starred_url": "https://api.github.com/users/acrosson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acrosson/subscriptions", "organizations_url": "https://api.github.com/users/acrosson/orgs", "repos_url": "https://api.github.com/users/acrosson/repos", "events_url": "https://api.github.com/users/acrosson/events{/privacy}", "received_events_url": "https://api.github.com/users/acrosson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2016-10-19T06:38:59Z", "updated_at": "2018-08-30T05:31:59Z", "closed_at": "2016-10-19T23:49:54Z", "author_association": "NONE", "body_html": "<p>I am unable to run a TF script on a single GPU. Both of my GTX 1080's memory are being fully absorbed by Tensorflow when the model is initialized, but only one of the GPU is being used for computations (based on what I'm seeing in nvidia-smi).</p>\n<p>Because both GPUs memory are fully occupied, I cannot run two models at once.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/4795661/19507497/4293ff2e-9588-11e6-92dd-4b79b86eaa9e.png\"><img width=\"572\" alt=\"screen shot 2016-10-18 at 11 11 38 pm\" src=\"https://cloud.githubusercontent.com/assets/4795661/19507497/4293ff2e-9588-11e6-92dd-4b79b86eaa9e.png\" style=\"max-width:100%;\"></a></p>\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n<p><a href=\"http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\" rel=\"nofollow\">http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory</a><br>\n<a href=\"https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/jw4FtKOivZE\" rel=\"nofollow\">https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/jw4FtKOivZE</a></p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nUbuntu 16.04</p>\n<p>Installed version of CUDA and cuDNN:<br>\nCuda Toolkit 8.0, cuDNN 5.1.5</p>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcudnn.so.5.1.5 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcurand.so.8.0 locally\n0.11.0rc0\n</code></pre>\n<pre><code>Build label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n</code></pre>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>I'm using the example from here: <a href=\"https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py</a></p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>CUDA_VISIBLE_DEVICES</p>\n<p>and</p>\n<p>config = tf.ConfigProto(<br>\ndevice_count = {'GPU': 1}<br>\n)</p>\n<p>sess = tf.Session(config=config)</p>\n<p>and</p>\n<p>with tf.device('/gpu:0'):<br>\n...</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>", "body_text": "I am unable to run a TF script on a single GPU. Both of my GTX 1080's memory are being fully absorbed by Tensorflow when the model is initialized, but only one of the GPU is being used for computations (based on what I'm seeing in nvidia-smi).\nBecause both GPUs memory are fully occupied, I cannot run two models at once.\n\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nhttp://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/jw4FtKOivZE\nEnvironment info\nOperating System:\nUbuntu 16.04\nInstalled version of CUDA and cuDNN:\nCuda Toolkit 8.0, cuDNN 5.1.5\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcudnn.so.5.1.5 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcurand.so.8.0 locally\n0.11.0rc0\n\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nI'm using the example from here: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\nWhat other attempted solutions have you tried?\nCUDA_VISIBLE_DEVICES\nand\nconfig = tf.ConfigProto(\ndevice_count = {'GPU': 1}\n)\nsess = tf.Session(config=config)\nand\nwith tf.device('/gpu:0'):\n...\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).", "body": "I am unable to run a TF script on a single GPU. Both of my GTX 1080's memory are being fully absorbed by Tensorflow when the model is initialized, but only one of the GPU is being used for computations (based on what I'm seeing in nvidia-smi).\n\nBecause both GPUs memory are fully occupied, I cannot run two models at once.\n\n<img width=\"572\" alt=\"screen shot 2016-10-18 at 11 11 38 pm\" src=\"https://cloud.githubusercontent.com/assets/4795661/19507497/4293ff2e-9588-11e6-92dd-4b79b86eaa9e.png\">\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttp://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/jw4FtKOivZE\n### Environment info\n\nOperating System:\nUbuntu 16.04\n\nInstalled version of CUDA and cuDNN: \nCuda Toolkit 8.0, cuDNN 5.1.5\n\n```\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcudnn.so.5.1.5 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcurand.so.8.0 locally\n0.11.0rc0\n```\n\n```\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI'm using the example from here: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\n### What other attempted solutions have you tried?\n\nCUDA_VISIBLE_DEVICES\n\nand\n\nconfig = tf.ConfigProto(\n    device_count = {'GPU': 1}\n)\n\nsess = tf.Session(config=config)\n\nand \n\nwith tf.device('/gpu:0'):\n ...\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n"}