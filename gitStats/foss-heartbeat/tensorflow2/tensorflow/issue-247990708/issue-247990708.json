{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12036", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12036/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12036/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12036/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12036", "id": 247990708, "node_id": "MDU6SXNzdWUyNDc5OTA3MDg=", "number": 12036, "title": "refresh model for spark streaming with sv = tf.train.Supervisor() under sv.managed_session()", "user": {"login": "WI-KIWI", "id": 18047253, "node_id": "MDQ6VXNlcjE4MDQ3MjUz", "avatar_url": "https://avatars1.githubusercontent.com/u/18047253?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WI-KIWI", "html_url": "https://github.com/WI-KIWI", "followers_url": "https://api.github.com/users/WI-KIWI/followers", "following_url": "https://api.github.com/users/WI-KIWI/following{/other_user}", "gists_url": "https://api.github.com/users/WI-KIWI/gists{/gist_id}", "starred_url": "https://api.github.com/users/WI-KIWI/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WI-KIWI/subscriptions", "organizations_url": "https://api.github.com/users/WI-KIWI/orgs", "repos_url": "https://api.github.com/users/WI-KIWI/repos", "events_url": "https://api.github.com/users/WI-KIWI/events{/privacy}", "received_events_url": "https://api.github.com/users/WI-KIWI/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-04T12:24:16Z", "updated_at": "2017-08-04T15:33:32Z", "closed_at": "2017-08-04T15:33:32Z", "author_association": "NONE", "body_html": "<p>System information</p>\n<p>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):<br>\nYes, I made custom distributed mnist code with spark streaming, based on <a href=\"https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/streaming/mnist_dist.py\">TensorFlowOnSpark example</a>.<br>\nOS Platform and Distribution: CentOS 7<br>\nTensorFlow installed from (source or binary): Unmodified source with HDFS enabled<br>\nTensorFlow version (use command below): 1.2.1<br>\nPython version: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)<br>\nBazel version (if compiling from source): bazel release 0.5.2<br>\nCUDA/cuDNN version: 8.0/5.1.10<br>\nGPU model and memory: NVIDIA Geforce 1070 8GB (shared by two yarn containers, each has two excuetors)</p>\n<p>The code works fine, but with --mode inference a yarn container will restore the model from logdir at the very beginning and use the same model during the whole prediction periode(based on spark logging info for restoring event).</p>\n<p>My question is, is there a way that I can reload some new trained model(same architecture) under sv.managed_session() periodically or after new model is generated? Because at the same time, the other yarn container with --mode train produced new model.</p>\n<p>Thanks for helping!</p>", "body_text": "System information\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes, I made custom distributed mnist code with spark streaming, based on TensorFlowOnSpark example.\nOS Platform and Distribution: CentOS 7\nTensorFlow installed from (source or binary): Unmodified source with HDFS enabled\nTensorFlow version (use command below): 1.2.1\nPython version: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)\nBazel version (if compiling from source): bazel release 0.5.2\nCUDA/cuDNN version: 8.0/5.1.10\nGPU model and memory: NVIDIA Geforce 1070 8GB (shared by two yarn containers, each has two excuetors)\nThe code works fine, but with --mode inference a yarn container will restore the model from logdir at the very beginning and use the same model during the whole prediction periode(based on spark logging info for restoring event).\nMy question is, is there a way that I can reload some new trained model(same architecture) under sv.managed_session() periodically or after new model is generated? Because at the same time, the other yarn container with --mode train produced new model.\nThanks for helping!", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, I made custom distributed mnist code with spark streaming, based on [TensorFlowOnSpark example](https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/streaming/mnist_dist.py).\r\nOS Platform and Distribution: CentOS 7\r\nTensorFlow installed from (source or binary): Unmodified source with HDFS enabled\r\nTensorFlow version (use command below): 1.2.1\r\nPython version: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)\r\nBazel version (if compiling from source): bazel release 0.5.2\r\nCUDA/cuDNN version: 8.0/5.1.10\r\nGPU model and memory: NVIDIA Geforce 1070 8GB (shared by two yarn containers, each has two excuetors)\r\n\r\nThe code works fine, but with --mode inference a yarn container will restore the model from logdir at the very beginning and use the same model during the whole prediction periode(based on spark logging info for restoring event).\r\n\r\nMy question is, is there a way that I can reload some new trained model(same architecture) under sv.managed_session() periodically or after new model is generated? Because at the same time, the other yarn container with --mode train produced new model.\r\n\r\nThanks for helping!"}