{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19016", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19016/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19016/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19016/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19016", "id": 319422558, "node_id": "MDU6SXNzdWUzMTk0MjI1NTg=", "number": 19016, "title": "No OpKernel was registered to support Op 'CudnnRNNParamsToCanonical' with these attrs.  Registered devices: [CPU], Registered kernels:   <no registered kernels>", "user": {"login": "praveeny1986", "id": 5249258, "node_id": "MDQ6VXNlcjUyNDkyNTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/5249258?v=4", "gravatar_id": "", "url": "https://api.github.com/users/praveeny1986", "html_url": "https://github.com/praveeny1986", "followers_url": "https://api.github.com/users/praveeny1986/followers", "following_url": "https://api.github.com/users/praveeny1986/following{/other_user}", "gists_url": "https://api.github.com/users/praveeny1986/gists{/gist_id}", "starred_url": "https://api.github.com/users/praveeny1986/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/praveeny1986/subscriptions", "organizations_url": "https://api.github.com/users/praveeny1986/orgs", "repos_url": "https://api.github.com/users/praveeny1986/repos", "events_url": "https://api.github.com/users/praveeny1986/events{/privacy}", "received_events_url": "https://api.github.com/users/praveeny1986/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2018-05-02T06:19:17Z", "updated_at": "2018-09-21T09:04:08Z", "closed_at": "2018-06-22T16:12:17Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.6.0</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.1.0/7.0</li>\n<li><strong>GPU model and memory</strong>:GTX 1080, 11GB</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nCode used to export the model on CPU/GPU:</li>\n</ul>\n<pre><code>with tf.Graph().as_default() as graph:\n        inputs, outputs = create_graph()\n\n\n        # Create a saver using variables from the above newly created graph\n        saver = tf.train.Saver(tf.global_variables())\n\n        with tf.Session() as sess:\n            # Restore the model from last checkpoints\n            ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n            saver.restore(sess, ckpt.model_checkpoint_path)\n\n            # (re-)create export directory\n            export_path = os.path.join(\n                tf.compat.as_bytes(FLAGS.export_dir),\n                tf.compat.as_bytes(str(FLAGS.export_version)))\n            if os.path.exists(export_path):\n                shutil.rmtree(export_path)\n\n            # create model builder\n            builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n\n            input_node = graph.get_tensor_by_name('input_node:0')\n            input_lengths = graph.get_tensor_by_name('input_lengths:0')\n            outputs = graph.get_tensor_by_name('output_node:0')\n\n            # create tensors info\n            predict_tensor_inputs_info = tf.saved_model.utils.build_tensor_info(input_node)\n            predict_tensor_inputs_length_info = tf.saved_model.utils.build_tensor_info(input_lengths)\n            predict_tensor_scores_info = tf.saved_model.utils.build_tensor_info(outputs)\n\n            # build prediction signature\n            prediction_signature = (\n                tf.saved_model.signature_def_utils.build_signature_def(\n                    inputs={'input': predict_tensor_inputs_info,'input_len':predict_tensor_inputs_length_info},\n                    outputs={'output': predict_tensor_scores_info},\n                    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n                )\n            )\n\n            # save the model\n            builder.add_meta_graph_and_variables(\n                sess, [tf.saved_model.tag_constants.SERVING],\n                signature_def_map={\n                    'infer': prediction_signature\n                })\n\n            builder.save()\n</code></pre>\n<h3>Describe the problem</h3>\n<p>Trained one model on GPU using CudnnLSTM (tf.contrib.cudnn_rnn.CudnnLSTM). When trying to export using saved_model API on CPU getting the below error. Export works on GPU and it works for predictions also only on GPU. We want to use the model for prediction on CPU also. So, how can we export the trained model on GPU so that it can be used on CPU ?</p>\n<h3>Error log:</h3>\n<pre><code>File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 501, in _create_saveable\n    name=\"%s_saveable\" % self.trainable_variables[0].name.split(\":\")[0])\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 262, in __init__\n    weights, biases = self._OpaqueParamsToCanonical()\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 315, in _OpaqueParamsToCanonical\n    direction=self._direction)\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/ops/gen_cudnn_rnn_ops.py\", line 769, in cudnn_rnn_params_to_canonical\n    name=name)\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNNParamsToCanonical' with these attrs.  Registered devices: [CPU], Registered kernels:\n  &lt;no registered kernels&gt;\n\n\t [[Node: CudnnRNNParamsToCanonical = CudnnRNNParamsToCanonical[T=DT_FLOAT, direction=\"bidirectional\", dropout=0, input_mode=\"linear_input\", num_params=16, rnn_mode=\"lstm\", seed=0, seed2=0, _device=\"/device:GPU:0\"](CudnnRNNParamsToCanonical/num_layers, CudnnRNNParamsToCanonical/num_units, CudnnRNNParamsToCanonical/input_size, cudnn_lstm/opaque_kernel/read)]]\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.6.0\nPython version: 2.7\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.1.0/7.0\nGPU model and memory:GTX 1080, 11GB\nExact command to reproduce:\nCode used to export the model on CPU/GPU:\n\nwith tf.Graph().as_default() as graph:\n        inputs, outputs = create_graph()\n\n\n        # Create a saver using variables from the above newly created graph\n        saver = tf.train.Saver(tf.global_variables())\n\n        with tf.Session() as sess:\n            # Restore the model from last checkpoints\n            ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n            saver.restore(sess, ckpt.model_checkpoint_path)\n\n            # (re-)create export directory\n            export_path = os.path.join(\n                tf.compat.as_bytes(FLAGS.export_dir),\n                tf.compat.as_bytes(str(FLAGS.export_version)))\n            if os.path.exists(export_path):\n                shutil.rmtree(export_path)\n\n            # create model builder\n            builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n\n            input_node = graph.get_tensor_by_name('input_node:0')\n            input_lengths = graph.get_tensor_by_name('input_lengths:0')\n            outputs = graph.get_tensor_by_name('output_node:0')\n\n            # create tensors info\n            predict_tensor_inputs_info = tf.saved_model.utils.build_tensor_info(input_node)\n            predict_tensor_inputs_length_info = tf.saved_model.utils.build_tensor_info(input_lengths)\n            predict_tensor_scores_info = tf.saved_model.utils.build_tensor_info(outputs)\n\n            # build prediction signature\n            prediction_signature = (\n                tf.saved_model.signature_def_utils.build_signature_def(\n                    inputs={'input': predict_tensor_inputs_info,'input_len':predict_tensor_inputs_length_info},\n                    outputs={'output': predict_tensor_scores_info},\n                    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n                )\n            )\n\n            # save the model\n            builder.add_meta_graph_and_variables(\n                sess, [tf.saved_model.tag_constants.SERVING],\n                signature_def_map={\n                    'infer': prediction_signature\n                })\n\n            builder.save()\n\nDescribe the problem\nTrained one model on GPU using CudnnLSTM (tf.contrib.cudnn_rnn.CudnnLSTM). When trying to export using saved_model API on CPU getting the below error. Export works on GPU and it works for predictions also only on GPU. We want to use the model for prediction on CPU also. So, how can we export the trained model on GPU so that it can be used on CPU ?\nError log:\nFile \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 501, in _create_saveable\n    name=\"%s_saveable\" % self.trainable_variables[0].name.split(\":\")[0])\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 262, in __init__\n    weights, biases = self._OpaqueParamsToCanonical()\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 315, in _OpaqueParamsToCanonical\n    direction=self._direction)\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/ops/gen_cudnn_rnn_ops.py\", line 769, in cudnn_rnn_params_to_canonical\n    name=name)\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNNParamsToCanonical' with these attrs.  Registered devices: [CPU], Registered kernels:\n  <no registered kernels>\n\n\t [[Node: CudnnRNNParamsToCanonical = CudnnRNNParamsToCanonical[T=DT_FLOAT, direction=\"bidirectional\", dropout=0, input_mode=\"linear_input\", num_params=16, rnn_mode=\"lstm\", seed=0, seed2=0, _device=\"/device:GPU:0\"](CudnnRNNParamsToCanonical/num_layers, CudnnRNNParamsToCanonical/num_units, CudnnRNNParamsToCanonical/input_size, cudnn_lstm/opaque_kernel/read)]]", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.1.0/7.0\r\n- **GPU model and memory**:GTX 1080, 11GB\r\n- **Exact command to reproduce**:\r\nCode used to export the model on CPU/GPU:\r\n```\r\nwith tf.Graph().as_default() as graph:\r\n        inputs, outputs = create_graph()\r\n\r\n\r\n        # Create a saver using variables from the above newly created graph\r\n        saver = tf.train.Saver(tf.global_variables())\r\n\r\n        with tf.Session() as sess:\r\n            # Restore the model from last checkpoints\r\n            ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\r\n            saver.restore(sess, ckpt.model_checkpoint_path)\r\n\r\n            # (re-)create export directory\r\n            export_path = os.path.join(\r\n                tf.compat.as_bytes(FLAGS.export_dir),\r\n                tf.compat.as_bytes(str(FLAGS.export_version)))\r\n            if os.path.exists(export_path):\r\n                shutil.rmtree(export_path)\r\n\r\n            # create model builder\r\n            builder = tf.saved_model.builder.SavedModelBuilder(export_path)\r\n\r\n            input_node = graph.get_tensor_by_name('input_node:0')\r\n            input_lengths = graph.get_tensor_by_name('input_lengths:0')\r\n            outputs = graph.get_tensor_by_name('output_node:0')\r\n\r\n            # create tensors info\r\n            predict_tensor_inputs_info = tf.saved_model.utils.build_tensor_info(input_node)\r\n            predict_tensor_inputs_length_info = tf.saved_model.utils.build_tensor_info(input_lengths)\r\n            predict_tensor_scores_info = tf.saved_model.utils.build_tensor_info(outputs)\r\n\r\n            # build prediction signature\r\n            prediction_signature = (\r\n                tf.saved_model.signature_def_utils.build_signature_def(\r\n                    inputs={'input': predict_tensor_inputs_info,'input_len':predict_tensor_inputs_length_info},\r\n                    outputs={'output': predict_tensor_scores_info},\r\n                    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\r\n                )\r\n            )\r\n\r\n            # save the model\r\n            builder.add_meta_graph_and_variables(\r\n                sess, [tf.saved_model.tag_constants.SERVING],\r\n                signature_def_map={\r\n                    'infer': prediction_signature\r\n                })\r\n\r\n            builder.save()\r\n```\r\n\r\n### Describe the problem\r\nTrained one model on GPU using CudnnLSTM (tf.contrib.cudnn_rnn.CudnnLSTM). When trying to export using saved_model API on CPU getting the below error. Export works on GPU and it works for predictions also only on GPU. We want to use the model for prediction on CPU also. So, how can we export the trained model on GPU so that it can be used on CPU ?\r\n\r\n### Error log:\r\n\r\n```\r\nFile \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 501, in _create_saveable\r\n    name=\"%s_saveable\" % self.trainable_variables[0].name.split(\":\")[0])\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 262, in __init__\r\n    weights, biases = self._OpaqueParamsToCanonical()\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 315, in _OpaqueParamsToCanonical\r\n    direction=self._direction)\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/ops/gen_cudnn_rnn_ops.py\", line 769, in cudnn_rnn_params_to_canonical\r\n    name=name)\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/home/deepak/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNNParamsToCanonical' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: CudnnRNNParamsToCanonical = CudnnRNNParamsToCanonical[T=DT_FLOAT, direction=\"bidirectional\", dropout=0, input_mode=\"linear_input\", num_params=16, rnn_mode=\"lstm\", seed=0, seed2=0, _device=\"/device:GPU:0\"](CudnnRNNParamsToCanonical/num_layers, CudnnRNNParamsToCanonical/num_units, CudnnRNNParamsToCanonical/input_size, cudnn_lstm/opaque_kernel/read)]]\r\n```"}