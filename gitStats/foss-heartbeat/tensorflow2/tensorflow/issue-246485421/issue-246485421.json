{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11863", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11863/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11863/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11863/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11863", "id": 246485421, "node_id": "MDU6SXNzdWUyNDY0ODU0MjE=", "number": 11863, "title": "AbortionError(code=StatusCode.NOT_FOUND, details=\"FeedInputs: unable to find feed output ToFloat:0\")", "user": {"login": "suryaprakaz", "id": 7610546, "node_id": "MDQ6VXNlcjc2MTA1NDY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7610546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suryaprakaz", "html_url": "https://github.com/suryaprakaz", "followers_url": "https://api.github.com/users/suryaprakaz/followers", "following_url": "https://api.github.com/users/suryaprakaz/following{/other_user}", "gists_url": "https://api.github.com/users/suryaprakaz/gists{/gist_id}", "starred_url": "https://api.github.com/users/suryaprakaz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suryaprakaz/subscriptions", "organizations_url": "https://api.github.com/users/suryaprakaz/orgs", "repos_url": "https://api.github.com/users/suryaprakaz/repos", "events_url": "https://api.github.com/users/suryaprakaz/events{/privacy}", "received_events_url": "https://api.github.com/users/suryaprakaz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-29T00:12:04Z", "updated_at": "2017-07-29T09:05:29Z", "closed_at": "2017-07-29T09:05:29Z", "author_association": "NONE", "body_html": "<p>Hello<br>\nI am trying to host the <em>ssd_mobilenet_v1_coco</em> model from the Tensorflow Object Detection API model zoo with Tensorflow Serving.</p>\n<p>I was able to successfully export the model with the exporter script in <code>models/object_detection/exporter.py</code> as a SavedModel. The issue arises when I tried to run the modified client</p>\n<pre><code>from __future__ import print_function\nfrom grpc.beta import implementations\nimport tensorflow as tf\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\n\ntf.app.flags.DEFINE_string('server', 'localhost:9000',\n                           'PredictionService host:port')\ntf.app.flags.DEFINE_string('image', '', 'path to image in JPEG format')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  host, port = FLAGS.server.split(':')\n  channel = implementations.insecure_channel(host, int(port))\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  with open(FLAGS.image, 'rb') as f:\n    data = f.read()\n    request = predict_pb2.PredictRequest()\n    request.model_spec.name = 'mobilenet_v1'\n    request.model_spec.signature_name = 'serving_default'\n    request.inputs['inputs'].CopyFrom(\n        tf.contrib.util.make_tensor_proto(data, shape=[1]))\n    result = stub.Predict(request, 10.0)  # 10 secs timeout\n    print(result)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n</code></pre>\n<p>Here's the traceback for the gRPC request</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 56, in &lt;module&gt;\n    tf.app.run()\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 51, in main\n    result = stub.Predict(request, 10.0)  # 10 secs timeout\n  File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 324, in __call__\n    self._request_serializer, self._response_deserializer)\n  File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 210, in _blocking_unary_unary\n    raise _abortion_error(rpc_error_call)\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.NOT_FOUND, details=\"FeedInputs: unable to find feed output ToFloat:0\")\n</code></pre>\n<p>I am using this <a href=\"https://github.com/tensorflow/tensorflow/tree/16d39e94e3724417fcaed87035434e098e892842\">commit</a> of tensorflow to build the model server which is the default submodule of tensorflow serving</p>", "body_text": "Hello\nI am trying to host the ssd_mobilenet_v1_coco model from the Tensorflow Object Detection API model zoo with Tensorflow Serving.\nI was able to successfully export the model with the exporter script in models/object_detection/exporter.py as a SavedModel. The issue arises when I tried to run the modified client\nfrom __future__ import print_function\nfrom grpc.beta import implementations\nimport tensorflow as tf\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\n\n\ntf.app.flags.DEFINE_string('server', 'localhost:9000',\n                           'PredictionService host:port')\ntf.app.flags.DEFINE_string('image', '', 'path to image in JPEG format')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  host, port = FLAGS.server.split(':')\n  channel = implementations.insecure_channel(host, int(port))\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  with open(FLAGS.image, 'rb') as f:\n    data = f.read()\n    request = predict_pb2.PredictRequest()\n    request.model_spec.name = 'mobilenet_v1'\n    request.model_spec.signature_name = 'serving_default'\n    request.inputs['inputs'].CopyFrom(\n        tf.contrib.util.make_tensor_proto(data, shape=[1]))\n    result = stub.Predict(request, 10.0)  # 10 secs timeout\n    print(result)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n\nHere's the traceback for the gRPC request\nTraceback (most recent call last):\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 56, in <module>\n    tf.app.run()\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 51, in main\n    result = stub.Predict(request, 10.0)  # 10 secs timeout\n  File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 324, in __call__\n    self._request_serializer, self._response_deserializer)\n  File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 210, in _blocking_unary_unary\n    raise _abortion_error(rpc_error_call)\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.NOT_FOUND, details=\"FeedInputs: unable to find feed output ToFloat:0\")\n\nI am using this commit of tensorflow to build the model server which is the default submodule of tensorflow serving", "body": "Hello\r\nI am trying to host the *ssd_mobilenet_v1_coco* model from the Tensorflow Object Detection API model zoo with Tensorflow Serving. \r\n\r\nI was able to successfully export the model with the exporter script in `models/object_detection/exporter.py` as a SavedModel. The issue arises when I tried to run the modified client\r\n\r\n```\r\nfrom __future__ import print_function\r\nfrom grpc.beta import implementations\r\nimport tensorflow as tf\r\nfrom tensorflow_serving.apis import predict_pb2\r\nfrom tensorflow_serving.apis import prediction_service_pb2\r\n\r\n\r\ntf.app.flags.DEFINE_string('server', 'localhost:9000',\r\n                           'PredictionService host:port')\r\ntf.app.flags.DEFINE_string('image', '', 'path to image in JPEG format')\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n\r\ndef main(_):\r\n  host, port = FLAGS.server.split(':')\r\n  channel = implementations.insecure_channel(host, int(port))\r\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n  with open(FLAGS.image, 'rb') as f:\r\n    data = f.read()\r\n    request = predict_pb2.PredictRequest()\r\n    request.model_spec.name = 'mobilenet_v1'\r\n    request.model_spec.signature_name = 'serving_default'\r\n    request.inputs['inputs'].CopyFrom(\r\n        tf.contrib.util.make_tensor_proto(data, shape=[1]))\r\n    result = stub.Predict(request, 10.0)  # 10 secs timeout\r\n    print(result)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n\r\nHere's the traceback for the gRPC request\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 56, in <module>\r\n    tf.app.run()\r\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 51, in main\r\n    result = stub.Predict(request, 10.0)  # 10 secs timeout\r\n  File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 324, in __call__\r\n    self._request_serializer, self._response_deserializer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 210, in _blocking_unary_unary\r\n    raise _abortion_error(rpc_error_call)\r\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.NOT_FOUND, details=\"FeedInputs: unable to find feed output ToFloat:0\")\r\n```\r\n\r\nI am using this [commit](https://github.com/tensorflow/tensorflow/tree/16d39e94e3724417fcaed87035434e098e892842) of tensorflow to build the model server which is the default submodule of tensorflow serving"}