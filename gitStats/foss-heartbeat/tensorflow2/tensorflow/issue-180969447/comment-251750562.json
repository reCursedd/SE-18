{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/251750562", "html_url": "https://github.com/tensorflow/tensorflow/issues/4762#issuecomment-251750562", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4762", "id": 251750562, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MTc1MDU2Mg==", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-05T17:59:01Z", "updated_at": "2016-10-05T17:59:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2342391\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuanbyu\">@yuanbyu</a> Thanks, good to hear that a doc is being prepared.</p>\n<p>So, here's what I understand so far. Perhaps you can correct any misunderstandings and also this will serve as a useful resource for the time being for anyone else who runs into this.</p>\n<ol>\n<li><code>tf.while_loop</code> accepts a list of loop variables, a function mapping loop variables to a boolean, and a function mapping loop variables to a new set of loop variables.</li>\n<li>Internally, this is represented using the special nodes Enter, Exit, NextIteration, Switch, and Merge. Enter, Exit, NextIteration are all semantically equivalent to identity ops (they just forward their input to their output, potentially as a reference), but the fact that they have type Enter, Exit, NextIteration is used by the executor to handle them in a special way. The graph is constructed as follows:\n<ul>\n<li>The loop variables are sent through \"Enter\" nodes.</li>\n<li>The Enter node results are then given to \"Merge\"  nodes. During the graph construction, the inputs to the \"Merge\"  nodes are two copies of each enter node; when the NextIteration nodes are constructed, the Merge nodes are fixed by replacing one of the Enter inputs with a NextIteration input. In this way, every Merge node (one per variable) gets an input from its respective variable's Enter and NextIteration nodes.</li>\n<li>The output of the Merge nodes is passed to the condition function, which takes them and outputs a boolean. This boolean is passed to a <code>LoopCond</code> node. This boolean, as well as the output of the Merge nodes, is passed to Switch nodes, again one per variable. The Switch nodes output a dead tensor to one of their outputs and a live tensor (the merge node output) to the other one, depending on the boolean.</li>\n<li>The output of the Switch node is sent to an Exit node (one per variable) or to an Identity op (one per variable), depending on whether the loop condition is false.</li>\n<li>The identity op output is given to the loop body, and the outputs of the loop body are fed to NextIteration ops; these ops are the ones patched back in as inputs to the Merge nodes.</li>\n</ul>\n</li>\n<li>The executor has special support for these five primitive ops which make this structure into a loop:\n<ul>\n<li>The executor has a concept of a Frame, which is essentially the current iteration of the innermost loop. A frame has state, where all the input and output tensors are stored in a flat vector; each op writes its outputs to a subset of the output vector and gets inputs from a subset of the input vectors; thus, the inputs and outputs of an op can be obtained by just going to the right offset in this vector of Entry values.</li>\n</ul>\n</li>\n<li>A new frame is created when the executor sees an Enter node. A frame is removed when it sees an Exit node. The next iteration of the frame is progressed to when it sees a NextIteration node.</li>\n<li>When it sees a NextIteration node, it finds the child of that node (namely the Merge op) and calls <code>ActivateNode</code> on it, in order to continue the loop. Since nodes are not marked ready until all their inputs are non-dead, the nodes that get dead inputs from Switch (e.g. the loop is done) will not get run again.</li>\n<li>For every loop during forward propagation, a few things have to happen to create the backprop gradient graph:\n<ul>\n<li>First of all, a loop is added to the forward propagation which counts the number of iterations. More accurately, the original loop is added to; this works because of the way the primitive ops are designed. This loop starts with a <code>f_count</code> Enter node and is created in <code>control_flow_ops.py</code> <code>AddForwardLoopCounter</code>.</li>\n<li>A <code>history_map</code> is maintained of tensors produced during forward prop, and whenever the backprop needs a tensor from the forward prop, a stack is introduced, and the forward prop has a <code>StackPush</code> added to it, while the backprop has a <code>StackPop</code> added to it that pops from the same stack. In that manner, the forward prop pushes anything the backprop will need onto a stack, and the backprop slowly consumes that stack.</li>\n</ul>\n</li>\n</ol>\n<p>The description above is not quite complete but I think I probably understand enough for what I want to do.</p>\n<p>Questions:</p>\n<ol>\n<li>Why is there a <code>LoopCond</code> node? Why not pass the output of the condition directly to <code>Switch</code>?</li>\n<li>What was the motivation for such a seemingly complicated set of primitive ops? It seems like it's possible to build other control structures on top of them \u2013 is that the goal? Were these primitive ops chosen because they make it possible to implement the fairly complex gradient loop generation?</li>\n<li>What is an example usecase for <code>parallel_iterations</code>? (This is a simple question which might make sense to add to the <code>tf.while_loop</code> docs)</li>\n</ol>", "body_text": "@yuanbyu Thanks, good to hear that a doc is being prepared.\nSo, here's what I understand so far. Perhaps you can correct any misunderstandings and also this will serve as a useful resource for the time being for anyone else who runs into this.\n\ntf.while_loop accepts a list of loop variables, a function mapping loop variables to a boolean, and a function mapping loop variables to a new set of loop variables.\nInternally, this is represented using the special nodes Enter, Exit, NextIteration, Switch, and Merge. Enter, Exit, NextIteration are all semantically equivalent to identity ops (they just forward their input to their output, potentially as a reference), but the fact that they have type Enter, Exit, NextIteration is used by the executor to handle them in a special way. The graph is constructed as follows:\n\nThe loop variables are sent through \"Enter\" nodes.\nThe Enter node results are then given to \"Merge\"  nodes. During the graph construction, the inputs to the \"Merge\"  nodes are two copies of each enter node; when the NextIteration nodes are constructed, the Merge nodes are fixed by replacing one of the Enter inputs with a NextIteration input. In this way, every Merge node (one per variable) gets an input from its respective variable's Enter and NextIteration nodes.\nThe output of the Merge nodes is passed to the condition function, which takes them and outputs a boolean. This boolean is passed to a LoopCond node. This boolean, as well as the output of the Merge nodes, is passed to Switch nodes, again one per variable. The Switch nodes output a dead tensor to one of their outputs and a live tensor (the merge node output) to the other one, depending on the boolean.\nThe output of the Switch node is sent to an Exit node (one per variable) or to an Identity op (one per variable), depending on whether the loop condition is false.\nThe identity op output is given to the loop body, and the outputs of the loop body are fed to NextIteration ops; these ops are the ones patched back in as inputs to the Merge nodes.\n\n\nThe executor has special support for these five primitive ops which make this structure into a loop:\n\nThe executor has a concept of a Frame, which is essentially the current iteration of the innermost loop. A frame has state, where all the input and output tensors are stored in a flat vector; each op writes its outputs to a subset of the output vector and gets inputs from a subset of the input vectors; thus, the inputs and outputs of an op can be obtained by just going to the right offset in this vector of Entry values.\n\n\nA new frame is created when the executor sees an Enter node. A frame is removed when it sees an Exit node. The next iteration of the frame is progressed to when it sees a NextIteration node.\nWhen it sees a NextIteration node, it finds the child of that node (namely the Merge op) and calls ActivateNode on it, in order to continue the loop. Since nodes are not marked ready until all their inputs are non-dead, the nodes that get dead inputs from Switch (e.g. the loop is done) will not get run again.\nFor every loop during forward propagation, a few things have to happen to create the backprop gradient graph:\n\nFirst of all, a loop is added to the forward propagation which counts the number of iterations. More accurately, the original loop is added to; this works because of the way the primitive ops are designed. This loop starts with a f_count Enter node and is created in control_flow_ops.py AddForwardLoopCounter.\nA history_map is maintained of tensors produced during forward prop, and whenever the backprop needs a tensor from the forward prop, a stack is introduced, and the forward prop has a StackPush added to it, while the backprop has a StackPop added to it that pops from the same stack. In that manner, the forward prop pushes anything the backprop will need onto a stack, and the backprop slowly consumes that stack.\n\n\n\nThe description above is not quite complete but I think I probably understand enough for what I want to do.\nQuestions:\n\nWhy is there a LoopCond node? Why not pass the output of the condition directly to Switch?\nWhat was the motivation for such a seemingly complicated set of primitive ops? It seems like it's possible to build other control structures on top of them \u2013 is that the goal? Were these primitive ops chosen because they make it possible to implement the fairly complex gradient loop generation?\nWhat is an example usecase for parallel_iterations? (This is a simple question which might make sense to add to the tf.while_loop docs)", "body": "@yuanbyu Thanks, good to hear that a doc is being prepared.\n\nSo, here's what I understand so far. Perhaps you can correct any misunderstandings and also this will serve as a useful resource for the time being for anyone else who runs into this.\n1. `tf.while_loop` accepts a list of loop variables, a function mapping loop variables to a boolean, and a function mapping loop variables to a new set of loop variables.\n2. Internally, this is represented using the special nodes Enter, Exit, NextIteration, Switch, and Merge. Enter, Exit, NextIteration are all semantically equivalent to identity ops (they just forward their input to their output, potentially as a reference), but the fact that they have type Enter, Exit, NextIteration is used by the executor to handle them in a special way. The graph is constructed as follows:\n   - The loop variables are sent through \"Enter\" nodes.\n   - The Enter node results are then given to \"Merge\"  nodes. During the graph construction, the inputs to the \"Merge\"  nodes are two copies of each enter node; when the NextIteration nodes are constructed, the Merge nodes are fixed by replacing one of the Enter inputs with a NextIteration input. In this way, every Merge node (one per variable) gets an input from its respective variable's Enter and NextIteration nodes.\n   - The output of the Merge nodes is passed to the condition function, which takes them and outputs a boolean. This boolean is passed to a `LoopCond` node. This boolean, as well as the output of the Merge nodes, is passed to Switch nodes, again one per variable. The Switch nodes output a dead tensor to one of their outputs and a live tensor (the merge node output) to the other one, depending on the boolean.\n   - The output of the Switch node is sent to an Exit node (one per variable) or to an Identity op (one per variable), depending on whether the loop condition is false.\n   - The identity op output is given to the loop body, and the outputs of the loop body are fed to NextIteration ops; these ops are the ones patched back in as inputs to the Merge nodes.\n3. The executor has special support for these five primitive ops which make this structure into a loop:\n   - The executor has a concept of a Frame, which is essentially the current iteration of the innermost loop. A frame has state, where all the input and output tensors are stored in a flat vector; each op writes its outputs to a subset of the output vector and gets inputs from a subset of the input vectors; thus, the inputs and outputs of an op can be obtained by just going to the right offset in this vector of Entry values.\n4. A new frame is created when the executor sees an Enter node. A frame is removed when it sees an Exit node. The next iteration of the frame is progressed to when it sees a NextIteration node.\n5. When it sees a NextIteration node, it finds the child of that node (namely the Merge op) and calls `ActivateNode` on it, in order to continue the loop. Since nodes are not marked ready until all their inputs are non-dead, the nodes that get dead inputs from Switch (e.g. the loop is done) will not get run again.\n6. For every loop during forward propagation, a few things have to happen to create the backprop gradient graph:\n   - First of all, a loop is added to the forward propagation which counts the number of iterations. More accurately, the original loop is added to; this works because of the way the primitive ops are designed. This loop starts with a `f_count` Enter node and is created in `control_flow_ops.py` `AddForwardLoopCounter`.\n   - A `history_map` is maintained of tensors produced during forward prop, and whenever the backprop needs a tensor from the forward prop, a stack is introduced, and the forward prop has a `StackPush` added to it, while the backprop has a `StackPop` added to it that pops from the same stack. In that manner, the forward prop pushes anything the backprop will need onto a stack, and the backprop slowly consumes that stack.\n\nThe description above is not quite complete but I think I probably understand enough for what I want to do.\n\nQuestions:\n1. Why is there a `LoopCond` node? Why not pass the output of the condition directly to `Switch`?\n2. What was the motivation for such a seemingly complicated set of primitive ops? It seems like it's possible to build other control structures on top of them \u2013 is that the goal? Were these primitive ops chosen because they make it possible to implement the fairly complex gradient loop generation?\n3. What is an example usecase for `parallel_iterations`? (This is a simple question which might make sense to add to the `tf.while_loop` docs)\n"}