{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266252811", "html_url": "https://github.com/tensorflow/tensorflow/issues/4762#issuecomment-266252811", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4762", "id": 266252811, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjI1MjgxMQ==", "user": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-11T00:41:17Z", "updated_at": "2016-12-11T00:41:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I have written a doc and would be happy to share it with you.</p>\n<p>For your questions:</p>\n<ol>\n<li>\n<p>LoopCond is just used as a unique marker so we could understand the graph structure in later stages of graph rewriting.  For example, rewriting the graph to support distributed execution of a loop.</p>\n</li>\n<li>\n<p>Yes, that was one of the main design considerations. If you are familiar with the dataflow machines invented in the 70s, you would not be surprised by the choice of the primitives. :-) The other main considerations are non-strictness, automatic differentiation, and parallel and distributed execution of both forward and backprop.</p>\n</li>\n<li>\n<p>A simple example is tf.map.  And it is one of the main reasons that the performance of dynamic_rnn can be as good as static unrolling. For example it allows the dynamic unrolling logic runs on CPU and the actual computation runs on GPU, completely in parallel.</p>\n</li>\n</ol>", "body_text": "I have written a doc and would be happy to share it with you.\nFor your questions:\n\n\nLoopCond is just used as a unique marker so we could understand the graph structure in later stages of graph rewriting.  For example, rewriting the graph to support distributed execution of a loop.\n\n\nYes, that was one of the main design considerations. If you are familiar with the dataflow machines invented in the 70s, you would not be surprised by the choice of the primitives. :-) The other main considerations are non-strictness, automatic differentiation, and parallel and distributed execution of both forward and backprop.\n\n\nA simple example is tf.map.  And it is one of the main reasons that the performance of dynamic_rnn can be as good as static unrolling. For example it allows the dynamic unrolling logic runs on CPU and the actual computation runs on GPU, completely in parallel.", "body": "I have written a doc and would be happy to share it with you.\r\n\r\nFor your questions:\r\n\r\n1. LoopCond is just used as a unique marker so we could understand the graph structure in later stages of graph rewriting.  For example, rewriting the graph to support distributed execution of a loop.\r\n\r\n2. Yes, that was one of the main design considerations. If you are familiar with the dataflow machines invented in the 70s, you would not be surprised by the choice of the primitives. :-) The other main considerations are non-strictness, automatic differentiation, and parallel and distributed execution of both forward and backprop.\r\n\r\n3. A simple example is tf.map.  And it is one of the main reasons that the performance of dynamic_rnn can be as good as static unrolling. For example it allows the dynamic unrolling logic runs on CPU and the actual computation runs on GPU, completely in parallel."}