{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/307609196", "html_url": "https://github.com/tensorflow/tensorflow/issues/10597#issuecomment-307609196", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10597", "id": 307609196, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzYwOTE5Ng==", "user": {"login": "Scitator", "id": 7606451, "node_id": "MDQ6VXNlcjc2MDY0NTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7606451?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Scitator", "html_url": "https://github.com/Scitator", "followers_url": "https://api.github.com/users/Scitator/followers", "following_url": "https://api.github.com/users/Scitator/following{/other_user}", "gists_url": "https://api.github.com/users/Scitator/gists{/gist_id}", "starred_url": "https://api.github.com/users/Scitator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Scitator/subscriptions", "organizations_url": "https://api.github.com/users/Scitator/orgs", "repos_url": "https://api.github.com/users/Scitator/repos", "events_url": "https://api.github.com/users/Scitator/events{/privacy}", "received_events_url": "https://api.github.com/users/Scitator/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-11T06:12:02Z", "updated_at": "2017-06-11T06:12:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>First of all, I always use grad clipping to prevent such problem, and come on, it's typical conv net for mnist :)<br>\nAnd secondly, I already done some debugging before reporting an issue:<br>\nDifferent combinations of <code>queue_capacity</code>, <code>num_epochs</code>, <code>shuffle</code> have no affect.<br>\n<code>sparse_softmax_cross_entropy_with_logits</code> also works correct (I tried with just <code>softmax_cross_entropy_with_logits</code> - still have such issue).<br>\nPython generator work pretty well, without any exceptions, <em>but</em> it still strange, that loss always explode at the end of first iteration over dataset.</p>\n<p>That's why I think, that something go wrong with multi-threading here. Nevertheless, you can always check notebook and run it.</p>", "body_text": "First of all, I always use grad clipping to prevent such problem, and come on, it's typical conv net for mnist :)\nAnd secondly, I already done some debugging before reporting an issue:\nDifferent combinations of queue_capacity, num_epochs, shuffle have no affect.\nsparse_softmax_cross_entropy_with_logits also works correct (I tried with just softmax_cross_entropy_with_logits - still have such issue).\nPython generator work pretty well, without any exceptions, but it still strange, that loss always explode at the end of first iteration over dataset.\nThat's why I think, that something go wrong with multi-threading here. Nevertheless, you can always check notebook and run it.", "body": "First of all, I always use grad clipping to prevent such problem, and come on, it's typical conv net for mnist :)\r\nAnd secondly, I already done some debugging before reporting an issue:\r\nDifferent combinations of `queue_capacity`, `num_epochs`, `shuffle` have no affect.\r\n`sparse_softmax_cross_entropy_with_logits` also works correct (I tried with just `softmax_cross_entropy_with_logits` - still have such issue).\r\nPython generator work pretty well, without any exceptions, *but* it still strange, that loss always explode at the end of first iteration over dataset.\r\n\r\nThat's why I think, that something go wrong with multi-threading here. Nevertheless, you can always check notebook and run it."}