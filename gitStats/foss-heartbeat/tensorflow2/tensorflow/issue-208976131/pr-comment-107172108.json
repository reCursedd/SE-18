{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107172108", "pull_request_review_id": 28127365, "id": 107172108, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNzE3MjEwOA==", "diff_hunk": "@@ -260,10 +288,206 @@ class GrpcRemoteWorker : public WorkerInterface {\n   TF_DISALLOW_COPY_AND_ASSIGN(GrpcRemoteWorker);\n };\n \n+#if USE_MPI\n+\n+class RDMARemoteWorker : public GrpcRemoteWorker {\n+\n+ private:\n+  // Returns the name of the desitnation specified in a rendezvous key\n+  // For idx=0 it is the source, for idx=2 it is the destination\n+  string GetWorkerName(const std::string& key, const int idx) {\n+    // Convert the key back to the subpieces\n+    const std::vector<string> num_strings = str_util::Split(key, ';');\n+    // Sanity check, should be 5 src;id;dst;name;frame_iter\n+    assert(num_strings.size() == 5);\n+    // Strip the device eg /cpu:0 to get the worker name\n+    return num_strings[idx].substr(0, num_strings[idx].find_last_of('/'));\n+  }\n+  string SourceWorker(const std::string& key) { return GetWorkerName(key, 0); }\n+  string DestinationWorker(const std::string& key) {\n+    return GetWorkerName(key, 2);\n+  }\n+\n+  const int getMPIPartnerID(const bool targetIsSource, std::string key,\n+                            const WorkerEnv* env) {\n+    // Convert the grpc-name to MPI process ID\n+    std::string name;\n+    if (targetIsSource)\n+      name = SourceWorker(key);\n+    else\n+      name = DestinationWorker(key);\n+    auto it = env->worker_name_MPI_idx.find(name);\n+    if (it == env->worker_name_MPI_idx.end()) {\n+      LOG(FATAL) << \"Failed to convert worker name to MPI index: \" << name;\n+    }\n+    return it->second;\n+  }\n+\n+  const int tensorHash(const std::string key) {\n+    const uint32 hash32 = Hash32(key.data(), key.size(), 20161211);\n+    const int tensorKeyHash = std::abs(static_cast<int>(hash32));\n+    return (tensorKeyHash % maxMessageTag);\n+  }\n+\n+  void MPISendTensor(const int dst, const int hash, const bool is_dead,\n+                     const Tensor& val) {\n+#if 1\n+    // Send a header using the 'hash', followed by a message identified by\n+    // the followUpTag, which is unique to this thread\n+    const int followUpTag =\n+        (std::abs(static_cast<int>(pthread_self()))) % maxMessageTag;\n+\n+    // Encode the properties of the tensor and send this to the destination\n+    RecvTensorResponse response;\n+    response.set_is_dead(is_dead);\n+    response.set_send_start_micros(Env::Default()->NowMicros());\n+    response.mutable_tensor()->set_dtype(val.dtype());\n+    val.shape().AsProto(response.mutable_tensor()->mutable_tensor_shape());\n+    std::vector<char> respBuff(response.ByteSize() + sizeof(int));\n+    response.SerializeToArray(&respBuff[4], respBuff.size() - sizeof(int));\n+\n+    // fprintf(stderr, \"JBDBG Going to send the following message hash: %d tag:\n+    // %d\\n\", hash, followUpTag);\n+\n+    ((int*)(&respBuff[0]))[0] = followUpTag;\n+\n+    MPICheck(MPI_Send(&respBuff[0], respBuff.size(), MPI_BYTE, dst, hash,\n+                      MPI_COMM_WORLD));\n+\n+    // Next transfer the actual Tensor data\n+    const size_t nBytes = val.TotalBytes();\n+    const char* data = val.tensor_data().data();\n+\n+    // frintf(stderr, \"JBDBG Going to send this data: %p bytes: %d hash: %d tag:\n+    // %d\\n\", data, nBytes, hash, followUpTag);\n+\n+    // Transfer the Tensor content, in maxSize chuncks\n+    for (size_t i = 0; i <= nBytes; i += maxSize) {\n+      // Use MPI_Ssend to prevent that the function returns before the message\n+      // has arrived. This would cause inconsistencies in the execution and\n+      // crashes.\n+      const int toSend = std::min(maxSize, nBytes - i);\n+      MPICheck(MPI_Ssend(&data[i], toSend, MPI_BYTE, dst, followUpTag,\n+                         MPI_COMM_WORLD));\n+    }\n+#endif\n+  }  // MPISendTensor\n+\n+  void MPIRecvTensor(const int src, const int hash, TensorResponse* response) {\n+#if 1\n+    MPI_Status status;\n+    MPI_Message msg;\n+\n+    // Receive the header message, probe as size is variable\n+    int incSize;\n+    MPICheck(MPI_Mprobe(src, hash, MPI_COMM_WORLD, &msg, &status));\n+    MPICheck(MPI_Get_count(&status, MPI_CHAR, &incSize));\n+    std::vector<char> sta(incSize);\n+    MPICheck(MPI_Mrecv(&sta[0], incSize, MPI_CHAR, &msg, &status));\n+\n+    const int followUpTag = *((int*)(&sta[0]));\n+    RecvTensorResponse RTresponse;\n+    // std::cerr << response->DebugString() << std::endl;\n+    RTresponse.ParseFromArray(&sta[4], sta.size() - sizeof(int));\n+    // std::cerr << RTresponse.DebugString() << std::endl;\n+\n+    // Initialize the destination tensor\n+    // TODO is it possible that this tensor already exists/reuse of memory\n+    // location?\n+    response->InitPartial(RTresponse);\n+    const size_t nBytes = response->tensor().TotalBytes();\n+\n+    // Receive the Tensor content\n+    char* data = const_cast<char*>(response->tensor().tensor_data().data());\n+    for (size_t i = 0; i <= nBytes; i += maxSize) {\n+      const int toRecv = std::min(maxSize, nBytes - i);\n+      MPICheck(MPI_Recv(&data[i], toRecv, MPI_BYTE, src, followUpTag,\n+                        MPI_COMM_WORLD, &status));\n+    }\n+#endif\n+  }  // MPIRecvTensor\n+\n+  // Different MPI implementations use different values for allowed tag ranges\n+  // so retrieve the max value to generate valid tags\n+  int maxMessageTag;\n+\n+  // Max size of the data chuncks, 512MB\n+  const size_t maxSize = 1024 * 1024 * 512;\n+\n+  // If the MPI_PATH_DISABLED variable is set then disable this code path\n+  bool disabledAtRuntime = false;\n+\n+ public:\n+  explicit RDMARemoteWorker(SharedGrpcChannelPtr channel,\n+                            ::grpc::CompletionQueue* completion_queue,\n+                            WorkerCacheLogger* logger)\n+      : GrpcRemoteWorker(channel, completion_queue, logger) {\n+    // Determine the maximum allowed message tag, used to determine hash\n+    void* v;\n+    int flag;\n+    MPICheck(MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_TAG_UB, &v, &flag));\n+    maxMessageTag = *(int*)v;\n+\n+    // Test if this path is disabled at launch\n+    const char* env = getenv(\"MPI_PATH_DISABLED\");\n+    if (env && env[0] == '1') disabledAtRuntime = true;\n+  }\n+\n+  void SendTensorSync(const WorkerEnv* env, const Rendezvous::ParsedKey& key,\n+                      const Rendezvous::Args& args, const Tensor& val,\n+                      const bool is_dead, const int64 step_id_,\n+                      Status& s) override {\n+    const int dst = getMPIPartnerID(false, key.FullKey().ToString(), env);\n+    const int hash =\n+        tensorHash(strings::StrCat(key.FullKey().ToString(), step_id_));\n+    // fprintf(stderr, \"JBDBG Going to send data to: %s  : %d  || %d\n+    // devContext: %d\\n\", key.FullKey().ToString().c_str(), dst, hash,\n+    // args.device_context);\n+    MPISendTensor(dst, hash, is_dead, val);\n+    s = Status::OK();\n+  }\n+\n+  void RecvTensorAsync(WorkerEnv* env, CallOptions* call_opts,\n+                       const RecvTensorRequest* request,\n+                       TensorResponse* response, StatusCallback done) override {\n+    if (disabledAtRuntime) {\n+      GrpcRemoteWorker::RecvTensorAsync(env, call_opts, request, response,\n+                                        done);\n+    } else {\n+      // TODO: Figure out how to get the size of the requested Tensor\n+      // Is this known by looking up the key/request?", "path": "tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc", "position": 237, "original_position": 237, "commit_id": "b43d22ed0ecd59779a97ad2d07048ff128658990", "original_commit_id": "b43d22ed0ecd59779a97ad2d07048ff128658990", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "body": "One low-hanging fruit for optimisation would be decide when to send the tensor inline in the first round given a certain configurable size limit, when memory pinning and other overhead is significant compared to data transmission. This could reduce latency for the common case when the tensor is small, and we can always fall back to 2 rounds when it gets large enough.", "created_at": "2017-03-21T14:38:32Z", "updated_at": "2017-03-21T14:38:32Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/7710#discussion_r107172108", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/7710", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107172108"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/7710#discussion_r107172108"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/7710"}}, "body_html": "<p>One low-hanging fruit for optimisation would be decide when to send the tensor inline in the first round given a certain configurable size limit, when memory pinning and other overhead is significant compared to data transmission. This could reduce latency for the common case when the tensor is small, and we can always fall back to 2 rounds when it gets large enough.</p>", "body_text": "One low-hanging fruit for optimisation would be decide when to send the tensor inline in the first round given a certain configurable size limit, when memory pinning and other overhead is significant compared to data transmission. This could reduce latency for the common case when the tensor is small, and we can always fall back to 2 rounds when it gets large enough.", "in_reply_to_id": 106685948}