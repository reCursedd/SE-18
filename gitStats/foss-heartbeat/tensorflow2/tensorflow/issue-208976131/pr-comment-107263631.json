{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107263631", "pull_request_review_id": 28225582, "id": 107263631, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNzI2MzYzMQ==", "diff_hunk": "@@ -260,10 +288,206 @@ class GrpcRemoteWorker : public WorkerInterface {\n   TF_DISALLOW_COPY_AND_ASSIGN(GrpcRemoteWorker);\n };\n \n+#if USE_MPI\n+\n+class RDMARemoteWorker : public GrpcRemoteWorker {\n+\n+ private:\n+  // Returns the name of the desitnation specified in a rendezvous key\n+  // For idx=0 it is the source, for idx=2 it is the destination\n+  string GetWorkerName(const std::string& key, const int idx) {\n+    // Convert the key back to the subpieces\n+    const std::vector<string> num_strings = str_util::Split(key, ';');\n+    // Sanity check, should be 5 src;id;dst;name;frame_iter\n+    assert(num_strings.size() == 5);\n+    // Strip the device eg /cpu:0 to get the worker name\n+    return num_strings[idx].substr(0, num_strings[idx].find_last_of('/'));\n+  }\n+  string SourceWorker(const std::string& key) { return GetWorkerName(key, 0); }\n+  string DestinationWorker(const std::string& key) {\n+    return GetWorkerName(key, 2);\n+  }\n+\n+  const int getMPIPartnerID(const bool targetIsSource, std::string key,\n+                            const WorkerEnv* env) {\n+    // Convert the grpc-name to MPI process ID\n+    std::string name;\n+    if (targetIsSource)\n+      name = SourceWorker(key);\n+    else\n+      name = DestinationWorker(key);\n+    auto it = env->worker_name_MPI_idx.find(name);\n+    if (it == env->worker_name_MPI_idx.end()) {\n+      LOG(FATAL) << \"Failed to convert worker name to MPI index: \" << name;\n+    }\n+    return it->second;\n+  }\n+\n+  const int tensorHash(const std::string key) {\n+    const uint32 hash32 = Hash32(key.data(), key.size(), 20161211);\n+    const int tensorKeyHash = std::abs(static_cast<int>(hash32));\n+    return (tensorKeyHash % maxMessageTag);\n+  }\n+\n+  void MPISendTensor(const int dst, const int hash, const bool is_dead,\n+                     const Tensor& val) {\n+#if 1\n+    // Send a header using the 'hash', followed by a message identified by\n+    // the followUpTag, which is unique to this thread\n+    const int followUpTag =\n+        (std::abs(static_cast<int>(pthread_self()))) % maxMessageTag;\n+\n+    // Encode the properties of the tensor and send this to the destination\n+    RecvTensorResponse response;\n+    response.set_is_dead(is_dead);\n+    response.set_send_start_micros(Env::Default()->NowMicros());\n+    response.mutable_tensor()->set_dtype(val.dtype());\n+    val.shape().AsProto(response.mutable_tensor()->mutable_tensor_shape());\n+    std::vector<char> respBuff(response.ByteSize() + sizeof(int));\n+    response.SerializeToArray(&respBuff[4], respBuff.size() - sizeof(int));\n+\n+    // fprintf(stderr, \"JBDBG Going to send the following message hash: %d tag:\n+    // %d\\n\", hash, followUpTag);\n+\n+    ((int*)(&respBuff[0]))[0] = followUpTag;\n+\n+    MPICheck(MPI_Send(&respBuff[0], respBuff.size(), MPI_BYTE, dst, hash,\n+                      MPI_COMM_WORLD));\n+\n+    // Next transfer the actual Tensor data\n+    const size_t nBytes = val.TotalBytes();\n+    const char* data = val.tensor_data().data();\n+\n+    // frintf(stderr, \"JBDBG Going to send this data: %p bytes: %d hash: %d tag:\n+    // %d\\n\", data, nBytes, hash, followUpTag);\n+\n+    // Transfer the Tensor content, in maxSize chuncks\n+    for (size_t i = 0; i <= nBytes; i += maxSize) {\n+      // Use MPI_Ssend to prevent that the function returns before the message\n+      // has arrived. This would cause inconsistencies in the execution and\n+      // crashes.\n+      const int toSend = std::min(maxSize, nBytes - i);\n+      MPICheck(MPI_Ssend(&data[i], toSend, MPI_BYTE, dst, followUpTag,\n+                         MPI_COMM_WORLD));\n+    }\n+#endif\n+  }  // MPISendTensor\n+\n+  void MPIRecvTensor(const int src, const int hash, TensorResponse* response) {\n+#if 1\n+    MPI_Status status;\n+    MPI_Message msg;\n+\n+    // Receive the header message, probe as size is variable\n+    int incSize;\n+    MPICheck(MPI_Mprobe(src, hash, MPI_COMM_WORLD, &msg, &status));\n+    MPICheck(MPI_Get_count(&status, MPI_CHAR, &incSize));\n+    std::vector<char> sta(incSize);\n+    MPICheck(MPI_Mrecv(&sta[0], incSize, MPI_CHAR, &msg, &status));\n+\n+    const int followUpTag = *((int*)(&sta[0]));\n+    RecvTensorResponse RTresponse;\n+    // std::cerr << response->DebugString() << std::endl;\n+    RTresponse.ParseFromArray(&sta[4], sta.size() - sizeof(int));\n+    // std::cerr << RTresponse.DebugString() << std::endl;\n+\n+    // Initialize the destination tensor\n+    // TODO is it possible that this tensor already exists/reuse of memory\n+    // location?\n+    response->InitPartial(RTresponse);\n+    const size_t nBytes = response->tensor().TotalBytes();\n+\n+    // Receive the Tensor content\n+    char* data = const_cast<char*>(response->tensor().tensor_data().data());\n+    for (size_t i = 0; i <= nBytes; i += maxSize) {\n+      const int toRecv = std::min(maxSize, nBytes - i);\n+      MPICheck(MPI_Recv(&data[i], toRecv, MPI_BYTE, src, followUpTag,\n+                        MPI_COMM_WORLD, &status));\n+    }\n+#endif\n+  }  // MPIRecvTensor\n+\n+  // Different MPI implementations use different values for allowed tag ranges\n+  // so retrieve the max value to generate valid tags\n+  int maxMessageTag;\n+\n+  // Max size of the data chuncks, 512MB\n+  const size_t maxSize = 1024 * 1024 * 512;\n+\n+  // If the MPI_PATH_DISABLED variable is set then disable this code path\n+  bool disabledAtRuntime = false;\n+\n+ public:\n+  explicit RDMARemoteWorker(SharedGrpcChannelPtr channel,\n+                            ::grpc::CompletionQueue* completion_queue,\n+                            WorkerCacheLogger* logger)\n+      : GrpcRemoteWorker(channel, completion_queue, logger) {\n+    // Determine the maximum allowed message tag, used to determine hash\n+    void* v;\n+    int flag;\n+    MPICheck(MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_TAG_UB, &v, &flag));\n+    maxMessageTag = *(int*)v;\n+\n+    // Test if this path is disabled at launch\n+    const char* env = getenv(\"MPI_PATH_DISABLED\");\n+    if (env && env[0] == '1') disabledAtRuntime = true;\n+  }\n+\n+  void SendTensorSync(const WorkerEnv* env, const Rendezvous::ParsedKey& key,\n+                      const Rendezvous::Args& args, const Tensor& val,\n+                      const bool is_dead, const int64 step_id_,\n+                      Status& s) override {\n+    const int dst = getMPIPartnerID(false, key.FullKey().ToString(), env);\n+    const int hash =\n+        tensorHash(strings::StrCat(key.FullKey().ToString(), step_id_));\n+    // fprintf(stderr, \"JBDBG Going to send data to: %s  : %d  || %d\n+    // devContext: %d\\n\", key.FullKey().ToString().c_str(), dst, hash,\n+    // args.device_context);\n+    MPISendTensor(dst, hash, is_dead, val);\n+    s = Status::OK();\n+  }\n+\n+  void RecvTensorAsync(WorkerEnv* env, CallOptions* call_opts,\n+                       const RecvTensorRequest* request,\n+                       TensorResponse* response, StatusCallback done) override {\n+    if (disabledAtRuntime) {\n+      GrpcRemoteWorker::RecvTensorAsync(env, call_opts, request, response,\n+                                        done);\n+    } else {\n+      // TODO: Figure out how to get the size of the requested Tensor\n+      // Is this known by looking up the key/request?", "path": "tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc", "position": 237, "original_position": 237, "commit_id": "b43d22ed0ecd59779a97ad2d07048ff128658990", "original_commit_id": "b43d22ed0ecd59779a97ad2d07048ff128658990", "user": {"login": "jbedorf", "id": 1643141, "node_id": "MDQ6VXNlcjE2NDMxNDE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1643141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbedorf", "html_url": "https://github.com/jbedorf", "followers_url": "https://api.github.com/users/jbedorf/followers", "following_url": "https://api.github.com/users/jbedorf/following{/other_user}", "gists_url": "https://api.github.com/users/jbedorf/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbedorf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbedorf/subscriptions", "organizations_url": "https://api.github.com/users/jbedorf/orgs", "repos_url": "https://api.github.com/users/jbedorf/repos", "events_url": "https://api.github.com/users/jbedorf/events{/privacy}", "received_events_url": "https://api.github.com/users/jbedorf/received_events", "type": "User", "site_admin": false}, "body": "In a future version it will have the option to send the tensor in a single step, not for latency per se (although this can be configured if needed), but for handling certain tensor types that can not be trivially copied.\r\n\r\nThe fact that MPI takes care of all these things in MPI_Send/MPI_Recv is the main reason that I chose for the MPI API. TensorFlow is likely be used on a variety of machines and configurations and it would not be doable to implement it with verbs such that it will be optimal on all these networks. Better to let MPI implementations take care of that as they have been doing this for decades. ", "created_at": "2017-03-21T20:17:39Z", "updated_at": "2017-03-21T20:17:39Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/7710#discussion_r107263631", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/7710", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107263631"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/7710#discussion_r107263631"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/7710"}}, "body_html": "<p>In a future version it will have the option to send the tensor in a single step, not for latency per se (although this can be configured if needed), but for handling certain tensor types that can not be trivially copied.</p>\n<p>The fact that MPI takes care of all these things in MPI_Send/MPI_Recv is the main reason that I chose for the MPI API. TensorFlow is likely be used on a variety of machines and configurations and it would not be doable to implement it with verbs such that it will be optimal on all these networks. Better to let MPI implementations take care of that as they have been doing this for decades.</p>", "body_text": "In a future version it will have the option to send the tensor in a single step, not for latency per se (although this can be configured if needed), but for handling certain tensor types that can not be trivially copied.\nThe fact that MPI takes care of all these things in MPI_Send/MPI_Recv is the main reason that I chose for the MPI API. TensorFlow is likely be used on a variety of machines and configurations and it would not be doable to implement it with verbs such that it will be optimal on all these networks. Better to let MPI implementations take care of that as they have been doing this for decades.", "in_reply_to_id": 106685948}