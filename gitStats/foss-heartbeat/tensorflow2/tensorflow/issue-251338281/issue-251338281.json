{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12402", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12402/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12402/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12402/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12402", "id": 251338281, "node_id": "MDU6SXNzdWUyNTEzMzgyODE=", "number": 12402, "title": "DataLossError. Checksum does not match when using multiple TFRecordDataset via tf.case", "user": {"login": "algoterranean", "id": 6758520, "node_id": "MDQ6VXNlcjY3NTg1MjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/6758520?v=4", "gravatar_id": "", "url": "https://api.github.com/users/algoterranean", "html_url": "https://github.com/algoterranean", "followers_url": "https://api.github.com/users/algoterranean/followers", "following_url": "https://api.github.com/users/algoterranean/following{/other_user}", "gists_url": "https://api.github.com/users/algoterranean/gists{/gist_id}", "starred_url": "https://api.github.com/users/algoterranean/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/algoterranean/subscriptions", "organizations_url": "https://api.github.com/users/algoterranean/orgs", "repos_url": "https://api.github.com/users/algoterranean/repos", "events_url": "https://api.github.com/users/algoterranean/events{/privacy}", "received_events_url": "https://api.github.com/users/algoterranean/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2017-08-18T19:30:57Z", "updated_at": "2018-01-03T19:27:29Z", "closed_at": "2018-01-03T19:27:28Z", "author_association": "NONE", "body_html": "<p>See below. Please let me know what I can do to provide more information for you.  I am working on pulling out the offending code into a standalone py file to replicate the bug elsewhere, but it might take a few days.</p>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: arch linux (LTS kernel 4.9.44-1)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3 (master branch as of commit <code>566d167c</code>)</li>\n<li><strong>Python version</strong>: 3.6.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.2-2</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0.61-2/6.0.21-2</li>\n<li><strong>GPU model and memory</strong>: 1080 GTX Ti</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>I don't have exact source for you yet of a trimmed down example. I will see if I can put one together. Generally, the steps are</p>\n<ul>\n<li>Generate a few <code>TFRecord</code> files</li>\n<li>Instantiate a few <code>TFRecordDataset</code> from the files.</li>\n<li>Perform necessary pre-processing on <code>TFRecordDataset</code> entries for each dataset, then on the dataset itself (<code>.cache</code>, <code>.repeat</code>, <code>.shuffle</code>, <code>.batch</code>, etc) For example, we might have datasets <code>train_dataset</code>, <code>validate_dataset</code>, and <code>test_dataset</code>.</li>\n<li>Initialize each dataset (<code>.make_initializable_iterator</code>)</li>\n<li>Train a model using an input tensor <code>x</code> of</li>\n</ul>\n<pre><code>train_phase = tf.placeholder(dtype=tf.int32, shape=())\nx = tf.case([(tf.equal(train_phase, 1), lambda: train_dataset),\n             (tf.equal(train_phase, 2), lambda: validate_dataset),\n             (tf.equal(train_phase,3), lambda: test_dataset)],\n         default=lambda: train_dataset)\n# ... construct model_op using tensor x as input ...\n# ... call the initializer ...\n\n# train for a bunch\nfor i in ...\n     sess.run(main_op, feed_dict={train_phase: 1})\n# validate\nfor i in ...\n     sess.run(main_op, feed_dict={train_phase: 2})\n# test\nfor i in ...\n     sess.run(main_op, feed_dict={train_phase: 3})\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I am using a single tensor <code>x</code> to represent my input values for all three phases of my model training (training, validation, and testing). To alternate between them without duplicating entire graphs, I set the tensor to be conditional on the value of <code>train_phase</code> (again, train, validate, or test) by using <code>tf.case</code>.</p>\n<p>If the above steps are followed, my system will eventually (non-deterministically) crash during the training/validating/testing of the model. This is independent of the dataset used (I am using well-scrubbed versions of NYUv2, CIFAR-10, KITTI, etc.) and does not occur in particular records. The error is always long the lines of</p>\n<pre><code>DataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\n Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\n</code></pre>\n<p>It can occur during any phase (that is, on any branch of the <code>tf.case</code>). It occurs with multiple versions of CUDA 8 and TF (at least since TFRecordDataset came out). It occurs with single or multiple GPUS. It occurs with different Linux kernel versions (multiple variants of 4.10, 4.11, 4.12, including the 4.9 TLS branch).</p>\n<p>However, it does <strong>NOT</strong> occur if I don't use <code>tf.case</code> (that is, I use a single TFRecordDataset tensor as the input, i.e., only do training and skip validate/test).</p>\n<p>I would assume this is a complication with the CUDA drivers, as I occasionally (though not always) get kernel panics at the same time. I will also occasionally (though not always) get errors like <code>malloc(): smallbin double linked list corrupted</code>.</p>\n<p>I should also add that I've already tested for hardware issues and seem to have ruled all of them out (memory failures, HD/SSD cables, motherboard, PSU spikes, etc.).</p>\n<h3>Source code / logs</h3>\n<p>The full trace/output is</p>\n<pre><code>Caught unexpected exception during training: Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\n\nCaused by op 'input_pipeline/IteratorGetNext_1', defined at:\n  File \"train.py\", line 266, in &lt;module&gt;\n    datasets = hem.get_datasets(args)\n  File \"/mnt/research/projects/autoencoders/hem/util/data.py\", line 64, in get_datasets\n    x = iterator.get_next()\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 311, in get_next\n    name=name))\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 698, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3046, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1604, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\n Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\n</code></pre>", "body_text": "See below. Please let me know what I can do to provide more information for you.  I am working on pulling out the offending code into a standalone py file to replicate the bug elsewhere, but it might take a few days.\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): arch linux (LTS kernel 4.9.44-1)\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.3 (master branch as of commit 566d167c)\nPython version: 3.6.2\nBazel version (if compiling from source): 0.5.2-2\nCUDA/cuDNN version: 8.0.61-2/6.0.21-2\nGPU model and memory: 1080 GTX Ti\nExact command to reproduce:\n\nI don't have exact source for you yet of a trimmed down example. I will see if I can put one together. Generally, the steps are\n\nGenerate a few TFRecord files\nInstantiate a few TFRecordDataset from the files.\nPerform necessary pre-processing on TFRecordDataset entries for each dataset, then on the dataset itself (.cache, .repeat, .shuffle, .batch, etc) For example, we might have datasets train_dataset, validate_dataset, and test_dataset.\nInitialize each dataset (.make_initializable_iterator)\nTrain a model using an input tensor x of\n\ntrain_phase = tf.placeholder(dtype=tf.int32, shape=())\nx = tf.case([(tf.equal(train_phase, 1), lambda: train_dataset),\n             (tf.equal(train_phase, 2), lambda: validate_dataset),\n             (tf.equal(train_phase,3), lambda: test_dataset)],\n         default=lambda: train_dataset)\n# ... construct model_op using tensor x as input ...\n# ... call the initializer ...\n\n# train for a bunch\nfor i in ...\n     sess.run(main_op, feed_dict={train_phase: 1})\n# validate\nfor i in ...\n     sess.run(main_op, feed_dict={train_phase: 2})\n# test\nfor i in ...\n     sess.run(main_op, feed_dict={train_phase: 3})\n\nDescribe the problem\nI am using a single tensor x to represent my input values for all three phases of my model training (training, validation, and testing). To alternate between them without duplicating entire graphs, I set the tensor to be conditional on the value of train_phase (again, train, validate, or test) by using tf.case.\nIf the above steps are followed, my system will eventually (non-deterministically) crash during the training/validating/testing of the model. This is independent of the dataset used (I am using well-scrubbed versions of NYUv2, CIFAR-10, KITTI, etc.) and does not occur in particular records. The error is always long the lines of\nDataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\n Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\n\nIt can occur during any phase (that is, on any branch of the tf.case). It occurs with multiple versions of CUDA 8 and TF (at least since TFRecordDataset came out). It occurs with single or multiple GPUS. It occurs with different Linux kernel versions (multiple variants of 4.10, 4.11, 4.12, including the 4.9 TLS branch).\nHowever, it does NOT occur if I don't use tf.case (that is, I use a single TFRecordDataset tensor as the input, i.e., only do training and skip validate/test).\nI would assume this is a complication with the CUDA drivers, as I occasionally (though not always) get kernel panics at the same time. I will also occasionally (though not always) get errors like malloc(): smallbin double linked list corrupted.\nI should also add that I've already tested for hardware issues and seem to have ruled all of them out (memory failures, HD/SSD cables, motherboard, PSU spikes, etc.).\nSource code / logs\nThe full trace/output is\nCaught unexpected exception during training: Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\n\nCaused by op 'input_pipeline/IteratorGetNext_1', defined at:\n  File \"train.py\", line 266, in <module>\n    datasets = hem.get_datasets(args)\n  File \"/mnt/research/projects/autoencoders/hem/util/data.py\", line 64, in get_datasets\n    x = iterator.get_next()\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 311, in get_next\n    name=name))\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 698, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3046, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1604, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\n Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]", "body": "See below. Please let me know what I can do to provide more information for you.  I am working on pulling out the offending code into a standalone py file to replicate the bug elsewhere, but it might take a few days.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: arch linux (LTS kernel 4.9.44-1)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3 (master branch as of commit `566d167c`)\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**: 0.5.2-2\r\n- **CUDA/cuDNN version**: 8.0.61-2/6.0.21-2\r\n- **GPU model and memory**: 1080 GTX Ti\r\n- **Exact command to reproduce**:\r\n\r\nI don't have exact source for you yet of a trimmed down example. I will see if I can put one together. Generally, the steps are\r\n\r\n- Generate a few `TFRecord` files\r\n- Instantiate a few `TFRecordDataset` from the files.\r\n- Perform necessary pre-processing on `TFRecordDataset` entries for each dataset, then on the dataset itself (`.cache`, `.repeat`, `.shuffle`, `.batch`, etc) For example, we might have datasets `train_dataset`, `validate_dataset`, and `test_dataset`.\r\n- Initialize each dataset (`.make_initializable_iterator`)\r\n- Train a model using an input tensor `x` of\r\n\r\n```\r\ntrain_phase = tf.placeholder(dtype=tf.int32, shape=())\r\nx = tf.case([(tf.equal(train_phase, 1), lambda: train_dataset),\r\n             (tf.equal(train_phase, 2), lambda: validate_dataset),\r\n             (tf.equal(train_phase,3), lambda: test_dataset)],\r\n         default=lambda: train_dataset)\r\n# ... construct model_op using tensor x as input ...\r\n# ... call the initializer ...\r\n\r\n# train for a bunch\r\nfor i in ...\r\n     sess.run(main_op, feed_dict={train_phase: 1})\r\n# validate\r\nfor i in ...\r\n     sess.run(main_op, feed_dict={train_phase: 2})\r\n# test\r\nfor i in ...\r\n     sess.run(main_op, feed_dict={train_phase: 3})\r\n```\r\n\r\n### Describe the problem\r\nI am using a single tensor `x` to represent my input values for all three phases of my model training (training, validation, and testing). To alternate between them without duplicating entire graphs, I set the tensor to be conditional on the value of `train_phase` (again, train, validate, or test) by using `tf.case`. \r\n\r\nIf the above steps are followed, my system will eventually (non-deterministically) crash during the training/validating/testing of the model. This is independent of the dataset used (I am using well-scrubbed versions of NYUv2, CIFAR-10, KITTI, etc.) and does not occur in particular records. The error is always long the lines of\r\n\r\n```\r\nDataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n``` \r\n\r\nIt can occur during any phase (that is, on any branch of the `tf.case`). It occurs with multiple versions of CUDA 8 and TF (at least since TFRecordDataset came out). It occurs with single or multiple GPUS. It occurs with different Linux kernel versions (multiple variants of 4.10, 4.11, 4.12, including the 4.9 TLS branch). \r\n\r\nHowever, it does **NOT** occur if I don't use `tf.case` (that is, I use a single TFRecordDataset tensor as the input, i.e., only do training and skip validate/test).\r\n\r\nI would assume this is a complication with the CUDA drivers, as I occasionally (though not always) get kernel panics at the same time. I will also occasionally (though not always) get errors like `malloc(): smallbin double linked list corrupted`.\r\n\r\nI should also add that I've already tested for hardware issues and seem to have ruled all of them out (memory failures, HD/SSD cables, motherboard, PSU spikes, etc.). \r\n\r\n\r\n### Source code / logs\r\nThe full trace/output is\r\n\r\n```\r\nCaught unexpected exception during training: Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n\r\nCaused by op 'input_pipeline/IteratorGetNext_1', defined at:\r\n  File \"train.py\", line 266, in <module>\r\n    datasets = hem.get_datasets(args)\r\n  File \"/mnt/research/projects/autoencoders/hem/util/data.py\", line 64, in get_datasets\r\n    x = iterator.get_next()\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 311, in get_next\r\n    name=name))\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 698, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3046, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1604, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nDataLossError (see above for traceback): Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n Checksum does not match: stored 3054820559 vs. calculated on the restored bytes 2831969904\r\n\t [[Node: input_pipeline/IteratorGetNext_1 = IteratorGetNext[output_shapes=[[?,?,64,64]], output_types=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_pipeline/Iterator_1)]]\r\n\t [[Node: add_1/_69 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_69_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n```\r\n"}