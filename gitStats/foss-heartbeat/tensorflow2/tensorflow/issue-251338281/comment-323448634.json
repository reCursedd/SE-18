{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323448634", "html_url": "https://github.com/tensorflow/tensorflow/issues/12402#issuecomment-323448634", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12402", "id": 323448634, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzQ0ODYzNA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-18T20:04:50Z", "updated_at": "2017-08-18T20:04:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6758520\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/algoterranean\">@algoterranean</a> Are you specifying a <code>filename</code> argument when you use <code>Dataset.cache()</code>? Is it possibly the same filename for each dataset? Does the problem go away if you specify an empty filename (and hence cache in memory)?</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1284535\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/saeta\">@saeta</a> It looks like this error might be coming from the <code>TensorBundle</code> used in the <code>CacheDataset</code>. Are there any obvious risks in using multiple instances of a dataset derived from the same op concurrently?</p>", "body_text": "@algoterranean Are you specifying a filename argument when you use Dataset.cache()? Is it possibly the same filename for each dataset? Does the problem go away if you specify an empty filename (and hence cache in memory)?\n@saeta It looks like this error might be coming from the TensorBundle used in the CacheDataset. Are there any obvious risks in using multiple instances of a dataset derived from the same op concurrently?", "body": "@algoterranean Are you specifying a `filename` argument when you use `Dataset.cache()`? Is it possibly the same filename for each dataset? Does the problem go away if you specify an empty filename (and hence cache in memory)?\r\n\r\n@saeta It looks like this error might be coming from the `TensorBundle` used in the `CacheDataset`. Are there any obvious risks in using multiple instances of a dataset derived from the same op concurrently?"}