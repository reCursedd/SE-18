{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/126108851", "pull_request_review_id": 48562429, "id": 126108851, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNjEwODg1MQ==", "diff_hunk": "@@ -125,6 +125,46 @@ struct EluGrad {\n   }\n };\n \n+// Functor used by SeluOp to do the computations.\n+template <typename Device, typename T>\n+struct Selu {\n+  // Computes Selu activation.\n+  //\n+  // features: any shape.\n+  // activations: same shape as \"features\".\n+  void operator()(const Device& d, typename TTypes<T>::ConstTensor features,\n+                  typename TTypes<T>::Tensor activations) {\n+    // features.constant(?)\n+    const auto scale = static_cast<T>(1.0507009873554804934193349852946);\n+    const auto scale_alpha = static_cast<T>(1.7580993408473768599402175208123);\n+    const auto one = static_cast<T>(1);\n+    const auto zero = static_cast<T>(0);\n+    activations.device(d) =\n+        (features < zero)\n+            .select(scale_alpha * (features.exp() - features.constant(one)),\n+                    scale * features);\n+  }\n+};\n+\n+// Functor used by SeluGradOp to do the computations.\n+template <typename Device, typename T>\n+struct SeluGrad {\n+  // Computes SeluGrad backprops.\n+  //\n+  // gradients: gradients backpropagated to the Selu op.\n+  // activations: outputs of the Selu op.\n+  // backprops: gradients to backpropagate to the Selu inputs.\n+  void operator()(const Device& d, typename TTypes<T>::ConstTensor gradients,\n+                  typename TTypes<T>::ConstTensor activations,\n+                  typename TTypes<T>::Tensor backprops) {\n+    const auto scale = static_cast<T>(1.0507009873554804934193349852946);\n+    const auto scale_alpha = static_cast<T>(1.7580993408473768599402175208123);\n+    backprops.device(d) =\n+        (activations < static_cast<T>(0)).select(\n+            gradients * (activations + scale_alpha), gradients * scale);", "path": "tensorflow/core/kernels/relu_op_functor.h", "position": 40, "original_position": 40, "commit_id": "97bafa09a7403db67facb78757ec35eaca1215bb", "original_commit_id": "a567c7f448ee3ae8f1320f11632763e88eb603cc", "user": {"login": "AnishShah", "id": 3175743, "node_id": "MDQ6VXNlcjMxNzU3NDM=", "avatar_url": "https://avatars1.githubusercontent.com/u/3175743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnishShah", "html_url": "https://github.com/AnishShah", "followers_url": "https://api.github.com/users/AnishShah/followers", "following_url": "https://api.github.com/users/AnishShah/following{/other_user}", "gists_url": "https://api.github.com/users/AnishShah/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnishShah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnishShah/subscriptions", "organizations_url": "https://api.github.com/users/AnishShah/orgs", "repos_url": "https://api.github.com/users/AnishShah/repos", "events_url": "https://api.github.com/users/AnishShah/events{/privacy}", "received_events_url": "https://api.github.com/users/AnishShah/received_events", "type": "User", "site_admin": false}, "body": "ohh is it? Sorry for creating the confusion. I thought `scale_alpha` was for `alpha` in ELU. ", "created_at": "2017-07-07T09:43:27Z", "updated_at": "2017-07-25T15:03:45Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/10818#discussion_r126108851", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/10818", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/126108851"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/10818#discussion_r126108851"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/10818"}}, "body_html": "<p>ohh is it? Sorry for creating the confusion. I thought <code>scale_alpha</code> was for <code>alpha</code> in ELU.</p>", "body_text": "ohh is it? Sorry for creating the confusion. I thought scale_alpha was for alpha in ELU.", "in_reply_to_id": 126094491}