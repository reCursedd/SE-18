{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22098", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22098/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22098/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22098/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22098", "id": 357366660, "node_id": "MDU6SXNzdWUzNTczNjY2NjA=", "number": 22098, "title": "Memory issue when inter_op_parallelism_threads > 1 on Ubuntu 16.04", "user": {"login": "btaba", "id": 7320238, "node_id": "MDQ6VXNlcjczMjAyMzg=", "avatar_url": "https://avatars3.githubusercontent.com/u/7320238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/btaba", "html_url": "https://github.com/btaba", "followers_url": "https://api.github.com/users/btaba/followers", "following_url": "https://api.github.com/users/btaba/following{/other_user}", "gists_url": "https://api.github.com/users/btaba/gists{/gist_id}", "starred_url": "https://api.github.com/users/btaba/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/btaba/subscriptions", "organizations_url": "https://api.github.com/users/btaba/orgs", "repos_url": "https://api.github.com/users/btaba/repos", "events_url": "https://api.github.com/users/btaba/events{/privacy}", "received_events_url": "https://api.github.com/users/btaba/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-09-05T19:15:48Z", "updated_at": "2018-11-12T22:41:58Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes, see below</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n</ul>\n<pre><code>== cat /etc/issue ===============================================\nLinux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.14.3)\nprotobuf (3.6.1)\ntensorflow (1.10.0)\ntensorflow-tensorboard (0.4.0)\n\n== check for virtualenv =========================================\nTrue\n\n== tensorflow import ============================================\ntf.VERSION = 1.10.0\ntf.GIT_VERSION = v1.10.0-0-g656e7a2b34\ntf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34\nSanity check: array([1], dtype=int32)\n/home/btabanpour/mlpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/cuda/lib64:/home/anaconda3/lib/\nDYLD_LIBRARY_PATH is unset\n\n</code></pre>\n<ul>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: NA</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10.0</li>\n<li><strong>Python version</strong>: Python 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>Save this file as <code>reproduce.py</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> tensorflow.keras.applications.inception_v3 <span class=\"pl-k\">import</span> InceptionV3\n<span class=\"pl-k\">from</span> memory_profiler <span class=\"pl-k\">import</span> profile\n\nimage <span class=\"pl-k\">=</span> np.random.random((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">299</span>, <span class=\"pl-c1\">299</span>, <span class=\"pl-c1\">3</span>))\n\n<span class=\"pl-en\">@profile</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">predict</span>():\n    m.predict(image)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    m <span class=\"pl-k\">=</span> InceptionV3()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> or any other large network like VGG19()</span>\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n        predict()</pre></div>\n<p>I run <code>python reproduce.py</code> and memory increases from ~500MiB to ~1200MiB after 100 calls of <code>predict()</code>, <strong>running on CPU</strong>. Here is truncated output:</p>\n<pre><code> &gt; python reproduce.py\n2018-09-05 14:41:10.229405: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nFilename: reproduce.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7    493.0 MiB    493.0 MiB   @profile\n     8                             def predict():\n     9    524.8 MiB     31.8 MiB       m.predict(image)\n\n\nFilename: reproduce.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7    524.8 MiB    524.8 MiB   @profile\n     8                             def predict():\n     9    557.2 MiB     32.4 MiB       m.predict(image)\n\n\n...\n\n\nFilename: reproduce.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7   1283.4 MiB   1283.4 MiB   @profile\n     8                             def predict():\n     9   1283.6 MiB      0.2 MiB       m.predict(image)\n\n\nFilename: reproduce.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7   1283.6 MiB   1283.6 MiB   @profile\n     8                             def predict():\n     9   1285.9 MiB      2.3 MiB       m.predict(image)\n\n</code></pre>\n<h3>Describe the problem</h3>\n<p>In short, memory keeps increasing after each predict call. I tried the above script with other networks such as <code>tensorflow.keras.applications.vgg19</code> and pure tensorflow convnets and I see the same issue.</p>\n<p>This issue seems to be resolved when I set <code>inter_op_parallelism_threads=1</code> in the <code>tf.ConfigProto</code> as such:</p>\n<div class=\"highlight highlight-source-python\"><pre>    config <span class=\"pl-k\">=</span> tf.ConfigProto(\n        <span class=\"pl-v\">inter_op_parallelism_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    sess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config)</pre></div>\n<p>And re-appears when I start increasing <code>inter_op_parallelism_threads</code> to 32 (I'm running on 32 CPU cores).</p>\n<h3>Another issue:</h3>\n<p>As I mentioned previously, setting <code>inter_op_parallelism_threads=1</code> stops the memory from increasing on each predict call. Here is a script that sets the config <code>inter_op_parallelism_threads=1</code> and does not exhibit a memory leak:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.keras.backend <span class=\"pl-k\">as</span> K\n\n<span class=\"pl-k\">from</span> tensorflow.keras.applications.inception_v3 <span class=\"pl-k\">import</span> InceptionV3\n<span class=\"pl-k\">from</span> memory_profiler <span class=\"pl-k\">import</span> profile\n\n\nimage <span class=\"pl-k\">=</span> np.random.random((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">299</span>, <span class=\"pl-c1\">299</span>, <span class=\"pl-c1\">3</span>))\n\n\n<span class=\"pl-en\">@profile</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">predict</span>():\n    m.predict(image)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    config <span class=\"pl-k\">=</span> tf.ConfigProto(\n        <span class=\"pl-v\">inter_op_parallelism_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    sess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config)\n\n    <span class=\"pl-c1\">print</span>(sess._config)\n    K.set_session(sess)\n\n    m <span class=\"pl-k\">=</span> InceptionV3()\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n        predict()</pre></div>\n<p>However, I see memory increasing again if I add an extra call to <code>tf.Session()</code> before calling the one with the <code>tf.ConfigProto</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.keras.backend <span class=\"pl-k\">as</span> K\n\n<span class=\"pl-k\">from</span> tensorflow.keras.applications.inception_v3 <span class=\"pl-k\">import</span> InceptionV3\n<span class=\"pl-k\">from</span> memory_profiler <span class=\"pl-k\">import</span> profile\n\nimage <span class=\"pl-k\">=</span> np.random.random((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">299</span>, <span class=\"pl-c1\">299</span>, <span class=\"pl-c1\">3</span>))\n\n\n<span class=\"pl-en\">@profile</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">predict</span>():\n    m.predict(image)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    s <span class=\"pl-k\">=</span> tf.Session()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Added an extra tf.Session() call here!!!</span>\n    s.close()\n\n    config <span class=\"pl-k\">=</span> tf.ConfigProto(\n        <span class=\"pl-v\">inter_op_parallelism_threads</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    sess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config)\n\n    <span class=\"pl-c1\">print</span>(sess._config)\n    K.set_session(sess)\n\n    m <span class=\"pl-k\">=</span> InceptionV3()\n\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n        predict()</pre></div>\n<p>It seems like the ConfigProto from the first <code>tf.Session()</code> is overwriting the config from the next session I create.</p>\n<hr>\n<p>Here is a graph of the script above for 1000 predict calls:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7320238/45115233-3cfa3a80-b11d-11e8-9b5e-781508fc8583.png\"><img src=\"https://user-images.githubusercontent.com/7320238/45115233-3cfa3a80-b11d-11e8-9b5e-781508fc8583.png\" alt=\"tf_issue\" style=\"max-width:100%;\"></a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\n== cat /etc/issue ===============================================\nLinux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.14.3)\nprotobuf (3.6.1)\ntensorflow (1.10.0)\ntensorflow-tensorboard (0.4.0)\n\n== check for virtualenv =========================================\nTrue\n\n== tensorflow import ============================================\ntf.VERSION = 1.10.0\ntf.GIT_VERSION = v1.10.0-0-g656e7a2b34\ntf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34\nSanity check: array([1], dtype=int32)\n/home/btabanpour/mlpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/cuda/lib64:/home/anaconda3/lib/\nDYLD_LIBRARY_PATH is unset\n\n\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.10.0\nPython version: Python 3.5.2\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce:\n\nSave this file as reproduce.py:\nimport numpy as np\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom memory_profiler import profile\n\nimage = np.random.random((1, 299, 299, 3))\n\n@profile\ndef predict():\n    m.predict(image)\n\n\nif __name__ == '__main__':\n    m = InceptionV3()  # or any other large network like VGG19()\n    for _ in range(100):\n        predict()\nI run python reproduce.py and memory increases from ~500MiB to ~1200MiB after 100 calls of predict(), running on CPU. Here is truncated output:\n > python reproduce.py\n2018-09-05 14:41:10.229405: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\nFilename: reproduce.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7    493.0 MiB    493.0 MiB   @profile\n     8                             def predict():\n     9    524.8 MiB     31.8 MiB       m.predict(image)\n\n\nFilename: reproduce.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7    524.8 MiB    524.8 MiB   @profile\n     8                             def predict():\n     9    557.2 MiB     32.4 MiB       m.predict(image)\n\n\n...\n\n\nFilename: reproduce.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7   1283.4 MiB   1283.4 MiB   @profile\n     8                             def predict():\n     9   1283.6 MiB      0.2 MiB       m.predict(image)\n\n\nFilename: reproduce.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7   1283.6 MiB   1283.6 MiB   @profile\n     8                             def predict():\n     9   1285.9 MiB      2.3 MiB       m.predict(image)\n\n\nDescribe the problem\nIn short, memory keeps increasing after each predict call. I tried the above script with other networks such as tensorflow.keras.applications.vgg19 and pure tensorflow convnets and I see the same issue.\nThis issue seems to be resolved when I set inter_op_parallelism_threads=1 in the tf.ConfigProto as such:\n    config = tf.ConfigProto(\n        inter_op_parallelism_threads=1)\n    sess = tf.Session(config=config)\nAnd re-appears when I start increasing inter_op_parallelism_threads to 32 (I'm running on 32 CPU cores).\nAnother issue:\nAs I mentioned previously, setting inter_op_parallelism_threads=1 stops the memory from increasing on each predict call. Here is a script that sets the config inter_op_parallelism_threads=1 and does not exhibit a memory leak:\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom memory_profiler import profile\n\n\nimage = np.random.random((1, 299, 299, 3))\n\n\n@profile\ndef predict():\n    m.predict(image)\n\n\nif __name__ == '__main__':\n    config = tf.ConfigProto(\n        inter_op_parallelism_threads=1)\n    sess = tf.Session(config=config)\n\n    print(sess._config)\n    K.set_session(sess)\n\n    m = InceptionV3()\n    for _ in range(100):\n        predict()\nHowever, I see memory increasing again if I add an extra call to tf.Session() before calling the one with the tf.ConfigProto:\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom memory_profiler import profile\n\nimage = np.random.random((1, 299, 299, 3))\n\n\n@profile\ndef predict():\n    m.predict(image)\n\n\nif __name__ == '__main__':\n    s = tf.Session()  # Added an extra tf.Session() call here!!!\n    s.close()\n\n    config = tf.ConfigProto(\n        inter_op_parallelism_threads=1)\n    sess = tf.Session(config=config)\n\n    print(sess._config)\n    K.set_session(sess)\n\n    m = InceptionV3()\n\n    for _ in range(100):\n        predict()\nIt seems like the ConfigProto from the first tf.Session() is overwriting the config from the next session I create.\n\nHere is a graph of the script above for 1000 predict calls:", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n```\r\n== cat /etc/issue ===============================================\r\nLinux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.3)\r\nprotobuf (3.6.1)\r\ntensorflow (1.10.0)\r\ntensorflow-tensorboard (0.4.0)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = v1.10.0-0-g656e7a2b34\r\ntf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34\r\nSanity check: array([1], dtype=int32)\r\n/home/btabanpour/mlpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64:/home/anaconda3/lib/\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n```\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n\r\nSave this file as `reproduce.py`:\r\n\r\n```python\r\nimport numpy as np\r\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\r\nfrom memory_profiler import profile\r\n\r\nimage = np.random.random((1, 299, 299, 3))\r\n\r\n@profile\r\ndef predict():\r\n    m.predict(image)\r\n\r\n\r\nif __name__ == '__main__':\r\n    m = InceptionV3()  # or any other large network like VGG19()\r\n    for _ in range(100):\r\n        predict()\r\n```\r\n\r\nI run `python reproduce.py` and memory increases from ~500MiB to ~1200MiB after 100 calls of `predict()`, **running on CPU**. Here is truncated output:\r\n\r\n```\r\n > python reproduce.py\r\n2018-09-05 14:41:10.229405: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nFilename: reproduce.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7    493.0 MiB    493.0 MiB   @profile\r\n     8                             def predict():\r\n     9    524.8 MiB     31.8 MiB       m.predict(image)\r\n\r\n\r\nFilename: reproduce.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7    524.8 MiB    524.8 MiB   @profile\r\n     8                             def predict():\r\n     9    557.2 MiB     32.4 MiB       m.predict(image)\r\n\r\n\r\n...\r\n\r\n\r\nFilename: reproduce.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7   1283.4 MiB   1283.4 MiB   @profile\r\n     8                             def predict():\r\n     9   1283.6 MiB      0.2 MiB       m.predict(image)\r\n\r\n\r\nFilename: reproduce.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7   1283.6 MiB   1283.6 MiB   @profile\r\n     8                             def predict():\r\n     9   1285.9 MiB      2.3 MiB       m.predict(image)\r\n\r\n```\r\n\r\n### Describe the problem\r\n\r\nIn short, memory keeps increasing after each predict call. I tried the above script with other networks such as `tensorflow.keras.applications.vgg19` and pure tensorflow convnets and I see the same issue.\r\n\r\nThis issue seems to be resolved when I set `inter_op_parallelism_threads=1` in the `tf.ConfigProto` as such:\r\n\r\n```python\r\n    config = tf.ConfigProto(\r\n        inter_op_parallelism_threads=1)\r\n    sess = tf.Session(config=config)\r\n```\r\n\r\nAnd re-appears when I start increasing `inter_op_parallelism_threads` to 32 (I'm running on 32 CPU cores).\r\n\r\n### Another issue:\r\n\r\nAs I mentioned previously, setting `inter_op_parallelism_threads=1` stops the memory from increasing on each predict call. Here is a script that sets the config `inter_op_parallelism_threads=1` and does not exhibit a memory leak:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\n\r\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\r\nfrom memory_profiler import profile\r\n\r\n\r\nimage = np.random.random((1, 299, 299, 3))\r\n\r\n\r\n@profile\r\ndef predict():\r\n    m.predict(image)\r\n\r\n\r\nif __name__ == '__main__':\r\n    config = tf.ConfigProto(\r\n        inter_op_parallelism_threads=1)\r\n    sess = tf.Session(config=config)\r\n\r\n    print(sess._config)\r\n    K.set_session(sess)\r\n\r\n    m = InceptionV3()\r\n    for _ in range(100):\r\n        predict()\r\n```\r\n\r\nHowever, I see memory increasing again if I add an extra call to `tf.Session()` before calling the one with the `tf.ConfigProto`:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\n\r\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\r\nfrom memory_profiler import profile\r\n\r\nimage = np.random.random((1, 299, 299, 3))\r\n\r\n\r\n@profile\r\ndef predict():\r\n    m.predict(image)\r\n\r\n\r\nif __name__ == '__main__':\r\n    s = tf.Session()  # Added an extra tf.Session() call here!!!\r\n    s.close()\r\n\r\n    config = tf.ConfigProto(\r\n        inter_op_parallelism_threads=1)\r\n    sess = tf.Session(config=config)\r\n\r\n    print(sess._config)\r\n    K.set_session(sess)\r\n\r\n    m = InceptionV3()\r\n\r\n    for _ in range(100):\r\n        predict()\r\n```\r\n\r\nIt seems like the ConfigProto from the first `tf.Session()` is overwriting the config from the next session I create.\r\n\r\n---\r\n\r\nHere is a graph of the script above for 1000 predict calls:\r\n\r\n![tf_issue](https://user-images.githubusercontent.com/7320238/45115233-3cfa3a80-b11d-11e8-9b5e-781508fc8583.png)\r\n"}