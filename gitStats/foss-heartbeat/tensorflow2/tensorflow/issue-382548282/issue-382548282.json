{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23873", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23873/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23873/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23873/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23873", "id": 382548282, "node_id": "MDU6SXNzdWUzODI1NDgyODI=", "number": 23873, "title": "Keras layers: no update ops added even when used as a \"simplified interface to Tensorflow\"", "user": {"login": "galeone", "id": 8427788, "node_id": "MDQ6VXNlcjg0Mjc3ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/8427788?v=4", "gravatar_id": "", "url": "https://api.github.com/users/galeone", "html_url": "https://github.com/galeone", "followers_url": "https://api.github.com/users/galeone/followers", "following_url": "https://api.github.com/users/galeone/following{/other_user}", "gists_url": "https://api.github.com/users/galeone/gists{/gist_id}", "starred_url": "https://api.github.com/users/galeone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/galeone/subscriptions", "organizations_url": "https://api.github.com/users/galeone/orgs", "repos_url": "https://api.github.com/users/galeone/repos", "events_url": "https://api.github.com/users/galeone/events{/privacy}", "received_events_url": "https://api.github.com/users/galeone/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-11-20T08:20:55Z", "updated_at": "2018-11-21T11:23:09Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux</li>\n<li>TensorFlow installed from (source or binary): binary</li>\n<li>TensorFlow version (use command below): 1.12</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: cuda 9.0, cudnn 7.1</li>\n<li>GPU model and memory: nvidia 1080ti</li>\n</ul>\n<p><strong>Describe the current behavior</strong></p>\n<p>I'm migrating my codebase to a Tensorflow 2.0 compatible version, thus I'm replacing all the <code>tf.layers</code> call with their <code>tf.keras.layers</code> counterpart.</p>\n<p>I want to continue using static graphs and MonitoredSession to train my models (since I do have all the input pipelines defined with tf.data.Dataset and all the training script defined to work in this way).</p>\n<p>However, there's a huge problem when a layer adds some update operation (like the BatchNormalization layer).</p>\n<p>The actual behavior is that the update operations (of moving mean and variance) are not called when training the model.</p>\n<p>This can be OK in a full-keras solution, where the connection among models, the update operations and so on, are managed by the <code>train_on_batch</code> + <code>model.compile</code> + <code>model.fit</code> call (that do some magic in the background).<br>\nIn fact, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fchollet\">@fchollet</a> said this is by design: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"327658707\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/19643\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/19643/hovercard?comment_id=394527486&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/19643#issuecomment-394527486\">#19643 (comment)</a></p>\n<p>But since I don't want to migrate to a full keras-based solution, how can I handle the updates?</p>\n<p>I found some theoretical workaround (collected the update operations <code>model.updates</code> + collect the inputs <code>model.inputs</code>, loop over the inputs, feed the correct input and execute with sess.run the updates), but those are really ugly and they don't work: I can trigger the parameter update, but the time-step of the update execution is wrong and the solution does not converge: moreover, when the model becomes complex (like in the example of BiGAN below) it can be a real mess and the code become unmaintenable.</p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>When I'm using keras layers to define the models and I'm not calling <code>train_on_batch</code> or <code>compile</code> or any other method to train the model that's pure keras, the update operations should be placed into the graph (having thus the same behavior of the batch normalization layer present in <code>tf.layers</code>) and executed when model <code>.trainable</code> is <code>True</code>.</p>\n<p>Moreover, there's another strange behavour: when I define a model output passing a new input (hence I invoke the <code>Model.call</code> method) the update operation have no idea of this new input.</p>\n<p>Probably, the <code>.call</code> method shouln't just return the output tensor, but it should return a new <code>Model</code> that shared the same parameters of the orignial one, but defined with a new input (and thus with its update ops aware of the new input).</p>\n<p><strong>Code to reproduce the issue</strong><br>\nA BiGAN implementation.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> typing <span class=\"pl-k\">import</span> Tuple, Callable, Any, Optional\n<span class=\"pl-k\">import</span> multiprocessing\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow <span class=\"pl-k\">import</span> keras <span class=\"pl-k\">as</span> k\n<span class=\"pl-k\">from</span> tensorflow.python.training.summary_io <span class=\"pl-k\">import</span> SummaryWriterCache\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">dataset</span>(\n    <span class=\"pl-smi\">shape</span>: Tuple[<span class=\"pl-c1\">int</span>, <span class=\"pl-c1\">int</span>],\n    <span class=\"pl-smi\">batch_size</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>,\n    <span class=\"pl-smi\">epochs</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>,\n    <span class=\"pl-smi\">train</span>: <span class=\"pl-c1\">bool</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>,\n    <span class=\"pl-smi\">_batch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n) -&gt; tf.data.Dataset:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Returns the dataset correcly batched and resized</span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        shape: The output shape of the images in the Dataset</span>\n<span class=\"pl-s\">        batch_size: The size of the batch to return at each invocation</span>\n<span class=\"pl-s\">        epochs: The the number of times that the dataset will repeat itself</span>\n<span class=\"pl-s\">                before throwing an exception</span>\n<span class=\"pl-s\">        train: when True, returns the shuffled train dataset, when False returns</span>\n<span class=\"pl-s\">               the test, not shuffled, dataset</span>\n<span class=\"pl-s\">        _batch: private, do not use</span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        The dataset</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_process</span>(<span class=\"pl-smi\">image</span>, <span class=\"pl-smi\">label</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>The function used to resize the image to the specified shape.</span>\n<span class=\"pl-s\">        Used in the tf.data.Dataset.map function</span>\n<span class=\"pl-s\">        Args:</span>\n<span class=\"pl-s\">            image: the input image</span>\n<span class=\"pl-s\">            label: the input label</span>\n<span class=\"pl-s\">        Return:</span>\n<span class=\"pl-s\">            resized_image, label</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n        <span class=\"pl-k\">nonlocal</span> shape\n        image <span class=\"pl-k\">=</span> tf.image.resize_images(\n            tf.expand_dims(image, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>), shape, tf.image.ResizeMethod.<span class=\"pl-c1\">NEAREST_NEIGHBOR</span>\n        )\n        image <span class=\"pl-k\">=</span> tf.cast(image, tf.float32)\n        image <span class=\"pl-k\">=</span> tf.squeeze(image, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>])\n        <span class=\"pl-k\">return</span> image, label\n\n    (train_images, train_labels), (\n        test_images,\n        test_labels,\n    ) <span class=\"pl-k\">=</span> k.datasets.cifar10.load_data()\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> train:\n        train_images <span class=\"pl-k\">=</span> test_images\n        train_labels <span class=\"pl-k\">=</span> test_labels\n    train_images <span class=\"pl-k\">=</span> (train_images <span class=\"pl-k\">-</span> <span class=\"pl-c1\">127.5</span>) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">127.5</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_generator</span>():\n        <span class=\"pl-s\"><span class=\"pl-k\">r</span><span class=\"pl-pds\">\"\"\"</span>The generator that returns the pair image,label</span>\n<span class=\"pl-s\">        This must be used in order to don't use tf.data.Dataset.from_tensor_slices.abs</span>\n<span class=\"pl-s\">        In this way, we can build a dataset from a generator and solve the problem of huge</span>\n<span class=\"pl-s\">        graphs created by from_tensor_slices (it creates a constant in the graph :\\)</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n        <span class=\"pl-k\">for</span> image, label <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(train_images, train_labels):\n            <span class=\"pl-k\">yield</span> image, label\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_set_shape</span>(<span class=\"pl-smi\">image</span>, <span class=\"pl-smi\">label</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Set the static + dynamic shape of the image, in order to correctly build the</span>\n<span class=\"pl-s\">        input pipeline in both phases</span>\n<span class=\"pl-s\">        Args:</span>\n<span class=\"pl-s\">            image: the input image</span>\n<span class=\"pl-s\">            label: the input label</span>\n<span class=\"pl-s\">        Return:</span>\n<span class=\"pl-s\">            image, label</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n        image.set_shape((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> static</span>\n        image <span class=\"pl-k\">=</span> tf.reshape(image, (<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> dynamic</span>\n        <span class=\"pl-k\">return</span> image, label\n\n    _dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_generator(\n        _generator, (tf.float32, tf.int32)\n    )  <span class=\"pl-c\"><span class=\"pl-c\">#</span> unkown shape</span>\n    _dataset <span class=\"pl-k\">=</span> _dataset.map(\n        _set_shape, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span>multiprocessing.cpu_count()\n    )  <span class=\"pl-c\"><span class=\"pl-c\">#</span> known static chsape</span>\n\n    _dataset <span class=\"pl-k\">=</span> _dataset.map(\n        _process, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span>multiprocessing.cpu_count()\n    )  <span class=\"pl-c\"><span class=\"pl-c\">#</span> resize to desired input shape</span>\n\n    <span class=\"pl-k\">if</span> _batch:\n        _dataset <span class=\"pl-k\">=</span> _dataset.batch(batch_size, <span class=\"pl-v\">drop_remainder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-k\">if</span> epochs:\n            _dataset <span class=\"pl-k\">=</span> _dataset.repeat(epochs)\n    <span class=\"pl-k\">elif</span> epochs:\n        _dataset <span class=\"pl-k\">=</span> _dataset.repeat(epochs)\n\n    _dataset <span class=\"pl-k\">=</span> _dataset.prefetch(<span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">return</span> _dataset\n\n\n<span class=\"pl-c1\">KERNEL_INITIALIZER</span> <span class=\"pl-k\">=</span> k.initializers.RandomNormal(<span class=\"pl-v\">mean</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.02</span>)\n<span class=\"pl-c1\">ALMOST_ONE</span> <span class=\"pl-k\">=</span> k.initializers.RandomNormal(<span class=\"pl-v\">mean</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.02</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">discriminator</span>(\n    <span class=\"pl-smi\">visual_shape</span>: tf.TensorShape,\n    <span class=\"pl-smi\">encoding_shape</span>: tf.TensorShape,\n    <span class=\"pl-smi\">conditioning</span>: Optional[Any] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>,\n    <span class=\"pl-smi\">l2_penalty</span>: <span class=\"pl-c1\">float</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>,\n) -&gt; k.Model:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Build the Discriminator model.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns a k.Model with 2 inputs and a single output.</span>\n<span class=\"pl-s\">    The inputs are an image and its encoded/latent representation.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        visual_shape: The shape of the visual input, 3D tensor</span>\n<span class=\"pl-s\">        encoding_shape: The shape of the latent/encoded representation</span>\n<span class=\"pl-s\">        # NOT IMPLEMENTED: Conditioning: data used as GAN conditioning</span>\n<span class=\"pl-s\">        # UNUSED: l2_penalty: l2 regularization strength</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        The discriminator model.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    kernel_size <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Inputs</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (64, 64, C)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (Latent Dimension, )</span>\n    input_visual <span class=\"pl-k\">=</span> k.layers.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>visual_shape)\n    input_encoding <span class=\"pl-k\">=</span> k.layers.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>encoding_shape)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Data</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 0</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (64, 64, 32)</span>\n    visual <span class=\"pl-k\">=</span> k.layers.Conv2D(\n        <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>,\n        <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n        <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n        <span class=\"pl-v\">kernel_regularizer</span><span class=\"pl-k\">=</span>k.regularizers.l2(l2_penalty),\n    )(input_visual)\n    visual <span class=\"pl-k\">=</span> k.layers.LeakyReLU(<span class=\"pl-v\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)(visual)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Data</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 1</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (32, 32, 32)</span>\n    visual <span class=\"pl-k\">=</span> k.layers.Conv2D(\n        <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>,\n        <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n        <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n        <span class=\"pl-v\">kernel_regularizer</span><span class=\"pl-k\">=</span>k.regularizers.l2(l2_penalty),\n    )(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.BatchNormalization()(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.LeakyReLU(<span class=\"pl-v\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.Dropout(<span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)(visual)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 2</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (16, 16, 64)</span>\n    visual <span class=\"pl-k\">=</span> k.layers.Conv2D(\n        <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>,\n        <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n        <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n    )(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.BatchNormalization()(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.LeakyReLU(<span class=\"pl-v\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.Dropout(<span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)(visual)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Flatten</span>\n    visual <span class=\"pl-k\">=</span> k.layers.Flatten()(visual)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Encoding</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 5 D(z)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (512,)</span>\n    encoding <span class=\"pl-k\">=</span> k.layers.Dense(<span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">512</span>, <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>)(\n        input_encoding\n    )\n    encoding <span class=\"pl-k\">=</span> k.layers.LeakyReLU(<span class=\"pl-v\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)(encoding)\n    encoding <span class=\"pl-k\">=</span> k.layers.Dropout(<span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)(encoding)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Data + Encoding</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 6 D(x, z)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (4608,)</span>\n    mixed <span class=\"pl-k\">=</span> k.layers.Concatenate()([visual, encoding])\n    mixed <span class=\"pl-k\">=</span> k.layers.Dense(<span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1024</span>, <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>)(mixed)\n    mixed <span class=\"pl-k\">=</span> k.layers.LeakyReLU(<span class=\"pl-v\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)(mixed)\n    mixed <span class=\"pl-k\">=</span> k.layers.Dropout(<span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)(mixed)\n    features <span class=\"pl-k\">=</span> mixed\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Final Step</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 7</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (1)</span>\n    out <span class=\"pl-k\">=</span> k.layers.Dense(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>)(mixed)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Use the functional interface</span>\n    model <span class=\"pl-k\">=</span> k.Model(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>[input_visual, input_encoding], <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>[out, features])\n    model.summary()\n    <span class=\"pl-k\">return</span> model\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">generator</span>(\n    <span class=\"pl-smi\">input_shape</span>: <span class=\"pl-c1\">int</span>,\n    <span class=\"pl-smi\">output_depth</span>: <span class=\"pl-c1\">int</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>,\n    <span class=\"pl-smi\">conditioning</span>: Optional[Any] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>,\n    <span class=\"pl-smi\">l2_penalty</span>: <span class=\"pl-c1\">float</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>,\n) -&gt; k.Model:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Build the Generator model.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Given a latent representation, generates a meaningful image.</span>\n<span class=\"pl-s\">    The input shape must be in the form of a vector 1x1xD.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        input_shape: The shape of the noise prior</span>\n<span class=\"pl-s\">        output_depth: The number of channels of the generated image</span>\n<span class=\"pl-s\">        # NOT IMPLEMENTED: Conditioning: data used as GAN conditioning</span>\n<span class=\"pl-s\">        l2_penalty: l2 regularization strength</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        The Generator model.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    kernel_size <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>)\n    model <span class=\"pl-k\">=</span> k.Sequential(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>generator<span class=\"pl-pds\">\"</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Project and Reshape the latent space</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 1</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (4*4*64,)</span>\n    model.add(\n        k.layers.Dense(\n            <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">64</span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n            <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>input_shape,\n            <span class=\"pl-v\">kernel_regularizer</span><span class=\"pl-k\">=</span>k.regularizers.l2(l2_penalty),\n        )\n    )\n    model.add(k.layers.Activation(k.activations.relu))\n    model.add(k.layers.Reshape((<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">64</span>)))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Starting Deconvolutions</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 2</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (8, 8, 64)</span>\n    model.add(\n        k.layers.Conv2DTranspose(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n        )\n    )\n    model.add(k.layers.BatchNormalization())\n    model.add(k.layers.Activation(k.activations.relu))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Starting Deconvolutions</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 3</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (16, 16, 128)</span>\n    model.add(\n        k.layers.Conv2DTranspose(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n        )\n    )\n    model.add(k.layers.BatchNormalization())\n    model.add(k.layers.Activation(k.activations.relu))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 4</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (32, 32, 256)</span>\n    model.add(\n        k.layers.Conv2DTranspose(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n        )\n    )\n    model.add(k.layers.BatchNormalization())\n    model.add(k.layers.Activation(k.activations.relu))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 5</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (64, 64, C)</span>\n    model.add(\n        k.layers.Conv2DTranspose(\n            <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span>output_depth,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n            <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n            <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n        )\n    )\n    model.add(k.layers.Activation(k.activations.tanh))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> G(z) in [-1,1]</span>\n\n    model.summary()\n    <span class=\"pl-k\">return</span> model\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">encoder</span>(\n    <span class=\"pl-smi\">visual_shape</span>: <span class=\"pl-c1\">int</span>, <span class=\"pl-smi\">latent_dimension</span>: <span class=\"pl-c1\">int</span>, <span class=\"pl-smi\">l2_penalty</span>: <span class=\"pl-c1\">float</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n) -&gt; k.Model:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Build the Encoder model.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    The encoder encodes the input in a vector with shape 1x1xlatent_dimension.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        visual_shape: The shape of the input to encode</span>\n<span class=\"pl-s\">        latent_dimension: The number of dimensions (along the depth) to use.</span>\n<span class=\"pl-s\">        # NOT IMPLEMENTED: conditioning: Data used as GAN conditioning</span>\n<span class=\"pl-s\">        l2_penalty: l2 regularization strength</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        The Encoder model.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    kernel_size <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Inputs</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (64, 64, C)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (Latent Dimension, )</span>\n    input_visual <span class=\"pl-k\">=</span> k.layers.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>visual_shape)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Data</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 0</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (64, 64, 32)</span>\n    visual <span class=\"pl-k\">=</span> k.layers.Conv2D(\n        <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>,\n        <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>),\n        <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n        <span class=\"pl-v\">kernel_regularizer</span><span class=\"pl-k\">=</span>k.regularizers.l2(l2_penalty),\n    )(input_visual)\n    visual <span class=\"pl-k\">=</span> k.layers.LeakyReLU(<span class=\"pl-v\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)(visual)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Data</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 1</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (32, 32, 32)</span>\n    visual <span class=\"pl-k\">=</span> k.layers.Conv2D(\n        <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>,\n        <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n        <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n        <span class=\"pl-v\">kernel_regularizer</span><span class=\"pl-k\">=</span>k.regularizers.l2(l2_penalty),\n    )(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.BatchNormalization()(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.LeakyReLU(<span class=\"pl-v\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.Dropout(<span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)(visual)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 2</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (16, 16, 64)</span>\n    visual <span class=\"pl-k\">=</span> k.layers.Conv2D(\n        <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>,\n        <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>),\n        <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>,\n    )(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.BatchNormalization()(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.LeakyReLU(<span class=\"pl-v\">alpha</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)(visual)\n    visual <span class=\"pl-k\">=</span> k.layers.Dropout(<span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)(visual)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Flatten</span>\n    visual <span class=\"pl-k\">=</span> k.layers.Flatten()(visual)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Encoding</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> (Latent space, )</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ### Layer 5</span>\n    visual <span class=\"pl-k\">=</span> k.layers.Dense(\n        <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>latent_dimension, <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">KERNEL_INITIALIZER</span>\n    )(visual)\n\n    model <span class=\"pl-k\">=</span> k.Model(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>input_visual, <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>visual)\n    model.summary()\n    <span class=\"pl-k\">return</span> model\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bce</span>(<span class=\"pl-smi\">x</span>: tf.Tensor, <span class=\"pl-smi\">label</span>: tf.Tensor) -&gt; tf.Tensor:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Returns the discrete binary cross entropy between x and the discrete label</span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        x: a 2D tensor</span>\n<span class=\"pl-s\">        label: the discrite label, aka, the distribution to match</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        The binary cros entropy</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">len</span>(x.shape) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">and</span> <span class=\"pl-c1\">len</span>(label.shape) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">return</span> tf.losses.sigmoid_cross_entropy(tf.ones_like(x) <span class=\"pl-k\">*</span> label, x)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">min_max</span>(<span class=\"pl-smi\">positive</span>: tf.Tensor, <span class=\"pl-smi\">negative</span>: tf.Tensor) -&gt; tf.Tensor:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Returns the discriminator (min max) loss</span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        positive: the discriminator output for the positive class: 2D tensor</span>\n<span class=\"pl-s\">        negative: the discriminator output for the negative class: 2D tensor</span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        The sum of 2 BCE</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    one <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">1.0</span>)\n    zero <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.0</span>)\n    d_loss <span class=\"pl-k\">=</span> bce(positive, one) <span class=\"pl-k\">+</span> bce(negative, zero)\n    <span class=\"pl-k\">return</span> d_loss\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>():\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Train the GAN.<span class=\"pl-pds\">\"\"\"</span></span>\n    batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\n    epochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n    latent_dimension <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n    l2_penalty <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n\n    x_, y_ <span class=\"pl-k\">=</span> dataset((<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>), batch_size, epochs).make_one_shot_iterator().get_next()\n\n    x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-c1\">list</span>(x_.shape))\n    tf.summary.image(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>, x, <span class=\"pl-v\">max_outputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the Models</span>\n    E <span class=\"pl-k\">=</span> encoder(x.shape[<span class=\"pl-c1\">1</span>:], latent_dimension, l2_penalty)\n\n    z_ <span class=\"pl-k\">=</span> tf.random_normal([batch_size, latent_dimension], <span class=\"pl-v\">mean</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n\n    z <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-c1\">list</span>(z_.shape))\n    G <span class=\"pl-k\">=</span> generator(z.shape[<span class=\"pl-c1\">1</span>:], x.shape[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>].value, l2_penalty)\n    D <span class=\"pl-k\">=</span> discriminator(x.shape[<span class=\"pl-c1\">1</span>:], E.output.shape[<span class=\"pl-c1\">1</span>:], l2_penalty)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Generate from latent representation z</span>\n    G_z <span class=\"pl-k\">=</span> G(z)\n    tf.summary.image(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>G(z)<span class=\"pl-pds\">\"</span></span>, G_z, <span class=\"pl-v\">max_outputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> encode x to a latent representation</span>\n    E_x <span class=\"pl-k\">=</span> E(x)\n\n    G_Ex <span class=\"pl-k\">=</span> G(E_x)\n    tf.summary.image(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>G(E(x))<span class=\"pl-pds\">\"</span></span>, G_Ex, <span class=\"pl-v\">max_outputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> plot image difference</span>\n    tf.summary.image(\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>G(E(x)) - x<span class=\"pl-pds\">\"</span></span>, tf.norm(G_Ex <span class=\"pl-k\">-</span> x_, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">keepdims</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), <span class=\"pl-v\">max_outputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>\n    )\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> The output of the discriminator is a bs,n,n,value</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> hence flatten all the values of the map and compute</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> the loss element wise</span>\n    D_Gz, F_Gz <span class=\"pl-k\">=</span> D(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>[G_z, z])\n    D_x, F_x <span class=\"pl-k\">=</span> D(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>[x, E_x])\n    D_Gz <span class=\"pl-k\">=</span> k.layers.Flatten()(D_Gz)\n    F_Gz <span class=\"pl-k\">=</span> k.layers.Flatten()(F_Gz)\n    D_x <span class=\"pl-k\">=</span> k.layers.Flatten()(D_x)\n    F_x <span class=\"pl-k\">=</span> k.layers.Flatten()(F_x)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span># Discriminator</span>\n    d_loss <span class=\"pl-k\">=</span> min_max(D_x, D_Gz)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span># Generator</span>\n    g_loss <span class=\"pl-k\">=</span> bce(D_Gz, tf.constant(<span class=\"pl-c1\">1.0</span>))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Encoder</span>\n    e_loss <span class=\"pl-k\">=</span> bce(D_x, tf.constant(<span class=\"pl-c1\">0.0</span>))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> add regularizations defined in the keras layers</span>\n    d_loss <span class=\"pl-k\">+=</span> tf.add_n(D.losses)\n    e_loss <span class=\"pl-k\">+=</span> tf.add_n(E.losses)\n    g_loss <span class=\"pl-k\">+=</span> tf.add_n(G.losses)\n\n    tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>d_loss<span class=\"pl-pds\">\"</span></span>, d_loss)\n    tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>g_loss<span class=\"pl-pds\">\"</span></span>, g_loss)\n    tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>e_loss<span class=\"pl-pds\">\"</span></span>, e_loss)\n\n    global_step <span class=\"pl-k\">=</span> tf.train.get_or_create_global_step()\n\n    lr <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1e-4</span>\n    tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lr<span class=\"pl-pds\">\"</span></span>, lr)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the D train op</span>\n    train_d <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(lr, <span class=\"pl-v\">beta1</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>).minimize(\n        d_loss, <span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>D.trainable_variables\n    )\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the G + E train ops (the models can be trained</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> the same step, but separately)</span>\n    train_g <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(lr, <span class=\"pl-v\">beta1</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>).minimize(\n        g_loss, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step, <span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>G.trainable_variables\n    )\n\n    train_e <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(lr, <span class=\"pl-v\">beta1</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>).minimize(\n        e_loss, <span class=\"pl-v\">var_list</span><span class=\"pl-k\">=</span>E.trainable_variables\n    )\n\n    log_dir <span class=\"pl-k\">=</span> <span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">logs/test</span><span class=\"pl-pds\">\"</span>\n    summary_op <span class=\"pl-k\">=</span> tf.summary.merge_all()\n\n    scaffold <span class=\"pl-k\">=</span> tf.train.Scaffold()\n    config <span class=\"pl-k\">=</span> tf.ConfigProto()\n    config.gpu_options.allow_growth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n    session_creator <span class=\"pl-k\">=</span> tf.train.ChiefSessionCreator(\n        <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config, <span class=\"pl-v\">scaffold</span><span class=\"pl-k\">=</span>scaffold, <span class=\"pl-v\">checkpoint_dir</span><span class=\"pl-k\">=</span>log_dir\n    )\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_loop</span>(<span class=\"pl-smi\">func</span>: Callable) -&gt; <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">        Execute func for the specified number of epochs or max_steps.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Args:</span>\n<span class=\"pl-s\">            func: callable to loop</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">        Returns:</span>\n<span class=\"pl-s\">            None.</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n        <span class=\"pl-k\">try</span>:\n            <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n                func()\n        <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n            <span class=\"pl-k\">pass</span>\n\n    <span class=\"pl-k\">with</span> tf.train.MonitoredSession(\n        <span class=\"pl-v\">session_creator</span><span class=\"pl-k\">=</span>session_creator,\n        <span class=\"pl-v\">hooks</span><span class=\"pl-k\">=</span>[\n            tf.train.CheckpointSaverHook(log_dir, <span class=\"pl-v\">save_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">scaffold</span><span class=\"pl-k\">=</span>scaffold)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> tf.train.ProfilerHook(save_steps=1000, output_dir=log_dir),</span>\n        ],\n    ) <span class=\"pl-k\">as</span> sess:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get the summary writer.</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> The rational behind using the writer (from the scaffold)</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> and not using the SummarySaverHook is that we want to log</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> every X steps the output of G, G(E(x)) and x</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> But since we need to use placeholders to feed the same data</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> to G, D and E, we can't use the Hook, because the first</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> sess.run on the data, will trigger the summary save op</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> and the summary save op needs the data from the placeholder</span>\n        writer <span class=\"pl-k\">=</span> SummaryWriterCache.get(log_dir)\n\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">_train</span>():\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> First create the input, that must be shared between the 2</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> training iteration</span>\n            real, noise <span class=\"pl-k\">=</span> sess.run([x_, z_])\n            feed_dict <span class=\"pl-k\">=</span> {x: real, z: noise}\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> train D</span>\n            d_gz_, d_x, _, d_loss_value <span class=\"pl-k\">=</span> sess.run(\n                [D_Gz, D_x, train_d, d_loss], feed_dict\n            )\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> train G+E</span>\n            _, g_loss_value, _, e_loss_value, step <span class=\"pl-k\">=</span> sess.run(\n                [train_g, g_loss, train_e, e_loss, global_step], feed_dict\n            )\n\n            <span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                summaries <span class=\"pl-k\">=</span> sess.run(summary_op, feed_dict)\n                <span class=\"pl-c1\">print</span>(\n                    <span class=\"pl-s\">f</span><span class=\"pl-pds\">\"</span><span class=\"pl-s\">[</span><span class=\"pl-c1\">{</span>step<span class=\"pl-c1\">}</span><span class=\"pl-s\">] d: </span><span class=\"pl-c1\">{</span>d_loss_value<span class=\"pl-c1\">}</span><span class=\"pl-s\"> - g: </span><span class=\"pl-c1\">{</span>g_loss_value<span class=\"pl-c1\">}</span><span class=\"pl-s\"> - e: </span><span class=\"pl-c1\">{</span>e_loss_value<span class=\"pl-c1\">}</span><span class=\"pl-pds\">\"</span>\n                )\n                <span class=\"pl-c1\">print</span>(np.mean(d_gz_), np.mean(d_x))\n                writer.add_summary(summaries, step)\n                writer.flush()\n\n        _loop(_train)\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">0</span>\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    sys.exit(train())</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.12\nPython version: 3.6\nCUDA/cuDNN version: cuda 9.0, cudnn 7.1\nGPU model and memory: nvidia 1080ti\n\nDescribe the current behavior\nI'm migrating my codebase to a Tensorflow 2.0 compatible version, thus I'm replacing all the tf.layers call with their tf.keras.layers counterpart.\nI want to continue using static graphs and MonitoredSession to train my models (since I do have all the input pipelines defined with tf.data.Dataset and all the training script defined to work in this way).\nHowever, there's a huge problem when a layer adds some update operation (like the BatchNormalization layer).\nThe actual behavior is that the update operations (of moving mean and variance) are not called when training the model.\nThis can be OK in a full-keras solution, where the connection among models, the update operations and so on, are managed by the train_on_batch + model.compile + model.fit call (that do some magic in the background).\nIn fact, @fchollet said this is by design: #19643 (comment)\nBut since I don't want to migrate to a full keras-based solution, how can I handle the updates?\nI found some theoretical workaround (collected the update operations model.updates + collect the inputs model.inputs, loop over the inputs, feed the correct input and execute with sess.run the updates), but those are really ugly and they don't work: I can trigger the parameter update, but the time-step of the update execution is wrong and the solution does not converge: moreover, when the model becomes complex (like in the example of BiGAN below) it can be a real mess and the code become unmaintenable.\nDescribe the expected behavior\nWhen I'm using keras layers to define the models and I'm not calling train_on_batch or compile or any other method to train the model that's pure keras, the update operations should be placed into the graph (having thus the same behavior of the batch normalization layer present in tf.layers) and executed when model .trainable is True.\nMoreover, there's another strange behavour: when I define a model output passing a new input (hence I invoke the Model.call method) the update operation have no idea of this new input.\nProbably, the .call method shouln't just return the output tensor, but it should return a new Model that shared the same parameters of the orignial one, but defined with a new input (and thus with its update ops aware of the new input).\nCode to reproduce the issue\nA BiGAN implementation.\nfrom typing import Tuple, Callable, Any, Optional\nimport multiprocessing\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras as k\nfrom tensorflow.python.training.summary_io import SummaryWriterCache\n\n\ndef dataset(\n    shape: Tuple[int, int],\n    batch_size: int = 32,\n    epochs: int = None,\n    train: bool = False,\n    _batch=True,\n) -> tf.data.Dataset:\n    \"\"\"Returns the dataset correcly batched and resized\n    Args:\n        shape: The output shape of the images in the Dataset\n        batch_size: The size of the batch to return at each invocation\n        epochs: The the number of times that the dataset will repeat itself\n                before throwing an exception\n        train: when True, returns the shuffled train dataset, when False returns\n               the test, not shuffled, dataset\n        _batch: private, do not use\n    Returns:\n        The dataset\n    \"\"\"\n\n    def _process(image, label):\n        \"\"\"The function used to resize the image to the specified shape.\n        Used in the tf.data.Dataset.map function\n        Args:\n            image: the input image\n            label: the input label\n        Return:\n            resized_image, label\n        \"\"\"\n        nonlocal shape\n        image = tf.image.resize_images(\n            tf.expand_dims(image, axis=0), shape, tf.image.ResizeMethod.NEAREST_NEIGHBOR\n        )\n        image = tf.cast(image, tf.float32)\n        image = tf.squeeze(image, axis=[0])\n        return image, label\n\n    (train_images, train_labels), (\n        test_images,\n        test_labels,\n    ) = k.datasets.cifar10.load_data()\n    if not train:\n        train_images = test_images\n        train_labels = test_labels\n    train_images = (train_images - 127.5) / 127.5\n\n    def _generator():\n        r\"\"\"The generator that returns the pair image,label\n        This must be used in order to don't use tf.data.Dataset.from_tensor_slices.abs\n        In this way, we can build a dataset from a generator and solve the problem of huge\n        graphs created by from_tensor_slices (it creates a constant in the graph :\\)\n        \"\"\"\n        for image, label in zip(train_images, train_labels):\n            yield image, label\n\n    def _set_shape(image, label):\n        \"\"\"Set the static + dynamic shape of the image, in order to correctly build the\n        input pipeline in both phases\n        Args:\n            image: the input image\n            label: the input label\n        Return:\n            image, label\n        \"\"\"\n        image.set_shape((32, 32, 3))  # static\n        image = tf.reshape(image, (32, 32, 3))  # dynamic\n        return image, label\n\n    _dataset = tf.data.Dataset.from_generator(\n        _generator, (tf.float32, tf.int32)\n    )  # unkown shape\n    _dataset = _dataset.map(\n        _set_shape, num_parallel_calls=multiprocessing.cpu_count()\n    )  # known static chsape\n\n    _dataset = _dataset.map(\n        _process, num_parallel_calls=multiprocessing.cpu_count()\n    )  # resize to desired input shape\n\n    if _batch:\n        _dataset = _dataset.batch(batch_size, drop_remainder=True)\n        if epochs:\n            _dataset = _dataset.repeat(epochs)\n    elif epochs:\n        _dataset = _dataset.repeat(epochs)\n\n    _dataset = _dataset.prefetch(1)\n    return _dataset\n\n\nKERNEL_INITIALIZER = k.initializers.RandomNormal(mean=0.0, stddev=0.02)\nALMOST_ONE = k.initializers.RandomNormal(mean=1.0, stddev=0.02)\n\n\ndef discriminator(\n    visual_shape: tf.TensorShape,\n    encoding_shape: tf.TensorShape,\n    conditioning: Optional[Any] = None,\n    l2_penalty: float = 0.0,\n) -> k.Model:\n    \"\"\"\n    Build the Discriminator model.\n\n    Returns a k.Model with 2 inputs and a single output.\n    The inputs are an image and its encoded/latent representation.\n\n    Args:\n        visual_shape: The shape of the visual input, 3D tensor\n        encoding_shape: The shape of the latent/encoded representation\n        # NOT IMPLEMENTED: Conditioning: data used as GAN conditioning\n        # UNUSED: l2_penalty: l2 regularization strength\n\n    Returns:\n        The discriminator model.\n\n    \"\"\"\n    kernel_size = (5, 5)\n\n    # Inputs\n    # (64, 64, C)\n    # (Latent Dimension, )\n    input_visual = k.layers.Input(shape=visual_shape)\n    input_encoding = k.layers.Input(shape=encoding_shape)\n\n    # Data\n    # ### Layer 0\n    # (64, 64, 32)\n    visual = k.layers.Conv2D(\n        filters=32,\n        kernel_size=kernel_size,\n        strides=(1, 1),\n        padding=\"same\",\n        kernel_initializer=KERNEL_INITIALIZER,\n        kernel_regularizer=k.regularizers.l2(l2_penalty),\n    )(input_visual)\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\n\n    # Data\n    # ### Layer 1\n    # (32, 32, 32)\n    visual = k.layers.Conv2D(\n        filters=32,\n        kernel_size=kernel_size,\n        strides=(2, 2),\n        padding=\"same\",\n        kernel_initializer=KERNEL_INITIALIZER,\n        kernel_regularizer=k.regularizers.l2(l2_penalty),\n    )(visual)\n    visual = k.layers.BatchNormalization()(visual)\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\n    visual = k.layers.Dropout(rate=0.5)(visual)\n\n    # ### Layer 2\n    # (16, 16, 64)\n    visual = k.layers.Conv2D(\n        filters=64,\n        kernel_size=kernel_size,\n        strides=(2, 2),\n        padding=\"same\",\n        kernel_initializer=KERNEL_INITIALIZER,\n    )(visual)\n    visual = k.layers.BatchNormalization()(visual)\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\n    visual = k.layers.Dropout(rate=0.5)(visual)\n\n    # Flatten\n    visual = k.layers.Flatten()(visual)\n\n    # Encoding\n    # ### Layer 5 D(z)\n    # (512,)\n    encoding = k.layers.Dense(units=512, kernel_initializer=KERNEL_INITIALIZER)(\n        input_encoding\n    )\n    encoding = k.layers.LeakyReLU(alpha=0.1)(encoding)\n    encoding = k.layers.Dropout(rate=0.5)(encoding)\n\n    # Data + Encoding\n    # ### Layer 6 D(x, z)\n    # (4608,)\n    mixed = k.layers.Concatenate()([visual, encoding])\n    mixed = k.layers.Dense(units=1024, kernel_initializer=KERNEL_INITIALIZER)(mixed)\n    mixed = k.layers.LeakyReLU(alpha=0.1)(mixed)\n    mixed = k.layers.Dropout(rate=0.5)(mixed)\n    features = mixed\n\n    # Final Step\n    # ### Layer 7\n    # (1)\n    out = k.layers.Dense(1, kernel_initializer=KERNEL_INITIALIZER)(mixed)\n\n    # Use the functional interface\n    model = k.Model(inputs=[input_visual, input_encoding], outputs=[out, features])\n    model.summary()\n    return model\n\n\ndef generator(\n    input_shape: int,\n    output_depth: int = 3,\n    conditioning: Optional[Any] = None,\n    l2_penalty: float = 0.0,\n) -> k.Model:\n    \"\"\"\n    Build the Generator model.\n\n    Given a latent representation, generates a meaningful image.\n    The input shape must be in the form of a vector 1x1xD.\n\n    Args:\n        input_shape: The shape of the noise prior\n        output_depth: The number of channels of the generated image\n        # NOT IMPLEMENTED: Conditioning: data used as GAN conditioning\n        l2_penalty: l2 regularization strength\n\n    Returns:\n        The Generator model.\n\n    \"\"\"\n    kernel_size = (5, 5)\n    model = k.Sequential(name=\"generator\")\n\n    # Project and Reshape the latent space\n    # ### Layer 1\n    # (4*4*64,)\n    model.add(\n        k.layers.Dense(\n            units=4 * 4 * 64,\n            kernel_initializer=KERNEL_INITIALIZER,\n            input_shape=input_shape,\n            kernel_regularizer=k.regularizers.l2(l2_penalty),\n        )\n    )\n    model.add(k.layers.Activation(k.activations.relu))\n    model.add(k.layers.Reshape((4, 4, 64)))\n\n    # Starting Deconvolutions\n    # ### Layer 2\n    # (8, 8, 64)\n    model.add(\n        k.layers.Conv2DTranspose(\n            filters=64,\n            kernel_size=kernel_size,\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=KERNEL_INITIALIZER,\n        )\n    )\n    model.add(k.layers.BatchNormalization())\n    model.add(k.layers.Activation(k.activations.relu))\n\n    # Starting Deconvolutions\n    # ### Layer 3\n    # (16, 16, 128)\n    model.add(\n        k.layers.Conv2DTranspose(\n            filters=128,\n            kernel_size=kernel_size,\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=KERNEL_INITIALIZER,\n        )\n    )\n    model.add(k.layers.BatchNormalization())\n    model.add(k.layers.Activation(k.activations.relu))\n\n    # ### Layer 4\n    # (32, 32, 256)\n    model.add(\n        k.layers.Conv2DTranspose(\n            filters=256,\n            kernel_size=kernel_size,\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=KERNEL_INITIALIZER,\n        )\n    )\n    model.add(k.layers.BatchNormalization())\n    model.add(k.layers.Activation(k.activations.relu))\n\n    # ### Layer 5\n    # (64, 64, C)\n    model.add(\n        k.layers.Conv2DTranspose(\n            filters=output_depth,\n            kernel_size=kernel_size,\n            strides=(2, 2),\n            padding=\"same\",\n            kernel_initializer=KERNEL_INITIALIZER,\n        )\n    )\n    model.add(k.layers.Activation(k.activations.tanh))  # G(z) in [-1,1]\n\n    model.summary()\n    return model\n\n\ndef encoder(\n    visual_shape: int, latent_dimension: int, l2_penalty: float = 0.0\n) -> k.Model:\n    \"\"\"\n    Build the Encoder model.\n\n    The encoder encodes the input in a vector with shape 1x1xlatent_dimension.\n\n    Args:\n        visual_shape: The shape of the input to encode\n        latent_dimension: The number of dimensions (along the depth) to use.\n        # NOT IMPLEMENTED: conditioning: Data used as GAN conditioning\n        l2_penalty: l2 regularization strength\n\n    Returns:\n        The Encoder model.\n\n    \"\"\"\n\n    kernel_size = (5, 5)\n\n    # Inputs\n    # (64, 64, C)\n    # (Latent Dimension, )\n    input_visual = k.layers.Input(shape=visual_shape)\n\n    # Data\n    # ### Layer 0\n    # (64, 64, 32)\n    visual = k.layers.Conv2D(\n        filters=32,\n        kernel_size=kernel_size,\n        strides=(1, 1),\n        padding=\"same\",\n        kernel_initializer=KERNEL_INITIALIZER,\n        kernel_regularizer=k.regularizers.l2(l2_penalty),\n    )(input_visual)\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\n\n    # Data\n    # ### Layer 1\n    # (32, 32, 32)\n    visual = k.layers.Conv2D(\n        filters=32,\n        kernel_size=kernel_size,\n        strides=(2, 2),\n        padding=\"same\",\n        kernel_initializer=KERNEL_INITIALIZER,\n        kernel_regularizer=k.regularizers.l2(l2_penalty),\n    )(visual)\n    visual = k.layers.BatchNormalization()(visual)\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\n    visual = k.layers.Dropout(rate=0.5)(visual)\n\n    # ### Layer 2\n    # (16, 16, 64)\n    visual = k.layers.Conv2D(\n        filters=128,\n        kernel_size=kernel_size,\n        strides=(2, 2),\n        padding=\"same\",\n        kernel_initializer=KERNEL_INITIALIZER,\n    )(visual)\n    visual = k.layers.BatchNormalization()(visual)\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\n    visual = k.layers.Dropout(rate=0.5)(visual)\n\n    # Flatten\n    visual = k.layers.Flatten()(visual)\n\n    # Encoding\n    # (Latent space, )\n    # ### Layer 5\n    visual = k.layers.Dense(\n        units=latent_dimension, kernel_initializer=KERNEL_INITIALIZER\n    )(visual)\n\n    model = k.Model(inputs=input_visual, outputs=visual)\n    model.summary()\n    return model\n\n\ndef bce(x: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n    \"\"\"Returns the discrete binary cross entropy between x and the discrete label\n    Args:\n        x: a 2D tensor\n        label: the discrite label, aka, the distribution to match\n\n    Returns:\n        The binary cros entropy\n    \"\"\"\n    assert len(x.shape) == 2 and len(label.shape) == 0\n\n    return tf.losses.sigmoid_cross_entropy(tf.ones_like(x) * label, x)\n\n\ndef min_max(positive: tf.Tensor, negative: tf.Tensor) -> tf.Tensor:\n    \"\"\"Returns the discriminator (min max) loss\n    Args:\n        positive: the discriminator output for the positive class: 2D tensor\n        negative: the discriminator output for the negative class: 2D tensor\n    Returns:\n        The sum of 2 BCE\n    \"\"\"\n    one = tf.constant(1.0)\n    zero = tf.constant(0.0)\n    d_loss = bce(positive, one) + bce(negative, zero)\n    return d_loss\n\n\ndef train():\n    \"\"\"Train the GAN.\"\"\"\n    batch_size = 32\n    epochs = 100\n    latent_dimension = 100\n    l2_penalty = 0.0\n\n    x_, y_ = dataset((64, 64), batch_size, epochs).make_one_shot_iterator().get_next()\n\n    x = tf.placeholder(tf.float32, list(x_.shape))\n    tf.summary.image(\"x\", x, max_outputs=3)\n\n    # Define the Models\n    E = encoder(x.shape[1:], latent_dimension, l2_penalty)\n\n    z_ = tf.random_normal([batch_size, latent_dimension], mean=0.0, stddev=1.0)\n\n    z = tf.placeholder(tf.float32, list(z_.shape))\n    G = generator(z.shape[1:], x.shape[-1].value, l2_penalty)\n    D = discriminator(x.shape[1:], E.output.shape[1:], l2_penalty)\n\n    # Generate from latent representation z\n    G_z = G(z)\n    tf.summary.image(\"G(z)\", G_z, max_outputs=3)\n\n    # encode x to a latent representation\n    E_x = E(x)\n\n    G_Ex = G(E_x)\n    tf.summary.image(\"G(E(x))\", G_Ex, max_outputs=3)\n\n    # plot image difference\n    tf.summary.image(\n        \"G(E(x)) - x\", tf.norm(G_Ex - x_, axis=3, keepdims=True), max_outputs=3\n    )\n\n    # The output of the discriminator is a bs,n,n,value\n    # hence flatten all the values of the map and compute\n    # the loss element wise\n    D_Gz, F_Gz = D(inputs=[G_z, z])\n    D_x, F_x = D(inputs=[x, E_x])\n    D_Gz = k.layers.Flatten()(D_Gz)\n    F_Gz = k.layers.Flatten()(F_Gz)\n    D_x = k.layers.Flatten()(D_x)\n    F_x = k.layers.Flatten()(F_x)\n\n    ## Discriminator\n    d_loss = min_max(D_x, D_Gz)\n\n    ## Generator\n    g_loss = bce(D_Gz, tf.constant(1.0))\n    # Encoder\n    e_loss = bce(D_x, tf.constant(0.0))\n\n    # add regularizations defined in the keras layers\n    d_loss += tf.add_n(D.losses)\n    e_loss += tf.add_n(E.losses)\n    g_loss += tf.add_n(G.losses)\n\n    tf.summary.scalar(\"d_loss\", d_loss)\n    tf.summary.scalar(\"g_loss\", g_loss)\n    tf.summary.scalar(\"e_loss\", e_loss)\n\n    global_step = tf.train.get_or_create_global_step()\n\n    lr = 1e-4\n    tf.summary.scalar(\"lr\", lr)\n\n    # Define the D train op\n    train_d = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(\n        d_loss, var_list=D.trainable_variables\n    )\n\n    # Define the G + E train ops (the models can be trained\n    # the same step, but separately)\n    train_g = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(\n        g_loss, global_step=global_step, var_list=G.trainable_variables\n    )\n\n    train_e = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(\n        e_loss, var_list=E.trainable_variables\n    )\n\n    log_dir = f\"logs/test\"\n    summary_op = tf.summary.merge_all()\n\n    scaffold = tf.train.Scaffold()\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    session_creator = tf.train.ChiefSessionCreator(\n        config=config, scaffold=scaffold, checkpoint_dir=log_dir\n    )\n\n    def _loop(func: Callable) -> None:\n        \"\"\"\n        Execute func for the specified number of epochs or max_steps.\n\n        Args:\n            func: callable to loop\n\n        Returns:\n            None.\n        \"\"\"\n        try:\n            while True:\n                func()\n        except tf.errors.OutOfRangeError:\n            pass\n\n    with tf.train.MonitoredSession(\n        session_creator=session_creator,\n        hooks=[\n            tf.train.CheckpointSaverHook(log_dir, save_steps=100, scaffold=scaffold)\n            # tf.train.ProfilerHook(save_steps=1000, output_dir=log_dir),\n        ],\n    ) as sess:\n        # Get the summary writer.\n        # The rational behind using the writer (from the scaffold)\n        # and not using the SummarySaverHook is that we want to log\n        # every X steps the output of G, G(E(x)) and x\n        # But since we need to use placeholders to feed the same data\n        # to G, D and E, we can't use the Hook, because the first\n        # sess.run on the data, will trigger the summary save op\n        # and the summary save op needs the data from the placeholder\n        writer = SummaryWriterCache.get(log_dir)\n\n        def _train():\n            # First create the input, that must be shared between the 2\n            # training iteration\n            real, noise = sess.run([x_, z_])\n            feed_dict = {x: real, z: noise}\n\n            # train D\n            d_gz_, d_x, _, d_loss_value = sess.run(\n                [D_Gz, D_x, train_d, d_loss], feed_dict\n            )\n\n            # train G+E\n            _, g_loss_value, _, e_loss_value, step = sess.run(\n                [train_g, g_loss, train_e, e_loss, global_step], feed_dict\n            )\n\n            if step % 100 == 0:\n                summaries = sess.run(summary_op, feed_dict)\n                print(\n                    f\"[{step}] d: {d_loss_value} - g: {g_loss_value} - e: {e_loss_value}\"\n                )\n                print(np.mean(d_gz_), np.mean(d_x))\n                writer.add_summary(summaries, step)\n                writer.flush()\n\n        _loop(_train)\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(train())", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: cuda 9.0, cudnn 7.1\r\n- GPU model and memory: nvidia 1080ti\r\n\r\n**Describe the current behavior**\r\n\r\nI'm migrating my codebase to a Tensorflow 2.0 compatible version, thus I'm replacing all the `tf.layers` call with their `tf.keras.layers` counterpart.\r\n\r\nI want to continue using static graphs and MonitoredSession to train my models (since I do have all the input pipelines defined with tf.data.Dataset and all the training script defined to work in this way).\r\n\r\nHowever, there's a huge problem when a layer adds some update operation (like the BatchNormalization layer).\r\n\r\nThe actual behavior is that the update operations (of moving mean and variance) are not called when training the model.\r\n\r\nThis can be OK in a full-keras solution, where the connection among models, the update operations and so on, are managed by the `train_on_batch` + `model.compile` + `model.fit` call (that do some magic in the background).\r\nIn fact, @fchollet said this is by design: https://github.com/tensorflow/tensorflow/issues/19643#issuecomment-394527486\r\n\r\nBut since I don't want to migrate to a full keras-based solution, how can I handle the updates?\r\n\r\nI found some theoretical workaround (collected the update operations `model.updates` + collect the inputs `model.inputs`, loop over the inputs, feed the correct input and execute with sess.run the updates), but those are really ugly and they don't work: I can trigger the parameter update, but the time-step of the update execution is wrong and the solution does not converge: moreover, when the model becomes complex (like in the example of BiGAN below) it can be a real mess and the code become unmaintenable.\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen I'm using keras layers to define the models and I'm not calling `train_on_batch` or `compile` or any other method to train the model that's pure keras, the update operations should be placed into the graph (having thus the same behavior of the batch normalization layer present in `tf.layers`) and executed when model `.trainable` is `True`.\r\n\r\nMoreover, there's another strange behavour: when I define a model output passing a new input (hence I invoke the `Model.call` method) the update operation have no idea of this new input.\r\n\r\nProbably, the `.call` method shouln't just return the output tensor, but it should return a new `Model` that shared the same parameters of the orignial one, but defined with a new input (and thus with its update ops aware of the new input).\r\n\r\n**Code to reproduce the issue**\r\nA BiGAN implementation.\r\n\r\n```python\r\nfrom typing import Tuple, Callable, Any, Optional\r\nimport multiprocessing\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras as k\r\nfrom tensorflow.python.training.summary_io import SummaryWriterCache\r\n\r\n\r\ndef dataset(\r\n    shape: Tuple[int, int],\r\n    batch_size: int = 32,\r\n    epochs: int = None,\r\n    train: bool = False,\r\n    _batch=True,\r\n) -> tf.data.Dataset:\r\n    \"\"\"Returns the dataset correcly batched and resized\r\n    Args:\r\n        shape: The output shape of the images in the Dataset\r\n        batch_size: The size of the batch to return at each invocation\r\n        epochs: The the number of times that the dataset will repeat itself\r\n                before throwing an exception\r\n        train: when True, returns the shuffled train dataset, when False returns\r\n               the test, not shuffled, dataset\r\n        _batch: private, do not use\r\n    Returns:\r\n        The dataset\r\n    \"\"\"\r\n\r\n    def _process(image, label):\r\n        \"\"\"The function used to resize the image to the specified shape.\r\n        Used in the tf.data.Dataset.map function\r\n        Args:\r\n            image: the input image\r\n            label: the input label\r\n        Return:\r\n            resized_image, label\r\n        \"\"\"\r\n        nonlocal shape\r\n        image = tf.image.resize_images(\r\n            tf.expand_dims(image, axis=0), shape, tf.image.ResizeMethod.NEAREST_NEIGHBOR\r\n        )\r\n        image = tf.cast(image, tf.float32)\r\n        image = tf.squeeze(image, axis=[0])\r\n        return image, label\r\n\r\n    (train_images, train_labels), (\r\n        test_images,\r\n        test_labels,\r\n    ) = k.datasets.cifar10.load_data()\r\n    if not train:\r\n        train_images = test_images\r\n        train_labels = test_labels\r\n    train_images = (train_images - 127.5) / 127.5\r\n\r\n    def _generator():\r\n        r\"\"\"The generator that returns the pair image,label\r\n        This must be used in order to don't use tf.data.Dataset.from_tensor_slices.abs\r\n        In this way, we can build a dataset from a generator and solve the problem of huge\r\n        graphs created by from_tensor_slices (it creates a constant in the graph :\\)\r\n        \"\"\"\r\n        for image, label in zip(train_images, train_labels):\r\n            yield image, label\r\n\r\n    def _set_shape(image, label):\r\n        \"\"\"Set the static + dynamic shape of the image, in order to correctly build the\r\n        input pipeline in both phases\r\n        Args:\r\n            image: the input image\r\n            label: the input label\r\n        Return:\r\n            image, label\r\n        \"\"\"\r\n        image.set_shape((32, 32, 3))  # static\r\n        image = tf.reshape(image, (32, 32, 3))  # dynamic\r\n        return image, label\r\n\r\n    _dataset = tf.data.Dataset.from_generator(\r\n        _generator, (tf.float32, tf.int32)\r\n    )  # unkown shape\r\n    _dataset = _dataset.map(\r\n        _set_shape, num_parallel_calls=multiprocessing.cpu_count()\r\n    )  # known static chsape\r\n\r\n    _dataset = _dataset.map(\r\n        _process, num_parallel_calls=multiprocessing.cpu_count()\r\n    )  # resize to desired input shape\r\n\r\n    if _batch:\r\n        _dataset = _dataset.batch(batch_size, drop_remainder=True)\r\n        if epochs:\r\n            _dataset = _dataset.repeat(epochs)\r\n    elif epochs:\r\n        _dataset = _dataset.repeat(epochs)\r\n\r\n    _dataset = _dataset.prefetch(1)\r\n    return _dataset\r\n\r\n\r\nKERNEL_INITIALIZER = k.initializers.RandomNormal(mean=0.0, stddev=0.02)\r\nALMOST_ONE = k.initializers.RandomNormal(mean=1.0, stddev=0.02)\r\n\r\n\r\ndef discriminator(\r\n    visual_shape: tf.TensorShape,\r\n    encoding_shape: tf.TensorShape,\r\n    conditioning: Optional[Any] = None,\r\n    l2_penalty: float = 0.0,\r\n) -> k.Model:\r\n    \"\"\"\r\n    Build the Discriminator model.\r\n\r\n    Returns a k.Model with 2 inputs and a single output.\r\n    The inputs are an image and its encoded/latent representation.\r\n\r\n    Args:\r\n        visual_shape: The shape of the visual input, 3D tensor\r\n        encoding_shape: The shape of the latent/encoded representation\r\n        # NOT IMPLEMENTED: Conditioning: data used as GAN conditioning\r\n        # UNUSED: l2_penalty: l2 regularization strength\r\n\r\n    Returns:\r\n        The discriminator model.\r\n\r\n    \"\"\"\r\n    kernel_size = (5, 5)\r\n\r\n    # Inputs\r\n    # (64, 64, C)\r\n    # (Latent Dimension, )\r\n    input_visual = k.layers.Input(shape=visual_shape)\r\n    input_encoding = k.layers.Input(shape=encoding_shape)\r\n\r\n    # Data\r\n    # ### Layer 0\r\n    # (64, 64, 32)\r\n    visual = k.layers.Conv2D(\r\n        filters=32,\r\n        kernel_size=kernel_size,\r\n        strides=(1, 1),\r\n        padding=\"same\",\r\n        kernel_initializer=KERNEL_INITIALIZER,\r\n        kernel_regularizer=k.regularizers.l2(l2_penalty),\r\n    )(input_visual)\r\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\r\n\r\n    # Data\r\n    # ### Layer 1\r\n    # (32, 32, 32)\r\n    visual = k.layers.Conv2D(\r\n        filters=32,\r\n        kernel_size=kernel_size,\r\n        strides=(2, 2),\r\n        padding=\"same\",\r\n        kernel_initializer=KERNEL_INITIALIZER,\r\n        kernel_regularizer=k.regularizers.l2(l2_penalty),\r\n    )(visual)\r\n    visual = k.layers.BatchNormalization()(visual)\r\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\r\n    visual = k.layers.Dropout(rate=0.5)(visual)\r\n\r\n    # ### Layer 2\r\n    # (16, 16, 64)\r\n    visual = k.layers.Conv2D(\r\n        filters=64,\r\n        kernel_size=kernel_size,\r\n        strides=(2, 2),\r\n        padding=\"same\",\r\n        kernel_initializer=KERNEL_INITIALIZER,\r\n    )(visual)\r\n    visual = k.layers.BatchNormalization()(visual)\r\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\r\n    visual = k.layers.Dropout(rate=0.5)(visual)\r\n\r\n    # Flatten\r\n    visual = k.layers.Flatten()(visual)\r\n\r\n    # Encoding\r\n    # ### Layer 5 D(z)\r\n    # (512,)\r\n    encoding = k.layers.Dense(units=512, kernel_initializer=KERNEL_INITIALIZER)(\r\n        input_encoding\r\n    )\r\n    encoding = k.layers.LeakyReLU(alpha=0.1)(encoding)\r\n    encoding = k.layers.Dropout(rate=0.5)(encoding)\r\n\r\n    # Data + Encoding\r\n    # ### Layer 6 D(x, z)\r\n    # (4608,)\r\n    mixed = k.layers.Concatenate()([visual, encoding])\r\n    mixed = k.layers.Dense(units=1024, kernel_initializer=KERNEL_INITIALIZER)(mixed)\r\n    mixed = k.layers.LeakyReLU(alpha=0.1)(mixed)\r\n    mixed = k.layers.Dropout(rate=0.5)(mixed)\r\n    features = mixed\r\n\r\n    # Final Step\r\n    # ### Layer 7\r\n    # (1)\r\n    out = k.layers.Dense(1, kernel_initializer=KERNEL_INITIALIZER)(mixed)\r\n\r\n    # Use the functional interface\r\n    model = k.Model(inputs=[input_visual, input_encoding], outputs=[out, features])\r\n    model.summary()\r\n    return model\r\n\r\n\r\ndef generator(\r\n    input_shape: int,\r\n    output_depth: int = 3,\r\n    conditioning: Optional[Any] = None,\r\n    l2_penalty: float = 0.0,\r\n) -> k.Model:\r\n    \"\"\"\r\n    Build the Generator model.\r\n\r\n    Given a latent representation, generates a meaningful image.\r\n    The input shape must be in the form of a vector 1x1xD.\r\n\r\n    Args:\r\n        input_shape: The shape of the noise prior\r\n        output_depth: The number of channels of the generated image\r\n        # NOT IMPLEMENTED: Conditioning: data used as GAN conditioning\r\n        l2_penalty: l2 regularization strength\r\n\r\n    Returns:\r\n        The Generator model.\r\n\r\n    \"\"\"\r\n    kernel_size = (5, 5)\r\n    model = k.Sequential(name=\"generator\")\r\n\r\n    # Project and Reshape the latent space\r\n    # ### Layer 1\r\n    # (4*4*64,)\r\n    model.add(\r\n        k.layers.Dense(\r\n            units=4 * 4 * 64,\r\n            kernel_initializer=KERNEL_INITIALIZER,\r\n            input_shape=input_shape,\r\n            kernel_regularizer=k.regularizers.l2(l2_penalty),\r\n        )\r\n    )\r\n    model.add(k.layers.Activation(k.activations.relu))\r\n    model.add(k.layers.Reshape((4, 4, 64)))\r\n\r\n    # Starting Deconvolutions\r\n    # ### Layer 2\r\n    # (8, 8, 64)\r\n    model.add(\r\n        k.layers.Conv2DTranspose(\r\n            filters=64,\r\n            kernel_size=kernel_size,\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=KERNEL_INITIALIZER,\r\n        )\r\n    )\r\n    model.add(k.layers.BatchNormalization())\r\n    model.add(k.layers.Activation(k.activations.relu))\r\n\r\n    # Starting Deconvolutions\r\n    # ### Layer 3\r\n    # (16, 16, 128)\r\n    model.add(\r\n        k.layers.Conv2DTranspose(\r\n            filters=128,\r\n            kernel_size=kernel_size,\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=KERNEL_INITIALIZER,\r\n        )\r\n    )\r\n    model.add(k.layers.BatchNormalization())\r\n    model.add(k.layers.Activation(k.activations.relu))\r\n\r\n    # ### Layer 4\r\n    # (32, 32, 256)\r\n    model.add(\r\n        k.layers.Conv2DTranspose(\r\n            filters=256,\r\n            kernel_size=kernel_size,\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=KERNEL_INITIALIZER,\r\n        )\r\n    )\r\n    model.add(k.layers.BatchNormalization())\r\n    model.add(k.layers.Activation(k.activations.relu))\r\n\r\n    # ### Layer 5\r\n    # (64, 64, C)\r\n    model.add(\r\n        k.layers.Conv2DTranspose(\r\n            filters=output_depth,\r\n            kernel_size=kernel_size,\r\n            strides=(2, 2),\r\n            padding=\"same\",\r\n            kernel_initializer=KERNEL_INITIALIZER,\r\n        )\r\n    )\r\n    model.add(k.layers.Activation(k.activations.tanh))  # G(z) in [-1,1]\r\n\r\n    model.summary()\r\n    return model\r\n\r\n\r\ndef encoder(\r\n    visual_shape: int, latent_dimension: int, l2_penalty: float = 0.0\r\n) -> k.Model:\r\n    \"\"\"\r\n    Build the Encoder model.\r\n\r\n    The encoder encodes the input in a vector with shape 1x1xlatent_dimension.\r\n\r\n    Args:\r\n        visual_shape: The shape of the input to encode\r\n        latent_dimension: The number of dimensions (along the depth) to use.\r\n        # NOT IMPLEMENTED: conditioning: Data used as GAN conditioning\r\n        l2_penalty: l2 regularization strength\r\n\r\n    Returns:\r\n        The Encoder model.\r\n\r\n    \"\"\"\r\n\r\n    kernel_size = (5, 5)\r\n\r\n    # Inputs\r\n    # (64, 64, C)\r\n    # (Latent Dimension, )\r\n    input_visual = k.layers.Input(shape=visual_shape)\r\n\r\n    # Data\r\n    # ### Layer 0\r\n    # (64, 64, 32)\r\n    visual = k.layers.Conv2D(\r\n        filters=32,\r\n        kernel_size=kernel_size,\r\n        strides=(1, 1),\r\n        padding=\"same\",\r\n        kernel_initializer=KERNEL_INITIALIZER,\r\n        kernel_regularizer=k.regularizers.l2(l2_penalty),\r\n    )(input_visual)\r\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\r\n\r\n    # Data\r\n    # ### Layer 1\r\n    # (32, 32, 32)\r\n    visual = k.layers.Conv2D(\r\n        filters=32,\r\n        kernel_size=kernel_size,\r\n        strides=(2, 2),\r\n        padding=\"same\",\r\n        kernel_initializer=KERNEL_INITIALIZER,\r\n        kernel_regularizer=k.regularizers.l2(l2_penalty),\r\n    )(visual)\r\n    visual = k.layers.BatchNormalization()(visual)\r\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\r\n    visual = k.layers.Dropout(rate=0.5)(visual)\r\n\r\n    # ### Layer 2\r\n    # (16, 16, 64)\r\n    visual = k.layers.Conv2D(\r\n        filters=128,\r\n        kernel_size=kernel_size,\r\n        strides=(2, 2),\r\n        padding=\"same\",\r\n        kernel_initializer=KERNEL_INITIALIZER,\r\n    )(visual)\r\n    visual = k.layers.BatchNormalization()(visual)\r\n    visual = k.layers.LeakyReLU(alpha=0.1)(visual)\r\n    visual = k.layers.Dropout(rate=0.5)(visual)\r\n\r\n    # Flatten\r\n    visual = k.layers.Flatten()(visual)\r\n\r\n    # Encoding\r\n    # (Latent space, )\r\n    # ### Layer 5\r\n    visual = k.layers.Dense(\r\n        units=latent_dimension, kernel_initializer=KERNEL_INITIALIZER\r\n    )(visual)\r\n\r\n    model = k.Model(inputs=input_visual, outputs=visual)\r\n    model.summary()\r\n    return model\r\n\r\n\r\ndef bce(x: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\r\n    \"\"\"Returns the discrete binary cross entropy between x and the discrete label\r\n    Args:\r\n        x: a 2D tensor\r\n        label: the discrite label, aka, the distribution to match\r\n\r\n    Returns:\r\n        The binary cros entropy\r\n    \"\"\"\r\n    assert len(x.shape) == 2 and len(label.shape) == 0\r\n\r\n    return tf.losses.sigmoid_cross_entropy(tf.ones_like(x) * label, x)\r\n\r\n\r\ndef min_max(positive: tf.Tensor, negative: tf.Tensor) -> tf.Tensor:\r\n    \"\"\"Returns the discriminator (min max) loss\r\n    Args:\r\n        positive: the discriminator output for the positive class: 2D tensor\r\n        negative: the discriminator output for the negative class: 2D tensor\r\n    Returns:\r\n        The sum of 2 BCE\r\n    \"\"\"\r\n    one = tf.constant(1.0)\r\n    zero = tf.constant(0.0)\r\n    d_loss = bce(positive, one) + bce(negative, zero)\r\n    return d_loss\r\n\r\n\r\ndef train():\r\n    \"\"\"Train the GAN.\"\"\"\r\n    batch_size = 32\r\n    epochs = 100\r\n    latent_dimension = 100\r\n    l2_penalty = 0.0\r\n\r\n    x_, y_ = dataset((64, 64), batch_size, epochs).make_one_shot_iterator().get_next()\r\n\r\n    x = tf.placeholder(tf.float32, list(x_.shape))\r\n    tf.summary.image(\"x\", x, max_outputs=3)\r\n\r\n    # Define the Models\r\n    E = encoder(x.shape[1:], latent_dimension, l2_penalty)\r\n\r\n    z_ = tf.random_normal([batch_size, latent_dimension], mean=0.0, stddev=1.0)\r\n\r\n    z = tf.placeholder(tf.float32, list(z_.shape))\r\n    G = generator(z.shape[1:], x.shape[-1].value, l2_penalty)\r\n    D = discriminator(x.shape[1:], E.output.shape[1:], l2_penalty)\r\n\r\n    # Generate from latent representation z\r\n    G_z = G(z)\r\n    tf.summary.image(\"G(z)\", G_z, max_outputs=3)\r\n\r\n    # encode x to a latent representation\r\n    E_x = E(x)\r\n\r\n    G_Ex = G(E_x)\r\n    tf.summary.image(\"G(E(x))\", G_Ex, max_outputs=3)\r\n\r\n    # plot image difference\r\n    tf.summary.image(\r\n        \"G(E(x)) - x\", tf.norm(G_Ex - x_, axis=3, keepdims=True), max_outputs=3\r\n    )\r\n\r\n    # The output of the discriminator is a bs,n,n,value\r\n    # hence flatten all the values of the map and compute\r\n    # the loss element wise\r\n    D_Gz, F_Gz = D(inputs=[G_z, z])\r\n    D_x, F_x = D(inputs=[x, E_x])\r\n    D_Gz = k.layers.Flatten()(D_Gz)\r\n    F_Gz = k.layers.Flatten()(F_Gz)\r\n    D_x = k.layers.Flatten()(D_x)\r\n    F_x = k.layers.Flatten()(F_x)\r\n\r\n    ## Discriminator\r\n    d_loss = min_max(D_x, D_Gz)\r\n\r\n    ## Generator\r\n    g_loss = bce(D_Gz, tf.constant(1.0))\r\n    # Encoder\r\n    e_loss = bce(D_x, tf.constant(0.0))\r\n\r\n    # add regularizations defined in the keras layers\r\n    d_loss += tf.add_n(D.losses)\r\n    e_loss += tf.add_n(E.losses)\r\n    g_loss += tf.add_n(G.losses)\r\n\r\n    tf.summary.scalar(\"d_loss\", d_loss)\r\n    tf.summary.scalar(\"g_loss\", g_loss)\r\n    tf.summary.scalar(\"e_loss\", e_loss)\r\n\r\n    global_step = tf.train.get_or_create_global_step()\r\n\r\n    lr = 1e-4\r\n    tf.summary.scalar(\"lr\", lr)\r\n\r\n    # Define the D train op\r\n    train_d = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(\r\n        d_loss, var_list=D.trainable_variables\r\n    )\r\n\r\n    # Define the G + E train ops (the models can be trained\r\n    # the same step, but separately)\r\n    train_g = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(\r\n        g_loss, global_step=global_step, var_list=G.trainable_variables\r\n    )\r\n\r\n    train_e = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(\r\n        e_loss, var_list=E.trainable_variables\r\n    )\r\n\r\n    log_dir = f\"logs/test\"\r\n    summary_op = tf.summary.merge_all()\r\n\r\n    scaffold = tf.train.Scaffold()\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    session_creator = tf.train.ChiefSessionCreator(\r\n        config=config, scaffold=scaffold, checkpoint_dir=log_dir\r\n    )\r\n\r\n    def _loop(func: Callable) -> None:\r\n        \"\"\"\r\n        Execute func for the specified number of epochs or max_steps.\r\n\r\n        Args:\r\n            func: callable to loop\r\n\r\n        Returns:\r\n            None.\r\n        \"\"\"\r\n        try:\r\n            while True:\r\n                func()\r\n        except tf.errors.OutOfRangeError:\r\n            pass\r\n\r\n    with tf.train.MonitoredSession(\r\n        session_creator=session_creator,\r\n        hooks=[\r\n            tf.train.CheckpointSaverHook(log_dir, save_steps=100, scaffold=scaffold)\r\n            # tf.train.ProfilerHook(save_steps=1000, output_dir=log_dir),\r\n        ],\r\n    ) as sess:\r\n        # Get the summary writer.\r\n        # The rational behind using the writer (from the scaffold)\r\n        # and not using the SummarySaverHook is that we want to log\r\n        # every X steps the output of G, G(E(x)) and x\r\n        # But since we need to use placeholders to feed the same data\r\n        # to G, D and E, we can't use the Hook, because the first\r\n        # sess.run on the data, will trigger the summary save op\r\n        # and the summary save op needs the data from the placeholder\r\n        writer = SummaryWriterCache.get(log_dir)\r\n\r\n        def _train():\r\n            # First create the input, that must be shared between the 2\r\n            # training iteration\r\n            real, noise = sess.run([x_, z_])\r\n            feed_dict = {x: real, z: noise}\r\n\r\n            # train D\r\n            d_gz_, d_x, _, d_loss_value = sess.run(\r\n                [D_Gz, D_x, train_d, d_loss], feed_dict\r\n            )\r\n\r\n            # train G+E\r\n            _, g_loss_value, _, e_loss_value, step = sess.run(\r\n                [train_g, g_loss, train_e, e_loss, global_step], feed_dict\r\n            )\r\n\r\n            if step % 100 == 0:\r\n                summaries = sess.run(summary_op, feed_dict)\r\n                print(\r\n                    f\"[{step}] d: {d_loss_value} - g: {g_loss_value} - e: {e_loss_value}\"\r\n                )\r\n                print(np.mean(d_gz_), np.mean(d_x))\r\n                writer.add_summary(summaries, step)\r\n                writer.flush()\r\n\r\n        _loop(_train)\r\n    return 0\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(train())\r\n```\r\n"}