{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14271", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14271/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14271/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14271/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14271", "id": 271361138, "node_id": "MDU6SXNzdWUyNzEzNjExMzg=", "number": 14271, "title": "tf.gradients return NaN when batch_norm is in inference mode (is_training=False)?", "user": {"login": "wenwei202", "id": 12142066, "node_id": "MDQ6VXNlcjEyMTQyMDY2", "avatar_url": "https://avatars0.githubusercontent.com/u/12142066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wenwei202", "html_url": "https://github.com/wenwei202", "followers_url": "https://api.github.com/users/wenwei202/followers", "following_url": "https://api.github.com/users/wenwei202/following{/other_user}", "gists_url": "https://api.github.com/users/wenwei202/gists{/gist_id}", "starred_url": "https://api.github.com/users/wenwei202/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wenwei202/subscriptions", "organizations_url": "https://api.github.com/users/wenwei202/orgs", "repos_url": "https://api.github.com/users/wenwei202/repos", "events_url": "https://api.github.com/users/wenwei202/events{/privacy}", "received_events_url": "https://api.github.com/users/wenwei202/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-06T05:07:46Z", "updated_at": "2017-11-06T15:18:43Z", "closed_at": "2017-11-06T15:18:43Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:('v1.3.0-rc2-20-g0787eee', '1.3.0')</li>\n<li><strong>Python version</strong>: Python 2.7.11 |Anaconda custom (64-bit)</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:/usr/local/cuda-8.0 &amp; libcudnn.so.6.0.21</li>\n<li><strong>GPU model and memory</strong>:GeForce GTX 1080</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>git clone https://github.com/wenwei202/models.git\ncd models\ngit checkout bug-fixing\ncd research/slim\npython eval_image_classifier_bn_grad.py -alsologtostderr --checkpoint_path /tmp/resnet_v1_152.ckpt  --dataset_dir /tmp/imagenet-data \\\n--dataset_name imagenet \\\n--dataset_split_name validation \\\n--model_name resnet_v1_152 \\\n--batch_size 5 --eval_dir /tmp/bn_grad \\\n--labels_offset=1  --max_num_batches 1\n</code></pre>\n<h3>Describe the problem, Source code / logs</h3>\n<p>I need to compute the gradients of dy/dx, where y is the max logit and x is input image. I just add a few lines to the standard <a href=\"https://github.com/tensorflow/models/blob/master/research/slim/eval_image_classifier.py\">research/slim/eval_image_classifier.py</a>:</p>\n<pre><code>    max_logits = tf.reduce_max(logits, axis=1)\n    themaps = tf.gradients(max_logits, images)\n    themaps = themaps[0]\n    with tf.control_dependencies([tf.Print(themaps,[themaps],first_n=3)]):\n      themaps = tf.abs(themaps)\n</code></pre>\n<p>which is <a href=\"https://github.com/wenwei202/models/blob/bug-fixing/research/slim/eval_image_classifier_bn_grad.py#L155-L159\">here</a>.<br>\nWhen I run <code>eval_image_classifier_bn_grad.py</code> to evaluate models <a href=\"https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models\">here</a>, vgg and lenet work well, but <code>tf.gradients</code> always return <code>nan</code> when I evaluate inception and resnet models.<br>\nCommand:</p>\n<pre><code>python eval_image_classifier_bn_grad.py -alsologtostderr --checkpoint_path /tmp/resnet_v1_152.ckpt  --dataset_dir /tmp/imagenet-data \\\n--dataset_name imagenet \\\n--dataset_split_name validation \\\n--model_name resnet_v1_152 \\\n--batch_size 5 --eval_dir /tmp/bn_grad \\\n--labels_offset=1  --max_num_batches 1\n</code></pre>\n<p>Output:</p>\n<pre><code>...\n2017-11-05 23:25:19.751859: I tensorflow/core/kernels/logging_ops.cc:79] [[[[nan nan nan]]]...]\n...\n</code></pre>\n<p>I guess the issue comes from the<code>slim.batch_norm</code> when it is in inference mode, and somehow, <code>tf.gradients</code> does not work anymore. Why?</p>\n<ol>\n<li>vgg and lenet without batch_norm layers work well</li>\n<li>inception and resnet with batch_norm layers return <code>nan</code></li>\n<li>inception and resnet work after I enforce the <code>is_training=True</code> of <code>slim.batch_norm</code></li>\n<li>it seems \"reasonable\" to ignore the possibility of the usage of <code>tf.gradients</code> when <code>batch_norm</code> is in inference mode.</li>\n</ol>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):('v1.3.0-rc2-20-g0787eee', '1.3.0')\nPython version: Python 2.7.11 |Anaconda custom (64-bit)\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:/usr/local/cuda-8.0 & libcudnn.so.6.0.21\nGPU model and memory:GeForce GTX 1080\nExact command to reproduce:\n\ngit clone https://github.com/wenwei202/models.git\ncd models\ngit checkout bug-fixing\ncd research/slim\npython eval_image_classifier_bn_grad.py -alsologtostderr --checkpoint_path /tmp/resnet_v1_152.ckpt  --dataset_dir /tmp/imagenet-data \\\n--dataset_name imagenet \\\n--dataset_split_name validation \\\n--model_name resnet_v1_152 \\\n--batch_size 5 --eval_dir /tmp/bn_grad \\\n--labels_offset=1  --max_num_batches 1\n\nDescribe the problem, Source code / logs\nI need to compute the gradients of dy/dx, where y is the max logit and x is input image. I just add a few lines to the standard research/slim/eval_image_classifier.py:\n    max_logits = tf.reduce_max(logits, axis=1)\n    themaps = tf.gradients(max_logits, images)\n    themaps = themaps[0]\n    with tf.control_dependencies([tf.Print(themaps,[themaps],first_n=3)]):\n      themaps = tf.abs(themaps)\n\nwhich is here.\nWhen I run eval_image_classifier_bn_grad.py to evaluate models here, vgg and lenet work well, but tf.gradients always return nan when I evaluate inception and resnet models.\nCommand:\npython eval_image_classifier_bn_grad.py -alsologtostderr --checkpoint_path /tmp/resnet_v1_152.ckpt  --dataset_dir /tmp/imagenet-data \\\n--dataset_name imagenet \\\n--dataset_split_name validation \\\n--model_name resnet_v1_152 \\\n--batch_size 5 --eval_dir /tmp/bn_grad \\\n--labels_offset=1  --max_num_batches 1\n\nOutput:\n...\n2017-11-05 23:25:19.751859: I tensorflow/core/kernels/logging_ops.cc:79] [[[[nan nan nan]]]...]\n...\n\nI guess the issue comes from theslim.batch_norm when it is in inference mode, and somehow, tf.gradients does not work anymore. Why?\n\nvgg and lenet without batch_norm layers work well\ninception and resnet with batch_norm layers return nan\ninception and resnet work after I enforce the is_training=True of slim.batch_norm\nit seems \"reasonable\" to ignore the possibility of the usage of tf.gradients when batch_norm is in inference mode.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: Python 2.7.11 |Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:/usr/local/cuda-8.0 & libcudnn.so.6.0.21\r\n- **GPU model and memory**:GeForce GTX 1080\r\n- **Exact command to reproduce**:\r\n```\r\ngit clone https://github.com/wenwei202/models.git\r\ncd models\r\ngit checkout bug-fixing\r\ncd research/slim\r\npython eval_image_classifier_bn_grad.py -alsologtostderr --checkpoint_path /tmp/resnet_v1_152.ckpt  --dataset_dir /tmp/imagenet-data \\\r\n--dataset_name imagenet \\\r\n--dataset_split_name validation \\\r\n--model_name resnet_v1_152 \\\r\n--batch_size 5 --eval_dir /tmp/bn_grad \\\r\n--labels_offset=1  --max_num_batches 1\r\n```\r\n\r\n### Describe the problem, Source code / logs\r\nI need to compute the gradients of dy/dx, where y is the max logit and x is input image. I just add a few lines to the standard [research/slim/eval_image_classifier.py](https://github.com/tensorflow/models/blob/master/research/slim/eval_image_classifier.py):\r\n```\r\n    max_logits = tf.reduce_max(logits, axis=1)\r\n    themaps = tf.gradients(max_logits, images)\r\n    themaps = themaps[0]\r\n    with tf.control_dependencies([tf.Print(themaps,[themaps],first_n=3)]):\r\n      themaps = tf.abs(themaps)\r\n```\r\nwhich is [here](https://github.com/wenwei202/models/blob/bug-fixing/research/slim/eval_image_classifier_bn_grad.py#L155-L159).\r\nWhen I run `eval_image_classifier_bn_grad.py` to evaluate models [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models), vgg and lenet work well, but `tf.gradients` always return `nan` when I evaluate inception and resnet models.\r\nCommand:\r\n```\r\npython eval_image_classifier_bn_grad.py -alsologtostderr --checkpoint_path /tmp/resnet_v1_152.ckpt  --dataset_dir /tmp/imagenet-data \\\r\n--dataset_name imagenet \\\r\n--dataset_split_name validation \\\r\n--model_name resnet_v1_152 \\\r\n--batch_size 5 --eval_dir /tmp/bn_grad \\\r\n--labels_offset=1  --max_num_batches 1\r\n```\r\nOutput:\r\n```\r\n...\r\n2017-11-05 23:25:19.751859: I tensorflow/core/kernels/logging_ops.cc:79] [[[[nan nan nan]]]...]\r\n...\r\n```\r\n\r\nI guess the issue comes from the`slim.batch_norm` when it is in inference mode, and somehow, `tf.gradients` does not work anymore. Why?\r\n  1. vgg and lenet without batch_norm layers work well \r\n  2. inception and resnet with batch_norm layers return `nan`\r\n  3. inception and resnet work after I enforce the `is_training=True` of `slim.batch_norm`\r\n  4. it seems \"reasonable\" to ignore the possibility of the usage of `tf.gradients` when `batch_norm` is in inference mode.\r\n"}