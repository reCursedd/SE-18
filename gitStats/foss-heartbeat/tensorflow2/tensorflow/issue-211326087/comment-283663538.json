{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/283663538", "html_url": "https://github.com/tensorflow/tensorflow/issues/8003#issuecomment-283663538", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8003", "id": 283663538, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzY2MzUzOA==", "user": {"login": "MicaelCarvalho", "id": 17184992, "node_id": "MDQ6VXNlcjE3MTg0OTky", "avatar_url": "https://avatars3.githubusercontent.com/u/17184992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MicaelCarvalho", "html_url": "https://github.com/MicaelCarvalho", "followers_url": "https://api.github.com/users/MicaelCarvalho/followers", "following_url": "https://api.github.com/users/MicaelCarvalho/following{/other_user}", "gists_url": "https://api.github.com/users/MicaelCarvalho/gists{/gist_id}", "starred_url": "https://api.github.com/users/MicaelCarvalho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MicaelCarvalho/subscriptions", "organizations_url": "https://api.github.com/users/MicaelCarvalho/orgs", "repos_url": "https://api.github.com/users/MicaelCarvalho/repos", "events_url": "https://api.github.com/users/MicaelCarvalho/events{/privacy}", "received_events_url": "https://api.github.com/users/MicaelCarvalho/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-02T14:13:19Z", "updated_at": "2017-03-02T14:16:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I came here to report the exact same behavior, and saw your issue on the first page.</p>\n<p>The same goes for vectors:</p>\n<pre lang=\"import\" data-meta=\"tensorflow as tf\"><code>\nvalue_1 = [tf.placeholder(tf.float32, shape=(2, 3, 4)), tf.placeholder(tf.float32, shape=(5, 6, 7)), tf.placeholder(tf.float32, shape=(8, 9, 10))]\nvalue_2 = [tf.placeholder(tf.float32, shape=(2, 3, 4)), tf.placeholder(tf.float32, shape=(5, 6, 7)), tf.placeholder(tf.float32, shape=(8, 9, 10))]\nswitcher = tf.placeholder(tf.bool)\n\ndef get_value_1():\n\treturn value_1\n\ndef get_value_2():\n\treturn value_2\n\nbatch_sample = tf.case([(switcher, get_value_1)], default=get_value_2, exclusive=True)\n\nprint(get_value_1())\nprint(get_value_2())\nprint(batch_sample)\n</code></pre>\n<p>A simple explanation is that tf.case can handle different output shapes for different cases. For example, if we change value_2 to have different shapes than value_1 (e.g. 2,3,3 instead of 2,3,4) the case will still work.</p>\n<p>However, I do believe it would be of best interest to have a defined output shape if all cases have the same shape. It would save us from forcing the shape with something like:</p>\n<p><code>for k in range(len(batch_sample)): batch_sample[k].set_shape(get_value_1()[k].shape)</code></p>\n<p>This isn't critical, of course, as there is a rather clean workaround.</p>", "body_text": "I came here to report the exact same behavior, and saw your issue on the first page.\nThe same goes for vectors:\n\nvalue_1 = [tf.placeholder(tf.float32, shape=(2, 3, 4)), tf.placeholder(tf.float32, shape=(5, 6, 7)), tf.placeholder(tf.float32, shape=(8, 9, 10))]\nvalue_2 = [tf.placeholder(tf.float32, shape=(2, 3, 4)), tf.placeholder(tf.float32, shape=(5, 6, 7)), tf.placeholder(tf.float32, shape=(8, 9, 10))]\nswitcher = tf.placeholder(tf.bool)\n\ndef get_value_1():\n\treturn value_1\n\ndef get_value_2():\n\treturn value_2\n\nbatch_sample = tf.case([(switcher, get_value_1)], default=get_value_2, exclusive=True)\n\nprint(get_value_1())\nprint(get_value_2())\nprint(batch_sample)\n\nA simple explanation is that tf.case can handle different output shapes for different cases. For example, if we change value_2 to have different shapes than value_1 (e.g. 2,3,3 instead of 2,3,4) the case will still work.\nHowever, I do believe it would be of best interest to have a defined output shape if all cases have the same shape. It would save us from forcing the shape with something like:\nfor k in range(len(batch_sample)): batch_sample[k].set_shape(get_value_1()[k].shape)\nThis isn't critical, of course, as there is a rather clean workaround.", "body": "I came here to report the exact same behavior, and saw your issue on the first page.\r\n\r\nThe same goes for vectors:\r\n\r\n```import tensorflow as tf\r\n\r\nvalue_1 = [tf.placeholder(tf.float32, shape=(2, 3, 4)), tf.placeholder(tf.float32, shape=(5, 6, 7)), tf.placeholder(tf.float32, shape=(8, 9, 10))]\r\nvalue_2 = [tf.placeholder(tf.float32, shape=(2, 3, 4)), tf.placeholder(tf.float32, shape=(5, 6, 7)), tf.placeholder(tf.float32, shape=(8, 9, 10))]\r\nswitcher = tf.placeholder(tf.bool)\r\n\r\ndef get_value_1():\r\n\treturn value_1\r\n\r\ndef get_value_2():\r\n\treturn value_2\r\n\r\nbatch_sample = tf.case([(switcher, get_value_1)], default=get_value_2, exclusive=True)\r\n\r\nprint(get_value_1())\r\nprint(get_value_2())\r\nprint(batch_sample)\r\n```\r\n\r\nA simple explanation is that tf.case can handle different output shapes for different cases. For example, if we change value_2 to have different shapes than value_1 (e.g. 2,3,3 instead of 2,3,4) the case will still work.\r\n\r\nHowever, I do believe it would be of best interest to have a defined output shape if all cases have the same shape. It would save us from forcing the shape with something like:\r\n\r\n`for k in range(len(batch_sample)): batch_sample[k].set_shape(get_value_1()[k].shape)`\r\n\r\nThis isn't critical, of course, as there is a rather clean workaround."}