{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/346403919", "html_url": "https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-346403919", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7927", "id": 346403919, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NjQwMzkxOQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-22T16:32:16Z", "updated_at": "2017-11-22T16:32:16Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">You get a new batch every time you call session.run(), independent of the\ngraph, and a new dropout pattern is created with every call to\nsession.run().  Therefore a new dropout pattern is applied to every batch.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Nov 22, 2017 at 1:07 AM, Yujia Liu ***@***.***&gt; wrote:\n After check out the implementation of variational_recurrent in the\n DropoutWrapper, I'm a little confused. When variational_recurrent=True,\n the mask is generated when the wrapper is initialized before the training\n process, and is applied across every batch. Would it be better if a new\n mask be generated every batch as <a class=\"user-mention\" href=\"https://github.com/beeCwright\">@beeCwright</a>\n &lt;<a href=\"https://github.com/beecwright\">https://github.com/beecwright</a>&gt; first requested?\n\n Since the wrapper implementation is to build a rnncell object, how can it\n be aware of a new batch is coming. From my limited understanding, only\n dynamic_rnn knows when a batch is coming and rnncell only deals with\n single timestep input and state. Correct me if I'm wrong :P\n\n \u2014\n You are receiving this because you modified the open/close state.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"210582261\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7927\" href=\"https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-346287303\">#7927 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimxd0_IPTH5UAYZ2RMZNTrJ-8OoZAks5s4-RTgaJpZM4MNheF\">https://github.com/notifications/unsubscribe-auth/ABtimxd0_IPTH5UAYZ2RMZNTrJ-8OoZAks5s4-RTgaJpZM4MNheF</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "You get a new batch every time you call session.run(), independent of the\ngraph, and a new dropout pattern is created with every call to\nsession.run().  Therefore a new dropout pattern is applied to every batch.\n\u2026\nOn Wed, Nov 22, 2017 at 1:07 AM, Yujia Liu ***@***.***> wrote:\n After check out the implementation of variational_recurrent in the\n DropoutWrapper, I'm a little confused. When variational_recurrent=True,\n the mask is generated when the wrapper is initialized before the training\n process, and is applied across every batch. Would it be better if a new\n mask be generated every batch as @beeCwright\n <https://github.com/beecwright> first requested?\n\n Since the wrapper implementation is to build a rnncell object, how can it\n be aware of a new batch is coming. From my limited understanding, only\n dynamic_rnn knows when a batch is coming and rnncell only deals with\n single timestep input and state. Correct me if I'm wrong :P\n\n \u2014\n You are receiving this because you modified the open/close state.\n Reply to this email directly, view it on GitHub\n <#7927 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtimxd0_IPTH5UAYZ2RMZNTrJ-8OoZAks5s4-RTgaJpZM4MNheF>\n .", "body": "You get a new batch every time you call session.run(), independent of the\ngraph, and a new dropout pattern is created with every call to\nsession.run().  Therefore a new dropout pattern is applied to every batch.\n\nOn Wed, Nov 22, 2017 at 1:07 AM, Yujia Liu <notifications@github.com> wrote:\n\n> After check out the implementation of variational_recurrent in the\n> DropoutWrapper, I'm a little confused. When variational_recurrent=True,\n> the mask is generated when the wrapper is initialized before the training\n> process, and is applied across every batch. Would it be better if a new\n> mask be generated every batch as @beeCwright\n> <https://github.com/beecwright> first requested?\n>\n> Since the wrapper implementation is to build a rnncell object, how can it\n> be aware of a new batch is coming. From my limited understanding, only\n> dynamic_rnn knows when a batch is coming and rnncell only deals with\n> single timestep input and state. Correct me if I'm wrong :P\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-346287303>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxd0_IPTH5UAYZ2RMZNTrJ-8OoZAks5s4-RTgaJpZM4MNheF>\n> .\n>\n"}