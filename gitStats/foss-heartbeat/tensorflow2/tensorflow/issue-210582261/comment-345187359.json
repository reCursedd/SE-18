{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345187359", "html_url": "https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-345187359", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7927", "id": 345187359, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTE4NzM1OQ==", "user": {"login": "zotroneneis", "id": 15320635, "node_id": "MDQ6VXNlcjE1MzIwNjM1", "avatar_url": "https://avatars0.githubusercontent.com/u/15320635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zotroneneis", "html_url": "https://github.com/zotroneneis", "followers_url": "https://api.github.com/users/zotroneneis/followers", "following_url": "https://api.github.com/users/zotroneneis/following{/other_user}", "gists_url": "https://api.github.com/users/zotroneneis/gists{/gist_id}", "starred_url": "https://api.github.com/users/zotroneneis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zotroneneis/subscriptions", "organizations_url": "https://api.github.com/users/zotroneneis/orgs", "repos_url": "https://api.github.com/users/zotroneneis/repos", "events_url": "https://api.github.com/users/zotroneneis/events{/privacy}", "received_events_url": "https://api.github.com/users/zotroneneis/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-17T09:13:41Z", "updated_at": "2017-11-17T10:56:00Z", "author_association": "NONE", "body_html": "<p>I have some questions regarding the implementation of <code>variational_recurrent</code> in the Dropout Wrapper. In the paper it says: 'Implementing our approximate inference is identical to implementing dropout in RNNs with the same network units dropped at each time step, randomly dropping inputs, outputs, and recurrent connections. This is in contrast to existing techniques, where different network units would be dropped at different time steps, and no dropout would be applied to the recurrent connections '</p>\n<p>When setting <code>variational_recurrent=True</code>, do I get the full functionality of variational dropout presented in the paper (Y. Gal, Z. Ghahramani) ? I have looked at the source code but I'm still not sure whether it includes all aspects of variational dropout presented in the paper. Recent papers like <a href=\"https://arxiv.org/pdf/1707.05589.pdf\" rel=\"nofollow\">this one</a> use intra-layer dropout in addition to variational_dropout. But when <code>variational_recurrent</code> is already dropping output units at each time step I don't see the need for using dropout between RNN layers as well.</p>", "body_text": "I have some questions regarding the implementation of variational_recurrent in the Dropout Wrapper. In the paper it says: 'Implementing our approximate inference is identical to implementing dropout in RNNs with the same network units dropped at each time step, randomly dropping inputs, outputs, and recurrent connections. This is in contrast to existing techniques, where different network units would be dropped at different time steps, and no dropout would be applied to the recurrent connections '\nWhen setting variational_recurrent=True, do I get the full functionality of variational dropout presented in the paper (Y. Gal, Z. Ghahramani) ? I have looked at the source code but I'm still not sure whether it includes all aspects of variational dropout presented in the paper. Recent papers like this one use intra-layer dropout in addition to variational_dropout. But when variational_recurrent is already dropping output units at each time step I don't see the need for using dropout between RNN layers as well.", "body": "I have some questions regarding the implementation of `variational_recurrent` in the Dropout Wrapper. In the paper it says: 'Implementing our approximate inference is identical to implementing dropout in RNNs with the same network units dropped at each time step, randomly dropping inputs, outputs, and recurrent connections. This is in contrast to existing techniques, where different network units would be dropped at different time steps, and no dropout would be applied to the recurrent connections '\r\n\r\nWhen setting `variational_recurrent=True`, do I get the full functionality of variational dropout presented in the paper (Y. Gal, Z. Ghahramani) ? I have looked at the source code but I'm still not sure whether it includes all aspects of variational dropout presented in the paper. Recent papers like [this one](https://arxiv.org/pdf/1707.05589.pdf) use intra-layer dropout in addition to variational_dropout. But when `variational_recurrent` is already dropping output units at each time step I don't see the need for using dropout between RNN layers as well."}