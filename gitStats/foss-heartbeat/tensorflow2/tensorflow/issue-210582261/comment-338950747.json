{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/338950747", "html_url": "https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-338950747", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7927", "id": 338950747, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODk1MDc0Nw==", "user": {"login": "georgesterpu", "id": 6018251, "node_id": "MDQ6VXNlcjYwMTgyNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6018251?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgesterpu", "html_url": "https://github.com/georgesterpu", "followers_url": "https://api.github.com/users/georgesterpu/followers", "following_url": "https://api.github.com/users/georgesterpu/following{/other_user}", "gists_url": "https://api.github.com/users/georgesterpu/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgesterpu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgesterpu/subscriptions", "organizations_url": "https://api.github.com/users/georgesterpu/orgs", "repos_url": "https://api.github.com/users/georgesterpu/repos", "events_url": "https://api.github.com/users/georgesterpu/events{/privacy}", "received_events_url": "https://api.github.com/users/georgesterpu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-24T10:47:44Z", "updated_at": "2017-10-24T10:47:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4604464\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oahziur\">@oahziur</a>, you are right.<br>\nI have added an extra parameter, the layer number, to my build_cell function, and I instantiate the DropoutWrapper like this:</p>\n<pre><code>cells = DropoutWrapper(cells,\n                                   ..............\n                                   variational_recurrent=True,\n                                   dtype=tf.float32,\n                                   input_size=self._inputs.get_shape()[-1] if layer == 0 else tf.TensorShape(num_units),\n                                )\n</code></pre>\n<p>and it appears to work.</p>\n<p>However, I get a new error on the decoder side in a seq2seq model:<br>\n<code>ValueError: Dimensions must be equal, but are 143 and 128 for 'Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/attention_wrapper/multi_rnn_cell/cell_0/mul' (op: 'Mul') with input shapes: [?,143], [1,128].</code></p>\n<p>I did make sure that for the decoder cells, input size is still num_units (128), and not <code>self._inputs.get_shape()[-1]</code>, because it is initialised from the encoder's final state. Still, I get the same error. Do you have any suggestion here ?</p>", "body_text": "Thanks, @oahziur, you are right.\nI have added an extra parameter, the layer number, to my build_cell function, and I instantiate the DropoutWrapper like this:\ncells = DropoutWrapper(cells,\n                                   ..............\n                                   variational_recurrent=True,\n                                   dtype=tf.float32,\n                                   input_size=self._inputs.get_shape()[-1] if layer == 0 else tf.TensorShape(num_units),\n                                )\n\nand it appears to work.\nHowever, I get a new error on the decoder side in a seq2seq model:\nValueError: Dimensions must be equal, but are 143 and 128 for 'Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/attention_wrapper/multi_rnn_cell/cell_0/mul' (op: 'Mul') with input shapes: [?,143], [1,128].\nI did make sure that for the decoder cells, input size is still num_units (128), and not self._inputs.get_shape()[-1], because it is initialised from the encoder's final state. Still, I get the same error. Do you have any suggestion here ?", "body": "Thanks, @oahziur, you are right.\r\nI have added an extra parameter, the layer number, to my build_cell function, and I instantiate the DropoutWrapper like this: \r\n\r\n```\r\ncells = DropoutWrapper(cells,\r\n                                   ..............\r\n                                   variational_recurrent=True,\r\n                                   dtype=tf.float32,\r\n                                   input_size=self._inputs.get_shape()[-1] if layer == 0 else tf.TensorShape(num_units),\r\n                                )\r\n```\r\nand it appears to work.\r\n\r\nHowever, I get a new error on the decoder side in a seq2seq model:\r\n`ValueError: Dimensions must be equal, but are 143 and 128 for 'Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/attention_wrapper/multi_rnn_cell/cell_0/mul' (op: 'Mul') with input shapes: [?,143], [1,128].`\r\n\r\nI did make sure that for the decoder cells, input size is still num_units (128), and not `self._inputs.get_shape()[-1]`, because it is initialised from the encoder's final state. Still, I get the same error. Do you have any suggestion here ?"}