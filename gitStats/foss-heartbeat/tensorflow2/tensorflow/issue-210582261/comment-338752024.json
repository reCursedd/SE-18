{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/338752024", "html_url": "https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-338752024", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7927", "id": 338752024, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODc1MjAyNA==", "user": {"login": "oahziur", "id": 4604464, "node_id": "MDQ6VXNlcjQ2MDQ0NjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4604464?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oahziur", "html_url": "https://github.com/oahziur", "followers_url": "https://api.github.com/users/oahziur/followers", "following_url": "https://api.github.com/users/oahziur/following{/other_user}", "gists_url": "https://api.github.com/users/oahziur/gists{/gist_id}", "starred_url": "https://api.github.com/users/oahziur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oahziur/subscriptions", "organizations_url": "https://api.github.com/users/oahziur/orgs", "repos_url": "https://api.github.com/users/oahziur/repos", "events_url": "https://api.github.com/users/oahziur/events{/privacy}", "received_events_url": "https://api.github.com/users/oahziur/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-23T18:24:20Z", "updated_at": "2017-10-23T18:24:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6018251\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/georgesterpu\">@georgesterpu</a></p>\n<p>From your error, it seems you have at least 2 layer of lstm? Is your dropout wrapper applied on the 2nd layer's cell? If so, the input size to your second layer should be your first layer's output, which is 128 instead of 132.</p>", "body_text": "@georgesterpu\nFrom your error, it seems you have at least 2 layer of lstm? Is your dropout wrapper applied on the 2nd layer's cell? If so, the input size to your second layer should be your first layer's output, which is 128 instead of 132.", "body": "@georgesterpu \r\n\r\nFrom your error, it seems you have at least 2 layer of lstm? Is your dropout wrapper applied on the 2nd layer's cell? If so, the input size to your second layer should be your first layer's output, which is 128 instead of 132."}