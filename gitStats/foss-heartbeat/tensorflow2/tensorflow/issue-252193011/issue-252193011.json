{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12519", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12519/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12519/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12519/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12519", "id": 252193011, "node_id": "MDU6SXNzdWUyNTIxOTMwMTE=", "number": 12519, "title": "Bug on the gradients graph computation - C++ API", "user": {"login": "theflofly", "id": 3902382, "node_id": "MDQ6VXNlcjM5MDIzODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3902382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/theflofly", "html_url": "https://github.com/theflofly", "followers_url": "https://api.github.com/users/theflofly/followers", "following_url": "https://api.github.com/users/theflofly/following{/other_user}", "gists_url": "https://api.github.com/users/theflofly/gists{/gist_id}", "starred_url": "https://api.github.com/users/theflofly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/theflofly/subscriptions", "organizations_url": "https://api.github.com/users/theflofly/orgs", "repos_url": "https://api.github.com/users/theflofly/repos", "events_url": "https://api.github.com/users/theflofly/events{/privacy}", "received_events_url": "https://api.github.com/users/theflofly/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-08-23T07:57:21Z", "updated_at": "2017-09-05T16:38:13Z", "closed_at": "2017-09-05T16:38:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The gradient computation in the C++ API works as follow.<br>\nLet's say that we have the following graph:</p>\n<pre><code>                Tanh\n                  |\n         Assign MatMul\n         /    \\ /    \\\n      Const   Var   Const\n</code></pre>\n<p>Here our Output is Tanh and our Input is Var. The gradient method does a BFS from Var to Tanh to count for each node how many backprop we should expect. If a node has two outgoing edges, it will be ready only when both would have been backpropagated and the gradient summed. In our case, Var has 2, Assign 0, MatMul 1. These values are saved into a pending array.</p>\n<p>Then the gradient method does a BFS from Tanh to Var, this is the actual backpropagation, the error is backpropagated until we reach Var. When a Node is reached, pending is decreased by one and if pending == 0, the Node is ready and is added to the queue of Nodes to be processed. If it is not ready, it will be reached again in the future and will be ready at some point.</p>\n<p>In our case, doing a BFS from Var to Tanh give us 2 expected gradients, one from Assign and one from MatMul, whereas doing a BFS from Tanh to Var, we will reach Var only once, because we can't reach Assign from Tanh. In that case, the pending count will never reach Zero, Var will never be ready and the BFS will end. At the end, a check is done and if pending nodes are still there, an error is raised.</p>\n<p>The PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"251287023\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12397\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/12397/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/12397\">#12397</a> updates the gradient method to ignore nodes that have 0 outgoing edges and are not in the list of Output (Tanh is the only one in the list of outputs in our case).</p>\n<p>As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=88808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/skye\">@skye</a> pointed it out as a comment in the PR, there is other cases where there is still a problem. I am working on it.</p>", "body_text": "The gradient computation in the C++ API works as follow.\nLet's say that we have the following graph:\n                Tanh\n                  |\n         Assign MatMul\n         /    \\ /    \\\n      Const   Var   Const\n\nHere our Output is Tanh and our Input is Var. The gradient method does a BFS from Var to Tanh to count for each node how many backprop we should expect. If a node has two outgoing edges, it will be ready only when both would have been backpropagated and the gradient summed. In our case, Var has 2, Assign 0, MatMul 1. These values are saved into a pending array.\nThen the gradient method does a BFS from Tanh to Var, this is the actual backpropagation, the error is backpropagated until we reach Var. When a Node is reached, pending is decreased by one and if pending == 0, the Node is ready and is added to the queue of Nodes to be processed. If it is not ready, it will be reached again in the future and will be ready at some point.\nIn our case, doing a BFS from Var to Tanh give us 2 expected gradients, one from Assign and one from MatMul, whereas doing a BFS from Tanh to Var, we will reach Var only once, because we can't reach Assign from Tanh. In that case, the pending count will never reach Zero, Var will never be ready and the BFS will end. At the end, a check is done and if pending nodes are still there, an error is raised.\nThe PR #12397 updates the gradient method to ignore nodes that have 0 outgoing edges and are not in the list of Output (Tanh is the only one in the list of outputs in our case).\nAs @skye pointed it out as a comment in the PR, there is other cases where there is still a problem. I am working on it.", "body": "The gradient computation in the C++ API works as follow.\r\nLet's say that we have the following graph:\r\n```\r\n                Tanh\r\n                  |\r\n         Assign MatMul\r\n         /    \\ /    \\\r\n      Const   Var   Const\r\n```\r\nHere our Output is Tanh and our Input is Var. The gradient method does a BFS from Var to Tanh to count for each node how many backprop we should expect. If a node has two outgoing edges, it will be ready only when both would have been backpropagated and the gradient summed. In our case, Var has 2, Assign 0, MatMul 1. These values are saved into a pending array.\r\n\r\nThen the gradient method does a BFS from Tanh to Var, this is the actual backpropagation, the error is backpropagated until we reach Var. When a Node is reached, pending is decreased by one and if pending == 0, the Node is ready and is added to the queue of Nodes to be processed. If it is not ready, it will be reached again in the future and will be ready at some point.\r\n\r\nIn our case, doing a BFS from Var to Tanh give us 2 expected gradients, one from Assign and one from MatMul, whereas doing a BFS from Tanh to Var, we will reach Var only once, because we can't reach Assign from Tanh. In that case, the pending count will never reach Zero, Var will never be ready and the BFS will end. At the end, a check is done and if pending nodes are still there, an error is raised.\r\n\r\nThe PR https://github.com/tensorflow/tensorflow/pull/12397 updates the gradient method to ignore nodes that have 0 outgoing edges and are not in the list of Output (Tanh is the only one in the list of outputs in our case).\r\n\r\nAs @skye pointed it out as a comment in the PR, there is other cases where there is still a problem. I am working on it."}