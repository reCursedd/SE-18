{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22584", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22584/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22584/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22584/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22584", "id": 364751989, "node_id": "MDU6SXNzdWUzNjQ3NTE5ODk=", "number": 22584, "title": "MappedByteBuffer is not a valid flatbuffer model TFLITE Model_conversion_issue", "user": {"login": "Raj-08", "id": 15856029, "node_id": "MDQ6VXNlcjE1ODU2MDI5", "avatar_url": "https://avatars3.githubusercontent.com/u/15856029?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Raj-08", "html_url": "https://github.com/Raj-08", "followers_url": "https://api.github.com/users/Raj-08/followers", "following_url": "https://api.github.com/users/Raj-08/following{/other_user}", "gists_url": "https://api.github.com/users/Raj-08/gists{/gist_id}", "starred_url": "https://api.github.com/users/Raj-08/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Raj-08/subscriptions", "organizations_url": "https://api.github.com/users/Raj-08/orgs", "repos_url": "https://api.github.com/users/Raj-08/repos", "events_url": "https://api.github.com/users/Raj-08/events{/privacy}", "received_events_url": "https://api.github.com/users/Raj-08/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-09-28T06:01:42Z", "updated_at": "2018-10-24T18:26:21Z", "closed_at": "2018-10-16T22:41:33Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.15.2</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0</li>\n<li><strong>GPU model and memory</strong>: NVIDIA 12 GB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>I have a model in which i have connected two mobilenet models . These are the following steps:</p>\n<ol>\n<li>\n<p>Train the model. - Success</p>\n</li>\n<li>\n<p>Freeze the graph using eval graph - Success<br>\n///Defiine the graph then<br>\n`<br>\nwith tf.Session() as sess:</p>\n<pre><code> saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')\n g = tf.get_default_graph()\n tf.contrib.quantize.create_eval_graph(input_graph=g)\n sess.run(tf.global_variables_initializer())\n saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')\n input_graph_def = graph.as_graph_def()\n output_graph_def = graph_util.convert_variables_to_constants(\n     sess,\n     input_graph_def,\n    [_OUTPUT_NAME])        \n with tf.gfile.GFile(output_graph_name, \"wb\") as f:\n     f.write(output_graph_def.SerializeToString())`\n</code></pre>\n</li>\n<li>\n<p>Toco convert -Success</p>\n</li>\n</ol>\n<p>tflite_convert  --output_file=./lite.tflite  --input=./frozen.pb --input_arrays=image_ph --output_arrays=SemanticPredictions --input_shapes=1,481,481,3</p>\n<p>4 . Load the model on phone - Error</p>\n<p>While loading the lite model on phone by running the command</p>\n<p><code>var localSource = FirebaseLocalModelSource.Builder(\"my_local_model\") .setAssetFilePath(\"lite.tflite\") .build() FirebaseModelManager.getInstance().registerLocalModelSource(localSource) var options = FirebaseModelOptions.Builder() .setLocalModelName(\"my_local_model\") .build() firebaseInterpreter = FirebaseModelInterpreter.getInstance(options) inputOutputOptions = FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3)) .setOutputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3)) .build()</code></p>\n<p>i get the following error:<br>\nMappedByteBuffer is not a valid flatbuffer model</p>\n<p>Any reason why am i getting this error or is it a tensorflow converter bug?`</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below): 1.10\nPython version: 2.7\nBazel version (if compiling from source): 0.15.2\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0\nGPU model and memory: NVIDIA 12 GB\nExact command to reproduce:\n\nI have a model in which i have connected two mobilenet models . These are the following steps:\n\n\nTrain the model. - Success\n\n\nFreeze the graph using eval graph - Success\n///Defiine the graph then\n`\nwith tf.Session() as sess:\n saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')\n g = tf.get_default_graph()\n tf.contrib.quantize.create_eval_graph(input_graph=g)\n sess.run(tf.global_variables_initializer())\n saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')\n input_graph_def = graph.as_graph_def()\n output_graph_def = graph_util.convert_variables_to_constants(\n     sess,\n     input_graph_def,\n    [_OUTPUT_NAME])        \n with tf.gfile.GFile(output_graph_name, \"wb\") as f:\n     f.write(output_graph_def.SerializeToString())`\n\n\n\nToco convert -Success\n\n\ntflite_convert  --output_file=./lite.tflite  --input=./frozen.pb --input_arrays=image_ph --output_arrays=SemanticPredictions --input_shapes=1,481,481,3\n4 . Load the model on phone - Error\nWhile loading the lite model on phone by running the command\nvar localSource = FirebaseLocalModelSource.Builder(\"my_local_model\") .setAssetFilePath(\"lite.tflite\") .build() FirebaseModelManager.getInstance().registerLocalModelSource(localSource) var options = FirebaseModelOptions.Builder() .setLocalModelName(\"my_local_model\") .build() firebaseInterpreter = FirebaseModelInterpreter.getInstance(options) inputOutputOptions = FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3)) .setOutputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3)) .build()\ni get the following error:\nMappedByteBuffer is not a valid flatbuffer model\nAny reason why am i getting this error or is it a tensorflow converter bug?`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: NVIDIA 12 GB\r\n- **Exact command to reproduce**:\r\n\r\nI have a model in which i have connected two mobilenet models . These are the following steps:\r\n\r\n1. Train the model. - Success\r\n2. Freeze the graph using eval graph - Success\r\n///Defiine the graph then \r\n` \r\n  with tf.Session() as sess:\r\n\r\n        saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')\r\n        g = tf.get_default_graph()\r\n        tf.contrib.quantize.create_eval_graph(input_graph=g)\r\n        sess.run(tf.global_variables_initializer())\r\n        saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')\r\n        input_graph_def = graph.as_graph_def()\r\n        output_graph_def = graph_util.convert_variables_to_constants(\r\n            sess,\r\n            input_graph_def,\r\n           [_OUTPUT_NAME])        \r\n        with tf.gfile.GFile(output_graph_name, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())`\r\n3. Toco convert -Success\r\n\r\ntflite_convert  --output_file=./lite.tflite  --input=./frozen.pb --input_arrays=image_ph --output_arrays=SemanticPredictions --input_shapes=1,481,481,3\r\n\r\n4 . Load the model on phone - Error\r\n\r\nWhile loading the lite model on phone by running the command\r\n\r\n`var localSource = FirebaseLocalModelSource.Builder(\"my_local_model\")\r\n               .setAssetFilePath(\"lite.tflite\")\r\n               .build()\r\n       FirebaseModelManager.getInstance().registerLocalModelSource(localSource)\r\n        var options = FirebaseModelOptions.Builder()\r\n               .setLocalModelName(\"my_local_model\")\r\n               .build()\r\n       firebaseInterpreter = FirebaseModelInterpreter.getInstance(options)\r\n       inputOutputOptions = FirebaseModelInputOutputOptions.Builder()\r\n               .setInputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3))\r\n               .setOutputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3))\r\n               .build()`\r\n\r\n i get the following error:\r\nMappedByteBuffer is not a valid flatbuffer model\r\n\r\nAny reason why am i getting this error or is it a tensorflow converter bug?`"}