{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7740", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7740/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7740/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7740/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7740", "id": 209171050, "node_id": "MDU6SXNzdWUyMDkxNzEwNTA=", "number": 7740, "title": "GradientDescentOptimizer got wrong result", "user": {"login": "ictxiangxin", "id": 3955842, "node_id": "MDQ6VXNlcjM5NTU4NDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3955842?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ictxiangxin", "html_url": "https://github.com/ictxiangxin", "followers_url": "https://api.github.com/users/ictxiangxin/followers", "following_url": "https://api.github.com/users/ictxiangxin/following{/other_user}", "gists_url": "https://api.github.com/users/ictxiangxin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ictxiangxin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ictxiangxin/subscriptions", "organizations_url": "https://api.github.com/users/ictxiangxin/orgs", "repos_url": "https://api.github.com/users/ictxiangxin/repos", "events_url": "https://api.github.com/users/ictxiangxin/events{/privacy}", "received_events_url": "https://api.github.com/users/ictxiangxin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-02-21T15:01:26Z", "updated_at": "2017-02-26T12:36:56Z", "closed_at": "2017-02-22T18:47:29Z", "author_association": "NONE", "body_html": "<p><strong>I want use gradient descent to solve equation set, but I got wrong result everytime, so I check my code and written a numpy edition, in this edition I provide explicit loss gradient and I can get currect result.</strong></p>\n<p>So I don't understand why GradientDescentOptimizer can not work.</p>\n<p><strong>here is my code without tf:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">SolveEquation</span>:\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">rate</span>: <span class=\"pl-c1\">float</span>, <span class=\"pl-smi\">loss_threshold</span>: <span class=\"pl-c1\">float</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0001</span>, <span class=\"pl-smi\">max_epochs</span>: <span class=\"pl-c1\">int</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>):\n        <span class=\"pl-c1\">self</span>.__rate <span class=\"pl-k\">=</span> rate\n        <span class=\"pl-c1\">self</span>.__loss_threshold <span class=\"pl-k\">=</span> loss_threshold\n        <span class=\"pl-c1\">self</span>.__max_epochs <span class=\"pl-k\">=</span> max_epochs\n        <span class=\"pl-c1\">self</span>.__x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">solve</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">coefficients</span>, <span class=\"pl-smi\">b</span>):\n        _a <span class=\"pl-k\">=</span> np.array(coefficients)\n        _b <span class=\"pl-k\">=</span> np.array(b).reshape([<span class=\"pl-c1\">len</span>(b), <span class=\"pl-c1\">1</span>])\n        _x <span class=\"pl-k\">=</span> np.zeros([_a.shape[<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">1</span>])\n        <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">self</span>.__max_epochs):\n            grad_loss <span class=\"pl-k\">=</span> np.matmul(np.transpose(_a), np.matmul(_a, _x) <span class=\"pl-k\">-</span> _b)\n            _x <span class=\"pl-k\">-=</span> <span class=\"pl-c1\">self</span>.__rate <span class=\"pl-k\">*</span> grad_loss\n            <span class=\"pl-k\">if</span> epoch <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                loss <span class=\"pl-k\">=</span> np.mean(np.square(np.subtract(np.matmul(_a, _x), _b)))\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss = <span class=\"pl-c1\">{<span class=\"pl-k\">:.8f</span>}</span><span class=\"pl-pds\">'</span></span>.format(loss))\n                <span class=\"pl-k\">if</span> loss <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">self</span>.__loss_threshold:\n                    <span class=\"pl-k\">break</span>\n        <span class=\"pl-k\">return</span> _x\n\ns <span class=\"pl-k\">=</span> SolveEquation(<span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">max_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(s.solve([[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>]], [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>]))</pre></div>\n<p><strong>And here is my code with tf:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">TFSolveEquation</span>:\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">rate</span>: <span class=\"pl-c1\">float</span>, <span class=\"pl-smi\">loss_threshold</span>: <span class=\"pl-c1\">float</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0001</span>, <span class=\"pl-smi\">max_epochs</span>: <span class=\"pl-c1\">int</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>):\n        <span class=\"pl-c1\">self</span>.__rate <span class=\"pl-k\">=</span> rate\n        <span class=\"pl-c1\">self</span>.__loss_threshold <span class=\"pl-k\">=</span> tf.constant(loss_threshold)\n        <span class=\"pl-c1\">self</span>.__max_epochs <span class=\"pl-k\">=</span> max_epochs\n        <span class=\"pl-c1\">self</span>.__session <span class=\"pl-k\">=</span> tf.Session()\n        <span class=\"pl-c1\">self</span>.__x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__del__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">try</span>:\n            <span class=\"pl-c1\">self</span>.__session.close()\n        <span class=\"pl-k\">finally</span>:\n            <span class=\"pl-k\">pass</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">solve</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">coefficients</span>, <span class=\"pl-smi\">b</span>):\n        coefficients_data <span class=\"pl-k\">=</span> np.array(coefficients)\n        b_data <span class=\"pl-k\">=</span> np.array(b)\n        _a <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\n        _b <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\n        _x <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([coefficients_data.shape[<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">1</span>]))\n        loss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.square(tf.matmul(_a, _x) <span class=\"pl-k\">-</span> _b))\n        optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">self</span>.__rate)\n        model <span class=\"pl-k\">=</span> optimizer.minimize(loss)\n        <span class=\"pl-c1\">self</span>.__session.run(tf.global_variables_initializer())\n        <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">self</span>.__max_epochs):\n            <span class=\"pl-c1\">self</span>.__session.run(model, {_a: coefficients_data, _b: b_data})\n            <span class=\"pl-k\">if</span> epoch <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.__session.run(loss <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">self</span>.__loss_threshold, {_a: coefficients_data, _b: b_data}):\n                    <span class=\"pl-k\">break</span>\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.__session.run(_x)\n\ns <span class=\"pl-k\">=</span> TFSolveEquation(<span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">max_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(s.solve([[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>]], [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>]))</pre></div>\n<p>I test these 2 codes with very simple equation set:</p>\n<p>x_1 + 2 * x_2 = 3<br>\nx_1 + 3 * x_3 = 4</p>\n<p>loss = 1/2 * || Ax - b ||^2</p>\n<p>Init x_1 = 0, x_2 = 0, rate = 0.1</p>\n<p><strong>Use gradient descent</strong><br>\n<strong>So at 1st compute, the delta x = (0.7, 1.8)</strong></p>\n<p>But unfortunately my code with tf give the<br>\ndelta x =<br>\n[[ 0.69999999]<br>\n[ 1.75      ]]</p>\n<p>And my code without tf give the<br>\ndelta x =<br>\n[[ 0.7]<br>\n[ 1.8]]</p>\n<p><strong>Absolutely code without tf is right, but why tf comput gradient may less 0.05 then currect result?</strong><br>\n<strong>I think this is the reason my code without tf can solve the equation set, but tf edition can not solve equation set currently.</strong></p>\n<p><strong>Can someone tell me why tf give a incurrent gradiant? Thanks</strong></p>\n<p>My platform is Win10 + tensorflow-gpu v1.0</p>", "body_text": "I want use gradient descent to solve equation set, but I got wrong result everytime, so I check my code and written a numpy edition, in this edition I provide explicit loss gradient and I can get currect result.\nSo I don't understand why GradientDescentOptimizer can not work.\nhere is my code without tf:\nimport numpy as np\n\n\nclass SolveEquation:\n    def __init__(self, rate: float, loss_threshold: float=0.0001, max_epochs: int=1000):\n        self.__rate = rate\n        self.__loss_threshold = loss_threshold\n        self.__max_epochs = max_epochs\n        self.__x = None\n\n    def solve(self, coefficients, b):\n        _a = np.array(coefficients)\n        _b = np.array(b).reshape([len(b), 1])\n        _x = np.zeros([_a.shape[1], 1])\n        for epoch in range(self.__max_epochs):\n            grad_loss = np.matmul(np.transpose(_a), np.matmul(_a, _x) - _b)\n            _x -= self.__rate * grad_loss\n            if epoch % 10 == 0:\n                loss = np.mean(np.square(np.subtract(np.matmul(_a, _x), _b)))\n                print('loss = {:.8f}'.format(loss))\n                if loss < self.__loss_threshold:\n                    break\n        return _x\n\ns = SolveEquation(0.1, max_epochs=1)\nprint(s.solve([[1, 2], [1, 3]], [3, 4]))\nAnd here is my code with tf:\nimport tensorflow as tf\nimport numpy as np\n\n\nclass TFSolveEquation:\n    def __init__(self, rate: float, loss_threshold: float=0.0001, max_epochs: int=1000):\n        self.__rate = rate\n        self.__loss_threshold = tf.constant(loss_threshold)\n        self.__max_epochs = max_epochs\n        self.__session = tf.Session()\n        self.__x = None\n\n    def __del__(self):\n        try:\n            self.__session.close()\n        finally:\n            pass\n\n    def solve(self, coefficients, b):\n        coefficients_data = np.array(coefficients)\n        b_data = np.array(b)\n        _a = tf.placeholder(tf.float32)\n        _b = tf.placeholder(tf.float32)\n        _x = tf.Variable(tf.zeros([coefficients_data.shape[1], 1]))\n        loss = tf.reduce_mean(tf.square(tf.matmul(_a, _x) - _b))\n        optimizer = tf.train.GradientDescentOptimizer(self.__rate)\n        model = optimizer.minimize(loss)\n        self.__session.run(tf.global_variables_initializer())\n        for epoch in range(self.__max_epochs):\n            self.__session.run(model, {_a: coefficients_data, _b: b_data})\n            if epoch % 10 == 0:\n                if self.__session.run(loss < self.__loss_threshold, {_a: coefficients_data, _b: b_data}):\n                    break\n        return self.__session.run(_x)\n\ns = TFSolveEquation(0.1, max_epochs=1)\nprint(s.solve([[1, 2], [1, 3]], [3, 4]))\nI test these 2 codes with very simple equation set:\nx_1 + 2 * x_2 = 3\nx_1 + 3 * x_3 = 4\nloss = 1/2 * || Ax - b ||^2\nInit x_1 = 0, x_2 = 0, rate = 0.1\nUse gradient descent\nSo at 1st compute, the delta x = (0.7, 1.8)\nBut unfortunately my code with tf give the\ndelta x =\n[[ 0.69999999]\n[ 1.75      ]]\nAnd my code without tf give the\ndelta x =\n[[ 0.7]\n[ 1.8]]\nAbsolutely code without tf is right, but why tf comput gradient may less 0.05 then currect result?\nI think this is the reason my code without tf can solve the equation set, but tf edition can not solve equation set currently.\nCan someone tell me why tf give a incurrent gradiant? Thanks\nMy platform is Win10 + tensorflow-gpu v1.0", "body": "**I want use gradient descent to solve equation set, but I got wrong result everytime, so I check my code and written a numpy edition, in this edition I provide explicit loss gradient and I can get currect result.**\r\n\r\nSo I don't understand why GradientDescentOptimizer can not work.\r\n\r\n**here is my code without tf:**\r\n\r\n```python\r\nimport numpy as np\r\n\r\n\r\nclass SolveEquation:\r\n    def __init__(self, rate: float, loss_threshold: float=0.0001, max_epochs: int=1000):\r\n        self.__rate = rate\r\n        self.__loss_threshold = loss_threshold\r\n        self.__max_epochs = max_epochs\r\n        self.__x = None\r\n\r\n    def solve(self, coefficients, b):\r\n        _a = np.array(coefficients)\r\n        _b = np.array(b).reshape([len(b), 1])\r\n        _x = np.zeros([_a.shape[1], 1])\r\n        for epoch in range(self.__max_epochs):\r\n            grad_loss = np.matmul(np.transpose(_a), np.matmul(_a, _x) - _b)\r\n            _x -= self.__rate * grad_loss\r\n            if epoch % 10 == 0:\r\n                loss = np.mean(np.square(np.subtract(np.matmul(_a, _x), _b)))\r\n                print('loss = {:.8f}'.format(loss))\r\n                if loss < self.__loss_threshold:\r\n                    break\r\n        return _x\r\n\r\ns = SolveEquation(0.1, max_epochs=1)\r\nprint(s.solve([[1, 2], [1, 3]], [3, 4]))\r\n```\r\n\r\n**And here is my code with tf:**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass TFSolveEquation:\r\n    def __init__(self, rate: float, loss_threshold: float=0.0001, max_epochs: int=1000):\r\n        self.__rate = rate\r\n        self.__loss_threshold = tf.constant(loss_threshold)\r\n        self.__max_epochs = max_epochs\r\n        self.__session = tf.Session()\r\n        self.__x = None\r\n\r\n    def __del__(self):\r\n        try:\r\n            self.__session.close()\r\n        finally:\r\n            pass\r\n\r\n    def solve(self, coefficients, b):\r\n        coefficients_data = np.array(coefficients)\r\n        b_data = np.array(b)\r\n        _a = tf.placeholder(tf.float32)\r\n        _b = tf.placeholder(tf.float32)\r\n        _x = tf.Variable(tf.zeros([coefficients_data.shape[1], 1]))\r\n        loss = tf.reduce_mean(tf.square(tf.matmul(_a, _x) - _b))\r\n        optimizer = tf.train.GradientDescentOptimizer(self.__rate)\r\n        model = optimizer.minimize(loss)\r\n        self.__session.run(tf.global_variables_initializer())\r\n        for epoch in range(self.__max_epochs):\r\n            self.__session.run(model, {_a: coefficients_data, _b: b_data})\r\n            if epoch % 10 == 0:\r\n                if self.__session.run(loss < self.__loss_threshold, {_a: coefficients_data, _b: b_data}):\r\n                    break\r\n        return self.__session.run(_x)\r\n\r\ns = TFSolveEquation(0.1, max_epochs=1)\r\nprint(s.solve([[1, 2], [1, 3]], [3, 4]))\r\n```\r\n\r\nI test these 2 codes with very simple equation set:\r\n\r\nx_1 + 2 * x_2 = 3\r\nx_1 + 3 * x_3 = 4\r\n\r\nloss = 1/2 * || Ax - b ||^2\r\n\r\nInit x_1 = 0, x_2 = 0, rate = 0.1\r\n\r\n**Use gradient descent**\r\n**So at 1st compute, the delta x = (0.7, 1.8)**\r\n\r\nBut unfortunately my code with tf give the \r\ndelta x = \r\n[[ 0.69999999]\r\n [ 1.75      ]]\r\n\r\nAnd my code without tf give the\r\ndelta x = \r\n[[ 0.7]\r\n [ 1.8]]\r\n\r\n**Absolutely code without tf is right, but why tf comput gradient may less 0.05 then currect result?**\r\n**I think this is the reason my code without tf can solve the equation set, but tf edition can not solve equation set currently.**\r\n\r\n**Can someone tell me why tf give a incurrent gradiant? Thanks**\r\n\r\nMy platform is Win10 + tensorflow-gpu v1.0\r\n"}