{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/319353378", "html_url": "https://github.com/tensorflow/tensorflow/issues/11868#issuecomment-319353378", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11868", "id": 319353378, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTM1MzM3OA==", "user": {"login": "taehoonlee", "id": 4445535, "node_id": "MDQ6VXNlcjQ0NDU1MzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/4445535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taehoonlee", "html_url": "https://github.com/taehoonlee", "followers_url": "https://api.github.com/users/taehoonlee/followers", "following_url": "https://api.github.com/users/taehoonlee/following{/other_user}", "gists_url": "https://api.github.com/users/taehoonlee/gists{/gist_id}", "starred_url": "https://api.github.com/users/taehoonlee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taehoonlee/subscriptions", "organizations_url": "https://api.github.com/users/taehoonlee/orgs", "repos_url": "https://api.github.com/users/taehoonlee/repos", "events_url": "https://api.github.com/users/taehoonlee/events{/privacy}", "received_events_url": "https://api.github.com/users/taehoonlee/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-01T12:16:45Z", "updated_at": "2017-08-01T12:16:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There is no significant change in the resnet script, but several tf-keras layers have been refactored in <a href=\"https://github.com/tensorflow/tensorflow/commit/35253fa89c5f8af25e3d84f76980729569091a6c\">here</a>. Thus, I tested the four individual layers mainly used in the resnet, and found something weird in <code>Conv2D</code> as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> keras\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nx <span class=\"pl-k\">=</span> np.random.random((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>))\nx1 <span class=\"pl-k\">=</span> keras.layers.Input((<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>))\nx2 <span class=\"pl-k\">=</span> tf.contrib.keras.layers.Input((<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>))\n\nlayers1 <span class=\"pl-k\">=</span> [\n    keras.layers.Conv2D(<span class=\"pl-c1\">10</span>, (<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>))(x1),\n    keras.layers.BatchNormalization()(x1),\n    keras.layers.MaxPooling2D()(x1),\n    keras.layers.AveragePooling2D()(x1)\n]\n\nlayers2 <span class=\"pl-k\">=</span> [\n    tf.contrib.keras.layers.Conv2D(<span class=\"pl-c1\">10</span>, (<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>))(x2),\n    tf.contrib.keras.layers.BatchNormalization()(x2),\n    tf.contrib.keras.layers.MaxPooling2D()(x2),\n    tf.contrib.keras.layers.AveragePooling2D()(x2)\n]\n\n<span class=\"pl-k\">for</span> (k, t) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(layers1, layers2):\n    model1 <span class=\"pl-k\">=</span> keras.models.Model(x1, k)\n    model2 <span class=\"pl-k\">=</span> tf.contrib.keras.models.Model(x2, t)\n    y1 <span class=\"pl-k\">=</span> model1.predict(x)\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        sess.run(tf.global_variables_initializer())\n        y2 <span class=\"pl-k\">=</span> model2.predict(x)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">%s</span>: <span class=\"pl-c1\">%.3f</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (t.name, np.sum((y1 <span class=\"pl-k\">-</span> y2) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>)))</pre></div>\n<p>The results are:</p>\n<pre><code>conv2d/BiasAdd:0: 9.476\nbatch_normalization/batchnorm/add_1:0: 0.000\nmax_pooling2d/MaxPool:0: 0.000\naverage_pooling2d/AvgPool:0: 0.000\n</code></pre>", "body_text": "There is no significant change in the resnet script, but several tf-keras layers have been refactored in here. Thus, I tested the four individual layers mainly used in the resnet, and found something weird in Conv2D as follows:\nimport keras\nimport tensorflow as tf\nimport numpy as np\n\nx = np.random.random((1, 6, 6, 3))\nx1 = keras.layers.Input((6, 6, 3))\nx2 = tf.contrib.keras.layers.Input((6, 6, 3))\n\nlayers1 = [\n    keras.layers.Conv2D(10, (5, 5))(x1),\n    keras.layers.BatchNormalization()(x1),\n    keras.layers.MaxPooling2D()(x1),\n    keras.layers.AveragePooling2D()(x1)\n]\n\nlayers2 = [\n    tf.contrib.keras.layers.Conv2D(10, (5, 5))(x2),\n    tf.contrib.keras.layers.BatchNormalization()(x2),\n    tf.contrib.keras.layers.MaxPooling2D()(x2),\n    tf.contrib.keras.layers.AveragePooling2D()(x2)\n]\n\nfor (k, t) in zip(layers1, layers2):\n    model1 = keras.models.Model(x1, k)\n    model2 = tf.contrib.keras.models.Model(x2, t)\n    y1 = model1.predict(x)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        y2 = model2.predict(x)\n    print(\"%s: %.3f\" % (t.name, np.sum((y1 - y2) ** 2)))\nThe results are:\nconv2d/BiasAdd:0: 9.476\nbatch_normalization/batchnorm/add_1:0: 0.000\nmax_pooling2d/MaxPool:0: 0.000\naverage_pooling2d/AvgPool:0: 0.000", "body": "There is no significant change in the resnet script, but several tf-keras layers have been refactored in [here](https://github.com/tensorflow/tensorflow/commit/35253fa89c5f8af25e3d84f76980729569091a6c). Thus, I tested the four individual layers mainly used in the resnet, and found something weird in `Conv2D` as follows:\r\n\r\n```python\r\nimport keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = np.random.random((1, 6, 6, 3))\r\nx1 = keras.layers.Input((6, 6, 3))\r\nx2 = tf.contrib.keras.layers.Input((6, 6, 3))\r\n\r\nlayers1 = [\r\n    keras.layers.Conv2D(10, (5, 5))(x1),\r\n    keras.layers.BatchNormalization()(x1),\r\n    keras.layers.MaxPooling2D()(x1),\r\n    keras.layers.AveragePooling2D()(x1)\r\n]\r\n\r\nlayers2 = [\r\n    tf.contrib.keras.layers.Conv2D(10, (5, 5))(x2),\r\n    tf.contrib.keras.layers.BatchNormalization()(x2),\r\n    tf.contrib.keras.layers.MaxPooling2D()(x2),\r\n    tf.contrib.keras.layers.AveragePooling2D()(x2)\r\n]\r\n\r\nfor (k, t) in zip(layers1, layers2):\r\n    model1 = keras.models.Model(x1, k)\r\n    model2 = tf.contrib.keras.models.Model(x2, t)\r\n    y1 = model1.predict(x)\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        y2 = model2.predict(x)\r\n    print(\"%s: %.3f\" % (t.name, np.sum((y1 - y2) ** 2)))\r\n```\r\nThe results are:\r\n```\r\nconv2d/BiasAdd:0: 9.476\r\nbatch_normalization/batchnorm/add_1:0: 0.000\r\nmax_pooling2d/MaxPool:0: 0.000\r\naverage_pooling2d/AvgPool:0: 0.000\r\n```"}