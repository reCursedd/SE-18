{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321169302", "html_url": "https://github.com/tensorflow/tensorflow/issues/11868#issuecomment-321169302", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11868", "id": 321169302, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTE2OTMwMg==", "user": {"login": "taehoonlee", "id": 4445535, "node_id": "MDQ6VXNlcjQ0NDU1MzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/4445535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taehoonlee", "html_url": "https://github.com/taehoonlee", "followers_url": "https://api.github.com/users/taehoonlee/followers", "following_url": "https://api.github.com/users/taehoonlee/following{/other_user}", "gists_url": "https://api.github.com/users/taehoonlee/gists{/gist_id}", "starred_url": "https://api.github.com/users/taehoonlee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taehoonlee/subscriptions", "organizations_url": "https://api.github.com/users/taehoonlee/orgs", "repos_url": "https://api.github.com/users/taehoonlee/repos", "events_url": "https://api.github.com/users/taehoonlee/events{/privacy}", "received_events_url": "https://api.github.com/users/taehoonlee/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-09T06:52:03Z", "updated_at": "2017-08-09T06:52:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I examined each of the layers in ResNet50 and found a slight difference between <code>BatchNormalization</code> layers of keras and tf-keras. Would you please check this comment <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fchollet\">@fchollet</a>?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> keras\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">from</span> keras.preprocessing <span class=\"pl-k\">import</span> image\n<span class=\"pl-k\">from</span> keras.applications.resnet50 <span class=\"pl-k\">import</span> preprocess_input, decode_predictions\nimg <span class=\"pl-k\">=</span> image.load_img(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>elephant.png<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">target_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>))\nx <span class=\"pl-k\">=</span> image.img_to_array(img)\nx <span class=\"pl-k\">=</span> np.expand_dims(x, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\nx <span class=\"pl-k\">=</span> preprocess_input(x)\n\nresnet1 <span class=\"pl-k\">=</span> keras.applications.ResNet50(<span class=\"pl-v\">weights</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>imagenet<span class=\"pl-pds\">'</span></span>)\nresnet2 <span class=\"pl-k\">=</span> tf.contrib.keras.applications.ResNet50(<span class=\"pl-v\">weights</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>imagenet<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>):\n    model1 <span class=\"pl-k\">=</span> keras.models.Model(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>resnet1.input, <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>resnet1.layers[i].output)\n    model2 <span class=\"pl-k\">=</span> tf.contrib.keras.models.Model(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>resnet2.input, <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>resnet2.layers[i].output)\n    y1 <span class=\"pl-k\">=</span> model1.predict(x)\n    y2 <span class=\"pl-k\">=</span> model2.predict(x)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>- <span class=\"pl-c1\">%s</span> -<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> model1.layers[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>].name)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Residual sum: <span class=\"pl-c1\">%.3f</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> np.sum((y1 <span class=\"pl-k\">-</span> y2) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Keras weights: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> model1.layers[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>].weights)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TF-Keras weights: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> model2.layers[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>].weights)</pre></div>\n<p>The results are:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">-</span> conv1 <span class=\"pl-k\">-</span>\nResidual <span class=\"pl-c1\">sum</span>: <span class=\"pl-c1\">0.000</span>\nKeras weights: [<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1/kernel:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>) dtype=float32_ref<span class=\"pl-k\">&gt;</span>, <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1/bias:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>]\n<span class=\"pl-c1\">TF</span><span class=\"pl-k\">-</span>Keras weights: [<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1/kernel_1:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>) dtype=float32_ref<span class=\"pl-k\">&gt;</span>, <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1/bias_1:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>]\n<span class=\"pl-k\">-</span> bn_conv1 <span class=\"pl-k\">-</span>\nResidual <span class=\"pl-c1\">sum</span>: <span class=\"pl-c1\">9147763.000</span>\nKeras weights: [<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_conv1/gamma:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>, <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_conv1/beta:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>, <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_conv1/moving_mean:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>, <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_conv1/moving_variance:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>]\n<span class=\"pl-c1\">TF</span><span class=\"pl-k\">-</span>Keras weights: [<span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_conv1/beta_1:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>, <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_conv1/gamma_1:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>, <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_conv1/moving_mean_1:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>, <span class=\"pl-k\">&lt;</span>tf.Variable <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn_conv1/moving_variance_1:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-c1\">64</span>,) dtype=float32_ref<span class=\"pl-k\">&gt;</span>]</pre></div>\n<p>While the gamma is defined first in original keras, the beta is done first in tf keras. This order is important because the <code>load_weights()</code> of keras API checks shapes of symbolic and numpy tensors according to the order. In order words, pretrained variables are stored as [gamma, beta] format but tf-keras reads them as [beta, gamma] without error due to the same shape. The <code>load_weights()</code> doesn't check tensor names.</p>\n<p>I listed up two concise solutions as follows:</p>\n<ul>\n<li>Swapping the two <code>self.add_variable</code> calls for gamma and beta in <code>tensorflow/python/layers/normalization.py</code>: I think this might be the best if the swapping doesn't break someone's code.</li>\n<li>Adding <code>self.trainable_weights = self.trainable_weights[::-1]</code> after layer building in <code>tensorflow/contrib/keras/python/keras/layers/normalization.py</code>: This seems to be a patchwork but no one's codes will break.<br>\nWhich one of these looks better?</li>\n</ul>", "body_text": "I examined each of the layers in ResNet50 and found a slight difference between BatchNormalization layers of keras and tf-keras. Would you please check this comment @aselle, @fchollet?\nimport keras\nimport tensorflow as tf\nimport numpy as np\n\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\nimg = image.load_img('elephant.png', target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nresnet1 = keras.applications.ResNet50(weights='imagenet')\nresnet2 = tf.contrib.keras.applications.ResNet50(weights='imagenet')\n\nfor i in range(1, 3):\n    model1 = keras.models.Model(inputs=resnet1.input, outputs=resnet1.layers[i].output)\n    model2 = tf.contrib.keras.models.Model(inputs=resnet2.input, outputs=resnet2.layers[i].output)\n    y1 = model1.predict(x)\n    y2 = model2.predict(x)\n    print(\"- %s -\" % model1.layers[-1].name)\n    print(\"Residual sum: %.3f\" % np.sum((y1 - y2) ** 2))\n    print(\"Keras weights: %s\" % model1.layers[-1].weights)\n    print(\"TF-Keras weights: %s\" % model2.layers[-1].weights)\nThe results are:\n- conv1 -\nResidual sum: 0.000\nKeras weights: [<tf.Variable 'conv1/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref>, <tf.Variable 'conv1/bias:0' shape=(64,) dtype=float32_ref>]\nTF-Keras weights: [<tf.Variable 'conv1/kernel_1:0' shape=(7, 7, 3, 64) dtype=float32_ref>, <tf.Variable 'conv1/bias_1:0' shape=(64,) dtype=float32_ref>]\n- bn_conv1 -\nResidual sum: 9147763.000\nKeras weights: [<tf.Variable 'bn_conv1/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_mean:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_variance:0' shape=(64,) dtype=float32_ref>]\nTF-Keras weights: [<tf.Variable 'bn_conv1/beta_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/gamma_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_mean_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_variance_1:0' shape=(64,) dtype=float32_ref>]\nWhile the gamma is defined first in original keras, the beta is done first in tf keras. This order is important because the load_weights() of keras API checks shapes of symbolic and numpy tensors according to the order. In order words, pretrained variables are stored as [gamma, beta] format but tf-keras reads them as [beta, gamma] without error due to the same shape. The load_weights() doesn't check tensor names.\nI listed up two concise solutions as follows:\n\nSwapping the two self.add_variable calls for gamma and beta in tensorflow/python/layers/normalization.py: I think this might be the best if the swapping doesn't break someone's code.\nAdding self.trainable_weights = self.trainable_weights[::-1] after layer building in tensorflow/contrib/keras/python/keras/layers/normalization.py: This seems to be a patchwork but no one's codes will break.\nWhich one of these looks better?", "body": "I examined each of the layers in ResNet50 and found a slight difference between `BatchNormalization` layers of keras and tf-keras. Would you please check this comment @aselle, @fchollet?\r\n\r\n```python\r\nimport keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom keras.preprocessing import image\r\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\r\nimg = image.load_img('elephant.png', target_size=(224, 224))\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\nx = preprocess_input(x)\r\n\r\nresnet1 = keras.applications.ResNet50(weights='imagenet')\r\nresnet2 = tf.contrib.keras.applications.ResNet50(weights='imagenet')\r\n\r\nfor i in range(1, 3):\r\n    model1 = keras.models.Model(inputs=resnet1.input, outputs=resnet1.layers[i].output)\r\n    model2 = tf.contrib.keras.models.Model(inputs=resnet2.input, outputs=resnet2.layers[i].output)\r\n    y1 = model1.predict(x)\r\n    y2 = model2.predict(x)\r\n    print(\"- %s -\" % model1.layers[-1].name)\r\n    print(\"Residual sum: %.3f\" % np.sum((y1 - y2) ** 2))\r\n    print(\"Keras weights: %s\" % model1.layers[-1].weights)\r\n    print(\"TF-Keras weights: %s\" % model2.layers[-1].weights)\r\n```\r\nThe results are:\r\n```python\r\n- conv1 -\r\nResidual sum: 0.000\r\nKeras weights: [<tf.Variable 'conv1/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref>, <tf.Variable 'conv1/bias:0' shape=(64,) dtype=float32_ref>]\r\nTF-Keras weights: [<tf.Variable 'conv1/kernel_1:0' shape=(7, 7, 3, 64) dtype=float32_ref>, <tf.Variable 'conv1/bias_1:0' shape=(64,) dtype=float32_ref>]\r\n- bn_conv1 -\r\nResidual sum: 9147763.000\r\nKeras weights: [<tf.Variable 'bn_conv1/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_mean:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_variance:0' shape=(64,) dtype=float32_ref>]\r\nTF-Keras weights: [<tf.Variable 'bn_conv1/beta_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/gamma_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_mean_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_variance_1:0' shape=(64,) dtype=float32_ref>]\r\n```\r\n\r\nWhile the gamma is defined first in original keras, the beta is done first in tf keras. This order is important because the `load_weights()` of keras API checks shapes of symbolic and numpy tensors according to the order. In order words, pretrained variables are stored as [gamma, beta] format but tf-keras reads them as [beta, gamma] without error due to the same shape. The `load_weights()` doesn't check tensor names.\r\n\r\nI listed up two concise solutions as follows:\r\n- Swapping the two `self.add_variable` calls for gamma and beta in `tensorflow/python/layers/normalization.py`: I think this might be the best if the swapping doesn't break someone's code.\r\n- Adding `self.trainable_weights = self.trainable_weights[::-1]` after layer building in `tensorflow/contrib/keras/python/keras/layers/normalization.py`: This seems to be a patchwork but no one's codes will break.\r\nWhich one of these looks better?"}