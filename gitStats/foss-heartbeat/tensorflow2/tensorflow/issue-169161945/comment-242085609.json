{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242085609", "html_url": "https://github.com/tensorflow/tensorflow/issues/3624#issuecomment-242085609", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3624", "id": 242085609, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MjA4NTYwOQ==", "user": {"login": "sbrodeur", "id": 4322357, "node_id": "MDQ6VXNlcjQzMjIzNTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/4322357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sbrodeur", "html_url": "https://github.com/sbrodeur", "followers_url": "https://api.github.com/users/sbrodeur/followers", "following_url": "https://api.github.com/users/sbrodeur/following{/other_user}", "gists_url": "https://api.github.com/users/sbrodeur/gists{/gist_id}", "starred_url": "https://api.github.com/users/sbrodeur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sbrodeur/subscriptions", "organizations_url": "https://api.github.com/users/sbrodeur/orgs", "repos_url": "https://api.github.com/users/sbrodeur/repos", "events_url": "https://api.github.com/users/sbrodeur/events{/privacy}", "received_events_url": "https://api.github.com/users/sbrodeur/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-24T14:31:48Z", "updated_at": "2016-08-24T14:32:39Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10001157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/iportillo\">@iportillo</a> - I will give it another try today. It would also significantly accelerate my experiments, since everything could run on the GPU. I'll try to see if it would be easy to use CUDABlas directly (rather than Eigen) for the basic math functions on complex numbers.</p>\n<p>tf.complex_abs is easy to implement on GPU right now:</p>\n<pre><code>def complex_abs(x):\n    return tf.sqrt(tf.square(tf.real(x)) + tf.square(tf.imag(x)))\n</code></pre>\n<p>By tf.exp(), do you mean converting from the Cartesian to the complex exponential form (angle and norm)? To calculate the angle, this means implementing the atan2 function (for complex x + iy):</p>\n<pre><code>def atan2(y, x):\n    angle = tf.select(tf.greater(x,0.0), tf.atan(y/x) + np.pi, tf.zeros_like(x))\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), np.nan * tf.zeros_like(x), angle)\n    return angle\n\ndef complex_arg(x):\n    return atan2(tf.imag(x), tf.real(x))\n</code></pre>\n<p>It's not optimized but works well on GPU.</p>", "body_text": "@iportillo - I will give it another try today. It would also significantly accelerate my experiments, since everything could run on the GPU. I'll try to see if it would be easy to use CUDABlas directly (rather than Eigen) for the basic math functions on complex numbers.\ntf.complex_abs is easy to implement on GPU right now:\ndef complex_abs(x):\n    return tf.sqrt(tf.square(tf.real(x)) + tf.square(tf.imag(x)))\n\nBy tf.exp(), do you mean converting from the Cartesian to the complex exponential form (angle and norm)? To calculate the angle, this means implementing the atan2 function (for complex x + iy):\ndef atan2(y, x):\n    angle = tf.select(tf.greater(x,0.0), tf.atan(y/x) + np.pi, tf.zeros_like(x))\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), np.nan * tf.zeros_like(x), angle)\n    return angle\n\ndef complex_arg(x):\n    return atan2(tf.imag(x), tf.real(x))\n\nIt's not optimized but works well on GPU.", "body": "@iportillo - I will give it another try today. It would also significantly accelerate my experiments, since everything could run on the GPU. I'll try to see if it would be easy to use CUDABlas directly (rather than Eigen) for the basic math functions on complex numbers.\n\ntf.complex_abs is easy to implement on GPU right now:\n\n```\ndef complex_abs(x):\n    return tf.sqrt(tf.square(tf.real(x)) + tf.square(tf.imag(x)))\n```\n\nBy tf.exp(), do you mean converting from the Cartesian to the complex exponential form (angle and norm)? To calculate the angle, this means implementing the atan2 function (for complex x + iy):\n\n```\ndef atan2(y, x):\n    angle = tf.select(tf.greater(x,0.0), tf.atan(y/x) + np.pi, tf.zeros_like(x))\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\n    angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\n    angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), np.nan * tf.zeros_like(x), angle)\n    return angle\n\ndef complex_arg(x):\n    return atan2(tf.imag(x), tf.real(x))\n```\n\nIt's not optimized but works well on GPU.\n"}