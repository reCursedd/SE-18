{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4033", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4033/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4033/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4033/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4033", "id": 173117674, "node_id": "MDU6SXNzdWUxNzMxMTc2NzQ=", "number": 4033, "title": "tensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them", "user": {"login": "abhijayghildyal", "id": 11029633, "node_id": "MDQ6VXNlcjExMDI5NjMz", "avatar_url": "https://avatars2.githubusercontent.com/u/11029633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhijayghildyal", "html_url": "https://github.com/abhijayghildyal", "followers_url": "https://api.github.com/users/abhijayghildyal/followers", "following_url": "https://api.github.com/users/abhijayghildyal/following{/other_user}", "gists_url": "https://api.github.com/users/abhijayghildyal/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhijayghildyal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhijayghildyal/subscriptions", "organizations_url": "https://api.github.com/users/abhijayghildyal/orgs", "repos_url": "https://api.github.com/users/abhijayghildyal/repos", "events_url": "https://api.github.com/users/abhijayghildyal/events{/privacy}", "received_events_url": "https://api.github.com/users/abhijayghildyal/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2016-08-25T05:33:37Z", "updated_at": "2016-08-30T07:11:53Z", "closed_at": "2016-08-30T07:01:27Z", "author_association": "NONE", "body_html": "<p>I am running <code>distributed tensorflow</code> version of <code>deep mnist</code> <a href=\"https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html\" rel=\"nofollow\">(link)</a> (<code>asynchronous</code> data parallelism on different machines with gpu). The batch_size I have taken for one epoch is 1000 images and I am running 1000 epochs. My architecture has one <code>ps</code> on machine 1 and two workers, with <code>worker task_index=0</code> as <code>chief</code> on machine 2 and machine 1 has <code>worker task_index=1</code>.</p>\n<p>I really don't understand why I get this error sometimes when I run <code>worker task_index=1</code> on machine 1. The error does not persist though if I try running it a couple of times.</p>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\ncannot import name hashtable\ncannot import name hashtable\ncannot import name hashtable\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:118] Found device 0 with properties: \nname: GeForce GTX 750 Ti\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\npciBusID 0000:02:00.0\nTotal memory: 2.00GiB\nFree memory: 125.53MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:02:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 125.53M (131629056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {172.25.1.127:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {172.25.1.108:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:203] Started server with target: grpc://localhost:2222\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nTraceback (most recent call last):\n  File \"dist_trainer_deepMnist_v2.py\", line 205, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"dist_trainer_deepMnist_v2.py\", line 157, in main\n    with sv.managed_session(server.target) as sess:\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 797, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 958, in managed_session\n    start_standard_services=start_standard_services)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 722, in prepare_or_wait_for_session\n    max_wait_secs=max_wait_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 351, in wait_for_session\n    is_ready, not_ready_msg = self._model_ready(sess)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 437, in _model_ready\n    return self._ready(self._ready_op, sess, \"Model not ready\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 406, in _ready\n    ready_value = sess.run(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 2402 elements; but when writing their indices, saw 19 elements.\n     [[Node: report_uninitialized_variables/Where = Where[_device=\"/job:ps/replica:0/task:0/cpu:0\"](report_uninitialized_variables/Reshape_1)]]\nCaused by op u'report_uninitialized_variables/Where', defined at:\n  File \"dist_trainer_deepMnist_v2.py\", line 205, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"dist_trainer_deepMnist_v2.py\", line 153, in main\n    save_model_secs=600)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 310, in __init__\n    ready_op=ready_op, ready_for_local_init_op=ready_for_local_init_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 399, in _init_ready_op\n    ready_op = variables.report_uninitialized_variables()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 1031, in report_uninitialized_variables\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 888, in boolean_mask\n    return _apply_mask_1d(tensor, mask)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 863, in _apply_mask_1d\n    indices = squeeze(where(mask), squeeze_dims=[1])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2663, in where\n    result = _op_def_lib.apply_op(\"Where\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2333, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1252, in __init__\n    self._traceback = _extract_stack()\n</code></pre>\n<p>I tried looking for a reason as to why this error occurs, but couldn't find one. One clue is this part in the error</p>\n<pre><code>tensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 2402 elements; but when writing their indices, saw 19 elements.\n [[Node: report_uninitialized_variables/Where = Where[_device=\"/job:ps/replica:0/task:0/cpu:0\"](report_uninitialized_variables/Reshape_1)]]\nCaused by op u'report_uninitialized_variables/Where',\n</code></pre>\n<p>Another clue is</p>\n<pre><code>E tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 125.53M (131629056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n</code></pre>\n<p>this is there when I am running the ps on the same machine which is also using the gpu. It'll be great if someone can explain why this happens?</p>\n<p>(Asked the same on <a href=\"http://stackoverflow.com/questions/39130553/tensorflow-python-framework-errors-invalidargumenterror-whereop-race-condition?noredirect=1#comment65613229_39130553\" rel=\"nofollow\">stack overflow</a>)</p>", "body_text": "I am running distributed tensorflow version of deep mnist (link) (asynchronous data parallelism on different machines with gpu). The batch_size I have taken for one epoch is 1000 images and I am running 1000 epochs. My architecture has one ps on machine 1 and two workers, with worker task_index=0 as chief on machine 2 and machine 1 has worker task_index=1.\nI really don't understand why I get this error sometimes when I run worker task_index=1 on machine 1. The error does not persist though if I try running it a couple of times.\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\ncannot import name hashtable\ncannot import name hashtable\ncannot import name hashtable\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:118] Found device 0 with properties: \nname: GeForce GTX 750 Ti\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\npciBusID 0000:02:00.0\nTotal memory: 2.00GiB\nFree memory: 125.53MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:02:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 125.53M (131629056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {172.25.1.127:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {172.25.1.108:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:203] Started server with target: grpc://localhost:2222\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nTraceback (most recent call last):\n  File \"dist_trainer_deepMnist_v2.py\", line 205, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"dist_trainer_deepMnist_v2.py\", line 157, in main\n    with sv.managed_session(server.target) as sess:\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 797, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 958, in managed_session\n    start_standard_services=start_standard_services)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 722, in prepare_or_wait_for_session\n    max_wait_secs=max_wait_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 351, in wait_for_session\n    is_ready, not_ready_msg = self._model_ready(sess)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 437, in _model_ready\n    return self._ready(self._ready_op, sess, \"Model not ready\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 406, in _ready\n    ready_value = sess.run(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 2402 elements; but when writing their indices, saw 19 elements.\n     [[Node: report_uninitialized_variables/Where = Where[_device=\"/job:ps/replica:0/task:0/cpu:0\"](report_uninitialized_variables/Reshape_1)]]\nCaused by op u'report_uninitialized_variables/Where', defined at:\n  File \"dist_trainer_deepMnist_v2.py\", line 205, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"dist_trainer_deepMnist_v2.py\", line 153, in main\n    save_model_secs=600)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 310, in __init__\n    ready_op=ready_op, ready_for_local_init_op=ready_for_local_init_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 399, in _init_ready_op\n    ready_op = variables.report_uninitialized_variables()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 1031, in report_uninitialized_variables\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 888, in boolean_mask\n    return _apply_mask_1d(tensor, mask)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 863, in _apply_mask_1d\n    indices = squeeze(where(mask), squeeze_dims=[1])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2663, in where\n    result = _op_def_lib.apply_op(\"Where\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2333, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1252, in __init__\n    self._traceback = _extract_stack()\n\nI tried looking for a reason as to why this error occurs, but couldn't find one. One clue is this part in the error\ntensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 2402 elements; but when writing their indices, saw 19 elements.\n [[Node: report_uninitialized_variables/Where = Where[_device=\"/job:ps/replica:0/task:0/cpu:0\"](report_uninitialized_variables/Reshape_1)]]\nCaused by op u'report_uninitialized_variables/Where',\n\nAnother clue is\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 125.53M (131629056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n\nthis is there when I am running the ps on the same machine which is also using the gpu. It'll be great if someone can explain why this happens?\n(Asked the same on stack overflow)", "body": "I am running `distributed tensorflow` version of `deep mnist` [(link)](https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html) (`asynchronous` data parallelism on different machines with gpu). The batch_size I have taken for one epoch is 1000 images and I am running 1000 epochs. My architecture has one `ps` on machine 1 and two workers, with `worker task_index=0` as `chief` on machine 2 and machine 1 has `worker task_index=1`.\n\nI really don't understand why I get this error sometimes when I run `worker task_index=1` on machine 1. The error does not persist though if I try running it a couple of times.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\ncannot import name hashtable\ncannot import name hashtable\ncannot import name hashtable\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:118] Found device 0 with properties: \nname: GeForce GTX 750 Ti\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\npciBusID 0000:02:00.0\nTotal memory: 2.00GiB\nFree memory: 125.53MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:02:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 125.53M (131629056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {172.25.1.127:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {172.25.1.108:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:203] Started server with target: grpc://localhost:2222\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nTraceback (most recent call last):\n  File \"dist_trainer_deepMnist_v2.py\", line 205, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"dist_trainer_deepMnist_v2.py\", line 157, in main\n    with sv.managed_session(server.target) as sess:\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 797, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 958, in managed_session\n    start_standard_services=start_standard_services)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 722, in prepare_or_wait_for_session\n    max_wait_secs=max_wait_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 351, in wait_for_session\n    is_ready, not_ready_msg = self._model_ready(sess)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 437, in _model_ready\n    return self._ready(self._ready_op, sess, \"Model not ready\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 406, in _ready\n    ready_value = sess.run(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 2402 elements; but when writing their indices, saw 19 elements.\n     [[Node: report_uninitialized_variables/Where = Where[_device=\"/job:ps/replica:0/task:0/cpu:0\"](report_uninitialized_variables/Reshape_1)]]\nCaused by op u'report_uninitialized_variables/Where', defined at:\n  File \"dist_trainer_deepMnist_v2.py\", line 205, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"dist_trainer_deepMnist_v2.py\", line 153, in main\n    save_model_secs=600)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 310, in __init__\n    ready_op=ready_op, ready_for_local_init_op=ready_for_local_init_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 399, in _init_ready_op\n    ready_op = variables.report_uninitialized_variables()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 1031, in report_uninitialized_variables\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 888, in boolean_mask\n    return _apply_mask_1d(tensor, mask)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 863, in _apply_mask_1d\n    indices = squeeze(where(mask), squeeze_dims=[1])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2663, in where\n    result = _op_def_lib.apply_op(\"Where\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2333, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1252, in __init__\n    self._traceback = _extract_stack()\n```\n\nI tried looking for a reason as to why this error occurs, but couldn't find one. One clue is this part in the error\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 2402 elements; but when writing their indices, saw 19 elements.\n [[Node: report_uninitialized_variables/Where = Where[_device=\"/job:ps/replica:0/task:0/cpu:0\"](report_uninitialized_variables/Reshape_1)]]\nCaused by op u'report_uninitialized_variables/Where',\n```\n\nAnother clue is \n\n```\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 125.53M (131629056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n```\n\nthis is there when I am running the ps on the same machine which is also using the gpu. It'll be great if someone can explain why this happens?\n\n(Asked the same on [stack overflow](http://stackoverflow.com/questions/39130553/tensorflow-python-framework-errors-invalidargumenterror-whereop-race-condition?noredirect=1#comment65613229_39130553))\n"}