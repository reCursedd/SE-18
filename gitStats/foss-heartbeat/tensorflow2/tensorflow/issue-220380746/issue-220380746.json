{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9064", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9064/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9064/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9064/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9064", "id": 220380746, "node_id": "MDU6SXNzdWUyMjAzODA3NDY=", "number": 9064, "title": "Distributed tensorflow", "user": {"login": "agggaurav", "id": 18380065, "node_id": "MDQ6VXNlcjE4MzgwMDY1", "avatar_url": "https://avatars0.githubusercontent.com/u/18380065?v=4", "gravatar_id": "", "url": "https://api.github.com/users/agggaurav", "html_url": "https://github.com/agggaurav", "followers_url": "https://api.github.com/users/agggaurav/followers", "following_url": "https://api.github.com/users/agggaurav/following{/other_user}", "gists_url": "https://api.github.com/users/agggaurav/gists{/gist_id}", "starred_url": "https://api.github.com/users/agggaurav/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/agggaurav/subscriptions", "organizations_url": "https://api.github.com/users/agggaurav/orgs", "repos_url": "https://api.github.com/users/agggaurav/repos", "events_url": "https://api.github.com/users/agggaurav/events{/privacy}", "received_events_url": "https://api.github.com/users/agggaurav/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-08T06:48:02Z", "updated_at": "2017-04-08T07:04:53Z", "closed_at": "2017-04-08T07:04:53Z", "author_association": "NONE", "body_html": "<p>NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.</p>\n<h3>You must complete this information or else your issue will be closed</h3>\n<ul>\n<li><em>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?</em>:Yes</li>\n<li><em>TensorFlow installed from (source or binary)?</em>:binary</li>\n<li><em>TensorFlow version</em>:1.0.0</li>\n<li><em>Bazel version (if compiling from source)</em>:No</li>\n<li><em>CUDA/cuDNN version</em>:No</li>\n<li><em>GPU Model and Memory</em>:No</li>\n<li><em>Exact command to reproduce</em>:</li>\n</ul>\n<h3>Describe the problem clearly</h3>\n<p>I am trying to apply distributed tensorflow and want to distribute my task on two pc.<br>\npc1 ip: 192.168.43.6-&gt;&gt;&gt;&gt;ps<br>\npc2 ip:192.168.43.107-&gt;&gt;&gt;worker</p>\n<p>Code snippet:</p>\n<p>tf.train.ClusterSpec({<br>\n\"worker\": [<br>\n\"192.168.43.107:2223\"<br>\n],<br>\n\"ps\": [<br>\n\"192.168.43.6:2222\"</p>\n<pre><code>]})\n</code></pre>\n<p>cluster = tf.train.ClusterSpec({\"local\": [\"192.168.43.6:2222\",\"192.168.43.107:2223\"]})<br>\nserver = tf.train.Server(cluster, job_name=\"local\", task_index=0)</p>\n<p>with tf.device(\"/job:ps/task:0\"):</p>\n<pre><code>weights = {\n    \n    'wc1': tf.Variable(tf.random_normal([5,5, 5, 1, 32])),\n\n    'wc2': tf.Variable(tf.random_normal([5,5, 5, 32, 64])),\n    \n    'wd1': tf.Variable(tf.random_normal([1216, 1024])),\n    \n    'out': tf.Variable(tf.random_normal([1024,n_classes]))\n}\n\nbiases = {\n    'bc1': tf.Variable(tf.random_normal([32])),\n    'bc2': tf.Variable(tf.random_normal([64])),\n    'bd1': tf.Variable(tf.random_normal([1024])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n</code></pre>\n<p>with tf.device(\"/job:worker/task:0\"):</p>\n<pre><code>def conv3d(x, W, b, strides=1):\n    x = tf.nn.conv3d(x, W, strides=[1, strides, strides,strides, 1], padding='SAME')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\n\ndef maxpool3d(x, k=3):\n    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n\n\n# Create model\ndef conv_net(x, weights, biases, dropout):\n    x = tf.reshape(x, shape=[1,20,149,239, 1])\n\n    conv1 = conv3d(x, weights['wc1'], biases['bc1'])\n    print (\"conv1\",conv1)\n    conv1 = maxpool3d(conv1, k=3)\n    print (\"max1\",conv1)\n    conv2 = conv3d(conv1, weights['wc2'], biases['bc2'])\n    print (\"conv2\",conv2)\n    conv2 = maxpool3d(conv2, k=3)\n    print (\"max2\",conv2)\n    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n    print (\"fc1\",fc1)\n    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n    fc1 = tf.nn.relu(fc1)\n    print (\"relu\",fc1)\n    fc1 = tf.nn.dropout(fc1, dropout)\n\n    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n    print (\"out\",out)\n    return out\n\nx = tf.placeholder(tf.float32)\ny = tf.placeholder(tf.float32)\nkeep_prob = tf.placeholder(tf.float32) \n\n# Construct model\npred = conv_net(x, weights, biases, keep_prob)\nprint (\"pred\",pred)\ncost = tf.reduce_mean(tf.nn.softmax(logits=pred))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorrect_pred = tf.equal(tf.argmax(pred, -1), tf.argmax(y, -1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\ninit = tf.global_variables_initializer()\n\nprint (\"time1\",time.clock())\nsaver = tf.train.Saver()\n</code></pre>\n<p>with tf.Session('grpc://192.168.43.107:2222') as sess:<br>\nsess.run(init)</p>\n<p>Command line on ps machine-&gt;&gt; python train3d5.py --job_name=\"ps\" --task_index=0</p>\n<p>Command line on ps machine-&gt;&gt; python train3d5.py --job_name=\"worker\" --task_index=0</p>\n<p>Please provide a solution how to distribute training in tensorflow.<br>\nWe also using hadoop multi-cluster.</p>\n<h3>Source Code / Logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem</p>\n<p>Error on worker machine:</p>\n<p>CreateSession still working for response from worker: /job:local/replica:0/task:1</p>", "body_text": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\nYou must complete this information or else your issue will be closed\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow)?:Yes\nTensorFlow installed from (source or binary)?:binary\nTensorFlow version:1.0.0\nBazel version (if compiling from source):No\nCUDA/cuDNN version:No\nGPU Model and Memory:No\nExact command to reproduce:\n\nDescribe the problem clearly\nI am trying to apply distributed tensorflow and want to distribute my task on two pc.\npc1 ip: 192.168.43.6->>>>ps\npc2 ip:192.168.43.107->>>worker\nCode snippet:\ntf.train.ClusterSpec({\n\"worker\": [\n\"192.168.43.107:2223\"\n],\n\"ps\": [\n\"192.168.43.6:2222\"\n]})\n\ncluster = tf.train.ClusterSpec({\"local\": [\"192.168.43.6:2222\",\"192.168.43.107:2223\"]})\nserver = tf.train.Server(cluster, job_name=\"local\", task_index=0)\nwith tf.device(\"/job:ps/task:0\"):\nweights = {\n    \n    'wc1': tf.Variable(tf.random_normal([5,5, 5, 1, 32])),\n\n    'wc2': tf.Variable(tf.random_normal([5,5, 5, 32, 64])),\n    \n    'wd1': tf.Variable(tf.random_normal([1216, 1024])),\n    \n    'out': tf.Variable(tf.random_normal([1024,n_classes]))\n}\n\nbiases = {\n    'bc1': tf.Variable(tf.random_normal([32])),\n    'bc2': tf.Variable(tf.random_normal([64])),\n    'bd1': tf.Variable(tf.random_normal([1024])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n\nwith tf.device(\"/job:worker/task:0\"):\ndef conv3d(x, W, b, strides=1):\n    x = tf.nn.conv3d(x, W, strides=[1, strides, strides,strides, 1], padding='SAME')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\n\ndef maxpool3d(x, k=3):\n    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n\n\n# Create model\ndef conv_net(x, weights, biases, dropout):\n    x = tf.reshape(x, shape=[1,20,149,239, 1])\n\n    conv1 = conv3d(x, weights['wc1'], biases['bc1'])\n    print (\"conv1\",conv1)\n    conv1 = maxpool3d(conv1, k=3)\n    print (\"max1\",conv1)\n    conv2 = conv3d(conv1, weights['wc2'], biases['bc2'])\n    print (\"conv2\",conv2)\n    conv2 = maxpool3d(conv2, k=3)\n    print (\"max2\",conv2)\n    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n    print (\"fc1\",fc1)\n    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n    fc1 = tf.nn.relu(fc1)\n    print (\"relu\",fc1)\n    fc1 = tf.nn.dropout(fc1, dropout)\n\n    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n    print (\"out\",out)\n    return out\n\nx = tf.placeholder(tf.float32)\ny = tf.placeholder(tf.float32)\nkeep_prob = tf.placeholder(tf.float32) \n\n# Construct model\npred = conv_net(x, weights, biases, keep_prob)\nprint (\"pred\",pred)\ncost = tf.reduce_mean(tf.nn.softmax(logits=pred))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\ncorrect_pred = tf.equal(tf.argmax(pred, -1), tf.argmax(y, -1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\ninit = tf.global_variables_initializer()\n\nprint (\"time1\",time.clock())\nsaver = tf.train.Saver()\n\nwith tf.Session('grpc://192.168.43.107:2222') as sess:\nsess.run(init)\nCommand line on ps machine->> python train3d5.py --job_name=\"ps\" --task_index=0\nCommand line on ps machine->> python train3d5.py --job_name=\"worker\" --task_index=0\nPlease provide a solution how to distribute training in tensorflow.\nWe also using hadoop multi-cluster.\nSource Code / Logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\nError on worker machine:\nCreateSession still working for response from worker: /job:local/replica:0/task:1", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:Yes\r\n- *TensorFlow installed from (source or binary)?*:binary\r\n- *TensorFlow version*:1.0.0\r\n- *Bazel version (if compiling from source)*:No\r\n- *CUDA/cuDNN version*:No\r\n- *GPU Model and Memory*:No\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\nI am trying to apply distributed tensorflow and want to distribute my task on two pc.\r\npc1 ip: 192.168.43.6->>>>ps\r\npc2 ip:192.168.43.107->>>worker\r\n\r\nCode snippet:\r\n\r\ntf.train.ClusterSpec({\r\n    \"worker\": [\r\n        \"192.168.43.107:2223\"\r\n            ],\r\n    \"ps\": [\r\n        \"192.168.43.6:2222\"\r\n        \r\n    ]})\r\n\r\ncluster = tf.train.ClusterSpec({\"local\": [\"192.168.43.6:2222\",\"192.168.43.107:2223\"]})\r\nserver = tf.train.Server(cluster, job_name=\"local\", task_index=0)\r\n\r\n\r\nwith tf.device(\"/job:ps/task:0\"):\r\n    \r\n    weights = {\r\n        \r\n        'wc1': tf.Variable(tf.random_normal([5,5, 5, 1, 32])),\r\n \r\n        'wc2': tf.Variable(tf.random_normal([5,5, 5, 32, 64])),\r\n        \r\n        'wd1': tf.Variable(tf.random_normal([1216, 1024])),\r\n        \r\n        'out': tf.Variable(tf.random_normal([1024,n_classes]))\r\n    }\r\n    \r\n    biases = {\r\n        'bc1': tf.Variable(tf.random_normal([32])),\r\n        'bc2': tf.Variable(tf.random_normal([64])),\r\n        'bd1': tf.Variable(tf.random_normal([1024])),\r\n        'out': tf.Variable(tf.random_normal([n_classes]))\r\n    }\r\n\r\n\r\nwith tf.device(\"/job:worker/task:0\"):\r\n\r\n\r\n    def conv3d(x, W, b, strides=1):\r\n        x = tf.nn.conv3d(x, W, strides=[1, strides, strides,strides, 1], padding='SAME')\r\n        x = tf.nn.bias_add(x, b)\r\n        return tf.nn.relu(x)\r\n\r\n\r\n    def maxpool3d(x, k=3):\r\n        return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\r\n\r\n\r\n    # Create model\r\n    def conv_net(x, weights, biases, dropout):\r\n        x = tf.reshape(x, shape=[1,20,149,239, 1])\r\n    \r\n        conv1 = conv3d(x, weights['wc1'], biases['bc1'])\r\n        print (\"conv1\",conv1)\r\n        conv1 = maxpool3d(conv1, k=3)\r\n        print (\"max1\",conv1)\r\n        conv2 = conv3d(conv1, weights['wc2'], biases['bc2'])\r\n        print (\"conv2\",conv2)\r\n        conv2 = maxpool3d(conv2, k=3)\r\n        print (\"max2\",conv2)\r\n        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\r\n        print (\"fc1\",fc1)\r\n        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\r\n        fc1 = tf.nn.relu(fc1)\r\n        print (\"relu\",fc1)\r\n        fc1 = tf.nn.dropout(fc1, dropout)\r\n    \r\n        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\r\n        print (\"out\",out)\r\n        return out\r\n    \r\n    x = tf.placeholder(tf.float32)\r\n    y = tf.placeholder(tf.float32)\r\n    keep_prob = tf.placeholder(tf.float32) \r\n\r\n    # Construct model\r\n    pred = conv_net(x, weights, biases, keep_prob)\r\n    print (\"pred\",pred)\r\n    cost = tf.reduce_mean(tf.nn.softmax(logits=pred))\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n    correct_pred = tf.equal(tf.argmax(pred, -1), tf.argmax(y, -1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n    init = tf.global_variables_initializer()\r\n\r\n    print (\"time1\",time.clock())\r\n    saver = tf.train.Saver()\r\n\r\nwith tf.Session('grpc://192.168.43.107:2222') as sess:\r\n    sess.run(init)\r\n\r\nCommand line on ps machine->> python train3d5.py --job_name=\"ps\" --task_index=0\r\n\r\nCommand line on ps machine->> python train3d5.py --job_name=\"worker\" --task_index=0\r\n\r\nPlease provide a solution how to distribute training in tensorflow.\r\nWe also using hadoop multi-cluster.\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n\r\nError on worker machine:\r\n\r\nCreateSession still working for response from worker: /job:local/replica:0/task:1\r\n\r\n"}