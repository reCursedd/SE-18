{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2791", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2791/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2791/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2791/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2791", "id": 159701748, "node_id": "MDU6SXNzdWUxNTk3MDE3NDg=", "number": 2791, "title": "map_fn: output type error when running the graph", "user": {"login": "jorgemf", "id": 1029554, "node_id": "MDQ6VXNlcjEwMjk1NTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/1029554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jorgemf", "html_url": "https://github.com/jorgemf", "followers_url": "https://api.github.com/users/jorgemf/followers", "following_url": "https://api.github.com/users/jorgemf/following{/other_user}", "gists_url": "https://api.github.com/users/jorgemf/gists{/gist_id}", "starred_url": "https://api.github.com/users/jorgemf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jorgemf/subscriptions", "organizations_url": "https://api.github.com/users/jorgemf/orgs", "repos_url": "https://api.github.com/users/jorgemf/repos", "events_url": "https://api.github.com/users/jorgemf/events{/privacy}", "received_events_url": "https://api.github.com/users/jorgemf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2016-06-10T19:07:47Z", "updated_at": "2016-07-11T13:27:05Z", "closed_at": "2016-06-20T22:51:27Z", "author_association": "NONE", "body_html": "<p>I am adding batch support to the inception example of tensorflow serving. In order to do that I have tweaked the <a href=\"https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/inception_export.py\">exporting script</a> to support batch with a map function (I have also tweaked the inference server but I think it is not relevant for the bug):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">...</span>\n    <span class=\"pl-k\">with</span> tf.Graph().as_default():\n        jpegs <span class=\"pl-k\">=</span> tf.placeholder(tf.string, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>))\n        images <span class=\"pl-k\">=</span> tf.map_fn(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">image_buffer</span> : tf.image.decode_jpeg(image_buffer, <span class=\"pl-v\">channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>), jpegs, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.uint8)\n        images <span class=\"pl-k\">=</span> tf.map_fn(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">decode_image</span> : tf.image.convert_image_dtype(decode_image, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32), images, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n        images <span class=\"pl-k\">=</span> tf.image.resize_bilinear(images, [<span class=\"pl-c1\">FLAGS</span>.image_size, <span class=\"pl-c1\">FLAGS</span>.image_size], <span class=\"pl-v\">align_corners</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        images <span class=\"pl-k\">=</span> tf.sub(images, <span class=\"pl-c1\">0.5</span>)\n        images <span class=\"pl-k\">=</span> tf.mul(images, <span class=\"pl-c1\">2.0</span>)\n        logits, _ <span class=\"pl-k\">=</span> inception_model.inference(images, <span class=\"pl-c1\">len</span>(labels_names))\n<span class=\"pl-c1\">...</span></pre></div>\n<p>The graph is exported without any issues but when testing it in tensorflow serving the client shows this error:</p>\n<blockquote>\n<p>grpc.framework.interfaces.face.face.NetworkError: NetworkError(code=StatusCode.INTERNAL, details=\"Output 0 of type string does not match declared output type float for node _recv_Mul_0 = _Recv<a href=\"\">client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=3531382951825773879, tensor_name=\"Mul:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"</a>\")</p>\n</blockquote>\n<p>Somehow when the graph is executed the type of the placeholder is passed through the tensors up to the last operation before the inference graph when it shouldn't. If I comment the lines tf.sub and tf.mul the error is similar but in the tensor of the resize_bilinear function:</p>\n<blockquote>\n<p>grpc.framework.interfaces.face.face.NetworkError: NetworkError(code=StatusCode.INTERNAL, details=\"Output 0 of type string does not match declared output type float for node _recv_ResizeBilinear_0 = _Recv<a href=\"\">client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-7455415421321039987, tensor_name=\"ResizeBilinear:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"</a>\")</p>\n</blockquote>\n<p>The error seems related with the map function.</p>\n<p>tensorflow-serving commit hash: 1d7a9ceae1b9630377885ae7f38129d22ae0ad93 (June 8, 2016)<br>\ntensorflow commit: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/592675b2b8d1cabbf923638942ea6f200abe353a/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/592675b2b8d1cabbf923638942ea6f200abe353a\"><tt>592675b</tt></a> (June 7, 2016)</p>", "body_text": "I am adding batch support to the inception example of tensorflow serving. In order to do that I have tweaked the exporting script to support batch with a map function (I have also tweaked the inference server but I think it is not relevant for the bug):\n...\n    with tf.Graph().as_default():\n        jpegs = tf.placeholder(tf.string, shape=(None))\n        images = tf.map_fn(lambda image_buffer : tf.image.decode_jpeg(image_buffer, channels=3), jpegs, dtype=tf.uint8)\n        images = tf.map_fn(lambda decode_image : tf.image.convert_image_dtype(decode_image, dtype=tf.float32), images, dtype=tf.float32)\n        images = tf.image.resize_bilinear(images, [FLAGS.image_size, FLAGS.image_size], align_corners=False)\n        images = tf.sub(images, 0.5)\n        images = tf.mul(images, 2.0)\n        logits, _ = inception_model.inference(images, len(labels_names))\n...\nThe graph is exported without any issues but when testing it in tensorflow serving the client shows this error:\n\ngrpc.framework.interfaces.face.face.NetworkError: NetworkError(code=StatusCode.INTERNAL, details=\"Output 0 of type string does not match declared output type float for node _recv_Mul_0 = _Recvclient_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=3531382951825773879, tensor_name=\"Mul:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"\")\n\nSomehow when the graph is executed the type of the placeholder is passed through the tensors up to the last operation before the inference graph when it shouldn't. If I comment the lines tf.sub and tf.mul the error is similar but in the tensor of the resize_bilinear function:\n\ngrpc.framework.interfaces.face.face.NetworkError: NetworkError(code=StatusCode.INTERNAL, details=\"Output 0 of type string does not match declared output type float for node _recv_ResizeBilinear_0 = _Recvclient_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-7455415421321039987, tensor_name=\"ResizeBilinear:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"\")\n\nThe error seems related with the map function.\ntensorflow-serving commit hash: 1d7a9ceae1b9630377885ae7f38129d22ae0ad93 (June 8, 2016)\ntensorflow commit: 592675b (June 7, 2016)", "body": "I am adding batch support to the inception example of tensorflow serving. In order to do that I have tweaked the [exporting script](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/inception_export.py) to support batch with a map function (I have also tweaked the inference server but I think it is not relevant for the bug):\n\n``` python\n...\n    with tf.Graph().as_default():\n        jpegs = tf.placeholder(tf.string, shape=(None))\n        images = tf.map_fn(lambda image_buffer : tf.image.decode_jpeg(image_buffer, channels=3), jpegs, dtype=tf.uint8)\n        images = tf.map_fn(lambda decode_image : tf.image.convert_image_dtype(decode_image, dtype=tf.float32), images, dtype=tf.float32)\n        images = tf.image.resize_bilinear(images, [FLAGS.image_size, FLAGS.image_size], align_corners=False)\n        images = tf.sub(images, 0.5)\n        images = tf.mul(images, 2.0)\n        logits, _ = inception_model.inference(images, len(labels_names))\n...\n```\n\nThe graph is exported without any issues but when testing it in tensorflow serving the client shows this error: \n\n> grpc.framework.interfaces.face.face.NetworkError: NetworkError(code=StatusCode.INTERNAL, details=\"Output 0 of type string does not match declared output type float for node _recv_Mul_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=3531382951825773879, tensor_name=\"Mul:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")\n\nSomehow when the graph is executed the type of the placeholder is passed through the tensors up to the last operation before the inference graph when it shouldn't. If I comment the lines tf.sub and tf.mul the error is similar but in the tensor of the resize_bilinear function:\n\n> grpc.framework.interfaces.face.face.NetworkError: NetworkError(code=StatusCode.INTERNAL, details=\"Output 0 of type string does not match declared output type float for node _recv_ResizeBilinear_0 = _Recv[client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=-7455415421321039987, tensor_name=\"ResizeBilinear:0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()\")\n\nThe error seems related with the map function.\n\ntensorflow-serving commit hash: 1d7a9ceae1b9630377885ae7f38129d22ae0ad93 (June 8, 2016)\ntensorflow commit: 592675b2b8d1cabbf923638942ea6f200abe353a (June 7, 2016)\n"}