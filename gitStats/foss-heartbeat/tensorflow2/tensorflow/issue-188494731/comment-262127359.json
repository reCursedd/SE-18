{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/262127359", "html_url": "https://github.com/tensorflow/tensorflow/issues/5516#issuecomment-262127359", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5516", "id": 262127359, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MjEyNzM1OQ==", "user": {"login": "zhangyaobit", "id": 1034716, "node_id": "MDQ6VXNlcjEwMzQ3MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1034716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangyaobit", "html_url": "https://github.com/zhangyaobit", "followers_url": "https://api.github.com/users/zhangyaobit/followers", "following_url": "https://api.github.com/users/zhangyaobit/following{/other_user}", "gists_url": "https://api.github.com/users/zhangyaobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangyaobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangyaobit/subscriptions", "organizations_url": "https://api.github.com/users/zhangyaobit/orgs", "repos_url": "https://api.github.com/users/zhangyaobit/repos", "events_url": "https://api.github.com/users/zhangyaobit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangyaobit/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-22T02:09:02Z", "updated_at": "2016-11-22T02:09:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I did more CPU profiling and found that the majority of the CPU overhead is actually from the host code inside the conv op, rather than from the executor. The CPU time is distributed across pre- and post- processing steps such as padding and filter transform, as well as in the cuDNN kernel invocation. There doesn't seem to be a single \"performance bug\" that could be fixed here, but the overall CPU side of the implementation of conv could be improved.</p>\n<p>Conv actually internally converts NHWC to NCHW, so using NCHW directly would avoid the conversion. However, for small models, as the performance is bottlenecked by the CPU code, using NCHW makes little performance difference.</p>\n<p>I marked the issue as contribution welcome for interested people to take a deeper look.</p>", "body_text": "I did more CPU profiling and found that the majority of the CPU overhead is actually from the host code inside the conv op, rather than from the executor. The CPU time is distributed across pre- and post- processing steps such as padding and filter transform, as well as in the cuDNN kernel invocation. There doesn't seem to be a single \"performance bug\" that could be fixed here, but the overall CPU side of the implementation of conv could be improved.\nConv actually internally converts NHWC to NCHW, so using NCHW directly would avoid the conversion. However, for small models, as the performance is bottlenecked by the CPU code, using NCHW makes little performance difference.\nI marked the issue as contribution welcome for interested people to take a deeper look.", "body": "I did more CPU profiling and found that the majority of the CPU overhead is actually from the host code inside the conv op, rather than from the executor. The CPU time is distributed across pre- and post- processing steps such as padding and filter transform, as well as in the cuDNN kernel invocation. There doesn't seem to be a single \"performance bug\" that could be fixed here, but the overall CPU side of the implementation of conv could be improved.\r\n\r\nConv actually internally converts NHWC to NCHW, so using NCHW directly would avoid the conversion. However, for small models, as the performance is bottlenecked by the CPU code, using NCHW makes little performance difference. \r\n\r\nI marked the issue as contribution welcome for interested people to take a deeper look."}