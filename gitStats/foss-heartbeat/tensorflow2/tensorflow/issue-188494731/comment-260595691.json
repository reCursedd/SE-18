{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/260595691", "html_url": "https://github.com/tensorflow/tensorflow/issues/5516#issuecomment-260595691", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5516", "id": 260595691, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MDU5NTY5MQ==", "user": {"login": "wjaskowski", "id": 4952605, "node_id": "MDQ6VXNlcjQ5NTI2MDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/4952605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wjaskowski", "html_url": "https://github.com/wjaskowski", "followers_url": "https://api.github.com/users/wjaskowski/followers", "following_url": "https://api.github.com/users/wjaskowski/following{/other_user}", "gists_url": "https://api.github.com/users/wjaskowski/gists{/gist_id}", "starred_url": "https://api.github.com/users/wjaskowski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wjaskowski/subscriptions", "organizations_url": "https://api.github.com/users/wjaskowski/orgs", "repos_url": "https://api.github.com/users/wjaskowski/repos", "events_url": "https://api.github.com/users/wjaskowski/events{/privacy}", "received_events_url": "https://api.github.com/users/wjaskowski/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-15T09:49:48Z", "updated_at": "2016-11-15T09:49:48Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> I see. Indeed, my first profiling results shows the overhead of fetching the data. When I removed it, TF/TH time ratio decreased to 2-2.5. I also confirm that when the model is larger (e.g. 512 filters, the difference between TH and TF shrinks. E.g., for \"128x128 + conv[512,512,256,128] + 512\") I got a ratio of 1.5 for forward pass and 0.85 for backprop (i.e. TF is actually faster).</p>\n<p>But still, it would be nice to make TF also quick for smaller models. In reinforcement learning, I cannot afford huge models. It is also not so easy to prefetch data to TF since the data are generated on-the-fly (but maybe with the queues?).</p>", "body_text": "@yaroslavvb I see. Indeed, my first profiling results shows the overhead of fetching the data. When I removed it, TF/TH time ratio decreased to 2-2.5. I also confirm that when the model is larger (e.g. 512 filters, the difference between TH and TF shrinks. E.g., for \"128x128 + conv[512,512,256,128] + 512\") I got a ratio of 1.5 for forward pass and 0.85 for backprop (i.e. TF is actually faster).\nBut still, it would be nice to make TF also quick for smaller models. In reinforcement learning, I cannot afford huge models. It is also not so easy to prefetch data to TF since the data are generated on-the-fly (but maybe with the queues?).", "body": "@yaroslavvb I see. Indeed, my first profiling results shows the overhead of fetching the data. When I removed it, TF/TH time ratio decreased to 2-2.5. I also confirm that when the model is larger (e.g. 512 filters, the difference between TH and TF shrinks. E.g., for \"128x128 + conv[512,512,256,128] + 512\") I got a ratio of 1.5 for forward pass and 0.85 for backprop (i.e. TF is actually faster).\n\nBut still, it would be nice to make TF also quick for smaller models. In reinforcement learning, I cannot afford huge models. It is also not so easy to prefetch data to TF since the data are generated on-the-fly (but maybe with the queues?).\n"}