{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/297322881", "html_url": "https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-297322881", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2916", "id": 297322881, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NzMyMjg4MQ==", "user": {"login": "shamoya", "id": 22274255, "node_id": "MDQ6VXNlcjIyMjc0MjU1", "avatar_url": "https://avatars2.githubusercontent.com/u/22274255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shamoya", "html_url": "https://github.com/shamoya", "followers_url": "https://api.github.com/users/shamoya/followers", "following_url": "https://api.github.com/users/shamoya/following{/other_user}", "gists_url": "https://api.github.com/users/shamoya/gists{/gist_id}", "starred_url": "https://api.github.com/users/shamoya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shamoya/subscriptions", "organizations_url": "https://api.github.com/users/shamoya/orgs", "repos_url": "https://api.github.com/users/shamoya/repos", "events_url": "https://api.github.com/users/shamoya/events{/privacy}", "received_events_url": "https://api.github.com/users/shamoya/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-26T09:43:44Z", "updated_at": "2017-04-26T09:47:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi @avader906,</p>\n<p>RoCE V2 penalty VS IB EDR is around 8%, this is from the same machines I ran the tests:</p>\n<pre>ib_write_bw -s 8388608 -d mlx5_1 --report_gbits 10.143.119.22\n---------------------------------------------------------------------------------------       \n                    RDMA_Write BW Test                                                        \n Dual-port       : OFF          Device         : mlx5_1                                       \n Number of qps   : 1            Transport type : IB                                           \n Connection type : RC           Using SRQ      : OFF                                          \n TX depth        : 128                                                                        \n CQ Moderation   : 100                                                                        \n Mtu             : 4096[B]                                                                    \n Link type       : IB                                                                   \n Max inline data : 0[B]                                                                       \n rdma_cm QPs     : OFF                                                                        \n Data ex. method : Ethernet                                                                   \n---------------------------------------------------------------------------------------       \n local address: LID 0x0b QPN 0x01e9 PSN 0xfe065 RKey 0x0ce502 VAddr 0x007f01ab331000          \n remote address: LID 0x0c QPN 0x01d3 PSN 0x4347ae RKey 0x0351af VAddr 0x007fe4e5351000        \n---------------------------------------------------------------------------------------       \n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]              \n8388608    5000             99.11              99.00              0.001475                   \n---------------------------------------------------------------------------------------\n\nib_write_bw -s 8388608 -d mlx5_0 -x 0 --report_gbits 10.143.119.22\n---------------------------------------------------------------------------------------\n                    RDMA_Write BW Test\n Dual-port       : OFF          Device         : mlx5_0\n Number of qps   : 1            Transport type : IB\n Connection type : RC           Using SRQ      : OFF\n TX depth        : 128\n CQ Moderation   : 100\n Mtu             : 1024[B]\n Link type       : Ethernet\n GID index       : 0\n Max inline data : 0[B]\n rdma_cm QPs     : OFF\n Data ex. method : Ethernet\n---------------------------------------------------------------------------------------\n local address: LID 0000 QPN 0x016b PSN 0x1833eb RKey 0x002d9c VAddr 0x007f12498bd000\n GID: 254:128:00:00:00:00:00:00:126:254:144:255:254:32:230:120\n remote address: LID 0000 QPN 0x016b PSN 0xa4b427 RKey 0x00962b VAddr 0x007f79e76f3000\n GID: 254:128:00:00:00:00:00:00:126:254:144:255:254:32:232:60\n---------------------------------------------------------------------------------------\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]\n 8388608    5000             91.57              91.57              0.001365\n---------------------------------------------------------------------------------------\n\n</pre>\n<p>25% is only on the Inception case (and tested on small scale - potentially more with larger scale), with VGG (which is much harder to scale) it's uncomparable improvement for now (gRPC TCP is literally unusable here).</p>\n<p>All TX/RX NIC Ethernet features are enabled (RX/TX Checksums, TSO/GRO/LRO etc..).<br>\nWe tried disabled TSO (CPU intensive) and played with Affinities to split the TCP stack code from the gRPC application and that also didn't helped.<br>\nThe problem here I think is the memory management done by the gRPC (allocation and freeing of Tensor buffers) - I'm still looking at it and will share once I know more.</p>\n<p>GPU Direct RDMA is in the pipeline, along with memory optimizations (ODP) and other improvement (connections management, etc..). I'm preparing a plan (depends on the resources we'll have here) and I will gladly share it soon I have it.</p>\n<p>As the above optimizations are related to verbs work provided by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12075848\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/junshi15\">@junshi15</a> (which is a P2P RDMA),<br>\nI think we must also talk about future work which will introduce collectives notation of reduction between the workers (thus remove the need of parameter server) <em>without integrating MPI</em> (but rather a \"native\" approach of collectives in tensorflow).</p>", "body_text": "Hi @avader906,\nRoCE V2 penalty VS IB EDR is around 8%, this is from the same machines I ran the tests:\nib_write_bw -s 8388608 -d mlx5_1 --report_gbits 10.143.119.22\n---------------------------------------------------------------------------------------       \n                    RDMA_Write BW Test                                                        \n Dual-port       : OFF          Device         : mlx5_1                                       \n Number of qps   : 1            Transport type : IB                                           \n Connection type : RC           Using SRQ      : OFF                                          \n TX depth        : 128                                                                        \n CQ Moderation   : 100                                                                        \n Mtu             : 4096[B]                                                                    \n Link type       : IB                                                                   \n Max inline data : 0[B]                                                                       \n rdma_cm QPs     : OFF                                                                        \n Data ex. method : Ethernet                                                                   \n---------------------------------------------------------------------------------------       \n local address: LID 0x0b QPN 0x01e9 PSN 0xfe065 RKey 0x0ce502 VAddr 0x007f01ab331000          \n remote address: LID 0x0c QPN 0x01d3 PSN 0x4347ae RKey 0x0351af VAddr 0x007fe4e5351000        \n---------------------------------------------------------------------------------------       \n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]              \n8388608    5000             99.11              99.00              0.001475                   \n---------------------------------------------------------------------------------------\n\nib_write_bw -s 8388608 -d mlx5_0 -x 0 --report_gbits 10.143.119.22\n---------------------------------------------------------------------------------------\n                    RDMA_Write BW Test\n Dual-port       : OFF          Device         : mlx5_0\n Number of qps   : 1            Transport type : IB\n Connection type : RC           Using SRQ      : OFF\n TX depth        : 128\n CQ Moderation   : 100\n Mtu             : 1024[B]\n Link type       : Ethernet\n GID index       : 0\n Max inline data : 0[B]\n rdma_cm QPs     : OFF\n Data ex. method : Ethernet\n---------------------------------------------------------------------------------------\n local address: LID 0000 QPN 0x016b PSN 0x1833eb RKey 0x002d9c VAddr 0x007f12498bd000\n GID: 254:128:00:00:00:00:00:00:126:254:144:255:254:32:230:120\n remote address: LID 0000 QPN 0x016b PSN 0xa4b427 RKey 0x00962b VAddr 0x007f79e76f3000\n GID: 254:128:00:00:00:00:00:00:126:254:144:255:254:32:232:60\n---------------------------------------------------------------------------------------\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]\n 8388608    5000             91.57              91.57              0.001365\n---------------------------------------------------------------------------------------\n\n\n25% is only on the Inception case (and tested on small scale - potentially more with larger scale), with VGG (which is much harder to scale) it's uncomparable improvement for now (gRPC TCP is literally unusable here).\nAll TX/RX NIC Ethernet features are enabled (RX/TX Checksums, TSO/GRO/LRO etc..).\nWe tried disabled TSO (CPU intensive) and played with Affinities to split the TCP stack code from the gRPC application and that also didn't helped.\nThe problem here I think is the memory management done by the gRPC (allocation and freeing of Tensor buffers) - I'm still looking at it and will share once I know more.\nGPU Direct RDMA is in the pipeline, along with memory optimizations (ODP) and other improvement (connections management, etc..). I'm preparing a plan (depends on the resources we'll have here) and I will gladly share it soon I have it.\nAs the above optimizations are related to verbs work provided by @junshi15 (which is a P2P RDMA),\nI think we must also talk about future work which will introduce collectives notation of reduction between the workers (thus remove the need of parameter server) without integrating MPI (but rather a \"native\" approach of collectives in tensorflow).", "body": "Hi @avader906, \r\n\r\nRoCE V2 penalty VS IB EDR is around 8%, this is from the same machines I ran the tests:\r\n\r\n<pre>\r\nib_write_bw -s 8388608 -d mlx5_1 --report_gbits 10.143.119.22\r\n---------------------------------------------------------------------------------------       \r\n                    RDMA_Write BW Test                                                        \r\n Dual-port       : OFF          Device         : mlx5_1                                       \r\n Number of qps   : 1            Transport type : IB                                           \r\n Connection type : RC           Using SRQ      : OFF                                          \r\n TX depth        : 128                                                                        \r\n CQ Moderation   : 100                                                                        \r\n Mtu             : 4096[B]                                                                    \r\n Link type       : IB                                                                   \r\n Max inline data : 0[B]                                                                       \r\n rdma_cm QPs     : OFF                                                                        \r\n Data ex. method : Ethernet                                                                   \r\n---------------------------------------------------------------------------------------       \r\n local address: LID 0x0b QPN 0x01e9 PSN 0xfe065 RKey 0x0ce502 VAddr 0x007f01ab331000          \r\n remote address: LID 0x0c QPN 0x01d3 PSN 0x4347ae RKey 0x0351af VAddr 0x007fe4e5351000        \r\n---------------------------------------------------------------------------------------       \r\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]              \r\n8388608    5000             99.11              99.00              0.001475                   \r\n---------------------------------------------------------------------------------------\r\n\r\nib_write_bw -s 8388608 -d mlx5_0 -x 0 --report_gbits 10.143.119.22\r\n---------------------------------------------------------------------------------------\r\n                    RDMA_Write BW Test\r\n Dual-port       : OFF          Device         : mlx5_0\r\n Number of qps   : 1            Transport type : IB\r\n Connection type : RC           Using SRQ      : OFF\r\n TX depth        : 128\r\n CQ Moderation   : 100\r\n Mtu             : 1024[B]\r\n Link type       : Ethernet\r\n GID index       : 0\r\n Max inline data : 0[B]\r\n rdma_cm QPs     : OFF\r\n Data ex. method : Ethernet\r\n---------------------------------------------------------------------------------------\r\n local address: LID 0000 QPN 0x016b PSN 0x1833eb RKey 0x002d9c VAddr 0x007f12498bd000\r\n GID: 254:128:00:00:00:00:00:00:126:254:144:255:254:32:230:120\r\n remote address: LID 0000 QPN 0x016b PSN 0xa4b427 RKey 0x00962b VAddr 0x007f79e76f3000\r\n GID: 254:128:00:00:00:00:00:00:126:254:144:255:254:32:232:60\r\n---------------------------------------------------------------------------------------\r\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]\r\n 8388608    5000             91.57              91.57              0.001365\r\n---------------------------------------------------------------------------------------\r\n\r\n</pre>\r\n\r\n25% is only on the Inception case (and tested on small scale - potentially more with larger scale), with VGG (which is much harder to scale) it's uncomparable improvement for now (gRPC TCP is literally unusable here).\r\n\r\nAll TX/RX NIC Ethernet features are enabled (RX/TX Checksums, TSO/GRO/LRO etc..).\r\nWe tried disabled TSO (CPU intensive) and played with Affinities to split the TCP stack code from the gRPC application and that also didn't helped.\r\nThe problem here I think is the memory management done by the gRPC (allocation and freeing of Tensor buffers) - I'm still looking at it and will share once I know more.\r\n\r\nGPU Direct RDMA is in the pipeline, along with memory optimizations (ODP) and other improvement (connections management, etc..). I'm preparing a plan (depends on the resources we'll have here) and I will gladly share it soon I have it.\r\n\r\nAs the above optimizations are related to verbs work provided by @junshi15 (which is a P2P RDMA),\r\nI think we must also talk about future work which will introduce collectives notation of reduction between the workers (thus remove the need of parameter server) *without integrating MPI* (but rather a \"native\" approach of collectives in tensorflow)."}