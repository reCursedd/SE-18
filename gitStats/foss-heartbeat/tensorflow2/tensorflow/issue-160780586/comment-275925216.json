{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275925216", "html_url": "https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-275925216", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2916", "id": 275925216, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTkyNTIxNg==", "user": {"login": "abhinavvishnu", "id": 6489071, "node_id": "MDQ6VXNlcjY0ODkwNzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/6489071?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhinavvishnu", "html_url": "https://github.com/abhinavvishnu", "followers_url": "https://api.github.com/users/abhinavvishnu/followers", "following_url": "https://api.github.com/users/abhinavvishnu/following{/other_user}", "gists_url": "https://api.github.com/users/abhinavvishnu/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhinavvishnu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhinavvishnu/subscriptions", "organizations_url": "https://api.github.com/users/abhinavvishnu/orgs", "repos_url": "https://api.github.com/users/abhinavvishnu/repos", "events_url": "https://api.github.com/users/abhinavvishnu/events{/privacy}", "received_events_url": "https://api.github.com/users/abhinavvishnu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-29T16:26:13Z", "updated_at": "2017-01-29T16:26:13Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">I would like to join in -- if possible. We have been identifying minimal\nchanges in TF runtime to support MPI based parallelism in the optimizer\nclass. We expect the implementation to be ready in a few weeks.\n\nPlease let me know, if you would like to discuss this further.\n\nSincerely,\n\n:- Abhinav</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sun, Jan 29, 2017 at 2:47 AM, avader906 ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/jeffhammond\">@jeffhammond</a> &lt;<a href=\"https://github.com/jeffhammond\">https://github.com/jeffhammond</a>&gt; and <a class=\"user-mention\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>\n &lt;<a href=\"https://github.com/yaroslavvb\">https://github.com/yaroslavvb</a>&gt; are the two people who's contribution to\n get TF on RDMA would be invaluable.\n\n TF does not scale beyond one host only in the multiple node / multiple GPU\n distributed learning environment. Just look at the calculation bandwidth\n that is now possible:\n <a href=\"https://devblogs.nvidia.com/parallelforall/benchmarking-\">https://devblogs.nvidia.com/parallelforall/benchmarking-</a>\n gpudirect-rdma-on-modern-server-platforms/\n\n MPI provides what we need today while gRPC might get there with time. What\n would be architectural and API choices to make sure TF could be compiled\n against both ?\n\n \u2014\n You are receiving this because you commented.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"160780586\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2916\" href=\"https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-275906182\">#2916 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AGMD7xpjB6-dsmuZWxZzMIDXJfRxeob3ks5rXG4ngaJpZM4I35Qd\">https://github.com/notifications/unsubscribe-auth/AGMD7xpjB6-dsmuZWxZzMIDXJfRxeob3ks5rXG4ngaJpZM4I35Qd</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \nAbhinav Vishnu\nResearch Scientist\nPacific Northwest National Lab\nRichland, WA 99352</div>\n</div>", "body_text": "I would like to join in -- if possible. We have been identifying minimal\nchanges in TF runtime to support MPI based parallelism in the optimizer\nclass. We expect the implementation to be ready in a few weeks.\n\nPlease let me know, if you would like to discuss this further.\n\nSincerely,\n\n:- Abhinav\n\u2026\nOn Sun, Jan 29, 2017 at 2:47 AM, avader906 ***@***.***> wrote:\n @jeffhammond <https://github.com/jeffhammond> and @yaroslavvb\n <https://github.com/yaroslavvb> are the two people who's contribution to\n get TF on RDMA would be invaluable.\n\n TF does not scale beyond one host only in the multiple node / multiple GPU\n distributed learning environment. Just look at the calculation bandwidth\n that is now possible:\n https://devblogs.nvidia.com/parallelforall/benchmarking-\n gpudirect-rdma-on-modern-server-platforms/\n\n MPI provides what we need today while gRPC might get there with time. What\n would be architectural and API choices to make sure TF could be compiled\n against both ?\n\n \u2014\n You are receiving this because you commented.\n Reply to this email directly, view it on GitHub\n <#2916 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AGMD7xpjB6-dsmuZWxZzMIDXJfRxeob3ks5rXG4ngaJpZM4I35Qd>\n .\n\n\n-- \nAbhinav Vishnu\nResearch Scientist\nPacific Northwest National Lab\nRichland, WA 99352", "body": "I would like to join in -- if possible. We have been identifying minimal\nchanges in TF runtime to support MPI based parallelism in the optimizer\nclass. We expect the implementation to be ready in a few weeks.\n\nPlease let me know, if you would like to discuss this further.\n\nSincerely,\n\n:- Abhinav\n\nOn Sun, Jan 29, 2017 at 2:47 AM, avader906 <notifications@github.com> wrote:\n\n> @jeffhammond <https://github.com/jeffhammond> and @yaroslavvb\n> <https://github.com/yaroslavvb> are the two people who's contribution to\n> get TF on RDMA would be invaluable.\n>\n> TF does not scale beyond one host only in the multiple node / multiple GPU\n> distributed learning environment. Just look at the calculation bandwidth\n> that is now possible:\n> https://devblogs.nvidia.com/parallelforall/benchmarking-\n> gpudirect-rdma-on-modern-server-platforms/\n>\n> MPI provides what we need today while gRPC might get there with time. What\n> would be architectural and API choices to make sure TF could be compiled\n> against both ?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-275906182>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGMD7xpjB6-dsmuZWxZzMIDXJfRxeob3ks5rXG4ngaJpZM4I35Qd>\n> .\n>\n\n\n\n-- \nAbhinav Vishnu\nResearch Scientist\nPacific Northwest National Lab\nRichland, WA 99352\n"}