{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/297259606", "html_url": "https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-297259606", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2916", "id": 297259606, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NzI1OTYwNg==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-26T07:06:09Z", "updated_at": "2017-04-26T07:26:27Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22274255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shamoya\">@shamoya</a> one-for-one on the hardware side (GPUs and nodes), RoCE (100GE) will give back that 25% vs pure IB (EDR, RDMA). The overhead will depend on the number of connections rather than amount of data - bandwidth would be approximately equivalent with lower latency for IB. The main ethernet NIC parameters for tuning are offloading Tx/Rx checksumming - if NICs are capable of doing it you wont see the mentioned CPU loads. NUMA nodes/Affinity is super important for <a href=\"https://community.mellanox.com/docs/DOC-2489\" rel=\"nofollow\">IB HCA performance</a>.</p>\n<p>About GPUdirect over RDMA - is that something in the pipeline?</p>\n<ul>\n<li><a href=\"https://htor.inf.ethz.ch/blog/index.php/2016/05/15/what-are-the-real-differences-between-rdma-infiniband-rma-and-pgas/\" rel=\"nofollow\">RDMA primer</a></li>\n<li><a href=\"http://docs.nvidia.com/cuda/gpudirect-rdma/#abstract\" rel=\"nofollow\">NVIDIA GPUdirect doc</a></li>\n<li><a href=\"http://www.mellanox.com/related-docs/prod_software/Mellanox_GPUDirect_User_Manual_v1.0.pdf\" rel=\"nofollow\">Mellanox GPUdirect doc</a></li>\n<li><a href=\"https://devblogs.nvidia.com/parallelforall/benchmarking-gpudirect-rdma-on-modern-server-platforms/\" rel=\"nofollow\">Benchmarking GPUdirect RDMA</a></li>\n</ul>", "body_text": "@shamoya one-for-one on the hardware side (GPUs and nodes), RoCE (100GE) will give back that 25% vs pure IB (EDR, RDMA). The overhead will depend on the number of connections rather than amount of data - bandwidth would be approximately equivalent with lower latency for IB. The main ethernet NIC parameters for tuning are offloading Tx/Rx checksumming - if NICs are capable of doing it you wont see the mentioned CPU loads. NUMA nodes/Affinity is super important for IB HCA performance.\nAbout GPUdirect over RDMA - is that something in the pipeline?\n\nRDMA primer\nNVIDIA GPUdirect doc\nMellanox GPUdirect doc\nBenchmarking GPUdirect RDMA", "body": "@shamoya one-for-one on the hardware side (GPUs and nodes), RoCE (100GE) will give back that 25% vs pure IB (EDR, RDMA). The overhead will depend on the number of connections rather than amount of data - bandwidth would be approximately equivalent with lower latency for IB. The main ethernet NIC parameters for tuning are offloading Tx/Rx checksumming - if NICs are capable of doing it you wont see the mentioned CPU loads. NUMA nodes/Affinity is super important for [IB HCA performance](https://community.mellanox.com/docs/DOC-2489).\r\n\r\nAbout GPUdirect over RDMA - is that something in the pipeline?\r\n- [RDMA primer](https://htor.inf.ethz.ch/blog/index.php/2016/05/15/what-are-the-real-differences-between-rdma-infiniband-rma-and-pgas/)\r\n- [NVIDIA GPUdirect doc](http://docs.nvidia.com/cuda/gpudirect-rdma/#abstract)\r\n- [Mellanox GPUdirect doc](http://www.mellanox.com/related-docs/prod_software/Mellanox_GPUDirect_User_Manual_v1.0.pdf)\r\n- [Benchmarking GPUdirect RDMA](https://devblogs.nvidia.com/parallelforall/benchmarking-gpudirect-rdma-on-modern-server-platforms/)\r\n"}