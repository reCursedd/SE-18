{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275937482", "html_url": "https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-275937482", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2916", "id": 275937482, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTkzNzQ4Mg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-29T19:07:51Z", "updated_at": "2017-01-29T20:36:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>@avader906 Some numbers/concrete performance goals would be useful here. IE, what is an important application, how fast is it now in TF GRPC, how fast is it in competing implementation?</p>\n<p>Note that there are two use-cases -- using distributed TensorFlow on public cloud, and using TensorFlow on dedicated compute clusters. Existing TF is optimized for the public cloud case because that's what compute at Google is like -- CPU and network resources are shared among many competing users, so there's variability in communication/computation performance across workers and individual workers can die without warning. At OpenAI we use distributed tensorflow to train RL agents like <a href=\"https://github.com/openai/universe-starter-agent\">this</a> on public cloud providers and maintaining performance despite some workers slowing down/restarting/network flakiness is an important feature.</p>\n<p>The \"dedicated compute cluster\" scenario is somewhat alien to Google, so this would need community to help define goals/requirements.</p>\n<p>It seems <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6489071\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/abhinavvishnu\">@abhinavvishnu</a> has done some work on integrating MPI. For a brief glance at the matex repo, it seems the approach is well-supported by current TF abstractions -- using <code>session.run</code> to do the computation, then using MPI for data transfers. The performance question is the latency introduced by going through TF client API. I've done preliminary investigation of the overhead introduced by <code>session.run</code> and it seems it can be reduced by 50% by using lower level TF_SessionRun (easy), and possibly another 50-95% by removing the memory copy (harder)</p>", "body_text": "@avader906 Some numbers/concrete performance goals would be useful here. IE, what is an important application, how fast is it now in TF GRPC, how fast is it in competing implementation?\nNote that there are two use-cases -- using distributed TensorFlow on public cloud, and using TensorFlow on dedicated compute clusters. Existing TF is optimized for the public cloud case because that's what compute at Google is like -- CPU and network resources are shared among many competing users, so there's variability in communication/computation performance across workers and individual workers can die without warning. At OpenAI we use distributed tensorflow to train RL agents like this on public cloud providers and maintaining performance despite some workers slowing down/restarting/network flakiness is an important feature.\nThe \"dedicated compute cluster\" scenario is somewhat alien to Google, so this would need community to help define goals/requirements.\nIt seems @abhinavvishnu has done some work on integrating MPI. For a brief glance at the matex repo, it seems the approach is well-supported by current TF abstractions -- using session.run to do the computation, then using MPI for data transfers. The performance question is the latency introduced by going through TF client API. I've done preliminary investigation of the overhead introduced by session.run and it seems it can be reduced by 50% by using lower level TF_SessionRun (easy), and possibly another 50-95% by removing the memory copy (harder)", "body": "@avader906 Some numbers/concrete performance goals would be useful here. IE, what is an important application, how fast is it now in TF GRPC, how fast is it in competing implementation?\r\n\r\nNote that there are two use-cases -- using distributed TensorFlow on public cloud, and using TensorFlow on dedicated compute clusters. Existing TF is optimized for the public cloud case because that's what compute at Google is like -- CPU and network resources are shared among many competing users, so there's variability in communication/computation performance across workers and individual workers can die without warning. At OpenAI we use distributed tensorflow to train RL agents like [this](https://github.com/openai/universe-starter-agent) on public cloud providers and maintaining performance despite some workers slowing down/restarting/network flakiness is an important feature.\r\n\r\nThe \"dedicated compute cluster\" scenario is somewhat alien to Google, so this would need community to help define goals/requirements.\r\n\r\nIt seems @abhinavvishnu has done some work on integrating MPI. For a brief glance at the matex repo, it seems the approach is well-supported by current TF abstractions -- using `session.run` to do the computation, then using MPI for data transfers. The performance question is the latency introduced by going through TF client API. I've done preliminary investigation of the overhead introduced by `session.run` and it seems it can be reduced by 50% by using lower level TF_SessionRun (easy), and possibly another 50-95% by removing the memory copy (harder)"}