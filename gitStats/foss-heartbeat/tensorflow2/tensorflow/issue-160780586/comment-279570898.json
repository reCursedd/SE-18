{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279570898", "html_url": "https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-279570898", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2916", "id": 279570898, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTU3MDg5OA==", "user": {"login": "jeffhammond", "id": 406118, "node_id": "MDQ6VXNlcjQwNjExOA==", "avatar_url": "https://avatars2.githubusercontent.com/u/406118?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffhammond", "html_url": "https://github.com/jeffhammond", "followers_url": "https://api.github.com/users/jeffhammond/followers", "following_url": "https://api.github.com/users/jeffhammond/following{/other_user}", "gists_url": "https://api.github.com/users/jeffhammond/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffhammond/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffhammond/subscriptions", "organizations_url": "https://api.github.com/users/jeffhammond/orgs", "repos_url": "https://api.github.com/users/jeffhammond/repos", "events_url": "https://api.github.com/users/jeffhammond/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffhammond/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-14T00:41:33Z", "updated_at": "2017-02-14T00:41:33Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2708344\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/anfeng\">@anfeng</a> What specific issues do you see with using MPI in a cloud setting?</p>\n<p>MPI has supported dynamic process management since version <a href=\"http://mpi-forum.org/docs/mpi-2.0/mpi2-report.pdf\" rel=\"nofollow\">MPI 2.0</a>, which was released in 2003.  Granted, high-quality support for functions like <a href=\"https://linux.die.net/man/3/mpi_comm_spawn_multiple\" rel=\"nofollow\">MPI_Comm_spawn_multiple</a> has not always been widely available, but that is only because the majority of MPI users do not care about that feature set (because they are using MPI in a static resource environment).</p>\n<p>Dynamically subtracting nodes from MPI jobs is a slightly more complicated issue than adding nodes, but this is intimately related to fault-tolerance activities that are of great interest to many in the MPI community right now.</p>\n<p>MPI-in-the-cloud proof points include:</p>\n<ul>\n<li>A friend of mine has enabled MPICH to work with Apache YARN as described <a href=\"http://lists.mpich.org/pipermail/devel/2016-July/000717.html\" rel=\"nofollow\">here</a>.</li>\n<li>Mesos appears to support MPICH via <a href=\"https://github.com/mesosphere/mesos-hydra\">this project</a>.</li>\n<li>AWS supports MPI as described <a href=\"https://cyclecomputing.com/running-mpi-applications-in-amazon-ec2/\" rel=\"nofollow\">here</a>.</li>\n<li>MPI plus Hadoop is described by Intel <a href=\"https://software.intel.com/en-us/articles/intel-mpi-library-supporting-the-hadoop-ecosystem\" rel=\"nofollow\">here</a>.</li>\n</ul>\n<p>One of the advantages of using MPI is that you can reuse a wide range of numerical linear algebra software from the HPC community, e.g. <a href=\"https://github.com/solomonik/ctf\">CTF</a> achieves close to 50% of peak on 1000+ node supercomputers (<a href=\"https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.html\" rel=\"nofollow\">details</a>) for tensor operations.  I'm not aware of any distributed-memory deep learning frameworks that can do this yet.</p>\n<p>Disclaimer: I work for Intel and was involved in two of the aforementioned efforts (which two should be obvious within two clicks).</p>", "body_text": "@anfeng What specific issues do you see with using MPI in a cloud setting?\nMPI has supported dynamic process management since version MPI 2.0, which was released in 2003.  Granted, high-quality support for functions like MPI_Comm_spawn_multiple has not always been widely available, but that is only because the majority of MPI users do not care about that feature set (because they are using MPI in a static resource environment).\nDynamically subtracting nodes from MPI jobs is a slightly more complicated issue than adding nodes, but this is intimately related to fault-tolerance activities that are of great interest to many in the MPI community right now.\nMPI-in-the-cloud proof points include:\n\nA friend of mine has enabled MPICH to work with Apache YARN as described here.\nMesos appears to support MPICH via this project.\nAWS supports MPI as described here.\nMPI plus Hadoop is described by Intel here.\n\nOne of the advantages of using MPI is that you can reuse a wide range of numerical linear algebra software from the HPC community, e.g. CTF achieves close to 50% of peak on 1000+ node supercomputers (details) for tensor operations.  I'm not aware of any distributed-memory deep learning frameworks that can do this yet.\nDisclaimer: I work for Intel and was involved in two of the aforementioned efforts (which two should be obvious within two clicks).", "body": "@anfeng What specific issues do you see with using MPI in a cloud setting?\r\n\r\nMPI has supported dynamic process management since version [MPI 2.0](http://mpi-forum.org/docs/mpi-2.0/mpi2-report.pdf), which was released in 2003.  Granted, high-quality support for functions like [MPI_Comm_spawn_multiple](https://linux.die.net/man/3/mpi_comm_spawn_multiple) has not always been widely available, but that is only because the majority of MPI users do not care about that feature set (because they are using MPI in a static resource environment).\r\n\r\nDynamically subtracting nodes from MPI jobs is a slightly more complicated issue than adding nodes, but this is intimately related to fault-tolerance activities that are of great interest to many in the MPI community right now.\r\n\r\nMPI-in-the-cloud proof points include:\r\n* A friend of mine has enabled MPICH to work with Apache YARN as described [here](http://lists.mpich.org/pipermail/devel/2016-July/000717.html).\r\n* Mesos appears to support MPICH via [this project](https://github.com/mesosphere/mesos-hydra).\r\n* AWS supports MPI as described [here](https://cyclecomputing.com/running-mpi-applications-in-amazon-ec2/).\r\n* MPI plus Hadoop is described by Intel [here](https://software.intel.com/en-us/articles/intel-mpi-library-supporting-the-hadoop-ecosystem).\r\n\r\nOne of the advantages of using MPI is that you can reuse a wide range of numerical linear algebra software from the HPC community, e.g. [CTF](https://github.com/solomonik/ctf) achieves close to 50% of peak on 1000+ node supercomputers ([details](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.html)) for tensor operations.  I'm not aware of any distributed-memory deep learning frameworks that can do this yet.\r\n\r\nDisclaimer: I work for Intel and was involved in two of the aforementioned efforts (which two should be obvious within two clicks)."}