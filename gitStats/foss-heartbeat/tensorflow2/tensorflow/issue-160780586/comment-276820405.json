{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/276820405", "html_url": "https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-276820405", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2916", "id": 276820405, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NjgyMDQwNQ==", "user": {"login": "junshi15", "id": 12075848, "node_id": "MDQ6VXNlcjEyMDc1ODQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/12075848?v=4", "gravatar_id": "", "url": "https://api.github.com/users/junshi15", "html_url": "https://github.com/junshi15", "followers_url": "https://api.github.com/users/junshi15/followers", "following_url": "https://api.github.com/users/junshi15/following{/other_user}", "gists_url": "https://api.github.com/users/junshi15/gists{/gist_id}", "starred_url": "https://api.github.com/users/junshi15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/junshi15/subscriptions", "organizations_url": "https://api.github.com/users/junshi15/orgs", "repos_url": "https://api.github.com/users/junshi15/repos", "events_url": "https://api.github.com/users/junshi15/events{/privacy}", "received_events_url": "https://api.github.com/users/junshi15/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-01T23:43:52Z", "updated_at": "2017-02-01T23:44:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>@avader906 Let's say I want to train a model with 32 GPU's. The maximum number of GPUs I can cram in a single box is 8. So I end up with 4 nodes. Then I connect the nodes with 1Gbps ethernet (those are all I have at this moment).  I launch the distributed version of Tensorflow, start training. It may take weeks to finish. I am not happy. Let's say I can not change the neural nets to reduce the computation. My next step may be upgrading ethernet cards to those shiny infiniband cards. I then somehow need to get tensorflow to work with inifiniband, maybe with help from this thread.</p>\n<p>Now before I invest in infiniband, I would like to know if the upgrade has any impact on my training. My VGG example, albeit artificial,  shows there is no need to upgrade the network for that particular case, since even with ethernet, my 1-worker-1-ps is as good as single-gpu, the best one can get.</p>\n<p>Yes, CaffeOnSpark, <a href=\"https://github.com/yahoo/CaffeOnSpark\">https://github.com/yahoo/CaffeOnSpark</a>, supports RDMA if you build with infiniband flag on and running in GPU mode. We see 2x improvement in speed although this  depends on a lot of settings.</p>", "body_text": "@avader906 Let's say I want to train a model with 32 GPU's. The maximum number of GPUs I can cram in a single box is 8. So I end up with 4 nodes. Then I connect the nodes with 1Gbps ethernet (those are all I have at this moment).  I launch the distributed version of Tensorflow, start training. It may take weeks to finish. I am not happy. Let's say I can not change the neural nets to reduce the computation. My next step may be upgrading ethernet cards to those shiny infiniband cards. I then somehow need to get tensorflow to work with inifiniband, maybe with help from this thread.\nNow before I invest in infiniband, I would like to know if the upgrade has any impact on my training. My VGG example, albeit artificial,  shows there is no need to upgrade the network for that particular case, since even with ethernet, my 1-worker-1-ps is as good as single-gpu, the best one can get.\nYes, CaffeOnSpark, https://github.com/yahoo/CaffeOnSpark, supports RDMA if you build with infiniband flag on and running in GPU mode. We see 2x improvement in speed although this  depends on a lot of settings.", "body": "@avader906 Let's say I want to train a model with 32 GPU's. The maximum number of GPUs I can cram in a single box is 8. So I end up with 4 nodes. Then I connect the nodes with 1Gbps ethernet (those are all I have at this moment).  I launch the distributed version of Tensorflow, start training. It may take weeks to finish. I am not happy. Let's say I can not change the neural nets to reduce the computation. My next step may be upgrading ethernet cards to those shiny infiniband cards. I then somehow need to get tensorflow to work with inifiniband, maybe with help from this thread.\r\n\r\nNow before I invest in infiniband, I would like to know if the upgrade has any impact on my training. My VGG example, albeit artificial,  shows there is no need to upgrade the network for that particular case, since even with ethernet, my 1-worker-1-ps is as good as single-gpu, the best one can get.\r\n\r\nYes, CaffeOnSpark, https://github.com/yahoo/CaffeOnSpark, supports RDMA if you build with infiniband flag on and running in GPU mode. We see 2x improvement in speed although this  depends on a lot of settings.\r\n"}