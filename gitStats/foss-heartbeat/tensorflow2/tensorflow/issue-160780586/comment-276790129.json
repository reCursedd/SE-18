{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/276790129", "html_url": "https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-276790129", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2916", "id": 276790129, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Njc5MDEyOQ==", "user": {"login": "junshi15", "id": 12075848, "node_id": "MDQ6VXNlcjEyMDc1ODQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/12075848?v=4", "gravatar_id": "", "url": "https://api.github.com/users/junshi15", "html_url": "https://github.com/junshi15", "followers_url": "https://api.github.com/users/junshi15/followers", "following_url": "https://api.github.com/users/junshi15/following{/other_user}", "gists_url": "https://api.github.com/users/junshi15/gists{/gist_id}", "starred_url": "https://api.github.com/users/junshi15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/junshi15/subscriptions", "organizations_url": "https://api.github.com/users/junshi15/orgs", "repos_url": "https://api.github.com/users/junshi15/repos", "events_url": "https://api.github.com/users/junshi15/events{/privacy}", "received_events_url": "https://api.github.com/users/junshi15/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-01T21:36:43Z", "updated_at": "2017-02-01T21:38:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Does anybody have a neural net that benefits significantly from fast networking (e.g. infiniband, etc)?</p>\n<p>I played with the vgg model from <a href=\"https://github.com/tensorflow/models/tree/master/slim\">https://github.com/tensorflow/models/tree/master/slim</a>. I compared two cases: (1) single-node-single-gpu, (2) 1-worker-1-ps, where the worker was on a GPU node and the parameter server was on a CPU node. In both cases, I used batch size 32 and I saw almost identical training speed (i.e. sec/step). If communication was bottleneck, I should see a lot slower with the worker/ps configuration.</p>\n<p>I could not run more than one workers with slim/vgg. I got out-of-memory error on the chief worker (I was using Nvidia K80 with 12GB memory).</p>\n<p>If anyone knows a good model that runs on multiple nodes and shows potential speedup with faster networking, I would like to know.</p>\n<p>Thanks.</p>", "body_text": "Does anybody have a neural net that benefits significantly from fast networking (e.g. infiniband, etc)?\nI played with the vgg model from https://github.com/tensorflow/models/tree/master/slim. I compared two cases: (1) single-node-single-gpu, (2) 1-worker-1-ps, where the worker was on a GPU node and the parameter server was on a CPU node. In both cases, I used batch size 32 and I saw almost identical training speed (i.e. sec/step). If communication was bottleneck, I should see a lot slower with the worker/ps configuration.\nI could not run more than one workers with slim/vgg. I got out-of-memory error on the chief worker (I was using Nvidia K80 with 12GB memory).\nIf anyone knows a good model that runs on multiple nodes and shows potential speedup with faster networking, I would like to know.\nThanks.", "body": "Does anybody have a neural net that benefits significantly from fast networking (e.g. infiniband, etc)?\r\n\r\nI played with the vgg model from https://github.com/tensorflow/models/tree/master/slim. I compared two cases: (1) single-node-single-gpu, (2) 1-worker-1-ps, where the worker was on a GPU node and the parameter server was on a CPU node. In both cases, I used batch size 32 and I saw almost identical training speed (i.e. sec/step). If communication was bottleneck, I should see a lot slower with the worker/ps configuration.\r\n\r\nI could not run more than one workers with slim/vgg. I got out-of-memory error on the chief worker (I was using Nvidia K80 with 12GB memory). \r\n\r\nIf anyone knows a good model that runs on multiple nodes and shows potential speedup with faster networking, I would like to know. \r\n\r\nThanks.\r\n\r\n"}