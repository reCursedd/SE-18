{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275757280", "html_url": "https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-275757280", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2916", "id": 275757280, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTc1NzI4MA==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-27T19:48:40Z", "updated_at": "2017-01-27T21:56:16Z", "author_association": "NONE", "body_html": "<p>If there is a convergence between distributed learning and distributed data - it will have to take into account investments people made on both ends. HPC community put down a lot into low latency networking to go with the compute clustering. Very different paradigm to high latency networking, commodity hardware and high redundancy suited for high data volume / low computation needs.</p>\n<p>MPI is mature and stable - powers HPC throughout the world since mid 90s. Has RDMA on all implementations microsoft, intel, and OpenMPI. Which one Baidu referred to?</p>\n<p>The first ML vendor to present RDMA / Infiniband support along the entire data path will win big.</p>", "body_text": "If there is a convergence between distributed learning and distributed data - it will have to take into account investments people made on both ends. HPC community put down a lot into low latency networking to go with the compute clustering. Very different paradigm to high latency networking, commodity hardware and high redundancy suited for high data volume / low computation needs.\nMPI is mature and stable - powers HPC throughout the world since mid 90s. Has RDMA on all implementations microsoft, intel, and OpenMPI. Which one Baidu referred to?\nThe first ML vendor to present RDMA / Infiniband support along the entire data path will win big.", "body": "If there is a convergence between distributed learning and distributed data - it will have to take into account investments people made on both ends. HPC community put down a lot into low latency networking to go with the compute clustering. Very different paradigm to high latency networking, commodity hardware and high redundancy suited for high data volume / low computation needs.\r\n\r\nMPI is mature and stable - powers HPC throughout the world since mid 90s. Has RDMA on all implementations microsoft, intel, and OpenMPI. Which one Baidu referred to? \r\n\r\nThe first ML vendor to present RDMA / Infiniband support along the entire data path will win big. "}