{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279574070", "html_url": "https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-279574070", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2916", "id": 279574070, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTU3NDA3MA==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-14T01:00:00Z", "updated_at": "2017-02-14T01:29:10Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Let them stare on to shadows. Everything in that thread - including\ntheoretical speedup from various interconnect / distributed computing stack\ncombinations can be calculated with a piece of paper. Latencies are known\nat every point and so is bandwidth. Scalability factors could be\napproximated from various benchmarks around there. If you give people\nnumbers and they resist, the issue is something else. There are multiples\nof alternatives to TF - it was not the first and not the last and is\nfundamentally flawed from the bottom of architecture up.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Tue, Feb 14, 2017 at 1:44 AM, Jeff Hammond ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/anfeng\">@anfeng</a> &lt;<a href=\"https://github.com/anfeng\">https://github.com/anfeng</a>&gt; What specific issues do you see with\n using MPI in a cloud setting?\n\n MPI has supported dynamic process management since version MPI 2.0\n &lt;<a href=\"http://mpi-forum.org/docs/mpi-2.0/mpi2-report.pdf\">http://mpi-forum.org/docs/mpi-2.0/mpi2-report.pdf</a>&gt;, which was released\n in 2003. Granted, high-quality support for functions like\n MPI_Comm_spawn_multiple\n &lt;<a href=\"https://linux.die.net/man/3/mpi_comm_spawn_multiple\">https://linux.die.net/man/3/mpi_comm_spawn_multiple</a>&gt; has not always been\n widely available, but that is only because the majority of MPI users do not\n care about that feature set (because they are using MPI in a static\n resource environment).\n\n Dynamically subtracting nodes from MPI jobs is a slightly more complicated\n issue than adding nodes, but this is intimately related to fault-tolerance\n activities that are of great interest to many in the MPI community right\n now.\n\n MPI-in-the-cloud proof points include:\n\n    - A friend of mine has enabled MPICH to work with Apache YARN as\n    described here\n    &lt;<a href=\"http://lists.mpich.org/pipermail/devel/2016-July/000717.html\">http://lists.mpich.org/pipermail/devel/2016-July/000717.html</a>&gt;.\n    - Mesos appears to support MPICH via this project\n    &lt;<a href=\"https://github.com/mesosphere/mesos-hydra\">https://github.com/mesosphere/mesos-hydra</a>&gt;.\n    - AWS supports MPI as described here\n    &lt;<a href=\"https://cyclecomputing.com/running-mpi-applications-in-amazon-ec2/\">https://cyclecomputing.com/running-mpi-applications-in-amazon-ec2/</a>&gt;.\n    - MPI plus Hadoop is described by Intel here\n    &lt;<a href=\"https://software.intel.com/en-us/articles/intel-mpi-library-supporting-the-hadoop-ecosystem\">https://software.intel.com/en-us/articles/intel-mpi-library-supporting-the-hadoop-ecosystem</a>&gt;\n    .\n\n One of the advantages of using MPI is that you can reuse a wide range of\n numerical linear algebra software from the HPC community, e.g. CTF\n &lt;<a href=\"https://github.com/solomonik/ctf\">https://github.com/solomonik/ctf</a>&gt; achieves close to 50% of peak on 1000+\n node supercomputers (details\n &lt;<a href=\"https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.html\">https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.html</a>&gt;)\n for tensor operations. I'm not aware of any distributed-memory deep\n learning frameworks that can do this yet.\n\n Disclaimer: I work for Intel and was involved in two of the aforementioned\n efforts (which two should be obvious within two clicks).\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"160780586\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2916\" href=\"https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-279570898\">#2916 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AVoAWGhhd_fa8Szyo3DZLsmURlFplg5wks5rcPjngaJpZM4I35Qd\">https://github.com/notifications/unsubscribe-auth/AVoAWGhhd_fa8Szyo3DZLsmURlFplg5wks5rcPjngaJpZM4I35Qd</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Let them stare on to shadows. Everything in that thread - including\ntheoretical speedup from various interconnect / distributed computing stack\ncombinations can be calculated with a piece of paper. Latencies are known\nat every point and so is bandwidth. Scalability factors could be\napproximated from various benchmarks around there. If you give people\nnumbers and they resist, the issue is something else. There are multiples\nof alternatives to TF - it was not the first and not the last and is\nfundamentally flawed from the bottom of architecture up.\n\u2026\nOn Tue, Feb 14, 2017 at 1:44 AM, Jeff Hammond ***@***.***> wrote:\n @anfeng <https://github.com/anfeng> What specific issues do you see with\n using MPI in a cloud setting?\n\n MPI has supported dynamic process management since version MPI 2.0\n <http://mpi-forum.org/docs/mpi-2.0/mpi2-report.pdf>, which was released\n in 2003. Granted, high-quality support for functions like\n MPI_Comm_spawn_multiple\n <https://linux.die.net/man/3/mpi_comm_spawn_multiple> has not always been\n widely available, but that is only because the majority of MPI users do not\n care about that feature set (because they are using MPI in a static\n resource environment).\n\n Dynamically subtracting nodes from MPI jobs is a slightly more complicated\n issue than adding nodes, but this is intimately related to fault-tolerance\n activities that are of great interest to many in the MPI community right\n now.\n\n MPI-in-the-cloud proof points include:\n\n    - A friend of mine has enabled MPICH to work with Apache YARN as\n    described here\n    <http://lists.mpich.org/pipermail/devel/2016-July/000717.html>.\n    - Mesos appears to support MPICH via this project\n    <https://github.com/mesosphere/mesos-hydra>.\n    - AWS supports MPI as described here\n    <https://cyclecomputing.com/running-mpi-applications-in-amazon-ec2/>.\n    - MPI plus Hadoop is described by Intel here\n    <https://software.intel.com/en-us/articles/intel-mpi-library-supporting-the-hadoop-ecosystem>\n    .\n\n One of the advantages of using MPI is that you can reuse a wide range of\n numerical linear algebra software from the HPC community, e.g. CTF\n <https://github.com/solomonik/ctf> achieves close to 50% of peak on 1000+\n node supercomputers (details\n <https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.html>)\n for tensor operations. I'm not aware of any distributed-memory deep\n learning frameworks that can do this yet.\n\n Disclaimer: I work for Intel and was involved in two of the aforementioned\n efforts (which two should be obvious within two clicks).\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#2916 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AVoAWGhhd_fa8Szyo3DZLsmURlFplg5wks5rcPjngaJpZM4I35Qd>\n .", "body": "Let them stare on to shadows. Everything in that thread - including\r\ntheoretical speedup from various interconnect / distributed computing stack\r\ncombinations can be calculated with a piece of paper. Latencies are known\r\nat every point and so is bandwidth. Scalability factors could be\r\napproximated from various benchmarks around there. If you give people\r\nnumbers and they resist, the issue is something else. There are multiples\r\nof alternatives to TF - it was not the first and not the last and is\r\nfundamentally flawed from the bottom of architecture up.\r\n\r\nOn Tue, Feb 14, 2017 at 1:44 AM, Jeff Hammond <notifications@github.com>\r\nwrote:\r\n\r\n> @anfeng <https://github.com/anfeng> What specific issues do you see with\r\n> using MPI in a cloud setting?\r\n>\r\n> MPI has supported dynamic process management since version MPI 2.0\r\n> <http://mpi-forum.org/docs/mpi-2.0/mpi2-report.pdf>, which was released\r\n> in 2003. Granted, high-quality support for functions like\r\n> MPI_Comm_spawn_multiple\r\n> <https://linux.die.net/man/3/mpi_comm_spawn_multiple> has not always been\r\n> widely available, but that is only because the majority of MPI users do not\r\n> care about that feature set (because they are using MPI in a static\r\n> resource environment).\r\n>\r\n> Dynamically subtracting nodes from MPI jobs is a slightly more complicated\r\n> issue than adding nodes, but this is intimately related to fault-tolerance\r\n> activities that are of great interest to many in the MPI community right\r\n> now.\r\n>\r\n> MPI-in-the-cloud proof points include:\r\n>\r\n>    - A friend of mine has enabled MPICH to work with Apache YARN as\r\n>    described here\r\n>    <http://lists.mpich.org/pipermail/devel/2016-July/000717.html>.\r\n>    - Mesos appears to support MPICH via this project\r\n>    <https://github.com/mesosphere/mesos-hydra>.\r\n>    - AWS supports MPI as described here\r\n>    <https://cyclecomputing.com/running-mpi-applications-in-amazon-ec2/>.\r\n>    - MPI plus Hadoop is described by Intel here\r\n>    <https://software.intel.com/en-us/articles/intel-mpi-library-supporting-the-hadoop-ecosystem>\r\n>    .\r\n>\r\n> One of the advantages of using MPI is that you can reuse a wide range of\r\n> numerical linear algebra software from the HPC community, e.g. CTF\r\n> <https://github.com/solomonik/ctf> achieves close to 50% of peak on 1000+\r\n> node supercomputers (details\r\n> <https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-143.html>)\r\n> for tensor operations. I'm not aware of any distributed-memory deep\r\n> learning frameworks that can do this yet.\r\n>\r\n> Disclaimer: I work for Intel and was involved in two of the aforementioned\r\n> efforts (which two should be obvious within two clicks).\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-279570898>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AVoAWGhhd_fa8Szyo3DZLsmURlFplg5wks5rcPjngaJpZM4I35Qd>\r\n> .\r\n>\r\n"}