{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/365475475", "html_url": "https://github.com/tensorflow/tensorflow/issues/14857#issuecomment-365475475", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14857", "id": 365475475, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTQ3NTQ3NQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-14T02:26:17Z", "updated_at": "2018-02-14T02:26:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8152811\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jcomfort4\">@jcomfort4</a> Looks mostly good. In answer to your questions:</p>\n<ol>\n<li>\n<p>Yes. You might want to move the <code>.map()</code> after the second <code>.shuffle()</code>, but that depends on whether the in-memory representation for parsed or unparsed data is more compact.</p>\n</li>\n<li>\n<p>Yes, this looks fine.</p>\n</li>\n<li>\n<p>The two values are somewhat independent. When the number of shards is small (like 25) then setting the <code>cycle_length</code> to that number seems like a good idea. If it's large (e.g. 1000) then you might consider setting <code>cycle_length</code> to a number less than 1000.</p>\n</li>\n<li>\n<p>With a slight modification:</p>\n<div class=\"highlight highlight-source-python\"><pre>files <span class=\"pl-k\">=</span> tf.matching_files(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gs://my-bucket/train-data-*.csv<span class=\"pl-pds\">\"</span></span>)\ndataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(files).shuffle(tf.shape(files)[<span class=\"pl-c1\">0</span>])</pre></div>\n</li>\n<li>\n<p>It'll affect startup time (because the buffer needs to fill up before it returns the first element) but if the buffer fits in RAM, it shouldn't affect training throughput.</p>\n</li>\n</ol>", "body_text": "@jcomfort4 Looks mostly good. In answer to your questions:\n\n\nYes. You might want to move the .map() after the second .shuffle(), but that depends on whether the in-memory representation for parsed or unparsed data is more compact.\n\n\nYes, this looks fine.\n\n\nThe two values are somewhat independent. When the number of shards is small (like 25) then setting the cycle_length to that number seems like a good idea. If it's large (e.g. 1000) then you might consider setting cycle_length to a number less than 1000.\n\n\nWith a slight modification:\nfiles = tf.matching_files(\"gs://my-bucket/train-data-*.csv\")\ndataset = tf.data.Dataset.from_tensor_slices(files).shuffle(tf.shape(files)[0])\n\n\nIt'll affect startup time (because the buffer needs to fill up before it returns the first element) but if the buffer fits in RAM, it shouldn't affect training throughput.", "body": "@jcomfort4 Looks mostly good. In answer to your questions:\r\n\r\n1. Yes. You might want to move the `.map()` after the second `.shuffle()`, but that depends on whether the in-memory representation for parsed or unparsed data is more compact.\r\n\r\n2. Yes, this looks fine.\r\n\r\n3. The two values are somewhat independent. When the number of shards is small (like 25) then setting the `cycle_length` to that number seems like a good idea. If it's large (e.g. 1000) then you might consider setting `cycle_length` to a number less than 1000.\r\n\r\n4. With a slight modification:\r\n\r\n   ```python\r\n   files = tf.matching_files(\"gs://my-bucket/train-data-*.csv\")\r\n   dataset = tf.data.Dataset.from_tensor_slices(files).shuffle(tf.shape(files)[0])\r\n   ```\r\n\r\n5. It'll affect startup time (because the buffer needs to fill up before it returns the first element) but if the buffer fits in RAM, it shouldn't affect training throughput."}