{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/365439428", "html_url": "https://github.com/tensorflow/tensorflow/issues/14857#issuecomment-365439428", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14857", "id": 365439428, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTQzOTQyOA==", "user": {"login": "jcomfort4", "id": 8152811, "node_id": "MDQ6VXNlcjgxNTI4MTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/8152811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcomfort4", "html_url": "https://github.com/jcomfort4", "followers_url": "https://api.github.com/users/jcomfort4/followers", "following_url": "https://api.github.com/users/jcomfort4/following{/other_user}", "gists_url": "https://api.github.com/users/jcomfort4/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcomfort4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcomfort4/subscriptions", "organizations_url": "https://api.github.com/users/jcomfort4/orgs", "repos_url": "https://api.github.com/users/jcomfort4/repos", "events_url": "https://api.github.com/users/jcomfort4/events{/privacy}", "received_events_url": "https://api.github.com/users/jcomfort4/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-13T23:20:04Z", "updated_at": "2018-02-13T23:20:54Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> I am trying to implement the your approach. I have a beam job that creates 25 shards of roughly equal size and I am trying to determine the best way to implement. Let's consider <code>filenames = gs://my-bucket/train-data-*.csv</code>. I am trying to implement this as a train_input_fn for a tf.estimator like the sample <a href=\"https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/customestimator/trainer/model.py#L321\">here</a>:</p>\n<p>Here is what I think you are suggesting, but I still feel like I am missing something...</p>\n<pre><code>def input_fn(filenames, num_shards, num_epochs):\n    dataset = Dataset.list_files(filenames).shuffle(num_shards) # num_shards = 25\n    dataset = dataset.interleave(lambda filename: \n        tf.data.TextLineDataset(filename).skip(1).map(parse_csv), cycle_length=num_shards) \n    dataset = dataset.shuffle(buffer_size = B) # this should be some value &gt; rows in single shard\n    dataset = dataset.repeat(num_epochs)\n    iterator = dataset.make_one_shot_iterator()\n    features = iterator.get_next()\n  return features, features.pop(LABEL_COLUMN)\n</code></pre>\n<p>Do you see any issues with this approach? Some questions I have on your write up are:</p>\n<ol>\n<li>\n<p>Am I using the <code>.skip()</code> and <code>.map()</code> in the proper location?</p>\n</li>\n<li>\n<p>You mention that you do this each epoch -- I believe that dataset.repeat(num_epochs) takes care of repeating this shuffling for each epoch, correct?</p>\n</li>\n<li>\n<p>Was there a reason your response had <code>num_shards</code> in the filename shuffle and then <code>cycle_length=N</code> in the interleave? My understanding from your description is that you would want these both to be the number of shards so that we are sampling from each shard evenly?</p>\n</li>\n<li>\n<p>Is there any way to get the number of shards dynamically? This should basically be the length of the <code>list_files</code> correct?</p>\n</li>\n<li>\n<p>Will the buffer_size have significant effects on how fast/slow the model takes? Hypothetically, If the shards are still rather large (let's say 500 MB), then I imagine that we would use up quite a lot of memory because the buffer would essentially load 500MB into memory (since buffer_size is basically the same as the size of one shard)</p>\n</li>\n</ol>", "body_text": "@mrry I am trying to implement the your approach. I have a beam job that creates 25 shards of roughly equal size and I am trying to determine the best way to implement. Let's consider filenames = gs://my-bucket/train-data-*.csv. I am trying to implement this as a train_input_fn for a tf.estimator like the sample here:\nHere is what I think you are suggesting, but I still feel like I am missing something...\ndef input_fn(filenames, num_shards, num_epochs):\n    dataset = Dataset.list_files(filenames).shuffle(num_shards) # num_shards = 25\n    dataset = dataset.interleave(lambda filename: \n        tf.data.TextLineDataset(filename).skip(1).map(parse_csv), cycle_length=num_shards) \n    dataset = dataset.shuffle(buffer_size = B) # this should be some value > rows in single shard\n    dataset = dataset.repeat(num_epochs)\n    iterator = dataset.make_one_shot_iterator()\n    features = iterator.get_next()\n  return features, features.pop(LABEL_COLUMN)\n\nDo you see any issues with this approach? Some questions I have on your write up are:\n\n\nAm I using the .skip() and .map() in the proper location?\n\n\nYou mention that you do this each epoch -- I believe that dataset.repeat(num_epochs) takes care of repeating this shuffling for each epoch, correct?\n\n\nWas there a reason your response had num_shards in the filename shuffle and then cycle_length=N in the interleave? My understanding from your description is that you would want these both to be the number of shards so that we are sampling from each shard evenly?\n\n\nIs there any way to get the number of shards dynamically? This should basically be the length of the list_files correct?\n\n\nWill the buffer_size have significant effects on how fast/slow the model takes? Hypothetically, If the shards are still rather large (let's say 500 MB), then I imagine that we would use up quite a lot of memory because the buffer would essentially load 500MB into memory (since buffer_size is basically the same as the size of one shard)", "body": "@mrry I am trying to implement the your approach. I have a beam job that creates 25 shards of roughly equal size and I am trying to determine the best way to implement. Let's consider `filenames = gs://my-bucket/train-data-*.csv`. I am trying to implement this as a train_input_fn for a tf.estimator like the sample [here](https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/customestimator/trainer/model.py#L321):  \r\n\r\nHere is what I think you are suggesting, but I still feel like I am missing something...\r\n\r\n```\r\ndef input_fn(filenames, num_shards, num_epochs):\r\n    dataset = Dataset.list_files(filenames).shuffle(num_shards) # num_shards = 25\r\n    dataset = dataset.interleave(lambda filename: \r\n        tf.data.TextLineDataset(filename).skip(1).map(parse_csv), cycle_length=num_shards) \r\n    dataset = dataset.shuffle(buffer_size = B) # this should be some value > rows in single shard\r\n    dataset = dataset.repeat(num_epochs)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    features = iterator.get_next()\r\n  return features, features.pop(LABEL_COLUMN)\r\n```\r\nDo you see any issues with this approach? Some questions I have on your write up are:\r\n\r\n1. Am I using the `.skip()` and `.map()` in the proper location?\r\n\r\n1. You mention that you do this each epoch -- I believe that dataset.repeat(num_epochs) takes care of repeating this shuffling for each epoch, correct?\r\n\r\n1. Was there a reason your response had `num_shards` in the filename shuffle and then `cycle_length=N` in the interleave? My understanding from your description is that you would want these both to be the number of shards so that we are sampling from each shard evenly?\r\n\r\n1. Is there any way to get the number of shards dynamically? This should basically be the length of the `list_files` correct?\r\n\r\n1. Will the buffer_size have significant effects on how fast/slow the model takes? Hypothetically, If the shards are still rather large (let's say 500 MB), then I imagine that we would use up quite a lot of memory because the buffer would essentially load 500MB into memory (since buffer_size is basically the same as the size of one shard)"}