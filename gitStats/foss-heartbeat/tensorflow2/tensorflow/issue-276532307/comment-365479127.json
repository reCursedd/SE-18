{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/365479127", "html_url": "https://github.com/tensorflow/tensorflow/issues/14857#issuecomment-365479127", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14857", "id": 365479127, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTQ3OTEyNw==", "user": {"login": "jcomfort4", "id": 8152811, "node_id": "MDQ6VXNlcjgxNTI4MTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/8152811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcomfort4", "html_url": "https://github.com/jcomfort4", "followers_url": "https://api.github.com/users/jcomfort4/followers", "following_url": "https://api.github.com/users/jcomfort4/following{/other_user}", "gists_url": "https://api.github.com/users/jcomfort4/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcomfort4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcomfort4/subscriptions", "organizations_url": "https://api.github.com/users/jcomfort4/orgs", "repos_url": "https://api.github.com/users/jcomfort4/repos", "events_url": "https://api.github.com/users/jcomfort4/events{/privacy}", "received_events_url": "https://api.github.com/users/jcomfort4/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-14T02:49:49Z", "updated_at": "2018-02-14T02:49:49Z", "author_association": "NONE", "body_html": "<p>Thanks! So how come I need to use <code>.from_tensor_slices(files)</code> -- would this still work with <code>list_files(\"gs://my-bucket/train-data-*.csv\").shuffle(tf.shape(files)[0])</code>? I guess I don't fully follow what from_tensor_slices is doing so I'm not completely following why that is necessary.</p>\n<p>I am running into some separate issues when upgrading from the old way of doing things so I'm not sure if I have done something incorrectly. Do either of these errors seem expected to you?</p>\n<p>I have a GCMLE experiment and I am trying to upgrade my <code>input_fn</code> to use the new <code>tf.data</code> functionality. I have created the following input_fn based off of this <a href=\"https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/customestimator/trainer/model.py#L321\">sample</a></p>\n<pre><code>def input_fn(...):\n    dataset = tf.data.Dataset.list_files(filenames).shuffle(num_shards) # shuffle up the list of input files\n\tdataset = dataset.interleave(lambda filename: # mix together records from cycle_length number of shards\n\t\t\t\ttf.data.TextLineDataset(filename).skip(1).map(lambda row: parse_csv(row, hparams)), cycle_length=5) \n    if shuffle:\n    dataset = dataset.shuffle(buffer_size = 10000)\n    dataset = dataset.repeat(num_epochs)\n\tdataset = dataset.batch(batch_size)\n\titerator = dataset.make_one_shot_iterator()\n    features = iterator.get_next()\n\n    labels = features.pop(LABEL_COLUMN)\n    \n    return features, labels\n</code></pre>\n<p>my <code>parse_csv</code> is the same as what I used previously, but it is not currently working. I can fix some of the issues, but I don't fully understand <strong>why</strong> I am having these issues. Here is my parse_csv() function</p>\n<pre><code>def parse_csv(..):\n    columns = tf.decode_csv(rows, record_defaults=CSV_COLUMN_DEFAULTS)\n\traw_features = dict(zip(FIELDNAMES, columns))\n\n    words = tf.string_split(raw_features['sentences']) # splitting words\n    vocab_table = tf.contrib.lookup.index_table_from_file(vocabulary_file = hparams.vocab_file,\n\t\t\t\tdefault_value = 0)\n\n....\n</code></pre>\n<ol>\n<li>\n<p>Right away this <code>tf.string_split()</code> stops working and the error is <code>ValueError: Shape must be rank 1 but is rank 0 for 'csv_preprocessing/input_sequence_generation/StringSplit' (op: 'StringSplit') with input shapes: [], [].</code> -- this is easily solved by packing <code>raw_features['sentences']</code> into a tensor via <code>[raw_features['sentences']]</code> but I do not understand why this is needed with the this <code>dataset</code> approach? How come in the old version this worked fine? For the shapes to match up with the rest of my model, I end up needing to remove this extra dimension at the end via <code>words = tf.squeeze(words, 0)</code> because I add this \"unecessary\" dimension to the tensor.</p>\n</li>\n<li>\n<p>For whatever reason, I am also getting an error that the table is not initialized <code>tensorflow.python.framework.errors_impl.FailedPreconditionError: Table not initialized.</code> however, this code works completely fine with my old <code>input_fn()</code> (see below) so I don't know why I would now need to initialize the tables? I have not figured out a solution to this part. Is there anything that I am missing to be able to use <code>tf.contrib.lookup.index_table_from_file</code> within my parse_csv function?</p>\n</li>\n</ol>\n<p>For reference, this is my old input_fn() that still does work:</p>\n<pre><code>def input_fn(...):\n    filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(filenames), \n\t\t\t\tnum_epochs=num_epochs, shuffle=shuffle, capacity=32)\n\treader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n\n\t_, rows = reader.read_up_to(filename_queue, num_records=batch_size)\n\n\tfeatures = parse_csv(rows, hparams)\n\n\t\t\n\t\tif shuffle:\n\t\t\tfeatures = tf.train.shuffle_batch(\n\t\t\t\tfeatures,\n\t\t\t\tbatch_size,\n\t\t\t\tmin_after_dequeue=2 * batch_size + 1,\n\t\t\t\tcapacity=batch_size * 10,\n\t\t\t\tnum_threads=multiprocessing.cpu_count(), \n\t\t\t\tenqueue_many=True,\n\t\t\t\tallow_smaller_final_batch=True\n\t\t\t)\n\t\telse:\n\t\t\tfeatures = tf.train.batch(\n\t\t\t\tfeatures,\n\t\t\t\tbatch_size,\n\t\t\t\tcapacity=batch_size * 10,\n\t\t\t\tnum_threads=multiprocessing.cpu_count(),\n\t\t\t\tenqueue_many=True,\n\t\t\t\tallow_smaller_final_batch=True\n\t\t\t)\n\nlabels = features.pop(LABEL_COLUMN)\n\nreturn features, labels\n</code></pre>", "body_text": "Thanks! So how come I need to use .from_tensor_slices(files) -- would this still work with list_files(\"gs://my-bucket/train-data-*.csv\").shuffle(tf.shape(files)[0])? I guess I don't fully follow what from_tensor_slices is doing so I'm not completely following why that is necessary.\nI am running into some separate issues when upgrading from the old way of doing things so I'm not sure if I have done something incorrectly. Do either of these errors seem expected to you?\nI have a GCMLE experiment and I am trying to upgrade my input_fn to use the new tf.data functionality. I have created the following input_fn based off of this sample\ndef input_fn(...):\n    dataset = tf.data.Dataset.list_files(filenames).shuffle(num_shards) # shuffle up the list of input files\n\tdataset = dataset.interleave(lambda filename: # mix together records from cycle_length number of shards\n\t\t\t\ttf.data.TextLineDataset(filename).skip(1).map(lambda row: parse_csv(row, hparams)), cycle_length=5) \n    if shuffle:\n    dataset = dataset.shuffle(buffer_size = 10000)\n    dataset = dataset.repeat(num_epochs)\n\tdataset = dataset.batch(batch_size)\n\titerator = dataset.make_one_shot_iterator()\n    features = iterator.get_next()\n\n    labels = features.pop(LABEL_COLUMN)\n    \n    return features, labels\n\nmy parse_csv is the same as what I used previously, but it is not currently working. I can fix some of the issues, but I don't fully understand why I am having these issues. Here is my parse_csv() function\ndef parse_csv(..):\n    columns = tf.decode_csv(rows, record_defaults=CSV_COLUMN_DEFAULTS)\n\traw_features = dict(zip(FIELDNAMES, columns))\n\n    words = tf.string_split(raw_features['sentences']) # splitting words\n    vocab_table = tf.contrib.lookup.index_table_from_file(vocabulary_file = hparams.vocab_file,\n\t\t\t\tdefault_value = 0)\n\n....\n\n\n\nRight away this tf.string_split() stops working and the error is ValueError: Shape must be rank 1 but is rank 0 for 'csv_preprocessing/input_sequence_generation/StringSplit' (op: 'StringSplit') with input shapes: [], []. -- this is easily solved by packing raw_features['sentences'] into a tensor via [raw_features['sentences']] but I do not understand why this is needed with the this dataset approach? How come in the old version this worked fine? For the shapes to match up with the rest of my model, I end up needing to remove this extra dimension at the end via words = tf.squeeze(words, 0) because I add this \"unecessary\" dimension to the tensor.\n\n\nFor whatever reason, I am also getting an error that the table is not initialized tensorflow.python.framework.errors_impl.FailedPreconditionError: Table not initialized. however, this code works completely fine with my old input_fn() (see below) so I don't know why I would now need to initialize the tables? I have not figured out a solution to this part. Is there anything that I am missing to be able to use tf.contrib.lookup.index_table_from_file within my parse_csv function?\n\n\nFor reference, this is my old input_fn() that still does work:\ndef input_fn(...):\n    filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(filenames), \n\t\t\t\tnum_epochs=num_epochs, shuffle=shuffle, capacity=32)\n\treader = tf.TextLineReader(skip_header_lines=skip_header_lines)\n\n\t_, rows = reader.read_up_to(filename_queue, num_records=batch_size)\n\n\tfeatures = parse_csv(rows, hparams)\n\n\t\t\n\t\tif shuffle:\n\t\t\tfeatures = tf.train.shuffle_batch(\n\t\t\t\tfeatures,\n\t\t\t\tbatch_size,\n\t\t\t\tmin_after_dequeue=2 * batch_size + 1,\n\t\t\t\tcapacity=batch_size * 10,\n\t\t\t\tnum_threads=multiprocessing.cpu_count(), \n\t\t\t\tenqueue_many=True,\n\t\t\t\tallow_smaller_final_batch=True\n\t\t\t)\n\t\telse:\n\t\t\tfeatures = tf.train.batch(\n\t\t\t\tfeatures,\n\t\t\t\tbatch_size,\n\t\t\t\tcapacity=batch_size * 10,\n\t\t\t\tnum_threads=multiprocessing.cpu_count(),\n\t\t\t\tenqueue_many=True,\n\t\t\t\tallow_smaller_final_batch=True\n\t\t\t)\n\nlabels = features.pop(LABEL_COLUMN)\n\nreturn features, labels", "body": "Thanks! So how come I need to use `.from_tensor_slices(files)` -- would this still work with `list_files(\"gs://my-bucket/train-data-*.csv\").shuffle(tf.shape(files)[0])`? I guess I don't fully follow what from_tensor_slices is doing so I'm not completely following why that is necessary.\r\n\r\nI am running into some separate issues when upgrading from the old way of doing things so I'm not sure if I have done something incorrectly. Do either of these errors seem expected to you?\r\n\r\nI have a GCMLE experiment and I am trying to upgrade my `input_fn` to use the new `tf.data` functionality. I have created the following input_fn based off of this [sample][1]\r\n\r\n    def input_fn(...):\r\n        dataset = tf.data.Dataset.list_files(filenames).shuffle(num_shards) # shuffle up the list of input files\r\n    \tdataset = dataset.interleave(lambda filename: # mix together records from cycle_length number of shards\r\n    \t\t\t\ttf.data.TextLineDataset(filename).skip(1).map(lambda row: parse_csv(row, hparams)), cycle_length=5) \r\n        if shuffle:\r\n        dataset = dataset.shuffle(buffer_size = 10000)\r\n        dataset = dataset.repeat(num_epochs)\r\n    \tdataset = dataset.batch(batch_size)\r\n    \titerator = dataset.make_one_shot_iterator()\r\n        features = iterator.get_next()\r\n\r\n        labels = features.pop(LABEL_COLUMN)\r\n        \r\n        return features, labels\r\n\r\nmy `parse_csv` is the same as what I used previously, but it is not currently working. I can fix some of the issues, but I don't fully understand **why** I am having these issues. Here is my parse_csv() function\r\n\r\n    def parse_csv(..):\r\n        columns = tf.decode_csv(rows, record_defaults=CSV_COLUMN_DEFAULTS)\r\n\t\traw_features = dict(zip(FIELDNAMES, columns))\r\n\r\n        words = tf.string_split(raw_features['sentences']) # splitting words\r\n        vocab_table = tf.contrib.lookup.index_table_from_file(vocabulary_file = hparams.vocab_file,\r\n\t\t\t\t\tdefault_value = 0)\r\n\r\n    ....\r\n\r\n1. Right away this `tf.string_split()` stops working and the error is `ValueError: Shape must be rank 1 but is rank 0 for 'csv_preprocessing/input_sequence_generation/StringSplit' (op: 'StringSplit') with input shapes: [], [].` -- this is easily solved by packing `raw_features['sentences']` into a tensor via `[raw_features['sentences']]` but I do not understand why this is needed with the this `dataset` approach? How come in the old version this worked fine? For the shapes to match up with the rest of my model, I end up needing to remove this extra dimension at the end via `words = tf.squeeze(words, 0)` because I add this \"unecessary\" dimension to the tensor. \r\n\r\n2. For whatever reason, I am also getting an error that the table is not initialized `tensorflow.python.framework.errors_impl.FailedPreconditionError: Table not initialized.` however, this code works completely fine with my old `input_fn()` (see below) so I don't know why I would now need to initialize the tables? I have not figured out a solution to this part. Is there anything that I am missing to be able to use `tf.contrib.lookup.index_table_from_file` within my parse_csv function?\r\n\r\nFor reference, this is my old input_fn() that still does work:\r\n\r\n    def input_fn(...):\r\n        filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(filenames), \r\n    \t\t\t\tnum_epochs=num_epochs, shuffle=shuffle, capacity=32)\r\n    \treader = tf.TextLineReader(skip_header_lines=skip_header_lines)\r\n\r\n\t\t_, rows = reader.read_up_to(filename_queue, num_records=batch_size)\r\n\r\n\t\tfeatures = parse_csv(rows, hparams)\r\n\r\n\t\t\t\r\n\t\t\tif shuffle:\r\n\t\t\t\tfeatures = tf.train.shuffle_batch(\r\n\t\t\t\t\tfeatures,\r\n\t\t\t\t\tbatch_size,\r\n\t\t\t\t\tmin_after_dequeue=2 * batch_size + 1,\r\n\t\t\t\t\tcapacity=batch_size * 10,\r\n\t\t\t\t\tnum_threads=multiprocessing.cpu_count(), \r\n\t\t\t\t\tenqueue_many=True,\r\n\t\t\t\t\tallow_smaller_final_batch=True\r\n\t\t\t\t)\r\n\t\t\telse:\r\n\t\t\t\tfeatures = tf.train.batch(\r\n\t\t\t\t\tfeatures,\r\n\t\t\t\t\tbatch_size,\r\n\t\t\t\t\tcapacity=batch_size * 10,\r\n\t\t\t\t\tnum_threads=multiprocessing.cpu_count(),\r\n\t\t\t\t\tenqueue_many=True,\r\n\t\t\t\t\tallow_smaller_final_batch=True\r\n\t\t\t\t)\r\n\r\n\tlabels = features.pop(LABEL_COLUMN)\r\n\r\n\treturn features, labels\r\n\r\n  [1]: https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/customestimator/trainer/model.py#L321"}