{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7576", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7576/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7576/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7576/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7576", "id": 208119686, "node_id": "MDU6SXNzdWUyMDgxMTk2ODY=", "number": 7576, "title": "GrpcRemoteMaster was chosen instead of LocalMaster", "user": {"login": "snnn", "id": 856316, "node_id": "MDQ6VXNlcjg1NjMxNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/856316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snnn", "html_url": "https://github.com/snnn", "followers_url": "https://api.github.com/users/snnn/followers", "following_url": "https://api.github.com/users/snnn/following{/other_user}", "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snnn/subscriptions", "organizations_url": "https://api.github.com/users/snnn/orgs", "repos_url": "https://api.github.com/users/snnn/repos", "events_url": "https://api.github.com/users/snnn/events{/privacy}", "received_events_url": "https://api.github.com/users/snnn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-02-16T13:22:31Z", "updated_at": "2017-02-17T02:10:46Z", "closed_at": "2017-02-17T01:54:23Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>none.</p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nWindows 10</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\nnone</p>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/9570f0f2804e857bc5593bed526eda7c1c915ed9/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/9570f0f2804e857bc5593bed526eda7c1c915ed9\"><tt>9570f0f</tt></a></li>\n<li>The output of <code>bazel version</code><br>\n(compiled by cmake)</li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.client import session\n\ndef main(_):\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\n    \"worker\": [\n        \"127.0.0.1:10000\"        \n    ],\n    \"ps\": [\n        \"127.0.0.1:10001\"\n    ]})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=\"worker\",\n                           task_index=0)\n  with tf.device(\"/job:ps/task:0\"):\n    weights = tf.Variable(tf.random_normal([10], stddev=0.35), name=\"weights\")  \n  init_op = tf.global_variables_initializer()\n  with tf.Session(\"grpc://127.0.0.1:10000\") as sess:\n    sess.run(init_op)\n    w  = sess.run(weights)\n    print(w)\n\nif __name__ == \"__main__\":\n  tf.app.run()\n</code></pre>\n<h3>What other attempted solutions have you tried?</h3>\n<p>change</p>\n<pre><code>  with tf.Session(\"grpc://127.0.0.1:10000\") as sess:\n</code></pre>\n<p>to</p>\n<pre><code>  with tf.Session(\"grpc://localhost:10000\") as sess:\n</code></pre>\n<p>solved it.</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).<br>\nThe reason is:<br>\nIn tensorflow\\core\\distributed_runtime\\rpc\\grpc_server_lib.cc: GrpcServer::target() function, the host part of target is hardcoded to \"localhost\"</p>\n<pre><code>const string GrpcServer::target() const {\n  return strings::StrCat(\"grpc://localhost:\", bound_port_);\n}\n</code></pre>\n<p>So, if the session was created with another address string, e.g. 127.0.0.1, (localip), etc. GrpcRemoteMaster  will be used instead of LocalMaster. I suggest LocalMaster class should have a isLocalIP() function to deal with this.</p>", "body_text": "What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nnone.\nEnvironment info\nOperating System:\nWindows 10\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nnone\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\n9570f0f\nThe output of bazel version\n(compiled by cmake)\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport tensorflow as tf\nfrom tensorflow.python.client import session\n\ndef main(_):\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\n    \"worker\": [\n        \"127.0.0.1:10000\"        \n    ],\n    \"ps\": [\n        \"127.0.0.1:10001\"\n    ]})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=\"worker\",\n                           task_index=0)\n  with tf.device(\"/job:ps/task:0\"):\n    weights = tf.Variable(tf.random_normal([10], stddev=0.35), name=\"weights\")  \n  init_op = tf.global_variables_initializer()\n  with tf.Session(\"grpc://127.0.0.1:10000\") as sess:\n    sess.run(init_op)\n    w  = sess.run(weights)\n    print(w)\n\nif __name__ == \"__main__\":\n  tf.app.run()\n\nWhat other attempted solutions have you tried?\nchange\n  with tf.Session(\"grpc://127.0.0.1:10000\") as sess:\n\nto\n  with tf.Session(\"grpc://localhost:10000\") as sess:\n\nsolved it.\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\nThe reason is:\nIn tensorflow\\core\\distributed_runtime\\rpc\\grpc_server_lib.cc: GrpcServer::target() function, the host part of target is hardcoded to \"localhost\"\nconst string GrpcServer::target() const {\n  return strings::StrCat(\"grpc://localhost:\", bound_port_);\n}\n\nSo, if the session was created with another address string, e.g. 127.0.0.1, (localip), etc. GrpcRemoteMaster  will be used instead of LocalMaster. I suggest LocalMaster class should have a isLocalIP() function to deal with this.", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nnone.\r\n\r\n### Environment info\r\nOperating System:\r\nWindows 10\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nnone\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n 9570f0f2804e857bc5593bed526eda7c1c915ed9\r\n2. The output of `bazel version`\r\n(compiled by cmake)\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import session\r\n\r\ndef main(_):\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\r\n    \"worker\": [\r\n        \"127.0.0.1:10000\"        \r\n    ],\r\n    \"ps\": [\r\n        \"127.0.0.1:10001\"\r\n    ]})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster,\r\n                           job_name=\"worker\",\r\n                           task_index=0)\r\n  with tf.device(\"/job:ps/task:0\"):\r\n    weights = tf.Variable(tf.random_normal([10], stddev=0.35), name=\"weights\")  \r\n  init_op = tf.global_variables_initializer()\r\n  with tf.Session(\"grpc://127.0.0.1:10000\") as sess:\r\n    sess.run(init_op)\r\n    w  = sess.run(weights)\r\n    print(w)\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n```\r\n### What other attempted solutions have you tried?\r\nchange \r\n```\r\n  with tf.Session(\"grpc://127.0.0.1:10000\") as sess:\r\n```\r\nto\r\n```\r\n  with tf.Session(\"grpc://localhost:10000\") as sess:\r\n```\r\n\r\nsolved it.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nThe reason is:\r\nIn tensorflow\\core\\distributed_runtime\\rpc\\grpc_server_lib.cc: GrpcServer::target() function, the host part of target is hardcoded to \"localhost\"\r\n```\r\nconst string GrpcServer::target() const {\r\n  return strings::StrCat(\"grpc://localhost:\", bound_port_);\r\n}\r\n```\r\nSo, if the session was created with another address string, e.g. 127.0.0.1, (localip), etc. GrpcRemoteMaster  will be used instead of LocalMaster. I suggest LocalMaster class should have a isLocalIP() function to deal with this.\r\n\r\n\r\n"}