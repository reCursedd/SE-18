{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8560", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8560/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8560/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8560/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8560", "id": 215499389, "node_id": "MDU6SXNzdWUyMTU0OTkzODk=", "number": 8560, "title": "GPU PoolAllocator never satisfied with the eviction rate. Can we limit its allocated size?", "user": {"login": "SeguinBe", "id": 7132817, "node_id": "MDQ6VXNlcjcxMzI4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7132817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SeguinBe", "html_url": "https://github.com/SeguinBe", "followers_url": "https://api.github.com/users/SeguinBe/followers", "following_url": "https://api.github.com/users/SeguinBe/following{/other_user}", "gists_url": "https://api.github.com/users/SeguinBe/gists{/gist_id}", "starred_url": "https://api.github.com/users/SeguinBe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SeguinBe/subscriptions", "organizations_url": "https://api.github.com/users/SeguinBe/orgs", "repos_url": "https://api.github.com/users/SeguinBe/repos", "events_url": "https://api.github.com/users/SeguinBe/events{/privacy}", "received_events_url": "https://api.github.com/users/SeguinBe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-03-20T17:39:02Z", "updated_at": "2017-12-22T17:30:05Z", "closed_at": "2017-12-22T17:29:56Z", "author_association": "NONE", "body_html": "<p>I have a tensorflow network where I call <code>sess.run()</code> with <strong>tensors of widly changing sizes</strong>. <em>(to be more precise it takes two tensors as input a [None, E, None, None] and a [None, None, M])</em>. Additionally, I use some <code>tf.where</code> calls which generate variable sized tensors to be transferred to the GPU.</p>\n<p>Everything works fine and training goes nicely and quickly, but eventually it seems that the <strong><code>pool_allocator</code> is never satisfied with the eviction rate it gets and constantly try to increase its size</strong>.</p>\n<pre><code>2017-03-20 18:24:07.895271: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 590087 get requests, put_count=590089 evicted_count=2000 eviction_rate=0.00338932 and unsatisfied allocation rate=0.0140911\n2017-03-20 18:24:07.895342: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 69492 to 76441\n</code></pre>\n<p>Eventually, it <strong>fills the whole machine RAM</strong> (256GB....) and <strong>kills itself by running out-of-memory</strong> (in less than 1h...).</p>\n<p>This behavior only happens when running on GPU. I checked that no ops are added during training through calling <code>graph.finalize()</code>.</p>\n<p>I also tried using <code>tcmalloc</code> as proposed <a href=\"http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201703201715326785429\" rel=\"nofollow\">here</a>, and ran the profiler which was saying it was staying at a happy 500MB while the memory usage in <code>top</code> was multiple GB. As far as I understand, the <code>pool_allocator</code> uses his own <code>malloc</code> system so it would not show in <code>tcmalloc</code> profiler right?</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/855907/output_1.pdf\">tcmalloc profiler output</a></p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/35151207/how-to-interpret-poolallocator-messages-in-tensorflow\" rel=\"nofollow\">How to interpret PoolAllocator messages</a></li>\n<li><a href=\"http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201703201715326785429\" rel=\"nofollow\">How to debug a memory leak in TF</a></li>\n<li>The question I asked <a href=\"http://stackoverflow.com/questions/42861956/gpu-poolallocator-explodes-the-cpu-memory\" rel=\"nofollow\">here</a> on SO, without much success.</li>\n</ul>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nUbuntu 14.04<br>\nCUDA 8.0 + cuDNN 5.1</p>\n<p>python 3.5 nighty build</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>I honestly tried, but it seems that if I simplify the system the problem goes away.</p>", "body_text": "I have a tensorflow network where I call sess.run() with tensors of widly changing sizes. (to be more precise it takes two tensors as input a [None, E, None, None] and a [None, None, M]). Additionally, I use some tf.where calls which generate variable sized tensors to be transferred to the GPU.\nEverything works fine and training goes nicely and quickly, but eventually it seems that the pool_allocator is never satisfied with the eviction rate it gets and constantly try to increase its size.\n2017-03-20 18:24:07.895271: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 590087 get requests, put_count=590089 evicted_count=2000 eviction_rate=0.00338932 and unsatisfied allocation rate=0.0140911\n2017-03-20 18:24:07.895342: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 69492 to 76441\n\nEventually, it fills the whole machine RAM (256GB....) and kills itself by running out-of-memory (in less than 1h...).\nThis behavior only happens when running on GPU. I checked that no ops are added during training through calling graph.finalize().\nI also tried using tcmalloc as proposed here, and ran the profiler which was saying it was staying at a happy 500MB while the memory usage in top was multiple GB. As far as I understand, the pool_allocator uses his own malloc system so it would not show in tcmalloc profiler right?\ntcmalloc profiler output\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nHow to interpret PoolAllocator messages\nHow to debug a memory leak in TF\nThe question I asked here on SO, without much success.\n\nEnvironment info\nOperating System:\nUbuntu 14.04\nCUDA 8.0 + cuDNN 5.1\npython 3.5 nighty build\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nI honestly tried, but it seems that if I simplify the system the problem goes away.", "body": "I have a tensorflow network where I call `sess.run()` with **tensors of widly changing sizes**. _(to be more precise it takes two tensors as input a [None, E, None, None] and a [None, None, M])_. Additionally, I use some `tf.where` calls which generate variable sized tensors to be transferred to the GPU.\r\n\r\nEverything works fine and training goes nicely and quickly, but eventually it seems that the **`pool_allocator` is never satisfied with the eviction rate it gets and constantly try to increase its size**. \r\n\r\n```\r\n2017-03-20 18:24:07.895271: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 590087 get requests, put_count=590089 evicted_count=2000 eviction_rate=0.00338932 and unsatisfied allocation rate=0.0140911\r\n2017-03-20 18:24:07.895342: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 69492 to 76441\r\n```\r\n\r\nEventually, it **fills the whole machine RAM** (256GB....) and **kills itself by running out-of-memory** (in less than 1h...).\r\n\r\nThis behavior only happens when running on GPU. I checked that no ops are added during training through calling `graph.finalize()`.\r\n\r\nI also tried using `tcmalloc` as proposed [here](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201703201715326785429), and ran the profiler which was saying it was staying at a happy 500MB while the memory usage in `top` was multiple GB. As far as I understand, the `pool_allocator` uses his own `malloc` system so it would not show in `tcmalloc` profiler right?\r\n\r\n[tcmalloc profiler output](https://github.com/tensorflow/tensorflow/files/855907/output_1.pdf)\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n- [How to interpret PoolAllocator messages](http://stackoverflow.com/questions/35151207/how-to-interpret-poolallocator-messages-in-tensorflow)\r\n- [How to debug a memory leak in TF](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201703201715326785429)\r\n- The question I asked [here](http://stackoverflow.com/questions/42861956/gpu-poolallocator-explodes-the-cpu-memory) on SO, without much success.\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04\r\nCUDA 8.0 + cuDNN 5.1\r\n\r\npython 3.5 nighty build\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI honestly tried, but it seems that if I simplify the system the problem goes away."}