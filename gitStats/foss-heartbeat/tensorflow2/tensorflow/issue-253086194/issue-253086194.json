{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12619", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12619/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12619/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12619/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12619", "id": 253086194, "node_id": "MDU6SXNzdWUyNTMwODYxOTQ=", "number": 12619, "title": "batch normalization", "user": {"login": "fffupeng", "id": 20182561, "node_id": "MDQ6VXNlcjIwMTgyNTYx", "avatar_url": "https://avatars2.githubusercontent.com/u/20182561?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fffupeng", "html_url": "https://github.com/fffupeng", "followers_url": "https://api.github.com/users/fffupeng/followers", "following_url": "https://api.github.com/users/fffupeng/following{/other_user}", "gists_url": "https://api.github.com/users/fffupeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/fffupeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fffupeng/subscriptions", "organizations_url": "https://api.github.com/users/fffupeng/orgs", "repos_url": "https://api.github.com/users/fffupeng/repos", "events_url": "https://api.github.com/users/fffupeng/events{/privacy}", "received_events_url": "https://api.github.com/users/fffupeng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-26T11:38:36Z", "updated_at": "2017-08-30T21:46:39Z", "closed_at": "2017-08-30T21:46:39Z", "author_association": "NONE", "body_html": "<p>These days i have meet some problem about BN layers, code is here, i want run my net on mnist<br>\ndataset, it worked when i am training, how when i verify on valiation data or test date,  when i change the state 'is_training'. what is wrong when i am verifing and how can i save mean and val in training state?</p>\n<p>``import tensorflow as tf</p>\n<p>from tensorflow.examples.tutorials.mnist import input_data<br>\nmnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)</p>\n<p>#define some weights<br>\ndef weight_variable(shape):<br>\ninitial = tf.truncated_normal(shape, stddev=0.01)<br>\nreturn tf.Variable(initial)</p>\n<p>def bias_variable(shape):<br>\ninitial = tf.constant(0.01, shape=shape)<br>\nreturn tf.Variable(initial)</p>\n<p>def conv2d(input, in_features, out_features, kernel_size, with_bias=False):<br>\nW = weight_variable([ kernel_size, kernel_size, in_features, out_features ])<br>\nconv = tf.nn.conv2d(input, W, [ 1, 1, 1, 1 ], padding='SAME')<br>\nif with_bias:<br>\nreturn conv + bias_variable([ out_features ])<br>\nreturn conv</p>\n<p>def batch_activ_conv(current, in_features, out_features, kernel_size, is_training, keep_prob):<br>\ncurrent = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)<br>\ncurrent = tf.nn.relu(current)<br>\ncurrent = conv2d(current, in_features, out_features, kernel_size)<br>\ncurrent = tf.nn.dropout(current, keep_prob)<br>\nreturn current</p>\n<p>def block(input, layers, in_features, growth, is_training, keep_prob):<br>\ncurrent = input<br>\nfeatures = in_features<br>\nfor idx in xrange(layers):<br>\ntmp = batch_activ_conv(current, features, growth, 3, is_training, keep_prob)<br>\ncurrent = tf.concat([current, tmp],3)<br>\nfeatures += growth<br>\nreturn current, features</p>\n<p>def avg_pool(input, s):<br>\nreturn tf.nn.avg_pool(input, [ 1, s, s, 1 ], [1, s, s, 1 ], 'VALID')</p>\n<p>#define graph</p>\n<p>layers =  12<br>\nprint 'create graph ...'</p>\n<p>x = tf.placeholder(tf.float32, [None, 784])<br>\ny_label = tf.placeholder(tf.float32, [None, 10])<br>\nlr = tf.placeholder(tf.float32)<br>\nkeep_prob = tf.placeholder(tf.float32)<br>\nis_training = tf.placeholder(tf.bool, shape=[])</p>\n<p>current = tf.reshape(x, [ -1, 28, 28, 1 ])<br>\ncurrent = conv2d(current, 1, 16, 3)</p>\n<p>current, features = block(current, layers, 16, 12, is_training, keep_prob)<br>\ncurrent = batch_activ_conv(current, features, features, 1, is_training, keep_prob)<br>\ncurrent = avg_pool(current, 2)  #14x14<br>\ncurrent, features = block(current, layers, features, 12, is_training, keep_prob)<br>\ncurrent = batch_activ_conv(current, features, features, 1, is_training, keep_prob)<br>\ncurrent = avg_pool(current, 2)#7x7<br>\ncurrent, features = block(current, layers, features, 12, is_training, keep_prob)</p>\n<p>current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)<br>\ncurrent = tf.nn.relu(current)<br>\ncurrent = avg_pool(current, 7)<br>\nfinal_dim = features<br>\ncurrent = tf.reshape(current, [-1, final_dim])<br>\nWfc = weight_variable([final_dim, 10])      #set classifiers<br>\nbfc = bias_variable([10])<br>\ny_predict = tf.nn.softmax(tf.matmul(current, Wfc) + bfc)</p>\n<p>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_predict))<br>\nl2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])<br>\nweight_decay = 1e-4<br>\n#update moving_mean and moving_variance<br>\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)<br>\nwith tf.control_dependencies(update_ops):<br>\ntrain_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy + l2 * weight_decay)</p>\n<p>correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y_label, 1))<br>\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  # caculate the right numbers</p>\n<p>def mytrain():<br>\nprint 'train ...'<br>\nsaver = tf.train.Saver()<br>\nwith tf.Session() as sess:<br>\nsess.run(tf.global_variables_initializer())<br>\nfor epoch in xrange(1, 10):    #train<br>\nif epoch &lt; 150:<br>\nl = 0.5<br>\nelif epoch &lt;200:<br>\nl = 0.1<br>\nelse:<br>\nl = 0.01<br>\nprint 'epoch: ',epoch<br>\nbatch_x, batch_y = mnist.train.next_batch(500)<br>\n_,acc,loss = sess.run([train_step,accuracy,cross_entropy],<br>\nfeed_dict={x: batch_x, y_label: batch_y, lr:l,is_training: True, keep_prob: 0.8})<br>\nprint 'train acc : ',acc,\" loss: \",loss<br>\n#val<br>\nbatch_x_val = mnist.validation.images<br>\nbatch_y_val = mnist.validation.labels<br>\nacc,loss = sess.run([accuracy,cross_entropy],feed_dict={x: batch_x_val, y_label: batch_y_val, is_training: False, keep_prob: 0.8})<br>\nprint 'val acc : ',acc,' loss: ',loss<br>\nsaver.save(sess, 'temp/densenet.ckpt')</p>\n<p>def mytest():<br>\nprint 'test ...'<br>\nsaver = tf.train.Saver()<br>\nwith tf.Session() as sess:<br>\nsaver.restore(sess, './temp/densenet.ckpt')<br>\nx_val = mnist.validation.images<br>\ny_val = mnist.validation.labels<br>\nval_results = sess.run(accuracy,<br>\nfeed_dict={x: x_val, y_label: y_val, is_training: True, keep_prob: 1.})<br>\nprint 'val acc: ', val_results<br>\nright = 0<br>\nfor i in range(100):<br>\nx_test, y_test = mnist.validation.next_batch(500)<br>\ntest_results = sess.run(accuracy,<br>\nfeed_dict={x: x_test, y_label: y_test, is_training: False, keep_prob: 1.})<br>\n#right = right + test_results<br>\nprint 'test:  acc: ', test_results</p>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\nmytrain()<br>\nmytest()</p>", "body_text": "These days i have meet some problem about BN layers, code is here, i want run my net on mnist\ndataset, it worked when i am training, how when i verify on valiation data or test date,  when i change the state 'is_training'. what is wrong when i am verifing and how can i save mean and val in training state?\n``import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n#define some weights\ndef weight_variable(shape):\ninitial = tf.truncated_normal(shape, stddev=0.01)\nreturn tf.Variable(initial)\ndef bias_variable(shape):\ninitial = tf.constant(0.01, shape=shape)\nreturn tf.Variable(initial)\ndef conv2d(input, in_features, out_features, kernel_size, with_bias=False):\nW = weight_variable([ kernel_size, kernel_size, in_features, out_features ])\nconv = tf.nn.conv2d(input, W, [ 1, 1, 1, 1 ], padding='SAME')\nif with_bias:\nreturn conv + bias_variable([ out_features ])\nreturn conv\ndef batch_activ_conv(current, in_features, out_features, kernel_size, is_training, keep_prob):\ncurrent = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\ncurrent = tf.nn.relu(current)\ncurrent = conv2d(current, in_features, out_features, kernel_size)\ncurrent = tf.nn.dropout(current, keep_prob)\nreturn current\ndef block(input, layers, in_features, growth, is_training, keep_prob):\ncurrent = input\nfeatures = in_features\nfor idx in xrange(layers):\ntmp = batch_activ_conv(current, features, growth, 3, is_training, keep_prob)\ncurrent = tf.concat([current, tmp],3)\nfeatures += growth\nreturn current, features\ndef avg_pool(input, s):\nreturn tf.nn.avg_pool(input, [ 1, s, s, 1 ], [1, s, s, 1 ], 'VALID')\n#define graph\nlayers =  12\nprint 'create graph ...'\nx = tf.placeholder(tf.float32, [None, 784])\ny_label = tf.placeholder(tf.float32, [None, 10])\nlr = tf.placeholder(tf.float32)\nkeep_prob = tf.placeholder(tf.float32)\nis_training = tf.placeholder(tf.bool, shape=[])\ncurrent = tf.reshape(x, [ -1, 28, 28, 1 ])\ncurrent = conv2d(current, 1, 16, 3)\ncurrent, features = block(current, layers, 16, 12, is_training, keep_prob)\ncurrent = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\ncurrent = avg_pool(current, 2)  #14x14\ncurrent, features = block(current, layers, features, 12, is_training, keep_prob)\ncurrent = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\ncurrent = avg_pool(current, 2)#7x7\ncurrent, features = block(current, layers, features, 12, is_training, keep_prob)\ncurrent = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\ncurrent = tf.nn.relu(current)\ncurrent = avg_pool(current, 7)\nfinal_dim = features\ncurrent = tf.reshape(current, [-1, final_dim])\nWfc = weight_variable([final_dim, 10])      #set classifiers\nbfc = bias_variable([10])\ny_predict = tf.nn.softmax(tf.matmul(current, Wfc) + bfc)\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_predict))\nl2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\nweight_decay = 1e-4\n#update moving_mean and moving_variance\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\ntrain_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy + l2 * weight_decay)\ncorrect_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y_label, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  # caculate the right numbers\ndef mytrain():\nprint 'train ...'\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\nsess.run(tf.global_variables_initializer())\nfor epoch in xrange(1, 10):    #train\nif epoch < 150:\nl = 0.5\nelif epoch <200:\nl = 0.1\nelse:\nl = 0.01\nprint 'epoch: ',epoch\nbatch_x, batch_y = mnist.train.next_batch(500)\n_,acc,loss = sess.run([train_step,accuracy,cross_entropy],\nfeed_dict={x: batch_x, y_label: batch_y, lr:l,is_training: True, keep_prob: 0.8})\nprint 'train acc : ',acc,\" loss: \",loss\n#val\nbatch_x_val = mnist.validation.images\nbatch_y_val = mnist.validation.labels\nacc,loss = sess.run([accuracy,cross_entropy],feed_dict={x: batch_x_val, y_label: batch_y_val, is_training: False, keep_prob: 0.8})\nprint 'val acc : ',acc,' loss: ',loss\nsaver.save(sess, 'temp/densenet.ckpt')\ndef mytest():\nprint 'test ...'\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\nsaver.restore(sess, './temp/densenet.ckpt')\nx_val = mnist.validation.images\ny_val = mnist.validation.labels\nval_results = sess.run(accuracy,\nfeed_dict={x: x_val, y_label: y_val, is_training: True, keep_prob: 1.})\nprint 'val acc: ', val_results\nright = 0\nfor i in range(100):\nx_test, y_test = mnist.validation.next_batch(500)\ntest_results = sess.run(accuracy,\nfeed_dict={x: x_test, y_label: y_test, is_training: False, keep_prob: 1.})\n#right = right + test_results\nprint 'test:  acc: ', test_results\nif name == 'main':\nmytrain()\nmytest()", "body": "These days i have meet some problem about BN layers, code is here, i want run my net on mnist\r\ndataset, it worked when i am training, how when i verify on valiation data or test date,  when i change the state 'is_training'. what is wrong when i am verifing and how can i save mean and val in training state? \r\n\r\n``import tensorflow as tf\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\r\n\r\n\r\n#define some weights\r\ndef weight_variable(shape):\r\n    initial = tf.truncated_normal(shape, stddev=0.01)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n\tinitial = tf.constant(0.01, shape=shape)\r\n\treturn tf.Variable(initial)\r\n\r\ndef conv2d(input, in_features, out_features, kernel_size, with_bias=False):\r\n\tW = weight_variable([ kernel_size, kernel_size, in_features, out_features ])\r\n\tconv = tf.nn.conv2d(input, W, [ 1, 1, 1, 1 ], padding='SAME')\r\n\tif with_bias:\r\n\t\treturn conv + bias_variable([ out_features ])\r\n\treturn conv\r\n\r\ndef batch_activ_conv(current, in_features, out_features, kernel_size, is_training, keep_prob):\r\n    current = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\r\n    current = tf.nn.relu(current)\r\n    current = conv2d(current, in_features, out_features, kernel_size)\r\n    current = tf.nn.dropout(current, keep_prob)\r\n    return current\r\n\r\ndef block(input, layers, in_features, growth, is_training, keep_prob):\r\n    current = input\r\n    features = in_features\r\n    for idx in xrange(layers):\r\n        tmp = batch_activ_conv(current, features, growth, 3, is_training, keep_prob)\r\n        current = tf.concat([current, tmp],3)\r\n        features += growth\r\n    return current, features\r\n\r\ndef avg_pool(input, s):\r\n    return tf.nn.avg_pool(input, [ 1, s, s, 1 ], [1, s, s, 1 ], 'VALID')\r\n\r\n\r\n#define graph\r\n\r\nlayers =  12\r\nprint 'create graph ...'\r\n\r\nx = tf.placeholder(tf.float32, [None, 784])\r\ny_label = tf.placeholder(tf.float32, [None, 10])\r\nlr = tf.placeholder(tf.float32)\r\nkeep_prob = tf.placeholder(tf.float32)\r\nis_training = tf.placeholder(tf.bool, shape=[])\r\n\r\ncurrent = tf.reshape(x, [ -1, 28, 28, 1 ])\r\ncurrent = conv2d(current, 1, 16, 3)\r\n\r\ncurrent, features = block(current, layers, 16, 12, is_training, keep_prob)\r\ncurrent = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\r\ncurrent = avg_pool(current, 2)  #14x14\r\ncurrent, features = block(current, layers, features, 12, is_training, keep_prob)\r\ncurrent = batch_activ_conv(current, features, features, 1, is_training, keep_prob)\r\ncurrent = avg_pool(current, 2)#7x7\r\ncurrent, features = block(current, layers, features, 12, is_training, keep_prob)\r\n\r\ncurrent = tf.contrib.layers.batch_norm(current, scale=True, is_training=is_training, updates_collections=None)\r\ncurrent = tf.nn.relu(current)\r\ncurrent = avg_pool(current, 7)\r\nfinal_dim = features\r\ncurrent = tf.reshape(current, [-1, final_dim])\r\nWfc = weight_variable([final_dim, 10])      #set classifiers\r\nbfc = bias_variable([10])\r\ny_predict = tf.nn.softmax(tf.matmul(current, Wfc) + bfc)\r\n\r\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_predict))\r\nl2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\r\nweight_decay = 1e-4\r\n#update moving_mean and moving_variance\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy + l2 * weight_decay)\r\n\r\ncorrect_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y_label, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  # caculate the right numbers\r\n\r\ndef mytrain():\r\n    print 'train ...'\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for epoch in xrange(1, 10):    #train\r\n            if epoch < 150:\r\n                l = 0.5\r\n            elif epoch <200:\r\n                l = 0.1\r\n            else:\r\n                l = 0.01\r\n            print 'epoch: ',epoch\r\n            batch_x, batch_y = mnist.train.next_batch(500)\r\n            _,acc,loss = sess.run([train_step,accuracy,cross_entropy],\r\n                                     feed_dict={x: batch_x, y_label: batch_y, lr:l,is_training: True, keep_prob: 0.8})\r\n            print 'train acc : ',acc,\" loss: \",loss\r\n            #val\r\n            batch_x_val = mnist.validation.images\r\n            batch_y_val = mnist.validation.labels\r\n            acc,loss = sess.run([accuracy,cross_entropy],feed_dict={x: batch_x_val, y_label: batch_y_val, is_training: False, keep_prob: 0.8})\r\n            print 'val acc : ',acc,' loss: ',loss\r\n        saver.save(sess, 'temp/densenet.ckpt')\r\n\r\ndef mytest():\r\n    print 'test ...'\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, './temp/densenet.ckpt')\r\n        x_val = mnist.validation.images\r\n        y_val = mnist.validation.labels\r\n        val_results = sess.run(accuracy,\r\n                                 feed_dict={x: x_val, y_label: y_val, is_training: True, keep_prob: 1.})\r\n        print 'val acc: ', val_results\r\n        right = 0\r\n        for i in range(100):\r\n            x_test, y_test = mnist.validation.next_batch(500)\r\n            test_results = sess.run(accuracy,\r\n                                    feed_dict={x: x_test, y_label: y_test, is_training: False, keep_prob: 1.})\r\n            #right = right + test_results\r\n        print 'test:  acc: ', test_results\r\n\r\nif __name__ == '__main__':\r\n    mytrain()\r\n    mytest()\r\n\r\n"}