{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/417670550", "html_url": "https://github.com/tensorflow/tensorflow/issues/21881#issuecomment-417670550", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21881", "id": 417670550, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzY3MDU1MA==", "user": {"login": "Coderx7", "id": 5382892, "node_id": "MDQ6VXNlcjUzODI4OTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/5382892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Coderx7", "html_url": "https://github.com/Coderx7", "followers_url": "https://api.github.com/users/Coderx7/followers", "following_url": "https://api.github.com/users/Coderx7/following{/other_user}", "gists_url": "https://api.github.com/users/Coderx7/gists{/gist_id}", "starred_url": "https://api.github.com/users/Coderx7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Coderx7/subscriptions", "organizations_url": "https://api.github.com/users/Coderx7/orgs", "repos_url": "https://api.github.com/users/Coderx7/repos", "events_url": "https://api.github.com/users/Coderx7/events{/privacy}", "received_events_url": "https://api.github.com/users/Coderx7/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-31T13:51:07Z", "updated_at": "2018-08-31T13:58:35Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1381301\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ppwwyyxx\">@ppwwyyxx</a> : Thanks for the code, except for a couple of differences, it's the same architecture(it needs <code>GlobalMaxPoling</code> instead of the <code>GlobalAvgPooling</code> and also biases needed to be enabled, the preprocessing and optimization policy are not considered that important for the moment and the overall architecture seems correct).<br>\nThe speed is significantly faster than our TF example! it has nearly the same speed of Pytorch( its only 2-3 seconds slower).<br>\nAfter setting <code>use_bias=True</code>, and changing Adam to SGD for both scripts, I get 47 to 48 seconds an epoch in Pytorch and 50-51 seconds in your code!</p>\n<details>\n<summary>Code</summary>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> -*- coding: utf-8 -*-</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> os\n\n<span class=\"pl-k\">from</span> tensorpack <span class=\"pl-k\">import</span> <span class=\"pl-k\">*</span>\n<span class=\"pl-k\">from</span> tensorpack.tfutils.summary <span class=\"pl-k\">import</span> <span class=\"pl-k\">*</span>\n<span class=\"pl-k\">from</span> tensorpack.dataflow <span class=\"pl-k\">import</span> dataset\n<span class=\"pl-k\">import</span> tflearn\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-e\">ModelDesc</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">cifar_classnum</span>):\n        <span class=\"pl-c1\">super</span>(Model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.cifar_classnum <span class=\"pl-k\">=</span> cifar_classnum\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">inputs</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> [tf.placeholder(tf.float32, (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input<span class=\"pl-pds\">'</span></span>),\n                tf.placeholder(tf.int32, (<span class=\"pl-c1\">None</span>,), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>label<span class=\"pl-pds\">'</span></span>)]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">build_graph</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">image</span>, <span class=\"pl-smi\">label</span>):\n        is_training <span class=\"pl-k\">=</span> get_current_tower_context().is_training\n\n        image <span class=\"pl-k\">=</span> tf.transpose(image, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n        data_format <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span>\n\n        <span class=\"pl-k\">with</span> argscope(Conv2D, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>BNReLU, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>), \\\n                argscope([Conv2D, MaxPooling, BatchNorm], <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format):\n            logits <span class=\"pl-k\">=</span> LinearWrap(image) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1.1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">66</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1.2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2.1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2.2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2.3<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .MaxPooling(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>) \\\n                .tf.nn.dropout(<span class=\"pl-c1\">0.95</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.3<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.4<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.5<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">288</span>) \\\n                .MaxPooling(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>) \\\n                .tf.nn.dropout(<span class=\"pl-c1\">0.95</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv5.1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">288</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv5.2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">355</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv5.3<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">432</span>)() \n            logits <span class=\"pl-k\">=</span> tflearn.layers.conv.global_max_pool (logits, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>GlobalMaxPool<span class=\"pl-pds\">'</span></span>)\n            logits <span class=\"pl-k\">=</span> LinearWrap(logits) \\\n                .FullyConnected(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>linear<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">out_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.cifar_classnum)()\n\n        cost <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>label)\n        cost <span class=\"pl-k\">=</span> tf.reduce_mean(cost, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cross_entropy_loss<span class=\"pl-pds\">'</span></span>)\n\n        correct <span class=\"pl-k\">=</span> tf.to_float(tf.nn.in_top_k(logits, label, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>correct<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> monitor training error</span>\n        add_moving_summary(tf.reduce_mean(correct, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>))\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> weight decay on all W of fc layers</span>\n        wd_cost <span class=\"pl-k\">=</span> regularize_cost(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc.*/W<span class=\"pl-pds\">'</span></span>, l2_regularizer(<span class=\"pl-c1\">4e-4</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>regularize_loss<span class=\"pl-pds\">'</span></span>)\n        add_moving_summary(cost, wd_cost)\n\n        add_param_summary((<span class=\"pl-s\"><span class=\"pl-pds\">'</span>.*/W<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>histogram<span class=\"pl-pds\">'</span></span>]))   <span class=\"pl-c\"><span class=\"pl-c\">#</span> monitor W</span>\n        <span class=\"pl-k\">return</span> tf.add_n([cost, wd_cost], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cost<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">optimizer</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        lr <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning_rate<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-2</span>, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>lr<span class=\"pl-pds\">'</span></span>, lr)\n        <span class=\"pl-k\">return</span> tf.train.GradientDescentOptimizer(lr)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_data</span>(<span class=\"pl-smi\">train_or_test</span>, <span class=\"pl-smi\">cifar_classnum</span>):\n    isTrain <span class=\"pl-k\">=</span> train_or_test <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-k\">if</span> cifar_classnum <span class=\"pl-k\">==</span> <span class=\"pl-c1\">10</span>:\n        ds <span class=\"pl-k\">=</span> dataset.Cifar10(train_or_test)\n    <span class=\"pl-k\">else</span>:\n        ds <span class=\"pl-k\">=</span> dataset.Cifar100(train_or_test)\n    <span class=\"pl-k\">if</span> isTrain:\n        augmentors <span class=\"pl-k\">=</span> [\n            imgaug.RandomCrop((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)),\n            imgaug.Flip(<span class=\"pl-v\">horiz</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),\n            imgaug.Brightness(<span class=\"pl-c1\">63</span>),\n            imgaug.Contrast((<span class=\"pl-c1\">0.2</span>, <span class=\"pl-c1\">1.8</span>)),\n            imgaug.MeanVarianceNormalize(<span class=\"pl-v\">all_channel</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        ]\n    <span class=\"pl-k\">else</span>:\n        augmentors <span class=\"pl-k\">=</span> [\n            imgaug.CenterCrop((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)),\n            imgaug.MeanVarianceNormalize(<span class=\"pl-v\">all_channel</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        ]\n    ds <span class=\"pl-k\">=</span> AugmentImageComponent(ds, augmentors)\n    ds <span class=\"pl-k\">=</span> BatchData(ds, <span class=\"pl-c1\">100</span>, <span class=\"pl-v\">remainder</span><span class=\"pl-k\">=</span><span class=\"pl-k\">not</span> isTrain)\n    <span class=\"pl-k\">if</span> isTrain:\n        ds <span class=\"pl-k\">=</span> PrefetchDataZMQ(ds, <span class=\"pl-c1\">5</span>)\n    <span class=\"pl-k\">return</span> ds\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_config</span>(<span class=\"pl-smi\">cifar_classnum</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> prepare dataset</span>\n    dataset_train <span class=\"pl-k\">=</span> get_data(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>, cifar_classnum)\n    dataset_test <span class=\"pl-k\">=</span> get_data(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test<span class=\"pl-pds\">'</span></span>, cifar_classnum)\n    <span class=\"pl-k\">return</span> TrainConfig(\n        <span class=\"pl-v\">model</span><span class=\"pl-k\">=</span>Model(cifar_classnum),\n        <span class=\"pl-v\">data</span><span class=\"pl-k\">=</span>QueueInput(dataset_train),\n        <span class=\"pl-v\">callbacks</span><span class=\"pl-k\">=</span>[\n            ModelSaver(),\n            InferenceRunner(dataset_test,\n                            ScalarStats([<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cost<span class=\"pl-pds\">'</span></span>])),\n        ],\n        <span class=\"pl-v\">max_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">150</span>,\n    )\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    <span class=\"pl-k\">with</span> tf.Graph().as_default():\n        logger.set_logger_dir(os.path.join(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_log<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cifar<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(<span class=\"pl-c1\">10</span>)))\n        config <span class=\"pl-k\">=</span> get_config(<span class=\"pl-c1\">10</span>)\n\n        trainer <span class=\"pl-k\">=</span> SimpleTrainer()\n        launch_train_with_config(config, trainer)</pre></div>\n</details>\n<p>Here is the log for tensorpack snippet:</p>\n<pre><code>100%|###################################################################################################|500/500[00:47&lt;00:00,10.60it/s]\n[0831 13:28:01 @base.py:260] Epoch 3 (global_step 1500) finished, time:47.2 seconds.\n[0831 13:28:01 @saver.py:77] Model saved to train_log/cifar10/model-1500.\n100%|###################################################################################################|100/100[00:04&lt;00:00,22.37it/s]\n[0831 13:28:06 @monitor.py:435] QueueInput/queue_size: 50\n[0831 13:28:06 @monitor.py:435] accuracy: 0.62778\n[0831 13:28:06 @monitor.py:435] cross_entropy_loss: 1.0565\n[0831 13:28:06 @monitor.py:435] lr: 0.01\n[0831 13:28:06 @monitor.py:435] regularize_loss_internals/empty_regularize_loss: 0\n[0831 13:28:06 @monitor.py:435] validation_accuracy: 0.6072\n[0831 13:28:06 @monitor.py:435] validation_cost: 1.1003\n[0831 13:28:06 @group.py:48] Callbacks took 4.524 sec in total. InferenceRunner: 4.47 seconds\n[0831 13:28:06 @base.py:250] Start Epoch 4 ...\n100%|###################################################################################################|500/500[00:46&lt;00:00,10.69it/s]\n[0831 13:28:53 @base.py:260] Epoch 4 (global_step 2000) finished, time:46.8 seconds.\n[0831 13:28:53 @saver.py:77] Model saved to train_log/cifar10/model-2000.\n100%|###################################################################################################|100/100[00:04&lt;00:00,22.46it/s]\n[0831 13:28:57 @monitor.py:435] QueueInput/queue_size: 50\n[0831 13:28:57 @monitor.py:435] accuracy: 0.664\n[0831 13:28:57 @monitor.py:435] cross_entropy_loss: 0.9451\n[0831 13:28:57 @monitor.py:435] lr: 0.01\n[0831 13:28:57 @monitor.py:435] regularize_loss_internals/empty_regularize_loss: 0\n[0831 13:28:57 @monitor.py:435] validation_accuracy: 0.6025\n[0831 13:28:57 @monitor.py:435] validation_cost: 1.0947\n[0831 13:28:57 @group.py:48] Callbacks took 4.505 sec in total. InferenceRunner: 4.45 seconds\n[0831 13:28:57 @base.py:250] Start Epoch 5 ...\n100%|###################################################################################################|500/500[00:46&lt;00:00,10.68it/s]\n[0831 13:29:44 @base.py:260] Epoch 5 (global_step 2500) finished, time:46.8 seconds.\n[0831 13:29:44 @saver.py:77] Model saved to train_log/cifar10/model-2500.\n100%|###################################################################################################|100/100[00:04&lt;00:00,22.10it/s]\n[0831 13:29:49 @monitor.py:435] QueueInput/queue_size: 50\n[0831 13:29:49 @monitor.py:435] accuracy: 0.69861\n[0831 13:29:49 @monitor.py:435] cross_entropy_loss: 0.86407\n[0831 13:29:49 @monitor.py:435] lr: 0.01\n[0831 13:29:49 @monitor.py:435] regularize_loss_internals/empty_regularize_loss: 0\n[0831 13:29:49 @monitor.py:435] validation_accuracy: 0.6526\n[0831 13:29:49 @monitor.py:435] validation_cost: 0.99593\n[0831 13:29:49 @group.py:48] Callbacks took 4.584 sec in total. InferenceRunner: 4.53 seconds\n</code></pre>\n<p>and here is pytorchs.  taking 47~48 seconds per each epoch!:</p>\n<pre><code>==&gt;&gt;[2018-08-31 13:32:02] [Epoch=002/450] [Need: 06:01:19] [learning_rate=0.010000] [Best : Accuracy=67.96, Error=32.04]\n  Epoch: [002][000/500]   Time 0.124 (0.124)   Data 0.088 (0.088)   Loss 0.9159 (0.9159)   Prec@1 63.000 (63.000)   Prec@5 98.000 (98.000)   [2018-08-31 13:32:02]\n  Epoch: [002][200/500]   Time 0.086 (0.088)   Data 0.000 (0.001)   Loss 0.8868 (0.8391)   Prec@1 75.000 (70.204)   Prec@5 97.000 (97.532)   [2018-08-31 13:32:19]\n  Epoch: [002][400/500]   Time 0.089 (0.088)   Data 0.000 (0.000)   Loss 0.7823 (0.8075)   Prec@1 74.000 (71.436)   Prec@5 99.000 (97.701)   [2018-08-31 13:32:37]\n  **Train** Prec@1 72.076 Prec@5 97.844 Error@1 27.924\n  **Test** Prec@1 72.950 Prec@5 98.340 Error@1 27.050\n---- save figure the accuracy/loss curve of train/val into ./snapshots/simpnet/plot_simpnet_cifar10_2018-08-31_13-30-12.png\n\n==&gt;&gt;[2018-08-31 13:32:49] [Epoch=003/450] [Need: 05:59:12] [learning_rate=0.010000] [Best : Accuracy=72.95, Error=27.05]\n  Epoch: [003][000/500]   Time 0.128 (0.128)   Data 0.093 (0.093)   Loss 0.7271 (0.7271)   Prec@1 71.000 (71.000)   Prec@5 99.000 (99.000)   [2018-08-31 13:32:50]\n  Epoch: [003][200/500]   Time 0.087 (0.089)   Data 0.000 (0.001)   Loss 0.6078 (0.6848)   Prec@1 77.000 (76.075)   Prec@5 99.000 (98.478)   [2018-08-31 13:33:07]\n  Epoch: [003][400/500]   Time 0.089 (0.089)   Data 0.000 (0.000)   Loss 0.5587 (0.6702)   Prec@1 80.000 (76.571)   Prec@5 99.000 (98.539)   [2018-08-31 13:33:25]\n  **Train** Prec@1 76.814 Prec@5 98.574 Error@1 23.186\n  **Test** Prec@1 76.680 Prec@5 98.350 Error@1 23.320\n---- save figure the accuracy/loss curve of train/val into ./snapshots/simpnet/plot_simpnet_cifar10_2018-08-31_13-30-12.png\n\n==&gt;&gt;[2018-08-31 13:33:37] [Epoch=004/450] [Need: 05:58:06] [learning_rate=0.010000] [Best : Accuracy=76.68, Error=23.32]\n  Epoch: [004][000/500]   Time 0.125 (0.125)   Data 0.090 (0.090)   Loss 0.6046 (0.6046)   Prec@1 82.000 (82.000)   Prec@5 98.000 (98.000)   [2018-08-31 13:33:38]\n  Epoch: [004][200/500]   Time 0.088 (0.088)   Data 0.000 (0.001)   Loss 0.5484 (0.5916)   Prec@1 83.000 (79.104)   Prec@5 99.000 (98.716)   [2018-08-31 13:33:55]\n  Epoch: [004][400/500]   Time 0.088 (0.088)   Data 0.000 (0.000)   Loss 0.7405 (0.5849)   Prec@1 76.000 (79.536)   Prec@5 98.000 (98.778)   [2018-08-31 13:34:13]\n  **Train** Prec@1 79.616 Prec@5 98.822 Error@1 20.384\n  **Test** Prec@1 80.310 Prec@5 98.710 Error@1 19.690\n---- save figure the accuracy/loss curve of train/val into ./snapshots/simpnet/plot_simpnet_cifar10_2018-08-31_13-30-12.png\n\n</code></pre>\n<p>This is indeed a massive speed up compared to the 2 times slower original TF implementation!!!</p>", "body_text": "@ppwwyyxx : Thanks for the code, except for a couple of differences, it's the same architecture(it needs GlobalMaxPoling instead of the GlobalAvgPooling and also biases needed to be enabled, the preprocessing and optimization policy are not considered that important for the moment and the overall architecture seems correct).\nThe speed is significantly faster than our TF example! it has nearly the same speed of Pytorch( its only 2-3 seconds slower).\nAfter setting use_bias=True, and changing Adam to SGD for both scripts, I get 47 to 48 seconds an epoch in Pytorch and 50-51 seconds in your code!\n\nCode\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport argparse\nimport os\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.summary import *\nfrom tensorpack.dataflow import dataset\nimport tflearn\n\nclass Model(ModelDesc):\n    def __init__(self, cifar_classnum):\n        super(Model, self).__init__()\n        self.cifar_classnum = cifar_classnum\n\n    def inputs(self):\n        return [tf.placeholder(tf.float32, (None, 32, 32, 3), 'input'),\n                tf.placeholder(tf.int32, (None,), 'label')]\n\n    def build_graph(self, image, label):\n        is_training = get_current_tower_context().is_training\n\n        image = tf.transpose(image, [0, 3, 1, 2])\n        data_format = 'channels_first'\n\n        with argscope(Conv2D, activation=BNReLU, use_bias=True, kernel_size=3), \\\n                argscope([Conv2D, MaxPooling, BatchNorm], data_format=data_format):\n            logits = LinearWrap(image) \\\n                .Conv2D('conv1.1', filters=66) \\\n                .Conv2D('conv1.2', filters=128) \\\n                .Conv2D('conv2.1', filters=128) \\\n                .Conv2D('conv2.2', filters=128) \\\n                .Conv2D('conv2.3', filters=192) \\\n                .MaxPooling('pool2', 2, stride=2, padding='SAME') \\\n                .tf.nn.dropout(0.95) \\\n                .Conv2D('conv4.1', filters=192) \\\n                .Conv2D('conv4.2', filters=192) \\\n                .Conv2D('conv4.3', filters=192) \\\n                .Conv2D('conv4.4', filters=192) \\\n                .Conv2D('conv4.5', filters=288) \\\n                .MaxPooling('pool2', 2, stride=2, padding='SAME') \\\n                .tf.nn.dropout(0.95) \\\n                .Conv2D('conv5.1', filters=288) \\\n                .Conv2D('conv5.2', filters=355) \\\n                .Conv2D('conv5.3', filters=432)() \n            logits = tflearn.layers.conv.global_max_pool (logits, name='GlobalMaxPool')\n            logits = LinearWrap(logits) \\\n                .FullyConnected('linear', out_dim=self.cifar_classnum)()\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n\n        correct = tf.to_float(tf.nn.in_top_k(logits, label, 1), name='correct')\n        # monitor training error\n        add_moving_summary(tf.reduce_mean(correct, name='accuracy'))\n\n        # weight decay on all W of fc layers\n        wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')\n        add_moving_summary(cost, wd_cost)\n\n        add_param_summary(('.*/W', ['histogram']))   # monitor W\n        return tf.add_n([cost, wd_cost], name='cost')\n\n    def optimizer(self):\n        lr = tf.get_variable('learning_rate', initializer=1e-2, trainable=False)\n        tf.summary.scalar('lr', lr)\n        return tf.train.GradientDescentOptimizer(lr)\n\n\ndef get_data(train_or_test, cifar_classnum):\n    isTrain = train_or_test == 'train'\n    if cifar_classnum == 10:\n        ds = dataset.Cifar10(train_or_test)\n    else:\n        ds = dataset.Cifar100(train_or_test)\n    if isTrain:\n        augmentors = [\n            imgaug.RandomCrop((32, 32)),\n            imgaug.Flip(horiz=True),\n            imgaug.Brightness(63),\n            imgaug.Contrast((0.2, 1.8)),\n            imgaug.MeanVarianceNormalize(all_channel=True)\n        ]\n    else:\n        augmentors = [\n            imgaug.CenterCrop((32, 32)),\n            imgaug.MeanVarianceNormalize(all_channel=True)\n        ]\n    ds = AugmentImageComponent(ds, augmentors)\n    ds = BatchData(ds, 100, remainder=not isTrain)\n    if isTrain:\n        ds = PrefetchDataZMQ(ds, 5)\n    return ds\n\n\ndef get_config(cifar_classnum):\n    # prepare dataset\n    dataset_train = get_data('train', cifar_classnum)\n    dataset_test = get_data('test', cifar_classnum)\n    return TrainConfig(\n        model=Model(cifar_classnum),\n        data=QueueInput(dataset_train),\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(dataset_test,\n                            ScalarStats(['accuracy', 'cost'])),\n        ],\n        max_epoch=150,\n    )\n\n\nif __name__ == '__main__':\n    with tf.Graph().as_default():\n        logger.set_logger_dir(os.path.join('train_log', 'cifar' + str(10)))\n        config = get_config(10)\n\n        trainer = SimpleTrainer()\n        launch_train_with_config(config, trainer)\n\nHere is the log for tensorpack snippet:\n100%|###################################################################################################|500/500[00:47<00:00,10.60it/s]\n[0831 13:28:01 @base.py:260] Epoch 3 (global_step 1500) finished, time:47.2 seconds.\n[0831 13:28:01 @saver.py:77] Model saved to train_log/cifar10/model-1500.\n100%|###################################################################################################|100/100[00:04<00:00,22.37it/s]\n[0831 13:28:06 @monitor.py:435] QueueInput/queue_size: 50\n[0831 13:28:06 @monitor.py:435] accuracy: 0.62778\n[0831 13:28:06 @monitor.py:435] cross_entropy_loss: 1.0565\n[0831 13:28:06 @monitor.py:435] lr: 0.01\n[0831 13:28:06 @monitor.py:435] regularize_loss_internals/empty_regularize_loss: 0\n[0831 13:28:06 @monitor.py:435] validation_accuracy: 0.6072\n[0831 13:28:06 @monitor.py:435] validation_cost: 1.1003\n[0831 13:28:06 @group.py:48] Callbacks took 4.524 sec in total. InferenceRunner: 4.47 seconds\n[0831 13:28:06 @base.py:250] Start Epoch 4 ...\n100%|###################################################################################################|500/500[00:46<00:00,10.69it/s]\n[0831 13:28:53 @base.py:260] Epoch 4 (global_step 2000) finished, time:46.8 seconds.\n[0831 13:28:53 @saver.py:77] Model saved to train_log/cifar10/model-2000.\n100%|###################################################################################################|100/100[00:04<00:00,22.46it/s]\n[0831 13:28:57 @monitor.py:435] QueueInput/queue_size: 50\n[0831 13:28:57 @monitor.py:435] accuracy: 0.664\n[0831 13:28:57 @monitor.py:435] cross_entropy_loss: 0.9451\n[0831 13:28:57 @monitor.py:435] lr: 0.01\n[0831 13:28:57 @monitor.py:435] regularize_loss_internals/empty_regularize_loss: 0\n[0831 13:28:57 @monitor.py:435] validation_accuracy: 0.6025\n[0831 13:28:57 @monitor.py:435] validation_cost: 1.0947\n[0831 13:28:57 @group.py:48] Callbacks took 4.505 sec in total. InferenceRunner: 4.45 seconds\n[0831 13:28:57 @base.py:250] Start Epoch 5 ...\n100%|###################################################################################################|500/500[00:46<00:00,10.68it/s]\n[0831 13:29:44 @base.py:260] Epoch 5 (global_step 2500) finished, time:46.8 seconds.\n[0831 13:29:44 @saver.py:77] Model saved to train_log/cifar10/model-2500.\n100%|###################################################################################################|100/100[00:04<00:00,22.10it/s]\n[0831 13:29:49 @monitor.py:435] QueueInput/queue_size: 50\n[0831 13:29:49 @monitor.py:435] accuracy: 0.69861\n[0831 13:29:49 @monitor.py:435] cross_entropy_loss: 0.86407\n[0831 13:29:49 @monitor.py:435] lr: 0.01\n[0831 13:29:49 @monitor.py:435] regularize_loss_internals/empty_regularize_loss: 0\n[0831 13:29:49 @monitor.py:435] validation_accuracy: 0.6526\n[0831 13:29:49 @monitor.py:435] validation_cost: 0.99593\n[0831 13:29:49 @group.py:48] Callbacks took 4.584 sec in total. InferenceRunner: 4.53 seconds\n\nand here is pytorchs.  taking 47~48 seconds per each epoch!:\n==>>[2018-08-31 13:32:02] [Epoch=002/450] [Need: 06:01:19] [learning_rate=0.010000] [Best : Accuracy=67.96, Error=32.04]\n  Epoch: [002][000/500]   Time 0.124 (0.124)   Data 0.088 (0.088)   Loss 0.9159 (0.9159)   Prec@1 63.000 (63.000)   Prec@5 98.000 (98.000)   [2018-08-31 13:32:02]\n  Epoch: [002][200/500]   Time 0.086 (0.088)   Data 0.000 (0.001)   Loss 0.8868 (0.8391)   Prec@1 75.000 (70.204)   Prec@5 97.000 (97.532)   [2018-08-31 13:32:19]\n  Epoch: [002][400/500]   Time 0.089 (0.088)   Data 0.000 (0.000)   Loss 0.7823 (0.8075)   Prec@1 74.000 (71.436)   Prec@5 99.000 (97.701)   [2018-08-31 13:32:37]\n  **Train** Prec@1 72.076 Prec@5 97.844 Error@1 27.924\n  **Test** Prec@1 72.950 Prec@5 98.340 Error@1 27.050\n---- save figure the accuracy/loss curve of train/val into ./snapshots/simpnet/plot_simpnet_cifar10_2018-08-31_13-30-12.png\n\n==>>[2018-08-31 13:32:49] [Epoch=003/450] [Need: 05:59:12] [learning_rate=0.010000] [Best : Accuracy=72.95, Error=27.05]\n  Epoch: [003][000/500]   Time 0.128 (0.128)   Data 0.093 (0.093)   Loss 0.7271 (0.7271)   Prec@1 71.000 (71.000)   Prec@5 99.000 (99.000)   [2018-08-31 13:32:50]\n  Epoch: [003][200/500]   Time 0.087 (0.089)   Data 0.000 (0.001)   Loss 0.6078 (0.6848)   Prec@1 77.000 (76.075)   Prec@5 99.000 (98.478)   [2018-08-31 13:33:07]\n  Epoch: [003][400/500]   Time 0.089 (0.089)   Data 0.000 (0.000)   Loss 0.5587 (0.6702)   Prec@1 80.000 (76.571)   Prec@5 99.000 (98.539)   [2018-08-31 13:33:25]\n  **Train** Prec@1 76.814 Prec@5 98.574 Error@1 23.186\n  **Test** Prec@1 76.680 Prec@5 98.350 Error@1 23.320\n---- save figure the accuracy/loss curve of train/val into ./snapshots/simpnet/plot_simpnet_cifar10_2018-08-31_13-30-12.png\n\n==>>[2018-08-31 13:33:37] [Epoch=004/450] [Need: 05:58:06] [learning_rate=0.010000] [Best : Accuracy=76.68, Error=23.32]\n  Epoch: [004][000/500]   Time 0.125 (0.125)   Data 0.090 (0.090)   Loss 0.6046 (0.6046)   Prec@1 82.000 (82.000)   Prec@5 98.000 (98.000)   [2018-08-31 13:33:38]\n  Epoch: [004][200/500]   Time 0.088 (0.088)   Data 0.000 (0.001)   Loss 0.5484 (0.5916)   Prec@1 83.000 (79.104)   Prec@5 99.000 (98.716)   [2018-08-31 13:33:55]\n  Epoch: [004][400/500]   Time 0.088 (0.088)   Data 0.000 (0.000)   Loss 0.7405 (0.5849)   Prec@1 76.000 (79.536)   Prec@5 98.000 (98.778)   [2018-08-31 13:34:13]\n  **Train** Prec@1 79.616 Prec@5 98.822 Error@1 20.384\n  **Test** Prec@1 80.310 Prec@5 98.710 Error@1 19.690\n---- save figure the accuracy/loss curve of train/val into ./snapshots/simpnet/plot_simpnet_cifar10_2018-08-31_13-30-12.png\n\n\nThis is indeed a massive speed up compared to the 2 times slower original TF implementation!!!", "body": "@ppwwyyxx : Thanks for the code, except for a couple of differences, it's the same architecture(it needs `GlobalMaxPoling` instead of the `GlobalAvgPooling` and also biases needed to be enabled, the preprocessing and optimization policy are not considered that important for the moment and the overall architecture seems correct). \r\nThe speed is significantly faster than our TF example! it has nearly the same speed of Pytorch( its only 2-3 seconds slower). \r\nAfter setting `use_bias=True`, and changing Adam to SGD for both scripts, I get 47 to 48 seconds an epoch in Pytorch and 50-51 seconds in your code! \r\n\r\n<details>\r\n<summary>Code</summary>\r\n\r\n```python\r\n#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport argparse\r\nimport os\r\n\r\nfrom tensorpack import *\r\nfrom tensorpack.tfutils.summary import *\r\nfrom tensorpack.dataflow import dataset\r\nimport tflearn\r\n\r\nclass Model(ModelDesc):\r\n    def __init__(self, cifar_classnum):\r\n        super(Model, self).__init__()\r\n        self.cifar_classnum = cifar_classnum\r\n\r\n    def inputs(self):\r\n        return [tf.placeholder(tf.float32, (None, 32, 32, 3), 'input'),\r\n                tf.placeholder(tf.int32, (None,), 'label')]\r\n\r\n    def build_graph(self, image, label):\r\n        is_training = get_current_tower_context().is_training\r\n\r\n        image = tf.transpose(image, [0, 3, 1, 2])\r\n        data_format = 'channels_first'\r\n\r\n        with argscope(Conv2D, activation=BNReLU, use_bias=True, kernel_size=3), \\\r\n                argscope([Conv2D, MaxPooling, BatchNorm], data_format=data_format):\r\n            logits = LinearWrap(image) \\\r\n                .Conv2D('conv1.1', filters=66) \\\r\n                .Conv2D('conv1.2', filters=128) \\\r\n                .Conv2D('conv2.1', filters=128) \\\r\n                .Conv2D('conv2.2', filters=128) \\\r\n                .Conv2D('conv2.3', filters=192) \\\r\n                .MaxPooling('pool2', 2, stride=2, padding='SAME') \\\r\n                .tf.nn.dropout(0.95) \\\r\n                .Conv2D('conv4.1', filters=192) \\\r\n                .Conv2D('conv4.2', filters=192) \\\r\n                .Conv2D('conv4.3', filters=192) \\\r\n                .Conv2D('conv4.4', filters=192) \\\r\n                .Conv2D('conv4.5', filters=288) \\\r\n                .MaxPooling('pool2', 2, stride=2, padding='SAME') \\\r\n                .tf.nn.dropout(0.95) \\\r\n                .Conv2D('conv5.1', filters=288) \\\r\n                .Conv2D('conv5.2', filters=355) \\\r\n                .Conv2D('conv5.3', filters=432)() \r\n            logits = tflearn.layers.conv.global_max_pool (logits, name='GlobalMaxPool')\r\n            logits = LinearWrap(logits) \\\r\n                .FullyConnected('linear', out_dim=self.cifar_classnum)()\r\n\r\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\r\n        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\r\n\r\n        correct = tf.to_float(tf.nn.in_top_k(logits, label, 1), name='correct')\r\n        # monitor training error\r\n        add_moving_summary(tf.reduce_mean(correct, name='accuracy'))\r\n\r\n        # weight decay on all W of fc layers\r\n        wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')\r\n        add_moving_summary(cost, wd_cost)\r\n\r\n        add_param_summary(('.*/W', ['histogram']))   # monitor W\r\n        return tf.add_n([cost, wd_cost], name='cost')\r\n\r\n    def optimizer(self):\r\n        lr = tf.get_variable('learning_rate', initializer=1e-2, trainable=False)\r\n        tf.summary.scalar('lr', lr)\r\n        return tf.train.GradientDescentOptimizer(lr)\r\n\r\n\r\ndef get_data(train_or_test, cifar_classnum):\r\n    isTrain = train_or_test == 'train'\r\n    if cifar_classnum == 10:\r\n        ds = dataset.Cifar10(train_or_test)\r\n    else:\r\n        ds = dataset.Cifar100(train_or_test)\r\n    if isTrain:\r\n        augmentors = [\r\n            imgaug.RandomCrop((32, 32)),\r\n            imgaug.Flip(horiz=True),\r\n            imgaug.Brightness(63),\r\n            imgaug.Contrast((0.2, 1.8)),\r\n            imgaug.MeanVarianceNormalize(all_channel=True)\r\n        ]\r\n    else:\r\n        augmentors = [\r\n            imgaug.CenterCrop((32, 32)),\r\n            imgaug.MeanVarianceNormalize(all_channel=True)\r\n        ]\r\n    ds = AugmentImageComponent(ds, augmentors)\r\n    ds = BatchData(ds, 100, remainder=not isTrain)\r\n    if isTrain:\r\n        ds = PrefetchDataZMQ(ds, 5)\r\n    return ds\r\n\r\n\r\ndef get_config(cifar_classnum):\r\n    # prepare dataset\r\n    dataset_train = get_data('train', cifar_classnum)\r\n    dataset_test = get_data('test', cifar_classnum)\r\n    return TrainConfig(\r\n        model=Model(cifar_classnum),\r\n        data=QueueInput(dataset_train),\r\n        callbacks=[\r\n            ModelSaver(),\r\n            InferenceRunner(dataset_test,\r\n                            ScalarStats(['accuracy', 'cost'])),\r\n        ],\r\n        max_epoch=150,\r\n    )\r\n\r\n\r\nif __name__ == '__main__':\r\n    with tf.Graph().as_default():\r\n        logger.set_logger_dir(os.path.join('train_log', 'cifar' + str(10)))\r\n        config = get_config(10)\r\n\r\n        trainer = SimpleTrainer()\r\n        launch_train_with_config(config, trainer)\r\n```\r\n</details>\r\n\r\nHere is the log for tensorpack snippet: \r\n```\r\n100%|###################################################################################################|500/500[00:47<00:00,10.60it/s]\r\n[0831 13:28:01 @base.py:260] Epoch 3 (global_step 1500) finished, time:47.2 seconds.\r\n[0831 13:28:01 @saver.py:77] Model saved to train_log/cifar10/model-1500.\r\n100%|###################################################################################################|100/100[00:04<00:00,22.37it/s]\r\n[0831 13:28:06 @monitor.py:435] QueueInput/queue_size: 50\r\n[0831 13:28:06 @monitor.py:435] accuracy: 0.62778\r\n[0831 13:28:06 @monitor.py:435] cross_entropy_loss: 1.0565\r\n[0831 13:28:06 @monitor.py:435] lr: 0.01\r\n[0831 13:28:06 @monitor.py:435] regularize_loss_internals/empty_regularize_loss: 0\r\n[0831 13:28:06 @monitor.py:435] validation_accuracy: 0.6072\r\n[0831 13:28:06 @monitor.py:435] validation_cost: 1.1003\r\n[0831 13:28:06 @group.py:48] Callbacks took 4.524 sec in total. InferenceRunner: 4.47 seconds\r\n[0831 13:28:06 @base.py:250] Start Epoch 4 ...\r\n100%|###################################################################################################|500/500[00:46<00:00,10.69it/s]\r\n[0831 13:28:53 @base.py:260] Epoch 4 (global_step 2000) finished, time:46.8 seconds.\r\n[0831 13:28:53 @saver.py:77] Model saved to train_log/cifar10/model-2000.\r\n100%|###################################################################################################|100/100[00:04<00:00,22.46it/s]\r\n[0831 13:28:57 @monitor.py:435] QueueInput/queue_size: 50\r\n[0831 13:28:57 @monitor.py:435] accuracy: 0.664\r\n[0831 13:28:57 @monitor.py:435] cross_entropy_loss: 0.9451\r\n[0831 13:28:57 @monitor.py:435] lr: 0.01\r\n[0831 13:28:57 @monitor.py:435] regularize_loss_internals/empty_regularize_loss: 0\r\n[0831 13:28:57 @monitor.py:435] validation_accuracy: 0.6025\r\n[0831 13:28:57 @monitor.py:435] validation_cost: 1.0947\r\n[0831 13:28:57 @group.py:48] Callbacks took 4.505 sec in total. InferenceRunner: 4.45 seconds\r\n[0831 13:28:57 @base.py:250] Start Epoch 5 ...\r\n100%|###################################################################################################|500/500[00:46<00:00,10.68it/s]\r\n[0831 13:29:44 @base.py:260] Epoch 5 (global_step 2500) finished, time:46.8 seconds.\r\n[0831 13:29:44 @saver.py:77] Model saved to train_log/cifar10/model-2500.\r\n100%|###################################################################################################|100/100[00:04<00:00,22.10it/s]\r\n[0831 13:29:49 @monitor.py:435] QueueInput/queue_size: 50\r\n[0831 13:29:49 @monitor.py:435] accuracy: 0.69861\r\n[0831 13:29:49 @monitor.py:435] cross_entropy_loss: 0.86407\r\n[0831 13:29:49 @monitor.py:435] lr: 0.01\r\n[0831 13:29:49 @monitor.py:435] regularize_loss_internals/empty_regularize_loss: 0\r\n[0831 13:29:49 @monitor.py:435] validation_accuracy: 0.6526\r\n[0831 13:29:49 @monitor.py:435] validation_cost: 0.99593\r\n[0831 13:29:49 @group.py:48] Callbacks took 4.584 sec in total. InferenceRunner: 4.53 seconds\r\n```\r\nand here is pytorchs.  taking 47~48 seconds per each epoch!: \r\n```\r\n==>>[2018-08-31 13:32:02] [Epoch=002/450] [Need: 06:01:19] [learning_rate=0.010000] [Best : Accuracy=67.96, Error=32.04]\r\n  Epoch: [002][000/500]   Time 0.124 (0.124)   Data 0.088 (0.088)   Loss 0.9159 (0.9159)   Prec@1 63.000 (63.000)   Prec@5 98.000 (98.000)   [2018-08-31 13:32:02]\r\n  Epoch: [002][200/500]   Time 0.086 (0.088)   Data 0.000 (0.001)   Loss 0.8868 (0.8391)   Prec@1 75.000 (70.204)   Prec@5 97.000 (97.532)   [2018-08-31 13:32:19]\r\n  Epoch: [002][400/500]   Time 0.089 (0.088)   Data 0.000 (0.000)   Loss 0.7823 (0.8075)   Prec@1 74.000 (71.436)   Prec@5 99.000 (97.701)   [2018-08-31 13:32:37]\r\n  **Train** Prec@1 72.076 Prec@5 97.844 Error@1 27.924\r\n  **Test** Prec@1 72.950 Prec@5 98.340 Error@1 27.050\r\n---- save figure the accuracy/loss curve of train/val into ./snapshots/simpnet/plot_simpnet_cifar10_2018-08-31_13-30-12.png\r\n\r\n==>>[2018-08-31 13:32:49] [Epoch=003/450] [Need: 05:59:12] [learning_rate=0.010000] [Best : Accuracy=72.95, Error=27.05]\r\n  Epoch: [003][000/500]   Time 0.128 (0.128)   Data 0.093 (0.093)   Loss 0.7271 (0.7271)   Prec@1 71.000 (71.000)   Prec@5 99.000 (99.000)   [2018-08-31 13:32:50]\r\n  Epoch: [003][200/500]   Time 0.087 (0.089)   Data 0.000 (0.001)   Loss 0.6078 (0.6848)   Prec@1 77.000 (76.075)   Prec@5 99.000 (98.478)   [2018-08-31 13:33:07]\r\n  Epoch: [003][400/500]   Time 0.089 (0.089)   Data 0.000 (0.000)   Loss 0.5587 (0.6702)   Prec@1 80.000 (76.571)   Prec@5 99.000 (98.539)   [2018-08-31 13:33:25]\r\n  **Train** Prec@1 76.814 Prec@5 98.574 Error@1 23.186\r\n  **Test** Prec@1 76.680 Prec@5 98.350 Error@1 23.320\r\n---- save figure the accuracy/loss curve of train/val into ./snapshots/simpnet/plot_simpnet_cifar10_2018-08-31_13-30-12.png\r\n\r\n==>>[2018-08-31 13:33:37] [Epoch=004/450] [Need: 05:58:06] [learning_rate=0.010000] [Best : Accuracy=76.68, Error=23.32]\r\n  Epoch: [004][000/500]   Time 0.125 (0.125)   Data 0.090 (0.090)   Loss 0.6046 (0.6046)   Prec@1 82.000 (82.000)   Prec@5 98.000 (98.000)   [2018-08-31 13:33:38]\r\n  Epoch: [004][200/500]   Time 0.088 (0.088)   Data 0.000 (0.001)   Loss 0.5484 (0.5916)   Prec@1 83.000 (79.104)   Prec@5 99.000 (98.716)   [2018-08-31 13:33:55]\r\n  Epoch: [004][400/500]   Time 0.088 (0.088)   Data 0.000 (0.000)   Loss 0.7405 (0.5849)   Prec@1 76.000 (79.536)   Prec@5 98.000 (98.778)   [2018-08-31 13:34:13]\r\n  **Train** Prec@1 79.616 Prec@5 98.822 Error@1 20.384\r\n  **Test** Prec@1 80.310 Prec@5 98.710 Error@1 19.690\r\n---- save figure the accuracy/loss curve of train/val into ./snapshots/simpnet/plot_simpnet_cifar10_2018-08-31_13-30-12.png\r\n\r\n```\r\nThis is indeed a massive speed up compared to the 2 times slower original TF implementation!!! \r\n"}