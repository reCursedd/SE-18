{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/417570368", "html_url": "https://github.com/tensorflow/tensorflow/issues/21881#issuecomment-417570368", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21881", "id": 417570368, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzU3MDM2OA==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-31T06:54:49Z", "updated_at": "2018-08-31T06:54:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5382892\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Coderx7\">@Coderx7</a> would you double-check that this script below has the same architecture as your \"simpnet\" (<a href=\"https://github.com/Coderx7/TF_Pytorch_testbed/blob/master/Pytorch/models/simpnet.py\">https://github.com/Coderx7/TF_Pytorch_testbed/blob/master/Pytorch/models/simpnet.py</a>), uses the same input size of 32 and batch size of 100 ?</p>\n<details><summary>code</summary>      \n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> -*- coding: utf-8 -*-</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> os\n\n<span class=\"pl-k\">from</span> tensorpack <span class=\"pl-k\">import</span> <span class=\"pl-k\">*</span>\n<span class=\"pl-k\">from</span> tensorpack.tfutils.summary <span class=\"pl-k\">import</span> <span class=\"pl-k\">*</span>\n<span class=\"pl-k\">from</span> tensorpack.dataflow <span class=\"pl-k\">import</span> dataset\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-e\">ModelDesc</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">cifar_classnum</span>):\n        <span class=\"pl-c1\">super</span>(Model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.cifar_classnum <span class=\"pl-k\">=</span> cifar_classnum\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">inputs</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> [tf.placeholder(tf.float32, (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input<span class=\"pl-pds\">'</span></span>),\n                tf.placeholder(tf.int32, (<span class=\"pl-c1\">None</span>,), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>label<span class=\"pl-pds\">'</span></span>)]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">build_graph</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">image</span>, <span class=\"pl-smi\">label</span>):\n        is_training <span class=\"pl-k\">=</span> get_current_tower_context().is_training\n\n        image <span class=\"pl-k\">=</span> tf.transpose(image, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n        data_format <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span>\n\n        <span class=\"pl-k\">with</span> argscope(Conv2D, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>BNReLU, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>), \\\n                argscope([Conv2D, MaxPooling, BatchNorm, GlobalAvgPooling], <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format):\n            logits <span class=\"pl-k\">=</span> LinearWrap(image) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1.1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">66</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1.2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2.1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2.2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2.3<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .MaxPooling(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>) \\\n                .tf.nn.dropout(<span class=\"pl-c1\">0.95</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.3<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.4<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">192</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv4.5<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">288</span>) \\\n                .MaxPooling(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>) \\\n                .tf.nn.dropout(<span class=\"pl-c1\">0.95</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv5.1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">288</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv5.2<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">355</span>) \\\n                .Conv2D(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv5.3<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">432</span>) \\\n                .GlobalAvgPooling(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>gap<span class=\"pl-pds\">'</span></span>) \\\n                .FullyConnected(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>linear<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">out_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.cifar_classnum)()\n\n        cost <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>label)\n        cost <span class=\"pl-k\">=</span> tf.reduce_mean(cost, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cross_entropy_loss<span class=\"pl-pds\">'</span></span>)\n\n        correct <span class=\"pl-k\">=</span> tf.to_float(tf.nn.in_top_k(logits, label, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>correct<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> monitor training error</span>\n        add_moving_summary(tf.reduce_mean(correct, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>))\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> weight decay on all W of fc layers</span>\n        wd_cost <span class=\"pl-k\">=</span> regularize_cost(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc.*/W<span class=\"pl-pds\">'</span></span>, l2_regularizer(<span class=\"pl-c1\">4e-4</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>regularize_loss<span class=\"pl-pds\">'</span></span>)\n        add_moving_summary(cost, wd_cost)\n\n        add_param_summary((<span class=\"pl-s\"><span class=\"pl-pds\">'</span>.*/W<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>histogram<span class=\"pl-pds\">'</span></span>]))   <span class=\"pl-c\"><span class=\"pl-c\">#</span> monitor W</span>\n        <span class=\"pl-k\">return</span> tf.add_n([cost, wd_cost], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cost<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">optimizer</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        lr <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning_rate<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-2</span>, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>lr<span class=\"pl-pds\">'</span></span>, lr)\n        <span class=\"pl-k\">return</span> tf.train.AdamOptimizer(lr, <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-3</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_data</span>(<span class=\"pl-smi\">train_or_test</span>, <span class=\"pl-smi\">cifar_classnum</span>):\n    isTrain <span class=\"pl-k\">=</span> train_or_test <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-k\">if</span> cifar_classnum <span class=\"pl-k\">==</span> <span class=\"pl-c1\">10</span>:\n        ds <span class=\"pl-k\">=</span> dataset.Cifar10(train_or_test)\n    <span class=\"pl-k\">else</span>:\n        ds <span class=\"pl-k\">=</span> dataset.Cifar100(train_or_test)\n    <span class=\"pl-k\">if</span> isTrain:\n        augmentors <span class=\"pl-k\">=</span> [\n            imgaug.RandomCrop((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)),\n            imgaug.Flip(<span class=\"pl-v\">horiz</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),\n            imgaug.Brightness(<span class=\"pl-c1\">63</span>),\n            imgaug.Contrast((<span class=\"pl-c1\">0.2</span>, <span class=\"pl-c1\">1.8</span>)),\n            imgaug.MeanVarianceNormalize(<span class=\"pl-v\">all_channel</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        ]\n    <span class=\"pl-k\">else</span>:\n        augmentors <span class=\"pl-k\">=</span> [\n            imgaug.CenterCrop((<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)),\n            imgaug.MeanVarianceNormalize(<span class=\"pl-v\">all_channel</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        ]\n    ds <span class=\"pl-k\">=</span> AugmentImageComponent(ds, augmentors)\n    ds <span class=\"pl-k\">=</span> BatchData(ds, <span class=\"pl-c1\">100</span>, <span class=\"pl-v\">remainder</span><span class=\"pl-k\">=</span><span class=\"pl-k\">not</span> isTrain)\n    <span class=\"pl-k\">if</span> isTrain:\n        ds <span class=\"pl-k\">=</span> PrefetchDataZMQ(ds, <span class=\"pl-c1\">5</span>)\n    <span class=\"pl-k\">return</span> ds\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_config</span>(<span class=\"pl-smi\">cifar_classnum</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> prepare dataset</span>\n    dataset_train <span class=\"pl-k\">=</span> get_data(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>, cifar_classnum)\n    dataset_test <span class=\"pl-k\">=</span> get_data(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test<span class=\"pl-pds\">'</span></span>, cifar_classnum)\n    <span class=\"pl-k\">return</span> TrainConfig(\n        <span class=\"pl-v\">model</span><span class=\"pl-k\">=</span>Model(cifar_classnum),\n        <span class=\"pl-v\">data</span><span class=\"pl-k\">=</span>QueueInput(dataset_train),\n        <span class=\"pl-v\">callbacks</span><span class=\"pl-k\">=</span>[\n            ModelSaver(),\n            InferenceRunner(dataset_test,\n                            ScalarStats([<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cost<span class=\"pl-pds\">'</span></span>])),\n        ],\n        <span class=\"pl-v\">max_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">150</span>,\n    )\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    <span class=\"pl-k\">with</span> tf.Graph().as_default():\n        logger.set_logger_dir(os.path.join(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_log<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cifar<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(<span class=\"pl-c1\">10</span>)))\n        config <span class=\"pl-k\">=</span> get_config(<span class=\"pl-c1\">10</span>)\n\n        trainer <span class=\"pl-k\">=</span> SimpleTrainer()\n        launch_train_with_config(config, trainer)</pre></div>\n </details>\n<p>I only copied an existing cifar10 training script and modify the relevant parts. It would take you some time but at least it's only 100 lines to read. <a href=\"https://github.com/Coderx7/TF_Pytorch_testbed/tree/master/TF/simpnet\">https://github.com/Coderx7/TF_Pytorch_testbed/tree/master/TF/simpnet</a> has much more lines of code for me to read on the other hand.</p>\n<p>You can run it by <code>pip install git+https://github.com/tensorpack/tensorpack.git; python thisfile.py</code>. On my machine it does run faster (30 seconds per epoch) than your pytorch code (37 seconds with <code>bash train_cifar10.sh</code>), ignoring the first epoch.</p>", "body_text": "@Coderx7 would you double-check that this script below has the same architecture as your \"simpnet\" (https://github.com/Coderx7/TF_Pytorch_testbed/blob/master/Pytorch/models/simpnet.py), uses the same input size of 32 and batch size of 100 ?\ncode      \n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport argparse\nimport os\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.summary import *\nfrom tensorpack.dataflow import dataset\n\n\nclass Model(ModelDesc):\n    def __init__(self, cifar_classnum):\n        super(Model, self).__init__()\n        self.cifar_classnum = cifar_classnum\n\n    def inputs(self):\n        return [tf.placeholder(tf.float32, (None, 32, 32, 3), 'input'),\n                tf.placeholder(tf.int32, (None,), 'label')]\n\n    def build_graph(self, image, label):\n        is_training = get_current_tower_context().is_training\n\n        image = tf.transpose(image, [0, 3, 1, 2])\n        data_format = 'channels_first'\n\n        with argscope(Conv2D, activation=BNReLU, use_bias=False, kernel_size=3), \\\n                argscope([Conv2D, MaxPooling, BatchNorm, GlobalAvgPooling], data_format=data_format):\n            logits = LinearWrap(image) \\\n                .Conv2D('conv1.1', filters=66) \\\n                .Conv2D('conv1.2', filters=128) \\\n                .Conv2D('conv2.1', filters=128) \\\n                .Conv2D('conv2.2', filters=128) \\\n                .Conv2D('conv2.3', filters=192) \\\n                .MaxPooling('pool2', 2, stride=2, padding='SAME') \\\n                .tf.nn.dropout(0.95) \\\n                .Conv2D('conv4.1', filters=192) \\\n                .Conv2D('conv4.2', filters=192) \\\n                .Conv2D('conv4.3', filters=192) \\\n                .Conv2D('conv4.4', filters=192) \\\n                .Conv2D('conv4.5', filters=288) \\\n                .MaxPooling('pool2', 2, stride=2, padding='SAME') \\\n                .tf.nn.dropout(0.95) \\\n                .Conv2D('conv5.1', filters=288) \\\n                .Conv2D('conv5.2', filters=355) \\\n                .Conv2D('conv5.3', filters=432) \\\n                .GlobalAvgPooling('gap') \\\n                .FullyConnected('linear', out_dim=self.cifar_classnum)()\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n\n        correct = tf.to_float(tf.nn.in_top_k(logits, label, 1), name='correct')\n        # monitor training error\n        add_moving_summary(tf.reduce_mean(correct, name='accuracy'))\n\n        # weight decay on all W of fc layers\n        wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')\n        add_moving_summary(cost, wd_cost)\n\n        add_param_summary(('.*/W', ['histogram']))   # monitor W\n        return tf.add_n([cost, wd_cost], name='cost')\n\n    def optimizer(self):\n        lr = tf.get_variable('learning_rate', initializer=1e-2, trainable=False)\n        tf.summary.scalar('lr', lr)\n        return tf.train.AdamOptimizer(lr, epsilon=1e-3)\n\n\ndef get_data(train_or_test, cifar_classnum):\n    isTrain = train_or_test == 'train'\n    if cifar_classnum == 10:\n        ds = dataset.Cifar10(train_or_test)\n    else:\n        ds = dataset.Cifar100(train_or_test)\n    if isTrain:\n        augmentors = [\n            imgaug.RandomCrop((32, 32)),\n            imgaug.Flip(horiz=True),\n            imgaug.Brightness(63),\n            imgaug.Contrast((0.2, 1.8)),\n            imgaug.MeanVarianceNormalize(all_channel=True)\n        ]\n    else:\n        augmentors = [\n            imgaug.CenterCrop((32, 32)),\n            imgaug.MeanVarianceNormalize(all_channel=True)\n        ]\n    ds = AugmentImageComponent(ds, augmentors)\n    ds = BatchData(ds, 100, remainder=not isTrain)\n    if isTrain:\n        ds = PrefetchDataZMQ(ds, 5)\n    return ds\n\n\ndef get_config(cifar_classnum):\n    # prepare dataset\n    dataset_train = get_data('train', cifar_classnum)\n    dataset_test = get_data('test', cifar_classnum)\n    return TrainConfig(\n        model=Model(cifar_classnum),\n        data=QueueInput(dataset_train),\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(dataset_test,\n                            ScalarStats(['accuracy', 'cost'])),\n        ],\n        max_epoch=150,\n    )\n\n\nif __name__ == '__main__':\n    with tf.Graph().as_default():\n        logger.set_logger_dir(os.path.join('train_log', 'cifar' + str(10)))\n        config = get_config(10)\n\n        trainer = SimpleTrainer()\n        launch_train_with_config(config, trainer)\n \nI only copied an existing cifar10 training script and modify the relevant parts. It would take you some time but at least it's only 100 lines to read. https://github.com/Coderx7/TF_Pytorch_testbed/tree/master/TF/simpnet has much more lines of code for me to read on the other hand.\nYou can run it by pip install git+https://github.com/tensorpack/tensorpack.git; python thisfile.py. On my machine it does run faster (30 seconds per epoch) than your pytorch code (37 seconds with bash train_cifar10.sh), ignoring the first epoch.", "body": "@Coderx7 would you double-check that this script below has the same architecture as your \"simpnet\" (https://github.com/Coderx7/TF_Pytorch_testbed/blob/master/Pytorch/models/simpnet.py), uses the same input size of 32 and batch size of 100 ?\r\n\r\n<details><summary>code</summary>      \r\n\r\n```python\r\n\r\n#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport argparse\r\nimport os\r\n\r\nfrom tensorpack import *\r\nfrom tensorpack.tfutils.summary import *\r\nfrom tensorpack.dataflow import dataset\r\n\r\n\r\nclass Model(ModelDesc):\r\n    def __init__(self, cifar_classnum):\r\n        super(Model, self).__init__()\r\n        self.cifar_classnum = cifar_classnum\r\n\r\n    def inputs(self):\r\n        return [tf.placeholder(tf.float32, (None, 32, 32, 3), 'input'),\r\n                tf.placeholder(tf.int32, (None,), 'label')]\r\n\r\n    def build_graph(self, image, label):\r\n        is_training = get_current_tower_context().is_training\r\n\r\n        image = tf.transpose(image, [0, 3, 1, 2])\r\n        data_format = 'channels_first'\r\n\r\n        with argscope(Conv2D, activation=BNReLU, use_bias=False, kernel_size=3), \\\r\n                argscope([Conv2D, MaxPooling, BatchNorm, GlobalAvgPooling], data_format=data_format):\r\n            logits = LinearWrap(image) \\\r\n                .Conv2D('conv1.1', filters=66) \\\r\n                .Conv2D('conv1.2', filters=128) \\\r\n                .Conv2D('conv2.1', filters=128) \\\r\n                .Conv2D('conv2.2', filters=128) \\\r\n                .Conv2D('conv2.3', filters=192) \\\r\n                .MaxPooling('pool2', 2, stride=2, padding='SAME') \\\r\n                .tf.nn.dropout(0.95) \\\r\n                .Conv2D('conv4.1', filters=192) \\\r\n                .Conv2D('conv4.2', filters=192) \\\r\n                .Conv2D('conv4.3', filters=192) \\\r\n                .Conv2D('conv4.4', filters=192) \\\r\n                .Conv2D('conv4.5', filters=288) \\\r\n                .MaxPooling('pool2', 2, stride=2, padding='SAME') \\\r\n                .tf.nn.dropout(0.95) \\\r\n                .Conv2D('conv5.1', filters=288) \\\r\n                .Conv2D('conv5.2', filters=355) \\\r\n                .Conv2D('conv5.3', filters=432) \\\r\n                .GlobalAvgPooling('gap') \\\r\n                .FullyConnected('linear', out_dim=self.cifar_classnum)()\r\n\r\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\r\n        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\r\n\r\n        correct = tf.to_float(tf.nn.in_top_k(logits, label, 1), name='correct')\r\n        # monitor training error\r\n        add_moving_summary(tf.reduce_mean(correct, name='accuracy'))\r\n\r\n        # weight decay on all W of fc layers\r\n        wd_cost = regularize_cost('fc.*/W', l2_regularizer(4e-4), name='regularize_loss')\r\n        add_moving_summary(cost, wd_cost)\r\n\r\n        add_param_summary(('.*/W', ['histogram']))   # monitor W\r\n        return tf.add_n([cost, wd_cost], name='cost')\r\n\r\n    def optimizer(self):\r\n        lr = tf.get_variable('learning_rate', initializer=1e-2, trainable=False)\r\n        tf.summary.scalar('lr', lr)\r\n        return tf.train.AdamOptimizer(lr, epsilon=1e-3)\r\n\r\n\r\ndef get_data(train_or_test, cifar_classnum):\r\n    isTrain = train_or_test == 'train'\r\n    if cifar_classnum == 10:\r\n        ds = dataset.Cifar10(train_or_test)\r\n    else:\r\n        ds = dataset.Cifar100(train_or_test)\r\n    if isTrain:\r\n        augmentors = [\r\n            imgaug.RandomCrop((32, 32)),\r\n            imgaug.Flip(horiz=True),\r\n            imgaug.Brightness(63),\r\n            imgaug.Contrast((0.2, 1.8)),\r\n            imgaug.MeanVarianceNormalize(all_channel=True)\r\n        ]\r\n    else:\r\n        augmentors = [\r\n            imgaug.CenterCrop((32, 32)),\r\n            imgaug.MeanVarianceNormalize(all_channel=True)\r\n        ]\r\n    ds = AugmentImageComponent(ds, augmentors)\r\n    ds = BatchData(ds, 100, remainder=not isTrain)\r\n    if isTrain:\r\n        ds = PrefetchDataZMQ(ds, 5)\r\n    return ds\r\n\r\n\r\ndef get_config(cifar_classnum):\r\n    # prepare dataset\r\n    dataset_train = get_data('train', cifar_classnum)\r\n    dataset_test = get_data('test', cifar_classnum)\r\n    return TrainConfig(\r\n        model=Model(cifar_classnum),\r\n        data=QueueInput(dataset_train),\r\n        callbacks=[\r\n            ModelSaver(),\r\n            InferenceRunner(dataset_test,\r\n                            ScalarStats(['accuracy', 'cost'])),\r\n        ],\r\n        max_epoch=150,\r\n    )\r\n\r\n\r\nif __name__ == '__main__':\r\n    with tf.Graph().as_default():\r\n        logger.set_logger_dir(os.path.join('train_log', 'cifar' + str(10)))\r\n        config = get_config(10)\r\n\r\n        trainer = SimpleTrainer()\r\n        launch_train_with_config(config, trainer)\r\n```\r\n </details>\r\n\r\n\r\nI only copied an existing cifar10 training script and modify the relevant parts. It would take you some time but at least it's only 100 lines to read. https://github.com/Coderx7/TF_Pytorch_testbed/tree/master/TF/simpnet has much more lines of code for me to read on the other hand.\r\n\r\nYou can run it by `pip install git+https://github.com/tensorpack/tensorpack.git; python thisfile.py`. On my machine it does run faster (30 seconds per epoch) than your pytorch code (37 seconds with `bash train_cifar10.sh`), ignoring the first epoch."}