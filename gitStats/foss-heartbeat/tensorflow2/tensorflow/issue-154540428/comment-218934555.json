{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/218934555", "html_url": "https://github.com/tensorflow/tensorflow/issues/2340#issuecomment-218934555", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2340", "id": 218934555, "node_id": "MDEyOklzc3VlQ29tbWVudDIxODkzNDU1NQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-13T02:07:21Z", "updated_at": "2016-05-13T02:07:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When we've seen this problem in the past, it's usually because something in the training loop is creating new graph nodes, and the TF runtime has to do more work each time you call <code>eval()</code>, because it believes it's running a new graph (even if it's structurally identical to the old ones). If you're calling <code>tf.concat()</code> in the training loop, that would be adding new nodes to the graph, and would lead to bad performance. One quick way to test this is to call <code>tf.get_default_graph().finalize()</code> before your training loop, and you will get an exception if anything is added to the graph.</p>\n<p>You can usually avoid these performance issues by creating <code>tf.placeholder()</code> nodes, defining the (e.g.) <code>tf.concat()</code> in terms of them outside the loop, and feeding in the appropriate values.</p>\n<p>I'm going to close this issue for now, but let us know if you have other questions!</p>", "body_text": "When we've seen this problem in the past, it's usually because something in the training loop is creating new graph nodes, and the TF runtime has to do more work each time you call eval(), because it believes it's running a new graph (even if it's structurally identical to the old ones). If you're calling tf.concat() in the training loop, that would be adding new nodes to the graph, and would lead to bad performance. One quick way to test this is to call tf.get_default_graph().finalize() before your training loop, and you will get an exception if anything is added to the graph.\nYou can usually avoid these performance issues by creating tf.placeholder() nodes, defining the (e.g.) tf.concat() in terms of them outside the loop, and feeding in the appropriate values.\nI'm going to close this issue for now, but let us know if you have other questions!", "body": "When we've seen this problem in the past, it's usually because something in the training loop is creating new graph nodes, and the TF runtime has to do more work each time you call `eval()`, because it believes it's running a new graph (even if it's structurally identical to the old ones). If you're calling `tf.concat()` in the training loop, that would be adding new nodes to the graph, and would lead to bad performance. One quick way to test this is to call `tf.get_default_graph().finalize()` before your training loop, and you will get an exception if anything is added to the graph.\n\nYou can usually avoid these performance issues by creating `tf.placeholder()` nodes, defining the (e.g.) `tf.concat()` in terms of them outside the loop, and feeding in the appropriate values.\n\nI'm going to close this issue for now, but let us know if you have other questions!\n"}