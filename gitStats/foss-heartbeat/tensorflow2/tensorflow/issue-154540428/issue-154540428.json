{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2340", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2340/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2340/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2340/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2340", "id": 154540428, "node_id": "MDU6SXNzdWUxNTQ1NDA0Mjg=", "number": 2340, "title": "Tensor.eval() Performance Decay", "user": {"login": "RuofanKong", "id": 7396554, "node_id": "MDQ6VXNlcjczOTY1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/7396554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RuofanKong", "html_url": "https://github.com/RuofanKong", "followers_url": "https://api.github.com/users/RuofanKong/followers", "following_url": "https://api.github.com/users/RuofanKong/following{/other_user}", "gists_url": "https://api.github.com/users/RuofanKong/gists{/gist_id}", "starred_url": "https://api.github.com/users/RuofanKong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RuofanKong/subscriptions", "organizations_url": "https://api.github.com/users/RuofanKong/orgs", "repos_url": "https://api.github.com/users/RuofanKong/repos", "events_url": "https://api.github.com/users/RuofanKong/events{/privacy}", "received_events_url": "https://api.github.com/users/RuofanKong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-05-12T17:41:46Z", "updated_at": "2016-05-13T02:07:21Z", "closed_at": "2016-05-13T02:07:21Z", "author_association": "NONE", "body_html": "<p>I used Tensorflow 0.6 (I pip installed TensorFlow: <code>pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl</code>) with Python 3 to train Neural Network on GPU (Nvidia GeForce GTX Titan X with Cuda 7.0 and Cudnn 6.5), and I noticed an interesting problem: The time consumption for running <code>tensor.eval(session=..., feed_dict=...)</code> at each iteration was continuously growing as iteration goes by, which indicated that performance of tensor.eval() decayed. Specifically, the code looks like this:</p>\n<pre><code>import tensorflow as tf\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\nsession = tf.Session(config=tensorflow.ConfigProto(gpu_options=gpu_options))\n\n\"\"\"Then build the neural network tensor graph, For example,\"\"\"\n\"\"\"we define an neural network to compute output layer tensor a.\"\"\"\n\n# input tensor is an numpy array with dimension 32 * 336 * 84.\ndict = {input_tensor: input_tensor}\nfor _ in 1000000:\n     a.eval(session=session, feed_dict=dict)\n</code></pre>\n<p>In this code, the running time for each <code>a.eval()</code> gradually got larger and larger as iteration time goes by. So I used Profiling tooI (CProfile) to dig into the code to see at which part of code performance going wrong, and it indicated that time consumption for executing <code>built-in method TF_Run</code> for running <code>a.eval()</code> was very long and kept growing. I am pretty sure the <code>input_tensor</code> size keeps exactly same for each iteration, I wonder if anyone can help me figure out the problem, and how I can improve my current performance. Thanks!!!</p>", "body_text": "I used Tensorflow 0.6 (I pip installed TensorFlow: pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl) with Python 3 to train Neural Network on GPU (Nvidia GeForce GTX Titan X with Cuda 7.0 and Cudnn 6.5), and I noticed an interesting problem: The time consumption for running tensor.eval(session=..., feed_dict=...) at each iteration was continuously growing as iteration goes by, which indicated that performance of tensor.eval() decayed. Specifically, the code looks like this:\nimport tensorflow as tf\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\nsession = tf.Session(config=tensorflow.ConfigProto(gpu_options=gpu_options))\n\n\"\"\"Then build the neural network tensor graph, For example,\"\"\"\n\"\"\"we define an neural network to compute output layer tensor a.\"\"\"\n\n# input tensor is an numpy array with dimension 32 * 336 * 84.\ndict = {input_tensor: input_tensor}\nfor _ in 1000000:\n     a.eval(session=session, feed_dict=dict)\n\nIn this code, the running time for each a.eval() gradually got larger and larger as iteration time goes by. So I used Profiling tooI (CProfile) to dig into the code to see at which part of code performance going wrong, and it indicated that time consumption for executing built-in method TF_Run for running a.eval() was very long and kept growing. I am pretty sure the input_tensor size keeps exactly same for each iteration, I wonder if anyone can help me figure out the problem, and how I can improve my current performance. Thanks!!!", "body": "I used Tensorflow 0.6 (I pip installed TensorFlow: `pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl`) with Python 3 to train Neural Network on GPU (Nvidia GeForce GTX Titan X with Cuda 7.0 and Cudnn 6.5), and I noticed an interesting problem: The time consumption for running `tensor.eval(session=..., feed_dict=...)` at each iteration was continuously growing as iteration goes by, which indicated that performance of tensor.eval() decayed. Specifically, the code looks like this:\n\n```\nimport tensorflow as tf\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\nsession = tf.Session(config=tensorflow.ConfigProto(gpu_options=gpu_options))\n\n\"\"\"Then build the neural network tensor graph, For example,\"\"\"\n\"\"\"we define an neural network to compute output layer tensor a.\"\"\"\n\n# input tensor is an numpy array with dimension 32 * 336 * 84.\ndict = {input_tensor: input_tensor}\nfor _ in 1000000:\n     a.eval(session=session, feed_dict=dict)\n```\n\nIn this code, the running time for each `a.eval()` gradually got larger and larger as iteration time goes by. So I used Profiling tooI (CProfile) to dig into the code to see at which part of code performance going wrong, and it indicated that time consumption for executing `built-in method TF_Run` for running `a.eval()` was very long and kept growing. I am pretty sure the `input_tensor` size keeps exactly same for each iteration, I wonder if anyone can help me figure out the problem, and how I can improve my current performance. Thanks!!!\n"}