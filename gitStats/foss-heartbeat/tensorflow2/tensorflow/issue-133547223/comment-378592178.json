{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/378592178", "html_url": "https://github.com/tensorflow/tensorflow/issues/1095#issuecomment-378592178", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1095", "id": 378592178, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODU5MjE3OA==", "user": {"login": "xisnu", "id": 15844017, "node_id": "MDQ6VXNlcjE1ODQ0MDE3", "avatar_url": "https://avatars2.githubusercontent.com/u/15844017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xisnu", "html_url": "https://github.com/xisnu", "followers_url": "https://api.github.com/users/xisnu/followers", "following_url": "https://api.github.com/users/xisnu/following{/other_user}", "gists_url": "https://api.github.com/users/xisnu/gists{/gist_id}", "starred_url": "https://api.github.com/users/xisnu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xisnu/subscriptions", "organizations_url": "https://api.github.com/users/xisnu/orgs", "repos_url": "https://api.github.com/users/xisnu/repos", "events_url": "https://api.github.com/users/xisnu/events{/privacy}", "received_events_url": "https://api.github.com/users/xisnu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-04T13:03:51Z", "updated_at": "2018-04-04T13:03:51Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>def _MySquareGrad(op, grad):</p>\n</blockquote>\n<p>I was wondering what exactly this <strong>grad</strong> is doing. I was trying to design a custom mean squared error. My loss computation is</p>\n<pre><code>def mse_loss(logits,labels):\n    total=len(logits)\n    loss=0.0\n    #print(logits)\n    #print(labels)\n    for t in range(total):\n        s=0.0\n        for j in range(2):\n           s=s+(logits[t][j]-labels[t][j])**2\n        loss=loss+s\n    #loss=loss/float(total)\n    return loss\n</code></pre>\n<p>for this function I have defined a gradient function as</p>\n<pre><code>def _MeanSquareGrad(op, grad):\n    x = op.inputs[0]\n    y = op.inputs[1]\n    return grad * (x-y),grad*0.01\n</code></pre>\n<p>I called it in main tensorflow body as<br>\n<code>loss=py_func(mse_loss,[logits,y],tf.float64,grad=_MeanSquareGrad)</code><br>\nNow, the code is correctly finding the Loss and optimizer is also minimizing it. But I am not sure about</p>\n<ol>\n<li>What is the purpose of using <code> grad * (x-y)</code>. What is the value of grad?</li>\n<li>I have send a dummy <code>grad*0.01</code> value back as I am sending two inputs as in <code>py_func(mse_loss,[logits,y],tf.float64,grad=_MeanSquareGrad)</code>. Why it is asking for two returns from grad procedure</li>\n</ol>", "body_text": "def _MySquareGrad(op, grad):\n\nI was wondering what exactly this grad is doing. I was trying to design a custom mean squared error. My loss computation is\ndef mse_loss(logits,labels):\n    total=len(logits)\n    loss=0.0\n    #print(logits)\n    #print(labels)\n    for t in range(total):\n        s=0.0\n        for j in range(2):\n           s=s+(logits[t][j]-labels[t][j])**2\n        loss=loss+s\n    #loss=loss/float(total)\n    return loss\n\nfor this function I have defined a gradient function as\ndef _MeanSquareGrad(op, grad):\n    x = op.inputs[0]\n    y = op.inputs[1]\n    return grad * (x-y),grad*0.01\n\nI called it in main tensorflow body as\nloss=py_func(mse_loss,[logits,y],tf.float64,grad=_MeanSquareGrad)\nNow, the code is correctly finding the Loss and optimizer is also minimizing it. But I am not sure about\n\nWhat is the purpose of using  grad * (x-y). What is the value of grad?\nI have send a dummy grad*0.01 value back as I am sending two inputs as in py_func(mse_loss,[logits,y],tf.float64,grad=_MeanSquareGrad). Why it is asking for two returns from grad procedure", "body": "> def _MySquareGrad(op, grad):\r\n\r\nI was wondering what exactly this **grad** is doing. I was trying to design a custom mean squared error. My loss computation is\r\n```\r\ndef mse_loss(logits,labels):\r\n    total=len(logits)\r\n    loss=0.0\r\n    #print(logits)\r\n    #print(labels)\r\n    for t in range(total):\r\n        s=0.0\r\n        for j in range(2):\r\n           s=s+(logits[t][j]-labels[t][j])**2\r\n        loss=loss+s\r\n    #loss=loss/float(total)\r\n    return loss\r\n```\r\nfor this function I have defined a gradient function as\r\n```\r\ndef _MeanSquareGrad(op, grad):\r\n    x = op.inputs[0]\r\n    y = op.inputs[1]\r\n    return grad * (x-y),grad*0.01\r\n```\r\nI called it in main tensorflow body as\r\n`loss=py_func(mse_loss,[logits,y],tf.float64,grad=_MeanSquareGrad)`\r\nNow, the code is correctly finding the Loss and optimizer is also minimizing it. But I am not sure about\r\n\r\n1. What is the purpose of using ` grad * (x-y)`. What is the value of grad?\r\n2. I have send a dummy `grad*0.01` value back as I am sending two inputs as in `py_func(mse_loss,[logits,y],tf.float64,grad=_MeanSquareGrad)`. Why it is asking for two returns from grad procedure"}