{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/384059102", "html_url": "https://github.com/tensorflow/tensorflow/issues/1095#issuecomment-384059102", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1095", "id": 384059102, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDA1OTEwMg==", "user": {"login": "IFLED", "id": 9903398, "node_id": "MDQ6VXNlcjk5MDMzOTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/9903398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IFLED", "html_url": "https://github.com/IFLED", "followers_url": "https://api.github.com/users/IFLED/followers", "following_url": "https://api.github.com/users/IFLED/following{/other_user}", "gists_url": "https://api.github.com/users/IFLED/gists{/gist_id}", "starred_url": "https://api.github.com/users/IFLED/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IFLED/subscriptions", "organizations_url": "https://api.github.com/users/IFLED/orgs", "repos_url": "https://api.github.com/users/IFLED/repos", "events_url": "https://api.github.com/users/IFLED/events{/privacy}", "received_events_url": "https://api.github.com/users/IFLED/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-24T19:55:00Z", "updated_at": "2018-04-24T19:55:00Z", "author_association": "NONE", "body_html": "<ol>\n<li>if you call <code>tf.gradients(F, x)</code> (and F depends on <code>loss</code>) then <code>grad</code> will be derivative of <code>F</code> with respect to <code>loss</code>. For example you can take <code>F = tf.sqrt(loss)</code>, print <code>grad</code> with different values of labels or logits.</li>\n<li><code>mse_loss</code> is a function that depends on two inputs (<code>logits</code>, <code>labels</code>), so <code>grad</code> procedure should return derivatives with respect to both inputs.</li>\n</ol>", "body_text": "if you call tf.gradients(F, x) (and F depends on loss) then grad will be derivative of F with respect to loss. For example you can take F = tf.sqrt(loss), print grad with different values of labels or logits.\nmse_loss is a function that depends on two inputs (logits, labels), so grad procedure should return derivatives with respect to both inputs.", "body": "1. if you call `tf.gradients(F, x)` (and F depends on `loss`) then `grad` will be derivative of `F` with respect to `loss`. For example you can take `F = tf.sqrt(loss)`, print `grad` with different values of labels or logits.\r\n2. `mse_loss` is a function that depends on two inputs (`logits`, `labels`), so `grad` procedure should return derivatives with respect to both inputs."}