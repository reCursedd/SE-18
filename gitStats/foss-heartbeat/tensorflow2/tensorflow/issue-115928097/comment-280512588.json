{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280512588", "html_url": "https://github.com/tensorflow/tensorflow/issues/22#issuecomment-280512588", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22", "id": 280512588, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDUxMjU4OA==", "user": {"login": "lukeiwanski", "id": 8373795, "node_id": "MDQ6VXNlcjgzNzM3OTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/8373795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukeiwanski", "html_url": "https://github.com/lukeiwanski", "followers_url": "https://api.github.com/users/lukeiwanski/followers", "following_url": "https://api.github.com/users/lukeiwanski/following{/other_user}", "gists_url": "https://api.github.com/users/lukeiwanski/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukeiwanski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukeiwanski/subscriptions", "organizations_url": "https://api.github.com/users/lukeiwanski/orgs", "repos_url": "https://api.github.com/users/lukeiwanski/repos", "events_url": "https://api.github.com/users/lukeiwanski/events{/privacy}", "received_events_url": "https://api.github.com/users/lukeiwanski/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-17T00:36:44Z", "updated_at": "2017-02-28T17:54:33Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>What are the difference between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision?</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1710528\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bhack\">@bhack</a> That would be a great discussion to be had at yesterday's TensorFlow Dev Summit</p>\n<p>Do you ask about the resources available / type of programmers needed to contribute?</p>\n<p>If so, in OpenCL/SYCL approach C++ programmers / OpenCL C programmers can quickly be brought up to speed and be able to contribute. XLA approach requires a compiler / llvm experience.</p>\n<p>XLA is Google's internal project by extension they have more resouces associated with it. But, on the other hand their task is way bigger too.. Writing a compiler is not an easy task.</p>\n<p>Otherwise, if you are asking about the model:</p>\n<p>As I mentioned earlier in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115928097\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/22\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/22/hovercard?comment_id=272908870&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/22#issuecomment-272908870\">#22 (comment)</a> we are seeing both efforts as complementary approches and both having different use cases. I still stand with that statement.</p>\n<p>For instance <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5453737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tatatodd\">@tatatodd</a> in his presentation mentioned that some of the Ops will never have XLA as a target. I believe that we are possible to fill that gap.</p>\n<p>Other things to consider are new platforms. I will use mobile and embedded enviroment for this argument's sake as a new chips tend to pop out more frequently than GPUs ( the principle is the same ).</p>\n<p>If the semiconductor support SYCL / OpenCL you get TF support out of the box ( some performance tweaks might be required ).</p>\n<p>If the architecture is exotic and there is no LLVM backend for it yet XLA needs to add it ( that might not happen too often but still ). What happens more often is the architecture changes a bit and then new optimisation passes need to be added or existing one must be modified to gain the benefit. Tweaking kernel code is easier.</p>\n<p>I haven't looked very deep to XLA but I assume that XLA has to call into the CUDA API somehow to run the PTX kernel code, so would have to be ported to OpenCL or Vulkan to run SPIR-V kernels instead - that I assume would go through StreamExecutor - another framework to get familliar with - probably quite a big effort.</p>\n<p>In short, we are providing an unified / stable platform in very fragmented / diverted ecosystem that both semiconductor companies and developers can target. Where as XLA would have to commit to support.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6969686\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benoitsteiner\">@benoitsteiner</a> or <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a>  might be able to give more inside knowledge of XLA as I am working with a lot of assumptions / conclusions based on conversations.</p>\n<p>Oh, as well I have created slack channel to eas up communication <a href=\"https://tensorflowopencl.slack.com/shared_invite/MTQzNDQ0NzgzNzAyLTE0ODcyOTE1NjctMDZhM2RkODRlYg\" rel=\"nofollow\">https://tensorflowopencl.slack.com/shared_invite/MTQzNDQ0NzgzNzAyLTE0ODcyOTE1NjctMDZhM2RkODRlYg</a></p>\n<p>Eidt:<br>\nSlack link is no longer valid. Please ping me if you'd like to join.</p>", "body_text": "What are the difference between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision?\n\n@bhack That would be a great discussion to be had at yesterday's TensorFlow Dev Summit\nDo you ask about the resources available / type of programmers needed to contribute?\nIf so, in OpenCL/SYCL approach C++ programmers / OpenCL C programmers can quickly be brought up to speed and be able to contribute. XLA approach requires a compiler / llvm experience.\nXLA is Google's internal project by extension they have more resouces associated with it. But, on the other hand their task is way bigger too.. Writing a compiler is not an easy task.\nOtherwise, if you are asking about the model:\nAs I mentioned earlier in #22 (comment) we are seeing both efforts as complementary approches and both having different use cases. I still stand with that statement.\nFor instance @tatatodd in his presentation mentioned that some of the Ops will never have XLA as a target. I believe that we are possible to fill that gap.\nOther things to consider are new platforms. I will use mobile and embedded enviroment for this argument's sake as a new chips tend to pop out more frequently than GPUs ( the principle is the same ).\nIf the semiconductor support SYCL / OpenCL you get TF support out of the box ( some performance tweaks might be required ).\nIf the architecture is exotic and there is no LLVM backend for it yet XLA needs to add it ( that might not happen too often but still ). What happens more often is the architecture changes a bit and then new optimisation passes need to be added or existing one must be modified to gain the benefit. Tweaking kernel code is easier.\nI haven't looked very deep to XLA but I assume that XLA has to call into the CUDA API somehow to run the PTX kernel code, so would have to be ported to OpenCL or Vulkan to run SPIR-V kernels instead - that I assume would go through StreamExecutor - another framework to get familliar with - probably quite a big effort.\nIn short, we are providing an unified / stable platform in very fragmented / diverted ecosystem that both semiconductor companies and developers can target. Where as XLA would have to commit to support.\n@benoitsteiner or @drpngx  might be able to give more inside knowledge of XLA as I am working with a lot of assumptions / conclusions based on conversations.\nOh, as well I have created slack channel to eas up communication https://tensorflowopencl.slack.com/shared_invite/MTQzNDQ0NzgzNzAyLTE0ODcyOTE1NjctMDZhM2RkODRlYg\nEidt:\nSlack link is no longer valid. Please ping me if you'd like to join.", "body": ">What are the difference between the sycl development effort and XLA targetting SPIR-V other than PTX in the middle term vision?\r\n\r\n@bhack That would be a great discussion to be had at yesterday's TensorFlow Dev Summit\r\n\r\nDo you ask about the resources available / type of programmers needed to contribute?\r\n\r\nIf so, in OpenCL/SYCL approach C++ programmers / OpenCL C programmers can quickly be brought up to speed and be able to contribute. XLA approach requires a compiler / llvm experience.\r\n\r\nXLA is Google's internal project by extension they have more resouces associated with it. But, on the other hand their task is way bigger too.. Writing a compiler is not an easy task.\r\n\r\nOtherwise, if you are asking about the model:\r\n \r\nAs I mentioned earlier in https://github.com/tensorflow/tensorflow/issues/22#issuecomment-272908870 we are seeing both efforts as complementary approches and both having different use cases. I still stand with that statement.\r\n\r\nFor instance @tatatodd in his presentation mentioned that some of the Ops will never have XLA as a target. I believe that we are possible to fill that gap.\r\n\r\nOther things to consider are new platforms. I will use mobile and embedded enviroment for this argument's sake as a new chips tend to pop out more frequently than GPUs ( the principle is the same ).\r\n\r\nIf the semiconductor support SYCL / OpenCL you get TF support out of the box ( some performance tweaks might be required ). \r\n\r\nIf the architecture is exotic and there is no LLVM backend for it yet XLA needs to add it ( that might not happen too often but still ). What happens more often is the architecture changes a bit and then new optimisation passes need to be added or existing one must be modified to gain the benefit. Tweaking kernel code is easier.\r\n\r\nI haven't looked very deep to XLA but I assume that XLA has to call into the CUDA API somehow to run the PTX kernel code, so would have to be ported to OpenCL or Vulkan to run SPIR-V kernels instead - that I assume would go through StreamExecutor - another framework to get familliar with - probably quite a big effort.\r\n\r\nIn short, we are providing an unified / stable platform in very fragmented / diverted ecosystem that both semiconductor companies and developers can target. Where as XLA would have to commit to support.\r\n\r\n@benoitsteiner or @drpngx  might be able to give more inside knowledge of XLA as I am working with a lot of assumptions / conclusions based on conversations.\r\n\r\nOh, as well I have created slack channel to eas up communication https://tensorflowopencl.slack.com/shared_invite/MTQzNDQ0NzgzNzAyLTE0ODcyOTE1NjctMDZhM2RkODRlYg\r\n\r\nEidt:\r\nSlack link is no longer valid. Please ping me if you'd like to join."}