{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323440487", "html_url": "https://github.com/tensorflow/tensorflow/issues/22#issuecomment-323440487", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22", "id": 323440487, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzQ0MDQ4Nw==", "user": {"login": "choongng", "id": 386088, "node_id": "MDQ6VXNlcjM4NjA4OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/386088?v=4", "gravatar_id": "", "url": "https://api.github.com/users/choongng", "html_url": "https://github.com/choongng", "followers_url": "https://api.github.com/users/choongng/followers", "following_url": "https://api.github.com/users/choongng/following{/other_user}", "gists_url": "https://api.github.com/users/choongng/gists{/gist_id}", "starred_url": "https://api.github.com/users/choongng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/choongng/subscriptions", "organizations_url": "https://api.github.com/users/choongng/orgs", "repos_url": "https://api.github.com/users/choongng/repos", "events_url": "https://api.github.com/users/choongng/events{/privacy}", "received_events_url": "https://api.github.com/users/choongng/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-18T19:25:32Z", "updated_at": "2017-08-18T19:25:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8373795\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukeiwanski\">@lukeiwanski</a> What's your approach to benchmarking? We time the models included with Keras and normalize against TF+cuDNN+K80 because that is a common and well optimized configuration. Our methodology is similar to Max Woolf (<a href=\"http://minimaxir.com/2017/06/keras-cntk/\" rel=\"nofollow\">http://minimaxir.com/2017/06/keras-cntk/</a>), it's not much code but we'd be happy to share it. We have some throughput numbers on our web site (<a href=\"http://vertex.ai\" rel=\"nofollow\">http://vertex.ai</a>), our code is very slightly faster than TF 1.2 on Xception inference and it would be interesting to compare more approaches side-by-side.</p>", "body_text": "@lukeiwanski What's your approach to benchmarking? We time the models included with Keras and normalize against TF+cuDNN+K80 because that is a common and well optimized configuration. Our methodology is similar to Max Woolf (http://minimaxir.com/2017/06/keras-cntk/), it's not much code but we'd be happy to share it. We have some throughput numbers on our web site (http://vertex.ai), our code is very slightly faster than TF 1.2 on Xception inference and it would be interesting to compare more approaches side-by-side.", "body": "@lukeiwanski What's your approach to benchmarking? We time the models included with Keras and normalize against TF+cuDNN+K80 because that is a common and well optimized configuration. Our methodology is similar to Max Woolf (http://minimaxir.com/2017/06/keras-cntk/), it's not much code but we'd be happy to share it. We have some throughput numbers on our web site (http://vertex.ai), our code is very slightly faster than TF 1.2 on Xception inference and it would be interesting to compare more approaches side-by-side."}