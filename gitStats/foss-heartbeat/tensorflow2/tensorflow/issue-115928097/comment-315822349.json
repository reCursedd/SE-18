{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/315822349", "html_url": "https://github.com/tensorflow/tensorflow/issues/22#issuecomment-315822349", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22", "id": 315822349, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTgyMjM0OQ==", "user": {"login": "rogerpasky", "id": 3244030, "node_id": "MDQ6VXNlcjMyNDQwMzA=", "avatar_url": "https://avatars3.githubusercontent.com/u/3244030?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rogerpasky", "html_url": "https://github.com/rogerpasky", "followers_url": "https://api.github.com/users/rogerpasky/followers", "following_url": "https://api.github.com/users/rogerpasky/following{/other_user}", "gists_url": "https://api.github.com/users/rogerpasky/gists{/gist_id}", "starred_url": "https://api.github.com/users/rogerpasky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rogerpasky/subscriptions", "organizations_url": "https://api.github.com/users/rogerpasky/orgs", "repos_url": "https://api.github.com/users/rogerpasky/repos", "events_url": "https://api.github.com/users/rogerpasky/events{/privacy}", "received_events_url": "https://api.github.com/users/rogerpasky/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-17T17:24:56Z", "updated_at": "2017-07-17T17:24:56Z", "author_association": "NONE", "body_html": "<p>Apple is solely aimed to sell Apple devices. Google is aimed to hire Google massive services.</p>\n<p>If you're willing to do AI (learning) with one single device, like one Apple Laptop, you'll do \"Superficial Learning\" instead of \"Deep Learning\", so you'd better give up doing anything but tutorials. Inference results in a trained model for one single user, in a single device (even in a not-too-many-multicore phone), could be neat to be done through GPUs, but is perfectly doable only with CPUs.</p>\n<p>On the other side, GPUs are absolutely needed if you're gonna feed extremely large datasets for learning or you're gonna serve trained inference to extremely large concurrent customer groups.</p>\n<p>Even though, doing it in such scale, is not that easy due to the network issues. Just have a look to the TPU-Pods physical architecture. It's in the antipode of a laptop (several GPUs per memory-overloaded multi-core server, with dedicated optical-fiber for inter-server communications).</p>\n<p>I have a MacBook Pro. It's a nice terminal to get to the cloud :-D</p>", "body_text": "Apple is solely aimed to sell Apple devices. Google is aimed to hire Google massive services.\nIf you're willing to do AI (learning) with one single device, like one Apple Laptop, you'll do \"Superficial Learning\" instead of \"Deep Learning\", so you'd better give up doing anything but tutorials. Inference results in a trained model for one single user, in a single device (even in a not-too-many-multicore phone), could be neat to be done through GPUs, but is perfectly doable only with CPUs.\nOn the other side, GPUs are absolutely needed if you're gonna feed extremely large datasets for learning or you're gonna serve trained inference to extremely large concurrent customer groups.\nEven though, doing it in such scale, is not that easy due to the network issues. Just have a look to the TPU-Pods physical architecture. It's in the antipode of a laptop (several GPUs per memory-overloaded multi-core server, with dedicated optical-fiber for inter-server communications).\nI have a MacBook Pro. It's a nice terminal to get to the cloud :-D", "body": "Apple is solely aimed to sell Apple devices. Google is aimed to hire Google massive services. \r\n\r\nIf you're willing to do AI (learning) with one single device, like one Apple Laptop, you'll do \"Superficial Learning\" instead of \"Deep Learning\", so you'd better give up doing anything but tutorials. Inference results in a trained model for one single user, in a single device (even in a not-too-many-multicore phone), could be neat to be done through GPUs, but is perfectly doable only with CPUs.\r\n\r\nOn the other side, GPUs are absolutely needed if you're gonna feed extremely large datasets for learning or you're gonna serve trained inference to extremely large concurrent customer groups.\r\n\r\nEven though, doing it in such scale, is not that easy due to the network issues. Just have a look to the TPU-Pods physical architecture. It's in the antipode of a laptop (several GPUs per memory-overloaded multi-core server, with dedicated optical-fiber for inter-server communications).\r\n\r\nI have a MacBook Pro. It's a nice terminal to get to the cloud :-D"}