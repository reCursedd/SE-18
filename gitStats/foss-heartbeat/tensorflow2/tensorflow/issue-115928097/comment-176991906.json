{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/176991906", "html_url": "https://github.com/tensorflow/tensorflow/issues/22#issuecomment-176991906", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22", "id": 176991906, "node_id": "MDEyOklzc3VlQ29tbWVudDE3Njk5MTkwNg==", "user": {"login": "keryell", "id": 1821746, "node_id": "MDQ6VXNlcjE4MjE3NDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1821746?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keryell", "html_url": "https://github.com/keryell", "followers_url": "https://api.github.com/users/keryell/followers", "following_url": "https://api.github.com/users/keryell/following{/other_user}", "gists_url": "https://api.github.com/users/keryell/gists{/gist_id}", "starred_url": "https://api.github.com/users/keryell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keryell/subscriptions", "organizations_url": "https://api.github.com/users/keryell/orgs", "repos_url": "https://api.github.com/users/keryell/repos", "events_url": "https://api.github.com/users/keryell/events{/privacy}", "received_events_url": "https://api.github.com/users/keryell/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-29T22:11:15Z", "updated_at": "2016-01-29T22:11:15Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3027176\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/karlrupp\">@karlrupp</a></p>\n<blockquote>\n<p>Is this a copy&amp;paste from OpenCL 1.0 marketing, which claimed exactly the same?</p>\n</blockquote>\n<p>Haha. :-)</p>\n<blockquote>\n<p>You will always need to go down to the details of the underlying hardware if you aim for maximum performance. This is especially the case in the context of fast tensor contractions.</p>\n</blockquote>\n<p>Of course, but before playing with the second-order optimization, it is useful to have the huge part of the whole templated C++ code running in some accelerated way.</p>\n<p>For the optimization, either you stitch your optimized binary kernels \u00e0 la NervanaSys or, since SYCL is pure C++, you can use asm(\"...\") in it with a lot of #ifdef to test the target architecture. :-) That said, SPIR-V is itself extensible and I cannot see why we could not put inline VHDL or Verilog in it at some point. :-)</p>\n<p>But more concretely, the recent introduction of sub-group operations should help to achieve good performance in a portable way and using simple built-in ad-hoc functions may help.</p>\n<p>C++ adds interesting metaprogramming features that allows to replace most of the code generators used such as in clBLAS or other frameworks to generate code more adapted to X or Y hardware.</p>", "body_text": "@karlrupp\n\nIs this a copy&paste from OpenCL 1.0 marketing, which claimed exactly the same?\n\nHaha. :-)\n\nYou will always need to go down to the details of the underlying hardware if you aim for maximum performance. This is especially the case in the context of fast tensor contractions.\n\nOf course, but before playing with the second-order optimization, it is useful to have the huge part of the whole templated C++ code running in some accelerated way.\nFor the optimization, either you stitch your optimized binary kernels \u00e0 la NervanaSys or, since SYCL is pure C++, you can use asm(\"...\") in it with a lot of #ifdef to test the target architecture. :-) That said, SPIR-V is itself extensible and I cannot see why we could not put inline VHDL or Verilog in it at some point. :-)\nBut more concretely, the recent introduction of sub-group operations should help to achieve good performance in a portable way and using simple built-in ad-hoc functions may help.\nC++ adds interesting metaprogramming features that allows to replace most of the code generators used such as in clBLAS or other frameworks to generate code more adapted to X or Y hardware.", "body": "@karlrupp\n\n> Is this a copy&paste from OpenCL 1.0 marketing, which claimed exactly the same?\n\nHaha. :-)\n\n> You will always need to go down to the details of the underlying hardware if you aim for maximum performance. This is especially the case in the context of fast tensor contractions.\n\nOf course, but before playing with the second-order optimization, it is useful to have the huge part of the whole templated C++ code running in some accelerated way.\n\nFor the optimization, either you stitch your optimized binary kernels \u00e0 la NervanaSys or, since SYCL is pure C++, you can use asm(\"...\") in it with a lot of #ifdef to test the target architecture. :-) That said, SPIR-V is itself extensible and I cannot see why we could not put inline VHDL or Verilog in it at some point. :-)\n\nBut more concretely, the recent introduction of sub-group operations should help to achieve good performance in a portable way and using simple built-in ad-hoc functions may help.\n\nC++ adds interesting metaprogramming features that allows to replace most of the code generators used such as in clBLAS or other frameworks to generate code more adapted to X or Y hardware.\n"}