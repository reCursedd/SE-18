{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/173058565", "html_url": "https://github.com/tensorflow/tensorflow/pull/662#issuecomment-173058565", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/662", "id": 173058565, "node_id": "MDEyOklzc3VlQ29tbWVudDE3MzA1ODU2NQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-20T01:52:17Z", "updated_at": "2016-01-20T01:52:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Sorry for the delay in getting back to you! Looking at your PR more closely, it seems like I misunderstood the nature of your change (and the nature of <code>slice_input_producer()</code>, which I had mistakenly assumed produces batches rather than single elements...). I'm still a bit unclear about why your change makes things more efficient: as far as I can tell, the change will end up copying more data out of <code>input_tensor</code> on each iteration, which is potentially wasteful. Do you have details of a benchmark that shows the improvement?</p>\n<p>Looking at the implementation, it also looks like the <code>array_ops.gather()</code> is the only thing that your change avoids, but at the expense of copying the input tensor into the queue repeatedly, which could potentially lead to a performance regression. It seems like you could get almost all of the benefit by replacing the <code>array_ops.gather()</code> with an <code>array_ops.slice()</code>, which in many cases could avoid the copy altogether.</p>\n<p>Does that make sense?</p>", "body_text": "Sorry for the delay in getting back to you! Looking at your PR more closely, it seems like I misunderstood the nature of your change (and the nature of slice_input_producer(), which I had mistakenly assumed produces batches rather than single elements...). I'm still a bit unclear about why your change makes things more efficient: as far as I can tell, the change will end up copying more data out of input_tensor on each iteration, which is potentially wasteful. Do you have details of a benchmark that shows the improvement?\nLooking at the implementation, it also looks like the array_ops.gather() is the only thing that your change avoids, but at the expense of copying the input tensor into the queue repeatedly, which could potentially lead to a performance regression. It seems like you could get almost all of the benefit by replacing the array_ops.gather() with an array_ops.slice(), which in many cases could avoid the copy altogether.\nDoes that make sense?", "body": "Sorry for the delay in getting back to you! Looking at your PR more closely, it seems like I misunderstood the nature of your change (and the nature of `slice_input_producer()`, which I had mistakenly assumed produces batches rather than single elements...). I'm still a bit unclear about why your change makes things more efficient: as far as I can tell, the change will end up copying more data out of `input_tensor` on each iteration, which is potentially wasteful. Do you have details of a benchmark that shows the improvement?\n\nLooking at the implementation, it also looks like the `array_ops.gather()` is the only thing that your change avoids, but at the expense of copying the input tensor into the queue repeatedly, which could potentially lead to a performance regression. It seems like you could get almost all of the benefit by replacing the `array_ops.gather()` with an `array_ops.slice()`, which in many cases could avoid the copy altogether.\n\nDoes that make sense?\n"}