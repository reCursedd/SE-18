{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/292106134", "html_url": "https://github.com/tensorflow/tensorflow/issues/9014#issuecomment-292106134", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9014", "id": 292106134, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MjEwNjEzNA==", "user": {"login": "berset", "id": 1536596, "node_id": "MDQ6VXNlcjE1MzY1OTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1536596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/berset", "html_url": "https://github.com/berset", "followers_url": "https://api.github.com/users/berset/followers", "following_url": "https://api.github.com/users/berset/following{/other_user}", "gists_url": "https://api.github.com/users/berset/gists{/gist_id}", "starred_url": "https://api.github.com/users/berset/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/berset/subscriptions", "organizations_url": "https://api.github.com/users/berset/orgs", "repos_url": "https://api.github.com/users/berset/repos", "events_url": "https://api.github.com/users/berset/events{/privacy}", "received_events_url": "https://api.github.com/users/berset/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-06T08:34:06Z", "updated_at": "2017-04-06T08:34:06Z", "author_association": "NONE", "body_html": "<p>ok, so I do the following (per <a href=\"https://www.tensorflow.org/performance/xla/tfcompile\" rel=\"nofollow\">https://www.tensorflow.org/performance/xla/tfcompile</a>):</p>\n<pre><code>python  tensorflow/compiler/aot/tests/make_test_graphs.py  --out_dir=.\ncat &gt; myfile.pbtxt &lt;&lt;EOT\n# Each feed is a positional input argument for the generated function.  The order\n# of each entry matches the order of each input argument.  Here \u201cx_hold\u201d and \u201cy_hold\u201d\n# refer to the names of placeholder nodes defined in the graph.\nfeed {\n  id { node_name: \"x_hold\" }\n  shape {\n    dim { size: 2 }\n    dim { size: 3 }\n  }\n}\nfeed {\n  id { node_name: \"y_hold\" }\n  shape {\n    dim { size: 3 }\n    dim { size: 2 }\n  }\n}\n\n# Each fetch is a positional output argument for the generated function.  The order\n# of each entry matches the order of each output argument.  Here \u201cx_y_prod\u201d\n# refers to the name of a matmul node defined in the graph.\nfetch {\n  id { node_name: \"x_y_prod\" }\n}\nEOT\nbazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_tfmatmul.pb --config=myfile.pbtxt\n</code></pre>\n<p>and I get the same result:</p>\n<pre><code>root@tfcompile-1266760932-s2dht:/tensorflow# bazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_tfmatmul.pb --config=myfile.pbtxt\n2017-04-06 08:33:28.828508: F tensorflow/compiler/aot/tfcompile_main.cc:136] Check failed: argc == 1 &amp;&amp; !flags.config.empty() &amp;&amp; (flags.dump_fetch_nodes || (!flags.graph.empty() &amp;&amp; !flags.entry_point.empty())) \ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\nresulting in an object file compiled for your target architecture, and a\nheader file that gives access to the functionality in the object file.\nA typical invocation looks like this:\n\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt\n\nusage: bazel-bin/tensorflow/compiler/aot/tfcompile\nFlags:\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\n\t--debug_dir=\"\"                   \tstring\tSpecifies a directory to dump debugging information, including rewritten graphs and the XLA HLO module.\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is &lt;arch&gt;&lt;sub&gt;-&lt;vendor&gt;-&lt;sys&gt;-&lt;abi&gt;.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\n\t--entry_point=\"\"                 \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[&lt;optional_namespace&gt;::],...]&lt;class_name&gt;.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\n\t--out_object=\"out.o\"             \tstring\tOutput object file name.\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\n\t--xla_debug_cpu_dump_ir=\"\"       \tstring\tDump IR, before optimizations to a path\n\t--xla_cpu_llvm_opt_level=2       \tint32\tThe LLVM optimization level for the CPU XLA backend. Valid range is from 0 to 3 where 0 means no optimizations.\n\t--xla_cpu_llvm_cl_opts=\"\"        \tstring\tComma-separated list of command line options to pass to LLVM.\n\t--xla_cpu_embed_ir=false         \tbool\tEmbed the LLVM IR module string in the resultant CpuExecutable.\n\t--xla_cpu_parallel=false         \tbool\tUse the multi-threaded CPU backend.\n\t--xla_cpu_use_eigen=true         \tbool\tUse Eigen for matrix multiply on the CPU platform. This is a useful hack for performance comparisons against XLA's implementation.\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen for matmul and conv, should single or multi-threaded eigen be used? Only used when --xla_cpu_use_eigen is true.\n\nAborted (core dumped)\nroot@tfcompile-1266760932-s2dht:/tensorflow# \n</code></pre>", "body_text": "ok, so I do the following (per https://www.tensorflow.org/performance/xla/tfcompile):\npython  tensorflow/compiler/aot/tests/make_test_graphs.py  --out_dir=.\ncat > myfile.pbtxt <<EOT\n# Each feed is a positional input argument for the generated function.  The order\n# of each entry matches the order of each input argument.  Here \u201cx_hold\u201d and \u201cy_hold\u201d\n# refer to the names of placeholder nodes defined in the graph.\nfeed {\n  id { node_name: \"x_hold\" }\n  shape {\n    dim { size: 2 }\n    dim { size: 3 }\n  }\n}\nfeed {\n  id { node_name: \"y_hold\" }\n  shape {\n    dim { size: 3 }\n    dim { size: 2 }\n  }\n}\n\n# Each fetch is a positional output argument for the generated function.  The order\n# of each entry matches the order of each output argument.  Here \u201cx_y_prod\u201d\n# refers to the name of a matmul node defined in the graph.\nfetch {\n  id { node_name: \"x_y_prod\" }\n}\nEOT\nbazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_tfmatmul.pb --config=myfile.pbtxt\n\nand I get the same result:\nroot@tfcompile-1266760932-s2dht:/tensorflow# bazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_tfmatmul.pb --config=myfile.pbtxt\n2017-04-06 08:33:28.828508: F tensorflow/compiler/aot/tfcompile_main.cc:136] Check failed: argc == 1 && !flags.config.empty() && (flags.dump_fetch_nodes || (!flags.graph.empty() && !flags.entry_point.empty())) \ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\nresulting in an object file compiled for your target architecture, and a\nheader file that gives access to the functionality in the object file.\nA typical invocation looks like this:\n\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt\n\nusage: bazel-bin/tensorflow/compiler/aot/tfcompile\nFlags:\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\n\t--debug_dir=\"\"                   \tstring\tSpecifies a directory to dump debugging information, including rewritten graphs and the XLA HLO module.\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is <arch><sub>-<vendor>-<sys>-<abi>.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\n\t--entry_point=\"\"                 \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[<optional_namespace>::],...]<class_name>.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\n\t--out_object=\"out.o\"             \tstring\tOutput object file name.\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\n\t--xla_debug_cpu_dump_ir=\"\"       \tstring\tDump IR, before optimizations to a path\n\t--xla_cpu_llvm_opt_level=2       \tint32\tThe LLVM optimization level for the CPU XLA backend. Valid range is from 0 to 3 where 0 means no optimizations.\n\t--xla_cpu_llvm_cl_opts=\"\"        \tstring\tComma-separated list of command line options to pass to LLVM.\n\t--xla_cpu_embed_ir=false         \tbool\tEmbed the LLVM IR module string in the resultant CpuExecutable.\n\t--xla_cpu_parallel=false         \tbool\tUse the multi-threaded CPU backend.\n\t--xla_cpu_use_eigen=true         \tbool\tUse Eigen for matrix multiply on the CPU platform. This is a useful hack for performance comparisons against XLA's implementation.\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen for matmul and conv, should single or multi-threaded eigen be used? Only used when --xla_cpu_use_eigen is true.\n\nAborted (core dumped)\nroot@tfcompile-1266760932-s2dht:/tensorflow#", "body": "ok, so I do the following (per https://www.tensorflow.org/performance/xla/tfcompile):\r\n\r\n```\r\npython  tensorflow/compiler/aot/tests/make_test_graphs.py  --out_dir=.\r\ncat > myfile.pbtxt <<EOT\r\n# Each feed is a positional input argument for the generated function.  The order\r\n# of each entry matches the order of each input argument.  Here \u201cx_hold\u201d and \u201cy_hold\u201d\r\n# refer to the names of placeholder nodes defined in the graph.\r\nfeed {\r\n  id { node_name: \"x_hold\" }\r\n  shape {\r\n    dim { size: 2 }\r\n    dim { size: 3 }\r\n  }\r\n}\r\nfeed {\r\n  id { node_name: \"y_hold\" }\r\n  shape {\r\n    dim { size: 3 }\r\n    dim { size: 2 }\r\n  }\r\n}\r\n\r\n# Each fetch is a positional output argument for the generated function.  The order\r\n# of each entry matches the order of each output argument.  Here \u201cx_y_prod\u201d\r\n# refers to the name of a matmul node defined in the graph.\r\nfetch {\r\n  id { node_name: \"x_y_prod\" }\r\n}\r\nEOT\r\nbazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_tfmatmul.pb --config=myfile.pbtxt\r\n```\r\nand I get the same result:\r\n```\r\nroot@tfcompile-1266760932-s2dht:/tensorflow# bazel-bin/tensorflow/compiler/aot/tfcompile --graph=test_graph_tfmatmul.pb --config=myfile.pbtxt\r\n2017-04-06 08:33:28.828508: F tensorflow/compiler/aot/tfcompile_main.cc:136] Check failed: argc == 1 && !flags.config.empty() && (flags.dump_fetch_nodes || (!flags.graph.empty() && !flags.entry_point.empty())) \r\ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\r\nresulting in an object file compiled for your target architecture, and a\r\nheader file that gives access to the functionality in the object file.\r\nA typical invocation looks like this:\r\n\r\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt\r\n\r\nusage: bazel-bin/tensorflow/compiler/aot/tfcompile\r\nFlags:\r\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\r\n\t--debug_dir=\"\"                   \tstring\tSpecifies a directory to dump debugging information, including rewritten graphs and the XLA HLO module.\r\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is <arch><sub>-<vendor>-<sys>-<abi>.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\r\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\r\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\r\n\t--entry_point=\"\"                 \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\r\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[<optional_namespace>::],...]<class_name>.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\r\n\t--out_object=\"out.o\"             \tstring\tOutput object file name.\r\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\r\n\t--xla_debug_cpu_dump_ir=\"\"       \tstring\tDump IR, before optimizations to a path\r\n\t--xla_cpu_llvm_opt_level=2       \tint32\tThe LLVM optimization level for the CPU XLA backend. Valid range is from 0 to 3 where 0 means no optimizations.\r\n\t--xla_cpu_llvm_cl_opts=\"\"        \tstring\tComma-separated list of command line options to pass to LLVM.\r\n\t--xla_cpu_embed_ir=false         \tbool\tEmbed the LLVM IR module string in the resultant CpuExecutable.\r\n\t--xla_cpu_parallel=false         \tbool\tUse the multi-threaded CPU backend.\r\n\t--xla_cpu_use_eigen=true         \tbool\tUse Eigen for matrix multiply on the CPU platform. This is a useful hack for performance comparisons against XLA's implementation.\r\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen for matmul and conv, should single or multi-threaded eigen be used? Only used when --xla_cpu_use_eigen is true.\r\n\r\nAborted (core dumped)\r\nroot@tfcompile-1266760932-s2dht:/tensorflow# \r\n```"}