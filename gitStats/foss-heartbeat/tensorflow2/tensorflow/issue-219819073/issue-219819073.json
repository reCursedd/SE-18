{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9014", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9014/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9014/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9014/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9014", "id": 219819073, "node_id": "MDU6SXNzdWUyMTk4MTkwNzM=", "number": 9014, "title": "Freshly built tfcompile core dumps", "user": {"login": "berset", "id": 1536596, "node_id": "MDQ6VXNlcjE1MzY1OTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1536596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/berset", "html_url": "https://github.com/berset", "followers_url": "https://api.github.com/users/berset/followers", "following_url": "https://api.github.com/users/berset/following{/other_user}", "gists_url": "https://api.github.com/users/berset/gists{/gist_id}", "starred_url": "https://api.github.com/users/berset/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/berset/subscriptions", "organizations_url": "https://api.github.com/users/berset/orgs", "repos_url": "https://api.github.com/users/berset/repos", "events_url": "https://api.github.com/users/berset/events{/privacy}", "received_events_url": "https://api.github.com/users/berset/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-04-06T08:13:06Z", "updated_at": "2017-10-04T00:04:35Z", "closed_at": "2017-04-06T08:25:13Z", "author_association": "NONE", "body_html": "<p>NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.</p>\n<h3>You must complete this information or else your issue will be closed</h3>\n<ul>\n<li><em>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?</em>: No</li>\n<li><em>TensorFlow installed from (source or binary)?</em>: Source</li>\n<li><em>TensorFlow version</em>: docker image: \"tensorflow/tensorflow:nightly-devel\" - <code>git describe</code> = 0.6.0-16017-ga9b7946 (aka: <code>docker-pullable://tensorflow/tensorflow@sha256:5b568f7dd9890bb0b86101bb27779c539f608724c7d4ecf4fdfbbab289bc29de</code>)</li>\n<li><em>Bazel version (if compiling from source)</em>: Build label: 0.4.5</li>\n<li><em>CUDA/cuDNN version</em>: -</li>\n<li><em>GPU Model and Memory</em>: -</li>\n<li><em>Exact command to reproduce</em>: cd /tensorflow; bazel build tensorflow/compiler/aot:tfcompile ; bazel-bin/tensorflow/compiler/aot/tfcompile</li>\n</ul>\n<h3>Describe the problem clearly</h3>\n<p>I'm trying to use tfcompile, when I build it in a Pod on my kubernetes cluster, the resulting binary exits with <code>Aborted (core dumped)</code> after displaying the help message.</p>\n<h3>Source Code / Logs</h3>\n<p>I create a pod like this:</p>\n<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: tfcompile\n  name: tfcompile\nspec:\n  containers:\n  - args:\n    - sh\n    - -c\n    - cd /tensorflow; bazel build tensorflow/compiler/aot:tfcompile ; sleep 864000\n    image: tensorflow/tensorflow:nightly-devel\n    name: tfcompile\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 16Gi\n      requests:\n        cpu: \"1\"\n        memory: 16Gi\n</code></pre>\n<p>and then when the build is finished, I execute into the pod and try the resulting binary: <code>bazel-bin/tensorflow/compiler/aot/tfcompile</code></p>\n<p>full output:</p>\n<pre><code>root@tfcompile-1266760932-s2dht:/tensorflow# bazel-bin/tensorflow/compiler/aot/tfcompile                                         \n2017-04-06 08:08:02.798285: F tensorflow/compiler/aot/tfcompile_main.cc:136] Check failed: argc == 1 &amp;&amp; !flags.config.empty() &amp;&amp; (flags.dump_fetch_nodes || (!flags.graph.empty() &amp;&amp; !flags.entry_point.empty())) \ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\nresulting in an object file compiled for your target architecture, and a\nheader file that gives access to the functionality in the object file.\nA typical invocation looks like this:\n\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt\n\nusage: bazel-bin/tensorflow/compiler/aot/tfcompile\nFlags:\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\n\t--debug_dir=\"\"                   \tstring\tSpecifies a directory to dump debugging information, including rewritten graphs and the XLA HLO module.\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is &lt;arch&gt;&lt;sub&gt;-&lt;vendor&gt;-&lt;sys&gt;-&lt;abi&gt;.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\n\t--entry_point=\"\"                 \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[&lt;optional_namespace&gt;::],...]&lt;class_name&gt;.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\n\t--out_object=\"out.o\"             \tstring\tOutput object file name.\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\n\t--xla_debug_cpu_dump_ir=\"\"       \tstring\tDump IR, before optimizations to a path\n\t--xla_cpu_llvm_opt_level=2       \tint32\tThe LLVM optimization level for the CPU XLA backend. Valid range is from 0 to 3 where 0 means no optimizations.\n\t--xla_cpu_llvm_cl_opts=\"\"        \tstring\tComma-separated list of command line options to pass to LLVM.\n\t--xla_cpu_embed_ir=false         \tbool\tEmbed the LLVM IR module string in the resultant CpuExecutable.\n\t--xla_cpu_parallel=false         \tbool\tUse the multi-threaded CPU backend.\n\t--xla_cpu_use_eigen=true         \tbool\tUse Eigen for matrix multiply on the CPU platform. This is a useful hack for performance comparisons against XLA's implementation.\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen for matmul and conv, should single or multi-threaded eigen be used? Only used when --xla_cpu_use_eigen is true.\n\nAborted (core dumped)\nroot@tfcompile-1266760932-s2dht:/tensorflow#\n</code></pre>\n<p>I've actually been trying this sporadically with different nightly/release builds for the past few weeks, hoping that this would be resolved, but it still does not work, so here is my GH issue ;)</p>", "body_text": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\nYou must complete this information or else your issue will be closed\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow)?: No\nTensorFlow installed from (source or binary)?: Source\nTensorFlow version: docker image: \"tensorflow/tensorflow:nightly-devel\" - git describe = 0.6.0-16017-ga9b7946 (aka: docker-pullable://tensorflow/tensorflow@sha256:5b568f7dd9890bb0b86101bb27779c539f608724c7d4ecf4fdfbbab289bc29de)\nBazel version (if compiling from source): Build label: 0.4.5\nCUDA/cuDNN version: -\nGPU Model and Memory: -\nExact command to reproduce: cd /tensorflow; bazel build tensorflow/compiler/aot:tfcompile ; bazel-bin/tensorflow/compiler/aot/tfcompile\n\nDescribe the problem clearly\nI'm trying to use tfcompile, when I build it in a Pod on my kubernetes cluster, the resulting binary exits with Aborted (core dumped) after displaying the help message.\nSource Code / Logs\nI create a pod like this:\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: tfcompile\n  name: tfcompile\nspec:\n  containers:\n  - args:\n    - sh\n    - -c\n    - cd /tensorflow; bazel build tensorflow/compiler/aot:tfcompile ; sleep 864000\n    image: tensorflow/tensorflow:nightly-devel\n    name: tfcompile\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 16Gi\n      requests:\n        cpu: \"1\"\n        memory: 16Gi\n\nand then when the build is finished, I execute into the pod and try the resulting binary: bazel-bin/tensorflow/compiler/aot/tfcompile\nfull output:\nroot@tfcompile-1266760932-s2dht:/tensorflow# bazel-bin/tensorflow/compiler/aot/tfcompile                                         \n2017-04-06 08:08:02.798285: F tensorflow/compiler/aot/tfcompile_main.cc:136] Check failed: argc == 1 && !flags.config.empty() && (flags.dump_fetch_nodes || (!flags.graph.empty() && !flags.entry_point.empty())) \ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\nresulting in an object file compiled for your target architecture, and a\nheader file that gives access to the functionality in the object file.\nA typical invocation looks like this:\n\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt\n\nusage: bazel-bin/tensorflow/compiler/aot/tfcompile\nFlags:\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\n\t--debug_dir=\"\"                   \tstring\tSpecifies a directory to dump debugging information, including rewritten graphs and the XLA HLO module.\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is <arch><sub>-<vendor>-<sys>-<abi>.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\n\t--entry_point=\"\"                 \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[<optional_namespace>::],...]<class_name>.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\n\t--out_object=\"out.o\"             \tstring\tOutput object file name.\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\n\t--xla_debug_cpu_dump_ir=\"\"       \tstring\tDump IR, before optimizations to a path\n\t--xla_cpu_llvm_opt_level=2       \tint32\tThe LLVM optimization level for the CPU XLA backend. Valid range is from 0 to 3 where 0 means no optimizations.\n\t--xla_cpu_llvm_cl_opts=\"\"        \tstring\tComma-separated list of command line options to pass to LLVM.\n\t--xla_cpu_embed_ir=false         \tbool\tEmbed the LLVM IR module string in the resultant CpuExecutable.\n\t--xla_cpu_parallel=false         \tbool\tUse the multi-threaded CPU backend.\n\t--xla_cpu_use_eigen=true         \tbool\tUse Eigen for matrix multiply on the CPU platform. This is a useful hack for performance comparisons against XLA's implementation.\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen for matmul and conv, should single or multi-threaded eigen be used? Only used when --xla_cpu_use_eigen is true.\n\nAborted (core dumped)\nroot@tfcompile-1266760932-s2dht:/tensorflow#\n\nI've actually been trying this sporadically with different nightly/release builds for the past few weeks, hoping that this would be resolved, but it still does not work, so here is my GH issue ;)", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: No\r\n- *TensorFlow installed from (source or binary)?*: Source\r\n- *TensorFlow version*: docker image: \"tensorflow/tensorflow:nightly-devel\" - `git describe` = 0.6.0-16017-ga9b7946 (aka: `docker-pullable://tensorflow/tensorflow@sha256:5b568f7dd9890bb0b86101bb27779c539f608724c7d4ecf4fdfbbab289bc29de`)\r\n- *Bazel version (if compiling from source)*: Build label: 0.4.5\r\n- *CUDA/cuDNN version*: - \r\n- *GPU Model and Memory*: - \r\n- *Exact command to reproduce*: cd /tensorflow; bazel build tensorflow/compiler/aot:tfcompile ; bazel-bin/tensorflow/compiler/aot/tfcompile \r\n\r\n### Describe the problem clearly\r\nI'm trying to use tfcompile, when I build it in a Pod on my kubernetes cluster, the resulting binary exits with `Aborted (core dumped)` after displaying the help message.\r\n\r\n### Source Code / Logs\r\nI create a pod like this:\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  labels:\r\n    name: tfcompile\r\n  name: tfcompile\r\nspec:\r\n  containers:\r\n  - args:\r\n    - sh\r\n    - -c\r\n    - cd /tensorflow; bazel build tensorflow/compiler/aot:tfcompile ; sleep 864000\r\n    image: tensorflow/tensorflow:nightly-devel\r\n    name: tfcompile\r\n    resources:\r\n      limits:\r\n        cpu: \"1\"\r\n        memory: 16Gi\r\n      requests:\r\n        cpu: \"1\"\r\n        memory: 16Gi\r\n```\r\n\r\nand then when the build is finished, I execute into the pod and try the resulting binary: `bazel-bin/tensorflow/compiler/aot/tfcompile`\r\n\r\nfull output:\r\n\r\n```\r\nroot@tfcompile-1266760932-s2dht:/tensorflow# bazel-bin/tensorflow/compiler/aot/tfcompile                                         \r\n2017-04-06 08:08:02.798285: F tensorflow/compiler/aot/tfcompile_main.cc:136] Check failed: argc == 1 && !flags.config.empty() && (flags.dump_fetch_nodes || (!flags.graph.empty() && !flags.entry_point.empty())) \r\ntfcompile performs ahead-of-time compilation of a TensorFlow graph,\r\nresulting in an object file compiled for your target architecture, and a\r\nheader file that gives access to the functionality in the object file.\r\nA typical invocation looks like this:\r\n\r\n   $ tfcompile --graph=mygraph.pb --config=myfile.pbtxt\r\n\r\nusage: bazel-bin/tensorflow/compiler/aot/tfcompile\r\nFlags:\r\n\t--graph=\"\"                       \tstring\tInput GraphDef file.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--config=\"\"                      \tstring\tInput file containing Config proto.  If the file ends in '.pbtxt' it is expected to be in the human-readable proto text format, otherwise it is expected to be in the proto binary format.\r\n\t--dump_fetch_nodes=false         \tbool\tIf set, only flags related to fetches are processed, and the resulting fetch nodes will be dumped to stdout in a comma-separated list.  Typically used to format arguments for other tools, e.g. freeze_graph.\r\n\t--debug_dir=\"\"                   \tstring\tSpecifies a directory to dump debugging information, including rewritten graphs and the XLA HLO module.\r\n\t--target_triple=\"x86_64-pc-linux\"\tstring\tTarget platform, similar to the clang -target flag.  The general format is <arch><sub>-<vendor>-<sys>-<abi>.  http://clang.llvm.org/docs/CrossCompilation.html#target-triple.\r\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\r\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\r\n\t--entry_point=\"\"                 \tstring\tName of the generated function.  If multiple generated object files will be linked into the same binary, each will need a unique entry point.\r\n\t--cpp_class=\"\"                   \tstring\tName of the generated C++ class, wrapping the generated function.  The syntax of this flag is [[<optional_namespace>::],...]<class_name>.  This mirrors the C++ syntax for referring to a class, where multiple namespaces may precede the class name, separated by double-colons.  The class will be generated in the given namespace(s), or if no namespaces are given, within the global namespace.\r\n\t--out_object=\"out.o\"             \tstring\tOutput object file name.\r\n\t--out_header=\"out.h\"             \tstring\tOutput header file name.\r\n\t--xla_debug_cpu_dump_ir=\"\"       \tstring\tDump IR, before optimizations to a path\r\n\t--xla_cpu_llvm_opt_level=2       \tint32\tThe LLVM optimization level for the CPU XLA backend. Valid range is from 0 to 3 where 0 means no optimizations.\r\n\t--xla_cpu_llvm_cl_opts=\"\"        \tstring\tComma-separated list of command line options to pass to LLVM.\r\n\t--xla_cpu_embed_ir=false         \tbool\tEmbed the LLVM IR module string in the resultant CpuExecutable.\r\n\t--xla_cpu_parallel=false         \tbool\tUse the multi-threaded CPU backend.\r\n\t--xla_cpu_use_eigen=true         \tbool\tUse Eigen for matrix multiply on the CPU platform. This is a useful hack for performance comparisons against XLA's implementation.\r\n\t--xla_cpu_multi_thread_eigen=true\tbool\tWhen generating calls to Eigen for matmul and conv, should single or multi-threaded eigen be used? Only used when --xla_cpu_use_eigen is true.\r\n\r\nAborted (core dumped)\r\nroot@tfcompile-1266760932-s2dht:/tensorflow#\r\n```\r\n\r\nI've actually been trying this sporadically with different nightly/release builds for the past few weeks, hoping that this would be resolved, but it still does not work, so here is my GH issue ;) "}