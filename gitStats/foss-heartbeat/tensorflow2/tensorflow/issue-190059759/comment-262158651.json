{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/262158651", "html_url": "https://github.com/tensorflow/tensorflow/issues/5665#issuecomment-262158651", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5665", "id": 262158651, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MjE1ODY1MQ==", "user": {"login": "htt210", "id": 16357422, "node_id": "MDQ6VXNlcjE2MzU3NDIy", "avatar_url": "https://avatars1.githubusercontent.com/u/16357422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/htt210", "html_url": "https://github.com/htt210", "followers_url": "https://api.github.com/users/htt210/followers", "following_url": "https://api.github.com/users/htt210/following{/other_user}", "gists_url": "https://api.github.com/users/htt210/gists{/gist_id}", "starred_url": "https://api.github.com/users/htt210/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/htt210/subscriptions", "organizations_url": "https://api.github.com/users/htt210/orgs", "repos_url": "https://api.github.com/users/htt210/repos", "events_url": "https://api.github.com/users/htt210/events{/privacy}", "received_events_url": "https://api.github.com/users/htt210/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-22T06:27:40Z", "updated_at": "2016-11-22T06:34:16Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> ,<br>\nI think this is a bug in the code and you did not understand <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6788909\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Syndrome777\">@Syndrome777</a> 's question correctly. The code here does not implement any attention mechanism I know.</p>\n<pre><code>cell_output, state = cell(x, state) # cell_output has the size of batch_size x num_symbols while the correct shape is batch_size x hidden_size \nwith variable_scope.variable_scope(\"AttnOutputProjection\"):\n        output = linear([cell_output] + attns, output_size, True) # so the linear op here has the weight matrix of (num_symbols + attn_vec_size) x num_symbols. The correct size shoulde be (hidden_size + attn_vec_size) x num_symbols\n</code></pre>\n<p>The output of the cell() must be the hidden state, not the projection to the vocabulary. I think this bug was created because the attention model code was derived from the normal seq2seq code in which the output of the decoder is projected directly to the vocabulary space.</p>\n<p>I quick fixed it by removing the OutputProjectWrapper in embedding_attention_seq2seq method. But this is not a good fix as it cannot fix the case where <code>output_projection is not None</code>. I hope for better fix from tensorflow community.</p>\n<pre><code>\noutput_size = None\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n</code></pre>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L827\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L827</a></p>", "body_text": "Hi @girving ,\nI think this is a bug in the code and you did not understand @Syndrome777 's question correctly. The code here does not implement any attention mechanism I know.\ncell_output, state = cell(x, state) # cell_output has the size of batch_size x num_symbols while the correct shape is batch_size x hidden_size \nwith variable_scope.variable_scope(\"AttnOutputProjection\"):\n        output = linear([cell_output] + attns, output_size, True) # so the linear op here has the weight matrix of (num_symbols + attn_vec_size) x num_symbols. The correct size shoulde be (hidden_size + attn_vec_size) x num_symbols\n\nThe output of the cell() must be the hidden state, not the projection to the vocabulary. I think this bug was created because the attention model code was derived from the normal seq2seq code in which the output of the decoder is projected directly to the vocabulary space.\nI quick fixed it by removing the OutputProjectWrapper in embedding_attention_seq2seq method. But this is not a good fix as it cannot fix the case where output_projection is not None. I hope for better fix from tensorflow community.\n\noutput_size = None\n    if output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L827", "body": "Hi @girving ,\r\nI think this is a bug in the code and you did not understand @Syndrome777 's question correctly. The code here does not implement any attention mechanism I know. \r\n\r\n```\r\ncell_output, state = cell(x, state) # cell_output has the size of batch_size x num_symbols while the correct shape is batch_size x hidden_size \r\nwith variable_scope.variable_scope(\"AttnOutputProjection\"):\r\n        output = linear([cell_output] + attns, output_size, True) # so the linear op here has the weight matrix of (num_symbols + attn_vec_size) x num_symbols. The correct size shoulde be (hidden_size + attn_vec_size) x num_symbols\r\n```\r\nThe output of the cell() must be the hidden state, not the projection to the vocabulary. I think this bug was created because the attention model code was derived from the normal seq2seq code in which the output of the decoder is projected directly to the vocabulary space.\r\n\r\nI quick fixed it by removing the OutputProjectWrapper in embedding_attention_seq2seq method. But this is not a good fix as it cannot fix the case where `output_projection is not None`. I hope for better fix from tensorflow community.\r\n```\r\n\r\noutput_size = None\r\n    if output_projection is None:\r\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\r\n      output_size = num_decoder_symbols\r\n\r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L827\r\n\r\n\r\n"}