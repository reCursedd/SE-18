{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5665", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5665/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5665/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5665/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5665", "id": 190059759, "node_id": "MDU6SXNzdWUxOTAwNTk3NTk=", "number": 5665, "title": "Is there a bug in embedding_attention_seq2seq?", "user": {"login": "Syndrome777", "id": 6788909, "node_id": "MDQ6VXNlcjY3ODg5MDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/6788909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Syndrome777", "html_url": "https://github.com/Syndrome777", "followers_url": "https://api.github.com/users/Syndrome777/followers", "following_url": "https://api.github.com/users/Syndrome777/following{/other_user}", "gists_url": "https://api.github.com/users/Syndrome777/gists{/gist_id}", "starred_url": "https://api.github.com/users/Syndrome777/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Syndrome777/subscriptions", "organizations_url": "https://api.github.com/users/Syndrome777/orgs", "repos_url": "https://api.github.com/users/Syndrome777/repos", "events_url": "https://api.github.com/users/Syndrome777/events{/privacy}", "received_events_url": "https://api.github.com/users/Syndrome777/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-11-17T14:19:55Z", "updated_at": "2016-11-22T17:55:42Z", "closed_at": "2016-11-22T17:53:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, all,</p>\n<p>I think the code of embedding_attention_seq2seq is confusing. And I can't run this code.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> output_projection <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n      cell <span class=\"pl-k\">=</span> rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size <span class=\"pl-k\">=</span> num_decoder_symbols\n\n<span class=\"pl-c1\">...</span>\n\n      x <span class=\"pl-k\">=</span> linear([inp] <span class=\"pl-k\">+</span> attns, input_size, <span class=\"pl-c1\">True</span>)\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the RNN.</span>\n      cell_output, state <span class=\"pl-k\">=</span> cell(x, state)\n\n<span class=\"pl-c1\">...</span>\n\n<span class=\"pl-k\">with</span> variable_scope.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>AttnOutputProjection<span class=\"pl-pds\">\"</span></span>):\n        output <span class=\"pl-k\">=</span> linear([cell_output] <span class=\"pl-k\">+</span> attns, output_size, <span class=\"pl-c1\">True</span>)\n</pre></div>\n<p>I don't know what's the meaning of \"AttnOutputProjection\". The attention information has been used in \"x = linear([inp] + attns, input_size, True)\", why adding it again?</p>\n<p>And meanwhile what is the meaning of \"[cell_output] + attns\", \"cell_output\" is equal with num_symbols, so why add attens?</p>\n<p>In my experiment, the num_decoder_symbols is 10000 and the hidden size is 32, and I get a matrix 10032*10000 at \"AttnOutputProjection\". It will be out of memory.<br>\n10032 is a confusing number, I don't know the physical meaning of this number.</p>\n<p>So I don't know if it's a bug. If it's a bug, I'm really pleasure to make a PR.<br>\nThanks so much.</p>", "body_text": "Hi, all,\nI think the code of embedding_attention_seq2seq is confusing. And I can't run this code.\nif output_projection is None:\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n...\n\n      x = linear([inp] + attns, input_size, True)\n      # Run the RNN.\n      cell_output, state = cell(x, state)\n\n...\n\nwith variable_scope.variable_scope(\"AttnOutputProjection\"):\n        output = linear([cell_output] + attns, output_size, True)\n\nI don't know what's the meaning of \"AttnOutputProjection\". The attention information has been used in \"x = linear([inp] + attns, input_size, True)\", why adding it again?\nAnd meanwhile what is the meaning of \"[cell_output] + attns\", \"cell_output\" is equal with num_symbols, so why add attens?\nIn my experiment, the num_decoder_symbols is 10000 and the hidden size is 32, and I get a matrix 10032*10000 at \"AttnOutputProjection\". It will be out of memory.\n10032 is a confusing number, I don't know the physical meaning of this number.\nSo I don't know if it's a bug. If it's a bug, I'm really pleasure to make a PR.\nThanks so much.", "body": "Hi, all,\r\n\r\nI think the code of embedding_attention_seq2seq is confusing. And I can't run this code.\r\n\r\n```python\r\nif output_projection is None:\r\n      cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\r\n      output_size = num_decoder_symbols\r\n\r\n...\r\n\r\n      x = linear([inp] + attns, input_size, True)\r\n      # Run the RNN.\r\n      cell_output, state = cell(x, state)\r\n\r\n...\r\n\r\nwith variable_scope.variable_scope(\"AttnOutputProjection\"):\r\n        output = linear([cell_output] + attns, output_size, True)\r\n\r\n```\r\n\r\nI don't know what's the meaning of \"AttnOutputProjection\". The attention information has been used in \"x = linear([inp] + attns, input_size, True)\", why adding it again?\r\n\r\nAnd meanwhile what is the meaning of \"[cell_output] + attns\", \"cell_output\" is equal with num_symbols, so why add attens?\r\n\r\nIn my experiment, the num_decoder_symbols is 10000 and the hidden size is 32, and I get a matrix 10032*10000 at \"AttnOutputProjection\". It will be out of memory.\r\n10032 is a confusing number, I don't know the physical meaning of this number.\r\n\r\nSo I don't know if it's a bug. If it's a bug, I'm really pleasure to make a PR.\r\nThanks so much.\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}