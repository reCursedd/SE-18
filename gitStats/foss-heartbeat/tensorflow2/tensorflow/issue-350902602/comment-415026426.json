{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/415026426", "html_url": "https://github.com/tensorflow/tensorflow/issues/21637#issuecomment-415026426", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21637", "id": 415026426, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNTAyNjQyNg==", "user": {"login": "lucncdm", "id": 24569806, "node_id": "MDQ6VXNlcjI0NTY5ODA2", "avatar_url": "https://avatars3.githubusercontent.com/u/24569806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucncdm", "html_url": "https://github.com/lucncdm", "followers_url": "https://api.github.com/users/lucncdm/followers", "following_url": "https://api.github.com/users/lucncdm/following{/other_user}", "gists_url": "https://api.github.com/users/lucncdm/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucncdm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucncdm/subscriptions", "organizations_url": "https://api.github.com/users/lucncdm/orgs", "repos_url": "https://api.github.com/users/lucncdm/repos", "events_url": "https://api.github.com/users/lucncdm/events{/privacy}", "received_events_url": "https://api.github.com/users/lucncdm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-22T13:11:36Z", "updated_at": "2018-08-22T13:11:36Z", "author_association": "NONE", "body_html": "<p>Hello, i also encountered this issue.<br>\nI isolated the layer in my network that produced this artefact: it comes from the operation Conv3D.<br>\nIt doesn't occur for every input size, output channel, or kernel size.</p>\n<p>Here is the code i used to reproduce the behaviour:</p>\n<pre><code>from keras.models import Model\nfrom keras.layers import Input, Conv3D\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef conv_model_kernel_1(in_channel, out_channel):\n    inputs = Input((None, None, None, in_channel))\n    output = Conv3D(out_channel, kernel_size=1, padding='same', kernel_initializer='ones', bias_initializer='zeros')(inputs)\n    return Model(inputs, output)\n\nchannels = 32\nvolume = np.ones((202, 236, 185, channels))\nnet = conv_model_kernel_1(channels, channels)\nconv_keras = net.predict(np.expand_dims(volume, 0))\n\nplt.figure()\nplt.imshow(conv_keras[0, :, :, 0, 0])\nplt.show()\n</code></pre>\n<p>The network as defined previously should take every input channel and sum them in each output channel. Each voxel of each volume contained in every output channel should have the same value : 32.</p>\n<p>Here the slice 0 of the output predicted volume contained in channel 0:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/24569806/44464042-eb7c8680-a619-11e8-969a-809973f84650.png\"><img src=\"https://user-images.githubusercontent.com/24569806/44464042-eb7c8680-a619-11e8-969a-809973f84650.png\" alt=\"conv3dartifact\" style=\"max-width:100%;\"></a></p>\n<p>Yellow pixels are 32s, purple ones are 0s.</p>\n<p>I tested this code on different system configurations and everytime it produced the artifact.</p>\n<p>I'd be thankful if you could take a look.<br>\nThanks</p>", "body_text": "Hello, i also encountered this issue.\nI isolated the layer in my network that produced this artefact: it comes from the operation Conv3D.\nIt doesn't occur for every input size, output channel, or kernel size.\nHere is the code i used to reproduce the behaviour:\nfrom keras.models import Model\nfrom keras.layers import Input, Conv3D\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef conv_model_kernel_1(in_channel, out_channel):\n    inputs = Input((None, None, None, in_channel))\n    output = Conv3D(out_channel, kernel_size=1, padding='same', kernel_initializer='ones', bias_initializer='zeros')(inputs)\n    return Model(inputs, output)\n\nchannels = 32\nvolume = np.ones((202, 236, 185, channels))\nnet = conv_model_kernel_1(channels, channels)\nconv_keras = net.predict(np.expand_dims(volume, 0))\n\nplt.figure()\nplt.imshow(conv_keras[0, :, :, 0, 0])\nplt.show()\n\nThe network as defined previously should take every input channel and sum them in each output channel. Each voxel of each volume contained in every output channel should have the same value : 32.\nHere the slice 0 of the output predicted volume contained in channel 0:\n\nYellow pixels are 32s, purple ones are 0s.\nI tested this code on different system configurations and everytime it produced the artifact.\nI'd be thankful if you could take a look.\nThanks", "body": "Hello, i also encountered this issue.\r\nI isolated the layer in my network that produced this artefact: it comes from the operation Conv3D.\r\nIt doesn't occur for every input size, output channel, or kernel size.\r\n\r\nHere is the code i used to reproduce the behaviour:\r\n```\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, Conv3D\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\ndef conv_model_kernel_1(in_channel, out_channel):\r\n    inputs = Input((None, None, None, in_channel))\r\n    output = Conv3D(out_channel, kernel_size=1, padding='same', kernel_initializer='ones', bias_initializer='zeros')(inputs)\r\n    return Model(inputs, output)\r\n\r\nchannels = 32\r\nvolume = np.ones((202, 236, 185, channels))\r\nnet = conv_model_kernel_1(channels, channels)\r\nconv_keras = net.predict(np.expand_dims(volume, 0))\r\n\r\nplt.figure()\r\nplt.imshow(conv_keras[0, :, :, 0, 0])\r\nplt.show()\r\n```\r\n\r\nThe network as defined previously should take every input channel and sum them in each output channel. Each voxel of each volume contained in every output channel should have the same value : 32.\r\n\r\nHere the slice 0 of the output predicted volume contained in channel 0:\r\n![conv3dartifact](https://user-images.githubusercontent.com/24569806/44464042-eb7c8680-a619-11e8-969a-809973f84650.png)\r\n\r\nYellow pixels are 32s, purple ones are 0s.\r\n\r\nI tested this code on different system configurations and everytime it produced the artifact.\r\n\r\nI'd be thankful if you could take a look.\r\nThanks\r\n"}