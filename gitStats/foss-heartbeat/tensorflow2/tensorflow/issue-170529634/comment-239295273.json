{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239295273", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-239295273", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 239295273, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTI5NTI3Mw==", "user": {"login": "msevrens", "id": 1441846, "node_id": "MDQ6VXNlcjE0NDE4NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1441846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msevrens", "html_url": "https://github.com/msevrens", "followers_url": "https://api.github.com/users/msevrens/followers", "following_url": "https://api.github.com/users/msevrens/following{/other_user}", "gists_url": "https://api.github.com/users/msevrens/gists{/gist_id}", "starred_url": "https://api.github.com/users/msevrens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msevrens/subscriptions", "organizations_url": "https://api.github.com/users/msevrens/orgs", "repos_url": "https://api.github.com/users/msevrens/repos", "events_url": "https://api.github.com/users/msevrens/events{/privacy}", "received_events_url": "https://api.github.com/users/msevrens/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-11T21:16:27Z", "updated_at": "2016-08-11T21:16:27Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a></p>\n<p>I don't think bucketing by input size works in this case. A sub-LSTM is encoding each token in the each sentence. There's only a limited number of tokens in each document. At each step a LSTM encoding of each token is concatenated with work a word embedding before fed into a higher level LSTM for tagging.</p>\n<p>I've attempted to not call the cell at ever step like this</p>\n<pre><code>flat_state = nest.flatten(state)\n    flat_zero_output = nest.flatten(zero_output)\n\n    def select_relevant_state(state, mask):\n        c = tf.boolean_mask(state[0], mask)\n        h = tf.boolean_mask(state[1], mask)\n        return (c, h)\n\n    # Function to Perform at Each Time Step\n    def time_step(time, output_ta, state):\n\n        mask = (time &lt; sequence_length)\n        indices = tf.squeeze(tf.to_int32(tf.where(mask)))\n        invert_indices = tf.squeeze(tf.to_int32(tf.where(invert_mask)))\n        invert_indices = tf.to_int32(tf.where(invert_mask))\n        input_t = tuple(ta.read(time) for ta in input_ta)\n\n        # Restore Shape Information\n        for input_, shape in zip(input_t, inputs_got_shape):\n            input_.set_shape(shape[1:])\n\n        input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n\n        # Select Only Relevant at This Time Step\n        input_t = tf.boolean_mask(input_t, mask)\n        state = select_relevant_state(state, mask)\n        call_cell = lambda: cell(input_t, state)\n\n        # Call Cell\n        (output, new_state) = call_cell()\n\n        # Fill Unprocessed Steps\n        filler_output = tf.boolean_mask(zero_output, invert_mask)\n        filler_state = select_relevant_state(state, invert_mask)\n\n        output = tf.dynamic_stitch([indices, invert_indices], [output, filler_output])\n        new_state_c = tf.dynamic_stitch([indices, invert_indices], [new_state[0], filler_state[0]])\n        new_state_h = tf.dynamic_stitch([indices, invert_indices], [new_state[1], filler_state[1]])\n        new_state = tf.pack([new_state_c, new_state_h], axis=0)\n\n        # Pack State if Using State Tuples\n        output = nest.flatten(output)\n\n        output_ta = tuple(ta.write(time, out) for ta, out in zip(output_ta, output))\n\n        return (time + 1, output_ta, new_state)\n</code></pre>\n<p>But this seems to make no difference. It appears that the majority of time is being taken up by back propagation. Is there a means to intervene at this level?</p>\n<p>Is there no way to use the same approach as pycnn and get the same performance? The difference is huge.</p>", "body_text": "@girving\nI don't think bucketing by input size works in this case. A sub-LSTM is encoding each token in the each sentence. There's only a limited number of tokens in each document. At each step a LSTM encoding of each token is concatenated with work a word embedding before fed into a higher level LSTM for tagging.\nI've attempted to not call the cell at ever step like this\nflat_state = nest.flatten(state)\n    flat_zero_output = nest.flatten(zero_output)\n\n    def select_relevant_state(state, mask):\n        c = tf.boolean_mask(state[0], mask)\n        h = tf.boolean_mask(state[1], mask)\n        return (c, h)\n\n    # Function to Perform at Each Time Step\n    def time_step(time, output_ta, state):\n\n        mask = (time < sequence_length)\n        indices = tf.squeeze(tf.to_int32(tf.where(mask)))\n        invert_indices = tf.squeeze(tf.to_int32(tf.where(invert_mask)))\n        invert_indices = tf.to_int32(tf.where(invert_mask))\n        input_t = tuple(ta.read(time) for ta in input_ta)\n\n        # Restore Shape Information\n        for input_, shape in zip(input_t, inputs_got_shape):\n            input_.set_shape(shape[1:])\n\n        input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n\n        # Select Only Relevant at This Time Step\n        input_t = tf.boolean_mask(input_t, mask)\n        state = select_relevant_state(state, mask)\n        call_cell = lambda: cell(input_t, state)\n\n        # Call Cell\n        (output, new_state) = call_cell()\n\n        # Fill Unprocessed Steps\n        filler_output = tf.boolean_mask(zero_output, invert_mask)\n        filler_state = select_relevant_state(state, invert_mask)\n\n        output = tf.dynamic_stitch([indices, invert_indices], [output, filler_output])\n        new_state_c = tf.dynamic_stitch([indices, invert_indices], [new_state[0], filler_state[0]])\n        new_state_h = tf.dynamic_stitch([indices, invert_indices], [new_state[1], filler_state[1]])\n        new_state = tf.pack([new_state_c, new_state_h], axis=0)\n\n        # Pack State if Using State Tuples\n        output = nest.flatten(output)\n\n        output_ta = tuple(ta.write(time, out) for ta, out in zip(output_ta, output))\n\n        return (time + 1, output_ta, new_state)\n\nBut this seems to make no difference. It appears that the majority of time is being taken up by back propagation. Is there a means to intervene at this level?\nIs there no way to use the same approach as pycnn and get the same performance? The difference is huge.", "body": "@girving\n\nI don't think bucketing by input size works in this case. A sub-LSTM is encoding each token in the each sentence. There's only a limited number of tokens in each document. At each step a LSTM encoding of each token is concatenated with work a word embedding before fed into a higher level LSTM for tagging. \n\nI've attempted to not call the cell at ever step like this\n\n```\nflat_state = nest.flatten(state)\n    flat_zero_output = nest.flatten(zero_output)\n\n    def select_relevant_state(state, mask):\n        c = tf.boolean_mask(state[0], mask)\n        h = tf.boolean_mask(state[1], mask)\n        return (c, h)\n\n    # Function to Perform at Each Time Step\n    def time_step(time, output_ta, state):\n\n        mask = (time < sequence_length)\n        indices = tf.squeeze(tf.to_int32(tf.where(mask)))\n        invert_indices = tf.squeeze(tf.to_int32(tf.where(invert_mask)))\n        invert_indices = tf.to_int32(tf.where(invert_mask))\n        input_t = tuple(ta.read(time) for ta in input_ta)\n\n        # Restore Shape Information\n        for input_, shape in zip(input_t, inputs_got_shape):\n            input_.set_shape(shape[1:])\n\n        input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n\n        # Select Only Relevant at This Time Step\n        input_t = tf.boolean_mask(input_t, mask)\n        state = select_relevant_state(state, mask)\n        call_cell = lambda: cell(input_t, state)\n\n        # Call Cell\n        (output, new_state) = call_cell()\n\n        # Fill Unprocessed Steps\n        filler_output = tf.boolean_mask(zero_output, invert_mask)\n        filler_state = select_relevant_state(state, invert_mask)\n\n        output = tf.dynamic_stitch([indices, invert_indices], [output, filler_output])\n        new_state_c = tf.dynamic_stitch([indices, invert_indices], [new_state[0], filler_state[0]])\n        new_state_h = tf.dynamic_stitch([indices, invert_indices], [new_state[1], filler_state[1]])\n        new_state = tf.pack([new_state_c, new_state_h], axis=0)\n\n        # Pack State if Using State Tuples\n        output = nest.flatten(output)\n\n        output_ta = tuple(ta.write(time, out) for ta, out in zip(output_ta, output))\n\n        return (time + 1, output_ta, new_state)\n```\n\nBut this seems to make no difference. It appears that the majority of time is being taken up by back propagation. Is there a means to intervene at this level?\n\nIs there no way to use the same approach as pycnn and get the same performance? The difference is huge. \n"}