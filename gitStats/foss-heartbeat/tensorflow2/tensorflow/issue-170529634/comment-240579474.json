{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/240579474", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-240579474", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 240579474, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MDU3OTQ3NA==", "user": {"login": "msevrens", "id": 1441846, "node_id": "MDQ6VXNlcjE0NDE4NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1441846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msevrens", "html_url": "https://github.com/msevrens", "followers_url": "https://api.github.com/users/msevrens/followers", "following_url": "https://api.github.com/users/msevrens/following{/other_user}", "gists_url": "https://api.github.com/users/msevrens/gists{/gist_id}", "starred_url": "https://api.github.com/users/msevrens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msevrens/subscriptions", "organizations_url": "https://api.github.com/users/msevrens/orgs", "repos_url": "https://api.github.com/users/msevrens/repos", "events_url": "https://api.github.com/users/msevrens/events{/privacy}", "received_events_url": "https://api.github.com/users/msevrens/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-17T23:23:38Z", "updated_at": "2016-08-17T23:23:38Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a></p>\n<p>The primary speed drain is the inner LSTM which does character encoding. Both the batch size and the max_sequence_length of this inner LSTM are variable. This helps speed it up quite a bit.</p>\n<p>Outer LSTM doesn't use batching, same as original paper.</p>\n<p>Removing sequence lengths throws an error. Hidden dims are 100. Running on CPU is fine though as long as long as it matches the speed of pycnn code. As of now on CPU it's about half the speed.</p>\n<p>I'm mainly interested now in the nested loop issue as this would seem to solve the fundamental problem of unnecessary calculation. In pycnn time scales relative to number of characters in the sentence. In TensorFlow time scales to <code>max_token_length * num_tokens</code>. As token length can be extremely variable in my dataset, until that issue is solved everything seems negligible.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a><br>\nI'm working on a more stripped down bare bones version of the issue to help debugging.</p>", "body_text": "@ebrevdo\nThe primary speed drain is the inner LSTM which does character encoding. Both the batch size and the max_sequence_length of this inner LSTM are variable. This helps speed it up quite a bit.\nOuter LSTM doesn't use batching, same as original paper.\nRemoving sequence lengths throws an error. Hidden dims are 100. Running on CPU is fine though as long as long as it matches the speed of pycnn code. As of now on CPU it's about half the speed.\nI'm mainly interested now in the nested loop issue as this would seem to solve the fundamental problem of unnecessary calculation. In pycnn time scales relative to number of characters in the sentence. In TensorFlow time scales to max_token_length * num_tokens. As token length can be extremely variable in my dataset, until that issue is solved everything seems negligible.\n@girving\nI'm working on a more stripped down bare bones version of the issue to help debugging.", "body": "@ebrevdo \n\nThe primary speed drain is the inner LSTM which does character encoding. Both the batch size and the max_sequence_length of this inner LSTM are variable. This helps speed it up quite a bit.\n\nOuter LSTM doesn't use batching, same as original paper.\n\nRemoving sequence lengths throws an error. Hidden dims are 100. Running on CPU is fine though as long as long as it matches the speed of pycnn code. As of now on CPU it's about half the speed.\n\nI'm mainly interested now in the nested loop issue as this would seem to solve the fundamental problem of unnecessary calculation. In pycnn time scales relative to number of characters in the sentence. In TensorFlow time scales to `max_token_length * num_tokens`. As token length can be extremely variable in my dataset, until that issue is solved everything seems negligible.\n\n@girving \nI'm working on a more stripped down bare bones version of the issue to help debugging. \n"}