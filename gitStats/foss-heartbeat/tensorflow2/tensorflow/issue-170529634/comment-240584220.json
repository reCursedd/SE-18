{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/240584220", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-240584220", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 240584220, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MDU4NDIyMA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-17T23:51:12Z", "updated_at": "2016-08-17T23:51:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>With a batch size of 1 and 100 dims, you generally don't get enough data<br>\ngoing through to saturate the GPU's matmul powers; and copying data to/from<br>\nthe GPU becomes the bottleneck.</p>\n<p>dynamic_rnn accepts a tensor with a shape of max_time_for_this_minibatch<br>\nand does not perform early stopping for two reasons:</p>\n<ol>\n<li>you can provide an inputs tensor where max_time_for_this_minibatch is<br>\ntruly the maximum time <em>for this minibatch</em></li>\n<li>having a bunch of cond ops (which must run on CPU) will add additional<br>\nslowdown to the computation, especially if most of the computation is on a<br>\nGPU.  and because of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a> it's unnecessary since the length of the<br>\ncomputation can be variable and is determined by the input tensor<br>\ndimensions.  if you are using placeholders, simply use<br>\nplaceholder(shape=(batch_size, None, input_depth), dtype=...) and feed in<br>\nproperly sized inputs.</li>\n</ol>\n<p>In contrast, with tf.nn.rnn, you always have a fixed graph and using cond()<br>\nto reduce the number of calls to the LSTM cell (especially the matmuls) is<br>\nextremely important.  In this case, sequence_lengths is used to replace<br>\nthis computation past max(sequence_lengths) with just symbolically copying<br>\nsome zero values through (this is essentially free).</p>\n<p>On Wed, Aug 17, 2016 at 4:24 PM, Matt Sevrens <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> <a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a></p>\n<p>The primary speed drain is the inner LSTM which does character encoding.<br>\nBoth the batch size and the max_sequence_length of this inner LSTM are<br>\nvariable. This helps speed it up quite a bit.</p>\n<p>Outer LSTM doesn't use batching, same as original paper.</p>\n<p>Removing sequence lengths throws an error. Hidden dims are 100. Running on<br>\nCPU is fine though as long as long as it matches the speed of pycnn code.<br>\nAs of now on CPU it's about half the speed.</p>\n<p>I'm mainly interested now in the nested loop issue as this would seem to<br>\nsolve the fundamental problem of unnecessary calculation. In pycnn time<br>\nscales relative to number of characters in the sentence. In TensorFlow time<br>\nscales to max_token_length * num_tokens. As token length can be extremely<br>\nvariable in my dataset, until that issue is solved everything seems<br>\nnegligible.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> <a href=\"https://github.com/girving\">https://github.com/girving</a><br>\nI'm working on a more stripped down bare bones version of the issue to<br>\nhelp debugging.</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"170529634\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3738\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3738/hovercard?comment_id=240579474&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-240579474\">#3738 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimzvOS5_V9atNeVWXVHP-pKl9Y5rMks5qg5gcgaJpZM4Jhn6r\">https://github.com/notifications/unsubscribe-auth/ABtimzvOS5_V9atNeVWXVHP-pKl9Y5rMks5qg5gcgaJpZM4Jhn6r</a><br>\n.</p>\n</blockquote>", "body_text": "With a batch size of 1 and 100 dims, you generally don't get enough data\ngoing through to saturate the GPU's matmul powers; and copying data to/from\nthe GPU becomes the bottleneck.\ndynamic_rnn accepts a tensor with a shape of max_time_for_this_minibatch\nand does not perform early stopping for two reasons:\n\nyou can provide an inputs tensor where max_time_for_this_minibatch is\ntruly the maximum time for this minibatch\nhaving a bunch of cond ops (which must run on CPU) will add additional\nslowdown to the computation, especially if most of the computation is on a\nGPU.  and because of #1 it's unnecessary since the length of the\ncomputation can be variable and is determined by the input tensor\ndimensions.  if you are using placeholders, simply use\nplaceholder(shape=(batch_size, None, input_depth), dtype=...) and feed in\nproperly sized inputs.\n\nIn contrast, with tf.nn.rnn, you always have a fixed graph and using cond()\nto reduce the number of calls to the LSTM cell (especially the matmuls) is\nextremely important.  In this case, sequence_lengths is used to replace\nthis computation past max(sequence_lengths) with just symbolically copying\nsome zero values through (this is essentially free).\nOn Wed, Aug 17, 2016 at 4:24 PM, Matt Sevrens notifications@github.com\nwrote:\n\n@ebrevdo https://github.com/ebrevdo\nThe primary speed drain is the inner LSTM which does character encoding.\nBoth the batch size and the max_sequence_length of this inner LSTM are\nvariable. This helps speed it up quite a bit.\nOuter LSTM doesn't use batching, same as original paper.\nRemoving sequence lengths throws an error. Hidden dims are 100. Running on\nCPU is fine though as long as long as it matches the speed of pycnn code.\nAs of now on CPU it's about half the speed.\nI'm mainly interested now in the nested loop issue as this would seem to\nsolve the fundamental problem of unnecessary calculation. In pycnn time\nscales relative to number of characters in the sentence. In TensorFlow time\nscales to max_token_length * num_tokens. As token length can be extremely\nvariable in my dataset, until that issue is solved everything seems\nnegligible.\n@girving https://github.com/girving\nI'm working on a more stripped down bare bones version of the issue to\nhelp debugging.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n#3738 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtimzvOS5_V9atNeVWXVHP-pKl9Y5rMks5qg5gcgaJpZM4Jhn6r\n.", "body": "With a batch size of 1 and 100 dims, you generally don't get enough data\ngoing through to saturate the GPU's matmul powers; and copying data to/from\nthe GPU becomes the bottleneck.\n\ndynamic_rnn accepts a tensor with a shape of max_time_for_this_minibatch\nand does not perform early stopping for two reasons:\n1. you can provide an inputs tensor where max_time_for_this_minibatch is\ntruly the maximum time _for this minibatch_\n2. having a bunch of cond ops (which must run on CPU) will add additional\nslowdown to the computation, especially if most of the computation is on a\nGPU.  and because of #1 it's unnecessary since the length of the\ncomputation can be variable and is determined by the input tensor\ndimensions.  if you are using placeholders, simply use\nplaceholder(shape=(batch_size, None, input_depth), dtype=...) and feed in\nproperly sized inputs.\n\nIn contrast, with tf.nn.rnn, you always have a fixed graph and using cond()\nto reduce the number of calls to the LSTM cell (especially the matmuls) is\nextremely important.  In this case, sequence_lengths is used to replace\nthis computation past max(sequence_lengths) with just symbolically copying\nsome zero values through (this is essentially free).\n\nOn Wed, Aug 17, 2016 at 4:24 PM, Matt Sevrens notifications@github.com\nwrote:\n\n> @ebrevdo https://github.com/ebrevdo\n> \n> The primary speed drain is the inner LSTM which does character encoding.\n> Both the batch size and the max_sequence_length of this inner LSTM are\n> variable. This helps speed it up quite a bit.\n> \n> Outer LSTM doesn't use batching, same as original paper.\n> \n> Removing sequence lengths throws an error. Hidden dims are 100. Running on\n> CPU is fine though as long as long as it matches the speed of pycnn code.\n> As of now on CPU it's about half the speed.\n> \n> I'm mainly interested now in the nested loop issue as this would seem to\n> solve the fundamental problem of unnecessary calculation. In pycnn time\n> scales relative to number of characters in the sentence. In TensorFlow time\n> scales to max_token_length \\* num_tokens. As token length can be extremely\n> variable in my dataset, until that issue is solved everything seems\n> negligible.\n> \n> @girving https://github.com/girving\n> I'm working on a more stripped down bare bones version of the issue to\n> help debugging.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-240579474,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimzvOS5_V9atNeVWXVHP-pKl9Y5rMks5qg5gcgaJpZM4Jhn6r\n> .\n"}