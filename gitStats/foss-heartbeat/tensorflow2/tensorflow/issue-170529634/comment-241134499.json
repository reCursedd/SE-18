{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241134499", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-241134499", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 241134499, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTEzNDQ5OQ==", "user": {"login": "msevrens", "id": 1441846, "node_id": "MDQ6VXNlcjE0NDE4NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1441846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msevrens", "html_url": "https://github.com/msevrens", "followers_url": "https://api.github.com/users/msevrens/followers", "following_url": "https://api.github.com/users/msevrens/following{/other_user}", "gists_url": "https://api.github.com/users/msevrens/gists{/gist_id}", "starred_url": "https://api.github.com/users/msevrens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msevrens/subscriptions", "organizations_url": "https://api.github.com/users/msevrens/orgs", "repos_url": "https://api.github.com/users/msevrens/repos", "events_url": "https://api.github.com/users/msevrens/events{/privacy}", "received_events_url": "https://api.github.com/users/msevrens/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-19T21:03:01Z", "updated_at": "2016-08-19T21:03:01Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a><br>\nThe original paper is not using truncated back propagation through time. The final outputs from the forward and backward passes of the inner LSTM on each token are concatenated to a word embedding and fed into the higher level LSTM. There is no loss at every step as the loss comes from the higher level LSTM.</p>\n<p>We also discussed bucketing above and that will not work either as the mini batch itself is the sentence input. Both the inner and outer LSTM are optimized together using the loss from the higher level LSTM. So the batch for the inner LSTM will always be the tokens from the sentence being tagged.</p>\n<p>This architecture seems pretty tied to what is causing the performance issue. At this point, barring a major change to how LSTM's are handled by TensorFlow I believe the best solution is this while loop approach.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a><br>\nI agree, I'm going to change the title</p>", "body_text": "@ebrevdo\nThe original paper is not using truncated back propagation through time. The final outputs from the forward and backward passes of the inner LSTM on each token are concatenated to a word embedding and fed into the higher level LSTM. There is no loss at every step as the loss comes from the higher level LSTM.\nWe also discussed bucketing above and that will not work either as the mini batch itself is the sentence input. Both the inner and outer LSTM are optimized together using the loss from the higher level LSTM. So the batch for the inner LSTM will always be the tokens from the sentence being tagged.\nThis architecture seems pretty tied to what is causing the performance issue. At this point, barring a major change to how LSTM's are handled by TensorFlow I believe the best solution is this while loop approach.\n@girving\nI agree, I'm going to change the title", "body": "@ebrevdo \nThe original paper is not using truncated back propagation through time. The final outputs from the forward and backward passes of the inner LSTM on each token are concatenated to a word embedding and fed into the higher level LSTM. There is no loss at every step as the loss comes from the higher level LSTM. \n\nWe also discussed bucketing above and that will not work either as the mini batch itself is the sentence input. Both the inner and outer LSTM are optimized together using the loss from the higher level LSTM. So the batch for the inner LSTM will always be the tokens from the sentence being tagged. \n\nThis architecture seems pretty tied to what is causing the performance issue. At this point, barring a major change to how LSTM's are handled by TensorFlow I believe the best solution is this while loop approach. \n\n@girving \nI agree, I'm going to change the title\n"}