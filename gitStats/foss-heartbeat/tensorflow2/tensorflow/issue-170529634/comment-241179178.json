{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241179178", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-241179178", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 241179178, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTE3OTE3OA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-20T04:44:58Z", "updated_at": "2016-08-20T04:44:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>One way to handle this might be to combine multiple shorter examples into a<br>\nsingle long example, with state reset indicators in between, and minibatch<br>\nthese longer examples. This kind of bookkeeping is not impossible in a Tf<br>\ngraph, but it's also not easy. I've never seen it done.</p>\n<p>On Aug 19, 2016 2:03 PM, \"Matt Sevrens\" <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> <a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a><br>\nThe original paper is not using truncated back propagation through time.<br>\nThe final outputs from the forward and backward passes of the inner LSTM on<br>\neach token are concatenated to a word embedding and fed into the higher<br>\nlevel LSTM. There is no loss at every step as the loss comes from the<br>\nhigher level LSTM.</p>\n<p>We also discussed bucketing above and that will not work either as the<br>\nmini batch itself is the sentence input. Both the inner and outer LSTM are<br>\noptimized together using the loss from the higher level LSTM. So the batch<br>\nfor the inner LSTM will always be the tokens from the sentence being<br>\ntagged.</p>\n<p>This architecture seems pretty tied to what is causing the performance<br>\nissue. At this point, barring a major change to how LSTM's are handled by<br>\nTensorFlow I believe the best solution is this while loop approach.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> <a href=\"https://github.com/girving\">https://github.com/girving</a><br>\nI agree, I'm going to change the title</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"170529634\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3738\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3738/hovercard?comment_id=241134499&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-241134499\">#3738 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim_O2pakgJmEw5xRYHpoCb5ToCe_dks5qhhomgaJpZM4Jhn6r\">https://github.com/notifications/unsubscribe-auth/ABtim_O2pakgJmEw5xRYHpoCb5ToCe_dks5qhhomgaJpZM4Jhn6r</a><br>\n.</p>\n</blockquote>", "body_text": "One way to handle this might be to combine multiple shorter examples into a\nsingle long example, with state reset indicators in between, and minibatch\nthese longer examples. This kind of bookkeeping is not impossible in a Tf\ngraph, but it's also not easy. I've never seen it done.\nOn Aug 19, 2016 2:03 PM, \"Matt Sevrens\" notifications@github.com wrote:\n\n@ebrevdo https://github.com/ebrevdo\nThe original paper is not using truncated back propagation through time.\nThe final outputs from the forward and backward passes of the inner LSTM on\neach token are concatenated to a word embedding and fed into the higher\nlevel LSTM. There is no loss at every step as the loss comes from the\nhigher level LSTM.\nWe also discussed bucketing above and that will not work either as the\nmini batch itself is the sentence input. Both the inner and outer LSTM are\noptimized together using the loss from the higher level LSTM. So the batch\nfor the inner LSTM will always be the tokens from the sentence being\ntagged.\nThis architecture seems pretty tied to what is causing the performance\nissue. At this point, barring a major change to how LSTM's are handled by\nTensorFlow I believe the best solution is this while loop approach.\n@girving https://github.com/girving\nI agree, I'm going to change the title\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n#3738 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtim_O2pakgJmEw5xRYHpoCb5ToCe_dks5qhhomgaJpZM4Jhn6r\n.", "body": "One way to handle this might be to combine multiple shorter examples into a\nsingle long example, with state reset indicators in between, and minibatch\nthese longer examples. This kind of bookkeeping is not impossible in a Tf\ngraph, but it's also not easy. I've never seen it done.\n\nOn Aug 19, 2016 2:03 PM, \"Matt Sevrens\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo\n> The original paper is not using truncated back propagation through time.\n> The final outputs from the forward and backward passes of the inner LSTM on\n> each token are concatenated to a word embedding and fed into the higher\n> level LSTM. There is no loss at every step as the loss comes from the\n> higher level LSTM.\n> \n> We also discussed bucketing above and that will not work either as the\n> mini batch itself is the sentence input. Both the inner and outer LSTM are\n> optimized together using the loss from the higher level LSTM. So the batch\n> for the inner LSTM will always be the tokens from the sentence being\n> tagged.\n> \n> This architecture seems pretty tied to what is causing the performance\n> issue. At this point, barring a major change to how LSTM's are handled by\n> TensorFlow I believe the best solution is this while loop approach.\n> \n> @girving https://github.com/girving\n> I agree, I'm going to change the title\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-241134499,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim_O2pakgJmEw5xRYHpoCb5ToCe_dks5qhhomgaJpZM4Jhn6r\n> .\n"}