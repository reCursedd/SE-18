{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241531134", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-241531134", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 241531134, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTUzMTEzNA==", "user": {"login": "msevrens", "id": 1441846, "node_id": "MDQ6VXNlcjE0NDE4NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1441846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msevrens", "html_url": "https://github.com/msevrens", "followers_url": "https://api.github.com/users/msevrens/followers", "following_url": "https://api.github.com/users/msevrens/following{/other_user}", "gists_url": "https://api.github.com/users/msevrens/gists{/gist_id}", "starred_url": "https://api.github.com/users/msevrens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msevrens/subscriptions", "organizations_url": "https://api.github.com/users/msevrens/orgs", "repos_url": "https://api.github.com/users/msevrens/repos", "events_url": "https://api.github.com/users/msevrens/events{/privacy}", "received_events_url": "https://api.github.com/users/msevrens/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-22T19:59:18Z", "updated_at": "2016-08-22T19:59:18Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2342391\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuanbyu\">@yuanbyu</a></p>\n<p>I've put together a much shorter and simpler code sample that replicates the bug. Hopefully this will be helpful in debugging:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.python.ops import tensor_array_ops\n\n# Prepare Input\n\ntokens = [\"the\", \"fox\", \"jumps\", \"over\", \"the\", \"erroneous\", \"dog\"]\ntags = [\"other\", \"noun\", \"other\", \"other\", \"other\", \"other\", \"noun\"]\n\ntag_map = {\n    \"noun\": 0,\n    \"other\": 1\n}\n\ntags = np.array([int(tag_map[tag]) for tag in tags])\nencoded_tags = (np.arange(len(tag_map)) == tags[:, None]).astype(np.float32)\n\nalphabet = \"abcdefghijklmnopqrstuvwxyz0\"\nc2i = {a : i for i, a in enumerate(alphabet)}\n\nchar_inputs = []\ntoken_lengths = [len(t) for t in tokens]\nmax_token_length = max(token_lengths)\nnum_tokens = len(tokens)\n\nfor token in tokens:\n    char_ind = [c2i[token[c]] if c &lt; len(token) else c2i[\"0\"] for c in range(max_token_length)]\n    char_inputs.append(char_ind)\n\n# Build Graph\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n    # Character Embeddings\n    character_indices = tf.placeholder(tf.int32, [len(tokens), max_token_length])\n    word_lengths = tf.placeholder(tf.int32, [None])\n\n    cembed_matrix = tf.Variable(tf.random_uniform([len(c2i), 100]))\n    inner_lstm = tf.nn.rnn_cell.BasicLSTMCell(100, state_is_tuple=True)\n\n    # Outer Loop Body\n    def outer_body(i, s_out):\n\n        token_input = tf.nn.embedding_lookup(cembed_matrix, tf.gather(character_indices, i))\n        token_input = tf.expand_dims(token_input, 0)\n        token_input = tf.transpose(token_input, perm=[1,0,2])\n\n        options = {\n            \"dtype\": tf.float32,\n            \"sequence_length\": tf.expand_dims(tf.gather(word_lengths, i), 0),\n            \"time_major\": True,\n            \"scope\": \"inner\"\n        }\n\n        output, state = tf.nn.dynamic_rnn(inner_lstm, token_input, **options)\n        last_output = tf.gather(output, tf.gather(word_lengths - 1, i))\n        s_out = s_out.write(i, tf.squeeze(last_output))\n\n        return tf.add(i, 1), s_out\n\n    # Build Loop in Graph\n    i = tf.constant(0)\n    sentence_output = tensor_array_ops.TensorArray(dtype=tf.float32, size=num_tokens)\n    outer_cond = lambda i, *_: tf.less(i, num_tokens)\n    _, encoded_chars = tf.while_loop(outer_cond, outer_body, [i, sentence_output])\n    encoded_chars = encoded_chars.pack()\n\n    # Outer LSTM\n    outer_lstm = tf.nn.rnn_cell.BasicLSTMCell(100, state_is_tuple=True)\n    weight = tf.Variable(tf.random_uniform([100, len(tag_map)]))\n    batched_input = tf.expand_dims(encoded_chars, 0)\n\n    options = {\n        \"dtype\": tf.float32,\n        \"sequence_length\": tf.expand_dims(num_tokens, 0),\n        \"scope\": \"outer\"\n    }\n\n    output, state = tf.nn.dynamic_rnn(outer_lstm, batched_input, **options)\n    prediction = tf.log(tf.nn.softmax(tf.matmul(tf.gather(output, num_tokens), weight)))\n\n    # Calculate Loss and Optimize\n    labels = tf.placeholder(tf.float32, shape=[None, len(tag_map)])\n    loss = tf.neg(tf.reduce_sum(prediction * labels))\n    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n\n# Run Session\n\nwith tf.Session(graph=graph) as sess:\n\n    tf.initialize_all_variables().run()\n\n    feed_dict = {\n        character_indices : char_inputs,\n        word_lengths : token_lengths,\n        labels: encoded_tags\n    }\n\n    sess.run(optimizer, feed_dict=feed_dict)\n</code></pre>", "body_text": "@girving @ebrevdo @yuanbyu\nI've put together a much shorter and simpler code sample that replicates the bug. Hopefully this will be helpful in debugging:\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.python.ops import tensor_array_ops\n\n# Prepare Input\n\ntokens = [\"the\", \"fox\", \"jumps\", \"over\", \"the\", \"erroneous\", \"dog\"]\ntags = [\"other\", \"noun\", \"other\", \"other\", \"other\", \"other\", \"noun\"]\n\ntag_map = {\n    \"noun\": 0,\n    \"other\": 1\n}\n\ntags = np.array([int(tag_map[tag]) for tag in tags])\nencoded_tags = (np.arange(len(tag_map)) == tags[:, None]).astype(np.float32)\n\nalphabet = \"abcdefghijklmnopqrstuvwxyz0\"\nc2i = {a : i for i, a in enumerate(alphabet)}\n\nchar_inputs = []\ntoken_lengths = [len(t) for t in tokens]\nmax_token_length = max(token_lengths)\nnum_tokens = len(tokens)\n\nfor token in tokens:\n    char_ind = [c2i[token[c]] if c < len(token) else c2i[\"0\"] for c in range(max_token_length)]\n    char_inputs.append(char_ind)\n\n# Build Graph\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n    # Character Embeddings\n    character_indices = tf.placeholder(tf.int32, [len(tokens), max_token_length])\n    word_lengths = tf.placeholder(tf.int32, [None])\n\n    cembed_matrix = tf.Variable(tf.random_uniform([len(c2i), 100]))\n    inner_lstm = tf.nn.rnn_cell.BasicLSTMCell(100, state_is_tuple=True)\n\n    # Outer Loop Body\n    def outer_body(i, s_out):\n\n        token_input = tf.nn.embedding_lookup(cembed_matrix, tf.gather(character_indices, i))\n        token_input = tf.expand_dims(token_input, 0)\n        token_input = tf.transpose(token_input, perm=[1,0,2])\n\n        options = {\n            \"dtype\": tf.float32,\n            \"sequence_length\": tf.expand_dims(tf.gather(word_lengths, i), 0),\n            \"time_major\": True,\n            \"scope\": \"inner\"\n        }\n\n        output, state = tf.nn.dynamic_rnn(inner_lstm, token_input, **options)\n        last_output = tf.gather(output, tf.gather(word_lengths - 1, i))\n        s_out = s_out.write(i, tf.squeeze(last_output))\n\n        return tf.add(i, 1), s_out\n\n    # Build Loop in Graph\n    i = tf.constant(0)\n    sentence_output = tensor_array_ops.TensorArray(dtype=tf.float32, size=num_tokens)\n    outer_cond = lambda i, *_: tf.less(i, num_tokens)\n    _, encoded_chars = tf.while_loop(outer_cond, outer_body, [i, sentence_output])\n    encoded_chars = encoded_chars.pack()\n\n    # Outer LSTM\n    outer_lstm = tf.nn.rnn_cell.BasicLSTMCell(100, state_is_tuple=True)\n    weight = tf.Variable(tf.random_uniform([100, len(tag_map)]))\n    batched_input = tf.expand_dims(encoded_chars, 0)\n\n    options = {\n        \"dtype\": tf.float32,\n        \"sequence_length\": tf.expand_dims(num_tokens, 0),\n        \"scope\": \"outer\"\n    }\n\n    output, state = tf.nn.dynamic_rnn(outer_lstm, batched_input, **options)\n    prediction = tf.log(tf.nn.softmax(tf.matmul(tf.gather(output, num_tokens), weight)))\n\n    # Calculate Loss and Optimize\n    labels = tf.placeholder(tf.float32, shape=[None, len(tag_map)])\n    loss = tf.neg(tf.reduce_sum(prediction * labels))\n    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n\n# Run Session\n\nwith tf.Session(graph=graph) as sess:\n\n    tf.initialize_all_variables().run()\n\n    feed_dict = {\n        character_indices : char_inputs,\n        word_lengths : token_lengths,\n        labels: encoded_tags\n    }\n\n    sess.run(optimizer, feed_dict=feed_dict)", "body": "@girving @ebrevdo @yuanbyu \n\nI've put together a much shorter and simpler code sample that replicates the bug. Hopefully this will be helpful in debugging:\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.python.ops import tensor_array_ops\n\n# Prepare Input\n\ntokens = [\"the\", \"fox\", \"jumps\", \"over\", \"the\", \"erroneous\", \"dog\"]\ntags = [\"other\", \"noun\", \"other\", \"other\", \"other\", \"other\", \"noun\"]\n\ntag_map = {\n    \"noun\": 0,\n    \"other\": 1\n}\n\ntags = np.array([int(tag_map[tag]) for tag in tags])\nencoded_tags = (np.arange(len(tag_map)) == tags[:, None]).astype(np.float32)\n\nalphabet = \"abcdefghijklmnopqrstuvwxyz0\"\nc2i = {a : i for i, a in enumerate(alphabet)}\n\nchar_inputs = []\ntoken_lengths = [len(t) for t in tokens]\nmax_token_length = max(token_lengths)\nnum_tokens = len(tokens)\n\nfor token in tokens:\n    char_ind = [c2i[token[c]] if c < len(token) else c2i[\"0\"] for c in range(max_token_length)]\n    char_inputs.append(char_ind)\n\n# Build Graph\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n    # Character Embeddings\n    character_indices = tf.placeholder(tf.int32, [len(tokens), max_token_length])\n    word_lengths = tf.placeholder(tf.int32, [None])\n\n    cembed_matrix = tf.Variable(tf.random_uniform([len(c2i), 100]))\n    inner_lstm = tf.nn.rnn_cell.BasicLSTMCell(100, state_is_tuple=True)\n\n    # Outer Loop Body\n    def outer_body(i, s_out):\n\n        token_input = tf.nn.embedding_lookup(cembed_matrix, tf.gather(character_indices, i))\n        token_input = tf.expand_dims(token_input, 0)\n        token_input = tf.transpose(token_input, perm=[1,0,2])\n\n        options = {\n            \"dtype\": tf.float32,\n            \"sequence_length\": tf.expand_dims(tf.gather(word_lengths, i), 0),\n            \"time_major\": True,\n            \"scope\": \"inner\"\n        }\n\n        output, state = tf.nn.dynamic_rnn(inner_lstm, token_input, **options)\n        last_output = tf.gather(output, tf.gather(word_lengths - 1, i))\n        s_out = s_out.write(i, tf.squeeze(last_output))\n\n        return tf.add(i, 1), s_out\n\n    # Build Loop in Graph\n    i = tf.constant(0)\n    sentence_output = tensor_array_ops.TensorArray(dtype=tf.float32, size=num_tokens)\n    outer_cond = lambda i, *_: tf.less(i, num_tokens)\n    _, encoded_chars = tf.while_loop(outer_cond, outer_body, [i, sentence_output])\n    encoded_chars = encoded_chars.pack()\n\n    # Outer LSTM\n    outer_lstm = tf.nn.rnn_cell.BasicLSTMCell(100, state_is_tuple=True)\n    weight = tf.Variable(tf.random_uniform([100, len(tag_map)]))\n    batched_input = tf.expand_dims(encoded_chars, 0)\n\n    options = {\n        \"dtype\": tf.float32,\n        \"sequence_length\": tf.expand_dims(num_tokens, 0),\n        \"scope\": \"outer\"\n    }\n\n    output, state = tf.nn.dynamic_rnn(outer_lstm, batched_input, **options)\n    prediction = tf.log(tf.nn.softmax(tf.matmul(tf.gather(output, num_tokens), weight)))\n\n    # Calculate Loss and Optimize\n    labels = tf.placeholder(tf.float32, shape=[None, len(tag_map)])\n    loss = tf.neg(tf.reduce_sum(prediction * labels))\n    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n\n# Run Session\n\nwith tf.Session(graph=graph) as sess:\n\n    tf.initialize_all_variables().run()\n\n    feed_dict = {\n        character_indices : char_inputs,\n        word_lengths : token_lengths,\n        labels: encoded_tags\n    }\n\n    sess.run(optimizer, feed_dict=feed_dict)\n```\n"}