{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/240570220", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-240570220", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 240570220, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MDU3MDIyMA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-17T22:35:13Z", "updated_at": "2016-08-17T22:35:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Matt: how large are your hidden states and inputs and batch sizes?  The<br>\nlarger each of these parameters is, the faster that GPU training will be<br>\nrelative to CPU training.</p>\n<p>Are you using the sequence_lengths input parameter?  How fast does training<br>\nbecome if you don't use it?</p>\n<p>On Wed, Aug 17, 2016 at 2:23 PM, Matt Sevrens <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> <a href=\"https://github.com/girving\">https://github.com/girving</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2342391\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuanbyu\">@yuanbyu</a><br>\n<a href=\"https://github.com/yuanbyu\">https://github.com/yuanbyu</a></p>\n<p>So I've discovered that training on the CPU only and clocked at 70ms per<br>\ntraining step versus 180ms per training step.</p>\n<p>Is this expected behavior? Are LSTMs inherently faster on a CPU?</p>\n<p>Note that the GradLoopState is still an issue and this is still half the<br>\nspeed that pycnn trains the same model.</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"170529634\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3738\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3738/hovercard?comment_id=240552898&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-240552898\">#3738 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim08QhHEDV4jviZly9jnBxscqyclOks5qg3u1gaJpZM4Jhn6r\">https://github.com/notifications/unsubscribe-auth/ABtim08QhHEDV4jviZly9jnBxscqyclOks5qg3u1gaJpZM4Jhn6r</a><br>\n.</p>\n</blockquote>", "body_text": "Matt: how large are your hidden states and inputs and batch sizes?  The\nlarger each of these parameters is, the faster that GPU training will be\nrelative to CPU training.\nAre you using the sequence_lengths input parameter?  How fast does training\nbecome if you don't use it?\nOn Wed, Aug 17, 2016 at 2:23 PM, Matt Sevrens notifications@github.com\nwrote:\n\n@girving https://github.com/girving @yuanbyu\nhttps://github.com/yuanbyu\nSo I've discovered that training on the CPU only and clocked at 70ms per\ntraining step versus 180ms per training step.\nIs this expected behavior? Are LSTMs inherently faster on a CPU?\nNote that the GradLoopState is still an issue and this is still half the\nspeed that pycnn trains the same model.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n#3738 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtim08QhHEDV4jviZly9jnBxscqyclOks5qg3u1gaJpZM4Jhn6r\n.", "body": "Matt: how large are your hidden states and inputs and batch sizes?  The\nlarger each of these parameters is, the faster that GPU training will be\nrelative to CPU training.\n\nAre you using the sequence_lengths input parameter?  How fast does training\nbecome if you don't use it?\n\nOn Wed, Aug 17, 2016 at 2:23 PM, Matt Sevrens notifications@github.com\nwrote:\n\n> @girving https://github.com/girving @yuanbyu\n> https://github.com/yuanbyu\n> \n> So I've discovered that training on the CPU only and clocked at 70ms per\n> training step versus 180ms per training step.\n> \n> Is this expected behavior? Are LSTMs inherently faster on a CPU?\n> \n> Note that the GradLoopState is still an issue and this is still half the\n> speed that pycnn trains the same model.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-240552898,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim08QhHEDV4jviZly9jnBxscqyclOks5qg3u1gaJpZM4Jhn6r\n> .\n"}