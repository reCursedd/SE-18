{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738", "id": 170529634, "node_id": "MDU6SXNzdWUxNzA1Mjk2MzQ=", "number": 3738, "title": "Bidirectional_dynamic_rnn in while_loop results in: AttributeError: 'GradLoopState' object has no attribute 'forward_ctxt'", "user": {"login": "msevrens", "id": 1441846, "node_id": "MDQ6VXNlcjE0NDE4NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1441846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msevrens", "html_url": "https://github.com/msevrens", "followers_url": "https://api.github.com/users/msevrens/followers", "following_url": "https://api.github.com/users/msevrens/following{/other_user}", "gists_url": "https://api.github.com/users/msevrens/gists{/gist_id}", "starred_url": "https://api.github.com/users/msevrens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msevrens/subscriptions", "organizations_url": "https://api.github.com/users/msevrens/orgs", "repos_url": "https://api.github.com/users/msevrens/repos", "events_url": "https://api.github.com/users/msevrens/events{/privacy}", "received_events_url": "https://api.github.com/users/msevrens/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 47, "created_at": "2016-08-10T22:02:41Z", "updated_at": "2016-09-10T22:58:22Z", "closed_at": "2016-09-10T22:58:22Z", "author_association": "NONE", "body_html": "<p>I'm attempting to recreate this <a href=\"https://github.com/bplank/bilstm-aux\">model</a> in TensorFlow but I can not seem to get anywhere close to the speed of the original code. On average the TensorFlow implementation is taking 60 ms per document on a GPU, while the pycnn is taking 12 ms per document on a CPU.</p>\n<p>What appears to be the issue is that _rnn_step will call the LSTM cell on every time step regardless of whether the input at that position is padding or not and then simply fill those values with zeros after the fact using sequence_length. This seems like a lot of unnecessary calculations.</p>\n<p>Processing the tokens sequentially in multiple session.run calls with one token per batch in a similar performance to the original code but the embeddings do not update as the flow is broken.</p>\n<p>Current working (but slow) code is below.</p>\n<pre><code>def char_encoding(config, graph, doc_len):\n    \"\"\"Create graph nodes for character encoding\"\"\"\n\n    c2i = config[\"c2i\"]\n    max_tokens = config[\"max_tokens\"]\n\n    with graph.as_default():\n\n        # Character Embedding\n        word_lengths = tf.placeholder(tf.int64, [None], name=\"word_lengths\")\n        word_lengths = tf.gather(word_lengths, tf.range(tf.to_int32(doc_len)))\n        char_inputs = tf.placeholder(tf.int32, [None, max_tokens], name=\"char_inputs\")\n        cembed_matrix = tf.Variable(tf.random_uniform([len(c2i.keys()), config[\"ce_dim\"]], -0.25, 0.25), name=\"cembeds\")\n\n        char_inputs = tf.transpose(char_inputs, perm=[1,0])\n        cembeds = tf.nn.embedding_lookup(cembed_matrix, char_inputs, name=\"ce_lookup\")\n        cembeds = tf.gather(cembeds, tf.range(tf.to_int32(doc_len)))\n        cembeds = tf.transpose(cembeds, perm=[1,0,2])\n\n        # Create LSTM for Character Encoding\n        fw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n        bw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n\n        # Encode Characters with LSTM\n        options = {\n            \"dtype\": tf.float32,\n            \"sequence_length\": word_lengths,\n            \"time_major\": True\n        }\n\n        (output_fw, output_bw), output_states = tf.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, cembeds, **options)\n        output_fw = tf.transpose(output_fw, perm=[1,0,2])\n        output_bw = tf.transpose(output_bw, perm=[1,0,2])\n\n        return output_fw, output_bw, word_lengths\n\n</code></pre>\n<p>An alternate approach where I loop through the tokens in the graph itself seems like it should be faster but is throwing an odd error:</p>\n<p>Code:</p>\n<pre><code>def char_encoding(config, graph, doc_len):\n    \"\"\"Create graph nodes for character encoding\"\"\"\n\n    c2i = config[\"c2i\"]\n    max_tokens = config[\"max_tokens\"]\n\n    with graph.as_default():\n\n        # Character Embedding\n        word_lengths = tf.placeholder(tf.int64, [None], name=\"word_lengths\")\n        word_lengths = tf.gather(word_lengths, tf.range(tf.to_int32(doc_len)))\n        char_inputs = tf.placeholder(tf.int32, [None, max_tokens], name=\"char_inputs\")\n        char_inputs = tf.transpose(char_inputs, perm=[1, 0])\n        cembed_matrix = tf.Variable(tf.random_uniform([len(c2i.keys()), config[\"ce_dim\"]], -0.25, 0.25), name=\"cembeds\")\n\n        # Create LSTM for Character Encoding\n        fw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n        bw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n\n        def one_pass(i, o_fw, o_bw):\n\n            int_i = tf.to_int32(i)\n\n            options = {\n                \"dtype\": tf.float32,\n                \"sequence_length\": tf.expand_dims(tf.gather(word_lengths, int_i), 0),\n                \"time_major\": True\n            }\n\n            cembeds_invert = tf.nn.embedding_lookup(cembed_matrix, tf.gather(char_inputs, int_i))\n            cembeds_invert = tf.transpose(tf.expand_dims(cembeds_invert, 0), perm=[1,0,2])\n\n            (output_fw, output_bw), output_states = tf.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, cembeds_invert, **options)\n\n            # Get Last Relevant\n            output_fw = tf.gather(output_fw, tf.gather(word_lengths, int_i) - 1)\n            output_bw = tf.gather(output_bw, tf.gather(word_lengths, int_i) - 1)\n\n            # Append to Previous Token Encodings\n            o_fw = o_fw.write(int_i, tf.squeeze(output_fw))\n            o_bw = o_bw.write(int_i, tf.squeeze(output_bw))\n\n            return tf.add(i, 1), o_fw, o_bw\n\n        # Build Loop in Graph\n        i = tf.constant(0.0)\n        float_doc_len = tf.to_float(doc_len)\n        o_fw = tensor_array_ops.TensorArray(dtype=tf.float32, size=tf.to_int32(doc_len))\n        o_bw = tensor_array_ops.TensorArray(dtype=tf.float32, size=tf.to_int32(doc_len))\n        cond = lambda i, *_: tf.less(i, float_doc_len)\n        i, char_embeds, rev_char_embeds = tf.while_loop(cond, one_pass, [i, o_fw, o_bw])\n\n        return char_embeds.pack(), rev_char_embeds.pack()\n</code></pre>\n<p>Error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 445, in &lt;module&gt;\n    run_from_command_line()\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 441, in run_from_command_line\n    graph, saver = build_graph(config)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 311, in build_graph\n    optimizer = tf.train.GradientDescentOptimizer(config[\"learning_rate\"]).minimize(loss, name=\"optimizer\")\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_grad.py\", line 202, in _EnterGrad\n    result = grad_ctxt.AddBackPropAccumulator(op, grad)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1646, in AddBackPropAccumulator\n    forward_ctxt = self.grad_state.forward_ctxt\nAttributeError: 'GradLoopState' object has no attribute 'forward_ctxt'\n</code></pre>\n<p>Let me know if seeing the full graph will help at all. Right now that higher level LSTM isn't causing issues because I can just run one document at a time.</p>", "body_text": "I'm attempting to recreate this model in TensorFlow but I can not seem to get anywhere close to the speed of the original code. On average the TensorFlow implementation is taking 60 ms per document on a GPU, while the pycnn is taking 12 ms per document on a CPU.\nWhat appears to be the issue is that _rnn_step will call the LSTM cell on every time step regardless of whether the input at that position is padding or not and then simply fill those values with zeros after the fact using sequence_length. This seems like a lot of unnecessary calculations.\nProcessing the tokens sequentially in multiple session.run calls with one token per batch in a similar performance to the original code but the embeddings do not update as the flow is broken.\nCurrent working (but slow) code is below.\ndef char_encoding(config, graph, doc_len):\n    \"\"\"Create graph nodes for character encoding\"\"\"\n\n    c2i = config[\"c2i\"]\n    max_tokens = config[\"max_tokens\"]\n\n    with graph.as_default():\n\n        # Character Embedding\n        word_lengths = tf.placeholder(tf.int64, [None], name=\"word_lengths\")\n        word_lengths = tf.gather(word_lengths, tf.range(tf.to_int32(doc_len)))\n        char_inputs = tf.placeholder(tf.int32, [None, max_tokens], name=\"char_inputs\")\n        cembed_matrix = tf.Variable(tf.random_uniform([len(c2i.keys()), config[\"ce_dim\"]], -0.25, 0.25), name=\"cembeds\")\n\n        char_inputs = tf.transpose(char_inputs, perm=[1,0])\n        cembeds = tf.nn.embedding_lookup(cembed_matrix, char_inputs, name=\"ce_lookup\")\n        cembeds = tf.gather(cembeds, tf.range(tf.to_int32(doc_len)))\n        cembeds = tf.transpose(cembeds, perm=[1,0,2])\n\n        # Create LSTM for Character Encoding\n        fw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n        bw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n\n        # Encode Characters with LSTM\n        options = {\n            \"dtype\": tf.float32,\n            \"sequence_length\": word_lengths,\n            \"time_major\": True\n        }\n\n        (output_fw, output_bw), output_states = tf.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, cembeds, **options)\n        output_fw = tf.transpose(output_fw, perm=[1,0,2])\n        output_bw = tf.transpose(output_bw, perm=[1,0,2])\n\n        return output_fw, output_bw, word_lengths\n\n\nAn alternate approach where I loop through the tokens in the graph itself seems like it should be faster but is throwing an odd error:\nCode:\ndef char_encoding(config, graph, doc_len):\n    \"\"\"Create graph nodes for character encoding\"\"\"\n\n    c2i = config[\"c2i\"]\n    max_tokens = config[\"max_tokens\"]\n\n    with graph.as_default():\n\n        # Character Embedding\n        word_lengths = tf.placeholder(tf.int64, [None], name=\"word_lengths\")\n        word_lengths = tf.gather(word_lengths, tf.range(tf.to_int32(doc_len)))\n        char_inputs = tf.placeholder(tf.int32, [None, max_tokens], name=\"char_inputs\")\n        char_inputs = tf.transpose(char_inputs, perm=[1, 0])\n        cembed_matrix = tf.Variable(tf.random_uniform([len(c2i.keys()), config[\"ce_dim\"]], -0.25, 0.25), name=\"cembeds\")\n\n        # Create LSTM for Character Encoding\n        fw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n        bw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n\n        def one_pass(i, o_fw, o_bw):\n\n            int_i = tf.to_int32(i)\n\n            options = {\n                \"dtype\": tf.float32,\n                \"sequence_length\": tf.expand_dims(tf.gather(word_lengths, int_i), 0),\n                \"time_major\": True\n            }\n\n            cembeds_invert = tf.nn.embedding_lookup(cembed_matrix, tf.gather(char_inputs, int_i))\n            cembeds_invert = tf.transpose(tf.expand_dims(cembeds_invert, 0), perm=[1,0,2])\n\n            (output_fw, output_bw), output_states = tf.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, cembeds_invert, **options)\n\n            # Get Last Relevant\n            output_fw = tf.gather(output_fw, tf.gather(word_lengths, int_i) - 1)\n            output_bw = tf.gather(output_bw, tf.gather(word_lengths, int_i) - 1)\n\n            # Append to Previous Token Encodings\n            o_fw = o_fw.write(int_i, tf.squeeze(output_fw))\n            o_bw = o_bw.write(int_i, tf.squeeze(output_bw))\n\n            return tf.add(i, 1), o_fw, o_bw\n\n        # Build Loop in Graph\n        i = tf.constant(0.0)\n        float_doc_len = tf.to_float(doc_len)\n        o_fw = tensor_array_ops.TensorArray(dtype=tf.float32, size=tf.to_int32(doc_len))\n        o_bw = tensor_array_ops.TensorArray(dtype=tf.float32, size=tf.to_int32(doc_len))\n        cond = lambda i, *_: tf.less(i, float_doc_len)\n        i, char_embeds, rev_char_embeds = tf.while_loop(cond, one_pass, [i, o_fw, o_bw])\n\n        return char_embeds.pack(), rev_char_embeds.pack()\n\nError:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 445, in <module>\n    run_from_command_line()\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 441, in run_from_command_line\n    graph, saver = build_graph(config)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 311, in build_graph\n    optimizer = tf.train.GradientDescentOptimizer(config[\"learning_rate\"]).minimize(loss, name=\"optimizer\")\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_grad.py\", line 202, in _EnterGrad\n    result = grad_ctxt.AddBackPropAccumulator(op, grad)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1646, in AddBackPropAccumulator\n    forward_ctxt = self.grad_state.forward_ctxt\nAttributeError: 'GradLoopState' object has no attribute 'forward_ctxt'\n\nLet me know if seeing the full graph will help at all. Right now that higher level LSTM isn't causing issues because I can just run one document at a time.", "body": "I'm attempting to recreate this [model](https://github.com/bplank/bilstm-aux) in TensorFlow but I can not seem to get anywhere close to the speed of the original code. On average the TensorFlow implementation is taking 60 ms per document on a GPU, while the pycnn is taking 12 ms per document on a CPU.\n\nWhat appears to be the issue is that _rnn_step will call the LSTM cell on every time step regardless of whether the input at that position is padding or not and then simply fill those values with zeros after the fact using sequence_length. This seems like a lot of unnecessary calculations. \n\nProcessing the tokens sequentially in multiple session.run calls with one token per batch in a similar performance to the original code but the embeddings do not update as the flow is broken.\n\nCurrent working (but slow) code is below.\n\n```\ndef char_encoding(config, graph, doc_len):\n    \"\"\"Create graph nodes for character encoding\"\"\"\n\n    c2i = config[\"c2i\"]\n    max_tokens = config[\"max_tokens\"]\n\n    with graph.as_default():\n\n        # Character Embedding\n        word_lengths = tf.placeholder(tf.int64, [None], name=\"word_lengths\")\n        word_lengths = tf.gather(word_lengths, tf.range(tf.to_int32(doc_len)))\n        char_inputs = tf.placeholder(tf.int32, [None, max_tokens], name=\"char_inputs\")\n        cembed_matrix = tf.Variable(tf.random_uniform([len(c2i.keys()), config[\"ce_dim\"]], -0.25, 0.25), name=\"cembeds\")\n\n        char_inputs = tf.transpose(char_inputs, perm=[1,0])\n        cembeds = tf.nn.embedding_lookup(cembed_matrix, char_inputs, name=\"ce_lookup\")\n        cembeds = tf.gather(cembeds, tf.range(tf.to_int32(doc_len)))\n        cembeds = tf.transpose(cembeds, perm=[1,0,2])\n\n        # Create LSTM for Character Encoding\n        fw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n        bw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n\n        # Encode Characters with LSTM\n        options = {\n            \"dtype\": tf.float32,\n            \"sequence_length\": word_lengths,\n            \"time_major\": True\n        }\n\n        (output_fw, output_bw), output_states = tf.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, cembeds, **options)\n        output_fw = tf.transpose(output_fw, perm=[1,0,2])\n        output_bw = tf.transpose(output_bw, perm=[1,0,2])\n\n        return output_fw, output_bw, word_lengths\n\n```\n\nAn alternate approach where I loop through the tokens in the graph itself seems like it should be faster but is throwing an odd error: \n\nCode:\n\n```\ndef char_encoding(config, graph, doc_len):\n    \"\"\"Create graph nodes for character encoding\"\"\"\n\n    c2i = config[\"c2i\"]\n    max_tokens = config[\"max_tokens\"]\n\n    with graph.as_default():\n\n        # Character Embedding\n        word_lengths = tf.placeholder(tf.int64, [None], name=\"word_lengths\")\n        word_lengths = tf.gather(word_lengths, tf.range(tf.to_int32(doc_len)))\n        char_inputs = tf.placeholder(tf.int32, [None, max_tokens], name=\"char_inputs\")\n        char_inputs = tf.transpose(char_inputs, perm=[1, 0])\n        cembed_matrix = tf.Variable(tf.random_uniform([len(c2i.keys()), config[\"ce_dim\"]], -0.25, 0.25), name=\"cembeds\")\n\n        # Create LSTM for Character Encoding\n        fw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n        bw_lstm = tf.nn.rnn_cell.BasicLSTMCell(config[\"ce_dim\"], state_is_tuple=True)\n\n        def one_pass(i, o_fw, o_bw):\n\n            int_i = tf.to_int32(i)\n\n            options = {\n                \"dtype\": tf.float32,\n                \"sequence_length\": tf.expand_dims(tf.gather(word_lengths, int_i), 0),\n                \"time_major\": True\n            }\n\n            cembeds_invert = tf.nn.embedding_lookup(cembed_matrix, tf.gather(char_inputs, int_i))\n            cembeds_invert = tf.transpose(tf.expand_dims(cembeds_invert, 0), perm=[1,0,2])\n\n            (output_fw, output_bw), output_states = tf.nn.bidirectional_dynamic_rnn(fw_lstm, bw_lstm, cembeds_invert, **options)\n\n            # Get Last Relevant\n            output_fw = tf.gather(output_fw, tf.gather(word_lengths, int_i) - 1)\n            output_bw = tf.gather(output_bw, tf.gather(word_lengths, int_i) - 1)\n\n            # Append to Previous Token Encodings\n            o_fw = o_fw.write(int_i, tf.squeeze(output_fw))\n            o_bw = o_bw.write(int_i, tf.squeeze(output_bw))\n\n            return tf.add(i, 1), o_fw, o_bw\n\n        # Build Loop in Graph\n        i = tf.constant(0.0)\n        float_doc_len = tf.to_float(doc_len)\n        o_fw = tensor_array_ops.TensorArray(dtype=tf.float32, size=tf.to_int32(doc_len))\n        o_bw = tensor_array_ops.TensorArray(dtype=tf.float32, size=tf.to_int32(doc_len))\n        cond = lambda i, *_: tf.less(i, float_doc_len)\n        i, char_embeds, rev_char_embeds = tf.while_loop(cond, one_pass, [i, o_fw, o_bw])\n\n        return char_embeds.pack(), rev_char_embeds.pack()\n```\n\nError:\n\n```\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 445, in <module>\n    run_from_command_line()\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 441, in run_from_command_line\n    graph, saver = build_graph(config)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 311, in build_graph\n    optimizer = tf.train.GradientDescentOptimizer(config[\"learning_rate\"]).minimize(loss, name=\"optimizer\")\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_grad.py\", line 202, in _EnterGrad\n    result = grad_ctxt.AddBackPropAccumulator(op, grad)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1646, in AddBackPropAccumulator\n    forward_ctxt = self.grad_state.forward_ctxt\nAttributeError: 'GradLoopState' object has no attribute 'forward_ctxt'\n```\n\nLet me know if seeing the full graph will help at all. Right now that higher level LSTM isn't causing issues because I can just run one document at a time.\n"}