{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/240477764", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-240477764", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 240477764, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MDQ3Nzc2NA==", "user": {"login": "msevrens", "id": 1441846, "node_id": "MDQ6VXNlcjE0NDE4NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1441846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msevrens", "html_url": "https://github.com/msevrens", "followers_url": "https://api.github.com/users/msevrens/followers", "following_url": "https://api.github.com/users/msevrens/following{/other_user}", "gists_url": "https://api.github.com/users/msevrens/gists{/gist_id}", "starred_url": "https://api.github.com/users/msevrens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msevrens/subscriptions", "organizations_url": "https://api.github.com/users/msevrens/orgs", "repos_url": "https://api.github.com/users/msevrens/repos", "events_url": "https://api.github.com/users/msevrens/events{/privacy}", "received_events_url": "https://api.github.com/users/msevrens/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-17T17:02:58Z", "updated_at": "2016-08-17T17:02:58Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a><br>\nIn reality if you look at _dynamic_rnn_loop all values at each time step are passed to the LSTM cell. The forward pass is then applied to everything regardless of the sequence_length provided.</p>\n<pre><code>input_t = tuple(ta.read(time) for ta in input_ta)\ncall_cell = lambda: cell(input_t, state)\n</code></pre>\n<p>Therefore, a max_time_step of 100 with all documents having 100 length with take the same amount of time as a max_time_step of 100 with all documents having a length of 20. The only thing sequence_length is used for is to copy in filler zeros AFTER calculation has already occurred.</p>\n<p>Seeing as pycnn does the same task on the CPU five times faster than TF does on the GPU shows there's definitely room from improvement here. And I would imagine actually having the \"computation exit early\" would be the first step. Though as I showed above, simply not calling the cell at every step during the forward pass does little to help. The gradient calculation and application is still a bit spaghetti to me so I can't comment on that yet.</p>", "body_text": "@ebrevdo\nIn reality if you look at _dynamic_rnn_loop all values at each time step are passed to the LSTM cell. The forward pass is then applied to everything regardless of the sequence_length provided.\ninput_t = tuple(ta.read(time) for ta in input_ta)\ncall_cell = lambda: cell(input_t, state)\n\nTherefore, a max_time_step of 100 with all documents having 100 length with take the same amount of time as a max_time_step of 100 with all documents having a length of 20. The only thing sequence_length is used for is to copy in filler zeros AFTER calculation has already occurred.\nSeeing as pycnn does the same task on the CPU five times faster than TF does on the GPU shows there's definitely room from improvement here. And I would imagine actually having the \"computation exit early\" would be the first step. Though as I showed above, simply not calling the cell at every step during the forward pass does little to help. The gradient calculation and application is still a bit spaghetti to me so I can't comment on that yet.", "body": "@ebrevdo \nIn reality if you look at _dynamic_rnn_loop all values at each time step are passed to the LSTM cell. The forward pass is then applied to everything regardless of the sequence_length provided.\n\n```\ninput_t = tuple(ta.read(time) for ta in input_ta)\ncall_cell = lambda: cell(input_t, state)\n```\n\nTherefore, a max_time_step of 100 with all documents having 100 length with take the same amount of time as a max_time_step of 100 with all documents having a length of 20. The only thing sequence_length is used for is to copy in filler zeros AFTER calculation has already occurred.\n\nSeeing as pycnn does the same task on the CPU five times faster than TF does on the GPU shows there's definitely room from improvement here. And I would imagine actually having the \"computation exit early\" would be the first step. Though as I showed above, simply not calling the cell at every step during the forward pass does little to help. The gradient calculation and application is still a bit spaghetti to me so I can't comment on that yet.\n"}