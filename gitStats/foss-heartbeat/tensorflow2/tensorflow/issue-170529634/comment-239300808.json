{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239300808", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-239300808", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 239300808, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTMwMDgwOA==", "user": {"login": "msevrens", "id": 1441846, "node_id": "MDQ6VXNlcjE0NDE4NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1441846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msevrens", "html_url": "https://github.com/msevrens", "followers_url": "https://api.github.com/users/msevrens/followers", "following_url": "https://api.github.com/users/msevrens/following{/other_user}", "gists_url": "https://api.github.com/users/msevrens/gists{/gist_id}", "starred_url": "https://api.github.com/users/msevrens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msevrens/subscriptions", "organizations_url": "https://api.github.com/users/msevrens/orgs", "repos_url": "https://api.github.com/users/msevrens/repos", "events_url": "https://api.github.com/users/msevrens/events{/privacy}", "received_events_url": "https://api.github.com/users/msevrens/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-11T21:37:34Z", "updated_at": "2016-08-11T21:41:35Z", "author_association": "NONE", "body_html": "<p>Pycnn isn't documented as well, but the original code at <a href=\"https://github.com/bplank/bilstm-aux\">https://github.com/bplank/bilstm-aux</a> simply iterates through each token and then concats the output to the word embeddings.</p>\n<pre><code>class RNNSequencePredictor(SequencePredictor):\n    def __init__(self, rnn_builder):\n        \"\"\"\n        rnn_builder: a LSTMBuilder or SimpleRNNBuilder object.\n        \"\"\"\n        self.builder = rnn_builder\n\n    def predict_sequence(self, inputs):\n        s_init = self.builder.initial_state()\n        return [x.output() for x in s_init.add_inputs(inputs)] #quicker version\n\nchar_rnn = RNNSequencePredictor(pycnn.LSTMBuilder(1, self.c_in_dim, self.c_in_dim, self.model))\n</code></pre>\n<pre><code>for chars_of_token in char_indices:\n            # use last state as word representation\n            last_state = self.char_rnn.predict_sequence([self.cembeds[c] for c in chars_of_token])[-1]\n            rev_last_state = self.char_rnn.predict_sequence([self.cembeds[c] for c in reversed(chars_of_token)])[-1]\n            char_emb.append(last_state)\n            rev_char_emb.append(rev_last_state)\n\n        wfeatures = [self.wembeds[w] for w in word_indices]\n        features = [pycnn.concatenate([w,c,rev_c]) for w,c,rev_c in zip(wfeatures,char_emb,reversed(rev_char_emb))]\n</code></pre>\n<p>I posted a similar approach in TensorFlow above but got the following error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 445, in &lt;module&gt;\n    run_from_command_line()\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 441, in run_from_command_line\n    graph, saver = build_graph(config)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 311, in build_graph\n    optimizer = tf.train.GradientDescentOptimizer(config[\"learning_rate\"]).minimize(loss, name=\"optimizer\")\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_grad.py\", line 202, in _EnterGrad\n    result = grad_ctxt.AddBackPropAccumulator(op, grad)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1646, in AddBackPropAccumulator\n    forward_ctxt = self.grad_state.forward_ctxt\nAttributeError: 'GradLoopState' object has no attribute 'forward_ctxt'\n</code></pre>\n<p>I've solved on how to run the RNN while only calling the cell on the relevant characters (can post code if necessary) but the issue persists so it seems the fundamental issue is the time that back propagation takes rather than the time it takes to do the forward pass.</p>", "body_text": "Pycnn isn't documented as well, but the original code at https://github.com/bplank/bilstm-aux simply iterates through each token and then concats the output to the word embeddings.\nclass RNNSequencePredictor(SequencePredictor):\n    def __init__(self, rnn_builder):\n        \"\"\"\n        rnn_builder: a LSTMBuilder or SimpleRNNBuilder object.\n        \"\"\"\n        self.builder = rnn_builder\n\n    def predict_sequence(self, inputs):\n        s_init = self.builder.initial_state()\n        return [x.output() for x in s_init.add_inputs(inputs)] #quicker version\n\nchar_rnn = RNNSequencePredictor(pycnn.LSTMBuilder(1, self.c_in_dim, self.c_in_dim, self.model))\n\nfor chars_of_token in char_indices:\n            # use last state as word representation\n            last_state = self.char_rnn.predict_sequence([self.cembeds[c] for c in chars_of_token])[-1]\n            rev_last_state = self.char_rnn.predict_sequence([self.cembeds[c] for c in reversed(chars_of_token)])[-1]\n            char_emb.append(last_state)\n            rev_char_emb.append(rev_last_state)\n\n        wfeatures = [self.wembeds[w] for w in word_indices]\n        features = [pycnn.concatenate([w,c,rev_c]) for w,c,rev_c in zip(wfeatures,char_emb,reversed(rev_char_emb))]\n\nI posted a similar approach in TensorFlow above but got the following error:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 445, in <module>\n    run_from_command_line()\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 441, in run_from_command_line\n    graph, saver = build_graph(config)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 311, in build_graph\n    optimizer = tf.train.GradientDescentOptimizer(config[\"learning_rate\"]).minimize(loss, name=\"optimizer\")\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_grad.py\", line 202, in _EnterGrad\n    result = grad_ctxt.AddBackPropAccumulator(op, grad)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1646, in AddBackPropAccumulator\n    forward_ctxt = self.grad_state.forward_ctxt\nAttributeError: 'GradLoopState' object has no attribute 'forward_ctxt'\n\nI've solved on how to run the RNN while only calling the cell on the relevant characters (can post code if necessary) but the issue persists so it seems the fundamental issue is the time that back propagation takes rather than the time it takes to do the forward pass.", "body": "Pycnn isn't documented as well, but the original code at https://github.com/bplank/bilstm-aux simply iterates through each token and then concats the output to the word embeddings. \n\n```\nclass RNNSequencePredictor(SequencePredictor):\n    def __init__(self, rnn_builder):\n        \"\"\"\n        rnn_builder: a LSTMBuilder or SimpleRNNBuilder object.\n        \"\"\"\n        self.builder = rnn_builder\n\n    def predict_sequence(self, inputs):\n        s_init = self.builder.initial_state()\n        return [x.output() for x in s_init.add_inputs(inputs)] #quicker version\n\nchar_rnn = RNNSequencePredictor(pycnn.LSTMBuilder(1, self.c_in_dim, self.c_in_dim, self.model))\n```\n\n```\nfor chars_of_token in char_indices:\n            # use last state as word representation\n            last_state = self.char_rnn.predict_sequence([self.cembeds[c] for c in chars_of_token])[-1]\n            rev_last_state = self.char_rnn.predict_sequence([self.cembeds[c] for c in reversed(chars_of_token)])[-1]\n            char_emb.append(last_state)\n            rev_char_emb.append(rev_last_state)\n\n        wfeatures = [self.wembeds[w] for w in word_indices]\n        features = [pycnn.concatenate([w,c,rev_c]) for w,c,rev_c in zip(wfeatures,char_emb,reversed(rev_char_emb))]\n```\n\nI posted a similar approach in TensorFlow above but got the following error:\n\n```\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 445, in <module>\n    run_from_command_line()\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 441, in run_from_command_line\n    graph, saver = build_graph(config)\n  File \"/home/ubuntu/git/Meerkat/meerkat/longtail/bilstm_tagger.py\", line 311, in build_graph\n    optimizer = tf.train.GradientDescentOptimizer(config[\"learning_rate\"]).minimize(loss, name=\"optimizer\")\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_grad.py\", line 202, in _EnterGrad\n    result = grad_ctxt.AddBackPropAccumulator(op, grad)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1646, in AddBackPropAccumulator\n    forward_ctxt = self.grad_state.forward_ctxt\nAttributeError: 'GradLoopState' object has no attribute 'forward_ctxt'\n```\n\nI've solved on how to run the RNN while only calling the cell on the relevant characters (can post code if necessary) but the issue persists so it seems the fundamental issue is the time that back propagation takes rather than the time it takes to do the forward pass.\n"}