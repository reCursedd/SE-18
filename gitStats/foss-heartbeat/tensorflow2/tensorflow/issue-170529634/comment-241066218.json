{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241066218", "html_url": "https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-241066218", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3738", "id": 241066218, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTA2NjIxOA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-19T16:29:08Z", "updated_at": "2016-08-19T16:29:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1441846\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/msevrens\">@msevrens</a> now you are really at the fundamental core of how to deal with RNNs and minibatches.  unfortunately this is out of the hands of e.g. <code>dynamic_rnn</code> and has to be handled by the preprocessing technique.</p>\n<p>there are two ways to solve this problem.  one is called truncated back propagation through time, and works on models where you have a loss at every time step (e.g. cross entropy loss).  for this we just added a contrib method called <code>tf.contrib.training.batch_sequence_with_states</code>.  You will have to look at truncated BPTT literature and the documentation for that algorithm in depth to understand what it's doing.</p>\n<p>The other approach is called bucketing, and the idea there is, when you read your examples in, group them according to similar lengths and put them in the appropriate queue.  The training step reads off the next available queue and processes the minibatch accordingly.  Each minibatch will contain sequences of approximately the same length.  I plan to make something like this available in contrib very soon.  Keep in mind this approach breaks the <code>independent</code> part of <code>i.i.d.</code> when training SGD minibatches; but in practice it works pretty well!</p>", "body_text": "@msevrens now you are really at the fundamental core of how to deal with RNNs and minibatches.  unfortunately this is out of the hands of e.g. dynamic_rnn and has to be handled by the preprocessing technique.\nthere are two ways to solve this problem.  one is called truncated back propagation through time, and works on models where you have a loss at every time step (e.g. cross entropy loss).  for this we just added a contrib method called tf.contrib.training.batch_sequence_with_states.  You will have to look at truncated BPTT literature and the documentation for that algorithm in depth to understand what it's doing.\nThe other approach is called bucketing, and the idea there is, when you read your examples in, group them according to similar lengths and put them in the appropriate queue.  The training step reads off the next available queue and processes the minibatch accordingly.  Each minibatch will contain sequences of approximately the same length.  I plan to make something like this available in contrib very soon.  Keep in mind this approach breaks the independent part of i.i.d. when training SGD minibatches; but in practice it works pretty well!", "body": "@msevrens now you are really at the fundamental core of how to deal with RNNs and minibatches.  unfortunately this is out of the hands of e.g. `dynamic_rnn` and has to be handled by the preprocessing technique.\n\nthere are two ways to solve this problem.  one is called truncated back propagation through time, and works on models where you have a loss at every time step (e.g. cross entropy loss).  for this we just added a contrib method called `tf.contrib.training.batch_sequence_with_states`.  You will have to look at truncated BPTT literature and the documentation for that algorithm in depth to understand what it's doing.\n\nThe other approach is called bucketing, and the idea there is, when you read your examples in, group them according to similar lengths and put them in the appropriate queue.  The training step reads off the next available queue and processes the minibatch accordingly.  Each minibatch will contain sequences of approximately the same length.  I plan to make something like this available in contrib very soon.  Keep in mind this approach breaks the `independent` part of `i.i.d.` when training SGD minibatches; but in practice it works pretty well!\n"}