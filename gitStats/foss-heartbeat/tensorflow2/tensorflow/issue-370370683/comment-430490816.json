{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/430490816", "html_url": "https://github.com/tensorflow/tensorflow/pull/23002#issuecomment-430490816", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23002", "id": 430490816, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDQ5MDgxNg==", "user": {"login": "galv", "id": 4767568, "node_id": "MDQ6VXNlcjQ3Njc1Njg=", "avatar_url": "https://avatars0.githubusercontent.com/u/4767568?v=4", "gravatar_id": "", "url": "https://api.github.com/users/galv", "html_url": "https://github.com/galv", "followers_url": "https://api.github.com/users/galv/followers", "following_url": "https://api.github.com/users/galv/following{/other_user}", "gists_url": "https://api.github.com/users/galv/gists{/gist_id}", "starred_url": "https://api.github.com/users/galv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/galv/subscriptions", "organizations_url": "https://api.github.com/users/galv/orgs", "repos_url": "https://api.github.com/users/galv/repos", "events_url": "https://api.github.com/users/galv/events{/privacy}", "received_events_url": "https://api.github.com/users/galv/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T05:05:27Z", "updated_at": "2018-10-17T05:05:27Z", "author_association": "NONE", "body_html": "<p>As a suggestion, you could consider making this a separate python package, which depends on both tensorflow and apache arrow to build. The tooling around these kinds of python packages with binary dependencies is still very new, but doable. Tensorflow <em>does</em> export its dataset header files. They are not under the API stability guarantees, but they have been fairly stable from what I observed (two method signatures changed trivially in 1.10, I believe). OF course, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> knows more, as the author.</p>\n<p>I myself have made my own Dataset in an external project, using cmake. I don't have a ready-made example, but I know that arrow use cmake as well, so it is very easily doable to have this live separately from the tensorflow repo. You can follow up with me about that.</p>\n<p>I mention this because tensorflow/contrib is going away.</p>\n<p>Super cool work, by the way! I remember chatting with a Databricks engineer at the Spark Summit about how Arrow could be a great complement to Project Hydrogen.</p>", "body_text": "As a suggestion, you could consider making this a separate python package, which depends on both tensorflow and apache arrow to build. The tooling around these kinds of python packages with binary dependencies is still very new, but doable. Tensorflow does export its dataset header files. They are not under the API stability guarantees, but they have been fairly stable from what I observed (two method signatures changed trivially in 1.10, I believe). OF course, @mrry knows more, as the author.\nI myself have made my own Dataset in an external project, using cmake. I don't have a ready-made example, but I know that arrow use cmake as well, so it is very easily doable to have this live separately from the tensorflow repo. You can follow up with me about that.\nI mention this because tensorflow/contrib is going away.\nSuper cool work, by the way! I remember chatting with a Databricks engineer at the Spark Summit about how Arrow could be a great complement to Project Hydrogen.", "body": "As a suggestion, you could consider making this a separate python package, which depends on both tensorflow and apache arrow to build. The tooling around these kinds of python packages with binary dependencies is still very new, but doable. Tensorflow *does* export its dataset header files. They are not under the API stability guarantees, but they have been fairly stable from what I observed (two method signatures changed trivially in 1.10, I believe). OF course, @mrry knows more, as the author.\r\n\r\nI myself have made my own Dataset in an external project, using cmake. I don't have a ready-made example, but I know that arrow use cmake as well, so it is very easily doable to have this live separately from the tensorflow repo. You can follow up with me about that.\r\n\r\nI mention this because tensorflow/contrib is going away.\r\n\r\nSuper cool work, by the way! I remember chatting with a Databricks engineer at the Spark Summit about how Arrow could be a great complement to Project Hydrogen."}