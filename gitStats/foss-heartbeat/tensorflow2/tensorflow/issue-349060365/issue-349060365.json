{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21515", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21515/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21515/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21515/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21515", "id": 349060365, "node_id": "MDU6SXNzdWUzNDkwNjAzNjU=", "number": 21515, "title": "BUG: optimizer.compute_gradients() produces inconsistent gradient with the same training instance and label ", "user": {"login": "EdwardLin2014", "id": 8342812, "node_id": "MDQ6VXNlcjgzNDI4MTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/8342812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EdwardLin2014", "html_url": "https://github.com/EdwardLin2014", "followers_url": "https://api.github.com/users/EdwardLin2014/followers", "following_url": "https://api.github.com/users/EdwardLin2014/following{/other_user}", "gists_url": "https://api.github.com/users/EdwardLin2014/gists{/gist_id}", "starred_url": "https://api.github.com/users/EdwardLin2014/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EdwardLin2014/subscriptions", "organizations_url": "https://api.github.com/users/EdwardLin2014/orgs", "repos_url": "https://api.github.com/users/EdwardLin2014/repos", "events_url": "https://api.github.com/users/EdwardLin2014/events{/privacy}", "received_events_url": "https://api.github.com/users/EdwardLin2014/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": {"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-08-09T10:01:21Z", "updated_at": "2018-11-15T19:03:45Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu Server &amp; Window 10</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.8.0-0-g93bc2e2072 &amp; v1.9.0-0-g25c197e023</li>\n<li><strong>Python version</strong>: 3.6.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: V9.0.178</li>\n<li><strong>GPU model and memory</strong>: NVIDIA Tesla V100-SXM2-16GB &amp; NVIDIA GeForce GTX 1080 Ti 11GB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n#########################################################################\n## Define Tensorflow Wrapper\nprint('Define Tensorflow Wrapper......')\ndef weight_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial = tf.constant(0.2, shape=shape)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef initialize_variable(shape, name):\n    with tf.name_scope(name + '_layer'):\n        with tf.name_scope('weights'):\n            weights = weight_variable(shape)\n        with tf.name_scope('biases'):\n            biases = bias_variable([1,shape[3],1,1])\n        return weights, biases\n\ndef conv2dLayer(inputs, weights, biases, name, act=tf.nn.relu):\n    preactivate = tf.nn.conv2d(inputs, weights, strides=[1,1,1,1], padding='VALID', data_format=\"NCHW\", dilations=[1,1,1,1], name='conv2d') + biases\n    return act(preactivate, name='activation')\n\ndef max_pool_3x3(x, name):\n    return tf.nn.max_pool(x, ksize=[1, 1, 3, 3], strides=[1, 1, 3, 3], padding='VALID', data_format=\"NCHW\", name=name)\n    \n#########################################################################\n## Create Neural Network\nprint('Create Neural Network......')\n# Initial Trainable Parameters\nConv1W,Conv1b = initialize_variable([3,3,1,2],'Conv1')\nConv2W,Conv2b = initialize_variable([3,3,2,2],'Conv2')\nConv3W,Conv3b = initialize_variable([3,3,2,2],'Conv3')\nConv4W,Conv4b = initialize_variable([3,3,2,2],'Conv4')\nfc1W,fc1b = initialize_variable([39,14,2,372],'fc1')\nfc2W,fc2b = initialize_variable([1,1,372,372],'fc2')\nfc3W,fc3b = initialize_variable([1,1,372,1],'fc3')\n# Placeholder for input and output\nx = tf.placeholder(tf.float32, [None,1,372,None], name='xInput')\ny_ = tf.placeholder(tf.float32, [None,1,1,None], name='yInput')\n# Architecture\nConv1 = conv2dLayer(x,Conv1W,Conv1b,'Conv1')\nConv2 = conv2dLayer(Conv1,Conv2W,Conv2b,'Conv2')\nMaxPool1 = max_pool_3x3(Conv2,'MaxPool1')\nConv3 = conv2dLayer(MaxPool1,Conv3W,Conv3b,'Conv3')\nConv4 = conv2dLayer(Conv3,Conv4W,Conv4b,'Conv4')\nMaxPool2 = max_pool_3x3(Conv4,'MaxPool2')\nfc1 = conv2dLayer(MaxPool2,fc1W,fc1b,'fc1')\nfc2 = conv2dLayer(fc1,fc2W,fc2b,'fc2')\ny = conv2dLayer(fc2,fc3W,fc3b,'fc3',act=tf.identity)\n# Define loss\ncrossEntropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y, name='crossEntropy')\nlossValue = tf.reduce_mean(crossEntropy, name='lossValue')\n# Define optimizer\n#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.001,rho=0.95,epsilon=1e-08,use_locking=False,name='Adadelta')\n#optimizer = tf.train.AdagradOptimizer(learning_rate=0.01,initial_accumulator_value=0.1,use_locking=False,name='Adagrad')\n#optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\n#optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.95, name='nestrov', use_nesterov=True)\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01, name='GradientDescent')\ngvs = optimizer.compute_gradients(lossValue)\ntrain_step = optimizer.apply_gradients(gvs)\n\n#########################################################################\n## Illustrate compute_gradients() random issue\nprint('Load x and y input......')\ndata = np.load('testGrad.npz')\nxInput = data['x']\nyInput = data['y']\n\nprint(\"Session 1\")\nwith tf.Session() as sess1:\n    sess1.run(tf.global_variables_initializer())\n    lossValue_eval1, gvs_eval1 = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\n    sess1.run(train_step, feed_dict={x:xInput,y_:yInput})\n    lossValue1_eval1, gvs1_eval1, = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\n    sess1.close()\n\nprint(\"Session 2\")\nwith tf.Session() as sess2:\n    sess2.run(tf.global_variables_initializer())\n    lossValue_eval2, gvs_eval2 = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\n    sess2.run(train_step, feed_dict={x:xInput,y_:yInput})\n    lossValue1_eval2, gvs1_eval2, = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\n    sess2.close()\n\nprint('--------------------------------------------------')\nprint('Check whether they have the same values in each session')\nprint('Before Training: ')\nprint('lossValue? %r' % all(lossValue_eval1==lossValue_eval2))\ncheck_gvs_BT = np.zeros((14,2))\nfor i in range(14):\n    for j in range(2):\n        check_gvs_BT[i][j] = all(gvs_eval1[i][j] == gvs_eval2[i][j])\nprint('gvs? %r' % all(check_gvs_BT))\nprint('After Training: ')\nprint('lossValue? %r' % all(lossValue1_eval1==lossValue1_eval2))\ncheck_gvs_AT = np.zeros((14,2))\nfor i in range(14):\n    for j in range(2):\n        check_gvs_AT[i][j] = all(gvs1_eval1[i][j] == gvs1_eval2[i][j])\nprint('gvs? %r' % all(check_gvs_AT))\n\ntf.reset_default_graph()\n</code></pre>\n<h3>Describe the problem</h3>\n<p>By executing the above code with the fixed training instance and label (which you can download from <a href=\"https://www.dropbox.com/s/828v71z4tm399z2/testGrad.npz?dl=0\" rel=\"nofollow\">https://www.dropbox.com/s/828v71z4tm399z2/testGrad.npz?dl=0</a> ), we can see that the gradient value computed by compute_gradients(),  gvs_eval1[0][0][0,0,0,0] &amp; gvs_eval2[0][0][0,0,0,0] are not always the same in each session. This happens regardless which optimizer you use.</p>\n<p>Correct me if I have made some stupid mistake in this code. As I have tried simple network architecture (for example, training y = weight * x), and compute_gradients() seems to be working fine.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu Server & Window 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.8.0-0-g93bc2e2072 & v1.9.0-0-g25c197e023\nPython version: 3.6.2\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: V9.0.178\nGPU model and memory: NVIDIA Tesla V100-SXM2-16GB & NVIDIA GeForce GTX 1080 Ti 11GB\nExact command to reproduce:\n\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\n\n#########################################################################\n## Define Tensorflow Wrapper\nprint('Define Tensorflow Wrapper......')\ndef weight_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial = tf.constant(0.2, shape=shape)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef initialize_variable(shape, name):\n    with tf.name_scope(name + '_layer'):\n        with tf.name_scope('weights'):\n            weights = weight_variable(shape)\n        with tf.name_scope('biases'):\n            biases = bias_variable([1,shape[3],1,1])\n        return weights, biases\n\ndef conv2dLayer(inputs, weights, biases, name, act=tf.nn.relu):\n    preactivate = tf.nn.conv2d(inputs, weights, strides=[1,1,1,1], padding='VALID', data_format=\"NCHW\", dilations=[1,1,1,1], name='conv2d') + biases\n    return act(preactivate, name='activation')\n\ndef max_pool_3x3(x, name):\n    return tf.nn.max_pool(x, ksize=[1, 1, 3, 3], strides=[1, 1, 3, 3], padding='VALID', data_format=\"NCHW\", name=name)\n    \n#########################################################################\n## Create Neural Network\nprint('Create Neural Network......')\n# Initial Trainable Parameters\nConv1W,Conv1b = initialize_variable([3,3,1,2],'Conv1')\nConv2W,Conv2b = initialize_variable([3,3,2,2],'Conv2')\nConv3W,Conv3b = initialize_variable([3,3,2,2],'Conv3')\nConv4W,Conv4b = initialize_variable([3,3,2,2],'Conv4')\nfc1W,fc1b = initialize_variable([39,14,2,372],'fc1')\nfc2W,fc2b = initialize_variable([1,1,372,372],'fc2')\nfc3W,fc3b = initialize_variable([1,1,372,1],'fc3')\n# Placeholder for input and output\nx = tf.placeholder(tf.float32, [None,1,372,None], name='xInput')\ny_ = tf.placeholder(tf.float32, [None,1,1,None], name='yInput')\n# Architecture\nConv1 = conv2dLayer(x,Conv1W,Conv1b,'Conv1')\nConv2 = conv2dLayer(Conv1,Conv2W,Conv2b,'Conv2')\nMaxPool1 = max_pool_3x3(Conv2,'MaxPool1')\nConv3 = conv2dLayer(MaxPool1,Conv3W,Conv3b,'Conv3')\nConv4 = conv2dLayer(Conv3,Conv4W,Conv4b,'Conv4')\nMaxPool2 = max_pool_3x3(Conv4,'MaxPool2')\nfc1 = conv2dLayer(MaxPool2,fc1W,fc1b,'fc1')\nfc2 = conv2dLayer(fc1,fc2W,fc2b,'fc2')\ny = conv2dLayer(fc2,fc3W,fc3b,'fc3',act=tf.identity)\n# Define loss\ncrossEntropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y, name='crossEntropy')\nlossValue = tf.reduce_mean(crossEntropy, name='lossValue')\n# Define optimizer\n#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.001,rho=0.95,epsilon=1e-08,use_locking=False,name='Adadelta')\n#optimizer = tf.train.AdagradOptimizer(learning_rate=0.01,initial_accumulator_value=0.1,use_locking=False,name='Adagrad')\n#optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\n#optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.95, name='nestrov', use_nesterov=True)\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01, name='GradientDescent')\ngvs = optimizer.compute_gradients(lossValue)\ntrain_step = optimizer.apply_gradients(gvs)\n\n#########################################################################\n## Illustrate compute_gradients() random issue\nprint('Load x and y input......')\ndata = np.load('testGrad.npz')\nxInput = data['x']\nyInput = data['y']\n\nprint(\"Session 1\")\nwith tf.Session() as sess1:\n    sess1.run(tf.global_variables_initializer())\n    lossValue_eval1, gvs_eval1 = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\n    sess1.run(train_step, feed_dict={x:xInput,y_:yInput})\n    lossValue1_eval1, gvs1_eval1, = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\n    sess1.close()\n\nprint(\"Session 2\")\nwith tf.Session() as sess2:\n    sess2.run(tf.global_variables_initializer())\n    lossValue_eval2, gvs_eval2 = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\n    sess2.run(train_step, feed_dict={x:xInput,y_:yInput})\n    lossValue1_eval2, gvs1_eval2, = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\n    sess2.close()\n\nprint('--------------------------------------------------')\nprint('Check whether they have the same values in each session')\nprint('Before Training: ')\nprint('lossValue? %r' % all(lossValue_eval1==lossValue_eval2))\ncheck_gvs_BT = np.zeros((14,2))\nfor i in range(14):\n    for j in range(2):\n        check_gvs_BT[i][j] = all(gvs_eval1[i][j] == gvs_eval2[i][j])\nprint('gvs? %r' % all(check_gvs_BT))\nprint('After Training: ')\nprint('lossValue? %r' % all(lossValue1_eval1==lossValue1_eval2))\ncheck_gvs_AT = np.zeros((14,2))\nfor i in range(14):\n    for j in range(2):\n        check_gvs_AT[i][j] = all(gvs1_eval1[i][j] == gvs1_eval2[i][j])\nprint('gvs? %r' % all(check_gvs_AT))\n\ntf.reset_default_graph()\n\nDescribe the problem\nBy executing the above code with the fixed training instance and label (which you can download from https://www.dropbox.com/s/828v71z4tm399z2/testGrad.npz?dl=0 ), we can see that the gradient value computed by compute_gradients(),  gvs_eval1[0][0][0,0,0,0] & gvs_eval2[0][0][0,0,0,0] are not always the same in each session. This happens regardless which optimizer you use.\nCorrect me if I have made some stupid mistake in this code. As I have tried simple network architecture (for example, training y = weight * x), and compute_gradients() seems to be working fine.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu Server & Window 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 & v1.9.0-0-g25c197e023\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: V9.0.178\r\n- **GPU model and memory**: NVIDIA Tesla V100-SXM2-16GB & NVIDIA GeForce GTX 1080 Ti 11GB\r\n- **Exact command to reproduce**:\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n#########################################################################\r\n## Define Tensorflow Wrapper\r\nprint('Define Tensorflow Wrapper......')\r\ndef weight_variable(shape):\r\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n    initial = tf.constant(0.2, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n    initial = tf.constant(0.1, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef initialize_variable(shape, name):\r\n    with tf.name_scope(name + '_layer'):\r\n        with tf.name_scope('weights'):\r\n            weights = weight_variable(shape)\r\n        with tf.name_scope('biases'):\r\n            biases = bias_variable([1,shape[3],1,1])\r\n        return weights, biases\r\n\r\ndef conv2dLayer(inputs, weights, biases, name, act=tf.nn.relu):\r\n    preactivate = tf.nn.conv2d(inputs, weights, strides=[1,1,1,1], padding='VALID', data_format=\"NCHW\", dilations=[1,1,1,1], name='conv2d') + biases\r\n    return act(preactivate, name='activation')\r\n\r\ndef max_pool_3x3(x, name):\r\n    return tf.nn.max_pool(x, ksize=[1, 1, 3, 3], strides=[1, 1, 3, 3], padding='VALID', data_format=\"NCHW\", name=name)\r\n    \r\n#########################################################################\r\n## Create Neural Network\r\nprint('Create Neural Network......')\r\n# Initial Trainable Parameters\r\nConv1W,Conv1b = initialize_variable([3,3,1,2],'Conv1')\r\nConv2W,Conv2b = initialize_variable([3,3,2,2],'Conv2')\r\nConv3W,Conv3b = initialize_variable([3,3,2,2],'Conv3')\r\nConv4W,Conv4b = initialize_variable([3,3,2,2],'Conv4')\r\nfc1W,fc1b = initialize_variable([39,14,2,372],'fc1')\r\nfc2W,fc2b = initialize_variable([1,1,372,372],'fc2')\r\nfc3W,fc3b = initialize_variable([1,1,372,1],'fc3')\r\n# Placeholder for input and output\r\nx = tf.placeholder(tf.float32, [None,1,372,None], name='xInput')\r\ny_ = tf.placeholder(tf.float32, [None,1,1,None], name='yInput')\r\n# Architecture\r\nConv1 = conv2dLayer(x,Conv1W,Conv1b,'Conv1')\r\nConv2 = conv2dLayer(Conv1,Conv2W,Conv2b,'Conv2')\r\nMaxPool1 = max_pool_3x3(Conv2,'MaxPool1')\r\nConv3 = conv2dLayer(MaxPool1,Conv3W,Conv3b,'Conv3')\r\nConv4 = conv2dLayer(Conv3,Conv4W,Conv4b,'Conv4')\r\nMaxPool2 = max_pool_3x3(Conv4,'MaxPool2')\r\nfc1 = conv2dLayer(MaxPool2,fc1W,fc1b,'fc1')\r\nfc2 = conv2dLayer(fc1,fc2W,fc2b,'fc2')\r\ny = conv2dLayer(fc2,fc3W,fc3b,'fc3',act=tf.identity)\r\n# Define loss\r\ncrossEntropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y, name='crossEntropy')\r\nlossValue = tf.reduce_mean(crossEntropy, name='lossValue')\r\n# Define optimizer\r\n#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.001,rho=0.95,epsilon=1e-08,use_locking=False,name='Adadelta')\r\n#optimizer = tf.train.AdagradOptimizer(learning_rate=0.01,initial_accumulator_value=0.1,use_locking=False,name='Adagrad')\r\n#optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\r\n#optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.95, name='nestrov', use_nesterov=True)\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01, name='GradientDescent')\r\ngvs = optimizer.compute_gradients(lossValue)\r\ntrain_step = optimizer.apply_gradients(gvs)\r\n\r\n#########################################################################\r\n## Illustrate compute_gradients() random issue\r\nprint('Load x and y input......')\r\ndata = np.load('testGrad.npz')\r\nxInput = data['x']\r\nyInput = data['y']\r\n\r\nprint(\"Session 1\")\r\nwith tf.Session() as sess1:\r\n    sess1.run(tf.global_variables_initializer())\r\n    lossValue_eval1, gvs_eval1 = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\r\n    sess1.run(train_step, feed_dict={x:xInput,y_:yInput})\r\n    lossValue1_eval1, gvs1_eval1, = sess1.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\r\n    sess1.close()\r\n\r\nprint(\"Session 2\")\r\nwith tf.Session() as sess2:\r\n    sess2.run(tf.global_variables_initializer())\r\n    lossValue_eval2, gvs_eval2 = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\r\n    sess2.run(train_step, feed_dict={x:xInput,y_:yInput})\r\n    lossValue1_eval2, gvs1_eval2, = sess2.run([lossValue,gvs], feed_dict={x:xInput,y_:yInput})\r\n    sess2.close()\r\n\r\nprint('--------------------------------------------------')\r\nprint('Check whether they have the same values in each session')\r\nprint('Before Training: ')\r\nprint('lossValue? %r' % all(lossValue_eval1==lossValue_eval2))\r\ncheck_gvs_BT = np.zeros((14,2))\r\nfor i in range(14):\r\n    for j in range(2):\r\n        check_gvs_BT[i][j] = all(gvs_eval1[i][j] == gvs_eval2[i][j])\r\nprint('gvs? %r' % all(check_gvs_BT))\r\nprint('After Training: ')\r\nprint('lossValue? %r' % all(lossValue1_eval1==lossValue1_eval2))\r\ncheck_gvs_AT = np.zeros((14,2))\r\nfor i in range(14):\r\n    for j in range(2):\r\n        check_gvs_AT[i][j] = all(gvs1_eval1[i][j] == gvs1_eval2[i][j])\r\nprint('gvs? %r' % all(check_gvs_AT))\r\n\r\ntf.reset_default_graph()\r\n```\r\n\r\n### Describe the problem\r\nBy executing the above code with the fixed training instance and label (which you can download from https://www.dropbox.com/s/828v71z4tm399z2/testGrad.npz?dl=0 ), we can see that the gradient value computed by compute_gradients(),  gvs_eval1[0][0][0,0,0,0] & gvs_eval2[0][0][0,0,0,0] are not always the same in each session. This happens regardless which optimizer you use.\r\n\r\nCorrect me if I have made some stupid mistake in this code. As I have tried simple network architecture (for example, training y = weight * x), and compute_gradients() seems to be working fine. \r\n\r\n"}