{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253087712", "html_url": "https://github.com/tensorflow/tensorflow/pull/4899#issuecomment-253087712", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4899", "id": 253087712, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzA4NzcxMg==", "user": {"login": "thuyen", "id": 4015328, "node_id": "MDQ6VXNlcjQwMTUzMjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/4015328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thuyen", "html_url": "https://github.com/thuyen", "followers_url": "https://api.github.com/users/thuyen/followers", "following_url": "https://api.github.com/users/thuyen/following{/other_user}", "gists_url": "https://api.github.com/users/thuyen/gists{/gist_id}", "starred_url": "https://api.github.com/users/thuyen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thuyen/subscriptions", "organizations_url": "https://api.github.com/users/thuyen/orgs", "repos_url": "https://api.github.com/users/thuyen/repos", "events_url": "https://api.github.com/users/thuyen/events{/privacy}", "received_events_url": "https://api.github.com/users/thuyen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-12T00:43:02Z", "updated_at": "2016-10-12T01:04:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi Yao,</p>\n<p>You should see the error with this snippet:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib import slim\nimport numpy as np\nis_training = tf.placeholder(tf.bool)\ninputs = tf.constant(np.zeros((10, 3, 100, 100)).astype('float32'))\noutputs = slim.batch_norm(inputs, is_training=is_training, fused=True, data_format='NCHW')\nloss = tf.reduce_sum(outputs)\ngrad = tf.gradients(loss, tf.trainable_variables())\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\np = sess.run(grad, feed_dict={is_training:True})\n</code></pre>\n<p>It's like:</p>\n<pre><code>File \"..../tensorflow/python/ops/gradients.py\", line 491, in gradients\nin_grad.set_shape(t_in.get_shape())\nFile \"..../tensorflow/python/framework/ops.py\", line 408, in set_shape\nself._shape = self._shape.merge_with(shape)\nFile \"..../tensorflow/python/framework/tensor_shape.py\", line 583, in merge_with\n(self, other))\nValueError: Shapes (0,) and (3,) are not compatible\n</code></pre>\n<p>Notice in the fused layer:</p>\n<pre><code>def _fused_batch_norm_training():\n  return nn.fused_batch_norm(\n      inputs, gamma, beta, epsilon=epsilon, data_format=data_format)\ndef _fused_batch_norm_inference():\n  return nn.fused_batch_norm(\n      inputs,\n      gamma,\n      beta,\n      mean=moving_mean,\n      variance=moving_variance,\n      epsilon=epsilon,\n      is_training=False,\n      data_format=data_format)\noutputs, mean, variance = utils.smart_cond(is_training,\n                                           _fused_batch_norm_training,\n                                           _fused_batch_norm_inference)\n\n</code></pre>\n<p>So when when <code>is_training</code> is a placeholder, we need both branches of the condition for the <code>outputs</code>. The same is true for gradients. So the tf.gradients function will actually go into the branch <code>is_training=False</code>, and perform gradient computation symbolically (even though at run time we never call the gradient op when <code>is_training=False</code>). Since we are not calling that branch at run time we shouldn't worry too much about it. But right now the shapes for  <code>moving_mean</code> and <code>moving_variance</code> gradients are set incorrectly when <code>is_training=False</code> (they are all <code>0</code> now, should be <code>0</code> for <code>is_training=True</code> only) and the tf.gradients catches that. I modified shape inference for gradient op in <code>nn_ops.cc</code> for that reason.</p>\n<p>Right now the <code>_fused_batch_norm</code> layer also has quite some redundant ops (the <code>mean</code> and <code>variance</code> at the end for example) and it doesn't support <code>2D</code> tensor (mine does).</p>", "body_text": "Hi Yao,\nYou should see the error with this snippet:\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport numpy as np\nis_training = tf.placeholder(tf.bool)\ninputs = tf.constant(np.zeros((10, 3, 100, 100)).astype('float32'))\noutputs = slim.batch_norm(inputs, is_training=is_training, fused=True, data_format='NCHW')\nloss = tf.reduce_sum(outputs)\ngrad = tf.gradients(loss, tf.trainable_variables())\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\np = sess.run(grad, feed_dict={is_training:True})\n\nIt's like:\nFile \"..../tensorflow/python/ops/gradients.py\", line 491, in gradients\nin_grad.set_shape(t_in.get_shape())\nFile \"..../tensorflow/python/framework/ops.py\", line 408, in set_shape\nself._shape = self._shape.merge_with(shape)\nFile \"..../tensorflow/python/framework/tensor_shape.py\", line 583, in merge_with\n(self, other))\nValueError: Shapes (0,) and (3,) are not compatible\n\nNotice in the fused layer:\ndef _fused_batch_norm_training():\n  return nn.fused_batch_norm(\n      inputs, gamma, beta, epsilon=epsilon, data_format=data_format)\ndef _fused_batch_norm_inference():\n  return nn.fused_batch_norm(\n      inputs,\n      gamma,\n      beta,\n      mean=moving_mean,\n      variance=moving_variance,\n      epsilon=epsilon,\n      is_training=False,\n      data_format=data_format)\noutputs, mean, variance = utils.smart_cond(is_training,\n                                           _fused_batch_norm_training,\n                                           _fused_batch_norm_inference)\n\n\nSo when when is_training is a placeholder, we need both branches of the condition for the outputs. The same is true for gradients. So the tf.gradients function will actually go into the branch is_training=False, and perform gradient computation symbolically (even though at run time we never call the gradient op when is_training=False). Since we are not calling that branch at run time we shouldn't worry too much about it. But right now the shapes for  moving_mean and moving_variance gradients are set incorrectly when is_training=False (they are all 0 now, should be 0 for is_training=True only) and the tf.gradients catches that. I modified shape inference for gradient op in nn_ops.cc for that reason.\nRight now the _fused_batch_norm layer also has quite some redundant ops (the mean and variance at the end for example) and it doesn't support 2D tensor (mine does).", "body": "Hi Yao, \n\nYou should see the error with this snippet:\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport numpy as np\nis_training = tf.placeholder(tf.bool)\ninputs = tf.constant(np.zeros((10, 3, 100, 100)).astype('float32'))\noutputs = slim.batch_norm(inputs, is_training=is_training, fused=True, data_format='NCHW')\nloss = tf.reduce_sum(outputs)\ngrad = tf.gradients(loss, tf.trainable_variables())\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\np = sess.run(grad, feed_dict={is_training:True})\n```\n\nIt's like: \n\n```\nFile \"..../tensorflow/python/ops/gradients.py\", line 491, in gradients\nin_grad.set_shape(t_in.get_shape())\nFile \"..../tensorflow/python/framework/ops.py\", line 408, in set_shape\nself._shape = self._shape.merge_with(shape)\nFile \"..../tensorflow/python/framework/tensor_shape.py\", line 583, in merge_with\n(self, other))\nValueError: Shapes (0,) and (3,) are not compatible\n```\n\nNotice in the fused layer:\n\n```\ndef _fused_batch_norm_training():\n  return nn.fused_batch_norm(\n      inputs, gamma, beta, epsilon=epsilon, data_format=data_format)\ndef _fused_batch_norm_inference():\n  return nn.fused_batch_norm(\n      inputs,\n      gamma,\n      beta,\n      mean=moving_mean,\n      variance=moving_variance,\n      epsilon=epsilon,\n      is_training=False,\n      data_format=data_format)\noutputs, mean, variance = utils.smart_cond(is_training,\n                                           _fused_batch_norm_training,\n                                           _fused_batch_norm_inference)\n\n```\n\nSo when when `is_training` is a placeholder, we need both branches of the condition for the `outputs`. The same is true for gradients. So the tf.gradients function will actually go into the branch `is_training=False`, and perform gradient computation symbolically (even though at run time we never call the gradient op when `is_training=False`). Since we are not calling that branch at run time we shouldn't worry too much about it. But right now the shapes for  `moving_mean` and `moving_variance` gradients are set incorrectly when `is_training=False` (they are all `0` now, should be `0` for `is_training=True` only) and the tf.gradients catches that. I modified shape inference for gradient op in `nn_ops.cc` for that reason. \n\nRight now the `_fused_batch_norm` layer also has quite some redundant ops (the `mean` and `variance` at the end for example) and it doesn't support `2D` tensor (mine does). \n"}