{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9918", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9918/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9918/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9918/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9918", "id": 228790793, "node_id": "MDU6SXNzdWUyMjg3OTA3OTM=", "number": 9918, "title": "preserving specific checkpoints", "user": {"login": "starcrest", "id": 28581427, "node_id": "MDQ6VXNlcjI4NTgxNDI3", "avatar_url": "https://avatars0.githubusercontent.com/u/28581427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/starcrest", "html_url": "https://github.com/starcrest", "followers_url": "https://api.github.com/users/starcrest/followers", "following_url": "https://api.github.com/users/starcrest/following{/other_user}", "gists_url": "https://api.github.com/users/starcrest/gists{/gist_id}", "starred_url": "https://api.github.com/users/starcrest/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/starcrest/subscriptions", "organizations_url": "https://api.github.com/users/starcrest/orgs", "repos_url": "https://api.github.com/users/starcrest/repos", "events_url": "https://api.github.com/users/starcrest/events{/privacy}", "received_events_url": "https://api.github.com/users/starcrest/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-15T17:43:28Z", "updated_at": "2017-05-19T10:01:02Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Savers automatically clean up checkpoints and that's lovely.  But there are special points during training that I want to be sure to save (e.g. transitioning from a pre-training phrase to full training), which I can't ensure with the current options (unless I just keep everything, by making <code>max_to_keep</code> &amp; <code>keep_checkpoint_every_n_hours</code> huge).</p>\n<p>Two possible approaches:</p>\n<ol>\n<li>\n<p><code>saver.save(..., preserve=True)</code><br>\nNever clean up this checkpoint.</p>\n</li>\n<li>\n<p><code>max_to_keep</code> is defined <strong>per</strong> <code>save_path</code><br>\ni.e. Whenever I change the <code>save_path</code> argument to <code>save()</code> in the middle of the session, don't clean up the checkpoints with the previous <code>save_path</code>.</p>\n</li>\n</ol>\n<p>This is related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"216428338\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8658\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8658/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/8658\">#8658</a> (with a little book-keeping on the client, you could do <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"216428338\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8658\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8658/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/8658\">#8658</a> yourself).</p>", "body_text": "Savers automatically clean up checkpoints and that's lovely.  But there are special points during training that I want to be sure to save (e.g. transitioning from a pre-training phrase to full training), which I can't ensure with the current options (unless I just keep everything, by making max_to_keep & keep_checkpoint_every_n_hours huge).\nTwo possible approaches:\n\n\nsaver.save(..., preserve=True)\nNever clean up this checkpoint.\n\n\nmax_to_keep is defined per save_path\ni.e. Whenever I change the save_path argument to save() in the middle of the session, don't clean up the checkpoints with the previous save_path.\n\n\nThis is related to #8658 (with a little book-keeping on the client, you could do #8658 yourself).", "body": "Savers automatically clean up checkpoints and that's lovely.  But there are special points during training that I want to be sure to save (e.g. transitioning from a pre-training phrase to full training), which I can't ensure with the current options (unless I just keep everything, by making ```max_to_keep``` & ```keep_checkpoint_every_n_hours``` huge).\r\n\r\nTwo possible approaches:\r\n\r\n1)  ```saver.save(..., preserve=True)```\r\nNever clean up this checkpoint.\r\n\r\n2)  ```max_to_keep``` is defined **per** ```save_path```\r\ni.e. Whenever I change the ```save_path``` argument to ```save()``` in the middle of the session, don't clean up the checkpoints with the previous ```save_path```.\r\n\r\nThis is related to #8658 (with a little book-keeping on the client, you could do #8658 yourself)."}