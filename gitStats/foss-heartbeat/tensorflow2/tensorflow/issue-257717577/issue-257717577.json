{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13040", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13040/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13040/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13040/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13040", "id": 257717577, "node_id": "MDU6SXNzdWUyNTc3MTc1Nzc=", "number": 13040, "title": "Feature request: Dataset.from_py_func", "user": {"login": "qtdaniel", "id": 21170884, "node_id": "MDQ6VXNlcjIxMTcwODg0", "avatar_url": "https://avatars2.githubusercontent.com/u/21170884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qtdaniel", "html_url": "https://github.com/qtdaniel", "followers_url": "https://api.github.com/users/qtdaniel/followers", "following_url": "https://api.github.com/users/qtdaniel/following{/other_user}", "gists_url": "https://api.github.com/users/qtdaniel/gists{/gist_id}", "starred_url": "https://api.github.com/users/qtdaniel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qtdaniel/subscriptions", "organizations_url": "https://api.github.com/users/qtdaniel/orgs", "repos_url": "https://api.github.com/users/qtdaniel/repos", "events_url": "https://api.github.com/users/qtdaniel/events{/privacy}", "received_events_url": "https://api.github.com/users/qtdaniel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-14T13:22:29Z", "updated_at": "2017-09-14T13:39:50Z", "closed_at": "2017-09-14T13:39:50Z", "author_association": "NONE", "body_html": "<p>I have an input pipeline that is especially complex. It is coded entirely in plain Python. While it <em>may</em> be possible to reimplement it in TensorFlow operations, that would be a huge amount of work and take far too much time to be worth it right now.</p>\n<p>It would be great if I could hook my existing pipeline into the new Dataset approach via something like <code>Dataset.from_py_func</code>. One would pass a reference to a Python function that has no inputs and, when executed, acts as a generator that yields examples one-at-a-time as numpy arrays. For example:</p>\n<pre><code>def generate():\n    for example in complex_input_pipeline():\n        yield example\n\ndataset = Dataset.from_py_func(generate)\n# Do normal things with dataset\n</code></pre>\n<p>I've been unable to figure out a way to do something like this using existing functionality so it would be great if it could be added.</p>", "body_text": "I have an input pipeline that is especially complex. It is coded entirely in plain Python. While it may be possible to reimplement it in TensorFlow operations, that would be a huge amount of work and take far too much time to be worth it right now.\nIt would be great if I could hook my existing pipeline into the new Dataset approach via something like Dataset.from_py_func. One would pass a reference to a Python function that has no inputs and, when executed, acts as a generator that yields examples one-at-a-time as numpy arrays. For example:\ndef generate():\n    for example in complex_input_pipeline():\n        yield example\n\ndataset = Dataset.from_py_func(generate)\n# Do normal things with dataset\n\nI've been unable to figure out a way to do something like this using existing functionality so it would be great if it could be added.", "body": "I have an input pipeline that is especially complex. It is coded entirely in plain Python. While it *may* be possible to reimplement it in TensorFlow operations, that would be a huge amount of work and take far too much time to be worth it right now.\r\n\r\nIt would be great if I could hook my existing pipeline into the new Dataset approach via something like `Dataset.from_py_func`. One would pass a reference to a Python function that has no inputs and, when executed, acts as a generator that yields examples one-at-a-time as numpy arrays. For example:\r\n\r\n    def generate():\r\n        for example in complex_input_pipeline():\r\n            yield example\r\n\r\n    dataset = Dataset.from_py_func(generate)\r\n    # Do normal things with dataset\r\n        \r\nI've been unable to figure out a way to do something like this using existing functionality so it would be great if it could be added."}