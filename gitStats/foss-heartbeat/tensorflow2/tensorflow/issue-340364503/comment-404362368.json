{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404362368", "html_url": "https://github.com/tensorflow/tensorflow/pull/20708#issuecomment-404362368", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20708", "id": 404362368, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDM2MjM2OA==", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-12T01:40:09Z", "updated_at": "2018-07-12T01:40:09Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>How do you think if we refactor the logic, and put this scratch memory size field into AlgorithmDesc, and ensure the following semantic: [...]</p>\n</blockquote>\n<p>sgtm!</p>\n<blockquote>\n<p>To my understanding a new MIOpen API termed as \"direct mode\" is under development where it's possible for client applications to specify exact algorithm be used. This should help realize your request to allow XLA and TensorFlow to pick the best/not-so-best algorithm. However this feature is not released yet.</p>\n</blockquote>\n<p>OK.</p>\n<p>Until direct mode is available, I think we can still implement the same API as we have for cudnn, though?  Just have ROCm return the one (autotuned) algorithm in the list of algorithms.</p>\n<p>If the presence of algorithm_no_scratch is a problem for ROCm (because you only get one algorithm with however much scratch required -- take it or leave it), we can try a bigger refactoring that gets rid of that...  Once you're returning the scratch memory requirement in the AlgorithmDescription, then we can simplify this whole thing quite a bit just by making the client preallocate the scratch buffer.  No need to have a ScratchAllocator in these calls at all -- it can just accept the preallocated scratch buffer instead.</p>", "body_text": "How do you think if we refactor the logic, and put this scratch memory size field into AlgorithmDesc, and ensure the following semantic: [...]\n\nsgtm!\n\nTo my understanding a new MIOpen API termed as \"direct mode\" is under development where it's possible for client applications to specify exact algorithm be used. This should help realize your request to allow XLA and TensorFlow to pick the best/not-so-best algorithm. However this feature is not released yet.\n\nOK.\nUntil direct mode is available, I think we can still implement the same API as we have for cudnn, though?  Just have ROCm return the one (autotuned) algorithm in the list of algorithms.\nIf the presence of algorithm_no_scratch is a problem for ROCm (because you only get one algorithm with however much scratch required -- take it or leave it), we can try a bigger refactoring that gets rid of that...  Once you're returning the scratch memory requirement in the AlgorithmDescription, then we can simplify this whole thing quite a bit just by making the client preallocate the scratch buffer.  No need to have a ScratchAllocator in these calls at all -- it can just accept the preallocated scratch buffer instead.", "body": "> How do you think if we refactor the logic, and put this scratch memory size field into AlgorithmDesc, and ensure the following semantic: [...]\r\n\r\nsgtm!\r\n\r\n> To my understanding a new MIOpen API termed as \"direct mode\" is under development where it's possible for client applications to specify exact algorithm be used. This should help realize your request to allow XLA and TensorFlow to pick the best/not-so-best algorithm. However this feature is not released yet.\r\n\r\nOK.\r\n\r\nUntil direct mode is available, I think we can still implement the same API as we have for cudnn, though?  Just have ROCm return the one (autotuned) algorithm in the list of algorithms.\r\n\r\nIf the presence of algorithm_no_scratch is a problem for ROCm (because you only get one algorithm with however much scratch required -- take it or leave it), we can try a bigger refactoring that gets rid of that...  Once you're returning the scratch memory requirement in the AlgorithmDescription, then we can simplify this whole thing quite a bit just by making the client preallocate the scratch buffer.  No need to have a ScratchAllocator in these calls at all -- it can just accept the preallocated scratch buffer instead."}