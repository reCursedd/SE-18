{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404572037", "html_url": "https://github.com/tensorflow/tensorflow/pull/20708#issuecomment-404572037", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20708", "id": 404572037, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDU3MjAzNw==", "user": {"login": "dagamayank", "id": 8866610, "node_id": "MDQ6VXNlcjg4NjY2MTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/8866610?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dagamayank", "html_url": "https://github.com/dagamayank", "followers_url": "https://api.github.com/users/dagamayank/followers", "following_url": "https://api.github.com/users/dagamayank/following{/other_user}", "gists_url": "https://api.github.com/users/dagamayank/gists{/gist_id}", "starred_url": "https://api.github.com/users/dagamayank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dagamayank/subscriptions", "organizations_url": "https://api.github.com/users/dagamayank/orgs", "repos_url": "https://api.github.com/users/dagamayank/repos", "events_url": "https://api.github.com/users/dagamayank/events{/privacy}", "received_events_url": "https://api.github.com/users/dagamayank/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-12T16:30:52Z", "updated_at": "2018-07-12T16:30:52Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a></p>\n<blockquote>\n<p>If at all possible we should use MIOpen the same way we use cudnn, where we ask MIOpen for a list of algorithms, and then TensorFlow / XLA runs all of the algorithms, choosing the fastest one.</p>\n</blockquote>\n<p>MIOpen attempts to auto-tune all the available algorithms for any particular inputs. This is similar to <code>cudnnFindConvolutionForwardAlgorithmEx</code>. But perhaps XLA, TensorFlow is not using that cuDNN API. Different algorithms require different scratch-space sizes ranging between zero to several MBs and hence, MIOpen provides another API to let the user know how much scratch is needed.</p>\n<blockquote>\n<p>We need the ability to run \"non-optimal\" algorithms for a variety of reasons, including checking for correctness or having limited scratch memory available.<br>\nUntil direct mode is available, I think we can still implement the same API as we have for cudnn, though? Just have ROCm return the one (autotuned) algorithm in the list of algorithms.</p>\n</blockquote>\n<p>I think this is still possible today. One can just pass zero as scratch-space size. However, in this case it is not guaranteed a valid algorithm is available for any and all inputs. In some scenarios scratch is <em>a</em> requirement. This <a href=\"https://github.com/ROCmSoftwarePlatform/MIOpen/wiki/MIOpen-Convolutions\">link</a> adds some more color to my description.</p>\n<p>With <code>direct</code> mode, MIOpen will more closely track cuDNN's usage of -<br>\n<code>cudnnGetConvolutionForwardAlgorithm</code>  -- returns the algo for a particular config<br>\n<code> cudnnGetConvolutionForwardWorkspaceSize</code> -- returns the scratch size for that config<br>\n<code>cudnnConvolutionForward</code> -- executes convolution</p>\n<p>I think both cuDNN and MIOpen are doing the same thing albeit the implementations are slightly different. If you look at the <a href=\"https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnFindConvolutionForwardAlgorithm\" rel=\"nofollow\">description</a> of <code>cudnnFindConvolutionForwardAlgorithm</code> -<br>\n<em>This function attempts all cuDNN algorithms (including CUDNN_TENSOR_OP_MATH and CUDNN_DEFAULT_MATH versions of algorithms where CUDNN_TENSOR_OP_MATH may be available) for cudnnConvolutionForward(), using memory allocated via <strong>cudaMalloc()</strong>,</em></p>\n<p>In this case, cuDNN internally manages all the memory including scratch as needed, whereas, MIOpen explicitly requires users to manage the memory.</p>", "body_text": "@jlebar\n\nIf at all possible we should use MIOpen the same way we use cudnn, where we ask MIOpen for a list of algorithms, and then TensorFlow / XLA runs all of the algorithms, choosing the fastest one.\n\nMIOpen attempts to auto-tune all the available algorithms for any particular inputs. This is similar to cudnnFindConvolutionForwardAlgorithmEx. But perhaps XLA, TensorFlow is not using that cuDNN API. Different algorithms require different scratch-space sizes ranging between zero to several MBs and hence, MIOpen provides another API to let the user know how much scratch is needed.\n\nWe need the ability to run \"non-optimal\" algorithms for a variety of reasons, including checking for correctness or having limited scratch memory available.\nUntil direct mode is available, I think we can still implement the same API as we have for cudnn, though? Just have ROCm return the one (autotuned) algorithm in the list of algorithms.\n\nI think this is still possible today. One can just pass zero as scratch-space size. However, in this case it is not guaranteed a valid algorithm is available for any and all inputs. In some scenarios scratch is a requirement. This link adds some more color to my description.\nWith direct mode, MIOpen will more closely track cuDNN's usage of -\ncudnnGetConvolutionForwardAlgorithm  -- returns the algo for a particular config\n cudnnGetConvolutionForwardWorkspaceSize -- returns the scratch size for that config\ncudnnConvolutionForward -- executes convolution\nI think both cuDNN and MIOpen are doing the same thing albeit the implementations are slightly different. If you look at the description of cudnnFindConvolutionForwardAlgorithm -\nThis function attempts all cuDNN algorithms (including CUDNN_TENSOR_OP_MATH and CUDNN_DEFAULT_MATH versions of algorithms where CUDNN_TENSOR_OP_MATH may be available) for cudnnConvolutionForward(), using memory allocated via cudaMalloc(),\nIn this case, cuDNN internally manages all the memory including scratch as needed, whereas, MIOpen explicitly requires users to manage the memory.", "body": "@jlebar \r\n\r\n> If at all possible we should use MIOpen the same way we use cudnn, where we ask MIOpen for a list of algorithms, and then TensorFlow / XLA runs all of the algorithms, choosing the fastest one.\r\n\r\nMIOpen attempts to auto-tune all the available algorithms for any particular inputs. This is similar to `cudnnFindConvolutionForwardAlgorithmEx`. But perhaps XLA, TensorFlow is not using that cuDNN API. Different algorithms require different scratch-space sizes ranging between zero to several MBs and hence, MIOpen provides another API to let the user know how much scratch is needed.\r\n\r\n> We need the ability to run \"non-optimal\" algorithms for a variety of reasons, including checking for correctness or having limited scratch memory available.\r\n> Until direct mode is available, I think we can still implement the same API as we have for cudnn, though? Just have ROCm return the one (autotuned) algorithm in the list of algorithms.\r\n\r\nI think this is still possible today. One can just pass zero as scratch-space size. However, in this case it is not guaranteed a valid algorithm is available for any and all inputs. In some scenarios scratch is _a_ requirement. This [link](https://github.com/ROCmSoftwarePlatform/MIOpen/wiki/MIOpen-Convolutions) adds some more color to my description.\r\n\r\nWith `direct` mode, MIOpen will more closely track cuDNN's usage of -\r\n`cudnnGetConvolutionForwardAlgorithm`  -- returns the algo for a particular config\r\n` cudnnGetConvolutionForwardWorkspaceSize` -- returns the scratch size for that config\r\n`cudnnConvolutionForward` -- executes convolution\r\n\r\nI think both cuDNN and MIOpen are doing the same thing albeit the implementations are slightly different. If you look at the [description](https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnFindConvolutionForwardAlgorithm) of `cudnnFindConvolutionForwardAlgorithm` -\r\n_This function attempts all cuDNN algorithms (including CUDNN_TENSOR_OP_MATH and CUDNN_DEFAULT_MATH versions of algorithms where CUDNN_TENSOR_OP_MATH may be available) for cudnnConvolutionForward(), using memory allocated via **cudaMalloc()**,_\r\n\r\nIn this case, cuDNN internally manages all the memory including scratch as needed, whereas, MIOpen explicitly requires users to manage the memory."}