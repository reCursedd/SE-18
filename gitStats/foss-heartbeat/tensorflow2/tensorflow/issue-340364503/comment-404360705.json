{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404360705", "html_url": "https://github.com/tensorflow/tensorflow/pull/20708#issuecomment-404360705", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20708", "id": 404360705, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDM2MDcwNQ==", "user": {"login": "whchung", "id": 1673574, "node_id": "MDQ6VXNlcjE2NzM1NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1673574?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whchung", "html_url": "https://github.com/whchung", "followers_url": "https://api.github.com/users/whchung/followers", "following_url": "https://api.github.com/users/whchung/following{/other_user}", "gists_url": "https://api.github.com/users/whchung/gists{/gist_id}", "starred_url": "https://api.github.com/users/whchung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whchung/subscriptions", "organizations_url": "https://api.github.com/users/whchung/orgs", "repos_url": "https://api.github.com/users/whchung/repos", "events_url": "https://api.github.com/users/whchung/events{/privacy}", "received_events_url": "https://api.github.com/users/whchung/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-12T01:29:08Z", "updated_at": "2018-07-12T01:29:08Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>How is a user supposed to avoid reading Algorithm::scratch_bytes_used() in the CUDA case, observing that it is zero, and then having a bug?</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a> How do you think if we refactor the logic, and put this scratch memory size field into <code>AlgorithmDesc</code>, and ensure the following semantic:</p>\n<p>For both CUDA and ROCm paths:</p>\n<ul>\n<li><code>AlgorithmConfig::algorithm_()</code> would contain the algorithm ID which uses scratch memory, plus the size of scratch memory.</li>\n<li><code>AlgorithmConfig::algorithm_no_scratch()_</code> would contain the algorithm ID which doesn't use scratch memory, with the size of scratch memory be 0.</li>\n</ul>\n<p>This could be implemented without changing MIOpen API. I'll address your other comment in another reply.</p>", "body_text": "How is a user supposed to avoid reading Algorithm::scratch_bytes_used() in the CUDA case, observing that it is zero, and then having a bug?\n\n@jlebar How do you think if we refactor the logic, and put this scratch memory size field into AlgorithmDesc, and ensure the following semantic:\nFor both CUDA and ROCm paths:\n\nAlgorithmConfig::algorithm_() would contain the algorithm ID which uses scratch memory, plus the size of scratch memory.\nAlgorithmConfig::algorithm_no_scratch()_ would contain the algorithm ID which doesn't use scratch memory, with the size of scratch memory be 0.\n\nThis could be implemented without changing MIOpen API. I'll address your other comment in another reply.", "body": "> How is a user supposed to avoid reading Algorithm::scratch_bytes_used() in the CUDA case, observing that it is zero, and then having a bug?\r\n\r\n@jlebar How do you think if we refactor the logic, and put this scratch memory size field into `AlgorithmDesc`, and ensure the following semantic:\r\n\r\nFor both CUDA and ROCm paths:\r\n- `AlgorithmConfig::algorithm_()` would contain the algorithm ID which uses scratch memory, plus the size of scratch memory.\r\n- `AlgorithmConfig::algorithm_no_scratch()_` would contain the algorithm ID which doesn't use scratch memory, with the size of scratch memory be 0.\r\n\r\nThis could be implemented without changing MIOpen API. I'll address your other comment in another reply."}