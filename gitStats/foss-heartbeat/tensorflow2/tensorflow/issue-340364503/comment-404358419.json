{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404358419", "html_url": "https://github.com/tensorflow/tensorflow/pull/20708#issuecomment-404358419", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20708", "id": 404358419, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDM1ODQxOQ==", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-12T01:13:35Z", "updated_at": "2018-07-12T01:13:35Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>this member variable won't be used in CuDNN path.</p>\n</blockquote>\n<p>How is a user supposed to avoid reading Algorithm::scratch_bytes_used() in the CUDA case, observing that it is zero, and then having a bug?</p>\n<blockquote>\n<p>In MIOpen algorithm finding phase, MIOpen FindConvolveXXXAlgorithm API is invoked in ROCm StreamExecutor, without the vector of available algorithms. MIOpen would find the best algorithm based on its pre-computed performance database, and return the ID of the algorithm, plus the size of scratch memory needed for the algorithm back to the client application (TensorFlow).</p>\n</blockquote>\n<p>If at all possible we should use MIOpen the same way we use cudnn, where we ask MIOpen for a list of algorithms, and then TensorFlow / XLA runs all of the algorithms, choosing the fastest one.</p>\n<p>We need the ability to run \"non-optimal\" algorithms for a variety of reasons, including checking for correctness or having limited scratch memory available.</p>\n<p>So MIOpen should support an API that works like the cudnn API works today.</p>", "body_text": "this member variable won't be used in CuDNN path.\n\nHow is a user supposed to avoid reading Algorithm::scratch_bytes_used() in the CUDA case, observing that it is zero, and then having a bug?\n\nIn MIOpen algorithm finding phase, MIOpen FindConvolveXXXAlgorithm API is invoked in ROCm StreamExecutor, without the vector of available algorithms. MIOpen would find the best algorithm based on its pre-computed performance database, and return the ID of the algorithm, plus the size of scratch memory needed for the algorithm back to the client application (TensorFlow).\n\nIf at all possible we should use MIOpen the same way we use cudnn, where we ask MIOpen for a list of algorithms, and then TensorFlow / XLA runs all of the algorithms, choosing the fastest one.\nWe need the ability to run \"non-optimal\" algorithms for a variety of reasons, including checking for correctness or having limited scratch memory available.\nSo MIOpen should support an API that works like the cudnn API works today.", "body": "> this member variable won't be used in CuDNN path.\r\n\r\nHow is a user supposed to avoid reading Algorithm::scratch_bytes_used() in the CUDA case, observing that it is zero, and then having a bug?\r\n\r\n> In MIOpen algorithm finding phase, MIOpen FindConvolveXXXAlgorithm API is invoked in ROCm StreamExecutor, without the vector of available algorithms. MIOpen would find the best algorithm based on its pre-computed performance database, and return the ID of the algorithm, plus the size of scratch memory needed for the algorithm back to the client application (TensorFlow).\r\n\r\nIf at all possible we should use MIOpen the same way we use cudnn, where we ask MIOpen for a list of algorithms, and then TensorFlow / XLA runs all of the algorithms, choosing the fastest one.\r\n\r\nWe need the ability to run \"non-optimal\" algorithms for a variety of reasons, including checking for correctness or having limited scratch memory available.\r\n\r\nSo MIOpen should support an API that works like the cudnn API works today."}