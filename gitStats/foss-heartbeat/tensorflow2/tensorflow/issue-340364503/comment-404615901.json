{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404615901", "html_url": "https://github.com/tensorflow/tensorflow/pull/20708#issuecomment-404615901", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20708", "id": 404615901, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDYxNTkwMQ==", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-12T18:58:32Z", "updated_at": "2018-07-12T18:58:32Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>Until direct mode is available, I think we can still implement the same API as we have for cudnn, though? Just have ROCm return the one (autotuned) algorithm in the list of algorithms.</p>\n</blockquote>\n<p>...or if that is hard because you'd need a to pass scratch allocator to the generate-a-list-of-algorithms function (that seems like a bad API), then in the generate-the-list-of-algorithms function, can we run autotuning with zero scratch memory and return that algorithm as our one result?  That will potentially return a slow conv, but it will be functionally correct, which will unblock us.</p>", "body_text": "Until direct mode is available, I think we can still implement the same API as we have for cudnn, though? Just have ROCm return the one (autotuned) algorithm in the list of algorithms.\n\n...or if that is hard because you'd need a to pass scratch allocator to the generate-a-list-of-algorithms function (that seems like a bad API), then in the generate-the-list-of-algorithms function, can we run autotuning with zero scratch memory and return that algorithm as our one result?  That will potentially return a slow conv, but it will be functionally correct, which will unblock us.", "body": "> Until direct mode is available, I think we can still implement the same API as we have for cudnn, though? Just have ROCm return the one (autotuned) algorithm in the list of algorithms.\r\n\r\n...or if that is hard because you'd need a to pass scratch allocator to the generate-a-list-of-algorithms function (that seems like a bad API), then in the generate-the-list-of-algorithms function, can we run autotuning with zero scratch memory and return that algorithm as our one result?  That will potentially return a slow conv, but it will be functionally correct, which will unblock us."}