{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/407901205", "html_url": "https://github.com/tensorflow/tensorflow/pull/20708#issuecomment-407901205", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20708", "id": 407901205, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzkwMTIwNQ==", "user": {"login": "whchung", "id": 1673574, "node_id": "MDQ6VXNlcjE2NzM1NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1673574?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whchung", "html_url": "https://github.com/whchung", "followers_url": "https://api.github.com/users/whchung/followers", "following_url": "https://api.github.com/users/whchung/following{/other_user}", "gists_url": "https://api.github.com/users/whchung/gists{/gist_id}", "starred_url": "https://api.github.com/users/whchung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whchung/subscriptions", "organizations_url": "https://api.github.com/users/whchung/orgs", "repos_url": "https://api.github.com/users/whchung/repos", "events_url": "https://api.github.com/users/whchung/events{/privacy}", "received_events_url": "https://api.github.com/users/whchung/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-25T21:26:09Z", "updated_at": "2018-07-25T21:26:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a> In my previous version of this PR. The proposed semantic is:</p>\n<p>The scratch memory size field into <code>AlgorithmDesc</code>, and ensure the following semantic:</p>\n<p>For both CUDA and ROCm paths:</p>\n<p><code>AlgorithmConfig::algorithm_()</code> would contain the algorithm ID which uses scratch memory, plus the size of scratch memory.<br>\n<code>AlgorithmConfig::algorithm_no_scratch()_</code> would contain the algorithm ID which doesn't use scratch memory, with the size of scratch memory be 0.</p>\n<p>Here's what be implemented in CUDA path:</p>\n<p>In algorithm finding stage (conv_ops.cc, conv_grad_*.cc), use <code>scratch_allocator.TotalByteSize()</code> to populate the field inside <code>AlgorithmDesc</code>, which is embedded in <code>ProfileResult</code> to determine the best algorithm.</p>\n<p>Same logic would probably need to be applied in XLA (service/gpu/cudnn_convolution_algorithm_picker.cc).</p>\n<p>In CUDA StreamExecutor (cuda_dnn.cc), prior to launching convolutions, <code>GetCudnnConvolutionXXXAlgorithm</code> would be invoked. Inside it, <code>GetCudnnConvolutionXXXAlgo</code> and <code>AllocateCudnnConvolutionXXXWorkspace</code> would be invoked to allocate scratch memory, and when supplied with <code>AlgorithmDesc</code> created in the previous stage, really allocate scratch memory and populate <code>AlgorithmDesc</code> which reflects the actual scratch memory allocated. And then launch the algorithm.</p>", "body_text": "@jlebar In my previous version of this PR. The proposed semantic is:\nThe scratch memory size field into AlgorithmDesc, and ensure the following semantic:\nFor both CUDA and ROCm paths:\nAlgorithmConfig::algorithm_() would contain the algorithm ID which uses scratch memory, plus the size of scratch memory.\nAlgorithmConfig::algorithm_no_scratch()_ would contain the algorithm ID which doesn't use scratch memory, with the size of scratch memory be 0.\nHere's what be implemented in CUDA path:\nIn algorithm finding stage (conv_ops.cc, conv_grad_*.cc), use scratch_allocator.TotalByteSize() to populate the field inside AlgorithmDesc, which is embedded in ProfileResult to determine the best algorithm.\nSame logic would probably need to be applied in XLA (service/gpu/cudnn_convolution_algorithm_picker.cc).\nIn CUDA StreamExecutor (cuda_dnn.cc), prior to launching convolutions, GetCudnnConvolutionXXXAlgorithm would be invoked. Inside it, GetCudnnConvolutionXXXAlgo and AllocateCudnnConvolutionXXXWorkspace would be invoked to allocate scratch memory, and when supplied with AlgorithmDesc created in the previous stage, really allocate scratch memory and populate AlgorithmDesc which reflects the actual scratch memory allocated. And then launch the algorithm.", "body": "@jlebar In my previous version of this PR. The proposed semantic is:\r\n\r\nThe scratch memory size field into `AlgorithmDesc`, and ensure the following semantic:\r\n\r\nFor both CUDA and ROCm paths:\r\n\r\n`AlgorithmConfig::algorithm_()` would contain the algorithm ID which uses scratch memory, plus the size of scratch memory.\r\n`AlgorithmConfig::algorithm_no_scratch()_` would contain the algorithm ID which doesn't use scratch memory, with the size of scratch memory be 0.\r\n\r\nHere's what be implemented in CUDA path:\r\n\r\nIn algorithm finding stage (conv_ops.cc, conv_grad_*.cc), use `scratch_allocator.TotalByteSize()` to populate the field inside `AlgorithmDesc`, which is embedded in `ProfileResult` to determine the best algorithm.\r\n\r\nSame logic would probably need to be applied in XLA (service/gpu/cudnn_convolution_algorithm_picker.cc).\r\n\r\nIn CUDA StreamExecutor (cuda_dnn.cc), prior to launching convolutions, `GetCudnnConvolutionXXXAlgorithm` would be invoked. Inside it, `GetCudnnConvolutionXXXAlgo` and `AllocateCudnnConvolutionXXXWorkspace` would be invoked to allocate scratch memory, and when supplied with `AlgorithmDesc` created in the previous stage, really allocate scratch memory and populate `AlgorithmDesc` which reflects the actual scratch memory allocated. And then launch the algorithm."}