{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1267", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1267/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1267/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1267/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1267", "id": 135937551, "node_id": "MDU6SXNzdWUxMzU5Mzc1NTE=", "number": 1267, "title": "learning rate reset to 0 in seq2seq example", "user": {"login": "zcyang", "id": 5890073, "node_id": "MDQ6VXNlcjU4OTAwNzM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5890073?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zcyang", "html_url": "https://github.com/zcyang", "followers_url": "https://api.github.com/users/zcyang/followers", "following_url": "https://api.github.com/users/zcyang/following{/other_user}", "gists_url": "https://api.github.com/users/zcyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/zcyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zcyang/subscriptions", "organizations_url": "https://api.github.com/users/zcyang/orgs", "repos_url": "https://api.github.com/users/zcyang/repos", "events_url": "https://api.github.com/users/zcyang/events{/privacy}", "received_events_url": "https://api.github.com/users/zcyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-02-24T02:53:26Z", "updated_at": "2016-02-24T03:24:03Z", "closed_at": "2016-02-24T03:24:03Z", "author_association": "NONE", "body_html": "<p>In the <code>translate.py</code> example, I used four layers without attention, the learning rate suddenly becomes zero...</p>\n<pre><code> itr 1/311830 epoch 1/10 lr 0.7000 cost 9.77 ppl 17455.77\n itr 2/311830 epoch 1/10 lr 0.0000 cost 9.86 ppl 19082.18\n itr 3/311830 epoch 1/10 lr 0.0000 cost 9.80 ppl 18084.05\n itr 4/311830 epoch 1/10 lr 0.0000 cost 9.78 ppl 17700.32\n</code></pre>\n<p>This is parameter specific, when I set the batch size to be 128, learning rate to be 0.7 or 1.<br>\nWhen I change the batch size to be 64, or set the learning rate to be 0.5, the problem DISAPPEARS.</p>\n<p>In debugging, when I try to set it back to 0.7 after it becomes 0., it will automatically go back to 0 in the next time step...</p>\n<p>I confirm the learning rate update operation is not called.</p>\n<p>I am using the <code>0.7.1</code> version with <code>cuda 7.5</code></p>", "body_text": "In the translate.py example, I used four layers without attention, the learning rate suddenly becomes zero...\n itr 1/311830 epoch 1/10 lr 0.7000 cost 9.77 ppl 17455.77\n itr 2/311830 epoch 1/10 lr 0.0000 cost 9.86 ppl 19082.18\n itr 3/311830 epoch 1/10 lr 0.0000 cost 9.80 ppl 18084.05\n itr 4/311830 epoch 1/10 lr 0.0000 cost 9.78 ppl 17700.32\n\nThis is parameter specific, when I set the batch size to be 128, learning rate to be 0.7 or 1.\nWhen I change the batch size to be 64, or set the learning rate to be 0.5, the problem DISAPPEARS.\nIn debugging, when I try to set it back to 0.7 after it becomes 0., it will automatically go back to 0 in the next time step...\nI confirm the learning rate update operation is not called.\nI am using the 0.7.1 version with cuda 7.5", "body": "In the `translate.py` example, I used four layers without attention, the learning rate suddenly becomes zero...\n\n```\n itr 1/311830 epoch 1/10 lr 0.7000 cost 9.77 ppl 17455.77\n itr 2/311830 epoch 1/10 lr 0.0000 cost 9.86 ppl 19082.18\n itr 3/311830 epoch 1/10 lr 0.0000 cost 9.80 ppl 18084.05\n itr 4/311830 epoch 1/10 lr 0.0000 cost 9.78 ppl 17700.32\n```\n\nThis is parameter specific, when I set the batch size to be 128, learning rate to be 0.7 or 1.\nWhen I change the batch size to be 64, or set the learning rate to be 0.5, the problem DISAPPEARS.\n\nIn debugging, when I try to set it back to 0.7 after it becomes 0., it will automatically go back to 0 in the next time step...\n\nI confirm the learning rate update operation is not called.\n\nI am using the `0.7.1` version with `cuda 7.5`\n"}