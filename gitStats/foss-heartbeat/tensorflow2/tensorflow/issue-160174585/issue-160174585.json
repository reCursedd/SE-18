{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2851", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2851/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2851/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2851/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2851", "id": 160174585, "node_id": "MDU6SXNzdWUxNjAxNzQ1ODU=", "number": 2851, "title": "How to reduce_max variable length sentences?", "user": {"login": "smartcat2010", "id": 10429114, "node_id": "MDQ6VXNlcjEwNDI5MTE0", "avatar_url": "https://avatars1.githubusercontent.com/u/10429114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smartcat2010", "html_url": "https://github.com/smartcat2010", "followers_url": "https://api.github.com/users/smartcat2010/followers", "following_url": "https://api.github.com/users/smartcat2010/following{/other_user}", "gists_url": "https://api.github.com/users/smartcat2010/gists{/gist_id}", "starred_url": "https://api.github.com/users/smartcat2010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smartcat2010/subscriptions", "organizations_url": "https://api.github.com/users/smartcat2010/orgs", "repos_url": "https://api.github.com/users/smartcat2010/repos", "events_url": "https://api.github.com/users/smartcat2010/events{/privacy}", "received_events_url": "https://api.github.com/users/smartcat2010/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-06-14T12:57:18Z", "updated_at": "2016-06-28T17:33:56Z", "closed_at": "2016-06-28T17:33:56Z", "author_association": "NONE", "body_html": "<p>When I use word-embedding to train model, variable-sentence-length is a problem that could hurt performance.<br>\nThe task is same as <a href=\"https://github.com/tensorflow/tensorflow/issues/2849\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2849/hovercard\">#2849</a> . I open this new issue because of variable-sentence-length problem.<br>\nThe task is to classify each sentence into 10 classes. Each sentence is length of 1<del>30 words. Firstly I use embedding_lookup to get all 1</del>30 word-vectors, then I use tf.map_fn(lambda x:tf.reduce_max(x,0), \u2026) to reduce each sentence\u2019s 1~30 word-vectors into a single \u201csentence vector\u201d. Then It put this \u201csentence vector\u201d as DNN's input. The code is same as <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"160152365\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2849\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2849/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2849\">#2849</a>:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/313877/WE_example.py.txt\">WE_example.py.txt</a></p>\n<p>In the above code, I have to use 30 as sentence length. Because the \u201ctf.map_fn(tf.reduce_max(...))\" could only merge sub-matrixes of the same size, for example, the minibatch is [100, 30, 200], then all sub-matrixes are [30, 200], using \"tf.map_fn(tf.reduce_max(...))\" could merge each sub-matrix into [200].</p>\n<p>But in the real world, sentences are of variable lengths. If I add padding data to the tail of sub-matrixes, I will not only give accuracy problem, but also bring more data to transfer between CPU&amp;GPU, and also give more unneccesary computation cost.<br>\nThe ideal solution is providing a powerful_reduce function to support reduce on sub-sections. For example, Five sentences' length are {3, 10, 7, 25, 2}. I gave powerful_reduce a matrix of size [3+10+7+25+2, 200], and a 1D array of content { 3, 10, 7, 25, 2}, it could return the reduce results of the five sub-matrixes, the result shape is [5, 200].<br>\nAnother way is to provide a \"mask\" array as input parameter. The reduce_max operation will only operater on the items with mask 1, and skip the items with mask 0.<br>\nPreviously I used CUDA Thrust Library, it could handle it by using \"for_each\".</p>\n<p>So, could tensorflow provide a powerful reduce_max function to support variable-size sub matrixes?<br>\nAnother question is , I found my GPU usage is only about 20%, in order to improve the GPU usage, I prefer to run multiple processes on the same GPU card using distributed tensorflow. But I don't know how much GPU memory my process will occupy(the upper-bound). In another word, how could I safely decide the minimum \"x\" in \"tf.GPUOptions(per_process_gpu_memory_fraction = x)\"?</p>\n<p>Thanks a lot in advance~</p>", "body_text": "When I use word-embedding to train model, variable-sentence-length is a problem that could hurt performance.\nThe task is same as #2849 . I open this new issue because of variable-sentence-length problem.\nThe task is to classify each sentence into 10 classes. Each sentence is length of 130 words. Firstly I use embedding_lookup to get all 130 word-vectors, then I use tf.map_fn(lambda x:tf.reduce_max(x,0), \u2026) to reduce each sentence\u2019s 1~30 word-vectors into a single \u201csentence vector\u201d. Then It put this \u201csentence vector\u201d as DNN's input. The code is same as #2849:\nWE_example.py.txt\nIn the above code, I have to use 30 as sentence length. Because the \u201ctf.map_fn(tf.reduce_max(...))\" could only merge sub-matrixes of the same size, for example, the minibatch is [100, 30, 200], then all sub-matrixes are [30, 200], using \"tf.map_fn(tf.reduce_max(...))\" could merge each sub-matrix into [200].\nBut in the real world, sentences are of variable lengths. If I add padding data to the tail of sub-matrixes, I will not only give accuracy problem, but also bring more data to transfer between CPU&GPU, and also give more unneccesary computation cost.\nThe ideal solution is providing a powerful_reduce function to support reduce on sub-sections. For example, Five sentences' length are {3, 10, 7, 25, 2}. I gave powerful_reduce a matrix of size [3+10+7+25+2, 200], and a 1D array of content { 3, 10, 7, 25, 2}, it could return the reduce results of the five sub-matrixes, the result shape is [5, 200].\nAnother way is to provide a \"mask\" array as input parameter. The reduce_max operation will only operater on the items with mask 1, and skip the items with mask 0.\nPreviously I used CUDA Thrust Library, it could handle it by using \"for_each\".\nSo, could tensorflow provide a powerful reduce_max function to support variable-size sub matrixes?\nAnother question is , I found my GPU usage is only about 20%, in order to improve the GPU usage, I prefer to run multiple processes on the same GPU card using distributed tensorflow. But I don't know how much GPU memory my process will occupy(the upper-bound). In another word, how could I safely decide the minimum \"x\" in \"tf.GPUOptions(per_process_gpu_memory_fraction = x)\"?\nThanks a lot in advance~", "body": "When I use word-embedding to train model, variable-sentence-length is a problem that could hurt performance.\nThe task is same as [#2849](https://github.com/tensorflow/tensorflow/issues/2849) . I open this new issue because of variable-sentence-length problem.\nThe task is to classify each sentence into 10 classes. Each sentence is length of 1~30 words. Firstly I use embedding_lookup to get all 1~30 word-vectors, then I use tf.map_fn(lambda x:tf.reduce_max(x,0), \u2026) to reduce each sentence\u2019s 1~30 word-vectors into a single \u201csentence vector\u201d. Then It put this \u201csentence vector\u201d as DNN's input. The code is same as #2849:\n[WE_example.py.txt](https://github.com/tensorflow/tensorflow/files/313877/WE_example.py.txt)\n\nIn the above code, I have to use 30 as sentence length. Because the \u201ctf.map_fn(tf.reduce_max(...))\" could only merge sub-matrixes of the same size, for example, the minibatch is [100, 30, 200], then all sub-matrixes are [30, 200], using \"tf.map_fn(tf.reduce_max(...))\" could merge each sub-matrix into [200]. \n\nBut in the real world, sentences are of variable lengths. If I add padding data to the tail of sub-matrixes, I will not only give accuracy problem, but also bring more data to transfer between CPU&GPU, and also give more unneccesary computation cost.\nThe ideal solution is providing a powerful_reduce function to support reduce on sub-sections. For example, Five sentences' length are {3, 10, 7, 25, 2}. I gave powerful_reduce a matrix of size [3+10+7+25+2, 200], and a 1D array of content { 3, 10, 7, 25, 2}, it could return the reduce results of the five sub-matrixes, the result shape is [5, 200]. \nAnother way is to provide a \"mask\" array as input parameter. The reduce_max operation will only operater on the items with mask 1, and skip the items with mask 0.\nPreviously I used CUDA Thrust Library, it could handle it by using \"for_each\". \n\nSo, could tensorflow provide a powerful reduce_max function to support variable-size sub matrixes?\nAnother question is , I found my GPU usage is only about 20%, in order to improve the GPU usage, I prefer to run multiple processes on the same GPU card using distributed tensorflow. But I don't know how much GPU memory my process will occupy(the upper-bound). In another word, how could I safely decide the minimum \"x\" in \"tf.GPUOptions(per_process_gpu_memory_fraction = x)\"?\n\nThanks a lot in advance~\n"}