{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15035", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15035/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15035/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15035/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15035", "id": 278428354, "node_id": "MDU6SXNzdWUyNzg0MjgzNTQ=", "number": 15035, "title": "NadamOptimizer does not work with sparse gradients", "user": {"login": "albertz", "id": 59132, "node_id": "MDQ6VXNlcjU5MTMy", "avatar_url": "https://avatars0.githubusercontent.com/u/59132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertz", "html_url": "https://github.com/albertz", "followers_url": "https://api.github.com/users/albertz/followers", "following_url": "https://api.github.com/users/albertz/following{/other_user}", "gists_url": "https://api.github.com/users/albertz/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertz/subscriptions", "organizations_url": "https://api.github.com/users/albertz/orgs", "repos_url": "https://api.github.com/users/albertz/repos", "events_url": "https://api.github.com/users/albertz/events{/privacy}", "received_events_url": "https://api.github.com/users/albertz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-01T10:51:58Z", "updated_at": "2017-12-27T19:28:39Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.4.0-rc1-11-g130a514 1.4.0</li>\n<li><strong>Python version</strong>: 3.6.3</li>\n</ul>\n<p>Code:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib.opt import NadamOptimizer\n\noptimizer = NadamOptimizer(learning_rate=0.001)\n# optimizer = tf.train.AdamOptimizer(learning_rate=0.001)  # this works\nw = tf.get_variable(\"w\", shape=(100, 10))\nidxs = tf.placeholder(tf.int32, shape=(None,))\nemb = tf.gather(w, idxs)\nloss = tf.reduce_sum(emb ** 2)\nminimize = optimizer.minimize(loss)\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  sess.run(minimize, feed_dict={idxs: [1, 2, 3]})\n</code></pre>\n<p>This fails with:</p>\n<pre><code>...\n  File \"/u/zeyer/.local/lib/python3.6/site-packages/tensorflow/contrib/opt/python/training/nadam_optimizer.py\", line 83, in _apply_sparse_shared\n    m_bar = m_scaled_g_values + beta1_t * m_t\n...\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [3,10] vs. [100,10]\n         [[Node: Adam/update_w/add = Add[T=DT_FLOAT, _class=[\"loc:@w\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam/update_w/mul_1, Adam/update_w/mul_3)]]\n</code></pre>\n<p>The bug is pretty obvious in <code>nadam_optimizer.py</code>. The fix would be to do it like in <code>adam.py</code>:</p>\n<pre><code>m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\nwith ops.control_dependencies([m_t]):\n  m_t = scatter_add(m, indices, m_scaled_g_values)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): pip\nTensorFlow version (use command below): v1.4.0-rc1-11-g130a514 1.4.0\nPython version: 3.6.3\n\nCode:\nimport tensorflow as tf\nfrom tensorflow.contrib.opt import NadamOptimizer\n\noptimizer = NadamOptimizer(learning_rate=0.001)\n# optimizer = tf.train.AdamOptimizer(learning_rate=0.001)  # this works\nw = tf.get_variable(\"w\", shape=(100, 10))\nidxs = tf.placeholder(tf.int32, shape=(None,))\nemb = tf.gather(w, idxs)\nloss = tf.reduce_sum(emb ** 2)\nminimize = optimizer.minimize(loss)\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  sess.run(minimize, feed_dict={idxs: [1, 2, 3]})\n\nThis fails with:\n...\n  File \"/u/zeyer/.local/lib/python3.6/site-packages/tensorflow/contrib/opt/python/training/nadam_optimizer.py\", line 83, in _apply_sparse_shared\n    m_bar = m_scaled_g_values + beta1_t * m_t\n...\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [3,10] vs. [100,10]\n         [[Node: Adam/update_w/add = Add[T=DT_FLOAT, _class=[\"loc:@w\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam/update_w/mul_1, Adam/update_w/mul_3)]]\n\nThe bug is pretty obvious in nadam_optimizer.py. The fix would be to do it like in adam.py:\nm_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\nwith ops.control_dependencies([m_t]):\n  m_t = scatter_add(m, indices, m_scaled_g_values)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6.3\r\n\r\nCode:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.opt import NadamOptimizer\r\n\r\noptimizer = NadamOptimizer(learning_rate=0.001)\r\n# optimizer = tf.train.AdamOptimizer(learning_rate=0.001)  # this works\r\nw = tf.get_variable(\"w\", shape=(100, 10))\r\nidxs = tf.placeholder(tf.int32, shape=(None,))\r\nemb = tf.gather(w, idxs)\r\nloss = tf.reduce_sum(emb ** 2)\r\nminimize = optimizer.minimize(loss)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  sess.run(minimize, feed_dict={idxs: [1, 2, 3]})\r\n```\r\n\r\nThis fails with:\r\n```\r\n...\r\n  File \"/u/zeyer/.local/lib/python3.6/site-packages/tensorflow/contrib/opt/python/training/nadam_optimizer.py\", line 83, in _apply_sparse_shared\r\n    m_bar = m_scaled_g_values + beta1_t * m_t\r\n...\r\n\r\nInvalidArgumentError (see above for traceback): Incompatible shapes: [3,10] vs. [100,10]\r\n         [[Node: Adam/update_w/add = Add[T=DT_FLOAT, _class=[\"loc:@w\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam/update_w/mul_1, Adam/update_w/mul_3)]]\r\n```\r\n\r\nThe bug is pretty obvious in `nadam_optimizer.py`. The fix would be to do it like in `adam.py`:\r\n```\r\nm_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\r\nwith ops.control_dependencies([m_t]):\r\n  m_t = scatter_add(m, indices, m_scaled_g_values)\r\n```\r\n\r\n"}