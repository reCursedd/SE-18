{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/372419733", "html_url": "https://github.com/tensorflow/tensorflow/issues/17067#issuecomment-372419733", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17067", "id": 372419733, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjQxOTczMw==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-12T18:41:29Z", "updated_at": "2018-03-12T18:41:29Z", "author_association": "NONE", "body_html": "<p>Hello <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1499733\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/marcionicolau\">@marcionicolau</a></p>\n<p>Trying out <code>mnist_mlp.py</code>, I am also observing a SEGFAULT. Here is a backtrace:</p>\n<pre>Thread 26 Crashed:\n0   libtensorflow_framework.so    \t0x0000000102959c10 void tensorflow::gtl::InlinedVector::emplace_back(tensorflow::EventMgr::InUse const&amp;&amp;&amp;) + 176\n1   libtensorflow_framework.so    \t0x000000010295902c tensorflow::EventMgr::PollEvents(bool, tensorflow::gtl::InlinedVector*) + 412\n2   libtensorflow_framework.so    \t0x00000001028f4022 tensorflow::EventMgr::ThenExecute(perftools::gputools::Stream*, std::__1::function) + 194\n3   libtensorflow_framework.so    \t0x00000001028f4bad tensorflow::GPUUtil::CopyCPUTensorToGPU(tensorflow::Tensor const*, tensorflow::DeviceContext const*, tensorflow::Device*, tensorflow::Tensor*, std::__1::function) + 797\n4   libtensorflow_framework.so    \t0x00000001028f6a05 tensorflow::GPUDeviceContext::CopyCPUTensorToDevice(tensorflow::Tensor const*, tensorflow::Device*, tensorflow::Tensor*, std::__1::function) const + 117\n5   libtensorflow_framework.so    \t0x00000001029098b5 tensorflow::(anonymous namespace)::CopyHostToDevice(tensorflow::Tensor const*, tensorflow::Allocator*, tensorflow::Allocator*, tensorflow::StringPiece, tensorflow::Device*, tensorflow::Tensor*, tensorflow::DeviceContext*, std::__1::function) + 437\n6   libtensorflow_framework.so    \t0x0000000102908b58 tensorflow::CopyTensor::ViaDMA(tensorflow::StringPiece, tensorflow::DeviceContext*, tensorflow::DeviceContext*, tensorflow::Device*, tensorflow::Device*, tensorflow::AllocatorAttributes, tensorflow::AllocatorAttributes, tensorflow::Tensor const*, tensorflow::Tensor*, std::__1::function) + 3592\n7   libtensorflow_framework.so    \t0x0000000102942dbe tensorflow::IntraProcessRendezvous::SameWorkerRecvDone(tensorflow::Rendezvous::ParsedKey const&amp;, tensorflow::Rendezvous::Args const&amp;, tensorflow::Rendezvous::Args const&amp;, tensorflow::Tensor const&amp;, tensorflow::Tensor*, std::__1::function) + 1102\n8   libtensorflow_framework.so    \t0x000000010294385d std::__1::__function::__func)::$_0, std::__1::allocator)::$_0&gt;, void (tensorflow::Status const&amp;, tensorflow::Rendezvous::Args const&amp;, tensorflow::Rendezvous::Args const&amp;, tensorflow::Tensor const&amp;, bool)&gt;::operator()(tensorflow::Status const&amp;, tensorflow::Rendezvous::Args const&amp;, tensorflow::Rendezvous::Args const&amp;, tensorflow::Tensor const&amp;, bool&amp;&amp;) + 813\n9   libtensorflow_framework.so    \t0x000000010244e003 tensorflow::LocalRendezvousImpl::RecvAsync(tensorflow::Rendezvous::ParsedKey const&amp;, tensorflow::Rendezvous::Args const&amp;, std::__1::function) + 883\n10  libtensorflow_framework.so    \t0x000000010294311f tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&amp;, tensorflow::Rendezvous::Args const&amp;, std::__1::function) + 799\n11  _pywrap_tensorflow_internal.so\t0x000000010b1a40a9 tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::__1::function) + 1145\n12  libtensorflow_framework.so    \t0x00000001028ea458 tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::__1::function) + 872\n13  libtensorflow_framework.so    \t0x0000000102918202 tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4338\n14  libtensorflow_framework.so    \t0x000000010292210a std::__1::__function::__func, std::__1::allocator &gt;, void ()&gt;::operator()() + 58\n15  libtensorflow_framework.so    \t0x000000010255bdff Eigen::NonBlockingThreadPoolTempl::WorkerLoop(int) + 2047\n16  libtensorflow_framework.so    \t0x000000010255b4ff std::__1::__function::__func)::'lambda'(), std::__1::allocator)::'lambda'()&gt;, void ()&gt;::operator()() + 47\n17  libtensorflow_framework.so    \t0x00000001025808a0 void* std::__1::__thread_proxy &gt;, std::__1::function &gt; &gt;(void*) + 48\n18  libsystem_pthread.dylib       \t0x00007fff53e376c1 _pthread_body + 340\n19  libsystem_pthread.dylib       \t0x00007fff53e3756d _pthread_start + 377\n20  libsystem_pthread.dylib       \t0x00007fff53e36c5d thread_start + 13\n</pre>\n<p>This appears similar to <a href=\"https://github.com/tensorflow/tensorflow/issues/9518#issuecomment-366854122\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9518/hovercard\">the other SEGFAULT issue that I have observed</a>. There, the SEGFAULT occurs within Copy<strong>GPU</strong>TensorToCPU(). Here, the SEGFAULT occurs within Copy<strong>CPU</strong>TensorToGPU().</p>", "body_text": "Hello @marcionicolau\nTrying out mnist_mlp.py, I am also observing a SEGFAULT. Here is a backtrace:\nThread 26 Crashed:\n0   libtensorflow_framework.so    \t0x0000000102959c10 void tensorflow::gtl::InlinedVector::emplace_back(tensorflow::EventMgr::InUse const&&&) + 176\n1   libtensorflow_framework.so    \t0x000000010295902c tensorflow::EventMgr::PollEvents(bool, tensorflow::gtl::InlinedVector*) + 412\n2   libtensorflow_framework.so    \t0x00000001028f4022 tensorflow::EventMgr::ThenExecute(perftools::gputools::Stream*, std::__1::function) + 194\n3   libtensorflow_framework.so    \t0x00000001028f4bad tensorflow::GPUUtil::CopyCPUTensorToGPU(tensorflow::Tensor const*, tensorflow::DeviceContext const*, tensorflow::Device*, tensorflow::Tensor*, std::__1::function) + 797\n4   libtensorflow_framework.so    \t0x00000001028f6a05 tensorflow::GPUDeviceContext::CopyCPUTensorToDevice(tensorflow::Tensor const*, tensorflow::Device*, tensorflow::Tensor*, std::__1::function) const + 117\n5   libtensorflow_framework.so    \t0x00000001029098b5 tensorflow::(anonymous namespace)::CopyHostToDevice(tensorflow::Tensor const*, tensorflow::Allocator*, tensorflow::Allocator*, tensorflow::StringPiece, tensorflow::Device*, tensorflow::Tensor*, tensorflow::DeviceContext*, std::__1::function) + 437\n6   libtensorflow_framework.so    \t0x0000000102908b58 tensorflow::CopyTensor::ViaDMA(tensorflow::StringPiece, tensorflow::DeviceContext*, tensorflow::DeviceContext*, tensorflow::Device*, tensorflow::Device*, tensorflow::AllocatorAttributes, tensorflow::AllocatorAttributes, tensorflow::Tensor const*, tensorflow::Tensor*, std::__1::function) + 3592\n7   libtensorflow_framework.so    \t0x0000000102942dbe tensorflow::IntraProcessRendezvous::SameWorkerRecvDone(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, tensorflow::Tensor*, std::__1::function) + 1102\n8   libtensorflow_framework.so    \t0x000000010294385d std::__1::__function::__func)::$_0, std::__1::allocator)::$_0>, void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>::operator()(tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&) + 813\n9   libtensorflow_framework.so    \t0x000000010244e003 tensorflow::LocalRendezvousImpl::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function) + 883\n10  libtensorflow_framework.so    \t0x000000010294311f tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function) + 799\n11  _pywrap_tensorflow_internal.so\t0x000000010b1a40a9 tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::__1::function) + 1145\n12  libtensorflow_framework.so    \t0x00000001028ea458 tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::__1::function) + 872\n13  libtensorflow_framework.so    \t0x0000000102918202 tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4338\n14  libtensorflow_framework.so    \t0x000000010292210a std::__1::__function::__func, std::__1::allocator >, void ()>::operator()() + 58\n15  libtensorflow_framework.so    \t0x000000010255bdff Eigen::NonBlockingThreadPoolTempl::WorkerLoop(int) + 2047\n16  libtensorflow_framework.so    \t0x000000010255b4ff std::__1::__function::__func)::'lambda'(), std::__1::allocator)::'lambda'()>, void ()>::operator()() + 47\n17  libtensorflow_framework.so    \t0x00000001025808a0 void* std::__1::__thread_proxy >, std::__1::function > >(void*) + 48\n18  libsystem_pthread.dylib       \t0x00007fff53e376c1 _pthread_body + 340\n19  libsystem_pthread.dylib       \t0x00007fff53e3756d _pthread_start + 377\n20  libsystem_pthread.dylib       \t0x00007fff53e36c5d thread_start + 13\n\nThis appears similar to the other SEGFAULT issue that I have observed. There, the SEGFAULT occurs within CopyGPUTensorToCPU(). Here, the SEGFAULT occurs within CopyCPUTensorToGPU().", "body": "Hello @marcionicolau\r\n\r\nTrying out `mnist_mlp.py`, I am also observing a SEGFAULT. Here is a backtrace:\r\n\r\n<pre>\r\nThread 26 Crashed:\r\n0   libtensorflow_framework.so    \t0x0000000102959c10 void tensorflow::gtl::InlinedVector<tensorflow::EventMgr::InUse, 4>::emplace_back<tensorflow::EventMgr::InUse const&>(tensorflow::EventMgr::InUse const&&&) + 176\r\n1   libtensorflow_framework.so    \t0x000000010295902c tensorflow::EventMgr::PollEvents(bool, tensorflow::gtl::InlinedVector<tensorflow::EventMgr::InUse, 4>*) + 412\r\n2   libtensorflow_framework.so    \t0x00000001028f4022 tensorflow::EventMgr::ThenExecute(perftools::gputools::Stream*, std::__1::function<void ()>) + 194\r\n3   libtensorflow_framework.so    \t0x00000001028f4bad tensorflow::GPUUtil::CopyCPUTensorToGPU(tensorflow::Tensor const*, tensorflow::DeviceContext const*, tensorflow::Device*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 797\r\n4   libtensorflow_framework.so    \t0x00000001028f6a05 tensorflow::GPUDeviceContext::CopyCPUTensorToDevice(tensorflow::Tensor const*, tensorflow::Device*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) const + 117\r\n5   libtensorflow_framework.so    \t0x00000001029098b5 tensorflow::(anonymous namespace)::CopyHostToDevice(tensorflow::Tensor const*, tensorflow::Allocator*, tensorflow::Allocator*, tensorflow::StringPiece, tensorflow::Device*, tensorflow::Tensor*, tensorflow::DeviceContext*, std::__1::function<void (tensorflow::Status const&)>) + 437\r\n6   libtensorflow_framework.so    \t0x0000000102908b58 tensorflow::CopyTensor::ViaDMA(tensorflow::StringPiece, tensorflow::DeviceContext*, tensorflow::DeviceContext*, tensorflow::Device*, tensorflow::Device*, tensorflow::AllocatorAttributes, tensorflow::AllocatorAttributes, tensorflow::Tensor const*, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 3592\r\n7   libtensorflow_framework.so    \t0x0000000102942dbe tensorflow::IntraProcessRendezvous::SameWorkerRecvDone(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, tensorflow::Tensor*, std::__1::function<void (tensorflow::Status const&)>) + 1102\r\n8   libtensorflow_framework.so    \t0x000000010294385d std::__1::__function::__func<tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::$_0, std::__1::allocator<tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>)::$_0>, void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>::operator()(tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool&&) + 813\r\n9   libtensorflow_framework.so    \t0x000000010244e003 tensorflow::LocalRendezvousImpl::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>) + 883\r\n10  libtensorflow_framework.so    \t0x000000010294311f tensorflow::IntraProcessRendezvous::RecvAsync(tensorflow::Rendezvous::ParsedKey const&, tensorflow::Rendezvous::Args const&, std::__1::function<void (tensorflow::Status const&, tensorflow::Rendezvous::Args const&, tensorflow::Rendezvous::Args const&, tensorflow::Tensor const&, bool)>) + 799\r\n11  _pywrap_tensorflow_internal.so\t0x000000010b1a40a9 tensorflow::RecvOp::ComputeAsync(tensorflow::OpKernelContext*, std::__1::function<void ()>) + 1145\r\n12  libtensorflow_framework.so    \t0x00000001028ea458 tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::__1::function<void ()>) + 872\r\n13  libtensorflow_framework.so    \t0x0000000102918202 tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4338\r\n14  libtensorflow_framework.so    \t0x000000010292210a std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()() + 58\r\n15  libtensorflow_framework.so    \t0x000000010255bdff Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 2047\r\n16  libtensorflow_framework.so    \t0x000000010255b4ff std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47\r\n17  libtensorflow_framework.so    \t0x00000001025808a0 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48\r\n18  libsystem_pthread.dylib       \t0x00007fff53e376c1 _pthread_body + 340\r\n19  libsystem_pthread.dylib       \t0x00007fff53e3756d _pthread_start + 377\r\n20  libsystem_pthread.dylib       \t0x00007fff53e36c5d thread_start + 13\r\n</pre>\r\n\r\nThis appears similar to [the other SEGFAULT issue that I have observed](https://github.com/tensorflow/tensorflow/issues/9518#issuecomment-366854122). There, the SEGFAULT occurs within Copy**GPU**TensorToCPU(). Here, the SEGFAULT occurs within Copy**CPU**TensorToGPU()."}