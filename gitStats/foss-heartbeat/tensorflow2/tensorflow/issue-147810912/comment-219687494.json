{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/219687494", "html_url": "https://github.com/tensorflow/tensorflow/issues/1882#issuecomment-219687494", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1882", "id": 219687494, "node_id": "MDEyOklzc3VlQ29tbWVudDIxOTY4NzQ5NA==", "user": {"login": "felixmaximilian", "id": 1540236, "node_id": "MDQ6VXNlcjE1NDAyMzY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1540236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/felixmaximilian", "html_url": "https://github.com/felixmaximilian", "followers_url": "https://api.github.com/users/felixmaximilian/followers", "following_url": "https://api.github.com/users/felixmaximilian/following{/other_user}", "gists_url": "https://api.github.com/users/felixmaximilian/gists{/gist_id}", "starred_url": "https://api.github.com/users/felixmaximilian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/felixmaximilian/subscriptions", "organizations_url": "https://api.github.com/users/felixmaximilian/orgs", "repos_url": "https://api.github.com/users/felixmaximilian/repos", "events_url": "https://api.github.com/users/felixmaximilian/events{/privacy}", "received_events_url": "https://api.github.com/users/felixmaximilian/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-17T11:10:04Z", "updated_at": "2016-05-25T15:50:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4645780\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oweingrod\">@oweingrod</a>,</p>\n<p>you probably find the explanation for the slow training <a href=\"https://github.com/tensorflow/tensorflow/blob/d4bf5e072478c92cae2bd71b96e10e77e229dfba/tensorflow/examples/image_retraining/retrain.py#L497\">here</a>.<br>\nIt's said that bottleneck caching cannot be applied if distortions are enabled. That means the bottlenecks are calculated by the inception model for each image and each step.</p>\n<p>Even it makes sense that the randomized distortion is applied ad-hoc when an images is processed, I wonder if it would be possible to apply the distortions before the training is executed to create more training examples. Then we are able to precompute and cache the bottlenecks again. I see one tricky problem arising: The handling of train and validation splits will be trickier.</p>\n<p>Cheers,<br>\nMax</p>", "body_text": "Hi @oweingrod,\nyou probably find the explanation for the slow training here.\nIt's said that bottleneck caching cannot be applied if distortions are enabled. That means the bottlenecks are calculated by the inception model for each image and each step.\nEven it makes sense that the randomized distortion is applied ad-hoc when an images is processed, I wonder if it would be possible to apply the distortions before the training is executed to create more training examples. Then we are able to precompute and cache the bottlenecks again. I see one tricky problem arising: The handling of train and validation splits will be trickier.\nCheers,\nMax", "body": "Hi @oweingrod,\n\nyou probably find the explanation for the slow training [here](https://github.com/tensorflow/tensorflow/blob/d4bf5e072478c92cae2bd71b96e10e77e229dfba/tensorflow/examples/image_retraining/retrain.py#L497).\nIt's said that bottleneck caching cannot be applied if distortions are enabled. That means the bottlenecks are calculated by the inception model for each image and each step.\n\nEven it makes sense that the randomized distortion is applied ad-hoc when an images is processed, I wonder if it would be possible to apply the distortions before the training is executed to create more training examples. Then we are able to precompute and cache the bottlenecks again. I see one tricky problem arising: The handling of train and validation splits will be trickier.\n\nCheers,\nMax\n"}