{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14662", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14662/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14662/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14662/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14662", "id": 274962849, "node_id": "MDU6SXNzdWUyNzQ5NjI4NDk=", "number": 14662, "title": "Distributed TF hangs because of \"CreateSession still waiting for response from worker....\"", "user": {"login": "OscarDPan", "id": 10855426, "node_id": "MDQ6VXNlcjEwODU1NDI2", "avatar_url": "https://avatars0.githubusercontent.com/u/10855426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OscarDPan", "html_url": "https://github.com/OscarDPan", "followers_url": "https://api.github.com/users/OscarDPan/followers", "following_url": "https://api.github.com/users/OscarDPan/following{/other_user}", "gists_url": "https://api.github.com/users/OscarDPan/gists{/gist_id}", "starred_url": "https://api.github.com/users/OscarDPan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OscarDPan/subscriptions", "organizations_url": "https://api.github.com/users/OscarDPan/orgs", "repos_url": "https://api.github.com/users/OscarDPan/repos", "events_url": "https://api.github.com/users/OscarDPan/events{/privacy}", "received_events_url": "https://api.github.com/users/OscarDPan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2017-11-17T18:57:03Z", "updated_at": "2018-01-24T16:25:15Z", "closed_at": "2018-01-24T16:25:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p><strong>UPDATE:</strong> The first 2 posts are no longer appropriate to describe the issue. Please jump to my 3rd post.</p>\n<p>Hi,</p>\n<p>I followed the idea of this <a href=\"https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L1670\">https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L1670</a> to implement a worker sync queue. Everything seemed to work fine except when I increased the size of the dataset or number of workers: all workers hang when they try to evaluate the sync op. Typical example of my code is as follow:</p>\n<p>For each worker:</p>\n<pre><code>    def create_sync_queue_ops(self, op_prefix):\n        \"\"\"\n        op_prefix: a string that denote where the sync is being used.\n        \"\"\"\n        #Evenly distribute queues to all ps or workers\n        device_name = \"/job:ps/task:{}/cpu:0\".format(self.sync_queue_counter % self.num_ps_nodes)\n        self.sync_queue_counter += 1\n        with tf.device(device_name):\n            sync_queues = [tf.FIFOQueue(self.num_worker_nodes, [tf.bool], shapes=[[]], shared_name=\"{0}_{1}\".format(op_prefix,i)) for i in xrange(self.num_worker_nodes)]\n            token = tf.constant(False)\n            queue_ops = []\n            for i, q in enumerate(sync_queues):\n                if i == self.worker_id:\n                    queue_ops.append(tf.no_op())\n                else:\n                    queue_ops.append(q.enqueue(token))\n            #Drain tokens off queue for this worker after enqueuing ops\n            with tf.control_dependencies(queue_ops):\n                wait_ops = sync_queues[self.worker_id].dequeue_many(len(sync_queues)-1)\n            return wait_ops\n\n(some graph definition...)\nsess = tf.Session()\ndemo_sync_ops = self.create_sync_queue_ops(\"demo\")\nif self.is_chief_worker: #only execute by worker 0, other workers do nothing\n    sess.run(tf.global_variables_initializer())\nprint \"finishing message\"\nsess.run(demo_sync_ops)\n</code></pre>\n<p>I could <strong>occasionally</strong> see all workers hang after printing the \"finishing message\".<br>\nMy observation so far is that this only happened when dataset is huge or number of worker is big. e.g. 10+TB dataset with 300-500 workers.</p>\n<p>I haven't been able to see why this occurred, not sure if it is a TF issue or some network bottleneck that I was not aware of. Any help would be much appreciated!</p>", "body_text": "UPDATE: The first 2 posts are no longer appropriate to describe the issue. Please jump to my 3rd post.\nHi,\nI followed the idea of this https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L1670 to implement a worker sync queue. Everything seemed to work fine except when I increased the size of the dataset or number of workers: all workers hang when they try to evaluate the sync op. Typical example of my code is as follow:\nFor each worker:\n    def create_sync_queue_ops(self, op_prefix):\n        \"\"\"\n        op_prefix: a string that denote where the sync is being used.\n        \"\"\"\n        #Evenly distribute queues to all ps or workers\n        device_name = \"/job:ps/task:{}/cpu:0\".format(self.sync_queue_counter % self.num_ps_nodes)\n        self.sync_queue_counter += 1\n        with tf.device(device_name):\n            sync_queues = [tf.FIFOQueue(self.num_worker_nodes, [tf.bool], shapes=[[]], shared_name=\"{0}_{1}\".format(op_prefix,i)) for i in xrange(self.num_worker_nodes)]\n            token = tf.constant(False)\n            queue_ops = []\n            for i, q in enumerate(sync_queues):\n                if i == self.worker_id:\n                    queue_ops.append(tf.no_op())\n                else:\n                    queue_ops.append(q.enqueue(token))\n            #Drain tokens off queue for this worker after enqueuing ops\n            with tf.control_dependencies(queue_ops):\n                wait_ops = sync_queues[self.worker_id].dequeue_many(len(sync_queues)-1)\n            return wait_ops\n\n(some graph definition...)\nsess = tf.Session()\ndemo_sync_ops = self.create_sync_queue_ops(\"demo\")\nif self.is_chief_worker: #only execute by worker 0, other workers do nothing\n    sess.run(tf.global_variables_initializer())\nprint \"finishing message\"\nsess.run(demo_sync_ops)\n\nI could occasionally see all workers hang after printing the \"finishing message\".\nMy observation so far is that this only happened when dataset is huge or number of worker is big. e.g. 10+TB dataset with 300-500 workers.\nI haven't been able to see why this occurred, not sure if it is a TF issue or some network bottleneck that I was not aware of. Any help would be much appreciated!", "body": "**UPDATE:** The first 2 posts are no longer appropriate to describe the issue. Please jump to my 3rd post.\r\n\r\nHi,\r\n\r\nI followed the idea of this https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L1670 to implement a worker sync queue. Everything seemed to work fine except when I increased the size of the dataset or number of workers: all workers hang when they try to evaluate the sync op. Typical example of my code is as follow:\r\n\r\nFor each worker:\r\n```\r\n    def create_sync_queue_ops(self, op_prefix):\r\n        \"\"\"\r\n        op_prefix: a string that denote where the sync is being used.\r\n        \"\"\"\r\n        #Evenly distribute queues to all ps or workers\r\n        device_name = \"/job:ps/task:{}/cpu:0\".format(self.sync_queue_counter % self.num_ps_nodes)\r\n        self.sync_queue_counter += 1\r\n        with tf.device(device_name):\r\n            sync_queues = [tf.FIFOQueue(self.num_worker_nodes, [tf.bool], shapes=[[]], shared_name=\"{0}_{1}\".format(op_prefix,i)) for i in xrange(self.num_worker_nodes)]\r\n            token = tf.constant(False)\r\n            queue_ops = []\r\n            for i, q in enumerate(sync_queues):\r\n                if i == self.worker_id:\r\n                    queue_ops.append(tf.no_op())\r\n                else:\r\n                    queue_ops.append(q.enqueue(token))\r\n            #Drain tokens off queue for this worker after enqueuing ops\r\n            with tf.control_dependencies(queue_ops):\r\n                wait_ops = sync_queues[self.worker_id].dequeue_many(len(sync_queues)-1)\r\n            return wait_ops\r\n\r\n(some graph definition...)\r\nsess = tf.Session()\r\ndemo_sync_ops = self.create_sync_queue_ops(\"demo\")\r\nif self.is_chief_worker: #only execute by worker 0, other workers do nothing\r\n    sess.run(tf.global_variables_initializer())\r\nprint \"finishing message\"\r\nsess.run(demo_sync_ops)\r\n```\r\n\r\nI could **occasionally** see all workers hang after printing the \"finishing message\". \r\nMy observation so far is that this only happened when dataset is huge or number of worker is big. e.g. 10+TB dataset with 300-500 workers.\r\n\r\nI haven't been able to see why this occurred, not sure if it is a TF issue or some network bottleneck that I was not aware of. Any help would be much appreciated!\r\n  "}