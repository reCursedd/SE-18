{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439638338", "html_url": "https://github.com/tensorflow/tensorflow/issues/6503#issuecomment-439638338", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6503", "id": 439638338, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTYzODMzOA==", "user": {"login": "JaeDukSeo", "id": 22832406, "node_id": "MDQ6VXNlcjIyODMyNDA2", "avatar_url": "https://avatars1.githubusercontent.com/u/22832406?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JaeDukSeo", "html_url": "https://github.com/JaeDukSeo", "followers_url": "https://api.github.com/users/JaeDukSeo/followers", "following_url": "https://api.github.com/users/JaeDukSeo/following{/other_user}", "gists_url": "https://api.github.com/users/JaeDukSeo/gists{/gist_id}", "starred_url": "https://api.github.com/users/JaeDukSeo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JaeDukSeo/subscriptions", "organizations_url": "https://api.github.com/users/JaeDukSeo/orgs", "repos_url": "https://api.github.com/users/JaeDukSeo/repos", "events_url": "https://api.github.com/users/JaeDukSeo/events{/privacy}", "received_events_url": "https://api.github.com/users/JaeDukSeo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-17T18:39:23Z", "updated_at": "2018-11-17T18:39:23Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Hi, I have composed one gradient function based on Matrix-backpropagation paper. Hope it helps.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">matrix_symmetric</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> (x <span class=\"pl-k\">+</span> tf.transpose(x, [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>])) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_eigen_K</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">square</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Get K = 1 / (sigma_i - sigma_j) for i != j, 0 otherwise</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Parameters</span>\n<span class=\"pl-s\">    ----------</span>\n<span class=\"pl-s\">    x : tf.Tensor with shape as [..., dim,]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns</span>\n<span class=\"pl-s\">    -------</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">if</span> square:\n        x <span class=\"pl-k\">=</span> tf.square(x)\n    res <span class=\"pl-k\">=</span> tf.expand_dims(x, <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">-</span> tf.expand_dims(x, <span class=\"pl-c1\">2</span>)\n    res <span class=\"pl-k\">+=</span> tf.eye(tf.shape(res)[<span class=\"pl-c1\">1</span>])\n    res <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">/</span> res\n    res <span class=\"pl-k\">-=</span> tf.eye(tf.shape(res)[<span class=\"pl-c1\">1</span>])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Keep the results clean</span>\n    res <span class=\"pl-k\">=</span> tf.where(tf.is_nan(res), tf.zeros_like(res), res)\n    res <span class=\"pl-k\">=</span> tf.where(tf.is_inf(res), tf.zeros_like(res), res)\n    <span class=\"pl-k\">return</span> res\n\n<span class=\"pl-en\">@tf.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Svd<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">gradient_svd</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad_s</span>, <span class=\"pl-smi\">grad_u</span>, <span class=\"pl-smi\">grad_v</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Define the gradient for SVD</span>\n<span class=\"pl-s\">    References</span>\n<span class=\"pl-s\">        Ionescu, C., et al, Matrix Backpropagation for Deep Networks with Structured Layers</span>\n<span class=\"pl-s\">        </span>\n<span class=\"pl-s\">    Parameters</span>\n<span class=\"pl-s\">    ----------</span>\n<span class=\"pl-s\">    op</span>\n<span class=\"pl-s\">    grad_s</span>\n<span class=\"pl-s\">    grad_u</span>\n<span class=\"pl-s\">    grad_v</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns</span>\n<span class=\"pl-s\">    -------</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    s, u, v <span class=\"pl-k\">=</span> op.outputs\n    v_t <span class=\"pl-k\">=</span> tf.transpose(v, [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>])\n\n    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>K<span class=\"pl-pds\">'</span></span>):\n        K <span class=\"pl-k\">=</span> get_eigen_K(s, <span class=\"pl-c1\">True</span>)\n    inner <span class=\"pl-k\">=</span> matrix_symmetric(K <span class=\"pl-k\">*</span> tf.matmul(v_t, grad_v))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the shape accordingly.</span>\n    u_shape <span class=\"pl-k\">=</span> u.get_shape()[<span class=\"pl-c1\">1</span>].value\n    v_shape <span class=\"pl-k\">=</span> v.get_shape()[<span class=\"pl-c1\">1</span>].value\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Recover the complete S matrices and its gradient</span>\n    eye_mat <span class=\"pl-k\">=</span> tf.eye(v_shape, u_shape)\n    realS <span class=\"pl-k\">=</span> tf.matmul(tf.reshape(tf.matrix_diag(s), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, v_shape]), eye_mat)\n    realS <span class=\"pl-k\">=</span> tf.transpose(tf.reshape(realS, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, v_shape, u_shape]), [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])\n\n    real_grad_S <span class=\"pl-k\">=</span> tf.matmul(tf.reshape(tf.matrix_diag(grad_s), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, v_shape]), eye_mat)\n    real_grad_S <span class=\"pl-k\">=</span> tf.transpose(tf.reshape(real_grad_S, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, v_shape, u_shape]), [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])\n\n    dxdz <span class=\"pl-k\">=</span> tf.matmul(u, tf.matmul(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> tf.matmul(realS, inner) <span class=\"pl-k\">+</span> real_grad_S, v_t))\n    <span class=\"pl-k\">return</span> dxdz</pre></div>\n</blockquote>\n<p>this is very useful, we are assuming that we don't use the U matrix when we have decomposed the original matrix A into U s V, since we do not calculate the derivative respect to U anywhere.</p>", "body_text": "Hi, I have composed one gradient function based on Matrix-backpropagation paper. Hope it helps.\ndef matrix_symmetric(x):\n    return (x + tf.transpose(x, [0,2,1])) / 2\n\ndef get_eigen_K(x, square=False):\n    \"\"\"\n    Get K = 1 / (sigma_i - sigma_j) for i != j, 0 otherwise\n\n    Parameters\n    ----------\n    x : tf.Tensor with shape as [..., dim,]\n\n    Returns\n    -------\n\n    \"\"\"\n    if square:\n        x = tf.square(x)\n    res = tf.expand_dims(x, 1) - tf.expand_dims(x, 2)\n    res += tf.eye(tf.shape(res)[1])\n    res = 1 / res\n    res -= tf.eye(tf.shape(res)[1])\n\n    # Keep the results clean\n    res = tf.where(tf.is_nan(res), tf.zeros_like(res), res)\n    res = tf.where(tf.is_inf(res), tf.zeros_like(res), res)\n    return res\n\n@tf.RegisterGradient('Svd')\ndef gradient_svd(op, grad_s, grad_u, grad_v):\n    \"\"\"\n    Define the gradient for SVD\n    References\n        Ionescu, C., et al, Matrix Backpropagation for Deep Networks with Structured Layers\n        \n    Parameters\n    ----------\n    op\n    grad_s\n    grad_u\n    grad_v\n\n    Returns\n    -------\n    \"\"\"\n    s, u, v = op.outputs\n    v_t = tf.transpose(v, [0,2,1])\n\n    with tf.name_scope('K'):\n        K = get_eigen_K(s, True)\n    inner = matrix_symmetric(K * tf.matmul(v_t, grad_v))\n\n    # Create the shape accordingly.\n    u_shape = u.get_shape()[1].value\n    v_shape = v.get_shape()[1].value\n\n    # Recover the complete S matrices and its gradient\n    eye_mat = tf.eye(v_shape, u_shape)\n    realS = tf.matmul(tf.reshape(tf.matrix_diag(s), [-1, v_shape]), eye_mat)\n    realS = tf.transpose(tf.reshape(realS, [-1, v_shape, u_shape]), [0, 2, 1])\n\n    real_grad_S = tf.matmul(tf.reshape(tf.matrix_diag(grad_s), [-1, v_shape]), eye_mat)\n    real_grad_S = tf.transpose(tf.reshape(real_grad_S, [-1, v_shape, u_shape]), [0, 2, 1])\n\n    dxdz = tf.matmul(u, tf.matmul(2 * tf.matmul(realS, inner) + real_grad_S, v_t))\n    return dxdz\n\nthis is very useful, we are assuming that we don't use the U matrix when we have decomposed the original matrix A into U s V, since we do not calculate the derivative respect to U anywhere.", "body": "> Hi, I have composed one gradient function based on Matrix-backpropagation paper. Hope it helps.\r\n> \r\n> ```python\r\n> def matrix_symmetric(x):\r\n>     return (x + tf.transpose(x, [0,2,1])) / 2\r\n> \r\n> def get_eigen_K(x, square=False):\r\n>     \"\"\"\r\n>     Get K = 1 / (sigma_i - sigma_j) for i != j, 0 otherwise\r\n> \r\n>     Parameters\r\n>     ----------\r\n>     x : tf.Tensor with shape as [..., dim,]\r\n> \r\n>     Returns\r\n>     -------\r\n> \r\n>     \"\"\"\r\n>     if square:\r\n>         x = tf.square(x)\r\n>     res = tf.expand_dims(x, 1) - tf.expand_dims(x, 2)\r\n>     res += tf.eye(tf.shape(res)[1])\r\n>     res = 1 / res\r\n>     res -= tf.eye(tf.shape(res)[1])\r\n> \r\n>     # Keep the results clean\r\n>     res = tf.where(tf.is_nan(res), tf.zeros_like(res), res)\r\n>     res = tf.where(tf.is_inf(res), tf.zeros_like(res), res)\r\n>     return res\r\n> \r\n> @tf.RegisterGradient('Svd')\r\n> def gradient_svd(op, grad_s, grad_u, grad_v):\r\n>     \"\"\"\r\n>     Define the gradient for SVD\r\n>     References\r\n>         Ionescu, C., et al, Matrix Backpropagation for Deep Networks with Structured Layers\r\n>         \r\n>     Parameters\r\n>     ----------\r\n>     op\r\n>     grad_s\r\n>     grad_u\r\n>     grad_v\r\n> \r\n>     Returns\r\n>     -------\r\n>     \"\"\"\r\n>     s, u, v = op.outputs\r\n>     v_t = tf.transpose(v, [0,2,1])\r\n> \r\n>     with tf.name_scope('K'):\r\n>         K = get_eigen_K(s, True)\r\n>     inner = matrix_symmetric(K * tf.matmul(v_t, grad_v))\r\n> \r\n>     # Create the shape accordingly.\r\n>     u_shape = u.get_shape()[1].value\r\n>     v_shape = v.get_shape()[1].value\r\n> \r\n>     # Recover the complete S matrices and its gradient\r\n>     eye_mat = tf.eye(v_shape, u_shape)\r\n>     realS = tf.matmul(tf.reshape(tf.matrix_diag(s), [-1, v_shape]), eye_mat)\r\n>     realS = tf.transpose(tf.reshape(realS, [-1, v_shape, u_shape]), [0, 2, 1])\r\n> \r\n>     real_grad_S = tf.matmul(tf.reshape(tf.matrix_diag(grad_s), [-1, v_shape]), eye_mat)\r\n>     real_grad_S = tf.transpose(tf.reshape(real_grad_S, [-1, v_shape, u_shape]), [0, 2, 1])\r\n> \r\n>     dxdz = tf.matmul(u, tf.matmul(2 * tf.matmul(realS, inner) + real_grad_S, v_t))\r\n>     return dxdz\r\n> ```\r\n\r\nthis is very useful, we are assuming that we don't use the U matrix when we have decomposed the original matrix A into U s V, since we do not calculate the derivative respect to U anywhere. "}