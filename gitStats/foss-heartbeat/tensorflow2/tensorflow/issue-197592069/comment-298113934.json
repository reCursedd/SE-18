{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/298113934", "html_url": "https://github.com/tensorflow/tensorflow/issues/6503#issuecomment-298113934", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6503", "id": 298113934, "node_id": "MDEyOklzc3VlQ29tbWVudDI5ODExMzkzNA==", "user": {"login": "kofd", "id": 5904259, "node_id": "MDQ6VXNlcjU5MDQyNTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/5904259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kofd", "html_url": "https://github.com/kofd", "followers_url": "https://api.github.com/users/kofd/followers", "following_url": "https://api.github.com/users/kofd/following{/other_user}", "gists_url": "https://api.github.com/users/kofd/gists{/gist_id}", "starred_url": "https://api.github.com/users/kofd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kofd/subscriptions", "organizations_url": "https://api.github.com/users/kofd/orgs", "repos_url": "https://api.github.com/users/kofd/repos", "events_url": "https://api.github.com/users/kofd/events{/privacy}", "received_events_url": "https://api.github.com/users/kofd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-28T21:45:06Z", "updated_at": "2017-04-28T21:46:16Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">svd</span>(<span class=\"pl-smi\">A</span>, <span class=\"pl-smi\">full_matrices</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">compute_uv</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> since dA = dUSVt + UdSVt + USdVt</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> we can simply recompute each matrix using A = USVt</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> while blocking gradients to the original op.</span>\n  _, M, N <span class=\"pl-k\">=</span> A.get_shape().as_list()\n  P <span class=\"pl-k\">=</span> <span class=\"pl-c1\">min</span>(M, N)\n  S0, U0, V0 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">map</span>(tf.stop_gradient, tf.svd(A, <span class=\"pl-v\">full_matrices</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>name))\n  Ui, Vti <span class=\"pl-k\">=</span> <span class=\"pl-c1\">map</span>(tf.matrix_inverse, [U0, tf.transpose(V0, (<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>))])\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> A = USVt</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> S = UiAVti</span>\n  S <span class=\"pl-k\">=</span> tf.matmul(Ui, tf.matmul(A, Vti))\n  S <span class=\"pl-k\">=</span> tf.matrix_diag_part(S)\n  <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> compute_uv:\n    <span class=\"pl-k\">return</span> S\n  Si <span class=\"pl-k\">=</span> tf.pad(tf.matrix_diag(<span class=\"pl-c1\">1</span><span class=\"pl-k\">/</span>S0), [[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>,N<span class=\"pl-k\">-</span>P], [<span class=\"pl-c1\">0</span>,M<span class=\"pl-k\">-</span>P]])\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> U = AVtiSi</span>\n  U <span class=\"pl-k\">=</span> tf.matmul(A, tf.matmul(Vti, Si))\n  U <span class=\"pl-k\">=</span> U <span class=\"pl-k\">if</span> full_matrices <span class=\"pl-k\">else</span> U[:, :M, :P]\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Vt = SiUiA</span>\n  V <span class=\"pl-k\">=</span> tf.transpose(tf.matmul(Si, tf.matmul(Ui, A)), (<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>))\n  V <span class=\"pl-k\">=</span> V <span class=\"pl-k\">if</span> full_matrices <span class=\"pl-k\">else</span> V[:, :N, :P]\n  <span class=\"pl-k\">return</span> S, U, V</pre></div>", "body_text": "def svd(A, full_matrices=False, compute_uv=True, name=None):\n  # since dA = dUSVt + UdSVt + USdVt\n  # we can simply recompute each matrix using A = USVt\n  # while blocking gradients to the original op.\n  _, M, N = A.get_shape().as_list()\n  P = min(M, N)\n  S0, U0, V0 = map(tf.stop_gradient, tf.svd(A, full_matrices=True, name=name))\n  Ui, Vti = map(tf.matrix_inverse, [U0, tf.transpose(V0, (0, 2, 1))])\n  # A = USVt\n  # S = UiAVti\n  S = tf.matmul(Ui, tf.matmul(A, Vti))\n  S = tf.matrix_diag_part(S)\n  if not compute_uv:\n    return S\n  Si = tf.pad(tf.matrix_diag(1/S0), [[0,0], [0,N-P], [0,M-P]])\n  # U = AVtiSi\n  U = tf.matmul(A, tf.matmul(Vti, Si))\n  U = U if full_matrices else U[:, :M, :P]\n  # Vt = SiUiA\n  V = tf.transpose(tf.matmul(Si, tf.matmul(Ui, A)), (0, 2, 1))\n  V = V if full_matrices else V[:, :N, :P]\n  return S, U, V", "body": "```python\r\ndef svd(A, full_matrices=False, compute_uv=True, name=None):\r\n  # since dA = dUSVt + UdSVt + USdVt\r\n  # we can simply recompute each matrix using A = USVt\r\n  # while blocking gradients to the original op.\r\n  _, M, N = A.get_shape().as_list()\r\n  P = min(M, N)\r\n  S0, U0, V0 = map(tf.stop_gradient, tf.svd(A, full_matrices=True, name=name))\r\n  Ui, Vti = map(tf.matrix_inverse, [U0, tf.transpose(V0, (0, 2, 1))])\r\n  # A = USVt\r\n  # S = UiAVti\r\n  S = tf.matmul(Ui, tf.matmul(A, Vti))\r\n  S = tf.matrix_diag_part(S)\r\n  if not compute_uv:\r\n    return S\r\n  Si = tf.pad(tf.matrix_diag(1/S0), [[0,0], [0,N-P], [0,M-P]])\r\n  # U = AVtiSi\r\n  U = tf.matmul(A, tf.matmul(Vti, Si))\r\n  U = U if full_matrices else U[:, :M, :P]\r\n  # Vt = SiUiA\r\n  V = tf.transpose(tf.matmul(Si, tf.matmul(Ui, A)), (0, 2, 1))\r\n  V = V if full_matrices else V[:, :N, :P]\r\n  return S, U, V\r\n```"}