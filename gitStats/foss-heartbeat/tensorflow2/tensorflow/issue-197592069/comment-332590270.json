{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/332590270", "html_url": "https://github.com/tensorflow/tensorflow/issues/6503#issuecomment-332590270", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6503", "id": 332590270, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjU5MDI3MA==", "user": {"login": "hicham-eyeem", "id": 24877550, "node_id": "MDQ6VXNlcjI0ODc3NTUw", "avatar_url": "https://avatars1.githubusercontent.com/u/24877550?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hicham-eyeem", "html_url": "https://github.com/hicham-eyeem", "followers_url": "https://api.github.com/users/hicham-eyeem/followers", "following_url": "https://api.github.com/users/hicham-eyeem/following{/other_user}", "gists_url": "https://api.github.com/users/hicham-eyeem/gists{/gist_id}", "starred_url": "https://api.github.com/users/hicham-eyeem/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hicham-eyeem/subscriptions", "organizations_url": "https://api.github.com/users/hicham-eyeem/orgs", "repos_url": "https://api.github.com/users/hicham-eyeem/repos", "events_url": "https://api.github.com/users/hicham-eyeem/events{/privacy}", "received_events_url": "https://api.github.com/users/hicham-eyeem/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-27T17:09:13Z", "updated_at": "2017-09-27T17:09:13Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> ah ok, thank you for pointing that out and actually even adding some regularisation doesn't help. Do you know the reason why it would give NaNs sometimes?<br>\nI guess also we can avoid using SVD by rather using a matrix factorization formulation if it's used in the loss function, since the matrix factorization formulation would require only matmul and transpose ops (+ some constraints that can be linearized with a proximal form)</p>", "body_text": "@yaroslavvb ah ok, thank you for pointing that out and actually even adding some regularisation doesn't help. Do you know the reason why it would give NaNs sometimes?\nI guess also we can avoid using SVD by rather using a matrix factorization formulation if it's used in the loss function, since the matrix factorization formulation would require only matmul and transpose ops (+ some constraints that can be linearized with a proximal form)", "body": "@yaroslavvb ah ok, thank you for pointing that out and actually even adding some regularisation doesn't help. Do you know the reason why it would give NaNs sometimes?\r\nI guess also we can avoid using SVD by rather using a matrix factorization formulation if it's used in the loss function, since the matrix factorization formulation would require only matmul and transpose ops (+ some constraints that can be linearized with a proximal form)"}