{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/861", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/861/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/861/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/861/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/861", "id": 128378400, "node_id": "MDU6SXNzdWUxMjgzNzg0MDA=", "number": 861, "title": "Noise in Variable assignment", "user": {"login": "srjoglekar246", "id": 2099293, "node_id": "MDQ6VXNlcjIwOTkyOTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/2099293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srjoglekar246", "html_url": "https://github.com/srjoglekar246", "followers_url": "https://api.github.com/users/srjoglekar246/followers", "following_url": "https://api.github.com/users/srjoglekar246/following{/other_user}", "gists_url": "https://api.github.com/users/srjoglekar246/gists{/gist_id}", "starred_url": "https://api.github.com/users/srjoglekar246/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srjoglekar246/subscriptions", "organizations_url": "https://api.github.com/users/srjoglekar246/orgs", "repos_url": "https://api.github.com/users/srjoglekar246/repos", "events_url": "https://api.github.com/users/srjoglekar246/events{/privacy}", "received_events_url": "https://api.github.com/users/srjoglekar246/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-01-24T07:15:29Z", "updated_at": "2016-01-24T20:22:17Z", "closed_at": "2016-01-24T20:22:17Z", "author_association": "MEMBER", "body_html": "<p>I am writing a code to use an LSTM in the following manner:</p>\n<ol>\n<li>Given an input <em>x</em> of dimensionality <em>d</em> at time <em>t</em>, two quantities are computed:<br>\na. The difference <em>x(t) - x(t-1)</em>. I call this the first derivative of <em>x</em>: <em>x'(t)</em>.<br>\nb. The difference <em>x'(t) - x'(t-1)</em>. I call this the second derivative of <em>x</em>: <em>x''(t)</em>.</li>\n<li><em>x(t)</em>, <em>x'(t)</em>, <em>x''(t)</em> are concatenated as the total input to an LSTM of state size <em>3xd</em>.</li>\n<li>The output from the LSTM, say <em>L(t)</em> is then passed onto a <em>d</em>-dimensional densely connected linear layer. So the final output out is <em>f(t) = W x L(t) + b</em></li>\n</ol>\n<p>I am using the <em>AdamOptimizer</em> with a learning rate of 0.0005.</p>\n<p>The problem I am facing is, when I do steps 1 and 2 as given above <em>outside</em> TensorFlow, the model works well. However, when I try to do the computation of derivatives inside a TensorFlow Graph, there seems to be some noise seeping into the values of the variables. Heres the code:</p>\n<pre><code>##The Input Layer as a Placeholder\n#Since we will provide data sequentially, the 'batch size'\n#is 1.\ninput_layer = tf.placeholder(tf.float32, [1, input_dim])\n\n##First Order Derivative Layer\n#This will store the last recorded value\nlast_value1 = tf.Variable(tf.zeros([1, input_dim]))\n#Subtract last value from current\nsub_value1 = tf.sub(input_layer, last_value1)\n#Update last recorded value\nlast_assign_op1 = last_value1.assign(input_layer)\n\n##Second Order Derivative Layer\n#This will store the last recorded derivative\nlast_value2 = tf.Variable(tf.zeros([1, input_dim]))\n#Subtract last value from current\nsub_value2 = tf.sub(sub_value1, last_value2)\n#Update last recorded value\nlast_assign_op2 = last_value2.assign(sub_value1)\n\n##Overall input to the LSTM\n#x and its first and second order derivatives as outputs of\n#earlier layers\nzero_order = input_layer\nfirst_order = sub_value1\nsecond_order = sub_value2\n#Concatenated\ntotal_input = tf.concat(1, [zero_order, first_order, second_order])\n</code></pre>\n<p>While running the model, I run in order: final_output (which is dependent on total_input), then last_assign_op1 and then last_assign_op2.<br>\nHowever, there always seems to be a difference of about 0.001 between what the actual values of <em>last_value1</em> and <em>last_value2</em> should be, and what they really are. This is causing a lot of noise in the final output.</p>\n<p>What is the reason for this? Is this somehow because of the Optimizer (I can't figure out why that would be)? Or am I doing something wrong?</p>", "body_text": "I am writing a code to use an LSTM in the following manner:\n\nGiven an input x of dimensionality d at time t, two quantities are computed:\na. The difference x(t) - x(t-1). I call this the first derivative of x: x'(t).\nb. The difference x'(t) - x'(t-1). I call this the second derivative of x: x''(t).\nx(t), x'(t), x''(t) are concatenated as the total input to an LSTM of state size 3xd.\nThe output from the LSTM, say L(t) is then passed onto a d-dimensional densely connected linear layer. So the final output out is f(t) = W x L(t) + b\n\nI am using the AdamOptimizer with a learning rate of 0.0005.\nThe problem I am facing is, when I do steps 1 and 2 as given above outside TensorFlow, the model works well. However, when I try to do the computation of derivatives inside a TensorFlow Graph, there seems to be some noise seeping into the values of the variables. Heres the code:\n##The Input Layer as a Placeholder\n#Since we will provide data sequentially, the 'batch size'\n#is 1.\ninput_layer = tf.placeholder(tf.float32, [1, input_dim])\n\n##First Order Derivative Layer\n#This will store the last recorded value\nlast_value1 = tf.Variable(tf.zeros([1, input_dim]))\n#Subtract last value from current\nsub_value1 = tf.sub(input_layer, last_value1)\n#Update last recorded value\nlast_assign_op1 = last_value1.assign(input_layer)\n\n##Second Order Derivative Layer\n#This will store the last recorded derivative\nlast_value2 = tf.Variable(tf.zeros([1, input_dim]))\n#Subtract last value from current\nsub_value2 = tf.sub(sub_value1, last_value2)\n#Update last recorded value\nlast_assign_op2 = last_value2.assign(sub_value1)\n\n##Overall input to the LSTM\n#x and its first and second order derivatives as outputs of\n#earlier layers\nzero_order = input_layer\nfirst_order = sub_value1\nsecond_order = sub_value2\n#Concatenated\ntotal_input = tf.concat(1, [zero_order, first_order, second_order])\n\nWhile running the model, I run in order: final_output (which is dependent on total_input), then last_assign_op1 and then last_assign_op2.\nHowever, there always seems to be a difference of about 0.001 between what the actual values of last_value1 and last_value2 should be, and what they really are. This is causing a lot of noise in the final output.\nWhat is the reason for this? Is this somehow because of the Optimizer (I can't figure out why that would be)? Or am I doing something wrong?", "body": "I am writing a code to use an LSTM in the following manner:\n1. Given an input _x_ of dimensionality _d_ at time _t_, two quantities are computed:\n   a. The difference _x(t) - x(t-1)_. I call this the first derivative of _x_: _x'(t)_.\n   b. The difference _x'(t) - x'(t-1)_. I call this the second derivative of _x_: _x''(t)_.\n2. _x(t)_, _x'(t)_, _x''(t)_ are concatenated as the total input to an LSTM of state size _3xd_.\n3. The output from the LSTM, say _L(t)_ is then passed onto a _d_-dimensional densely connected linear layer. So the final output out is _f(t) = W x L(t) + b_\n\nI am using the _AdamOptimizer_ with a learning rate of 0.0005.\n\nThe problem I am facing is, when I do steps 1 and 2 as given above _outside_ TensorFlow, the model works well. However, when I try to do the computation of derivatives inside a TensorFlow Graph, there seems to be some noise seeping into the values of the variables. Heres the code:\n\n```\n##The Input Layer as a Placeholder\n#Since we will provide data sequentially, the 'batch size'\n#is 1.\ninput_layer = tf.placeholder(tf.float32, [1, input_dim])\n\n##First Order Derivative Layer\n#This will store the last recorded value\nlast_value1 = tf.Variable(tf.zeros([1, input_dim]))\n#Subtract last value from current\nsub_value1 = tf.sub(input_layer, last_value1)\n#Update last recorded value\nlast_assign_op1 = last_value1.assign(input_layer)\n\n##Second Order Derivative Layer\n#This will store the last recorded derivative\nlast_value2 = tf.Variable(tf.zeros([1, input_dim]))\n#Subtract last value from current\nsub_value2 = tf.sub(sub_value1, last_value2)\n#Update last recorded value\nlast_assign_op2 = last_value2.assign(sub_value1)\n\n##Overall input to the LSTM\n#x and its first and second order derivatives as outputs of\n#earlier layers\nzero_order = input_layer\nfirst_order = sub_value1\nsecond_order = sub_value2\n#Concatenated\ntotal_input = tf.concat(1, [zero_order, first_order, second_order])\n```\n\nWhile running the model, I run in order: final_output (which is dependent on total_input), then last_assign_op1 and then last_assign_op2.\nHowever, there always seems to be a difference of about 0.001 between what the actual values of _last_value1_ and _last_value2_ should be, and what they really are. This is causing a lot of noise in the final output.\n\nWhat is the reason for this? Is this somehow because of the Optimizer (I can't figure out why that would be)? Or am I doing something wrong?\n"}