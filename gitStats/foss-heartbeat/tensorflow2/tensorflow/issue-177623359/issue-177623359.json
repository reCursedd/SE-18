{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4429", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4429/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4429/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4429/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4429", "id": 177623359, "node_id": "MDU6SXNzdWUxNzc2MjMzNTk=", "number": 4429, "title": "A very simple script using Optimizer.compute_gradients() prodeces Nan for all gradients", "user": {"login": "lan2720", "id": 5330101, "node_id": "MDQ6VXNlcjUzMzAxMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/5330101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lan2720", "html_url": "https://github.com/lan2720", "followers_url": "https://api.github.com/users/lan2720/followers", "following_url": "https://api.github.com/users/lan2720/following{/other_user}", "gists_url": "https://api.github.com/users/lan2720/gists{/gist_id}", "starred_url": "https://api.github.com/users/lan2720/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lan2720/subscriptions", "organizations_url": "https://api.github.com/users/lan2720/orgs", "repos_url": "https://api.github.com/users/lan2720/repos", "events_url": "https://api.github.com/users/lan2720/events{/privacy}", "received_events_url": "https://api.github.com/users/lan2720/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-09-18T02:03:48Z", "updated_at": "2016-09-18T05:30:19Z", "closed_at": "2016-09-18T05:30:19Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nbatch_size = 5\ndim = 3\nhidden_units = 8\n\n\nsess = tf.Session()\n\nwith sess.as_default():\n    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n    w = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units]), name=\"w\")\n    b = tf.Variable(initial_value=tf.zeros(shape=[hidden_units]), name=\"b\")\n    logits = tf.nn.tanh(tf.matmul(x, w) + b)\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y,name=\"xentropy\")\n    # define model end\n\n\n    # begin training\n    optimizer = tf.train.GradientDescentOptimizer(1e-5)\n    grads_and_vars = optimizer.compute_gradients(cross_entropy, tf.trainable_variables())\n\n    # generate data\n    data = np.random.randn(batch_size, dim)\n    labels = np.random.randint(0, 10, size=batch_size)\n\n    sess.run(tf.initialize_all_variables())\n    gradients_and_vars = sess.run(grads_and_vars, feed_dict={x:data, y:labels})\n    for g, v in gradients_and_vars:\n        if g is not None:\n            print \"****************this is variable*************\"\n            print \"variable's shape:\", v.shape\n            print v\n            print \"****************this is gradient*************\"\n            print \"gradient's shape:\", g.shape\n            print g\n\nsess.close()\n</code></pre>\n<p>Run it, I got the following result.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/5330101/18612210/706d7be2-7d86-11e6-9796-ab98b73b74ae.png\"><img src=\"https://cloud.githubusercontent.com/assets/5330101/18612210/706d7be2-7d86-11e6-9796-ab98b73b74ae.png\" alt=\"selection_107\" style=\"max-width:100%;\"></a></p>\n<p>As you see all GRADIENTS are Nan. Is there anything wrong in my code or PC? I don't know why it cannot compute correct gradients.</p>", "body_text": "Hi all,\nimport tensorflow as tf\nimport numpy as np\n\nbatch_size = 5\ndim = 3\nhidden_units = 8\n\n\nsess = tf.Session()\n\nwith sess.as_default():\n    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n    w = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units]), name=\"w\")\n    b = tf.Variable(initial_value=tf.zeros(shape=[hidden_units]), name=\"b\")\n    logits = tf.nn.tanh(tf.matmul(x, w) + b)\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y,name=\"xentropy\")\n    # define model end\n\n\n    # begin training\n    optimizer = tf.train.GradientDescentOptimizer(1e-5)\n    grads_and_vars = optimizer.compute_gradients(cross_entropy, tf.trainable_variables())\n\n    # generate data\n    data = np.random.randn(batch_size, dim)\n    labels = np.random.randint(0, 10, size=batch_size)\n\n    sess.run(tf.initialize_all_variables())\n    gradients_and_vars = sess.run(grads_and_vars, feed_dict={x:data, y:labels})\n    for g, v in gradients_and_vars:\n        if g is not None:\n            print \"****************this is variable*************\"\n            print \"variable's shape:\", v.shape\n            print v\n            print \"****************this is gradient*************\"\n            print \"gradient's shape:\", g.shape\n            print g\n\nsess.close()\n\nRun it, I got the following result.\n\nAs you see all GRADIENTS are Nan. Is there anything wrong in my code or PC? I don't know why it cannot compute correct gradients.", "body": "Hi all, \n\n```\nimport tensorflow as tf\nimport numpy as np\n\nbatch_size = 5\ndim = 3\nhidden_units = 8\n\n\nsess = tf.Session()\n\nwith sess.as_default():\n    x = tf.placeholder(dtype=tf.float32, shape=[None, dim], name=\"x\")\n    y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n    w = tf.Variable(initial_value=tf.random_normal(shape=[dim, hidden_units]), name=\"w\")\n    b = tf.Variable(initial_value=tf.zeros(shape=[hidden_units]), name=\"b\")\n    logits = tf.nn.tanh(tf.matmul(x, w) + b)\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y,name=\"xentropy\")\n    # define model end\n\n\n    # begin training\n    optimizer = tf.train.GradientDescentOptimizer(1e-5)\n    grads_and_vars = optimizer.compute_gradients(cross_entropy, tf.trainable_variables())\n\n    # generate data\n    data = np.random.randn(batch_size, dim)\n    labels = np.random.randint(0, 10, size=batch_size)\n\n    sess.run(tf.initialize_all_variables())\n    gradients_and_vars = sess.run(grads_and_vars, feed_dict={x:data, y:labels})\n    for g, v in gradients_and_vars:\n        if g is not None:\n            print \"****************this is variable*************\"\n            print \"variable's shape:\", v.shape\n            print v\n            print \"****************this is gradient*************\"\n            print \"gradient's shape:\", g.shape\n            print g\n\nsess.close()\n```\n\nRun it, I got the following result.\n\n![selection_107](https://cloud.githubusercontent.com/assets/5330101/18612210/706d7be2-7d86-11e6-9796-ab98b73b74ae.png)\n\nAs you see all GRADIENTS are Nan. Is there anything wrong in my code or PC? I don't know why it cannot compute correct gradients.\n"}