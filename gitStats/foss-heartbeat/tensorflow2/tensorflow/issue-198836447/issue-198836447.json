{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6646", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6646/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6646/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6646/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6646", "id": 198836447, "node_id": "MDU6SXNzdWUxOTg4MzY0NDc=", "number": 6646, "title": "A suggested improvement for tf.nn.embedding_lookup_sparse() (with code)", "user": {"login": "chentingpc", "id": 1329361, "node_id": "MDQ6VXNlcjEzMjkzNjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1329361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chentingpc", "html_url": "https://github.com/chentingpc", "followers_url": "https://api.github.com/users/chentingpc/followers", "following_url": "https://api.github.com/users/chentingpc/following{/other_user}", "gists_url": "https://api.github.com/users/chentingpc/gists{/gist_id}", "starred_url": "https://api.github.com/users/chentingpc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chentingpc/subscriptions", "organizations_url": "https://api.github.com/users/chentingpc/orgs", "repos_url": "https://api.github.com/users/chentingpc/repos", "events_url": "https://api.github.com/users/chentingpc/events{/privacy}", "received_events_url": "https://api.github.com/users/chentingpc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-01-04T23:19:38Z", "updated_at": "2017-12-21T18:07:30Z", "closed_at": "2017-12-21T18:07:30Z", "author_association": "NONE", "body_html": "<p>The sp_ids and sp_weights parameters in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L203\">embedding_lookup_sparse()</a> can be merged into a single parameter sp_mat which has clearer semantics. In sp_ids and sp_weights, cols are not utilized. However, in sp_mat, we will use row as instance, col as embedding ids, and its entry/value as weight. This makes better sense and reduce parameters for the function. I have written up the function attached below (slightly modified from original code), and also provided some test cases. Hope you may consider it.</p>\n<pre><code>def embedding_lookup_sparse(params, sp_mat,\n                            partition_strategy=\"mod\",\n                            name=None,\n                            combiner=None,\n                            max_norm=None):\n  \"\"\"Computes embeddings for the given ids and weights.\n\n  This op assumes that there is at least one id for each row in the dense tensor\n  represented by sp_mat (i.e. there are no rows with empty features, if so, \n  put 0.0 in sp_mat entry), and that all the indices of sp_mat are in\n  canonical row-major order.\n\n  It also assumes that all id values lie in the range [0, p0), where p0\n  is the sum of the size of params along dimension 0.\n\n  Args:\n    params: A single tensor representing the complete embedding tensor,\n      or a list of P tensors all of same shape except for the first dimension,\n      representing sharded embedding tensors.  Alternatively, a\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\n      element must be appropriately sized for the given `partition_strategy`.\n    sp_mat: N x M SparseTensor of zero or non-zero weights, \n      where N is typically batch size and M is the embedding table size.\n    partition_strategy: A string specifying the partitioning strategy, relevant\n      if `len(params) &gt; 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n    name: Optional name for the op.\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\n      and \"sum\" are supported.\n      \"sum\" computes the weighted sum of the embedding results for each row.\n      \"mean\" is the weighted sum divided by the total weight.\n      \"sqrtn\" is the weighted sum divided by the square root of the sum of the\n      squares of the weights.\n    max_norm: If not None, each embedding is normalized to have l2 norm equal\n      to max_norm before combining.\n\n  Returns:\n    A dense tensor representing the combined embeddings for the sparse ids. \n    For each row in the dense tensor represented by sp_mat, the op looks up \n    the embeddings for all (non-zero) ids in that row, multiplies them by the\n    corresponding weight, and combines these embeddings as specified.\n\n    In other words, if\n\n      shape(combined params) = [p0, p1, ..., pm]\n\n    and\n\n      shape(sp_mat) = [d0, d1, ..., dn]\n\n    then\n\n      shape(output) = [d0, d1, ..., dn-1, p1, ..., pm].\n\n    For instance, if params is a 10x20 matrix, and sp_mat is\n\n      [0, 0]: 1.0\n      [0, 1]: 3.0\n      [1, 0]: 0.0\n      [2, 3]: 1.0\n\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\n\n      output[0, :] = (params[0, :] * 1.0 + params[1, :] * 3.0) / (1.0 + 3.0)\n      output[1, :] = params[0, :] * 0.0 / div_protect\n      output[2, :] = params[3, :] * 1.0 / 1.0\n\n  Raises:\n    TypeError: If sp_mat is not a SparseTensor.\n    ValueError: If combiner is not one of {\"mean\", \"sqrtn\", \"sum\"}.\n  \"\"\"\n  if combiner is None:\n    logging.warn(\"The default value of combiner will change from \\\"mean\\\" \"\n                 \"to \\\"sqrtn\\\" after 2016/11/01.\")\n    combiner = \"mean\"\n  if combiner not in (\"mean\", \"sqrtn\", \"sum\"):\n    raise ValueError(\"combiner must be one of 'mean', 'sqrtn' or 'sum'\")\n  if isinstance(params, variables.PartitionedVariable):\n    params = list(params)  # Iterate to get the underlying Variables.\n  if not isinstance(params, list):\n    params = [params]\n  if not isinstance(sp_mat, sparse_tensor.SparseTensor):\n    raise TypeError(\"sp_mat must be SparseTensor\")\n\n  with ops.name_scope(name, \"embedding_lookup_sparse\",\n                      params + [sp_mat]) as name:\n    segment_ids = sp_mat.indices[:, 0]\n    if segment_ids.dtype != dtypes.int32:\n      segment_ids = math_ops.cast(segment_ids, dtypes.int32)\n\n    ids = sp_mat.indices[:, 1]\n\n    embeddings = embedding_lookup(\n        params, ids, partition_strategy=partition_strategy, max_norm=max_norm)\n\n    weights = sp_mat.values\n    if weights.dtype != embeddings.dtype:\n      weights = math_ops.cast(weights, embeddings.dtype)\n\n    # Reshape weights to allow broadcast\n    ones = array_ops.fill(\n        array_ops.expand_dims(array_ops.rank(embeddings) - 1, 0), 1)\n    bcast_weights_shape = array_ops.concat_v2(\n        [array_ops.shape(weights), ones], 0)\n\n    orig_weights_shape = weights.get_shape()\n    weights = array_ops.reshape(weights, bcast_weights_shape)\n\n    # Set the weight shape, since after reshaping to bcast_weights_shape,\n    # the shape becomes None.\n    if embeddings.get_shape().ndims is not None:\n      weights.set_shape(orig_weights_shape.concatenate(\n          [1 for _ in range(embeddings.get_shape().ndims - 1)]))\n\n    embeddings *= weights\n\n    div_protect = 1e-32  # would not work for float16 or float8\n    if combiner == \"sum\":\n      embeddings = math_ops.segment_sum(embeddings, segment_ids, name=name)\n    elif combiner == \"mean\":\n      embeddings = math_ops.segment_sum(embeddings, segment_ids)\n      weight_sum = math_ops.segment_sum(weights, segment_ids)\n      embeddings = math_ops.div(embeddings, weight_sum + div_protect, name=name)\n    elif combiner == \"sqrtn\":\n      embeddings = math_ops.segment_sum(embeddings, segment_ids)\n      weights_squared = math_ops.pow(weights, 2)\n      weight_sum = math_ops.segment_sum(weights_squared, segment_ids)\n      weight_sum_sqrt = math_ops.sqrt(weight_sum)\n      embeddings = math_ops.div(embeddings, weight_sum_sqrt + div_protect, name=name)\n    else:\n      assert False, \"Unrecognized combiner\"\n\n    return embeddings\n</code></pre>\n<p>Test cases</p>\n<pre><code>sp_mat = [np.array([(0, 0), (0,1), (1, 0), (2, 1), (3, 0), (3, 1)]), np.array((0.5, 0.5, 1, 1, 0, 0)), (-1, -1)]\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        idx = tf.sparse_placeholder(dtype=tf.float32)\n        emb = tf.Variable(initial_value=np.random.random((100, 2)).astype('float32'))\n        y = embedding_lookup_sparse(emb, idx, combiner='mean')\n        sess.run(tf.global_variables_initializer())\n        result = sess.run([y], feed_dict={idx: sp_mat})[0]\n\nassert (result[0] == (result[1] + result[2]) / 2).all()\nassert (result[3] == np.array([0, 0])).all()\n</code></pre>", "body_text": "The sp_ids and sp_weights parameters in embedding_lookup_sparse() can be merged into a single parameter sp_mat which has clearer semantics. In sp_ids and sp_weights, cols are not utilized. However, in sp_mat, we will use row as instance, col as embedding ids, and its entry/value as weight. This makes better sense and reduce parameters for the function. I have written up the function attached below (slightly modified from original code), and also provided some test cases. Hope you may consider it.\ndef embedding_lookup_sparse(params, sp_mat,\n                            partition_strategy=\"mod\",\n                            name=None,\n                            combiner=None,\n                            max_norm=None):\n  \"\"\"Computes embeddings for the given ids and weights.\n\n  This op assumes that there is at least one id for each row in the dense tensor\n  represented by sp_mat (i.e. there are no rows with empty features, if so, \n  put 0.0 in sp_mat entry), and that all the indices of sp_mat are in\n  canonical row-major order.\n\n  It also assumes that all id values lie in the range [0, p0), where p0\n  is the sum of the size of params along dimension 0.\n\n  Args:\n    params: A single tensor representing the complete embedding tensor,\n      or a list of P tensors all of same shape except for the first dimension,\n      representing sharded embedding tensors.  Alternatively, a\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\n      element must be appropriately sized for the given `partition_strategy`.\n    sp_mat: N x M SparseTensor of zero or non-zero weights, \n      where N is typically batch size and M is the embedding table size.\n    partition_strategy: A string specifying the partitioning strategy, relevant\n      if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n    name: Optional name for the op.\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\n      and \"sum\" are supported.\n      \"sum\" computes the weighted sum of the embedding results for each row.\n      \"mean\" is the weighted sum divided by the total weight.\n      \"sqrtn\" is the weighted sum divided by the square root of the sum of the\n      squares of the weights.\n    max_norm: If not None, each embedding is normalized to have l2 norm equal\n      to max_norm before combining.\n\n  Returns:\n    A dense tensor representing the combined embeddings for the sparse ids. \n    For each row in the dense tensor represented by sp_mat, the op looks up \n    the embeddings for all (non-zero) ids in that row, multiplies them by the\n    corresponding weight, and combines these embeddings as specified.\n\n    In other words, if\n\n      shape(combined params) = [p0, p1, ..., pm]\n\n    and\n\n      shape(sp_mat) = [d0, d1, ..., dn]\n\n    then\n\n      shape(output) = [d0, d1, ..., dn-1, p1, ..., pm].\n\n    For instance, if params is a 10x20 matrix, and sp_mat is\n\n      [0, 0]: 1.0\n      [0, 1]: 3.0\n      [1, 0]: 0.0\n      [2, 3]: 1.0\n\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\n\n      output[0, :] = (params[0, :] * 1.0 + params[1, :] * 3.0) / (1.0 + 3.0)\n      output[1, :] = params[0, :] * 0.0 / div_protect\n      output[2, :] = params[3, :] * 1.0 / 1.0\n\n  Raises:\n    TypeError: If sp_mat is not a SparseTensor.\n    ValueError: If combiner is not one of {\"mean\", \"sqrtn\", \"sum\"}.\n  \"\"\"\n  if combiner is None:\n    logging.warn(\"The default value of combiner will change from \\\"mean\\\" \"\n                 \"to \\\"sqrtn\\\" after 2016/11/01.\")\n    combiner = \"mean\"\n  if combiner not in (\"mean\", \"sqrtn\", \"sum\"):\n    raise ValueError(\"combiner must be one of 'mean', 'sqrtn' or 'sum'\")\n  if isinstance(params, variables.PartitionedVariable):\n    params = list(params)  # Iterate to get the underlying Variables.\n  if not isinstance(params, list):\n    params = [params]\n  if not isinstance(sp_mat, sparse_tensor.SparseTensor):\n    raise TypeError(\"sp_mat must be SparseTensor\")\n\n  with ops.name_scope(name, \"embedding_lookup_sparse\",\n                      params + [sp_mat]) as name:\n    segment_ids = sp_mat.indices[:, 0]\n    if segment_ids.dtype != dtypes.int32:\n      segment_ids = math_ops.cast(segment_ids, dtypes.int32)\n\n    ids = sp_mat.indices[:, 1]\n\n    embeddings = embedding_lookup(\n        params, ids, partition_strategy=partition_strategy, max_norm=max_norm)\n\n    weights = sp_mat.values\n    if weights.dtype != embeddings.dtype:\n      weights = math_ops.cast(weights, embeddings.dtype)\n\n    # Reshape weights to allow broadcast\n    ones = array_ops.fill(\n        array_ops.expand_dims(array_ops.rank(embeddings) - 1, 0), 1)\n    bcast_weights_shape = array_ops.concat_v2(\n        [array_ops.shape(weights), ones], 0)\n\n    orig_weights_shape = weights.get_shape()\n    weights = array_ops.reshape(weights, bcast_weights_shape)\n\n    # Set the weight shape, since after reshaping to bcast_weights_shape,\n    # the shape becomes None.\n    if embeddings.get_shape().ndims is not None:\n      weights.set_shape(orig_weights_shape.concatenate(\n          [1 for _ in range(embeddings.get_shape().ndims - 1)]))\n\n    embeddings *= weights\n\n    div_protect = 1e-32  # would not work for float16 or float8\n    if combiner == \"sum\":\n      embeddings = math_ops.segment_sum(embeddings, segment_ids, name=name)\n    elif combiner == \"mean\":\n      embeddings = math_ops.segment_sum(embeddings, segment_ids)\n      weight_sum = math_ops.segment_sum(weights, segment_ids)\n      embeddings = math_ops.div(embeddings, weight_sum + div_protect, name=name)\n    elif combiner == \"sqrtn\":\n      embeddings = math_ops.segment_sum(embeddings, segment_ids)\n      weights_squared = math_ops.pow(weights, 2)\n      weight_sum = math_ops.segment_sum(weights_squared, segment_ids)\n      weight_sum_sqrt = math_ops.sqrt(weight_sum)\n      embeddings = math_ops.div(embeddings, weight_sum_sqrt + div_protect, name=name)\n    else:\n      assert False, \"Unrecognized combiner\"\n\n    return embeddings\n\nTest cases\nsp_mat = [np.array([(0, 0), (0,1), (1, 0), (2, 1), (3, 0), (3, 1)]), np.array((0.5, 0.5, 1, 1, 0, 0)), (-1, -1)]\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        idx = tf.sparse_placeholder(dtype=tf.float32)\n        emb = tf.Variable(initial_value=np.random.random((100, 2)).astype('float32'))\n        y = embedding_lookup_sparse(emb, idx, combiner='mean')\n        sess.run(tf.global_variables_initializer())\n        result = sess.run([y], feed_dict={idx: sp_mat})[0]\n\nassert (result[0] == (result[1] + result[2]) / 2).all()\nassert (result[3] == np.array([0, 0])).all()", "body": "The sp_ids and sp_weights parameters in [embedding_lookup_sparse()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L203) can be merged into a single parameter sp_mat which has clearer semantics. In sp_ids and sp_weights, cols are not utilized. However, in sp_mat, we will use row as instance, col as embedding ids, and its entry/value as weight. This makes better sense and reduce parameters for the function. I have written up the function attached below (slightly modified from original code), and also provided some test cases. Hope you may consider it.\r\n\r\n```\r\ndef embedding_lookup_sparse(params, sp_mat,\r\n                            partition_strategy=\"mod\",\r\n                            name=None,\r\n                            combiner=None,\r\n                            max_norm=None):\r\n  \"\"\"Computes embeddings for the given ids and weights.\r\n\r\n  This op assumes that there is at least one id for each row in the dense tensor\r\n  represented by sp_mat (i.e. there are no rows with empty features, if so, \r\n  put 0.0 in sp_mat entry), and that all the indices of sp_mat are in\r\n  canonical row-major order.\r\n\r\n  It also assumes that all id values lie in the range [0, p0), where p0\r\n  is the sum of the size of params along dimension 0.\r\n\r\n  Args:\r\n    params: A single tensor representing the complete embedding tensor,\r\n      or a list of P tensors all of same shape except for the first dimension,\r\n      representing sharded embedding tensors.  Alternatively, a\r\n      `PartitionedVariable`, created by partitioning along dimension 0. Each\r\n      element must be appropriately sized for the given `partition_strategy`.\r\n    sp_mat: N x M SparseTensor of zero or non-zero weights, \r\n      where N is typically batch size and M is the embedding table size.\r\n    partition_strategy: A string specifying the partitioning strategy, relevant\r\n      if `len(params) > 1`. Currently `\"div\"` and `\"mod\"` are supported. Default\r\n      is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\r\n    name: Optional name for the op.\r\n    combiner: A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\r\n      and \"sum\" are supported.\r\n      \"sum\" computes the weighted sum of the embedding results for each row.\r\n      \"mean\" is the weighted sum divided by the total weight.\r\n      \"sqrtn\" is the weighted sum divided by the square root of the sum of the\r\n      squares of the weights.\r\n    max_norm: If not None, each embedding is normalized to have l2 norm equal\r\n      to max_norm before combining.\r\n\r\n  Returns:\r\n    A dense tensor representing the combined embeddings for the sparse ids. \r\n    For each row in the dense tensor represented by sp_mat, the op looks up \r\n    the embeddings for all (non-zero) ids in that row, multiplies them by the\r\n    corresponding weight, and combines these embeddings as specified.\r\n\r\n    In other words, if\r\n\r\n      shape(combined params) = [p0, p1, ..., pm]\r\n\r\n    and\r\n\r\n      shape(sp_mat) = [d0, d1, ..., dn]\r\n\r\n    then\r\n\r\n      shape(output) = [d0, d1, ..., dn-1, p1, ..., pm].\r\n\r\n    For instance, if params is a 10x20 matrix, and sp_mat is\r\n\r\n      [0, 0]: 1.0\r\n      [0, 1]: 3.0\r\n      [1, 0]: 0.0\r\n      [2, 3]: 1.0\r\n\r\n    with `combiner`=\"mean\", then the output will be a 3x20 matrix where\r\n\r\n      output[0, :] = (params[0, :] * 1.0 + params[1, :] * 3.0) / (1.0 + 3.0)\r\n      output[1, :] = params[0, :] * 0.0 / div_protect\r\n      output[2, :] = params[3, :] * 1.0 / 1.0\r\n\r\n  Raises:\r\n    TypeError: If sp_mat is not a SparseTensor.\r\n    ValueError: If combiner is not one of {\"mean\", \"sqrtn\", \"sum\"}.\r\n  \"\"\"\r\n  if combiner is None:\r\n    logging.warn(\"The default value of combiner will change from \\\"mean\\\" \"\r\n                 \"to \\\"sqrtn\\\" after 2016/11/01.\")\r\n    combiner = \"mean\"\r\n  if combiner not in (\"mean\", \"sqrtn\", \"sum\"):\r\n    raise ValueError(\"combiner must be one of 'mean', 'sqrtn' or 'sum'\")\r\n  if isinstance(params, variables.PartitionedVariable):\r\n    params = list(params)  # Iterate to get the underlying Variables.\r\n  if not isinstance(params, list):\r\n    params = [params]\r\n  if not isinstance(sp_mat, sparse_tensor.SparseTensor):\r\n    raise TypeError(\"sp_mat must be SparseTensor\")\r\n\r\n  with ops.name_scope(name, \"embedding_lookup_sparse\",\r\n                      params + [sp_mat]) as name:\r\n    segment_ids = sp_mat.indices[:, 0]\r\n    if segment_ids.dtype != dtypes.int32:\r\n      segment_ids = math_ops.cast(segment_ids, dtypes.int32)\r\n\r\n    ids = sp_mat.indices[:, 1]\r\n\r\n    embeddings = embedding_lookup(\r\n        params, ids, partition_strategy=partition_strategy, max_norm=max_norm)\r\n\r\n    weights = sp_mat.values\r\n    if weights.dtype != embeddings.dtype:\r\n      weights = math_ops.cast(weights, embeddings.dtype)\r\n\r\n    # Reshape weights to allow broadcast\r\n    ones = array_ops.fill(\r\n        array_ops.expand_dims(array_ops.rank(embeddings) - 1, 0), 1)\r\n    bcast_weights_shape = array_ops.concat_v2(\r\n        [array_ops.shape(weights), ones], 0)\r\n\r\n    orig_weights_shape = weights.get_shape()\r\n    weights = array_ops.reshape(weights, bcast_weights_shape)\r\n\r\n    # Set the weight shape, since after reshaping to bcast_weights_shape,\r\n    # the shape becomes None.\r\n    if embeddings.get_shape().ndims is not None:\r\n      weights.set_shape(orig_weights_shape.concatenate(\r\n          [1 for _ in range(embeddings.get_shape().ndims - 1)]))\r\n\r\n    embeddings *= weights\r\n\r\n    div_protect = 1e-32  # would not work for float16 or float8\r\n    if combiner == \"sum\":\r\n      embeddings = math_ops.segment_sum(embeddings, segment_ids, name=name)\r\n    elif combiner == \"mean\":\r\n      embeddings = math_ops.segment_sum(embeddings, segment_ids)\r\n      weight_sum = math_ops.segment_sum(weights, segment_ids)\r\n      embeddings = math_ops.div(embeddings, weight_sum + div_protect, name=name)\r\n    elif combiner == \"sqrtn\":\r\n      embeddings = math_ops.segment_sum(embeddings, segment_ids)\r\n      weights_squared = math_ops.pow(weights, 2)\r\n      weight_sum = math_ops.segment_sum(weights_squared, segment_ids)\r\n      weight_sum_sqrt = math_ops.sqrt(weight_sum)\r\n      embeddings = math_ops.div(embeddings, weight_sum_sqrt + div_protect, name=name)\r\n    else:\r\n      assert False, \"Unrecognized combiner\"\r\n\r\n    return embeddings\r\n```\r\n\r\nTest cases\r\n\r\n```\r\nsp_mat = [np.array([(0, 0), (0,1), (1, 0), (2, 1), (3, 0), (3, 1)]), np.array((0.5, 0.5, 1, 1, 0, 0)), (-1, -1)]\r\nwith tf.Graph().as_default():\r\n    with tf.Session() as sess:\r\n        idx = tf.sparse_placeholder(dtype=tf.float32)\r\n        emb = tf.Variable(initial_value=np.random.random((100, 2)).astype('float32'))\r\n        y = embedding_lookup_sparse(emb, idx, combiner='mean')\r\n        sess.run(tf.global_variables_initializer())\r\n        result = sess.run([y], feed_dict={idx: sp_mat})[0]\r\n\r\nassert (result[0] == (result[1] + result[2]) / 2).all()\r\nassert (result[3] == np.array([0, 0])).all()\r\n```\r\n"}