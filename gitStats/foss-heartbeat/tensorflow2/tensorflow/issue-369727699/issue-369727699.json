{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22955", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22955/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22955/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22955/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22955", "id": 369727699, "node_id": "MDU6SXNzdWUzNjk3Mjc2OTk=", "number": 22955, "title": "How to convert Tensorrt Optimized graph to Tensorrt engine", "user": {"login": "LinHungShi", "id": 12586217, "node_id": "MDQ6VXNlcjEyNTg2MjE3", "avatar_url": "https://avatars1.githubusercontent.com/u/12586217?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LinHungShi", "html_url": "https://github.com/LinHungShi", "followers_url": "https://api.github.com/users/LinHungShi/followers", "following_url": "https://api.github.com/users/LinHungShi/following{/other_user}", "gists_url": "https://api.github.com/users/LinHungShi/gists{/gist_id}", "starred_url": "https://api.github.com/users/LinHungShi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LinHungShi/subscriptions", "organizations_url": "https://api.github.com/users/LinHungShi/orgs", "repos_url": "https://api.github.com/users/LinHungShi/repos", "events_url": "https://api.github.com/users/LinHungShi/events{/privacy}", "received_events_url": "https://api.github.com/users/LinHungShi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-10-12T22:15:03Z", "updated_at": "2018-10-31T17:25:30Z", "closed_at": "2018-10-31T17:25:30Z", "author_association": "NONE", "body_html": "<p>I need to convert my tensorflow model to tensorrt engine. However, since there are some unsupported layers in the model, I decided to use create_inference_graph to skip those operations. Although it works well with Python, I need to write it to *.engine file so that I can read it in C++. I tried to convert it to uff model using uff_model_from_tensorflow_frozen_model, but got key error. Is there a way I can convert the optimized graph to an engine? Any suggestion would be appreciated.</p>\n<h3>System information</h3>\n<ul>\n<li>**OS Platform and Distribution : Ubuntu 16.04</li>\n<li>**TensorFlow installed from (source or binary) : source</li>\n<li>**TensorFlow version 1.7 ~ 1.11</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 8.0, 9.0; CUDNN 7.0, 7.3</li>\n<li>Have I written custom code : No</li>\n<li>Bazel version : NA</li>\n<li>GPU model and memory : NA</li>\n<li>Mobile device : NA</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>def load_graph(file):\n    with tf.gfile.GFile(file, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def)\n    return graph, graph_def\n\ngraph = load_graph('tensorflow_model.pb')\ntensorrt_graph = trt.create_inference_graph(graph_def, outputs=['output_node'], max_batch_size=1, precision_mode='FP32', max_workspace_size_bytes=1&lt;&lt;33)\n\nwith tf.gfile.GFile('tensorrt_model.pb', 'wb') as f:\n    f.write(tensorrt_graph.SerializeToString())\n\nloaded_tensorrt_graph, loaded_tensorrt_graph_def = load_graph('tensorrt_model.pb')\nuff_model = uff.from_tensorflow_frozen_model(loaded_tensorrt_graph_def)\n</code></pre>\n<p>Traceback (most recent call last):<br>\nFile \"\", line 1, in <br>\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py\", line 149, in from_tensorflow_frozen_model<br>\nreturn from_tensorflow(graphdef, output_nodes, preprocessor, **kwargs)<br>\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py\", line 120, in from_tensorflow<br>\nname=\"main\")<br>\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 76, in convert_tf2uff_graph<br>\nuff_graph, input_replacements)<br>\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 63, in convert_tf2uff_node<br>\nop, name, tf_node, inputs, uff_graph, tf_nodes=tf_nodes)<br>\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 38, in convert_layer<br>\nfields = cls.parse_tf_attrs(tf_node.attr)<br>\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in parse_tf_attrs<br>\nfor key, val in attrs.items()}<br>\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in <br>\nfor key, val in attrs.items()}<br>\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 204, in parse_tf_attr_value<br>\nreturn cls.convert_tf2uff_field(code, val)<br>\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 189, in convert_tf2uff_field<br>\n'type': 'dtype', 'list': 'list'}<br>\nKeyError: 'shape'</p>", "body_text": "I need to convert my tensorflow model to tensorrt engine. However, since there are some unsupported layers in the model, I decided to use create_inference_graph to skip those operations. Although it works well with Python, I need to write it to *.engine file so that I can read it in C++. I tried to convert it to uff model using uff_model_from_tensorflow_frozen_model, but got key error. Is there a way I can convert the optimized graph to an engine? Any suggestion would be appreciated.\nSystem information\n\n**OS Platform and Distribution : Ubuntu 16.04\n**TensorFlow installed from (source or binary) : source\n**TensorFlow version 1.7 ~ 1.11\nPython version: 3.5\nCUDA/cuDNN version: CUDA 8.0, 9.0; CUDNN 7.0, 7.3\nHave I written custom code : No\nBazel version : NA\nGPU model and memory : NA\nMobile device : NA\nExact command to reproduce:\n\ndef load_graph(file):\n    with tf.gfile.GFile(file, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def)\n    return graph, graph_def\n\ngraph = load_graph('tensorflow_model.pb')\ntensorrt_graph = trt.create_inference_graph(graph_def, outputs=['output_node'], max_batch_size=1, precision_mode='FP32', max_workspace_size_bytes=1<<33)\n\nwith tf.gfile.GFile('tensorrt_model.pb', 'wb') as f:\n    f.write(tensorrt_graph.SerializeToString())\n\nloaded_tensorrt_graph, loaded_tensorrt_graph_def = load_graph('tensorrt_model.pb')\nuff_model = uff.from_tensorflow_frozen_model(loaded_tensorrt_graph_def)\n\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py\", line 149, in from_tensorflow_frozen_model\nreturn from_tensorflow(graphdef, output_nodes, preprocessor, **kwargs)\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py\", line 120, in from_tensorflow\nname=\"main\")\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 76, in convert_tf2uff_graph\nuff_graph, input_replacements)\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 63, in convert_tf2uff_node\nop, name, tf_node, inputs, uff_graph, tf_nodes=tf_nodes)\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 38, in convert_layer\nfields = cls.parse_tf_attrs(tf_node.attr)\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in parse_tf_attrs\nfor key, val in attrs.items()}\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in \nfor key, val in attrs.items()}\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 204, in parse_tf_attr_value\nreturn cls.convert_tf2uff_field(code, val)\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 189, in convert_tf2uff_field\n'type': 'dtype', 'list': 'list'}\nKeyError: 'shape'", "body": "I need to convert my tensorflow model to tensorrt engine. However, since there are some unsupported layers in the model, I decided to use create_inference_graph to skip those operations. Although it works well with Python, I need to write it to *.engine file so that I can read it in C++. I tried to convert it to uff model using uff_model_from_tensorflow_frozen_model, but got key error. Is there a way I can convert the optimized graph to an engine? Any suggestion would be appreciated.\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution : Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary) : source\r\n- **TensorFlow version 1.7 ~ 1.11\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: CUDA 8.0, 9.0; CUDNN 7.0, 7.3\r\n- Have I written custom code : No\r\n- Bazel version : NA\r\n- GPU model and memory : NA\r\n- Mobile device : NA\r\n- **Exact command to reproduce**:\r\n```\r\ndef load_graph(file):\r\n    with tf.gfile.GFile(file, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graph_def)\r\n    return graph, graph_def\r\n\r\ngraph = load_graph('tensorflow_model.pb')\r\ntensorrt_graph = trt.create_inference_graph(graph_def, outputs=['output_node'], max_batch_size=1, precision_mode='FP32', max_workspace_size_bytes=1<<33)\r\n\r\nwith tf.gfile.GFile('tensorrt_model.pb', 'wb') as f:\r\n    f.write(tensorrt_graph.SerializeToString())\r\n\r\nloaded_tensorrt_graph, loaded_tensorrt_graph_def = load_graph('tensorrt_model.pb')\r\nuff_model = uff.from_tensorflow_frozen_model(loaded_tensorrt_graph_def)\r\n```\r\n\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py\", line 149, in from_tensorflow_frozen_model\r\nreturn from_tensorflow(graphdef, output_nodes, preprocessor, **kwargs)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py\", line 120, in from_tensorflow\r\nname=\"main\")\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 76, in convert_tf2uff_graph\r\nuff_graph, input_replacements)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 63, in convert_tf2uff_node\r\nop, name, tf_node, inputs, uff_graph, tf_nodes=tf_nodes)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 38, in convert_layer\r\nfields = cls.parse_tf_attrs(tf_node.attr)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in parse_tf_attrs\r\nfor key, val in attrs.items()}\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 209, in <dictcomp>\r\nfor key, val in attrs.items()}\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 204, in parse_tf_attr_value\r\nreturn cls.convert_tf2uff_field(code, val)\r\nFile \"/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py\", line 189, in convert_tf2uff_field\r\n'type': 'dtype', 'list': 'list'}\r\nKeyError: 'shape'\r\n"}