{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/165336127", "html_url": "https://github.com/tensorflow/tensorflow/issues/464#issuecomment-165336127", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/464", "id": 165336127, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NTMzNjEyNw==", "user": {"login": "fabiencro", "id": 6006273, "node_id": "MDQ6VXNlcjYwMDYyNzM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6006273?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fabiencro", "html_url": "https://github.com/fabiencro", "followers_url": "https://api.github.com/users/fabiencro/followers", "following_url": "https://api.github.com/users/fabiencro/following{/other_user}", "gists_url": "https://api.github.com/users/fabiencro/gists{/gist_id}", "starred_url": "https://api.github.com/users/fabiencro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fabiencro/subscriptions", "organizations_url": "https://api.github.com/users/fabiencro/orgs", "repos_url": "https://api.github.com/users/fabiencro/repos", "events_url": "https://api.github.com/users/fabiencro/events{/privacy}", "received_events_url": "https://api.github.com/users/fabiencro/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-17T04:50:33Z", "updated_at": "2015-12-17T04:50:33Z", "author_association": "NONE", "body_html": "<p>Yes, for Adagrad it is enough to not upgrade the squared gradient sum (and keep track of the global total number of updates).</p>\n<p>Would you consider implementing this? I could probably do it myself at some point when I have time by looking at the existing code for RMSProp and Adam. But with lack of time and the annoyance of getting a Corporate Contributor Agreement, I would probably not contribute that anytime soon...</p>\n<p>In any case, if nothing is changed, I think the docs should mention that ADAM is slower than Adagrad on sparse updates; and that RMSProp is not compatible with those.</p>", "body_text": "Yes, for Adagrad it is enough to not upgrade the squared gradient sum (and keep track of the global total number of updates).\nWould you consider implementing this? I could probably do it myself at some point when I have time by looking at the existing code for RMSProp and Adam. But with lack of time and the annoyance of getting a Corporate Contributor Agreement, I would probably not contribute that anytime soon...\nIn any case, if nothing is changed, I think the docs should mention that ADAM is slower than Adagrad on sparse updates; and that RMSProp is not compatible with those.", "body": "Yes, for Adagrad it is enough to not upgrade the squared gradient sum (and keep track of the global total number of updates).\n\nWould you consider implementing this? I could probably do it myself at some point when I have time by looking at the existing code for RMSProp and Adam. But with lack of time and the annoyance of getting a Corporate Contributor Agreement, I would probably not contribute that anytime soon...\n\nIn any case, if nothing is changed, I think the docs should mention that ADAM is slower than Adagrad on sparse updates; and that RMSProp is not compatible with those.\n"}