{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/165171770", "html_url": "https://github.com/tensorflow/tensorflow/issues/464#issuecomment-165171770", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/464", "id": 165171770, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NTE3MTc3MA==", "user": {"login": "rafaljozefowicz", "id": 2983769, "node_id": "MDQ6VXNlcjI5ODM3Njk=", "avatar_url": "https://avatars1.githubusercontent.com/u/2983769?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rafaljozefowicz", "html_url": "https://github.com/rafaljozefowicz", "followers_url": "https://api.github.com/users/rafaljozefowicz/followers", "following_url": "https://api.github.com/users/rafaljozefowicz/following{/other_user}", "gists_url": "https://api.github.com/users/rafaljozefowicz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rafaljozefowicz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rafaljozefowicz/subscriptions", "organizations_url": "https://api.github.com/users/rafaljozefowicz/orgs", "repos_url": "https://api.github.com/users/rafaljozefowicz/repos", "events_url": "https://api.github.com/users/rafaljozefowicz/events{/privacy}", "received_events_url": "https://api.github.com/users/rafaljozefowicz/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-16T16:48:18Z", "updated_at": "2015-12-16T16:48:18Z", "author_association": "NONE", "body_html": "<p>What kind of behavior would you expect from sparse tensors in RMSProp?<br>\nThere are two options I think:</p>\n<ul>\n<li>Ignore the momentum terms for the embedding rows that are not present in the current batch</li>\n<li>Apply momentum terms to the whole embedding (AFAIR, that's what we do for Adam)</li>\n</ul>\n<p>The first might not be correct (though, not sure), the second is very slow. What I typically do now is to split my variables into two groups, and train the dense part with any optimizer I want and the rest with GradientDescent or AdaGrad.</p>", "body_text": "What kind of behavior would you expect from sparse tensors in RMSProp?\nThere are two options I think:\n\nIgnore the momentum terms for the embedding rows that are not present in the current batch\nApply momentum terms to the whole embedding (AFAIR, that's what we do for Adam)\n\nThe first might not be correct (though, not sure), the second is very slow. What I typically do now is to split my variables into two groups, and train the dense part with any optimizer I want and the rest with GradientDescent or AdaGrad.", "body": "What kind of behavior would you expect from sparse tensors in RMSProp?\nThere are two options I think:\n- Ignore the momentum terms for the embedding rows that are not present in the current batch\n- Apply momentum terms to the whole embedding (AFAIR, that's what we do for Adam)\n\nThe first might not be correct (though, not sure), the second is very slow. What I typically do now is to split my variables into two groups, and train the dense part with any optimizer I want and the rest with GradientDescent or AdaGrad.\n"}