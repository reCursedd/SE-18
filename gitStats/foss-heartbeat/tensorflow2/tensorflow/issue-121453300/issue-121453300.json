{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/464", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/464/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/464/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/464/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/464", "id": 121453300, "node_id": "MDU6SXNzdWUxMjE0NTMzMDA=", "number": 464, "title": "RMSProp optimization support for sparse tensors", "user": {"login": "fabiencro", "id": 6006273, "node_id": "MDQ6VXNlcjYwMDYyNzM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6006273?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fabiencro", "html_url": "https://github.com/fabiencro", "followers_url": "https://api.github.com/users/fabiencro/followers", "following_url": "https://api.github.com/users/fabiencro/following{/other_user}", "gists_url": "https://api.github.com/users/fabiencro/gists{/gist_id}", "starred_url": "https://api.github.com/users/fabiencro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fabiencro/subscriptions", "organizations_url": "https://api.github.com/users/fabiencro/orgs", "repos_url": "https://api.github.com/users/fabiencro/repos", "events_url": "https://api.github.com/users/fabiencro/events{/privacy}", "received_events_url": "https://api.github.com/users/fabiencro/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 19, "created_at": "2015-12-10T10:33:18Z", "updated_at": "2018-04-03T06:26:37Z", "closed_at": "2018-04-03T06:26:37Z", "author_association": "NONE", "body_html": "<p>It seems that tf.nce_loss is not compatible with the optimizers RMSProp, ADAGRAD and Momentum. (while SGD, ADAM and FTRL works fine).</p>\n<p>When using rmsprop, I get this error:</p>\n<pre><code>    optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate, decay = rms_prop_decay).minimize(nce_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 167, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 256, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py\", line 81, in _apply_sparse\n    raise NotImplementedError()\nNotImplementedError\n</code></pre>\n<p>When using adagrad or momentum, I get this error:</p>\n<pre><code>    optimizer = tf.train.MomentumOptimizer(learning_rate, learning_momentum).minimize(nce_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 167, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 256, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/momentum.py\", line 51, in _apply_sparse\n    self._momentum_tensor, use_locking=self._use_locking).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 237, in sparse_apply_momentum\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 633, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1712, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1417, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_ops.py\", line 111, in _SparseApplyMomentumShape\n    tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 481, in merge_with\n    self.assert_same_rank(other)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 524, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes TensorShape([Dimension(128), Dimension(11), Dimension(192)]) and TensorShape([Dimension(None), Dimension(192)]) must have the same rank\n</code></pre>\n<p>Is that expected?<br>\nThe exact same code works perfectly fine with adam or sgd optimizers, so I do not think I made a mistake when constructing the graph.</p>", "body_text": "It seems that tf.nce_loss is not compatible with the optimizers RMSProp, ADAGRAD and Momentum. (while SGD, ADAM and FTRL works fine).\nWhen using rmsprop, I get this error:\n    optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate, decay = rms_prop_decay).minimize(nce_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 167, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 256, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py\", line 81, in _apply_sparse\n    raise NotImplementedError()\nNotImplementedError\n\nWhen using adagrad or momentum, I get this error:\n    optimizer = tf.train.MomentumOptimizer(learning_rate, learning_momentum).minimize(nce_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 167, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 256, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/momentum.py\", line 51, in _apply_sparse\n    self._momentum_tensor, use_locking=self._use_locking).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 237, in sparse_apply_momentum\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 633, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1712, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1417, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_ops.py\", line 111, in _SparseApplyMomentumShape\n    tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 481, in merge_with\n    self.assert_same_rank(other)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 524, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes TensorShape([Dimension(128), Dimension(11), Dimension(192)]) and TensorShape([Dimension(None), Dimension(192)]) must have the same rank\n\nIs that expected?\nThe exact same code works perfectly fine with adam or sgd optimizers, so I do not think I made a mistake when constructing the graph.", "body": "It seems that tf.nce_loss is not compatible with the optimizers RMSProp, ADAGRAD and Momentum. (while SGD, ADAM and FTRL works fine).\n\nWhen using rmsprop, I get this error:\n\n```\n    optimizer = tf.train.RMSPropOptimizer(learning_rate = learning_rate, decay = rms_prop_decay).minimize(nce_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 167, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 256, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py\", line 81, in _apply_sparse\n    raise NotImplementedError()\nNotImplementedError\n```\n\nWhen using adagrad or momentum, I get this error:\n\n```\n    optimizer = tf.train.MomentumOptimizer(learning_rate, learning_momentum).minimize(nce_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 167, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 256, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/momentum.py\", line 51, in _apply_sparse\n    self._momentum_tensor, use_locking=self._use_locking).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 237, in sparse_apply_momentum\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 633, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1712, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1417, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_ops.py\", line 111, in _SparseApplyMomentumShape\n    tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 481, in merge_with\n    self.assert_same_rank(other)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 524, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes TensorShape([Dimension(128), Dimension(11), Dimension(192)]) and TensorShape([Dimension(None), Dimension(192)]) must have the same rank\n```\n\nIs that expected?\nThe exact same code works perfectly fine with adam or sgd optimizers, so I do not think I made a mistake when constructing the graph.\n"}