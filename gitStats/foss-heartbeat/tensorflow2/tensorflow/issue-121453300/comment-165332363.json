{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/165332363", "html_url": "https://github.com/tensorflow/tensorflow/issues/464#issuecomment-165332363", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/464", "id": 165332363, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NTMzMjM2Mw==", "user": {"login": "fabiencro", "id": 6006273, "node_id": "MDQ6VXNlcjYwMDYyNzM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6006273?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fabiencro", "html_url": "https://github.com/fabiencro", "followers_url": "https://api.github.com/users/fabiencro/followers", "following_url": "https://api.github.com/users/fabiencro/following{/other_user}", "gists_url": "https://api.github.com/users/fabiencro/gists{/gist_id}", "starred_url": "https://api.github.com/users/fabiencro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fabiencro/subscriptions", "organizations_url": "https://api.github.com/users/fabiencro/orgs", "repos_url": "https://api.github.com/users/fabiencro/repos", "events_url": "https://api.github.com/users/fabiencro/events{/privacy}", "received_events_url": "https://api.github.com/users/fabiencro/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-17T04:11:00Z", "updated_at": "2015-12-17T04:11:00Z", "author_association": "NONE", "body_html": "<p>It is quite possible that I do not fully understand the issue, but I was not expecting RMSProp to be much more difficult to use than Adagrad. I will try to briefly state how I see things (and please excuse me if I write something obviously stupid).</p>\n<p>As for the two options you gave, I take it that you are discussing the update of the running average of the squared gradient, right? \"ignore the momentum\" would consist in not updating the running average for dimensions for which the gradient is zero due to sparsity. And \"apply momentum for the whole embedding\" would be to actually update the running average of all dimensions at each iteration.</p>\n<p>Then I think there is a third option. You could keep track of the last iteration in which the running average was updated for each dimension.</p>\n<p>Let us call last(d) the iteration step at which the running average of dimension d was last updated. Then, when you want to update dimension d at iteration step i, you can update the running average of the square gradient of d by rms(d) = rms(d) * 0.9 ^ ( i - last(d)) * 0.9 + 0.1 * grad_i(d)^2. Then set last(d) = i.</p>\n<p>The 0.9 ^ ( i - last(d)) part account for the modification in the running gradient since the last time we saw a non-zero gradient for dimension d (null gradient (i - last(d) times). The values of last(d) and rms(d) only need to be updated when there is a non-null gradient for dimension d. Therefore the updates should be efficient in a sparse context.</p>", "body_text": "It is quite possible that I do not fully understand the issue, but I was not expecting RMSProp to be much more difficult to use than Adagrad. I will try to briefly state how I see things (and please excuse me if I write something obviously stupid).\nAs for the two options you gave, I take it that you are discussing the update of the running average of the squared gradient, right? \"ignore the momentum\" would consist in not updating the running average for dimensions for which the gradient is zero due to sparsity. And \"apply momentum for the whole embedding\" would be to actually update the running average of all dimensions at each iteration.\nThen I think there is a third option. You could keep track of the last iteration in which the running average was updated for each dimension.\nLet us call last(d) the iteration step at which the running average of dimension d was last updated. Then, when you want to update dimension d at iteration step i, you can update the running average of the square gradient of d by rms(d) = rms(d) * 0.9 ^ ( i - last(d)) * 0.9 + 0.1 * grad_i(d)^2. Then set last(d) = i.\nThe 0.9 ^ ( i - last(d)) part account for the modification in the running gradient since the last time we saw a non-zero gradient for dimension d (null gradient (i - last(d) times). The values of last(d) and rms(d) only need to be updated when there is a non-null gradient for dimension d. Therefore the updates should be efficient in a sparse context.", "body": "It is quite possible that I do not fully understand the issue, but I was not expecting RMSProp to be much more difficult to use than Adagrad. I will try to briefly state how I see things (and please excuse me if I write something obviously stupid).\n\nAs for the two options you gave, I take it that you are discussing the update of the running average of the squared gradient, right? \"ignore the momentum\" would consist in not updating the running average for dimensions for which the gradient is zero due to sparsity. And \"apply momentum for the whole embedding\" would be to actually update the running average of all dimensions at each iteration. \n\nThen I think there is a third option. You could keep track of the last iteration in which the running average was updated for each dimension.\n\nLet us call last(d) the iteration step at which the running average of dimension d was last updated. Then, when you want to update dimension d at iteration step i, you can update the running average of the square gradient of d by rms(d) = rms(d) \\* 0.9 ^ ( i - last(d)) \\* 0.9 + 0.1 \\* grad_i(d)^2. Then set last(d) = i.\n\nThe 0.9 ^ ( i - last(d)) part account for the modification in the running gradient since the last time we saw a non-zero gradient for dimension d (null gradient (i - last(d) times). The values of last(d) and rms(d) only need to be updated when there is a non-null gradient for dimension d. Therefore the updates should be efficient in a sparse context.\n"}