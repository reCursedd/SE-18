{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23800", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23800/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23800/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23800/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23800", "id": 381563141, "node_id": "MDU6SXNzdWUzODE1NjMxNDE=", "number": 23800, "title": "TFLite: bug result in conv int8 reference op implementation", "user": {"login": "jackwish", "id": 4936589, "node_id": "MDQ6VXNlcjQ5MzY1ODk=", "avatar_url": "https://avatars2.githubusercontent.com/u/4936589?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jackwish", "html_url": "https://github.com/jackwish", "followers_url": "https://api.github.com/users/jackwish/followers", "following_url": "https://api.github.com/users/jackwish/following{/other_user}", "gists_url": "https://api.github.com/users/jackwish/gists{/gist_id}", "starred_url": "https://api.github.com/users/jackwish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jackwish/subscriptions", "organizations_url": "https://api.github.com/users/jackwish/orgs", "repos_url": "https://api.github.com/users/jackwish/repos", "events_url": "https://api.github.com/users/jackwish/events{/privacy}", "received_events_url": "https://api.github.com/users/jackwish/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-11-16T11:36:32Z", "updated_at": "2018-11-21T09:59:44Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, but dump the runtime data</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Null</li>\n<li>TensorFlow installed from (source or binary): source</li>\n<li>TensorFlow version (use command below): 1.11.0 and 1.12.0</li>\n<li>Python version: 3.6</li>\n<li>Bazel version (if compiling from source): 0.16.1</li>\n<li>GCC/Compiler version (if compiling from source): 5.5.0</li>\n<li>CUDA/cuDNN version: No</li>\n<li>GPU model and memory: No</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nWhen running mobiletnet int8 model, starting from the third Conv(<code>MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6</code>), TFLite's output mismatches with another framework.</p>\n<p>I looked into it and found that <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/lite/kernels/kernel_util.cc#L25\">GetQuantizedConvolutionMultipler()</a> generates unexpected results with the calculation <code>multiplier = input_scale * weight_scale / output_scale</code>. By <em>unexpected</em> I mean the math result is wrong, on my machine at least.</p>\n<p>Log example of parameter of <code>MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6</code> is as below. Since <code>input_scale</code> and <code>output_scale</code> are the same, the <code>real_mulpliter</code> shall be same as <code>filter_scale</code> - more even, these parameters shall be the same for most Conv ops in quantized MobileNetV1.</p>\n<pre lang=\"conv\" data-meta=\"pre computed args --------------------------\"><code>            input scale : 0.023528477177023888\n            filter scale : 0.015148180536925793\n            output scale : 0.023528477177023888\n            real_multiplier: 0.015148180864416702\n            output_multiplier: 2081950125\n</code></pre>\n<p><strong>Describe the expected behavior</strong><br>\nWell, the expected behavior is that generates correct result.</p>\n<p><strong>Code to reproduce the issue</strong><br>\nTo generates the log in r1.11, try <a href=\"https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.11\">https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.11</a> . Note that, I have already used <strong><a href=\"https://github.com/jackwish/tensorflow/commit/f3677c512ebe06a2dd0dda129ba9ef71de639d87#diff-ca0f46c80fd3cf1f2040be6147a2d8cfR47\">another code style</a> such that the output multiplier is correct</strong>.<br>\nYou can also try <a href=\"https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.12\">r1.12</a> (there is bug in dumping conv outputs here, but the <code>Prepare()</code> still has multiplier log).</p>\n<p><strong>Other info / logs</strong><br>\nI was running quantized MobileNetV1  with TFLite on a x86 server. By chance, i noticed that some output of Conv int8 is unexpected. TFLite was invoked through python interfaces, the model comes from <a href=\"https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\">TF model zoo</a> (the TFLite model inside <a href=\"http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz\" rel=\"nofollow\">MobileNet_v1_1.0_224_quant</a>). Nothing else special.</p>\n<p>Note sure if there is similar behavior on other machine/devices.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No, but dump the runtime data\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Null\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.11.0 and 1.12.0\nPython version: 3.6\nBazel version (if compiling from source): 0.16.1\nGCC/Compiler version (if compiling from source): 5.5.0\nCUDA/cuDNN version: No\nGPU model and memory: No\n\nDescribe the current behavior\nWhen running mobiletnet int8 model, starting from the third Conv(MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6), TFLite's output mismatches with another framework.\nI looked into it and found that GetQuantizedConvolutionMultipler() generates unexpected results with the calculation multiplier = input_scale * weight_scale / output_scale. By unexpected I mean the math result is wrong, on my machine at least.\nLog example of parameter of MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6 is as below. Since input_scale and output_scale are the same, the real_mulpliter shall be same as filter_scale - more even, these parameters shall be the same for most Conv ops in quantized MobileNetV1.\n            input scale : 0.023528477177023888\n            filter scale : 0.015148180536925793\n            output scale : 0.023528477177023888\n            real_multiplier: 0.015148180864416702\n            output_multiplier: 2081950125\n\nDescribe the expected behavior\nWell, the expected behavior is that generates correct result.\nCode to reproduce the issue\nTo generates the log in r1.11, try https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.11 . Note that, I have already used another code style such that the output multiplier is correct.\nYou can also try r1.12 (there is bug in dumping conv outputs here, but the Prepare() still has multiplier log).\nOther info / logs\nI was running quantized MobileNetV1  with TFLite on a x86 server. By chance, i noticed that some output of Conv int8 is unexpected. TFLite was invoked through python interfaces, the model comes from TF model zoo (the TFLite model inside MobileNet_v1_1.0_224_quant). Nothing else special.\nNote sure if there is similar behavior on other machine/devices.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, but dump the runtime data\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Null\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.11.0 and 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): 5.5.0\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nWhen running mobiletnet int8 model, starting from the third Conv(`MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6`), TFLite's output mismatches with another framework.\r\n\r\nI looked into it and found that [GetQuantizedConvolutionMultipler()](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/lite/kernels/kernel_util.cc#L25) generates unexpected results with the calculation `multiplier = input_scale * weight_scale / output_scale`. By *unexpected* I mean the math result is wrong, on my machine at least.\r\n\r\nLog example of parameter of `MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6` is as below. Since `input_scale` and `output_scale` are the same, the `real_mulpliter` shall be same as `filter_scale` - more even, these parameters shall be the same for most Conv ops in quantized MobileNetV1.\r\n```conv pre computed args --------------------------\r\n            input scale : 0.023528477177023888\r\n            filter scale : 0.015148180536925793\r\n            output scale : 0.023528477177023888\r\n            real_multiplier: 0.015148180864416702\r\n            output_multiplier: 2081950125\r\n```\r\n\r\n**Describe the expected behavior**\r\nWell, the expected behavior is that generates correct result.\r\n\r\n**Code to reproduce the issue**\r\nTo generates the log in r1.11, try https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.11 . Note that, I have already used **[another code style](https://github.com/jackwish/tensorflow/commit/f3677c512ebe06a2dd0dda129ba9ef71de639d87#diff-ca0f46c80fd3cf1f2040be6147a2d8cfR47) such that the output multiplier is correct**.\r\nYou can also try [r1.12](https://github.com/jackwish/tensorflow/tree/d/lite/qconv-r1.12) (there is bug in dumping conv outputs here, but the `Prepare()` still has multiplier log).\r\n\r\n**Other info / logs**\r\nI was running quantized MobileNetV1  with TFLite on a x86 server. By chance, i noticed that some output of Conv int8 is unexpected. TFLite was invoked through python interfaces, the model comes from [TF model zoo](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md) (the TFLite model inside [MobileNet_v1_1.0_224_quant](http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz)). Nothing else special.\r\n\r\nNote sure if there is similar behavior on other machine/devices."}