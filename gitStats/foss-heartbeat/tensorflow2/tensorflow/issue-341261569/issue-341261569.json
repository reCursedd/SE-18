{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20805", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20805/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20805/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20805/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20805", "id": 341261569, "node_id": "MDU6SXNzdWUzNDEyNjE1Njk=", "number": 20805, "title": "Eager execution and custom layer", "user": {"login": "nairouz", "id": 10966954, "node_id": "MDQ6VXNlcjEwOTY2OTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/10966954?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairouz", "html_url": "https://github.com/nairouz", "followers_url": "https://api.github.com/users/nairouz/followers", "following_url": "https://api.github.com/users/nairouz/following{/other_user}", "gists_url": "https://api.github.com/users/nairouz/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairouz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairouz/subscriptions", "organizations_url": "https://api.github.com/users/nairouz/orgs", "repos_url": "https://api.github.com/users/nairouz/repos", "events_url": "https://api.github.com/users/nairouz/events{/privacy}", "received_events_url": "https://api.github.com/users/nairouz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, {"login": "anj-s", "id": 32556631, "node_id": "MDQ6VXNlcjMyNTU2NjMx", "avatar_url": "https://avatars1.githubusercontent.com/u/32556631?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anj-s", "html_url": "https://github.com/anj-s", "followers_url": "https://api.github.com/users/anj-s/followers", "following_url": "https://api.github.com/users/anj-s/following{/other_user}", "gists_url": "https://api.github.com/users/anj-s/gists{/gist_id}", "starred_url": "https://api.github.com/users/anj-s/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anj-s/subscriptions", "organizations_url": "https://api.github.com/users/anj-s/orgs", "repos_url": "https://api.github.com/users/anj-s/repos", "events_url": "https://api.github.com/users/anj-s/events{/privacy}", "received_events_url": "https://api.github.com/users/anj-s/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2018-07-14T19:32:37Z", "updated_at": "2018-11-14T19:22:43Z", "closed_at": null, "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Google Colaboratory</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9.0-rc2</li>\n<li><strong>Python version</strong>:  version 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>: Colab GPU</li>\n<li><strong>Exact command to reproduce</strong>: <a href=\"https://gist.github.com/nairouz/035a830d1e58a3759a6a1e193f5defed\">https://gist.github.com/nairouz/035a830d1e58a3759a6a1e193f5defed</a></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I have a simple Keras model with one custom layer which works fine on the graph based execution. When I switched to eager execution via tf.enable_eager_execution(), I got stuck on a weird error.</p>\n<h3>Source code</h3>\n<pre><code>import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Layer, Input\nfrom tensorflow.keras.losses import kullback_leibler_divergence\n\ntf.enable_eager_execution()\n\nclass ClusteringLayer(Layer):\n    def __init__(self, output_dim, input_dim=None, alpha=1.0, **kwargs):\n        self.output_dim = output_dim\n        self.input_dim = input_dim\n        self.alpha = alpha\n        super(ClusteringLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(name='kernel', shape=(self.output_dim, input_shape[1].value), initializer='Identity', trainable=True)\n        super(ClusteringLayer, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        q = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(x, 1) - self.W), axis=2))**2 /self.alpha)\n        q = q**((self.alpha+1.0)/2.0)\n        q = K.transpose(K.transpose(q)/K.sum(q, axis=1))\n        return q\n\ndef compute_output_shape(self, input_shape):\n    return (input_shape[0], self.output_dim)\n\ndef clustering_loss(y_true, y_pred): \n    a = K.square(y_pred) / K.sum(y_pred, axis=0) \n    p = K.transpose(K.transpose(a) / K.sum(a, axis=1))\n    loss = kullback_leibler_divergence(p, y_pred)\n    return loss\n\ninput1 = Input(shape=(10,), name=\"input\")\nout = ClusteringLayer(output_dim = 5, name='clustering')(input1)\nmodel = Model(inputs=input1, outputs=out) \nmodel.compile(optimizer=tf.train.AdamOptimizer(1e-3), loss={'clustering' : clustering_loss})\nnp.random.seed(0)\nX = np.random.random((20, 10)).astype(np.float32)\nY = np.random.random((20, 5)).astype(np.float32)\nmodel.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)\n</code></pre>\n<p>###Logs<br>\nclustering<br>\nEpoch 1/10</p>\n<hr>\n<p>AssertionError                            Traceback (most recent call last)<br>\n in ()<br>\n37 Y = np.random.random((20, 5))<br>\n38<br>\n---&gt; 39 model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)<br>\n40</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)<br>\n1331           initial_epoch=initial_epoch,<br>\n1332           steps_per_epoch=steps_per_epoch,<br>\n-&gt; 1333           validation_steps=validation_steps)<br>\n1334     else:<br>\n1335       return training_arrays.fit_loop(</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)<br>\n1039             shuffle=shuffle,<br>\n1040             num_train_samples=num_train_samples,<br>\n-&gt; 1041             do_validation=do_validation)<br>\n1042       callbacks.on_epoch_end(epoch, epoch_logs)<br>\n1043       if callback_model.stop_training:</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in batch_fit_loop(model, inputs, targets, epoch_logs, index_array, out_labels, callback_model, batch_size, sample_weights, val_inputs, val_targets, val_sample_weights, callbacks, shuffle, num_train_samples, do_validation)<br>\n395         targets_batch,<br>\n396         sample_weights=sample_weights_batch,<br>\n--&gt; 397         training=True)<br>\n398<br>\n399     if not isinstance(outs, list):</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)<br>\n787       outs, loss, loss_metrics = _model_loss(model, inputs, targets,<br>\n788                                              sample_weights=sample_weights,<br>\n--&gt; 789                                              training=training)<br>\n790       if loss is None:<br>\n791         raise ValueError('The model cannot be run '</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)<br>\n126       outs = model.call(inputs[0], training=training)<br>\n127     else:<br>\n--&gt; 128       outs = model.call(inputs[0])<br>\n129   else:<br>\n130     if model._expects_training_arg:</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in call(self, inputs, training, mask)<br>\n718     outputs, _ = self._run_internal_graph(inputs,<br>\n719                                           training=training,<br>\n--&gt; 720                                           mask=masks)<br>\n721     return outputs<br>\n722</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)<br>\n939     output_shapes = []<br>\n940     for x in self.outputs:<br>\n--&gt; 941       assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)<br>\n942       tensor, mask = tensor_map[str(id(x))]<br>\n943       output_shapes.append(backend.int_shape(x))</p>\n<p>AssertionError: Could not compute output DeferredTensor('None', shape=(5,), dtype=float32)</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below): 1.9.0-rc2\nPython version:  version 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory: Colab GPU\nExact command to reproduce: https://gist.github.com/nairouz/035a830d1e58a3759a6a1e193f5defed\n\nDescribe the problem\nI have a simple Keras model with one custom layer which works fine on the graph based execution. When I switched to eager execution via tf.enable_eager_execution(), I got stuck on a weird error.\nSource code\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Layer, Input\nfrom tensorflow.keras.losses import kullback_leibler_divergence\n\ntf.enable_eager_execution()\n\nclass ClusteringLayer(Layer):\n    def __init__(self, output_dim, input_dim=None, alpha=1.0, **kwargs):\n        self.output_dim = output_dim\n        self.input_dim = input_dim\n        self.alpha = alpha\n        super(ClusteringLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(name='kernel', shape=(self.output_dim, input_shape[1].value), initializer='Identity', trainable=True)\n        super(ClusteringLayer, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        q = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(x, 1) - self.W), axis=2))**2 /self.alpha)\n        q = q**((self.alpha+1.0)/2.0)\n        q = K.transpose(K.transpose(q)/K.sum(q, axis=1))\n        return q\n\ndef compute_output_shape(self, input_shape):\n    return (input_shape[0], self.output_dim)\n\ndef clustering_loss(y_true, y_pred): \n    a = K.square(y_pred) / K.sum(y_pred, axis=0) \n    p = K.transpose(K.transpose(a) / K.sum(a, axis=1))\n    loss = kullback_leibler_divergence(p, y_pred)\n    return loss\n\ninput1 = Input(shape=(10,), name=\"input\")\nout = ClusteringLayer(output_dim = 5, name='clustering')(input1)\nmodel = Model(inputs=input1, outputs=out) \nmodel.compile(optimizer=tf.train.AdamOptimizer(1e-3), loss={'clustering' : clustering_loss})\nnp.random.seed(0)\nX = np.random.random((20, 10)).astype(np.float32)\nY = np.random.random((20, 5)).astype(np.float32)\nmodel.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)\n\n###Logs\nclustering\nEpoch 1/10\n\nAssertionError                            Traceback (most recent call last)\n in ()\n37 Y = np.random.random((20, 5))\n38\n---> 39 model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)\n40\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\n1331           initial_epoch=initial_epoch,\n1332           steps_per_epoch=steps_per_epoch,\n-> 1333           validation_steps=validation_steps)\n1334     else:\n1335       return training_arrays.fit_loop(\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\n1039             shuffle=shuffle,\n1040             num_train_samples=num_train_samples,\n-> 1041             do_validation=do_validation)\n1042       callbacks.on_epoch_end(epoch, epoch_logs)\n1043       if callback_model.stop_training:\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in batch_fit_loop(model, inputs, targets, epoch_logs, index_array, out_labels, callback_model, batch_size, sample_weights, val_inputs, val_targets, val_sample_weights, callbacks, shuffle, num_train_samples, do_validation)\n395         targets_batch,\n396         sample_weights=sample_weights_batch,\n--> 397         training=True)\n398\n399     if not isinstance(outs, list):\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)\n787       outs, loss, loss_metrics = _model_loss(model, inputs, targets,\n788                                              sample_weights=sample_weights,\n--> 789                                              training=training)\n790       if loss is None:\n791         raise ValueError('The model cannot be run '\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)\n126       outs = model.call(inputs[0], training=training)\n127     else:\n--> 128       outs = model.call(inputs[0])\n129   else:\n130     if model._expects_training_arg:\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in call(self, inputs, training, mask)\n718     outputs, _ = self._run_internal_graph(inputs,\n719                                           training=training,\n--> 720                                           mask=masks)\n721     return outputs\n722\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\n939     output_shapes = []\n940     for x in self.outputs:\n--> 941       assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\n942       tensor, mask = tensor_map[str(id(x))]\n943       output_shapes.append(backend.int_shape(x))\nAssertionError: Could not compute output DeferredTensor('None', shape=(5,), dtype=float32)", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colaboratory\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.9.0-rc2\r\n- **Python version**:  version 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: Colab GPU\r\n- **Exact command to reproduce**: https://gist.github.com/nairouz/035a830d1e58a3759a6a1e193f5defed\r\n\r\n### Describe the problem\r\nI have a simple Keras model with one custom layer which works fine on the graph based execution. When I switched to eager execution via tf.enable_eager_execution(), I got stuck on a weird error.\r\n\r\n### Source code \r\n    import numpy as np\r\n    import tensorflow as tf\r\n    import tensorflow.keras.backend as K\r\n    from tensorflow.keras.models import Model\r\n    from tensorflow.keras.layers import Layer, Input\r\n    from tensorflow.keras.losses import kullback_leibler_divergence\r\n\r\n    tf.enable_eager_execution()\r\n\r\n    class ClusteringLayer(Layer):\r\n        def __init__(self, output_dim, input_dim=None, alpha=1.0, **kwargs):\r\n            self.output_dim = output_dim\r\n            self.input_dim = input_dim\r\n            self.alpha = alpha\r\n            super(ClusteringLayer, self).__init__(**kwargs)\r\n\r\n        def build(self, input_shape):\r\n            self.W = self.add_weight(name='kernel', shape=(self.output_dim, input_shape[1].value), initializer='Identity', trainable=True)\r\n            super(ClusteringLayer, self).build(input_shape)\r\n\r\n        def call(self, x, mask=None):\r\n            q = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(x, 1) - self.W), axis=2))**2 /self.alpha)\r\n            q = q**((self.alpha+1.0)/2.0)\r\n            q = K.transpose(K.transpose(q)/K.sum(q, axis=1))\r\n            return q\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], self.output_dim)\r\n\r\n    def clustering_loss(y_true, y_pred): \r\n        a = K.square(y_pred) / K.sum(y_pred, axis=0) \r\n        p = K.transpose(K.transpose(a) / K.sum(a, axis=1))\r\n        loss = kullback_leibler_divergence(p, y_pred)\r\n        return loss\r\n\r\n    input1 = Input(shape=(10,), name=\"input\")\r\n    out = ClusteringLayer(output_dim = 5, name='clustering')(input1)\r\n    model = Model(inputs=input1, outputs=out) \r\n    model.compile(optimizer=tf.train.AdamOptimizer(1e-3), loss={'clustering' : clustering_loss})\r\n    np.random.seed(0)\r\n    X = np.random.random((20, 10)).astype(np.float32)\r\n    Y = np.random.random((20, 5)).astype(np.float32)\r\n    model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)\r\n\r\n###Logs\r\nclustering\r\nEpoch 1/10\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-2-1f474aaabb09> in <module>()\r\n     37 Y = np.random.random((20, 5))\r\n     38 \r\n---> 39 model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)\r\n     40 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1331           initial_epoch=initial_epoch,\r\n   1332           steps_per_epoch=steps_per_epoch,\r\n-> 1333           validation_steps=validation_steps)\r\n   1334     else:\r\n   1335       return training_arrays.fit_loop(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n   1039             shuffle=shuffle,\r\n   1040             num_train_samples=num_train_samples,\r\n-> 1041             do_validation=do_validation)\r\n   1042       callbacks.on_epoch_end(epoch, epoch_logs)\r\n   1043       if callback_model.stop_training:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in batch_fit_loop(model, inputs, targets, epoch_logs, index_array, out_labels, callback_model, batch_size, sample_weights, val_inputs, val_targets, val_sample_weights, callbacks, shuffle, num_train_samples, do_validation)\r\n    395         targets_batch,\r\n    396         sample_weights=sample_weights_batch,\r\n--> 397         training=True)\r\n    398 \r\n    399     if not isinstance(outs, list):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)\r\n    787       outs, loss, loss_metrics = _model_loss(model, inputs, targets,\r\n    788                                              sample_weights=sample_weights,\r\n--> 789                                              training=training)\r\n    790       if loss is None:\r\n    791         raise ValueError('The model cannot be run '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)\r\n    126       outs = model.call(inputs[0], training=training)\r\n    127     else:\r\n--> 128       outs = model.call(inputs[0])\r\n    129   else:\r\n    130     if model._expects_training_arg:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    718     outputs, _ = self._run_internal_graph(inputs,\r\n    719                                           training=training,\r\n--> 720                                           mask=masks)\r\n    721     return outputs\r\n    722 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\r\n    939     output_shapes = []\r\n    940     for x in self.outputs:\r\n--> 941       assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\r\n    942       tensor, mask = tensor_map[str(id(x))]\r\n    943       output_shapes.append(backend.int_shape(x))\r\n\r\nAssertionError: Could not compute output DeferredTensor('None', shape=(5,), dtype=float32)\r\n\r\n\r\n"}