{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/428975690", "html_url": "https://github.com/tensorflow/tensorflow/issues/22869#issuecomment-428975690", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22869", "id": 428975690, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODk3NTY5MA==", "user": {"login": "FrancescoFornasa", "id": 7392401, "node_id": "MDQ6VXNlcjczOTI0MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/7392401?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FrancescoFornasa", "html_url": "https://github.com/FrancescoFornasa", "followers_url": "https://api.github.com/users/FrancescoFornasa/followers", "following_url": "https://api.github.com/users/FrancescoFornasa/following{/other_user}", "gists_url": "https://api.github.com/users/FrancescoFornasa/gists{/gist_id}", "starred_url": "https://api.github.com/users/FrancescoFornasa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FrancescoFornasa/subscriptions", "organizations_url": "https://api.github.com/users/FrancescoFornasa/orgs", "repos_url": "https://api.github.com/users/FrancescoFornasa/repos", "events_url": "https://api.github.com/users/FrancescoFornasa/events{/privacy}", "received_events_url": "https://api.github.com/users/FrancescoFornasa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-11T14:25:50Z", "updated_at": "2018-10-11T14:25:50Z", "author_association": "NONE", "body_html": "<p>Personally, it is also my belief that the documentation is lacking an indication on how to correctly perform a batched inference step.<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2693144\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sabhiram\">@sabhiram</a> , you pointed out that a model must have an input pipeline that allows multiple inputs, but i am working with a single image input layer (since I am using the same exact model on Keras) and I believe that I should be able to perform a batched inference pass on that network.</p>\n<p>If I use a SessionRun inside a loop (on the same graph) am I not introducing an overhead caused by the overhead time needed to load an image and a graph on gpu each time?</p>\n<p>Thank you all for your kind attention.</p>\n<p>P.s. Since I personally believe that the documentation is lacking some information, this place should be the right place in which to ask for an answer :)</p>", "body_text": "Personally, it is also my belief that the documentation is lacking an indication on how to correctly perform a batched inference step.\n@sabhiram , you pointed out that a model must have an input pipeline that allows multiple inputs, but i am working with a single image input layer (since I am using the same exact model on Keras) and I believe that I should be able to perform a batched inference pass on that network.\nIf I use a SessionRun inside a loop (on the same graph) am I not introducing an overhead caused by the overhead time needed to load an image and a graph on gpu each time?\nThank you all for your kind attention.\nP.s. Since I personally believe that the documentation is lacking some information, this place should be the right place in which to ask for an answer :)", "body": "Personally, it is also my belief that the documentation is lacking an indication on how to correctly perform a batched inference step.\r\n@sabhiram , you pointed out that a model must have an input pipeline that allows multiple inputs, but i am working with a single image input layer (since I am using the same exact model on Keras) and I believe that I should be able to perform a batched inference pass on that network.\r\n\r\nIf I use a SessionRun inside a loop (on the same graph) am I not introducing an overhead caused by the overhead time needed to load an image and a graph on gpu each time?\r\n\r\nThank you all for your kind attention.\r\n\r\nP.s. Since I personally believe that the documentation is lacking some information, this place should be the right place in which to ask for an answer :) "}