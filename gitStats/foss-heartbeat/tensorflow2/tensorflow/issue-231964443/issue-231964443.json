{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10270", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10270/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10270/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10270/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10270", "id": 231964443, "node_id": "MDU6SXNzdWUyMzE5NjQ0NDM=", "number": 10270, "title": "Performance degradation with large lookup tables - optimizer._apply_sparse_duplicate_indices  (TF V1.0.1)", "user": {"login": "KashiErez", "id": 8734262, "node_id": "MDQ6VXNlcjg3MzQyNjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/8734262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KashiErez", "html_url": "https://github.com/KashiErez", "followers_url": "https://api.github.com/users/KashiErez/followers", "following_url": "https://api.github.com/users/KashiErez/following{/other_user}", "gists_url": "https://api.github.com/users/KashiErez/gists{/gist_id}", "starred_url": "https://api.github.com/users/KashiErez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KashiErez/subscriptions", "organizations_url": "https://api.github.com/users/KashiErez/orgs", "repos_url": "https://api.github.com/users/KashiErez/repos", "events_url": "https://api.github.com/users/KashiErez/events{/privacy}", "received_events_url": "https://api.github.com/users/KashiErez/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2017-05-29T08:56:55Z", "updated_at": "2018-11-14T19:12:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I ran into this performance issue while trying to upgrade tensorflow from version 0.12.1  to 1.X.</p>\n<p>We ran a network with large embedding lookup tables:</p>\n<ul>\n<li>100K X 32 (for example, word embedding -  with 100K unique words)</li>\n<li>300K X 128 (for example, categorical feature with cardinality of 300K unique items)</li>\n</ul>\n<p>After upgrading TF version to 1.0.1,  GPU usage dropped in from 60% to 30%.<br>\nTraining time went up in 50%-200% (depends on how big is the embedding lookup table).</p>\n<p>This is the commit that caused the performance degradation:<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/f9f56f9dc7fe41ef1128290a77ac88e889ea5229/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/f9f56f9dc7fe41ef1128290a77ac88e889ea5229\"><tt>f9f56f9</tt></a></p>\n<p>The handling of unique indexes is very slow and does not run in parallel with others operations.<br>\nPlease note the big unique blocks in the middle.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/8734262/26542969/ab0f3740-4464-11e7-9dcb-f3ccd58dfc8a.png\"><img src=\"https://cloud.githubusercontent.com/assets/8734262/26542969/ab0f3740-4464-11e7-9dcb-f3ccd58dfc8a.png\" alt=\"trace_unique\" style=\"max-width:100%;\"></a></p>\n<p>Here is a work around (not handling unique indexes ):</p>\n<pre><code>class MyOptimizer(tf.train.AdamOptimizer):\n        def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n               use_locking=False, name=\"Adam\"):\n                super(MyOptimizer,self).__init__(learning_rate,beta1, beta2, epsilon, use_locking,name)\n\n        def _apply_sparse_duplicate_indices(self, grad, var):\n                return self._apply_sparse(grad, var)\n</code></pre>\n<p>Thanks,<br>\nErez</p>", "body_text": "Hi,\nI ran into this performance issue while trying to upgrade tensorflow from version 0.12.1  to 1.X.\nWe ran a network with large embedding lookup tables:\n\n100K X 32 (for example, word embedding -  with 100K unique words)\n300K X 128 (for example, categorical feature with cardinality of 300K unique items)\n\nAfter upgrading TF version to 1.0.1,  GPU usage dropped in from 60% to 30%.\nTraining time went up in 50%-200% (depends on how big is the embedding lookup table).\nThis is the commit that caused the performance degradation:\nf9f56f9\nThe handling of unique indexes is very slow and does not run in parallel with others operations.\nPlease note the big unique blocks in the middle.\n\nHere is a work around (not handling unique indexes ):\nclass MyOptimizer(tf.train.AdamOptimizer):\n        def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n               use_locking=False, name=\"Adam\"):\n                super(MyOptimizer,self).__init__(learning_rate,beta1, beta2, epsilon, use_locking,name)\n\n        def _apply_sparse_duplicate_indices(self, grad, var):\n                return self._apply_sparse(grad, var)\n\nThanks,\nErez", "body": "Hi,\r\n\r\nI ran into this performance issue while trying to upgrade tensorflow from version 0.12.1  to 1.X.\r\n\r\nWe ran a network with large embedding lookup tables:\r\n- 100K X 32 (for example, word embedding -  with 100K unique words)\r\n- 300K X 128 (for example, categorical feature with cardinality of 300K unique items)\r\n\r\n After upgrading TF version to 1.0.1,  GPU usage dropped in from 60% to 30%.\r\nTraining time went up in 50%-200% (depends on how big is the embedding lookup table). \r\n\r\n\r\nThis is the commit that caused the performance degradation:\r\nhttps://github.com/tensorflow/tensorflow/commit/f9f56f9dc7fe41ef1128290a77ac88e889ea5229\r\n\r\nThe handling of unique indexes is very slow and does not run in parallel with others operations. \r\nPlease note the big unique blocks in the middle.\r\n![trace_unique](https://cloud.githubusercontent.com/assets/8734262/26542969/ab0f3740-4464-11e7-9dcb-f3ccd58dfc8a.png)\r\n\r\nHere is a work around (not handling unique indexes ):\r\n```\r\nclass MyOptimizer(tf.train.AdamOptimizer):\r\n        def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\r\n               use_locking=False, name=\"Adam\"):\r\n                super(MyOptimizer,self).__init__(learning_rate,beta1, beta2, epsilon, use_locking,name)\r\n\r\n        def _apply_sparse_duplicate_indices(self, grad, var):\r\n                return self._apply_sparse(grad, var)\r\n```\r\n\r\n\r\nThanks,\r\nErez\r\n"}