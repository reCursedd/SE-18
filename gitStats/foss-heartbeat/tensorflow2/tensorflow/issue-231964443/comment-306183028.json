{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/306183028", "html_url": "https://github.com/tensorflow/tensorflow/issues/10270#issuecomment-306183028", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10270", "id": 306183028, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjE4MzAyOA==", "user": {"login": "KashiErez", "id": 8734262, "node_id": "MDQ6VXNlcjg3MzQyNjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/8734262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KashiErez", "html_url": "https://github.com/KashiErez", "followers_url": "https://api.github.com/users/KashiErez/followers", "following_url": "https://api.github.com/users/KashiErez/following{/other_user}", "gists_url": "https://api.github.com/users/KashiErez/gists{/gist_id}", "starred_url": "https://api.github.com/users/KashiErez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KashiErez/subscriptions", "organizations_url": "https://api.github.com/users/KashiErez/orgs", "repos_url": "https://api.github.com/users/KashiErez/repos", "events_url": "https://api.github.com/users/KashiErez/events{/privacy}", "received_events_url": "https://api.github.com/users/KashiErez/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-05T13:08:26Z", "updated_at": "2017-06-05T13:08:43Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>This is the optimizer I used to print the indices shape:<br>\n(I copied the code from optimizer.Optimizer class, and injected a tf.Print statement).</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\n\n\ndef _my_deduplicate_indexed_slices(values, indices, grad):\n    \"\"\"Sums `values` associated with any non-unique `indices`.\n    Args:\n      values: A `Tensor` with rank &gt;= 1.\n      indices: A one-dimensional integer `Tensor`, indexing into the first\n        dimension of `values` (as in an IndexedSlices object).\n    Returns:\n      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\n      de-duplicated version of `indices` and `summed_values` contains the sum of\n      `values` slices associated with each unique index.\n    \"\"\"\n\n    indices = tf.Print(indices,\n                       data=[grad.name, tf.shape(indices)],\n                       message='$$$$$$$$ indices.shape   ----- ',\n                       first_n=1)\n\n    unique_indices, new_index_positions = array_ops.unique(indices)\n    summed_values = math_ops.unsorted_segment_sum(\n        values, new_index_positions,\n        array_ops.shape(unique_indices)[0])\n    return (summed_values, unique_indices)\n\n\nclass MyAdamOptimizer(tf.train.AdamOptimizer):\n    \"\"\"\n    This Optimizer is a workaround for this issue:\n    https://github.com/tensorflow/tensorflow/issues/10270\n    \"\"\"\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n                 use_locking=False, name=\"MyAdam\"):\n        super(MyAdamOptimizer, self).__init__(learning_rate, beta1, beta2, epsilon, use_locking, name)\n\n    def _apply_sparse_duplicate_indices(self, grad, var):\n        \"\"\"\n        overriding method to avoid 'unique indexes' logic.\n        'unique indexes' logic produce performance degradation in large lookup tables\n        \"\"\"\n\n\n        summed_values, unique_indices = _my_deduplicate_indexed_slices(\n            values=grad.values, indices=grad.indices, grad=grad)\n\n        gradient_no_duplicate_indices = ops.IndexedSlices(\n            indices=unique_indices,\n            values=summed_values,\n            dense_shape=grad.dense_shape)\n        return self._apply_sparse(gradient_no_duplicate_indices, var)\n\n</code></pre>\n<p>I ran it on 3 models, here are the results:</p>\n<p>Model 1 - few small categorical features (no more then 10K each):</p>\n<pre><code>2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_8:0][10266]\n2017-06-05 12:49:30.234021: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_4:0][1058]\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat:0][8687]\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_6:0][1033]\n2017-06-05 12:49:30.237197: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_2:0][1036]\n</code></pre>\n<p>Model 2 - small categorical features (no more then 10K each) + words (50K unique values):</p>\n<pre><code>2017-06-05 11:39:45.333000: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_8:0][10266]\n2017-06-05 11:39:45.332994: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_4:0][1058]\n2017-06-05 11:39:45.332984: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_2:0][1036]\n2017-06-05 11:39:45.333465: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat:0][8687]\n2017-06-05 11:39:45.333727: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_6:0][1033]\n2017-06-05 11:39:45.333756: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_10:0][70681]\n</code></pre>\n<p>Model 3 - small categorical features (no more then 10K each) + words (50K unique values) + big categorical feature (300K unique values):</p>\n<pre><code>2017-06-05 12:41:13.841337: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_6:0][1033]\n2017-06-05 12:41:13.842412: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_4:0][1057]\n2017-06-05 12:41:13.840662: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_8:0][9131]\n2017-06-05 12:41:13.841332: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_12:0][40896]\n2017-06-05 12:41:13.841334: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat:0][8632]\n2017-06-05 12:41:13.841974: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_10:0][315805]\n2017-06-05 12:41:13.844600: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_2:0][1036]\n\n</code></pre>\n<p>Summing up indices sizes:<br>\nModel 1 -   22080  (small categorical features)<br>\nModel 2 -   92761  (small categorical features + 50k feature)<br>\nModel 3 - 377590 (small categorical features + 50k feature + 300K feature)</p>\n<p>Summing up, feature cardinality size and indices size are correlated.</p>\n<p>Small Note, regarding the gradient scopes - my_optimizer_0/gradients/concat*  :</p>\n<p>my_optimizer_0  - this is my code.<br>\ngradients/concat* - I think it comes from here: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py</a></p>", "body_text": "Hi,\nThis is the optimizer I used to print the indices shape:\n(I copied the code from optimizer.Optimizer class, and injected a tf.Print statement).\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\n\n\ndef _my_deduplicate_indexed_slices(values, indices, grad):\n    \"\"\"Sums `values` associated with any non-unique `indices`.\n    Args:\n      values: A `Tensor` with rank >= 1.\n      indices: A one-dimensional integer `Tensor`, indexing into the first\n        dimension of `values` (as in an IndexedSlices object).\n    Returns:\n      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\n      de-duplicated version of `indices` and `summed_values` contains the sum of\n      `values` slices associated with each unique index.\n    \"\"\"\n\n    indices = tf.Print(indices,\n                       data=[grad.name, tf.shape(indices)],\n                       message='$$$$$$$$ indices.shape   ----- ',\n                       first_n=1)\n\n    unique_indices, new_index_positions = array_ops.unique(indices)\n    summed_values = math_ops.unsorted_segment_sum(\n        values, new_index_positions,\n        array_ops.shape(unique_indices)[0])\n    return (summed_values, unique_indices)\n\n\nclass MyAdamOptimizer(tf.train.AdamOptimizer):\n    \"\"\"\n    This Optimizer is a workaround for this issue:\n    https://github.com/tensorflow/tensorflow/issues/10270\n    \"\"\"\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n                 use_locking=False, name=\"MyAdam\"):\n        super(MyAdamOptimizer, self).__init__(learning_rate, beta1, beta2, epsilon, use_locking, name)\n\n    def _apply_sparse_duplicate_indices(self, grad, var):\n        \"\"\"\n        overriding method to avoid 'unique indexes' logic.\n        'unique indexes' logic produce performance degradation in large lookup tables\n        \"\"\"\n\n\n        summed_values, unique_indices = _my_deduplicate_indexed_slices(\n            values=grad.values, indices=grad.indices, grad=grad)\n\n        gradient_no_duplicate_indices = ops.IndexedSlices(\n            indices=unique_indices,\n            values=summed_values,\n            dense_shape=grad.dense_shape)\n        return self._apply_sparse(gradient_no_duplicate_indices, var)\n\n\nI ran it on 3 models, here are the results:\nModel 1 - few small categorical features (no more then 10K each):\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_8:0][10266]\n2017-06-05 12:49:30.234021: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_4:0][1058]\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat:0][8687]\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_6:0][1033]\n2017-06-05 12:49:30.237197: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_2:0][1036]\n\nModel 2 - small categorical features (no more then 10K each) + words (50K unique values):\n2017-06-05 11:39:45.333000: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_8:0][10266]\n2017-06-05 11:39:45.332994: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_4:0][1058]\n2017-06-05 11:39:45.332984: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_2:0][1036]\n2017-06-05 11:39:45.333465: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat:0][8687]\n2017-06-05 11:39:45.333727: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_6:0][1033]\n2017-06-05 11:39:45.333756: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_10:0][70681]\n\nModel 3 - small categorical features (no more then 10K each) + words (50K unique values) + big categorical feature (300K unique values):\n2017-06-05 12:41:13.841337: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_6:0][1033]\n2017-06-05 12:41:13.842412: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_4:0][1057]\n2017-06-05 12:41:13.840662: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_8:0][9131]\n2017-06-05 12:41:13.841332: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_12:0][40896]\n2017-06-05 12:41:13.841334: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat:0][8632]\n2017-06-05 12:41:13.841974: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_10:0][315805]\n2017-06-05 12:41:13.844600: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_2:0][1036]\n\n\nSumming up indices sizes:\nModel 1 -   22080  (small categorical features)\nModel 2 -   92761  (small categorical features + 50k feature)\nModel 3 - 377590 (small categorical features + 50k feature + 300K feature)\nSumming up, feature cardinality size and indices size are correlated.\nSmall Note, regarding the gradient scopes - my_optimizer_0/gradients/concat*  :\nmy_optimizer_0  - this is my code.\ngradients/concat* - I think it comes from here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py", "body": "Hi,\r\n\r\nThis is the optimizer I used to print the indices shape:\r\n(I copied the code from optimizer.Optimizer class, and injected a tf.Print statement).\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import math_ops\r\n\r\n\r\ndef _my_deduplicate_indexed_slices(values, indices, grad):\r\n    \"\"\"Sums `values` associated with any non-unique `indices`.\r\n    Args:\r\n      values: A `Tensor` with rank >= 1.\r\n      indices: A one-dimensional integer `Tensor`, indexing into the first\r\n        dimension of `values` (as in an IndexedSlices object).\r\n    Returns:\r\n      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\r\n      de-duplicated version of `indices` and `summed_values` contains the sum of\r\n      `values` slices associated with each unique index.\r\n    \"\"\"\r\n\r\n    indices = tf.Print(indices,\r\n                       data=[grad.name, tf.shape(indices)],\r\n                       message='$$$$$$$$ indices.shape   ----- ',\r\n                       first_n=1)\r\n\r\n    unique_indices, new_index_positions = array_ops.unique(indices)\r\n    summed_values = math_ops.unsorted_segment_sum(\r\n        values, new_index_positions,\r\n        array_ops.shape(unique_indices)[0])\r\n    return (summed_values, unique_indices)\r\n\r\n\r\nclass MyAdamOptimizer(tf.train.AdamOptimizer):\r\n    \"\"\"\r\n    This Optimizer is a workaround for this issue:\r\n    https://github.com/tensorflow/tensorflow/issues/10270\r\n    \"\"\"\r\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\r\n                 use_locking=False, name=\"MyAdam\"):\r\n        super(MyAdamOptimizer, self).__init__(learning_rate, beta1, beta2, epsilon, use_locking, name)\r\n\r\n    def _apply_sparse_duplicate_indices(self, grad, var):\r\n        \"\"\"\r\n        overriding method to avoid 'unique indexes' logic.\r\n        'unique indexes' logic produce performance degradation in large lookup tables\r\n        \"\"\"\r\n\r\n\r\n        summed_values, unique_indices = _my_deduplicate_indexed_slices(\r\n            values=grad.values, indices=grad.indices, grad=grad)\r\n\r\n        gradient_no_duplicate_indices = ops.IndexedSlices(\r\n            indices=unique_indices,\r\n            values=summed_values,\r\n            dense_shape=grad.dense_shape)\r\n        return self._apply_sparse(gradient_no_duplicate_indices, var)\r\n\r\n```\r\n\r\nI ran it on 3 models, here are the results:\r\n\r\nModel 1 - few small categorical features (no more then 10K each):\r\n\r\n```\r\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_8:0][10266]\r\n2017-06-05 12:49:30.234021: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_4:0][1058]\r\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat:0][8687]\r\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_6:0][1033]\r\n2017-06-05 12:49:30.237197: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_2:0][1036]\r\n```\r\n\r\nModel 2 - small categorical features (no more then 10K each) + words (50K unique values):\r\n\r\n```\r\n2017-06-05 11:39:45.333000: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_8:0][10266]\r\n2017-06-05 11:39:45.332994: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_4:0][1058]\r\n2017-06-05 11:39:45.332984: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_2:0][1036]\r\n2017-06-05 11:39:45.333465: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat:0][8687]\r\n2017-06-05 11:39:45.333727: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_6:0][1033]\r\n2017-06-05 11:39:45.333756: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_10:0][70681]\r\n```\r\n\r\n\r\nModel 3 - small categorical features (no more then 10K each) + words (50K unique values) + big categorical feature (300K unique values):\r\n\r\n```\r\n2017-06-05 12:41:13.841337: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_6:0][1033]\r\n2017-06-05 12:41:13.842412: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_4:0][1057]\r\n2017-06-05 12:41:13.840662: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_8:0][9131]\r\n2017-06-05 12:41:13.841332: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_12:0][40896]\r\n2017-06-05 12:41:13.841334: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat:0][8632]\r\n2017-06-05 12:41:13.841974: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_10:0][315805]\r\n2017-06-05 12:41:13.844600: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_2:0][1036]\r\n\r\n```\r\n\r\nSumming up indices sizes:\r\nModel 1 -   22080  (small categorical features)\r\nModel 2 -   92761  (small categorical features + 50k feature)\r\nModel 3 - 377590 (small categorical features + 50k feature + 300K feature)\r\n\r\nSumming up, feature cardinality size and indices size are correlated.\r\n\r\n \r\nSmall Note, regarding the gradient scopes - my_optimizer_0/gradients/concat*  :\r\n\r\nmy_optimizer_0  - this is my code.\r\ngradients/concat* - I think it comes from here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py\r\n\r\n\r\n"}