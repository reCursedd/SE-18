{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/412579295", "html_url": "https://github.com/tensorflow/tensorflow/issues/10270#issuecomment-412579295", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10270", "id": 412579295, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjU3OTI5NQ==", "user": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-13T16:28:31Z", "updated_at": "2018-08-13T16:28:31Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1762074\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ningyuwhut\">@ningyuwhut</a> I'd suggest using the workaround <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8734262\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/KashiErez\">@KashiErez</a> mentioned for now if you don't care about deduplicating sparse gradients and want the previous behavior. This was a bug fix, so we can't just go back to the old behavior by default.</p>\n<p>AFAIK nobody is working on a GPU kernel for UniqueOp, but that still seems like the resolution here if you're interested in taking the bug.</p>", "body_text": "@ningyuwhut I'd suggest using the workaround @KashiErez mentioned for now if you don't care about deduplicating sparse gradients and want the previous behavior. This was a bug fix, so we can't just go back to the old behavior by default.\nAFAIK nobody is working on a GPU kernel for UniqueOp, but that still seems like the resolution here if you're interested in taking the bug.", "body": "@ningyuwhut I'd suggest using the workaround @KashiErez mentioned for now if you don't care about deduplicating sparse gradients and want the previous behavior. This was a bug fix, so we can't just go back to the old behavior by default.\r\n\r\nAFAIK nobody is working on a GPU kernel for UniqueOp, but that still seems like the resolution here if you're interested in taking the bug."}