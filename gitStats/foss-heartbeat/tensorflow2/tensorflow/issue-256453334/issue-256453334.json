{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12937", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12937/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12937/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12937/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12937", "id": 256453334, "node_id": "MDU6SXNzdWUyNTY0NTMzMzQ=", "number": 12937, "title": "Possible Bug: while_loop, map_fn do not parallize ", "user": {"login": "georgh", "id": 1831252, "node_id": "MDQ6VXNlcjE4MzEyNTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1831252?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgh", "html_url": "https://github.com/georgh", "followers_url": "https://api.github.com/users/georgh/followers", "following_url": "https://api.github.com/users/georgh/following{/other_user}", "gists_url": "https://api.github.com/users/georgh/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgh/subscriptions", "organizations_url": "https://api.github.com/users/georgh/orgs", "repos_url": "https://api.github.com/users/georgh/repos", "events_url": "https://api.github.com/users/georgh/events{/privacy}", "received_events_url": "https://api.github.com/users/georgh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-09-09T17:27:27Z", "updated_at": "2017-11-07T01:13:18Z", "closed_at": "2017-11-07T01:12:25Z", "author_association": "NONE", "body_html": "<p>The <code>parallel_iterations</code> parameter of <code>while_loop</code> and <code>map_fn</code> do not reduce the runtime as expected.<br>\nIn comparison to the same operations created in a python loop, <code>while_loop</code> and <code>map_fn</code> are at least 4 times slower. <code>while_loop</code> does not scale at all</p>\n<p>More details:<br>\nFor my project, I need to calculate a matrix blockwise.<br>\nI execute the calculation in a loop, each loop iteration working on a small part of the matrix, not accessing the rest of the matrix.<br>\nTo achieve maximum performance, the loop has to be executed in parallel.<br>\nIn addition, I need to control the number of parallel threads because of memory limitations.<br>\nI tried to use <code>parallel_iterations</code>, but changing this values doesn't change the runtime at all.</p>\n<p>Using a python-for loop to create the same number of operations and syncing them with <code>control_dependencies</code> is around 4 times faster.<br>\nIn contrast to <code>while_loop</code>, <code>map_fn</code> is getting faster with bigger <code>parallel_iterations</code> but remains noticeable slower then the python creation.<br>\nIf this issue is known (didn't find anything at stackoverflow or here) and not resolvable,<br>\nthe documentation of while_loop needs to be improved heavily - to avoid this operation if the loop should be parallized.</p>\n<p>Benchmark-Code:</p>\n<pre><code>import tensorflow as tf\nimport time\n\nif __name__ == '__main__':\n      runs = 100\n      N = 100\n      M = 30\n\n      KernelRow = tf.Variable(tf.zeros([M, N*M]), name='KernelRow')\n\n      def calEntry(KernelVariable, pos):\n            op = tf.assign(KernelVariable[:, pos*M:(pos+1)*M], \n                  tf.eye(M) * 123. + tf.exp(tf.random_uniform([M, M]))) #perform some operations \n            with tf.control_dependencies([op]):\n                  return tf.constant(0)\n\n      def calRow(KernelVariable, N, parallelNum):\n            ops = []\n            for ic in range(0, N, parallelNum):\n                  with tf.control_dependencies(ops):\n                        for j in range(parallelNum):\n                              if ic + j &lt; N:\n                                    ops += [calEntry(KernelVariable, ic+j)]\n                        \n            with tf.control_dependencies(ops):\n                  return tf.identity(KernelVariable)\n\n      def calRow_while(KernelVariable, N, parallelNum):\n            i = tf.constant(0)\n            condition = lambda i: tf.less(i, N)\n            def body(ic):\n                  updateKernel = calEntry(KernelVariable, ic)\n                  with tf.control_dependencies([updateKernel]):\n                        return ic + 1\n            return tf.while_loop(condition, body, [i], back_prop=False, parallel_iterations=parallelNum)\n            \n      def calRow_map(KernelVariable, N, parallelNum):\n            f = lambda x: calEntry(KernelVariable, x)\n            return tf.map_fn(f, tf.range(N, dtype=tf.int32), parallel_iterations=parallelNum)\n\n      paraOps = []\n      paraOpsWhile = []\n      paraOpsMap = []\n      for paraEntry in [1, 10, 50, 100]:\n            paraOps += [(calRow(KernelRow, N, paraEntry), paraEntry )]\n            paraOpsWhile += [(calRow_while(KernelRow, N, paraEntry), paraEntry )]\n            paraOpsMap += [(calRow_map(KernelRow, N, paraEntry), paraEntry )]\n\n      init = tf.global_variables_initializer()\n      #perform calculation\n      with tf.Session() as sess:\n            sess.run(init)\n            for op in paraOps:\n                  runtime = []\n                  sess.run(op[0]) #warm up\n                  for i in range(runs):\n                        startTime = time.time()\n                        sess.run(op[0])\n                        runtime += [time.time() - startTime]\n                  print(\"Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\n            \n            for op in paraOpsWhile:\n                  runtime = []\n                  sess.run(op[0]) #warm up\n                  for i in range(runs):\n                        startTime = time.time()\n                        sess.run(op[0])\n                        runtime += [time.time() - startTime]\n                  print(\"WHILE: Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\n                \n            for op in paraOpsMap:\n                  runtime = []\n                  sess.run(op[0]) #warm up\n                  for i in range(runs):\n                        startTime = time.time()\n                        sess.run(op[0])\n                        runtime += [time.time() - startTime]\n                  print(\"MAP: Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\n</code></pre>\n<p>Example Output:</p>\n<pre><code>Calculation using 1 threads took 0.0044 +- 0.0004\nCalculation using 10 threads took 0.0026 +- 0.0006\nCalculation using 50 threads took 0.0018 +- 0.0000\nCalculation using 100 threads took 0.0014 +- 0.0001\nWHILE: Calculation using 1 threads took 0.0063 +- 0.0001\nWHILE: Calculation using 10 threads took 0.0063 +- 0.0007\nWHILE: Calculation using 50 threads took 0.0063 +- 0.0001\nWHILE: Calculation using 100 threads took 0.0062 +- 0.0004\nMAP: Calculation using 1 threads took 0.0072 +- 0.0004\nMAP: Calculation using 10 threads took 0.0035 +- 0.0003\nMAP: Calculation using 50 threads took 0.0035 +- 0.0002\nMAP: Calculation using 100 threads took 0.0035 +- 0.0002\n\n</code></pre>\n<h3>System information</h3>\n<p>Ubuntu, Tensorflow installed using pip, version 1.3<br>\ntested on two different systems</p>", "body_text": "The parallel_iterations parameter of while_loop and map_fn do not reduce the runtime as expected.\nIn comparison to the same operations created in a python loop, while_loop and map_fn are at least 4 times slower. while_loop does not scale at all\nMore details:\nFor my project, I need to calculate a matrix blockwise.\nI execute the calculation in a loop, each loop iteration working on a small part of the matrix, not accessing the rest of the matrix.\nTo achieve maximum performance, the loop has to be executed in parallel.\nIn addition, I need to control the number of parallel threads because of memory limitations.\nI tried to use parallel_iterations, but changing this values doesn't change the runtime at all.\nUsing a python-for loop to create the same number of operations and syncing them with control_dependencies is around 4 times faster.\nIn contrast to while_loop, map_fn is getting faster with bigger parallel_iterations but remains noticeable slower then the python creation.\nIf this issue is known (didn't find anything at stackoverflow or here) and not resolvable,\nthe documentation of while_loop needs to be improved heavily - to avoid this operation if the loop should be parallized.\nBenchmark-Code:\nimport tensorflow as tf\nimport time\n\nif __name__ == '__main__':\n      runs = 100\n      N = 100\n      M = 30\n\n      KernelRow = tf.Variable(tf.zeros([M, N*M]), name='KernelRow')\n\n      def calEntry(KernelVariable, pos):\n            op = tf.assign(KernelVariable[:, pos*M:(pos+1)*M], \n                  tf.eye(M) * 123. + tf.exp(tf.random_uniform([M, M]))) #perform some operations \n            with tf.control_dependencies([op]):\n                  return tf.constant(0)\n\n      def calRow(KernelVariable, N, parallelNum):\n            ops = []\n            for ic in range(0, N, parallelNum):\n                  with tf.control_dependencies(ops):\n                        for j in range(parallelNum):\n                              if ic + j < N:\n                                    ops += [calEntry(KernelVariable, ic+j)]\n                        \n            with tf.control_dependencies(ops):\n                  return tf.identity(KernelVariable)\n\n      def calRow_while(KernelVariable, N, parallelNum):\n            i = tf.constant(0)\n            condition = lambda i: tf.less(i, N)\n            def body(ic):\n                  updateKernel = calEntry(KernelVariable, ic)\n                  with tf.control_dependencies([updateKernel]):\n                        return ic + 1\n            return tf.while_loop(condition, body, [i], back_prop=False, parallel_iterations=parallelNum)\n            \n      def calRow_map(KernelVariable, N, parallelNum):\n            f = lambda x: calEntry(KernelVariable, x)\n            return tf.map_fn(f, tf.range(N, dtype=tf.int32), parallel_iterations=parallelNum)\n\n      paraOps = []\n      paraOpsWhile = []\n      paraOpsMap = []\n      for paraEntry in [1, 10, 50, 100]:\n            paraOps += [(calRow(KernelRow, N, paraEntry), paraEntry )]\n            paraOpsWhile += [(calRow_while(KernelRow, N, paraEntry), paraEntry )]\n            paraOpsMap += [(calRow_map(KernelRow, N, paraEntry), paraEntry )]\n\n      init = tf.global_variables_initializer()\n      #perform calculation\n      with tf.Session() as sess:\n            sess.run(init)\n            for op in paraOps:\n                  runtime = []\n                  sess.run(op[0]) #warm up\n                  for i in range(runs):\n                        startTime = time.time()\n                        sess.run(op[0])\n                        runtime += [time.time() - startTime]\n                  print(\"Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\n            \n            for op in paraOpsWhile:\n                  runtime = []\n                  sess.run(op[0]) #warm up\n                  for i in range(runs):\n                        startTime = time.time()\n                        sess.run(op[0])\n                        runtime += [time.time() - startTime]\n                  print(\"WHILE: Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\n                \n            for op in paraOpsMap:\n                  runtime = []\n                  sess.run(op[0]) #warm up\n                  for i in range(runs):\n                        startTime = time.time()\n                        sess.run(op[0])\n                        runtime += [time.time() - startTime]\n                  print(\"MAP: Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\n\nExample Output:\nCalculation using 1 threads took 0.0044 +- 0.0004\nCalculation using 10 threads took 0.0026 +- 0.0006\nCalculation using 50 threads took 0.0018 +- 0.0000\nCalculation using 100 threads took 0.0014 +- 0.0001\nWHILE: Calculation using 1 threads took 0.0063 +- 0.0001\nWHILE: Calculation using 10 threads took 0.0063 +- 0.0007\nWHILE: Calculation using 50 threads took 0.0063 +- 0.0001\nWHILE: Calculation using 100 threads took 0.0062 +- 0.0004\nMAP: Calculation using 1 threads took 0.0072 +- 0.0004\nMAP: Calculation using 10 threads took 0.0035 +- 0.0003\nMAP: Calculation using 50 threads took 0.0035 +- 0.0002\nMAP: Calculation using 100 threads took 0.0035 +- 0.0002\n\n\nSystem information\nUbuntu, Tensorflow installed using pip, version 1.3\ntested on two different systems", "body": "The `parallel_iterations` parameter of `while_loop` and `map_fn` do not reduce the runtime as expected.\r\nIn comparison to the same operations created in a python loop, `while_loop` and `map_fn` are at least 4 times slower. `while_loop` does not scale at all\r\n\r\nMore details:\r\nFor my project, I need to calculate a matrix blockwise. \r\nI execute the calculation in a loop, each loop iteration working on a small part of the matrix, not accessing the rest of the matrix.\r\nTo achieve maximum performance, the loop has to be executed in parallel.\r\nIn addition, I need to control the number of parallel threads because of memory limitations.\r\nI tried to use `parallel_iterations`, but changing this values doesn't change the runtime at all.\r\n\r\nUsing a python-for loop to create the same number of operations and syncing them with `control_dependencies` is around 4 times faster.\r\nIn contrast to `while_loop`, `map_fn` is getting faster with bigger `parallel_iterations` but remains noticeable slower then the python creation.\r\nIf this issue is known (didn't find anything at stackoverflow or here) and not resolvable, \r\nthe documentation of while_loop needs to be improved heavily - to avoid this operation if the loop should be parallized.\r\n\r\nBenchmark-Code:\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\nif __name__ == '__main__':\r\n      runs = 100\r\n      N = 100\r\n      M = 30\r\n\r\n      KernelRow = tf.Variable(tf.zeros([M, N*M]), name='KernelRow')\r\n\r\n      def calEntry(KernelVariable, pos):\r\n            op = tf.assign(KernelVariable[:, pos*M:(pos+1)*M], \r\n                  tf.eye(M) * 123. + tf.exp(tf.random_uniform([M, M]))) #perform some operations \r\n            with tf.control_dependencies([op]):\r\n                  return tf.constant(0)\r\n\r\n      def calRow(KernelVariable, N, parallelNum):\r\n            ops = []\r\n            for ic in range(0, N, parallelNum):\r\n                  with tf.control_dependencies(ops):\r\n                        for j in range(parallelNum):\r\n                              if ic + j < N:\r\n                                    ops += [calEntry(KernelVariable, ic+j)]\r\n                        \r\n            with tf.control_dependencies(ops):\r\n                  return tf.identity(KernelVariable)\r\n\r\n      def calRow_while(KernelVariable, N, parallelNum):\r\n            i = tf.constant(0)\r\n            condition = lambda i: tf.less(i, N)\r\n            def body(ic):\r\n                  updateKernel = calEntry(KernelVariable, ic)\r\n                  with tf.control_dependencies([updateKernel]):\r\n                        return ic + 1\r\n            return tf.while_loop(condition, body, [i], back_prop=False, parallel_iterations=parallelNum)\r\n            \r\n      def calRow_map(KernelVariable, N, parallelNum):\r\n            f = lambda x: calEntry(KernelVariable, x)\r\n            return tf.map_fn(f, tf.range(N, dtype=tf.int32), parallel_iterations=parallelNum)\r\n\r\n      paraOps = []\r\n      paraOpsWhile = []\r\n      paraOpsMap = []\r\n      for paraEntry in [1, 10, 50, 100]:\r\n            paraOps += [(calRow(KernelRow, N, paraEntry), paraEntry )]\r\n            paraOpsWhile += [(calRow_while(KernelRow, N, paraEntry), paraEntry )]\r\n            paraOpsMap += [(calRow_map(KernelRow, N, paraEntry), paraEntry )]\r\n\r\n      init = tf.global_variables_initializer()\r\n      #perform calculation\r\n      with tf.Session() as sess:\r\n            sess.run(init)\r\n            for op in paraOps:\r\n                  runtime = []\r\n                  sess.run(op[0]) #warm up\r\n                  for i in range(runs):\r\n                        startTime = time.time()\r\n                        sess.run(op[0])\r\n                        runtime += [time.time() - startTime]\r\n                  print(\"Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\r\n            \r\n            for op in paraOpsWhile:\r\n                  runtime = []\r\n                  sess.run(op[0]) #warm up\r\n                  for i in range(runs):\r\n                        startTime = time.time()\r\n                        sess.run(op[0])\r\n                        runtime += [time.time() - startTime]\r\n                  print(\"WHILE: Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\r\n                \r\n            for op in paraOpsMap:\r\n                  runtime = []\r\n                  sess.run(op[0]) #warm up\r\n                  for i in range(runs):\r\n                        startTime = time.time()\r\n                        sess.run(op[0])\r\n                        runtime += [time.time() - startTime]\r\n                  print(\"MAP: Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\r\n```\r\n Example Output:\r\n\r\n```\r\nCalculation using 1 threads took 0.0044 +- 0.0004\r\nCalculation using 10 threads took 0.0026 +- 0.0006\r\nCalculation using 50 threads took 0.0018 +- 0.0000\r\nCalculation using 100 threads took 0.0014 +- 0.0001\r\nWHILE: Calculation using 1 threads took 0.0063 +- 0.0001\r\nWHILE: Calculation using 10 threads took 0.0063 +- 0.0007\r\nWHILE: Calculation using 50 threads took 0.0063 +- 0.0001\r\nWHILE: Calculation using 100 threads took 0.0062 +- 0.0004\r\nMAP: Calculation using 1 threads took 0.0072 +- 0.0004\r\nMAP: Calculation using 10 threads took 0.0035 +- 0.0003\r\nMAP: Calculation using 50 threads took 0.0035 +- 0.0002\r\nMAP: Calculation using 100 threads took 0.0035 +- 0.0002\r\n\r\n```\r\n\r\n \r\n\r\n\r\n### System information\r\nUbuntu, Tensorflow installed using pip, version 1.3\r\ntested on two different systems"}