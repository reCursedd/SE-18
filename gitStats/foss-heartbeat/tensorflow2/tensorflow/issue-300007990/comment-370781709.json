{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/370781709", "html_url": "https://github.com/tensorflow/tensorflow/issues/17252#issuecomment-370781709", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17252", "id": 370781709, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDc4MTcwOQ==", "user": {"login": "RT-TL", "id": 13029793, "node_id": "MDQ6VXNlcjEzMDI5Nzkz", "avatar_url": "https://avatars1.githubusercontent.com/u/13029793?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RT-TL", "html_url": "https://github.com/RT-TL", "followers_url": "https://api.github.com/users/RT-TL/followers", "following_url": "https://api.github.com/users/RT-TL/following{/other_user}", "gists_url": "https://api.github.com/users/RT-TL/gists{/gist_id}", "starred_url": "https://api.github.com/users/RT-TL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RT-TL/subscriptions", "organizations_url": "https://api.github.com/users/RT-TL/orgs", "repos_url": "https://api.github.com/users/RT-TL/repos", "events_url": "https://api.github.com/users/RT-TL/events{/privacy}", "received_events_url": "https://api.github.com/users/RT-TL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-06T13:31:20Z", "updated_at": "2018-03-06T13:31:20Z", "author_association": "NONE", "body_html": "<p>A post on Stackoverflow helped me fix this. Switching from Python 2.7.12 to Python 3.4.6 removes the issue. So perhaps this is not a tensorflow issue - but it probably deserves a note in the docs because numeric instability is in general a common problem.</p>\n<p><strong>Details on the issue</strong></p>\n<p>It turns out the error originated mostly from the calculation of the softmax activation layer, it sometimes occurred later during cross entropy calculation. I used <code>tf.losses.sparse_softmax_cross_entropy</code> to calculate the loss.</p>\n<p>Tensorflow uses some normalization in order to produce inf values due to large exponents in the softmax calculation by replacing e^x with e^(x-c) where c = max(0,x). Usually this should mitigate the issue and it obviously works in Python 3.</p>\n<p>Let me know if I can provide any additional info. Luckily for me: issue solved :-)</p>", "body_text": "A post on Stackoverflow helped me fix this. Switching from Python 2.7.12 to Python 3.4.6 removes the issue. So perhaps this is not a tensorflow issue - but it probably deserves a note in the docs because numeric instability is in general a common problem.\nDetails on the issue\nIt turns out the error originated mostly from the calculation of the softmax activation layer, it sometimes occurred later during cross entropy calculation. I used tf.losses.sparse_softmax_cross_entropy to calculate the loss.\nTensorflow uses some normalization in order to produce inf values due to large exponents in the softmax calculation by replacing e^x with e^(x-c) where c = max(0,x). Usually this should mitigate the issue and it obviously works in Python 3.\nLet me know if I can provide any additional info. Luckily for me: issue solved :-)", "body": "A post on Stackoverflow helped me fix this. Switching from Python 2.7.12 to Python 3.4.6 removes the issue. So perhaps this is not a tensorflow issue - but it probably deserves a note in the docs because numeric instability is in general a common problem.\r\n\r\n**Details on the issue**\r\n\r\nIt turns out the error originated mostly from the calculation of the softmax activation layer, it sometimes occurred later during cross entropy calculation. I used `tf.losses.sparse_softmax_cross_entropy` to calculate the loss.\r\n\r\nTensorflow uses some normalization in order to produce inf values due to large exponents in the softmax calculation by replacing e^x with e^(x-c) where c = max(0,x). Usually this should mitigate the issue and it obviously works in Python 3. \r\n\r\nLet me know if I can provide any additional info. Luckily for me: issue solved :-)"}