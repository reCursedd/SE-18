{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/282862063", "html_url": "https://github.com/tensorflow/tensorflow/issues/7841#issuecomment-282862063", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7841", "id": 282862063, "node_id": "MDEyOklzc3VlQ29tbWVudDI4Mjg2MjA2Mw==", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-27T21:36:54Z", "updated_at": "2017-02-27T21:36:54Z", "author_association": "MEMBER", "body_html": "<p>Your last three lines here</p>\n<pre><code>list of tags to a scalar summary op is no longer supported. WARNING:tensorflow:Skipping\nsummary for global_step, must be a float or np.float32. {'loss': 1.209928e-08, 'global_step': 1000} ENDING\n</code></pre>\n<p>have</p>\n<pre><code>{'loss': 1.209928e-08, 'global_step': 1000} \n</code></pre>\n<p>So it did work. the loss became effectively zero (1e-8 is about zero for the purposes of this discussion). The loss changes because the examples are ordered randomly and the weights are initialized randomly. That means that the exact numeric values are not deterministic (this is inherent in stochastic gradient descent learning approaches).</p>\n<p>TensorFlow is verbose on startup so it is hard to see the result, but it is there.</p>", "body_text": "Your last three lines here\nlist of tags to a scalar summary op is no longer supported. WARNING:tensorflow:Skipping\nsummary for global_step, must be a float or np.float32. {'loss': 1.209928e-08, 'global_step': 1000} ENDING\n\nhave\n{'loss': 1.209928e-08, 'global_step': 1000} \n\nSo it did work. the loss became effectively zero (1e-8 is about zero for the purposes of this discussion). The loss changes because the examples are ordered randomly and the weights are initialized randomly. That means that the exact numeric values are not deterministic (this is inherent in stochastic gradient descent learning approaches).\nTensorFlow is verbose on startup so it is hard to see the result, but it is there.", "body": "Your last three lines here\r\n```\r\nlist of tags to a scalar summary op is no longer supported. WARNING:tensorflow:Skipping\r\nsummary for global_step, must be a float or np.float32. {'loss': 1.209928e-08, 'global_step': 1000} ENDING\r\n```\r\nhave \r\n```\r\n{'loss': 1.209928e-08, 'global_step': 1000} \r\n```\r\nSo it did work. the loss became effectively zero (1e-8 is about zero for the purposes of this discussion). The loss changes because the examples are ordered randomly and the weights are initialized randomly. That means that the exact numeric values are not deterministic (this is inherent in stochastic gradient descent learning approaches).\r\n\r\nTensorFlow is verbose on startup so it is hard to see the result, but it is there.\r\n"}