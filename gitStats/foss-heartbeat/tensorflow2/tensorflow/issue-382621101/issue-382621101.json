{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23878", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23878/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23878/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23878/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23878", "id": 382621101, "node_id": "MDU6SXNzdWUzODI2MjExMDE=", "number": 23878, "title": "Bug: instantiating dynamic_rnn with tf.int32 in input and state raises TypeError", "user": {"login": "javiermas", "id": 22636546, "node_id": "MDQ6VXNlcjIyNjM2NTQ2", "avatar_url": "https://avatars2.githubusercontent.com/u/22636546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/javiermas", "html_url": "https://github.com/javiermas", "followers_url": "https://api.github.com/users/javiermas/followers", "following_url": "https://api.github.com/users/javiermas/following{/other_user}", "gists_url": "https://api.github.com/users/javiermas/gists{/gist_id}", "starred_url": "https://api.github.com/users/javiermas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/javiermas/subscriptions", "organizations_url": "https://api.github.com/users/javiermas/orgs", "repos_url": "https://api.github.com/users/javiermas/repos", "events_url": "https://api.github.com/users/javiermas/events{/privacy}", "received_events_url": "https://api.github.com/users/javiermas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-20T11:31:19Z", "updated_at": "2018-11-20T11:31:19Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>OS Platform: OS X 10.13.3</li>\n<li>Custom code</li>\n<li>tensorflow version: 1.12.0</li>\n<li>python version: 3.6.5</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nTensorflow raises a TypeError when creating a dynamic_rnn with tf.int32 type in its input and state. When changing the type to tf.float32 the error is not raised.</p>\n<p><strong>Describe the expected behavior</strong><br>\nIdeally, a dynamic_rnn should support tf.in32 types. If there's any reason why instantiating a dynamic_rnn with tf.int32 type in its input and state should not be allowed, a custom error should be raised.</p>\n<p><strong>Code to reproduce the issue</strong><br>\nThe code below reproduces the error:</p>\n<pre><code>import tensorflow as tf\n\nX = tf.placeholder(tf.int32, [None, 10, 1])\ncell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)\noutput, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)\n\n</code></pre>\n<p>The code below doesn't:</p>\n<pre><code>\nimport tensorflow as tf\n\nX = tf.placeholder(tf.float32, [None, 10, 1])\ncell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.float32)\noutput, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32)\n\n</code></pre>\n<p>Note the change in dtype.</p>\n<h2><strong>Other info / logs</strong><br>\nTRACEBACK:</h2>\n<p>TypeError                                 Traceback (most recent call last)<br>\n in ()<br>\n2 X = tf.placeholder(tf.int32, [None, 10, 1])<br>\n3 cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)<br>\n----&gt; 4 output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)#, initial_state=state)<br>\n5</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)<br>\n662         swap_memory=swap_memory,<br>\n663         sequence_length=sequence_length,<br>\n--&gt; 664         dtype=dtype)<br>\n665<br>\n666     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)<br>\n870       parallel_iterations=parallel_iterations,<br>\n871       maximum_iterations=time_steps,<br>\n--&gt; 872       swap_memory=swap_memory)<br>\n873<br>\n874   # Unpack final output if not using output tuples.</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)<br>\n3289       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)<br>\n3290     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,<br>\n-&gt; 3291                                     return_same_structure)<br>\n3292     if maximum_iterations is not None:<br>\n3293       return result[1]</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)<br>\n3002       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access<br>\n3003         original_body_result, exit_vars = self._BuildLoop(<br>\n-&gt; 3004             pred, body, original_loop_vars, loop_vars, shape_invariants)<br>\n3005     finally:<br>\n3006       self.Exit()</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)<br>\n2937         flat_sequence=vars_for_body_with_tensor_arrays)<br>\n2938     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access<br>\n-&gt; 2939     body_result = body(*packed_vars_for_body)<br>\n2940     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access<br>\n2941     if not nest.is_sequence(body_result):</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in (i, lv)<br>\n3258         cond = lambda i, lv: (  # pylint: disable=g-long-lambda<br>\n3259             math_ops.logical_and(i &lt; maximum_iterations, orig_cond(*lv)))<br>\n-&gt; 3260         body = lambda i, lv: (i + 1, orig_body(*lv))<br>\n3261<br>\n3262     if context.executing_eagerly():</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _time_step(time, output_ta_t, state)<br>\n838           skip_conditionals=True)<br>\n839     else:<br>\n--&gt; 840       (output, new_state) = call_cell()<br>\n841<br>\n842     # Keras cells always wrap state as list, even if it's a single tensor.</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in ()<br>\n824     if is_keras_rnn_cell and not nest.is_sequence(state):<br>\n825       state = [state]<br>\n--&gt; 826     call_cell = lambda: cell(input_t, state)<br>\n827<br>\n828     if sequence_length is not None:</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in <strong>call</strong>(self, inputs, state, scope, *args, **kwargs)<br>\n368     # method.  See the class docstring for more details.<br>\n369     return base_layer.Layer.<strong>call</strong>(self, inputs, state, scope=scope,<br>\n--&gt; 370                                      *args, **kwargs)<br>\n371<br>\n372</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in <strong>call</strong>(self, inputs, *args, **kwargs)<br>\n372<br>\n373       # Actually call layer<br>\n--&gt; 374       outputs = super(Layer, self).<strong>call</strong>(inputs, *args, **kwargs)<br>\n375<br>\n376     if not context.executing_eagerly():</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in <strong>call</strong>(self, inputs, *args, **kwargs)<br>\n755       if not in_deferred_mode:<br>\n756         self._in_call = True<br>\n--&gt; 757         outputs = self.call(inputs, *args, **kwargs)<br>\n758         self._in_call = False<br>\n759         if outputs is None:</p>\n<p>~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)<br>\n1003            sigmoid(i + self._w_i_diag * c_prev) * self._activation(j))<br>\n1004     else:<br>\n-&gt; 1005       c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *<br>\n1006            self._activation(j))<br>\n1007</p>\n<p>TypeError: unsupported operand type(s) for +: 'Tensor' and 'float'</p>", "body_text": "System information\n\nOS Platform: OS X 10.13.3\nCustom code\ntensorflow version: 1.12.0\npython version: 3.6.5\n\nDescribe the current behavior\nTensorflow raises a TypeError when creating a dynamic_rnn with tf.int32 type in its input and state. When changing the type to tf.float32 the error is not raised.\nDescribe the expected behavior\nIdeally, a dynamic_rnn should support tf.in32 types. If there's any reason why instantiating a dynamic_rnn with tf.int32 type in its input and state should not be allowed, a custom error should be raised.\nCode to reproduce the issue\nThe code below reproduces the error:\nimport tensorflow as tf\n\nX = tf.placeholder(tf.int32, [None, 10, 1])\ncell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)\noutput, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)\n\n\nThe code below doesn't:\n\nimport tensorflow as tf\n\nX = tf.placeholder(tf.float32, [None, 10, 1])\ncell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.float32)\noutput, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32)\n\n\nNote the change in dtype.\nOther info / logs\nTRACEBACK:\nTypeError                                 Traceback (most recent call last)\n in ()\n2 X = tf.placeholder(tf.int32, [None, 10, 1])\n3 cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)\n----> 4 output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)#, initial_state=state)\n5\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\n662         swap_memory=swap_memory,\n663         sequence_length=sequence_length,\n--> 664         dtype=dtype)\n665\n666     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\n870       parallel_iterations=parallel_iterations,\n871       maximum_iterations=time_steps,\n--> 872       swap_memory=swap_memory)\n873\n874   # Unpack final output if not using output tuples.\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\n3289       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\n3290     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n-> 3291                                     return_same_structure)\n3292     if maximum_iterations is not None:\n3293       return result[1]\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)\n3002       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access\n3003         original_body_result, exit_vars = self._BuildLoop(\n-> 3004             pred, body, original_loop_vars, loop_vars, shape_invariants)\n3005     finally:\n3006       self.Exit()\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n2937         flat_sequence=vars_for_body_with_tensor_arrays)\n2938     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\n-> 2939     body_result = body(*packed_vars_for_body)\n2940     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\n2941     if not nest.is_sequence(body_result):\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in (i, lv)\n3258         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n3259             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n-> 3260         body = lambda i, lv: (i + 1, orig_body(*lv))\n3261\n3262     if context.executing_eagerly():\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _time_step(time, output_ta_t, state)\n838           skip_conditionals=True)\n839     else:\n--> 840       (output, new_state) = call_cell()\n841\n842     # Keras cells always wrap state as list, even if it's a single tensor.\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in ()\n824     if is_keras_rnn_cell and not nest.is_sequence(state):\n825       state = [state]\n--> 826     call_cell = lambda: cell(input_t, state)\n827\n828     if sequence_length is not None:\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state, scope, *args, **kwargs)\n368     # method.  See the class docstring for more details.\n369     return base_layer.Layer.call(self, inputs, state, scope=scope,\n--> 370                                      *args, **kwargs)\n371\n372\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in call(self, inputs, *args, **kwargs)\n372\n373       # Actually call layer\n--> 374       outputs = super(Layer, self).call(inputs, *args, **kwargs)\n375\n376     if not context.executing_eagerly():\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in call(self, inputs, *args, **kwargs)\n755       if not in_deferred_mode:\n756         self._in_call = True\n--> 757         outputs = self.call(inputs, *args, **kwargs)\n758         self._in_call = False\n759         if outputs is None:\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\n1003            sigmoid(i + self._w_i_diag * c_prev) * self._activation(j))\n1004     else:\n-> 1005       c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *\n1006            self._activation(j))\n1007\nTypeError: unsupported operand type(s) for +: 'Tensor' and 'float'", "body": "**System information**\r\n- OS Platform: OS X 10.13.3\r\n- Custom code\r\n- tensorflow version: 1.12.0\r\n- python version: 3.6.5\r\n\r\n**Describe the current behavior**\r\nTensorflow raises a TypeError when creating a dynamic_rnn with tf.int32 type in its input and state. When changing the type to tf.float32 the error is not raised.\r\n\r\n**Describe the expected behavior**\r\nIdeally, a dynamic_rnn should support tf.in32 types. If there's any reason why instantiating a dynamic_rnn with tf.int32 type in its input and state should not be allowed, a custom error should be raised.  \r\n\r\n**Code to reproduce the issue**\r\nThe code below reproduces the error:\r\n```\r\nimport tensorflow as tf\r\n\r\nX = tf.placeholder(tf.int32, [None, 10, 1])\r\ncell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)\r\noutput, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)\r\n\r\n```\r\nThe code below doesn't:\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nX = tf.placeholder(tf.float32, [None, 10, 1])\r\ncell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.float32)\r\noutput, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32)\r\n\r\n```\r\n\r\nNote the change in dtype.\r\n\r\n**Other info / logs**\r\nTRACEBACK:\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-1d83a30f7748> in <module>()\r\n      2 X = tf.placeholder(tf.int32, [None, 10, 1])\r\n      3 cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)\r\n----> 4 output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)#, initial_state=state)\r\n      5 \r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    662         swap_memory=swap_memory,\r\n    663         sequence_length=sequence_length,\r\n--> 664         dtype=dtype)\r\n    665 \r\n    666     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\r\n    870       parallel_iterations=parallel_iterations,\r\n    871       maximum_iterations=time_steps,\r\n--> 872       swap_memory=swap_memory)\r\n    873 \r\n    874   # Unpack final output if not using output tuples.\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\r\n   3289       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n   3290     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\r\n-> 3291                                     return_same_structure)\r\n   3292     if maximum_iterations is not None:\r\n   3293       return result[1]\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)\r\n   3002       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access\r\n   3003         original_body_result, exit_vars = self._BuildLoop(\r\n-> 3004             pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   3005     finally:\r\n   3006       self.Exit()\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2937         flat_sequence=vars_for_body_with_tensor_arrays)\r\n   2938     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n-> 2939     body_result = body(*packed_vars_for_body)\r\n   2940     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n   2941     if not nest.is_sequence(body_result):\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in <lambda>(i, lv)\r\n   3258         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\r\n   3259             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\r\n-> 3260         body = lambda i, lv: (i + 1, orig_body(*lv))\r\n   3261 \r\n   3262     if context.executing_eagerly():\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _time_step(time, output_ta_t, state)\r\n    838           skip_conditionals=True)\r\n    839     else:\r\n--> 840       (output, new_state) = call_cell()\r\n    841 \r\n    842     # Keras cells always wrap state as list, even if it's a single tensor.\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in <lambda>()\r\n    824     if is_keras_rnn_cell and not nest.is_sequence(state):\r\n    825       state = [state]\r\n--> 826     call_cell = lambda: cell(input_t, state)\r\n    827 \r\n    828     if sequence_length is not None:\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope, *args, **kwargs)\r\n    368     # method.  See the class docstring for more details.\r\n    369     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\r\n--> 370                                      *args, **kwargs)\r\n    371 \r\n    372 \r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    372 \r\n    373       # Actually call layer\r\n--> 374       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n    375 \r\n    376     if not context.executing_eagerly():\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    755       if not in_deferred_mode:\r\n    756         self._in_call = True\r\n--> 757         outputs = self.call(inputs, *args, **kwargs)\r\n    758         self._in_call = False\r\n    759         if outputs is None:\r\n\r\n~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\r\n   1003            sigmoid(i + self._w_i_diag * c_prev) * self._activation(j))\r\n   1004     else:\r\n-> 1005       c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *\r\n   1006            self._activation(j))\r\n   1007 \r\n\r\nTypeError: unsupported operand type(s) for +: 'Tensor' and 'float'"}