{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271464252", "html_url": "https://github.com/tensorflow/tensorflow/issues/6716#issuecomment-271464252", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6716", "id": 271464252, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTQ2NDI1Mg==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-10T01:53:53Z", "updated_at": "2017-01-10T01:54:31Z", "author_association": "MEMBER", "body_html": "<p>As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5376757\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/michaelisard\">@michaelisard</a>  says, the Timeline class <em>does</em> have some code in to try and track the refcounts of Tensors using information in the trace, but it is very unreliable since the only information it has access to is the string names of inputs to each operation (from the ASCII timeline label).  These aren't robust to graph partitioning, rewriting and constant folding etc., and this is why the memory histograms are disabled by default.</p>\n<p>Now that it is possible to retrieve the rewritten graphdefs, tf.Timeline could probably do a much better job of reference count tracking  - but as Michael says, it would be better to actually track the deallocations.  Unfortunately these can happen asynchronously at arbitrary times, so the protobuf format would need extending.   There is also the subtlety of GPU allocations and deallocations happening long before the CUDA kernels actually execute, which potentially makes the UI a little confusing. (not an issue for simply recording peak memory usage)</p>", "body_text": "As @michaelisard  says, the Timeline class does have some code in to try and track the refcounts of Tensors using information in the trace, but it is very unreliable since the only information it has access to is the string names of inputs to each operation (from the ASCII timeline label).  These aren't robust to graph partitioning, rewriting and constant folding etc., and this is why the memory histograms are disabled by default.\nNow that it is possible to retrieve the rewritten graphdefs, tf.Timeline could probably do a much better job of reference count tracking  - but as Michael says, it would be better to actually track the deallocations.  Unfortunately these can happen asynchronously at arbitrary times, so the protobuf format would need extending.   There is also the subtlety of GPU allocations and deallocations happening long before the CUDA kernels actually execute, which potentially makes the UI a little confusing. (not an issue for simply recording peak memory usage)", "body": "As @michaelisard  says, the Timeline class _does_ have some code in to try and track the refcounts of Tensors using information in the trace, but it is very unreliable since the only information it has access to is the string names of inputs to each operation (from the ASCII timeline label).  These aren't robust to graph partitioning, rewriting and constant folding etc., and this is why the memory histograms are disabled by default.\r\n\r\nNow that it is possible to retrieve the rewritten graphdefs, tf.Timeline could probably do a much better job of reference count tracking  - but as Michael says, it would be better to actually track the deallocations.  Unfortunately these can happen asynchronously at arbitrary times, so the protobuf format would need extending.   There is also the subtlety of GPU allocations and deallocations happening long before the CUDA kernels actually execute, which potentially makes the UI a little confusing. (not an issue for simply recording peak memory usage)"}