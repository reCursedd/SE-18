{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275554632", "html_url": "https://github.com/tensorflow/tensorflow/issues/6716#issuecomment-275554632", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6716", "id": 275554632, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTU1NDYzMg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-27T00:14:07Z", "updated_at": "2017-01-27T00:15:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I was seeing those \"orphan\" deallocations in <a href=\"https://github.com/tensorflow/models/tree/master/resnet\">https://github.com/tensorflow/models/tree/master/resnet</a> which uses queues...although <code>cuda_host_bfc</code> seems to rule that out, queues should be using CPU</p>\n<p>To summarize, I think any change that would make memory timelines easier to build would be welcome. My current vlog memory parsing logic seems to mostly work, but there are occasionally missing bits (ie, because of missing step_id in deallocation), and it breaks when vlog output gets too large (ie, vlog=2 doesn't work because of gazillion GPU polling messages), or when protobuf text output changes, and it relies on order of stderr to be correct (we can only get second timestamps in logs, no microsecond timestamps)</p>", "body_text": "I was seeing those \"orphan\" deallocations in https://github.com/tensorflow/models/tree/master/resnet which uses queues...although cuda_host_bfc seems to rule that out, queues should be using CPU\nTo summarize, I think any change that would make memory timelines easier to build would be welcome. My current vlog memory parsing logic seems to mostly work, but there are occasionally missing bits (ie, because of missing step_id in deallocation), and it breaks when vlog output gets too large (ie, vlog=2 doesn't work because of gazillion GPU polling messages), or when protobuf text output changes, and it relies on order of stderr to be correct (we can only get second timestamps in logs, no microsecond timestamps)", "body": "I was seeing those \"orphan\" deallocations in https://github.com/tensorflow/models/tree/master/resnet which uses queues...although `cuda_host_bfc` seems to rule that out, queues should be using CPU \r\n\r\nTo summarize, I think any change that would make memory timelines easier to build would be welcome. My current vlog memory parsing logic seems to mostly work, but there are occasionally missing bits (ie, because of missing step_id in deallocation), and it breaks when vlog output gets too large (ie, vlog=2 doesn't work because of gazillion GPU polling messages), or when protobuf text output changes, and it relies on order of stderr to be correct (we can only get second timestamps in logs, no microsecond timestamps)"}