{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275551980", "html_url": "https://github.com/tensorflow/tensorflow/issues/6716#issuecomment-275551980", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6716", "id": 275551980, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTU1MTk4MA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-26T23:57:39Z", "updated_at": "2017-01-26T23:57:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So the problem I'm trying to solve is that we have models that use too much memory, and in order to have confidence that our optimizations are working, we want to be able to track down \"MemoryExceeded\" error to a particular sequence of allocations and deallocations.</p>\n<p>For your proposal, my question would be how robust it is. What do you mean mean by \"hopefully it's rare\"? If it's pretty rare, then I think it would simplify/robustify things if I could build the memory usage timeline using step-metadata instead of parsing stderr.</p>\n<p>BTW, another thing I came across with LOG_MEMORY messages is that sometimes I get deallocation messages even when there's no mention of this allocation_id in this run call. Can things be de-allocated in a run call if they weren't allocated in the same session.run call?</p>\n<p>IE, message like this</p>\n<p><code>    41:2017-01-26 15:06:29: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 29330 allocator_name: \"cuda_host_bfc\" }</code></p>", "body_text": "So the problem I'm trying to solve is that we have models that use too much memory, and in order to have confidence that our optimizations are working, we want to be able to track down \"MemoryExceeded\" error to a particular sequence of allocations and deallocations.\nFor your proposal, my question would be how robust it is. What do you mean mean by \"hopefully it's rare\"? If it's pretty rare, then I think it would simplify/robustify things if I could build the memory usage timeline using step-metadata instead of parsing stderr.\nBTW, another thing I came across with LOG_MEMORY messages is that sometimes I get deallocation messages even when there's no mention of this allocation_id in this run call. Can things be de-allocated in a run call if they weren't allocated in the same session.run call?\nIE, message like this\n    41:2017-01-26 15:06:29: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 29330 allocator_name: \"cuda_host_bfc\" }", "body": "So the problem I'm trying to solve is that we have models that use too much memory, and in order to have confidence that our optimizations are working, we want to be able to track down \"MemoryExceeded\" error to a particular sequence of allocations and deallocations.\r\n\r\nFor your proposal, my question would be how robust it is. What do you mean mean by \"hopefully it's rare\"? If it's pretty rare, then I think it would simplify/robustify things if I could build the memory usage timeline using step-metadata instead of parsing stderr.\r\n\r\nBTW, another thing I came across with LOG_MEMORY messages is that sometimes I get deallocation messages even when there's no mention of this allocation_id in this run call. Can things be de-allocated in a run call if they weren't allocated in the same session.run call?\r\n\r\nIE, message like this\r\n\r\n`     41:2017-01-26 15:06:29: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 29330 allocator_name: \"cuda_host_bfc\" }\r\n`"}