{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/501", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/501/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/501/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/501/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/501", "id": 121943735, "node_id": "MDU6SXNzdWUxMjE5NDM3MzU=", "number": 501, "title": "assertion when running on GPU with debug enabled", "user": {"login": "bas-aarts", "id": 10091048, "node_id": "MDQ6VXNlcjEwMDkxMDQ4", "avatar_url": "https://avatars1.githubusercontent.com/u/10091048?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bas-aarts", "html_url": "https://github.com/bas-aarts", "followers_url": "https://api.github.com/users/bas-aarts/followers", "following_url": "https://api.github.com/users/bas-aarts/following{/other_user}", "gists_url": "https://api.github.com/users/bas-aarts/gists{/gist_id}", "starred_url": "https://api.github.com/users/bas-aarts/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bas-aarts/subscriptions", "organizations_url": "https://api.github.com/users/bas-aarts/orgs", "repos_url": "https://api.github.com/users/bas-aarts/repos", "events_url": "https://api.github.com/users/bas-aarts/events{/privacy}", "received_events_url": "https://api.github.com/users/bas-aarts/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2015-12-13T21:06:56Z", "updated_at": "2015-12-28T20:29:21Z", "closed_at": "2015-12-28T20:29:21Z", "author_association": "NONE", "body_html": "<p>when I compile TensorFlow with --config=cuda  -c dbg --strip=never, I get an assertion when running the mnist example (same goes for cifar10. Also tried CUDA 7.5, with same outcome. Reproduces with both TF 0.5.0 and TF 0.6.0.)</p>\n<p>The GPU being used is a Titan X.</p>\n<p>[~/tensorflow/tensorflow/models/image/mnist] python convolutional.py<br>\n......<br>\npython: third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceType.h:734: void Eigen::assertCudaOk(): Assertion `err == cudaSuccess' failed.<br>\nAborted (core dumped)</p>\n<p>dmesg shows that an illegal memory access was performed:<br>\n[780540.251853] NVRM: Xid (PCI:0000:03:00): 31, Ch 0000000f, engmask 00000101, intr 10000000</p>\n<p>any when running with cuda-memcheck, I get:<br>\n========= CUDA-MEMCHECK<br>\n========= Invalid <strong>global</strong> read of size 4<br>\n=========     at 0x000002f0 in /opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:346:_ZNK5Eigen15TensorEvaluatorIKNS_18TensorCwiseUnaryOpINS_8internal13scalar_log_opIfEEKNS_9TensorMapINS_6TensorIfLm2ELi1ElEELi1EEEEENS_9GpuDeviceEE6packetILi1EEE6float4l<br>\n=========     by thread (15,0,0) in block (0,0,0)<br>\n=========     Address 0x00000000 is out of bounds<br>\n=========     Device Frame:/opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvalTo.h:122:Eigen::TensorEvaluatorEigen::TensorEvalToOp&lt;Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_log_op&lt;float, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, unsigned long=2, int=1, long&gt;, int=1&gt; const &gt; const &gt; const , Eigen::GpuDevice(long)&gt;::evalPacket (Eigen::TensorEvaluatorEigen::TensorEvalToOp&lt;Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_log_op&lt;float, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, unsigned long=2, int=1, long&gt;, int=1&gt; const &gt; const &gt; const , Eigen::GpuDevice(long)&gt;::evalPacket : 0x350)<br>\n=========     Device Frame:/opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:407:void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator&lt;Eigen::TensorEvalToOp&lt;Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_log_op&lt;float, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, unsigned long=2, int=1, long&gt;, int=1&gt; const &gt; const &gt; const , Eigen::GpuDevice&gt;, long&gt;(float, Eigen::internal::scalar_log_op) (void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator&lt;Eigen::TensorEvalToOp&lt;Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_log_op&lt;float, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, unsigned long=2, int=1, long&gt;, int=1&gt; const &gt; const &gt; const , Eigen::GpuDevice&gt;, long&gt;(float, Eigen::internal::scalar_log_op) : 0x1460)<br>\n=========     Saved host backtrace up to driver entry point at kernel launch time<br>\n=========     Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so (cuLaunchKernel + 0x2cd) [0x15865d]<br>\n=========     Host Frame:/usr/local/cuda/lib64/libcudart.so.7.0 [0x131b0]<br>\n=========     Host Frame:/usr/local/cuda/lib64/libcudart.so.7.0 (cudaLaunch + 0x143) [0x2d653]<br>\n=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bc036]<br>\n=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bb2f8]<br>\n.....</p>\n<p>The assertion changes from run to run (non determinism due to multi threading), but the cause is always the NULL pointer dereference in float4 Eigen::TensorEvaluatorEigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_log_op&lt;float, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 2ul, 1, long&gt;, 1&gt; const&gt; const, Eigen::GpuDevice&gt;::packet&lt;1&gt;(long) const</p>\n<p>Could be an nvcc compiler bug  in debug mode only (kernel runs fine when optimized)</p>", "body_text": "when I compile TensorFlow with --config=cuda  -c dbg --strip=never, I get an assertion when running the mnist example (same goes for cifar10. Also tried CUDA 7.5, with same outcome. Reproduces with both TF 0.5.0 and TF 0.6.0.)\nThe GPU being used is a Titan X.\n[~/tensorflow/tensorflow/models/image/mnist] python convolutional.py\n......\npython: third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceType.h:734: void Eigen::assertCudaOk(): Assertion `err == cudaSuccess' failed.\nAborted (core dumped)\ndmesg shows that an illegal memory access was performed:\n[780540.251853] NVRM: Xid (PCI:0000:03:00): 31, Ch 0000000f, engmask 00000101, intr 10000000\nany when running with cuda-memcheck, I get:\n========= CUDA-MEMCHECK\n========= Invalid global read of size 4\n=========     at 0x000002f0 in /opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:346:_ZNK5Eigen15TensorEvaluatorIKNS_18TensorCwiseUnaryOpINS_8internal13scalar_log_opIfEEKNS_9TensorMapINS_6TensorIfLm2ELi1ElEELi1EEEEENS_9GpuDeviceEE6packetILi1EEE6float4l\n=========     by thread (15,0,0) in block (0,0,0)\n=========     Address 0x00000000 is out of bounds\n=========     Device Frame:/opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvalTo.h:122:Eigen::TensorEvaluatorEigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice(long)>::evalPacket (Eigen::TensorEvaluatorEigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice(long)>::evalPacket : 0x350)\n=========     Device Frame:/opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:407:void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator<Eigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice>, long>(float, Eigen::internal::scalar_log_op) (void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator<Eigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice>, long>(float, Eigen::internal::scalar_log_op) : 0x1460)\n=========     Saved host backtrace up to driver entry point at kernel launch time\n=========     Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so (cuLaunchKernel + 0x2cd) [0x15865d]\n=========     Host Frame:/usr/local/cuda/lib64/libcudart.so.7.0 [0x131b0]\n=========     Host Frame:/usr/local/cuda/lib64/libcudart.so.7.0 (cudaLaunch + 0x143) [0x2d653]\n=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bc036]\n=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bb2f8]\n.....\nThe assertion changes from run to run (non determinism due to multi threading), but the cause is always the NULL pointer dereference in float4 Eigen::TensorEvaluatorEigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, 2ul, 1, long>, 1> const> const, Eigen::GpuDevice>::packet<1>(long) const\nCould be an nvcc compiler bug  in debug mode only (kernel runs fine when optimized)", "body": "when I compile TensorFlow with --config=cuda  -c dbg --strip=never, I get an assertion when running the mnist example (same goes for cifar10. Also tried CUDA 7.5, with same outcome. Reproduces with both TF 0.5.0 and TF 0.6.0.)\n\nThe GPU being used is a Titan X.\n\n[~/tensorflow/tensorflow/models/image/mnist] python convolutional.py\n ......\npython: third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceType.h:734: void Eigen::assertCudaOk(): Assertion `err == cudaSuccess' failed.\nAborted (core dumped)\n\ndmesg shows that an illegal memory access was performed:\n[780540.251853] NVRM: Xid (PCI:0000:03:00): 31, Ch 0000000f, engmask 00000101, intr 10000000\n\nany when running with cuda-memcheck, I get:\n========= CUDA-MEMCHECK\n========= Invalid **global** read of size 4\n=========     at 0x000002f0 in /opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:346:_ZNK5Eigen15TensorEvaluatorIKNS_18TensorCwiseUnaryOpINS_8internal13scalar_log_opIfEEKNS_9TensorMapINS_6TensorIfLm2ELi1ElEELi1EEEEENS_9GpuDeviceEE6packetILi1EEE6float4l\n=========     by thread (15,0,0) in block (0,0,0)\n=========     Address 0x00000000 is out of bounds\n=========     Device Frame:/opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorEvalTo.h:122:Eigen::TensorEvaluatorEigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice(long)>::evalPacket (Eigen::TensorEvaluatorEigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice(long)>::evalPacket : 0x350)\n=========     Device Frame:/opt/bas/bazel/_bazel_bas/194e5d2548bb77b7040a7c94ff604a15/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:407:void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator<Eigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice>, long>(float, Eigen::internal::scalar_log_op<float>) (void Eigen::internal::EigenMetaKernel_VectorizableEigen::TensorEvaluator<Eigen::TensorEvalToOp<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, unsigned long=2, int=1, long>, int=1> const > const > const , Eigen::GpuDevice>, long>(float, Eigen::internal::scalar_log_op<float>) : 0x1460)\n=========     Saved host backtrace up to driver entry point at kernel launch time\n=========     Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so (cuLaunchKernel + 0x2cd) [0x15865d]\n=========     Host Frame:/usr/local/cuda/lib64/libcudart.so.7.0 [0x131b0]\n=========     Host Frame:/usr/local/cuda/lib64/libcudart.so.7.0 (cudaLaunch + 0x143) [0x2d653]\n=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bc036]\n=========     Host Frame:/home/users/bas/.python_packages/package1/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so [0xb4bb2f8]\n.....\n\nThe assertion changes from run to run (non determinism due to multi threading), but the cause is always the NULL pointer dereference in float4 Eigen::TensorEvaluatorEigen::TensorCwiseUnaryOp<Eigen::internal::scalar_log_op<float, Eigen::TensorMap<Eigen::Tensor<float, 2ul, 1, long>, 1> const> const, Eigen::GpuDevice>::packet<1>(long) const\n\nCould be an nvcc compiler bug  in debug mode only (kernel runs fine when optimized)\n"}