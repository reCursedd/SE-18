{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/143103419", "pull_request_review_id": 67566090, "id": 143103419, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MzEwMzQxOQ==", "diff_hunk": "@@ -1258,3 +1258,101 @@ def sampled_softmax_loss(weights,\n       labels=labels, logits=logits)\n   # sampled_losses is a [batch_size] tensor.\n   return sampled_losses\n+\n+\n+def sampled_sparse_softmax_loss(weights,\n+                                biases,\n+                                labels,\n+                                inputs,\n+                                num_sampled,\n+                                num_classes,\n+                                sampled_values=None,\n+                                remove_accidental_hits=True,\n+                                partition_strategy=\"mod\",\n+                                name=\"sampled_sparse_softmax_loss\"):\n+  \"\"\"Computes and returns the sampled sparse softmax training loss.\n+\n+  This is a faster way to train a softmax classifier over a huge number of\n+  classes.\n+\n+  This operation is for training only.  It is generally an underestimate of\n+  the full softmax loss.\n+\n+  A common use case is to use this method for training, and calculate the full\n+  softmax loss for evaluation or inference. In this case, you must set\n+  `partition_strategy=\"div\"` for the two losses to be consistent, as in the\n+  following example:\n+\n+  ```python\n+  if mode == \"train\":\n+    loss = tf.nn.sampled_sparse_softmax_loss(\n+        weights=weights,\n+        biases=biases,\n+        labels=labels,\n+        inputs=inputs,\n+        ...,\n+        partition_strategy=\"div\")\n+  elif mode == \"eval\":\n+    logits = tf.matmul(inputs, tf.transpose(weights))\n+    logits = tf.nn.bias_add(logits, biases)\n+    labels_one_hot = tf.one_hot(labels, n_classes)\n+    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n+        labels=labels_one_hot,\n+        logits=logits)\n+  ```\n+\n+  See our [Candidate Sampling Algorithms Reference]\n+  (https://www.tensorflow.org/extras/candidate_sampling.pdf)\n+\n+  Also see Section 3 of [Jean et al., 2014](http://arxiv.org/abs/1412.2007)\n+  ([pdf](http://arxiv.org/pdf/1412.2007.pdf)) for the math.\n+\n+  Args:\n+    weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`", "path": "tensorflow/python/ops/nn_impl.py", "position": null, "original_position": 54, "commit_id": "7ba5810c105640f218993d989142d7e91da6703e", "original_commit_id": "00343a48d39d9ff74ceb662c5140048295f2610a", "user": {"login": "TTrapper", "id": 9273021, "node_id": "MDQ6VXNlcjkyNzMwMjE=", "avatar_url": "https://avatars2.githubusercontent.com/u/9273021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TTrapper", "html_url": "https://github.com/TTrapper", "followers_url": "https://api.github.com/users/TTrapper/followers", "following_url": "https://api.github.com/users/TTrapper/following{/other_user}", "gists_url": "https://api.github.com/users/TTrapper/gists{/gist_id}", "starred_url": "https://api.github.com/users/TTrapper/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TTrapper/subscriptions", "organizations_url": "https://api.github.com/users/TTrapper/orgs", "repos_url": "https://api.github.com/users/TTrapper/repos", "events_url": "https://api.github.com/users/TTrapper/events{/privacy}", "received_events_url": "https://api.github.com/users/TTrapper/received_events", "type": "User", "site_admin": false}, "body": "Certainly a Variable, but this is the terminology used in the documentation of similar functions (eg `sampled_softmax_loss`). I take it that the term Tensor is in inclusive of variable Tensors? Let me know if you would still like it changed.", "created_at": "2017-10-06T03:27:56Z", "updated_at": "2017-11-14T17:48:45Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13453#discussion_r143103419", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13453", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/143103419"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13453#discussion_r143103419"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13453"}}, "body_html": "<p>Certainly a Variable, but this is the terminology used in the documentation of similar functions (eg <code>sampled_softmax_loss</code>). I take it that the term Tensor is in inclusive of variable Tensors? Let me know if you would still like it changed.</p>", "body_text": "Certainly a Variable, but this is the terminology used in the documentation of similar functions (eg sampled_softmax_loss). I take it that the term Tensor is in inclusive of variable Tensors? Let me know if you would still like it changed.", "in_reply_to_id": 142850064}