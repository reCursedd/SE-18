{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/301552521", "html_url": "https://github.com/tensorflow/tensorflow/issues/9091#issuecomment-301552521", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9091", "id": 301552521, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTU1MjUyMQ==", "user": {"login": "Panaetius", "id": 664486, "node_id": "MDQ6VXNlcjY2NDQ4Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/664486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Panaetius", "html_url": "https://github.com/Panaetius", "followers_url": "https://api.github.com/users/Panaetius/followers", "following_url": "https://api.github.com/users/Panaetius/following{/other_user}", "gists_url": "https://api.github.com/users/Panaetius/gists{/gist_id}", "starred_url": "https://api.github.com/users/Panaetius/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Panaetius/subscriptions", "organizations_url": "https://api.github.com/users/Panaetius/orgs", "repos_url": "https://api.github.com/users/Panaetius/repos", "events_url": "https://api.github.com/users/Panaetius/events{/privacy}", "received_events_url": "https://api.github.com/users/Panaetius/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-15T17:53:03Z", "updated_at": "2017-05-15T18:33:24Z", "author_association": "NONE", "body_html": "<p>I have a similar issue and stumbled upon this report. I used the code supplied by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1411079\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/JonathanRaiman\">@JonathanRaiman</a>  to quickly try and test what exactly (which instruction) is causing my issue.</p>\n<p>After a lot of different tests, evaluating different ops, I got the following code that reliably reproduces this problem:</p>\n<pre><code>import argparse\nimport psutil\n\nfrom os import getpid\nimport tensorflow as tf\nimport numpy as np\n\ndef create_model(input_size, output_size):\n    # model placeholders:\n    shape = tf.clip_by_value(tf.cast(tf.random_normal([2]) * 38.0 + 64.0, tf.int32), 38, 120)\n    shape = tf.concat([[1], shape, [512]], axis=0)\n\n    return tf.ones(shape, dtype=tf.int32)\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--max_epochs', type=int, default=10000)\n    parser.add_argument('--batch_size', type=int, default=7000)\n    parser.add_argument('--input_size', type=int, default=100)\n    parser.add_argument('--output_size', type=int, default=100)\n    parser.add_argument('--device', type=str, default=\"gpu:0\")\n    return parser.parse_args(args=args)\n\ndef main():\n    args = parse_args()\n    session_conf = tf.ConfigProto(allow_soft_placement=True)\n    np.random.seed(1234)\n    process = psutil.Process(getpid())\n\n    with tf.Session(config=session_conf) as session, tf.device(args.device):\n        op = create_model(args.input_size, args.output_size)\n        session.run(tf.global_variables_initializer())\n        before = process.memory_percent()\n\n        for epoch in range(args.max_epochs):\n            session.run(op)\n            \n            if epoch % 100 == 0:\n                after = process.memory_percent()\n                print(\"MEMORY CHANGE %.4f -&gt; %.4f\" % (before, after))\n                before = after\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n<p>The tf.ones(shape, dtype=tf.int32) instruction causes the issue. Same with tf.zeros, tf.ones_like and tf.zeros_like. But the interesting part is, this ONLY happens with dtype=tf.int32, it doesn't happen for int64, int16, int8, uints, floats.</p>\n<p>Another observation is that, while the reported memory usage by python on the first run is roughly the same for all those data types, the memory usage in the XFCE Task manager is more than twice as high for the int32 variant than for other datatypes. So it seems like python is incorrectly reporting memory usage when tf.int32 is used.</p>\n<p>Examples (first bump is int64 with no growth, second bump is int32 with fast growth):</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/664486/26070693/cc62a25c-39a5-11e7-9233-b765f16d3813.png\"><img src=\"https://cloud.githubusercontent.com/assets/664486/26070693/cc62a25c-39a5-11e7-9233-b765f16d3813.png\" alt=\"selection_001\" style=\"max-width:100%;\"></a></p>\n<p>Please also note that the memory usage increases rather quickly (from 2% memory to 8% memory in 10'000 interations, which takes about 10-15 seconds) and that having multiple tf.ones instruction makes it go up even faster, which can have a pretty noticeable effect on larger and more complex models.</p>\n<p>But this only happens when the input-dimensions are random. If the shape supplied to tf.ones is the same on every run, memory used does not increased. So it only affects variable sized tensors.</p>\n<p>Also, tf.cast(tf.ones(shape, dtype=tf.int64), tf.int32) works fine</p>\n<p>I'm not 100% sure this is the same issue as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1411079\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/JonathanRaiman\">@JonathanRaiman</a>'s, since he's not using int32 as far as I can tell, but his example does have a \"None\" dimension and the behaviour looks exactly the same.</p>\n<p>And I'm using tensorflow built from master yesterday, though the problem also existed in 1.1, on Arch Linux</p>", "body_text": "I have a similar issue and stumbled upon this report. I used the code supplied by @JonathanRaiman  to quickly try and test what exactly (which instruction) is causing my issue.\nAfter a lot of different tests, evaluating different ops, I got the following code that reliably reproduces this problem:\nimport argparse\nimport psutil\n\nfrom os import getpid\nimport tensorflow as tf\nimport numpy as np\n\ndef create_model(input_size, output_size):\n    # model placeholders:\n    shape = tf.clip_by_value(tf.cast(tf.random_normal([2]) * 38.0 + 64.0, tf.int32), 38, 120)\n    shape = tf.concat([[1], shape, [512]], axis=0)\n\n    return tf.ones(shape, dtype=tf.int32)\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--max_epochs', type=int, default=10000)\n    parser.add_argument('--batch_size', type=int, default=7000)\n    parser.add_argument('--input_size', type=int, default=100)\n    parser.add_argument('--output_size', type=int, default=100)\n    parser.add_argument('--device', type=str, default=\"gpu:0\")\n    return parser.parse_args(args=args)\n\ndef main():\n    args = parse_args()\n    session_conf = tf.ConfigProto(allow_soft_placement=True)\n    np.random.seed(1234)\n    process = psutil.Process(getpid())\n\n    with tf.Session(config=session_conf) as session, tf.device(args.device):\n        op = create_model(args.input_size, args.output_size)\n        session.run(tf.global_variables_initializer())\n        before = process.memory_percent()\n\n        for epoch in range(args.max_epochs):\n            session.run(op)\n            \n            if epoch % 100 == 0:\n                after = process.memory_percent()\n                print(\"MEMORY CHANGE %.4f -> %.4f\" % (before, after))\n                before = after\n\nif __name__ == \"__main__\":\n    main()\n\nThe tf.ones(shape, dtype=tf.int32) instruction causes the issue. Same with tf.zeros, tf.ones_like and tf.zeros_like. But the interesting part is, this ONLY happens with dtype=tf.int32, it doesn't happen for int64, int16, int8, uints, floats.\nAnother observation is that, while the reported memory usage by python on the first run is roughly the same for all those data types, the memory usage in the XFCE Task manager is more than twice as high for the int32 variant than for other datatypes. So it seems like python is incorrectly reporting memory usage when tf.int32 is used.\nExamples (first bump is int64 with no growth, second bump is int32 with fast growth):\n\nPlease also note that the memory usage increases rather quickly (from 2% memory to 8% memory in 10'000 interations, which takes about 10-15 seconds) and that having multiple tf.ones instruction makes it go up even faster, which can have a pretty noticeable effect on larger and more complex models.\nBut this only happens when the input-dimensions are random. If the shape supplied to tf.ones is the same on every run, memory used does not increased. So it only affects variable sized tensors.\nAlso, tf.cast(tf.ones(shape, dtype=tf.int64), tf.int32) works fine\nI'm not 100% sure this is the same issue as @JonathanRaiman's, since he's not using int32 as far as I can tell, but his example does have a \"None\" dimension and the behaviour looks exactly the same.\nAnd I'm using tensorflow built from master yesterday, though the problem also existed in 1.1, on Arch Linux", "body": "I have a similar issue and stumbled upon this report. I used the code supplied by @JonathanRaiman  to quickly try and test what exactly (which instruction) is causing my issue.\r\n\r\nAfter a lot of different tests, evaluating different ops, I got the following code that reliably reproduces this problem:\r\n\r\n```\r\nimport argparse\r\nimport psutil\r\n\r\nfrom os import getpid\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef create_model(input_size, output_size):\r\n    # model placeholders:\r\n    shape = tf.clip_by_value(tf.cast(tf.random_normal([2]) * 38.0 + 64.0, tf.int32), 38, 120)\r\n    shape = tf.concat([[1], shape, [512]], axis=0)\r\n\r\n    return tf.ones(shape, dtype=tf.int32)\r\n\r\ndef parse_args(args=None):\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--max_epochs', type=int, default=10000)\r\n    parser.add_argument('--batch_size', type=int, default=7000)\r\n    parser.add_argument('--input_size', type=int, default=100)\r\n    parser.add_argument('--output_size', type=int, default=100)\r\n    parser.add_argument('--device', type=str, default=\"gpu:0\")\r\n    return parser.parse_args(args=args)\r\n\r\ndef main():\r\n    args = parse_args()\r\n    session_conf = tf.ConfigProto(allow_soft_placement=True)\r\n    np.random.seed(1234)\r\n    process = psutil.Process(getpid())\r\n\r\n    with tf.Session(config=session_conf) as session, tf.device(args.device):\r\n        op = create_model(args.input_size, args.output_size)\r\n        session.run(tf.global_variables_initializer())\r\n        before = process.memory_percent()\r\n\r\n        for epoch in range(args.max_epochs):\r\n            session.run(op)\r\n            \r\n            if epoch % 100 == 0:\r\n                after = process.memory_percent()\r\n                print(\"MEMORY CHANGE %.4f -> %.4f\" % (before, after))\r\n                before = after\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nThe tf.ones(shape, dtype=tf.int32) instruction causes the issue. Same with tf.zeros, tf.ones_like and tf.zeros_like. But the interesting part is, this ONLY happens with dtype=tf.int32, it doesn't happen for int64, int16, int8, uints, floats.\r\n\r\nAnother observation is that, while the reported memory usage by python on the first run is roughly the same for all those data types, the memory usage in the XFCE Task manager is more than twice as high for the int32 variant than for other datatypes. So it seems like python is incorrectly reporting memory usage when tf.int32 is used.\r\n\r\nExamples (first bump is int64 with no growth, second bump is int32 with fast growth):\r\n\r\n![selection_001](https://cloud.githubusercontent.com/assets/664486/26070693/cc62a25c-39a5-11e7-9233-b765f16d3813.png)\r\n\r\nPlease also note that the memory usage increases rather quickly (from 2% memory to 8% memory in 10'000 interations, which takes about 10-15 seconds) and that having multiple tf.ones instruction makes it go up even faster, which can have a pretty noticeable effect on larger and more complex models.\r\n\r\nBut this only happens when the input-dimensions are random. If the shape supplied to tf.ones is the same on every run, memory used does not increased. So it only affects variable sized tensors.\r\n\r\nAlso, tf.cast(tf.ones(shape, dtype=tf.int64), tf.int32) works fine\r\n\r\nI'm not 100% sure this is the same issue as @JonathanRaiman's, since he's not using int32 as far as I can tell, but his example does have a \"None\" dimension and the behaviour looks exactly the same.\r\n\r\nAnd I'm using tensorflow built from master yesterday, though the problem also existed in 1.1, on Arch Linux\r\n"}