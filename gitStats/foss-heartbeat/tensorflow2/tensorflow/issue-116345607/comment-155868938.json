{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/155868938", "html_url": "https://github.com/tensorflow/tensorflow/issues/140#issuecomment-155868938", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/140", "id": 155868938, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NTg2ODkzOA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-11T18:29:32Z", "updated_at": "2015-11-11T18:29:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is a good question.  Our current suggestion is to pad your inputs to a fixed length, then slice them into reasonable frame count chunks (e.g. 50); and use truncated BPTT.  The PTB tutorial has an example of truncated BPTT, but not of padding.  However this is easy enough to do in python for now.</p>\n<p>We are considering other ways to work with dynamic length sequences, but nothing is in the release as of now.  But even with purely dynamic RNNs you'll still probably have to pad if you want to minibatch.</p>\n<p>A way of dealing with masking is to add a num_steps length list of [batch_size] weight vectors.  Then perform a distributed multiply this with the loss output, aggregate and make that the new loss.  Set weights past your sequence length for each minibatch entry to zero; these particular gradients won't be backpropagated.</p>", "body_text": "This is a good question.  Our current suggestion is to pad your inputs to a fixed length, then slice them into reasonable frame count chunks (e.g. 50); and use truncated BPTT.  The PTB tutorial has an example of truncated BPTT, but not of padding.  However this is easy enough to do in python for now.\nWe are considering other ways to work with dynamic length sequences, but nothing is in the release as of now.  But even with purely dynamic RNNs you'll still probably have to pad if you want to minibatch.\nA way of dealing with masking is to add a num_steps length list of [batch_size] weight vectors.  Then perform a distributed multiply this with the loss output, aggregate and make that the new loss.  Set weights past your sequence length for each minibatch entry to zero; these particular gradients won't be backpropagated.", "body": "This is a good question.  Our current suggestion is to pad your inputs to a fixed length, then slice them into reasonable frame count chunks (e.g. 50); and use truncated BPTT.  The PTB tutorial has an example of truncated BPTT, but not of padding.  However this is easy enough to do in python for now.\n\nWe are considering other ways to work with dynamic length sequences, but nothing is in the release as of now.  But even with purely dynamic RNNs you'll still probably have to pad if you want to minibatch.\n\nA way of dealing with masking is to add a num_steps length list of [batch_size] weight vectors.  Then perform a distributed multiply this with the loss output, aggregate and make that the new loss.  Set weights past your sequence length for each minibatch entry to zero; these particular gradients won't be backpropagated.\n"}