{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/234143736", "html_url": "https://github.com/tensorflow/tensorflow/issues/1592#issuecomment-234143736", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1592", "id": 234143736, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDE0MzczNg==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-21T03:00:53Z", "updated_at": "2016-07-21T03:01:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19379953\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/isabel-schwende\">@isabel-schwende</a> Yes the released implementation uses float32 to <strong>hold</strong> low-bitwidth numbers, because there is simply no low-bitwidth operations available in TF. And we never planned to build such operations into TF because we already have our own low bitwidth run-time implementations working smoothly on ARM.<br>\nThe released model is similar: it uses tf.float32 to hold all the binary weights as well as run all the computation. But anyone who would like to implement those binary operations can make use of our pretrained model directly and gain a speedup.</p>", "body_text": "@isabel-schwende Yes the released implementation uses float32 to hold low-bitwidth numbers, because there is simply no low-bitwidth operations available in TF. And we never planned to build such operations into TF because we already have our own low bitwidth run-time implementations working smoothly on ARM.\nThe released model is similar: it uses tf.float32 to hold all the binary weights as well as run all the computation. But anyone who would like to implement those binary operations can make use of our pretrained model directly and gain a speedup.", "body": "@isabel-schwende Yes the released implementation uses float32 to **hold** low-bitwidth numbers, because there is simply no low-bitwidth operations available in TF. And we never planned to build such operations into TF because we already have our own low bitwidth run-time implementations working smoothly on ARM.  \nThe released model is similar: it uses tf.float32 to hold all the binary weights as well as run all the computation. But anyone who would like to implement those binary operations can make use of our pretrained model directly and gain a speedup.\n"}