{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239167371", "html_url": "https://github.com/tensorflow/tensorflow/issues/1592#issuecomment-239167371", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1592", "id": 239167371, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTE2NzM3MQ==", "user": {"login": "rapatel0", "id": 18704704, "node_id": "MDQ6VXNlcjE4NzA0NzA0", "avatar_url": "https://avatars3.githubusercontent.com/u/18704704?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rapatel0", "html_url": "https://github.com/rapatel0", "followers_url": "https://api.github.com/users/rapatel0/followers", "following_url": "https://api.github.com/users/rapatel0/following{/other_user}", "gists_url": "https://api.github.com/users/rapatel0/gists{/gist_id}", "starred_url": "https://api.github.com/users/rapatel0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rapatel0/subscriptions", "organizations_url": "https://api.github.com/users/rapatel0/orgs", "repos_url": "https://api.github.com/users/rapatel0/repos", "events_url": "https://api.github.com/users/rapatel0/events{/privacy}", "received_events_url": "https://api.github.com/users/rapatel0/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-11T13:54:14Z", "updated_at": "2016-08-11T13:56:42Z", "author_association": "NONE", "body_html": "<p>We got a \"version\" working in Eigen::Tensor (~7x performance over float on Xeon AVX-256), but we're still hitting relatively low accuracies (20 to 40 % more error than a float based net). The accuracy drops quickly as you increase the output channels in a layer.</p>\n<p>From a performance POV the bit packing code slows things down a bit and the calculation of beta values still takes time. I'm not sure if the original paper took this into account because it defined a \"convolution\" as seperate from the binarize step. In practice, however, you need both for every conv2d layer. Still, I might be missing something.</p>\n<p>BTW I'm having issues getting the code to compile in tensorflow. Works fine in Eigen though.</p>", "body_text": "We got a \"version\" working in Eigen::Tensor (~7x performance over float on Xeon AVX-256), but we're still hitting relatively low accuracies (20 to 40 % more error than a float based net). The accuracy drops quickly as you increase the output channels in a layer.\nFrom a performance POV the bit packing code slows things down a bit and the calculation of beta values still takes time. I'm not sure if the original paper took this into account because it defined a \"convolution\" as seperate from the binarize step. In practice, however, you need both for every conv2d layer. Still, I might be missing something.\nBTW I'm having issues getting the code to compile in tensorflow. Works fine in Eigen though.", "body": "We got a \"version\" working in Eigen::Tensor (~7x performance over float on Xeon AVX-256), but we're still hitting relatively low accuracies (20 to 40 % more error than a float based net). The accuracy drops quickly as you increase the output channels in a layer.  \n\nFrom a performance POV the bit packing code slows things down a bit and the calculation of beta values still takes time. I'm not sure if the original paper took this into account because it defined a \"convolution\" as seperate from the binarize step. In practice, however, you need both for every conv2d layer. Still, I might be missing something. \n\nBTW I'm having issues getting the code to compile in tensorflow. Works fine in Eigen though.\n"}