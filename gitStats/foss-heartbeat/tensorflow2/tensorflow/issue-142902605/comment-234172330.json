{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/234172330", "html_url": "https://github.com/tensorflow/tensorflow/issues/1592#issuecomment-234172330", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1592", "id": 234172330, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDE3MjMzMA==", "user": {"login": "isabel-schwende", "id": 19379953, "node_id": "MDQ6VXNlcjE5Mzc5OTUz", "avatar_url": "https://avatars3.githubusercontent.com/u/19379953?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isabel-schwende", "html_url": "https://github.com/isabel-schwende", "followers_url": "https://api.github.com/users/isabel-schwende/followers", "following_url": "https://api.github.com/users/isabel-schwende/following{/other_user}", "gists_url": "https://api.github.com/users/isabel-schwende/gists{/gist_id}", "starred_url": "https://api.github.com/users/isabel-schwende/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isabel-schwende/subscriptions", "organizations_url": "https://api.github.com/users/isabel-schwende/orgs", "repos_url": "https://api.github.com/users/isabel-schwende/repos", "events_url": "https://api.github.com/users/isabel-schwende/events{/privacy}", "received_events_url": "https://api.github.com/users/isabel-schwende/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-21T06:55:34Z", "updated_at": "2016-07-21T06:55:34Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1381301\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ppwwyyxx\">@ppwwyyxx</a> Thank you for your clarification but I think I have to disagree at this point. Yes, there is no Datatype in TensorFlow available for 1,2 or 6 bit but there is the tf.quint8 datatype for tensors. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=161459\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petewarden\">@petewarden</a> and his team have introduced it in their tutorial here <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/quantization/index.md\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/quantization/index.md</a>  Sure, the method of quantization is different to what DoReFa is doing as they keep minimums and maximums as floats. I've played around with the tutorial and also used a customised AlexNet saved in a protobuf-file to quantize down to 8 bit. I was able to observe that the protobuf-file of the quantized network was indeed much smaller compared to the 32 float original. For now, the way how low-bitwidth weights/activations are used is not compatible but so I was wondering if there is a way to, let's say, create a customized version of the existing quantization tool to also decrease the memory of the DoReFa AlexNet model for inference tasks on small devices. But I guess that it would be still too much work at the moment.</p>", "body_text": "@ppwwyyxx Thank you for your clarification but I think I have to disagree at this point. Yes, there is no Datatype in TensorFlow available for 1,2 or 6 bit but there is the tf.quint8 datatype for tensors. @petewarden and his team have introduced it in their tutorial here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/quantization/index.md  Sure, the method of quantization is different to what DoReFa is doing as they keep minimums and maximums as floats. I've played around with the tutorial and also used a customised AlexNet saved in a protobuf-file to quantize down to 8 bit. I was able to observe that the protobuf-file of the quantized network was indeed much smaller compared to the 32 float original. For now, the way how low-bitwidth weights/activations are used is not compatible but so I was wondering if there is a way to, let's say, create a customized version of the existing quantization tool to also decrease the memory of the DoReFa AlexNet model for inference tasks on small devices. But I guess that it would be still too much work at the moment.", "body": "@ppwwyyxx Thank you for your clarification but I think I have to disagree at this point. Yes, there is no Datatype in TensorFlow available for 1,2 or 6 bit but there is the tf.quint8 datatype for tensors. @petewarden and his team have introduced it in their tutorial here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/quantization/index.md  Sure, the method of quantization is different to what DoReFa is doing as they keep minimums and maximums as floats. I've played around with the tutorial and also used a customised AlexNet saved in a protobuf-file to quantize down to 8 bit. I was able to observe that the protobuf-file of the quantized network was indeed much smaller compared to the 32 float original. For now, the way how low-bitwidth weights/activations are used is not compatible but so I was wondering if there is a way to, let's say, create a customized version of the existing quantization tool to also decrease the memory of the DoReFa AlexNet model for inference tasks on small devices. But I guess that it would be still too much work at the moment. \n"}