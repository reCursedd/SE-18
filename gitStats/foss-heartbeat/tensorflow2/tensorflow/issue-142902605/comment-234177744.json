{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/234177744", "html_url": "https://github.com/tensorflow/tensorflow/issues/1592#issuecomment-234177744", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1592", "id": 234177744, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDE3Nzc0NA==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-21T07:27:06Z", "updated_at": "2016-07-21T07:27:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I totally agree and the code we released was never intended for running on mobiles, but for showing how to train such networks in tensorflow, as a supplementary material and proof of the paper. After all I've never seen a public release of binary-weight ImageNet models before.</p>\n<p>I can certainly compress our models to about 30x smaller because they are essentially binary, and maybe use tf.quint8 for computation. But that doesn't make sense to me, because I think anyone who really want 1 or 2 bit level of performance &amp; compression rate, should use a much more compact &amp; tiny run-time toolchain as we did, instead of using tensorflow. If the speed &amp; storage of 8 bit models is good enough for the use case, then I would certainly suggest trying the TF quantization tutorial instead of DoReFa-Net, because 8 bit models would have better accuracy.</p>", "body_text": "I totally agree and the code we released was never intended for running on mobiles, but for showing how to train such networks in tensorflow, as a supplementary material and proof of the paper. After all I've never seen a public release of binary-weight ImageNet models before.\nI can certainly compress our models to about 30x smaller because they are essentially binary, and maybe use tf.quint8 for computation. But that doesn't make sense to me, because I think anyone who really want 1 or 2 bit level of performance & compression rate, should use a much more compact & tiny run-time toolchain as we did, instead of using tensorflow. If the speed & storage of 8 bit models is good enough for the use case, then I would certainly suggest trying the TF quantization tutorial instead of DoReFa-Net, because 8 bit models would have better accuracy.", "body": "I totally agree and the code we released was never intended for running on mobiles, but for showing how to train such networks in tensorflow, as a supplementary material and proof of the paper. After all I've never seen a public release of binary-weight ImageNet models before.\n\nI can certainly compress our models to about 30x smaller because they are essentially binary, and maybe use tf.quint8 for computation. But that doesn't make sense to me, because I think anyone who really want 1 or 2 bit level of performance & compression rate, should use a much more compact & tiny run-time toolchain as we did, instead of using tensorflow. If the speed & storage of 8 bit models is good enough for the use case, then I would certainly suggest trying the TF quantization tutorial instead of DoReFa-Net, because 8 bit models would have better accuracy. \n"}