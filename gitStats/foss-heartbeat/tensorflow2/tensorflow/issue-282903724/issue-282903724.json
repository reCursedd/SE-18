{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15448", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15448/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15448/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15448/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15448", "id": 282903724, "node_id": "MDU6SXNzdWUyODI5MDM3MjQ=", "number": 15448, "title": "[Feature] Dataset API - Reinitializable Iterator resets to first dataset element", "user": {"login": "maxfiedler", "id": 4192637, "node_id": "MDQ6VXNlcjQxOTI2Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/4192637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maxfiedler", "html_url": "https://github.com/maxfiedler", "followers_url": "https://api.github.com/users/maxfiedler/followers", "following_url": "https://api.github.com/users/maxfiedler/following{/other_user}", "gists_url": "https://api.github.com/users/maxfiedler/gists{/gist_id}", "starred_url": "https://api.github.com/users/maxfiedler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maxfiedler/subscriptions", "organizations_url": "https://api.github.com/users/maxfiedler/orgs", "repos_url": "https://api.github.com/users/maxfiedler/repos", "events_url": "https://api.github.com/users/maxfiedler/events{/privacy}", "received_events_url": "https://api.github.com/users/maxfiedler/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2017-12-18T14:49:11Z", "updated_at": "2018-07-09T16:28:09Z", "closed_at": "2018-01-02T18:44:01Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: slightly altered stock example (see below)</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: 1.4.0 from source</li>\n<li><strong>Python version</strong>: 3.6.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.7.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0</li>\n<li><strong>GPU model and memory</strong>: Nvidia 1060</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Currently the re-initializable iterator API using .from_structure and Iterator.make_initializer always resets the get_next() op of the iterator to fetch the first element of its Dataset instance again after running <code>sess.run(training_init_op)</code> or <code>sess.run(validation_init_op)</code>, respectively.</p>\n<p>Is this really the intended behavior? This means, that if you want to switch between training and validation Datasets within one epoch (i.e. in a shorter rhythm than the full dataset length) you will always only iterate over the first <code>batch_size * number-of-training-steps-before-validation-step</code> elements of the training set during training.</p>\n<h3>Source code / logs</h3>\n<p>I guess the code example will make it clearer:</p>\n<pre><code># Define training and validation datasets with the same structure.\ntraining_dataset = tf.data.Dataset.range(100)\nvalidation_dataset = tf.data.Dataset.range(50)\n\n# A reinitializable iterator is defined by its structure. We could use the\n# `output_types` and `output_shapes` properties of either `training_dataset`\n# or `validation_dataset` here, because they are compatible.\niterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n                                  training_dataset.output_shapes)\n\nnext_element = iterator.get_next()\n\n\ntraining_init_op = iterator.make_initializer(training_dataset)\nvalidation_init_op = iterator.make_initializer(validation_dataset)\n\nwith tf.Session() as sess:\n   # Run 20 epochs in which the training dataset is traversed, followed by the\n   # validation dataset.\n   for i in range(5):\n       # Initialize an iterator over the training dataset.\n       print(\"#########################  \", i)\n       sess.run(training_init_op)\n       for _ in range(10):\n           nel = sess.run(next_element)\n           print(\"train: \", type(nel), nel)\n\n       # Initialize an iterator over the validation dataset.\n       sess.run(validation_init_op)\n       for _ in range(5):\n           nel = sess.run(next_element)\n           print(\"valid: \", type(nel), nel)\n</code></pre>\n<p>Produces the output:</p>\n<pre><code>#########################   0\ntrain:  &lt;class 'numpy.int64'&gt; 0\ntrain:  &lt;class 'numpy.int64'&gt; 1\ntrain:  &lt;class 'numpy.int64'&gt; 2\ntrain:  &lt;class 'numpy.int64'&gt; 3\ntrain:  &lt;class 'numpy.int64'&gt; 4\ntrain:  &lt;class 'numpy.int64'&gt; 5\ntrain:  &lt;class 'numpy.int64'&gt; 6\ntrain:  &lt;class 'numpy.int64'&gt; 7\ntrain:  &lt;class 'numpy.int64'&gt; 8\ntrain:  &lt;class 'numpy.int64'&gt; 9\nvalid:  &lt;class 'numpy.int64'&gt; 0\nvalid:  &lt;class 'numpy.int64'&gt; 1\nvalid:  &lt;class 'numpy.int64'&gt; 2\nvalid:  &lt;class 'numpy.int64'&gt; 3\nvalid:  &lt;class 'numpy.int64'&gt; 4\n#########################   1\ntrain:  &lt;class 'numpy.int64'&gt; 0\ntrain:  &lt;class 'numpy.int64'&gt; 1\ntrain:  &lt;class 'numpy.int64'&gt; 2\ntrain:  &lt;class 'numpy.int64'&gt; 3\ntrain:  &lt;class 'numpy.int64'&gt; 4\ntrain:  &lt;class 'numpy.int64'&gt; 5\ntrain:  &lt;class 'numpy.int64'&gt; 6\ntrain:  &lt;class 'numpy.int64'&gt; 7\ntrain:  &lt;class 'numpy.int64'&gt; 8\ntrain:  &lt;class 'numpy.int64'&gt; 9\nvalid:  &lt;class 'numpy.int64'&gt; 0\nvalid:  &lt;class 'numpy.int64'&gt; 1\nvalid:  &lt;class 'numpy.int64'&gt; 2\nvalid:  &lt;class 'numpy.int64'&gt; 3\nvalid:  &lt;class 'numpy.int64'&gt; 4\n...\n</code></pre>\n<p>Apparently, the latter 90 elements of the training set and the latter 45 elements of the validation set never get evaluated. I don't really see the real-world use-case for this behavior.</p>\n<p>I know that you can implement the other functionality via the feedable iterator scheme using one_shot_iterators (but not using initializable iterators) as highlighted by the code below (pay attention to the different iterators used for training and validation here):</p>\n<pre><code># Define training and validation datasets with the same structure.\ntraining_dataset = tf.data.Dataset.range(10000000).repeat(2)\nvalidation_dataset = tf.data.Dataset.range(5000000).repeat(2)\n\n# A feedable iterator is defined by a handle placeholder and its structure. We\n# could use the `output_types` and `output_shapes` properties of either\n# `training_dataset` or `validation_dataset` here, because they have\n# identical structure.\nhandle = tf.placeholder(tf.string, shape=[])\niterator = tf.data.Iterator.from_string_handle(\n    handle, training_dataset.output_types, training_dataset.output_shapes)\nnext_element = iterator.get_next()\n\n# You can use feedable iterators with a variety of different kinds of iterator\ntraining_iterator = training_dataset.make_one_shot_iterator()\nvalidation_iterator = validation_dataset.make_initializable_iterator()\n\nwith tf.Session() as sess:\n    # The `Iterator.string_handle()` method returns a tensor that can be evaluated\n    # and used to feed the `handle` placeholder.\n    training_handle = sess.run(training_iterator.string_handle())\n    validation_handle = sess.run(validation_iterator.string_handle())\n    # Loop forever, alternating between training and validation.\n    for i in range(5):\n        print(\"######################## \", i)\n        i += 1\n        # Run 10 steps using the training dataset. Note that the training dataset is\n        # 2 * the original set, i.e. we run 2 epochs (see .repeat() argument), and we resume from where\n        # we left off in the previous `while` loop iteration.\n        for _ in range(10):\n            nel = sess.run(next_element, feed_dict={handle: training_handle})\n            print(\"train: \", type(nel), nel)\n\n        # Run one pass over the validation dataset.\n        sess.run(validation_iterator.initializer)\n        for _ in range(5):\n            nel = sess.run(next_element, feed_dict={handle: validation_handle})\n            print(\"valid: \", type(nel), nel)\n</code></pre>\n<p>creates output:</p>\n<pre><code>########################  0\ntrain:  &lt;class 'numpy.int64'&gt; 0\ntrain:  &lt;class 'numpy.int64'&gt; 1\ntrain:  &lt;class 'numpy.int64'&gt; 2\ntrain:  &lt;class 'numpy.int64'&gt; 3\ntrain:  &lt;class 'numpy.int64'&gt; 4\ntrain:  &lt;class 'numpy.int64'&gt; 5\ntrain:  &lt;class 'numpy.int64'&gt; 6\ntrain:  &lt;class 'numpy.int64'&gt; 7\ntrain:  &lt;class 'numpy.int64'&gt; 8\ntrain:  &lt;class 'numpy.int64'&gt; 9\nvalid:  &lt;class 'numpy.int64'&gt; 0\nvalid:  &lt;class 'numpy.int64'&gt; 1\nvalid:  &lt;class 'numpy.int64'&gt; 2\nvalid:  &lt;class 'numpy.int64'&gt; 3\nvalid:  &lt;class 'numpy.int64'&gt; 4\n########################  1\ntrain:  &lt;class 'numpy.int64'&gt; 10\ntrain:  &lt;class 'numpy.int64'&gt; 11\ntrain:  &lt;class 'numpy.int64'&gt; 12\ntrain:  &lt;class 'numpy.int64'&gt; 13\ntrain:  &lt;class 'numpy.int64'&gt; 14\ntrain:  &lt;class 'numpy.int64'&gt; 15\ntrain:  &lt;class 'numpy.int64'&gt; 16\ntrain:  &lt;class 'numpy.int64'&gt; 17\ntrain:  &lt;class 'numpy.int64'&gt; 18\ntrain:  &lt;class 'numpy.int64'&gt; 19\nvalid:  &lt;class 'numpy.int64'&gt; 0\nvalid:  &lt;class 'numpy.int64'&gt; 1\nvalid:  &lt;class 'numpy.int64'&gt; 2\nvalid:  &lt;class 'numpy.int64'&gt; 3\nvalid:  &lt;class 'numpy.int64'&gt; 4\n########################  2\ntrain:  &lt;class 'numpy.int64'&gt; 20\ntrain:  &lt;class 'numpy.int64'&gt; 21\ntrain:  &lt;class 'numpy.int64'&gt; 22\ntrain:  &lt;class 'numpy.int64'&gt; 23\ntrain:  &lt;class 'numpy.int64'&gt; 24\ntrain:  &lt;class 'numpy.int64'&gt; 25\ntrain:  &lt;class 'numpy.int64'&gt; 26\ntrain:  &lt;class 'numpy.int64'&gt; 27\ntrain:  &lt;class 'numpy.int64'&gt; 28\ntrain:  &lt;class 'numpy.int64'&gt; 29\nvalid:  &lt;class 'numpy.int64'&gt; 0\nvalid:  &lt;class 'numpy.int64'&gt; 1\nvalid:  &lt;class 'numpy.int64'&gt; 2\nvalid:  &lt;class 'numpy.int64'&gt; 3\nvalid:  &lt;class 'numpy.int64'&gt; 4\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): slightly altered stock example (see below)\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): 1.4.0 from source\nPython version: 3.6.3\nBazel version (if compiling from source): 0.7.0\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\nCUDA/cuDNN version: 8.0\nGPU model and memory: Nvidia 1060\n\nDescribe the problem\nCurrently the re-initializable iterator API using .from_structure and Iterator.make_initializer always resets the get_next() op of the iterator to fetch the first element of its Dataset instance again after running sess.run(training_init_op) or sess.run(validation_init_op), respectively.\nIs this really the intended behavior? This means, that if you want to switch between training and validation Datasets within one epoch (i.e. in a shorter rhythm than the full dataset length) you will always only iterate over the first batch_size * number-of-training-steps-before-validation-step elements of the training set during training.\nSource code / logs\nI guess the code example will make it clearer:\n# Define training and validation datasets with the same structure.\ntraining_dataset = tf.data.Dataset.range(100)\nvalidation_dataset = tf.data.Dataset.range(50)\n\n# A reinitializable iterator is defined by its structure. We could use the\n# `output_types` and `output_shapes` properties of either `training_dataset`\n# or `validation_dataset` here, because they are compatible.\niterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n                                  training_dataset.output_shapes)\n\nnext_element = iterator.get_next()\n\n\ntraining_init_op = iterator.make_initializer(training_dataset)\nvalidation_init_op = iterator.make_initializer(validation_dataset)\n\nwith tf.Session() as sess:\n   # Run 20 epochs in which the training dataset is traversed, followed by the\n   # validation dataset.\n   for i in range(5):\n       # Initialize an iterator over the training dataset.\n       print(\"#########################  \", i)\n       sess.run(training_init_op)\n       for _ in range(10):\n           nel = sess.run(next_element)\n           print(\"train: \", type(nel), nel)\n\n       # Initialize an iterator over the validation dataset.\n       sess.run(validation_init_op)\n       for _ in range(5):\n           nel = sess.run(next_element)\n           print(\"valid: \", type(nel), nel)\n\nProduces the output:\n#########################   0\ntrain:  <class 'numpy.int64'> 0\ntrain:  <class 'numpy.int64'> 1\ntrain:  <class 'numpy.int64'> 2\ntrain:  <class 'numpy.int64'> 3\ntrain:  <class 'numpy.int64'> 4\ntrain:  <class 'numpy.int64'> 5\ntrain:  <class 'numpy.int64'> 6\ntrain:  <class 'numpy.int64'> 7\ntrain:  <class 'numpy.int64'> 8\ntrain:  <class 'numpy.int64'> 9\nvalid:  <class 'numpy.int64'> 0\nvalid:  <class 'numpy.int64'> 1\nvalid:  <class 'numpy.int64'> 2\nvalid:  <class 'numpy.int64'> 3\nvalid:  <class 'numpy.int64'> 4\n#########################   1\ntrain:  <class 'numpy.int64'> 0\ntrain:  <class 'numpy.int64'> 1\ntrain:  <class 'numpy.int64'> 2\ntrain:  <class 'numpy.int64'> 3\ntrain:  <class 'numpy.int64'> 4\ntrain:  <class 'numpy.int64'> 5\ntrain:  <class 'numpy.int64'> 6\ntrain:  <class 'numpy.int64'> 7\ntrain:  <class 'numpy.int64'> 8\ntrain:  <class 'numpy.int64'> 9\nvalid:  <class 'numpy.int64'> 0\nvalid:  <class 'numpy.int64'> 1\nvalid:  <class 'numpy.int64'> 2\nvalid:  <class 'numpy.int64'> 3\nvalid:  <class 'numpy.int64'> 4\n...\n\nApparently, the latter 90 elements of the training set and the latter 45 elements of the validation set never get evaluated. I don't really see the real-world use-case for this behavior.\nI know that you can implement the other functionality via the feedable iterator scheme using one_shot_iterators (but not using initializable iterators) as highlighted by the code below (pay attention to the different iterators used for training and validation here):\n# Define training and validation datasets with the same structure.\ntraining_dataset = tf.data.Dataset.range(10000000).repeat(2)\nvalidation_dataset = tf.data.Dataset.range(5000000).repeat(2)\n\n# A feedable iterator is defined by a handle placeholder and its structure. We\n# could use the `output_types` and `output_shapes` properties of either\n# `training_dataset` or `validation_dataset` here, because they have\n# identical structure.\nhandle = tf.placeholder(tf.string, shape=[])\niterator = tf.data.Iterator.from_string_handle(\n    handle, training_dataset.output_types, training_dataset.output_shapes)\nnext_element = iterator.get_next()\n\n# You can use feedable iterators with a variety of different kinds of iterator\ntraining_iterator = training_dataset.make_one_shot_iterator()\nvalidation_iterator = validation_dataset.make_initializable_iterator()\n\nwith tf.Session() as sess:\n    # The `Iterator.string_handle()` method returns a tensor that can be evaluated\n    # and used to feed the `handle` placeholder.\n    training_handle = sess.run(training_iterator.string_handle())\n    validation_handle = sess.run(validation_iterator.string_handle())\n    # Loop forever, alternating between training and validation.\n    for i in range(5):\n        print(\"######################## \", i)\n        i += 1\n        # Run 10 steps using the training dataset. Note that the training dataset is\n        # 2 * the original set, i.e. we run 2 epochs (see .repeat() argument), and we resume from where\n        # we left off in the previous `while` loop iteration.\n        for _ in range(10):\n            nel = sess.run(next_element, feed_dict={handle: training_handle})\n            print(\"train: \", type(nel), nel)\n\n        # Run one pass over the validation dataset.\n        sess.run(validation_iterator.initializer)\n        for _ in range(5):\n            nel = sess.run(next_element, feed_dict={handle: validation_handle})\n            print(\"valid: \", type(nel), nel)\n\ncreates output:\n########################  0\ntrain:  <class 'numpy.int64'> 0\ntrain:  <class 'numpy.int64'> 1\ntrain:  <class 'numpy.int64'> 2\ntrain:  <class 'numpy.int64'> 3\ntrain:  <class 'numpy.int64'> 4\ntrain:  <class 'numpy.int64'> 5\ntrain:  <class 'numpy.int64'> 6\ntrain:  <class 'numpy.int64'> 7\ntrain:  <class 'numpy.int64'> 8\ntrain:  <class 'numpy.int64'> 9\nvalid:  <class 'numpy.int64'> 0\nvalid:  <class 'numpy.int64'> 1\nvalid:  <class 'numpy.int64'> 2\nvalid:  <class 'numpy.int64'> 3\nvalid:  <class 'numpy.int64'> 4\n########################  1\ntrain:  <class 'numpy.int64'> 10\ntrain:  <class 'numpy.int64'> 11\ntrain:  <class 'numpy.int64'> 12\ntrain:  <class 'numpy.int64'> 13\ntrain:  <class 'numpy.int64'> 14\ntrain:  <class 'numpy.int64'> 15\ntrain:  <class 'numpy.int64'> 16\ntrain:  <class 'numpy.int64'> 17\ntrain:  <class 'numpy.int64'> 18\ntrain:  <class 'numpy.int64'> 19\nvalid:  <class 'numpy.int64'> 0\nvalid:  <class 'numpy.int64'> 1\nvalid:  <class 'numpy.int64'> 2\nvalid:  <class 'numpy.int64'> 3\nvalid:  <class 'numpy.int64'> 4\n########################  2\ntrain:  <class 'numpy.int64'> 20\ntrain:  <class 'numpy.int64'> 21\ntrain:  <class 'numpy.int64'> 22\ntrain:  <class 'numpy.int64'> 23\ntrain:  <class 'numpy.int64'> 24\ntrain:  <class 'numpy.int64'> 25\ntrain:  <class 'numpy.int64'> 26\ntrain:  <class 'numpy.int64'> 27\ntrain:  <class 'numpy.int64'> 28\ntrain:  <class 'numpy.int64'> 29\nvalid:  <class 'numpy.int64'> 0\nvalid:  <class 'numpy.int64'> 1\nvalid:  <class 'numpy.int64'> 2\nvalid:  <class 'numpy.int64'> 3\nvalid:  <class 'numpy.int64'> 4", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: slightly altered stock example (see below)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: 1.4.0 from source\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: Nvidia 1060\r\n\r\n### Describe the problem\r\nCurrently the re-initializable iterator API using .from_structure and Iterator.make_initializer always resets the get_next() op of the iterator to fetch the first element of its Dataset instance again after running `sess.run(training_init_op)` or `sess.run(validation_init_op)`, respectively. \r\n\r\nIs this really the intended behavior? This means, that if you want to switch between training and validation Datasets within one epoch (i.e. in a shorter rhythm than the full dataset length) you will always only iterate over the first `batch_size * number-of-training-steps-before-validation-step` elements of the training set during training. \r\n\r\n### Source code / logs\r\nI guess the code example will make it clearer:\r\n```\r\n# Define training and validation datasets with the same structure.\r\ntraining_dataset = tf.data.Dataset.range(100)\r\nvalidation_dataset = tf.data.Dataset.range(50)\r\n\r\n# A reinitializable iterator is defined by its structure. We could use the\r\n# `output_types` and `output_shapes` properties of either `training_dataset`\r\n# or `validation_dataset` here, because they are compatible.\r\niterator = tf.data.Iterator.from_structure(training_dataset.output_types,\r\n                                  training_dataset.output_shapes)\r\n\r\nnext_element = iterator.get_next()\r\n\r\n\r\ntraining_init_op = iterator.make_initializer(training_dataset)\r\nvalidation_init_op = iterator.make_initializer(validation_dataset)\r\n\r\nwith tf.Session() as sess:\r\n   # Run 20 epochs in which the training dataset is traversed, followed by the\r\n   # validation dataset.\r\n   for i in range(5):\r\n       # Initialize an iterator over the training dataset.\r\n       print(\"#########################  \", i)\r\n       sess.run(training_init_op)\r\n       for _ in range(10):\r\n           nel = sess.run(next_element)\r\n           print(\"train: \", type(nel), nel)\r\n\r\n       # Initialize an iterator over the validation dataset.\r\n       sess.run(validation_init_op)\r\n       for _ in range(5):\r\n           nel = sess.run(next_element)\r\n           print(\"valid: \", type(nel), nel)\r\n```\r\n\r\nProduces the output:\r\n\r\n```\r\n#########################   0\r\ntrain:  <class 'numpy.int64'> 0\r\ntrain:  <class 'numpy.int64'> 1\r\ntrain:  <class 'numpy.int64'> 2\r\ntrain:  <class 'numpy.int64'> 3\r\ntrain:  <class 'numpy.int64'> 4\r\ntrain:  <class 'numpy.int64'> 5\r\ntrain:  <class 'numpy.int64'> 6\r\ntrain:  <class 'numpy.int64'> 7\r\ntrain:  <class 'numpy.int64'> 8\r\ntrain:  <class 'numpy.int64'> 9\r\nvalid:  <class 'numpy.int64'> 0\r\nvalid:  <class 'numpy.int64'> 1\r\nvalid:  <class 'numpy.int64'> 2\r\nvalid:  <class 'numpy.int64'> 3\r\nvalid:  <class 'numpy.int64'> 4\r\n#########################   1\r\ntrain:  <class 'numpy.int64'> 0\r\ntrain:  <class 'numpy.int64'> 1\r\ntrain:  <class 'numpy.int64'> 2\r\ntrain:  <class 'numpy.int64'> 3\r\ntrain:  <class 'numpy.int64'> 4\r\ntrain:  <class 'numpy.int64'> 5\r\ntrain:  <class 'numpy.int64'> 6\r\ntrain:  <class 'numpy.int64'> 7\r\ntrain:  <class 'numpy.int64'> 8\r\ntrain:  <class 'numpy.int64'> 9\r\nvalid:  <class 'numpy.int64'> 0\r\nvalid:  <class 'numpy.int64'> 1\r\nvalid:  <class 'numpy.int64'> 2\r\nvalid:  <class 'numpy.int64'> 3\r\nvalid:  <class 'numpy.int64'> 4\r\n...\r\n```\r\nApparently, the latter 90 elements of the training set and the latter 45 elements of the validation set never get evaluated. I don't really see the real-world use-case for this behavior. \r\n\r\n\r\nI know that you can implement the other functionality via the feedable iterator scheme using one_shot_iterators (but not using initializable iterators) as highlighted by the code below (pay attention to the different iterators used for training and validation here):\r\n\r\n```\r\n# Define training and validation datasets with the same structure.\r\ntraining_dataset = tf.data.Dataset.range(10000000).repeat(2)\r\nvalidation_dataset = tf.data.Dataset.range(5000000).repeat(2)\r\n\r\n# A feedable iterator is defined by a handle placeholder and its structure. We\r\n# could use the `output_types` and `output_shapes` properties of either\r\n# `training_dataset` or `validation_dataset` here, because they have\r\n# identical structure.\r\nhandle = tf.placeholder(tf.string, shape=[])\r\niterator = tf.data.Iterator.from_string_handle(\r\n    handle, training_dataset.output_types, training_dataset.output_shapes)\r\nnext_element = iterator.get_next()\r\n\r\n# You can use feedable iterators with a variety of different kinds of iterator\r\ntraining_iterator = training_dataset.make_one_shot_iterator()\r\nvalidation_iterator = validation_dataset.make_initializable_iterator()\r\n\r\nwith tf.Session() as sess:\r\n    # The `Iterator.string_handle()` method returns a tensor that can be evaluated\r\n    # and used to feed the `handle` placeholder.\r\n    training_handle = sess.run(training_iterator.string_handle())\r\n    validation_handle = sess.run(validation_iterator.string_handle())\r\n    # Loop forever, alternating between training and validation.\r\n    for i in range(5):\r\n        print(\"######################## \", i)\r\n        i += 1\r\n        # Run 10 steps using the training dataset. Note that the training dataset is\r\n        # 2 * the original set, i.e. we run 2 epochs (see .repeat() argument), and we resume from where\r\n        # we left off in the previous `while` loop iteration.\r\n        for _ in range(10):\r\n            nel = sess.run(next_element, feed_dict={handle: training_handle})\r\n            print(\"train: \", type(nel), nel)\r\n\r\n        # Run one pass over the validation dataset.\r\n        sess.run(validation_iterator.initializer)\r\n        for _ in range(5):\r\n            nel = sess.run(next_element, feed_dict={handle: validation_handle})\r\n            print(\"valid: \", type(nel), nel)\r\n```\r\n\r\ncreates output:\r\n\r\n```\r\n########################  0\r\ntrain:  <class 'numpy.int64'> 0\r\ntrain:  <class 'numpy.int64'> 1\r\ntrain:  <class 'numpy.int64'> 2\r\ntrain:  <class 'numpy.int64'> 3\r\ntrain:  <class 'numpy.int64'> 4\r\ntrain:  <class 'numpy.int64'> 5\r\ntrain:  <class 'numpy.int64'> 6\r\ntrain:  <class 'numpy.int64'> 7\r\ntrain:  <class 'numpy.int64'> 8\r\ntrain:  <class 'numpy.int64'> 9\r\nvalid:  <class 'numpy.int64'> 0\r\nvalid:  <class 'numpy.int64'> 1\r\nvalid:  <class 'numpy.int64'> 2\r\nvalid:  <class 'numpy.int64'> 3\r\nvalid:  <class 'numpy.int64'> 4\r\n########################  1\r\ntrain:  <class 'numpy.int64'> 10\r\ntrain:  <class 'numpy.int64'> 11\r\ntrain:  <class 'numpy.int64'> 12\r\ntrain:  <class 'numpy.int64'> 13\r\ntrain:  <class 'numpy.int64'> 14\r\ntrain:  <class 'numpy.int64'> 15\r\ntrain:  <class 'numpy.int64'> 16\r\ntrain:  <class 'numpy.int64'> 17\r\ntrain:  <class 'numpy.int64'> 18\r\ntrain:  <class 'numpy.int64'> 19\r\nvalid:  <class 'numpy.int64'> 0\r\nvalid:  <class 'numpy.int64'> 1\r\nvalid:  <class 'numpy.int64'> 2\r\nvalid:  <class 'numpy.int64'> 3\r\nvalid:  <class 'numpy.int64'> 4\r\n########################  2\r\ntrain:  <class 'numpy.int64'> 20\r\ntrain:  <class 'numpy.int64'> 21\r\ntrain:  <class 'numpy.int64'> 22\r\ntrain:  <class 'numpy.int64'> 23\r\ntrain:  <class 'numpy.int64'> 24\r\ntrain:  <class 'numpy.int64'> 25\r\ntrain:  <class 'numpy.int64'> 26\r\ntrain:  <class 'numpy.int64'> 27\r\ntrain:  <class 'numpy.int64'> 28\r\ntrain:  <class 'numpy.int64'> 29\r\nvalid:  <class 'numpy.int64'> 0\r\nvalid:  <class 'numpy.int64'> 1\r\nvalid:  <class 'numpy.int64'> 2\r\nvalid:  <class 'numpy.int64'> 3\r\nvalid:  <class 'numpy.int64'> 4\r\n```"}