{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21196", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21196/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21196/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21196/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21196", "id": 345402975, "node_id": "MDU6SXNzdWUzNDU0MDI5NzU=", "number": 21196, "title": "MobileNet v2 slower than v1 when loading from Frozen GraphDef", "user": {"login": "suryaprakaz", "id": 7610546, "node_id": "MDQ6VXNlcjc2MTA1NDY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7610546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suryaprakaz", "html_url": "https://github.com/suryaprakaz", "followers_url": "https://api.github.com/users/suryaprakaz/followers", "following_url": "https://api.github.com/users/suryaprakaz/following{/other_user}", "gists_url": "https://api.github.com/users/suryaprakaz/gists{/gist_id}", "starred_url": "https://api.github.com/users/suryaprakaz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suryaprakaz/subscriptions", "organizations_url": "https://api.github.com/users/suryaprakaz/orgs", "repos_url": "https://api.github.com/users/suryaprakaz/repos", "events_url": "https://api.github.com/users/suryaprakaz/events{/privacy}", "received_events_url": "https://api.github.com/users/suryaprakaz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "derekjchow", "id": 1195088, "node_id": "MDQ6VXNlcjExOTUwODg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1195088?v=4", "gravatar_id": "", "url": "https://api.github.com/users/derekjchow", "html_url": "https://github.com/derekjchow", "followers_url": "https://api.github.com/users/derekjchow/followers", "following_url": "https://api.github.com/users/derekjchow/following{/other_user}", "gists_url": "https://api.github.com/users/derekjchow/gists{/gist_id}", "starred_url": "https://api.github.com/users/derekjchow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/derekjchow/subscriptions", "organizations_url": "https://api.github.com/users/derekjchow/orgs", "repos_url": "https://api.github.com/users/derekjchow/repos", "events_url": "https://api.github.com/users/derekjchow/events{/privacy}", "received_events_url": "https://api.github.com/users/derekjchow/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "derekjchow", "id": 1195088, "node_id": "MDQ6VXNlcjExOTUwODg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1195088?v=4", "gravatar_id": "", "url": "https://api.github.com/users/derekjchow", "html_url": "https://github.com/derekjchow", "followers_url": "https://api.github.com/users/derekjchow/followers", "following_url": "https://api.github.com/users/derekjchow/following{/other_user}", "gists_url": "https://api.github.com/users/derekjchow/gists{/gist_id}", "starred_url": "https://api.github.com/users/derekjchow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/derekjchow/subscriptions", "organizations_url": "https://api.github.com/users/derekjchow/orgs", "repos_url": "https://api.github.com/users/derekjchow/repos", "events_url": "https://api.github.com/users/derekjchow/events{/privacy}", "received_events_url": "https://api.github.com/users/derekjchow/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-07-28T00:51:55Z", "updated_at": "2018-11-14T19:23:38Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04.3 LTS</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:<br>\nDoesn't apply</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nBinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nv1.9.0-0-g25c197e023 1.9.0</li>\n<li><strong>Python version</strong>:<br>\nPython3</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nDoesn't apply</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nDoesn't apply</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nCUDA: 9.0 ; cuDNN: 7.1.4</li>\n<li><strong>GPU model and memory</strong>:<br>\nGeForce GTX 1080 Ti</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<h3>Summary</h3>\n<p>MobileNet v2 is faster than v1 only when loading from the checkpoint format i.e, Variable Ops (meta, index, data) whereas it is slower than v1 when running in the frozen graph format (Const Ops) (.pb)</p>\n<h3>Description</h3>\n<p>I have two TensorFlow trained models that are in the checkpoint format (meta, index, data) namely mobilenetv1_0.75.ckpt and mobilenetv2_0.75_6.ckpt.</p>\n<p>The model definitions are as described in the <a href=\"https://arxiv.org/abs/1704.04861\" rel=\"nofollow\">MobileNetv1</a> and <a href=\"https://arxiv.org/pdf/1801.04381.pdf\" rel=\"nofollow\">MobileNetv2</a> papers. Both models are trained with a width_multiplier of 0.75. MobileNetv2 has an expansion factor of 6. This means that MAC wise, v2 is better than v1 (v1: 26.5 Mil, v2: 20.6Mil) and is expected to be slightly faster than v1.</p>\n<p>To compare how the models actually perform, I evaluated them in two ways<br>\nMethod 1: After Training - Inference by loading the models from checkpoints<br>\nMethod 2: During Deployment - Inference by loading the models from frozen GraphDefs (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py\">freeze_graph</a> for converting Variables to Consts)</p>\n<p><strong>Tools Used for Testing:</strong></p>\n<ul>\n<li>TF's Timeline Trace Tool</li>\n<li>benchmark_model Tool</li>\n<li>Naive Python time module</li>\n</ul>\n<h3>Timeline Tool</h3>\n<p>I used the TensorFlow's <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py\">timeline</a> tool to view the execution times in the Chrome Trace format.</p>\n<p><strong>Method 1:</strong><br>\nMobileNetv1 Trace</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7610546/43350531-b0ad3f8e-9225-11e8-95da-d21ac7cdc1e6.png\"><img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350531-b0ad3f8e-9225-11e8-95da-d21ac7cdc1e6.png\" style=\"max-width:100%;\"></a></p>\n<p>MobileNetv2 Trace</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7610546/43350534-bc7a644a-9225-11e8-8032-ec10bddf52a9.png\"><img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350534-bc7a644a-9225-11e8-8032-ec10bddf52a9.png\" style=\"max-width:100%;\"></a></p>\n<p>Eyeballing it, V2 is only slightly faster than V1; measuring from Conv1 to SoftMax,</p>\n<p>v1: 9.73 ms<br>\nv2: 8.153 ms</p>\n<p><strong>Method 2:</strong></p>\n<p>MobileNetv1 Trace</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7610546/43350556-ea35af48-9225-11e8-8f2d-56ccd7e600d3.png\"><img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350556-ea35af48-9225-11e8-8f2d-56ccd7e600d3.png\" style=\"max-width:100%;\"></a></p>\n<p>MobileNet v2 Trace</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7610546/43350562-f60ebda0-9225-11e8-992d-5c7ae66a5eb4.png\"><img width=\"1437\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350562-f60ebda0-9225-11e8-992d-5c7ae66a5eb4.png\" style=\"max-width:100%;\"></a></p>\n<p>As you can clearly see, when the models are loaded from a frozen format, v2 is slower than v1. This affects the performance timings for v2 especially while deployment since models are usually exported in the frozen format.</p>\n<h3>Time Measurements with Python's time module</h3>\n<p>Apart from the time traces above, I calculated the FPS by measuring the time between session.run. Even though the frozen models are faster than the checkpoint format, v2 is slower than v1 in the frozen format. Why is that so?</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Checkpoint - FPS</th>\n<th>Frozen (FPS)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MobileNetv1</td>\n<td>97.6</td>\n<td>254.86</td>\n</tr>\n<tr>\n<td>MobileNetv2</td>\n<td>159.22</td>\n<td>185.04</td>\n</tr>\n</tbody>\n</table>\n<h3>Benchmark Model Tool Results</h3>\n<p><strong>MobileNet_v1_75</strong></p>\n<p>Command Used:<br>\n<code>bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V1_75.pb --input_layer=\"input/Placeholder\" --input_layer_shape=\"1,64,64,3\" --input_layer_type=\"float\" --output_layer=\"output/Softmax_1\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true</code></p>\n<table>\n<thead>\n<tr>\n<th>[Node type]</th>\n<th>[count]</th>\n<th>[avg ms]</th>\n<th>[avg %]</th>\n<th>[cdf %]</th>\n<th>[mem KB]</th>\n<th>[times called]</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Conv2D</td>\n<td>15</td>\n<td>1.044</td>\n<td>21.20%</td>\n<td>21.20%</td>\n<td>8658.432</td>\n<td>15</td>\n</tr>\n<tr>\n<td>gpu:Conv2D</td>\n<td>15</td>\n<td>0.571</td>\n<td>11.60%</td>\n<td>32.80%</td>\n<td>0</td>\n<td>35</td>\n</tr>\n<tr>\n<td>BiasAdd</td>\n<td>28</td>\n<td>0.557</td>\n<td>11.31%</td>\n<td>44.11%</td>\n<td>0</td>\n<td>28</td>\n</tr>\n<tr>\n<td>Add</td>\n<td>27</td>\n<td>0.493</td>\n<td>10.01%</td>\n<td>54.12%</td>\n<td>0</td>\n<td>27</td>\n</tr>\n<tr>\n<td>Mul</td>\n<td>27</td>\n<td>0.48</td>\n<td>9.75%</td>\n<td>63.87%</td>\n<td>0</td>\n<td>27</td>\n</tr>\n<tr>\n<td>Relu6</td>\n<td>27</td>\n<td>0.414</td>\n<td>8.41%</td>\n<td>72.28%</td>\n<td>0</td>\n<td>27</td>\n</tr>\n<tr>\n<td>DepthwiseConv2dNative</td>\n<td>13</td>\n<td>0.392</td>\n<td>7.96%</td>\n<td>80.24%</td>\n<td>540.672</td>\n<td>13</td>\n</tr>\n<tr>\n<td>Const</td>\n<td>113</td>\n<td>0.37</td>\n<td>7.51%</td>\n<td>87.75%</td>\n<td>0</td>\n<td>113</td>\n</tr>\n<tr>\n<td>gpu:Mul</td>\n<td>27</td>\n<td>0.082</td>\n<td>1.67%</td>\n<td>89.42%</td>\n<td>0</td>\n<td>27</td>\n</tr>\n<tr>\n<td>gpu:Add</td>\n<td>27</td>\n<td>0.081</td>\n<td>1.65%</td>\n<td>91.06%</td>\n<td>0</td>\n<td>27</td>\n</tr>\n<tr>\n<td>gpu:DepthwiseConv2dNative</td>\n<td>13</td>\n<td>0.061</td>\n<td>1.24%</td>\n<td>92.30%</td>\n<td>0</td>\n<td>13</td>\n</tr>\n<tr>\n<td>NoOp</td>\n<td>1</td>\n<td>0.061</td>\n<td>1.24%</td>\n<td>93.54%</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>gpu:BiasAdd</td>\n<td>28</td>\n<td>0.059</td>\n<td>1.20%</td>\n<td>94.74%</td>\n<td>0</td>\n<td>28</td>\n</tr>\n<tr>\n<td>Transpose</td>\n<td>2</td>\n<td>0.055</td>\n<td>1.12%</td>\n<td>95.86%</td>\n<td>49.152</td>\n<td>2</td>\n</tr>\n<tr>\n<td>gpu:Relu6</td>\n<td>27</td>\n<td>0.054</td>\n<td>1.10%</td>\n<td>96.95%</td>\n<td>0</td>\n<td>27</td>\n</tr>\n<tr>\n<td>gpu:MEMCPYHtoD</td>\n<td>1</td>\n<td>0.043</td>\n<td>0.87%</td>\n<td>97.83%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Softmax</td>\n<td>1</td>\n<td>0.04</td>\n<td>0.81%</td>\n<td>98.64%</td>\n<td>0.512</td>\n<td>1</td>\n</tr>\n<tr>\n<td>AvgPool</td>\n<td>1</td>\n<td>0.033</td>\n<td>0.67%</td>\n<td>99.31%</td>\n<td>3.072</td>\n<td>1</td>\n</tr>\n<tr>\n<td>gpu:Softmax</td>\n<td>1</td>\n<td>0.009</td>\n<td>0.18%</td>\n<td>99.49%</td>\n<td>0</td>\n<td>3</td>\n</tr>\n<tr>\n<td>gpu:AvgPool</td>\n<td>1</td>\n<td>0.006</td>\n<td>0.12%</td>\n<td>99.61%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>_Arg</td>\n<td>1</td>\n<td>0.006</td>\n<td>0.12%</td>\n<td>99.74%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>gpu:Transpose</td>\n<td>1</td>\n<td>0.005</td>\n<td>0.10%</td>\n<td>99.84%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>_Retval</td>\n<td>1</td>\n<td>0.004</td>\n<td>0.08%</td>\n<td>99.92%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Reshape</td>\n<td>1</td>\n<td>0.003</td>\n<td>0.06%</td>\n<td>99.98%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>gpu:MEMCPYDtoH</td>\n<td>1</td>\n<td>0.001</td>\n<td>0.02%</td>\n<td>100.00%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Total</td>\n<td>\u00a0</td>\n<td>4.924</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p><strong>MobileNet_v2_75</strong></p>\n<p>Command Used:<br>\n<code>bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V2_75.pb --input_layer=\"input/Placeholder\" --input_layer_shape=\"1,64,64,3\" --input_layer_type=\"float\" --output_layer=\"output/Softmax_1\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true</code></p>\n<table>\n<thead>\n<tr>\n<th>[Node type]</th>\n<th>[count]</th>\n<th>[avg ms]</th>\n<th>[avg %]</th>\n<th>[cdf %]</th>\n<th>[mem KB]</th>\n<th>[times called]</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Conv2D</td>\n<td>39</td>\n<td>2.967</td>\n<td>37.97%</td>\n<td>37.97%</td>\n<td>7955.2</td>\n<td>39</td>\n</tr>\n<tr>\n<td>Add</td>\n<td>53</td>\n<td>1.217</td>\n<td>15.58%</td>\n<td>53.55%</td>\n<td>0</td>\n<td>53</td>\n</tr>\n<tr>\n<td>gpu:Conv2D</td>\n<td>39</td>\n<td>1.036</td>\n<td>13.26%</td>\n<td>66.80%</td>\n<td>0</td>\n<td>93</td>\n</tr>\n<tr>\n<td>Relu6</td>\n<td>36</td>\n<td>0.554</td>\n<td>7.09%</td>\n<td>73.89%</td>\n<td>0</td>\n<td>36</td>\n</tr>\n<tr>\n<td>DepthwiseConv2dNative</td>\n<td>17</td>\n<td>0.408</td>\n<td>5.22%</td>\n<td>79.11%</td>\n<td>959.232</td>\n<td>17</td>\n</tr>\n<tr>\n<td>Const</td>\n<td>129</td>\n<td>0.403</td>\n<td>5.16%</td>\n<td>84.27%</td>\n<td>0</td>\n<td>129</td>\n</tr>\n<tr>\n<td>Mul</td>\n<td>17</td>\n<td>0.304</td>\n<td>3.89%</td>\n<td>88.16%</td>\n<td>0</td>\n<td>17</td>\n</tr>\n<tr>\n<td>AddN</td>\n<td>12</td>\n<td>0.264</td>\n<td>3.38%</td>\n<td>91.54%</td>\n<td>0</td>\n<td>12</td>\n</tr>\n<tr>\n<td>gpu:Add</td>\n<td>53</td>\n<td>0.161</td>\n<td>2.06%</td>\n<td>93.60%</td>\n<td>0</td>\n<td>53</td>\n</tr>\n<tr>\n<td>NoOp</td>\n<td>1</td>\n<td>0.076</td>\n<td>0.97%</td>\n<td>94.57%</td>\n<td>0</td>\n<td>2</td>\n</tr>\n<tr>\n<td>gpu:Relu6</td>\n<td>36</td>\n<td>0.073</td>\n<td>0.93%</td>\n<td>95.51%</td>\n<td>0</td>\n<td>36</td>\n</tr>\n<tr>\n<td>gpu:DepthwiseConv2dNative</td>\n<td>17</td>\n<td>0.07</td>\n<td>0.90%</td>\n<td>96.40%</td>\n<td>0</td>\n<td>17</td>\n</tr>\n<tr>\n<td>Transpose</td>\n<td>2</td>\n<td>0.053</td>\n<td>0.68%</td>\n<td>97.08%</td>\n<td>49.152</td>\n<td>2</td>\n</tr>\n<tr>\n<td>gpu:Mul</td>\n<td>17</td>\n<td>0.052</td>\n<td>0.67%</td>\n<td>97.75%</td>\n<td>0</td>\n<td>17</td>\n</tr>\n<tr>\n<td>gpu:MEMCPYHtoD</td>\n<td>1</td>\n<td>0.044</td>\n<td>0.56%</td>\n<td>98.31%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Softmax</td>\n<td>1</td>\n<td>0.041</td>\n<td>0.53%</td>\n<td>98.84%</td>\n<td>0.512</td>\n<td>1</td>\n</tr>\n<tr>\n<td>AvgPool</td>\n<td>1</td>\n<td>0.031</td>\n<td>0.40%</td>\n<td>99.23%</td>\n<td>3.84</td>\n<td>1</td>\n</tr>\n<tr>\n<td>gpu:AddN</td>\n<td>12</td>\n<td>0.025</td>\n<td>0.32%</td>\n<td>99.55%</td>\n<td>0</td>\n<td>12</td>\n</tr>\n<tr>\n<td>gpu:Softmax</td>\n<td>1</td>\n<td>0.009</td>\n<td>0.12%</td>\n<td>99.67%</td>\n<td>0</td>\n<td>3</td>\n</tr>\n<tr>\n<td>gpu:AvgPool</td>\n<td>1</td>\n<td>0.007</td>\n<td>0.09%</td>\n<td>99.76%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>_Arg</td>\n<td>1</td>\n<td>0.006</td>\n<td>0.08%</td>\n<td>99.83%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>gpu:Transpose</td>\n<td>1</td>\n<td>0.005</td>\n<td>0.06%</td>\n<td>99.90%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>_Retval</td>\n<td>1</td>\n<td>0.004</td>\n<td>0.05%</td>\n<td>99.95%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Reshape</td>\n<td>1</td>\n<td>0.003</td>\n<td>0.04%</td>\n<td>99.99%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>gpu:MEMCPYDtoH</td>\n<td>1</td>\n<td>0.001</td>\n<td>0.01%</td>\n<td>100.00%</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Total</td>\n<td>\u00a0</td>\n<td>7.814</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td>\u00a0</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p><strong>Avg Time<br>\nv1: 4.924 ms<br>\nv2: 7.814 ms</strong></p>\n<h3>Source code / logs</h3>\n<p>How I load the model with method 1:</p>\n<pre><code>    def __load_model(self):\n        latest_checkpoint = tf.train.latest_checkpoint(self.args.checkpoint_dir)\n        if latest_checkpoint:\n            print(\"Loading model checkpoint {} ...\\n\".format(latest_checkpoint))\n            self.saver.restore(self.sess, latest_checkpoint)\n            print(\"Checkpoint loaded\\n\\n\")\n        else:\n            print(\"No checkpoints available!\\n\\n\")\n</code></pre>\n<p>How I load the model with method 2:</p>\n<pre><code>def create_graph(checkpoint_path):\n   with tf.gfile.FastGFile(checkpoint_path, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        _ = tf.import_graph_def(graph_def, name='')\n</code></pre>\n<p>I'm interested to know the details on why v2's performance is only half as good as v1 when in the paper it is discussed that v2 is supposed to be 35% faster.</p>\n<p>Note:<br>\nI've cross checked this with other hardware such as P40, an i5 7th Gen CPU and an alternative TF version (1.5.0) and this pattern is the same.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04.3 LTS\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nDoesn't apply\nTensorFlow installed from (source or binary):\nBinary\nTensorFlow version (use command below):\nv1.9.0-0-g25c197e023 1.9.0\nPython version:\nPython3\nBazel version (if compiling from source):\nDoesn't apply\nGCC/Compiler version (if compiling from source):\nDoesn't apply\nCUDA/cuDNN version:\nCUDA: 9.0 ; cuDNN: 7.1.4\nGPU model and memory:\nGeForce GTX 1080 Ti\nExact command to reproduce:\n\nDescribe the problem\nSummary\nMobileNet v2 is faster than v1 only when loading from the checkpoint format i.e, Variable Ops (meta, index, data) whereas it is slower than v1 when running in the frozen graph format (Const Ops) (.pb)\nDescription\nI have two TensorFlow trained models that are in the checkpoint format (meta, index, data) namely mobilenetv1_0.75.ckpt and mobilenetv2_0.75_6.ckpt.\nThe model definitions are as described in the MobileNetv1 and MobileNetv2 papers. Both models are trained with a width_multiplier of 0.75. MobileNetv2 has an expansion factor of 6. This means that MAC wise, v2 is better than v1 (v1: 26.5 Mil, v2: 20.6Mil) and is expected to be slightly faster than v1.\nTo compare how the models actually perform, I evaluated them in two ways\nMethod 1: After Training - Inference by loading the models from checkpoints\nMethod 2: During Deployment - Inference by loading the models from frozen GraphDefs (freeze_graph for converting Variables to Consts)\nTools Used for Testing:\n\nTF's Timeline Trace Tool\nbenchmark_model Tool\nNaive Python time module\n\nTimeline Tool\nI used the TensorFlow's timeline tool to view the execution times in the Chrome Trace format.\nMethod 1:\nMobileNetv1 Trace\n\nMobileNetv2 Trace\n\nEyeballing it, V2 is only slightly faster than V1; measuring from Conv1 to SoftMax,\nv1: 9.73 ms\nv2: 8.153 ms\nMethod 2:\nMobileNetv1 Trace\n\nMobileNet v2 Trace\n\nAs you can clearly see, when the models are loaded from a frozen format, v2 is slower than v1. This affects the performance timings for v2 especially while deployment since models are usually exported in the frozen format.\nTime Measurements with Python's time module\nApart from the time traces above, I calculated the FPS by measuring the time between session.run. Even though the frozen models are faster than the checkpoint format, v2 is slower than v1 in the frozen format. Why is that so?\n\n\n\nModel\nCheckpoint - FPS\nFrozen (FPS)\n\n\n\n\nMobileNetv1\n97.6\n254.86\n\n\nMobileNetv2\n159.22\n185.04\n\n\n\nBenchmark Model Tool Results\nMobileNet_v1_75\nCommand Used:\nbazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V1_75.pb --input_layer=\"input/Placeholder\" --input_layer_shape=\"1,64,64,3\" --input_layer_type=\"float\" --output_layer=\"output/Softmax_1\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true\n\n\n\n[Node type]\n[count]\n[avg ms]\n[avg %]\n[cdf %]\n[mem KB]\n[times called]\n\n\n\n\nConv2D\n15\n1.044\n21.20%\n21.20%\n8658.432\n15\n\n\ngpu:Conv2D\n15\n0.571\n11.60%\n32.80%\n0\n35\n\n\nBiasAdd\n28\n0.557\n11.31%\n44.11%\n0\n28\n\n\nAdd\n27\n0.493\n10.01%\n54.12%\n0\n27\n\n\nMul\n27\n0.48\n9.75%\n63.87%\n0\n27\n\n\nRelu6\n27\n0.414\n8.41%\n72.28%\n0\n27\n\n\nDepthwiseConv2dNative\n13\n0.392\n7.96%\n80.24%\n540.672\n13\n\n\nConst\n113\n0.37\n7.51%\n87.75%\n0\n113\n\n\ngpu:Mul\n27\n0.082\n1.67%\n89.42%\n0\n27\n\n\ngpu:Add\n27\n0.081\n1.65%\n91.06%\n0\n27\n\n\ngpu:DepthwiseConv2dNative\n13\n0.061\n1.24%\n92.30%\n0\n13\n\n\nNoOp\n1\n0.061\n1.24%\n93.54%\n0\n2\n\n\ngpu:BiasAdd\n28\n0.059\n1.20%\n94.74%\n0\n28\n\n\nTranspose\n2\n0.055\n1.12%\n95.86%\n49.152\n2\n\n\ngpu:Relu6\n27\n0.054\n1.10%\n96.95%\n0\n27\n\n\ngpu:MEMCPYHtoD\n1\n0.043\n0.87%\n97.83%\n0\n1\n\n\nSoftmax\n1\n0.04\n0.81%\n98.64%\n0.512\n1\n\n\nAvgPool\n1\n0.033\n0.67%\n99.31%\n3.072\n1\n\n\ngpu:Softmax\n1\n0.009\n0.18%\n99.49%\n0\n3\n\n\ngpu:AvgPool\n1\n0.006\n0.12%\n99.61%\n0\n1\n\n\n_Arg\n1\n0.006\n0.12%\n99.74%\n0\n1\n\n\ngpu:Transpose\n1\n0.005\n0.10%\n99.84%\n0\n1\n\n\n_Retval\n1\n0.004\n0.08%\n99.92%\n0\n1\n\n\nReshape\n1\n0.003\n0.06%\n99.98%\n0\n1\n\n\ngpu:MEMCPYDtoH\n1\n0.001\n0.02%\n100.00%\n0\n1\n\n\nTotal\n\u00a0\n4.924\n\u00a0\n\u00a0\n\u00a0\n\n\n\n\nMobileNet_v2_75\nCommand Used:\nbazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V2_75.pb --input_layer=\"input/Placeholder\" --input_layer_shape=\"1,64,64,3\" --input_layer_type=\"float\" --output_layer=\"output/Softmax_1\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true\n\n\n\n[Node type]\n[count]\n[avg ms]\n[avg %]\n[cdf %]\n[mem KB]\n[times called]\n\n\n\n\nConv2D\n39\n2.967\n37.97%\n37.97%\n7955.2\n39\n\n\nAdd\n53\n1.217\n15.58%\n53.55%\n0\n53\n\n\ngpu:Conv2D\n39\n1.036\n13.26%\n66.80%\n0\n93\n\n\nRelu6\n36\n0.554\n7.09%\n73.89%\n0\n36\n\n\nDepthwiseConv2dNative\n17\n0.408\n5.22%\n79.11%\n959.232\n17\n\n\nConst\n129\n0.403\n5.16%\n84.27%\n0\n129\n\n\nMul\n17\n0.304\n3.89%\n88.16%\n0\n17\n\n\nAddN\n12\n0.264\n3.38%\n91.54%\n0\n12\n\n\ngpu:Add\n53\n0.161\n2.06%\n93.60%\n0\n53\n\n\nNoOp\n1\n0.076\n0.97%\n94.57%\n0\n2\n\n\ngpu:Relu6\n36\n0.073\n0.93%\n95.51%\n0\n36\n\n\ngpu:DepthwiseConv2dNative\n17\n0.07\n0.90%\n96.40%\n0\n17\n\n\nTranspose\n2\n0.053\n0.68%\n97.08%\n49.152\n2\n\n\ngpu:Mul\n17\n0.052\n0.67%\n97.75%\n0\n17\n\n\ngpu:MEMCPYHtoD\n1\n0.044\n0.56%\n98.31%\n0\n1\n\n\nSoftmax\n1\n0.041\n0.53%\n98.84%\n0.512\n1\n\n\nAvgPool\n1\n0.031\n0.40%\n99.23%\n3.84\n1\n\n\ngpu:AddN\n12\n0.025\n0.32%\n99.55%\n0\n12\n\n\ngpu:Softmax\n1\n0.009\n0.12%\n99.67%\n0\n3\n\n\ngpu:AvgPool\n1\n0.007\n0.09%\n99.76%\n0\n1\n\n\n_Arg\n1\n0.006\n0.08%\n99.83%\n0\n1\n\n\ngpu:Transpose\n1\n0.005\n0.06%\n99.90%\n0\n1\n\n\n_Retval\n1\n0.004\n0.05%\n99.95%\n0\n1\n\n\nReshape\n1\n0.003\n0.04%\n99.99%\n0\n1\n\n\ngpu:MEMCPYDtoH\n1\n0.001\n0.01%\n100.00%\n0\n1\n\n\nTotal\n\u00a0\n7.814\n\u00a0\n\u00a0\n\u00a0\n\n\n\n\nAvg Time\nv1: 4.924 ms\nv2: 7.814 ms\nSource code / logs\nHow I load the model with method 1:\n    def __load_model(self):\n        latest_checkpoint = tf.train.latest_checkpoint(self.args.checkpoint_dir)\n        if latest_checkpoint:\n            print(\"Loading model checkpoint {} ...\\n\".format(latest_checkpoint))\n            self.saver.restore(self.sess, latest_checkpoint)\n            print(\"Checkpoint loaded\\n\\n\")\n        else:\n            print(\"No checkpoints available!\\n\\n\")\n\nHow I load the model with method 2:\ndef create_graph(checkpoint_path):\n   with tf.gfile.FastGFile(checkpoint_path, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        _ = tf.import_graph_def(graph_def, name='')\n\nI'm interested to know the details on why v2's performance is only half as good as v1 when in the paper it is discussed that v2 is supposed to be 35% faster.\nNote:\nI've cross checked this with other hardware such as P40, an i5 7th Gen CPU and an alternative TF version (1.5.0) and this pattern is the same.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04.3 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nDoesn't apply\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**:\r\nPython3\r\n- **Bazel version (if compiling from source)**:\r\nDoesn't apply\r\n- **GCC/Compiler version (if compiling from source)**:\r\nDoesn't apply\r\n- **CUDA/cuDNN version**:\r\nCUDA: 9.0 ; cuDNN: 7.1.4\r\n- **GPU model and memory**:\r\nGeForce GTX 1080 Ti\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n### Summary\r\nMobileNet v2 is faster than v1 only when loading from the checkpoint format i.e, Variable Ops (meta, index, data) whereas it is slower than v1 when running in the frozen graph format (Const Ops) (.pb)\r\n\r\n### Description\r\nI have two TensorFlow trained models that are in the checkpoint format (meta, index, data) namely mobilenetv1_0.75.ckpt and mobilenetv2_0.75_6.ckpt. \r\n\r\nThe model definitions are as described in the [MobileNetv1](https://arxiv.org/abs/1704.04861) and [MobileNetv2](https://arxiv.org/pdf/1801.04381.pdf) papers. Both models are trained with a width_multiplier of 0.75. MobileNetv2 has an expansion factor of 6. This means that MAC wise, v2 is better than v1 (v1: 26.5 Mil, v2: 20.6Mil) and is expected to be slightly faster than v1.\r\n\r\nTo compare how the models actually perform, I evaluated them in two ways\r\nMethod 1: After Training - Inference by loading the models from checkpoints\r\nMethod 2: During Deployment - Inference by loading the models from frozen GraphDefs ([freeze_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) for converting Variables to Consts)\r\n\r\n**Tools Used for Testing:**\r\n- TF's Timeline Trace Tool\r\n- benchmark_model Tool\r\n- Naive Python time module\r\n\r\n### Timeline Tool\r\nI used the TensorFlow's [timeline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py) tool to view the execution times in the Chrome Trace format. \r\n\r\n**Method 1:** \r\nMobileNetv1 Trace\r\n\r\n<img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350531-b0ad3f8e-9225-11e8-95da-d21ac7cdc1e6.png\">\r\n\r\nMobileNetv2 Trace\r\n\r\n<img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350534-bc7a644a-9225-11e8-8032-ec10bddf52a9.png\">\r\n\r\nEyeballing it, V2 is only slightly faster than V1; measuring from Conv1 to SoftMax,\r\n\r\nv1: 9.73 ms\r\nv2: 8.153 ms\r\n\r\n**Method 2:**\r\n\r\nMobileNetv1 Trace\r\n\r\n<img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350556-ea35af48-9225-11e8-8f2d-56ccd7e600d3.png\">\r\n\r\nMobileNet v2 Trace\r\n\r\n<img width=\"1437\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350562-f60ebda0-9225-11e8-992d-5c7ae66a5eb4.png\">\r\n\r\nAs you can clearly see, when the models are loaded from a frozen format, v2 is slower than v1. This affects the performance timings for v2 especially while deployment since models are usually exported in the frozen format.\r\n\r\n\r\n### Time Measurements with Python's time module\r\n\r\nApart from the time traces above, I calculated the FPS by measuring the time between session.run. Even though the frozen models are faster than the checkpoint format, v2 is slower than v1 in the frozen format. Why is that so?\r\n\r\nModel | Checkpoint - FPS | Frozen (FPS)\r\n-- | -- | --\r\nMobileNetv1 | 97.6 | 254.86\r\nMobileNetv2 | 159.22 | 185.04\r\n\r\n\r\n### Benchmark Model Tool Results\r\n\r\n\r\n**MobileNet_v1_75**\r\n\r\nCommand Used:\r\n`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V1_75.pb --input_layer=\"input/Placeholder\" --input_layer_shape=\"1,64,64,3\" --input_layer_type=\"float\" --output_layer=\"output/Softmax_1\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true`\r\n\r\n[Node type] | [count] | [avg ms] | [avg %] | [cdf %] | [mem KB] | [times called]\r\n-- | -- | -- | -- | -- | -- | --\r\nConv2D | 15 | 1.044 | 21.20% | 21.20% | 8658.432 | 15\r\ngpu:Conv2D | 15 | 0.571 | 11.60% | 32.80% | 0 | 35\r\nBiasAdd | 28 | 0.557 | 11.31% | 44.11% | 0 | 28\r\nAdd | 27 | 0.493 | 10.01% | 54.12% | 0 | 27\r\nMul | 27 | 0.48 | 9.75% | 63.87% | 0 | 27\r\nRelu6 | 27 | 0.414 | 8.41% | 72.28% | 0 | 27\r\nDepthwiseConv2dNative | 13 | 0.392 | 7.96% | 80.24% | 540.672 | 13\r\nConst | 113 | 0.37 | 7.51% | 87.75% | 0 | 113\r\ngpu:Mul | 27 | 0.082 | 1.67% | 89.42% | 0 | 27\r\ngpu:Add | 27 | 0.081 | 1.65% | 91.06% | 0 | 27\r\ngpu:DepthwiseConv2dNative | 13 | 0.061 | 1.24% | 92.30% | 0 | 13\r\nNoOp | 1 | 0.061 | 1.24% | 93.54% | 0 | 2\r\ngpu:BiasAdd | 28 | 0.059 | 1.20% | 94.74% | 0 | 28\r\nTranspose | 2 | 0.055 | 1.12% | 95.86% | 49.152 | 2\r\ngpu:Relu6 | 27 | 0.054 | 1.10% | 96.95% | 0 | 27\r\ngpu:MEMCPYHtoD | 1 | 0.043 | 0.87% | 97.83% | 0 | 1\r\nSoftmax | 1 | 0.04 | 0.81% | 98.64% | 0.512 | 1\r\nAvgPool | 1 | 0.033 | 0.67% | 99.31% | 3.072 | 1\r\ngpu:Softmax | 1 | 0.009 | 0.18% | 99.49% | 0 | 3\r\ngpu:AvgPool | 1 | 0.006 | 0.12% | 99.61% | 0 | 1\r\n_Arg | 1 | 0.006 | 0.12% | 99.74% | 0 | 1\r\ngpu:Transpose | 1 | 0.005 | 0.10% | 99.84% | 0 | 1\r\n_Retval | 1 | 0.004 | 0.08% | 99.92% | 0 | 1\r\nReshape | 1 | 0.003 | 0.06% | 99.98% | 0 | 1\r\ngpu:MEMCPYDtoH | 1 | 0.001 | 0.02% | 100.00% | 0 | 1\r\nTotal | \u00a0 | 4.924 | \u00a0 | \u00a0 | \u00a0\r\n\r\n**MobileNet_v2_75**\r\n\r\nCommand Used:\r\n`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V2_75.pb --input_layer=\"input/Placeholder\" --input_layer_shape=\"1,64,64,3\" --input_layer_type=\"float\" --output_layer=\"output/Softmax_1\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true`\r\n\r\n[Node type] | [count] | [avg ms] | [avg %] | [cdf %] | [mem KB] | [times called]\r\n-- | -- | -- | -- | -- | -- | --\r\nConv2D | 39 | 2.967 | 37.97% | 37.97% | 7955.2 | 39\r\nAdd | 53 | 1.217 | 15.58% | 53.55% | 0 | 53\r\ngpu:Conv2D | 39 | 1.036 | 13.26% | 66.80% | 0 | 93\r\nRelu6 | 36 | 0.554 | 7.09% | 73.89% | 0 | 36\r\nDepthwiseConv2dNative | 17 | 0.408 | 5.22% | 79.11% | 959.232 | 17\r\nConst | 129 | 0.403 | 5.16% | 84.27% | 0 | 129\r\nMul | 17 | 0.304 | 3.89% | 88.16% | 0 | 17\r\nAddN | 12 | 0.264 | 3.38% | 91.54% | 0 | 12\r\ngpu:Add | 53 | 0.161 | 2.06% | 93.60% | 0 | 53\r\nNoOp | 1 | 0.076 | 0.97% | 94.57% | 0 | 2\r\ngpu:Relu6 | 36 | 0.073 | 0.93% | 95.51% | 0 | 36\r\ngpu:DepthwiseConv2dNative | 17 | 0.07 | 0.90% | 96.40% | 0 | 17\r\nTranspose | 2 | 0.053 | 0.68% | 97.08% | 49.152 | 2\r\ngpu:Mul | 17 | 0.052 | 0.67% | 97.75% | 0 | 17\r\ngpu:MEMCPYHtoD | 1 | 0.044 | 0.56% | 98.31% | 0 | 1\r\nSoftmax | 1 | 0.041 | 0.53% | 98.84% | 0.512 | 1\r\nAvgPool | 1 | 0.031 | 0.40% | 99.23% | 3.84 | 1\r\ngpu:AddN | 12 | 0.025 | 0.32% | 99.55% | 0 | 12\r\ngpu:Softmax | 1 | 0.009 | 0.12% | 99.67% | 0 | 3\r\ngpu:AvgPool | 1 | 0.007 | 0.09% | 99.76% | 0 | 1\r\n_Arg | 1 | 0.006 | 0.08% | 99.83% | 0 | 1\r\ngpu:Transpose | 1 | 0.005 | 0.06% | 99.90% | 0 | 1\r\n_Retval | 1 | 0.004 | 0.05% | 99.95% | 0 | 1\r\nReshape | 1 | 0.003 | 0.04% | 99.99% | 0 | 1\r\ngpu:MEMCPYDtoH | 1 | 0.001 | 0.01% | 100.00% | 0 | 1\r\nTotal | \u00a0 | 7.814 | \u00a0 | \u00a0 | \u00a0\r\n\r\n\r\n**Avg Time\r\nv1: 4.924 ms\r\nv2: 7.814 ms**\r\n\r\n### Source code / logs\r\n\r\nHow I load the model with method 1:\r\n\r\n```\r\n    def __load_model(self):\r\n        latest_checkpoint = tf.train.latest_checkpoint(self.args.checkpoint_dir)\r\n        if latest_checkpoint:\r\n            print(\"Loading model checkpoint {} ...\\n\".format(latest_checkpoint))\r\n            self.saver.restore(self.sess, latest_checkpoint)\r\n            print(\"Checkpoint loaded\\n\\n\")\r\n        else:\r\n            print(\"No checkpoints available!\\n\\n\")\r\n```\r\n\r\nHow I load the model with method 2:\r\n\r\n```\r\ndef create_graph(checkpoint_path):\r\n   with tf.gfile.FastGFile(checkpoint_path, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        _ = tf.import_graph_def(graph_def, name='')\r\n```\r\n\r\nI'm interested to know the details on why v2's performance is only half as good as v1 when in the paper it is discussed that v2 is supposed to be 35% faster.\r\n\r\nNote:\r\nI've cross checked this with other hardware such as P40, an i5 7th Gen CPU and an alternative TF version (1.5.0) and this pattern is the same."}