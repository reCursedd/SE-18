{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/350396437", "html_url": "https://github.com/tensorflow/tensorflow/issues/15190#issuecomment-350396437", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15190", "id": 350396437, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDM5NjQzNw==", "user": {"login": "thunterdb", "id": 7594753, "node_id": "MDQ6VXNlcjc1OTQ3NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/7594753?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thunterdb", "html_url": "https://github.com/thunterdb", "followers_url": "https://api.github.com/users/thunterdb/followers", "following_url": "https://api.github.com/users/thunterdb/following{/other_user}", "gists_url": "https://api.github.com/users/thunterdb/gists{/gist_id}", "starred_url": "https://api.github.com/users/thunterdb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thunterdb/subscriptions", "organizations_url": "https://api.github.com/users/thunterdb/orgs", "repos_url": "https://api.github.com/users/thunterdb/repos", "events_url": "https://api.github.com/users/thunterdb/events{/privacy}", "received_events_url": "https://api.github.com/users/thunterdb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-08T23:18:42Z", "updated_at": "2017-12-08T23:18:42Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> I updated the gist following your suggestion. It failed after ~ 10M reads on S3 (a few hours) with the following stack trace.</p>\n<p>Has anyone else encountered this issue before? Our current workaround is to copy the data locally to the machines' drives before starting the training, but this adds some complexity.</p>\n<pre><code>  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 443, in mainloop\n    self.interact(display_banner=display_banner)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 567, in interact\n    self.run_cell(source_raw, store_history=True)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n    interactivity=interactivity, compiler=compiler)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2833, in run_ast_nodes\n    if self.run_code(code):\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-8-201cc217ea8a&gt;\", line 1, in &lt;module&gt;\n    test_s3_read_singlemachine(worker_index=0, num_workers=2, data_dir=data_dir, num_readers=2, shuffle_buffer_size=32, batch_size=32, num_prefetch_batches=1, num_steps=int(1e7))\n  File \"&lt;ipython-input-7-58d40f4ce8d4&gt;\", line 15, in test_s3_read_singlemachine\n    it_op = iterator.get_next()\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 259, in get_next\n    name=name))\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 706, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): truncated record at 785862\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n</code></pre>", "body_text": "@mrry I updated the gist following your suggestion. It failed after ~ 10M reads on S3 (a few hours) with the following stack trace.\nHas anyone else encountered this issue before? Our current workaround is to copy the data locally to the machines' drives before starting the training, but this adds some complexity.\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 443, in mainloop\n    self.interact(display_banner=display_banner)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 567, in interact\n    self.run_cell(source_raw, store_history=True)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n    interactivity=interactivity, compiler=compiler)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2833, in run_ast_nodes\n    if self.run_code(code):\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-201cc217ea8a>\", line 1, in <module>\n    test_s3_read_singlemachine(worker_index=0, num_workers=2, data_dir=data_dir, num_readers=2, shuffle_buffer_size=32, batch_size=32, num_prefetch_batches=1, num_steps=int(1e7))\n  File \"<ipython-input-7-58d40f4ce8d4>\", line 15, in test_s3_read_singlemachine\n    it_op = iterator.get_next()\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 259, in get_next\n    name=name))\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 706, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): truncated record at 785862\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]", "body": "@mrry I updated the gist following your suggestion. It failed after ~ 10M reads on S3 (a few hours) with the following stack trace.\r\n\r\nHas anyone else encountered this issue before? Our current workaround is to copy the data locally to the machines' drives before starting the training, but this adds some complexity.\r\n\r\n```\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 443, in mainloop\r\n    self.interact(display_banner=display_banner)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 567, in interact\r\n    self.run_cell(source_raw, store_history=True)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\r\n    interactivity=interactivity, compiler=compiler)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2833, in run_ast_nodes\r\n    if self.run_code(code):\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-201cc217ea8a>\", line 1, in <module>\r\n    test_s3_read_singlemachine(worker_index=0, num_workers=2, data_dir=data_dir, num_readers=2, shuffle_buffer_size=32, batch_size=32, num_prefetch_batches=1, num_steps=int(1e7))\r\n  File \"<ipython-input-7-58d40f4ce8d4>\", line 15, in test_s3_read_singlemachine\r\n    it_op = iterator.get_next()\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 259, in get_next\r\n    name=name))\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 706, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/databricks/python2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nDataLossError (see above for traceback): truncated record at 785862\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n```"}