{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5289", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5289/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5289/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5289/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5289", "id": 186160624, "node_id": "MDU6SXNzdWUxODYxNjA2MjQ=", "number": 5289, "title": "TF freezes and gets killed while training /saving a network", "user": {"login": "deeptigp", "id": 2092933, "node_id": "MDQ6VXNlcjIwOTI5MzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/2092933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deeptigp", "html_url": "https://github.com/deeptigp", "followers_url": "https://api.github.com/users/deeptigp/followers", "following_url": "https://api.github.com/users/deeptigp/following{/other_user}", "gists_url": "https://api.github.com/users/deeptigp/gists{/gist_id}", "starred_url": "https://api.github.com/users/deeptigp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deeptigp/subscriptions", "organizations_url": "https://api.github.com/users/deeptigp/orgs", "repos_url": "https://api.github.com/users/deeptigp/repos", "events_url": "https://api.github.com/users/deeptigp/events{/privacy}", "received_events_url": "https://api.github.com/users/deeptigp/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2016-10-30T20:25:16Z", "updated_at": "2017-03-03T23:39:27Z", "closed_at": "2017-03-03T23:39:27Z", "author_association": "NONE", "body_html": "<p>I am trying to train a deep network from scratch (a 4 layer <a href=\"https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html#cifar-10-model\" rel=\"nofollow\">CIFAR</a> network) on an image collection of 100K images. The TF instance hangs (while training or while saving using tf.Saver) and then gets killed without any error message.</p>\n<p>I've tried the following things without any use:</p>\n<p>a. Reduced the batch size from 32 to 8.</p>\n<p>b. Set config's allow GPU growth option to True</p>\n<p>But the problem still persists.</p>\n<p>Has anybody else faced this issue? Is this because of insufficient memory? Is there a way to train a model under constrained memory conditions (although, 12 GB isn't bad)?<br>\nAny tips to avoid this would be very helpful.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>I've looked at other similar issues posted but haven't found any useful solution.<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"151269940\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2121\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2121/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2121\">#2121</a><br>\n<a href=\"http://stackoverflow.com/questions/38958737/tensorflow-training-got-stuck-after-some-steps-how-to-investigate\" rel=\"nofollow\">http://stackoverflow.com/questions/38958737/tensorflow-training-got-stuck-after-some-steps-how-to-investigate</a><br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"148589163\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1962\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1962/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1962\">#1962</a></p>\n<h3>Environment info</h3>\n<p>GPU details: I am running this model on a  Tesla K40c (12GB memory).<br>\nOperating System:  4.7.0-1-amd64 <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a> SMP Debian 4.7.6-1 (2016-10-07) x86_64 GNU/Linux</p>\n<p>Installed version of CUDA and cuDNN:<br>\n/opt/cuda-8.0/lib64/libcudnn.so.5<br>\n/opt/cuda-8.0/lib64/libcudart.so -&gt; libcudart.so.8.0</p>\n<p>(Cuda version: 8.0 and cuDNN version 5)</p>\n<ol>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.<br>\n0.11.0rc1</li>\n</ol>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/ec7f37e40fedb23435bfb7e28668e5fa63ff52f3/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/ec7f37e40fedb23435bfb7e28668e5fa63ff52f3\"><tt>ec7f37e</tt></a></li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<p>Build label: 0.3.2<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)<br>\nBuild timestamp: 1475861110<br>\nBuild timestamp as int: 1475861110</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>This issue is happening when I am trying to train/ save a model</p>", "body_text": "I am trying to train a deep network from scratch (a 4 layer CIFAR network) on an image collection of 100K images. The TF instance hangs (while training or while saving using tf.Saver) and then gets killed without any error message.\nI've tried the following things without any use:\na. Reduced the batch size from 32 to 8.\nb. Set config's allow GPU growth option to True\nBut the problem still persists.\nHas anybody else faced this issue? Is this because of insufficient memory? Is there a way to train a model under constrained memory conditions (although, 12 GB isn't bad)?\nAny tips to avoid this would be very helpful.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nI've looked at other similar issues posted but haven't found any useful solution.\n#2121\nhttp://stackoverflow.com/questions/38958737/tensorflow-training-got-stuck-after-some-steps-how-to-investigate\n#1962\nEnvironment info\nGPU details: I am running this model on a  Tesla K40c (12GB memory).\nOperating System:  4.7.0-1-amd64 #1 SMP Debian 4.7.6-1 (2016-10-07) x86_64 GNU/Linux\nInstalled version of CUDA and cuDNN:\n/opt/cuda-8.0/lib64/libcudnn.so.5\n/opt/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\n(Cuda version: 8.0 and cuDNN version 5)\n\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n0.11.0rc1\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nec7f37e\nThe output of bazel version\n\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nThis issue is happening when I am trying to train/ save a model", "body": "I am trying to train a deep network from scratch (a 4 layer [CIFAR](https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html#cifar-10-model) network) on an image collection of 100K images. The TF instance hangs (while training or while saving using tf.Saver) and then gets killed without any error message.\n\nI've tried the following things without any use:\n\na. Reduced the batch size from 32 to 8.\n\nb. Set config's allow GPU growth option to True\n\nBut the problem still persists. \n\nHas anybody else faced this issue? Is this because of insufficient memory? Is there a way to train a model under constrained memory conditions (although, 12 GB isn't bad)? \n Any tips to avoid this would be very helpful. \n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI've looked at other similar issues posted but haven't found any useful solution.\nhttps://github.com/tensorflow/tensorflow/issues/2121\nhttp://stackoverflow.com/questions/38958737/tensorflow-training-got-stuck-after-some-steps-how-to-investigate\nhttps://github.com/tensorflow/tensorflow/issues/1962\n### Environment info\n\nGPU details: I am running this model on a  Tesla K40c (12GB memory).\nOperating System:  4.7.0-1-amd64 #1 SMP Debian 4.7.6-1 (2016-10-07) x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \n/opt/cuda-8.0/lib64/libcudnn.so.5\n/opt/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\n\n(Cuda version: 8.0 and cuDNN version 5)\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.11.0rc1\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   ec7f37e40fedb23435bfb7e28668e5fa63ff52f3\n2. The output of `bazel version`\n\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nThis issue is happening when I am trying to train/ save a model\n"}