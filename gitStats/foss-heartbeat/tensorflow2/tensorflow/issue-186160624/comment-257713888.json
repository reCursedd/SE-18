{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/257713888", "html_url": "https://github.com/tensorflow/tensorflow/issues/5289#issuecomment-257713888", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5289", "id": 257713888, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NzcxMzg4OA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-01T22:08:19Z", "updated_at": "2016-11-01T22:08:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4693141\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nroth1\">@nroth1</a> -- a common way for \"random hangs\" is by causing a deadlock with<br>\nqueues, partial solution  is to set operation timeout as <a href=\"https://github.com/tensorflow/tensorflow/issues/2130#issuecomment-215180165\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2130/hovercard\">here</a><br>\nand retry on error. More fundamentally is to analyze your queues and<br>\nprevent deadlock from occuring, as <a href=\"https://github.com/tensorflow/tensorflow/issues/4917\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4917/hovercard\">here</a></p>\n<p>On Tue, Nov 1, 2016 at 2:45 PM, nroth1 <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p>I am seeing what I think is a similar issue, but am only training on CPU;<br>\nwhen I sample the process in a hung state, I see get the following info. In<br>\nmy case, I just see an indefinite hang. Seems to happen randomly, but<br>\nconsistently if I run the program for a few hours, unfortunately. I am also<br>\non version, 0.11.0rc0, if that matters.</p>\n<p>tflow_hang.txt<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/565125/tflow_hang.txt\">https://github.com/tensorflow/tensorflow/files/565125/tflow_hang.txt</a></p>\n<p>\u2014<br>\nYou are receiving this because you commented.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"186160624\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5289\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/5289/hovercard?comment_id=257708344&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/5289#issuecomment-257708344\">#5289 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/AABaHBm5M5lHsKvIUr-8rrAXgy2CARPhks5q57LjgaJpZM4KkfA-\">https://github.com/notifications/unsubscribe-auth/AABaHBm5M5lHsKvIUr-8rrAXgy2CARPhks5q57LjgaJpZM4KkfA-</a><br>\n.</p>\n</blockquote>", "body_text": "@nroth1 -- a common way for \"random hangs\" is by causing a deadlock with\nqueues, partial solution  is to set operation timeout as here\nand retry on error. More fundamentally is to analyze your queues and\nprevent deadlock from occuring, as here\nOn Tue, Nov 1, 2016 at 2:45 PM, nroth1 notifications@github.com wrote:\n\nI am seeing what I think is a similar issue, but am only training on CPU;\nwhen I sample the process in a hung state, I see get the following info. In\nmy case, I just see an indefinite hang. Seems to happen randomly, but\nconsistently if I run the program for a few hours, unfortunately. I am also\non version, 0.11.0rc0, if that matters.\ntflow_hang.txt\nhttps://github.com/tensorflow/tensorflow/files/565125/tflow_hang.txt\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n#5289 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABaHBm5M5lHsKvIUr-8rrAXgy2CARPhks5q57LjgaJpZM4KkfA-\n.", "body": "@nroth1 -- a common way for \"random hangs\" is by causing a deadlock with\nqueues, partial solution  is to set operation timeout as [here](https://github.com/tensorflow/tensorflow/issues/2130#issuecomment-215180165)\nand retry on error. More fundamentally is to analyze your queues and\nprevent deadlock from occuring, as [here](https://github.com/tensorflow/tensorflow/issues/4917)\n\nOn Tue, Nov 1, 2016 at 2:45 PM, nroth1 notifications@github.com wrote:\n\n> I am seeing what I think is a similar issue, but am only training on CPU;\n> when I sample the process in a hung state, I see get the following info. In\n> my case, I just see an indefinite hang. Seems to happen randomly, but\n> consistently if I run the program for a few hours, unfortunately. I am also\n> on version, 0.11.0rc0, if that matters.\n> \n> tflow_hang.txt\n> https://github.com/tensorflow/tensorflow/files/565125/tflow_hang.txt\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5289#issuecomment-257708344,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHBm5M5lHsKvIUr-8rrAXgy2CARPhks5q57LjgaJpZM4KkfA-\n> .\n"}