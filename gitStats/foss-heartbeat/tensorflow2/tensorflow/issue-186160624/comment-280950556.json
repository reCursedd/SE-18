{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280950556", "html_url": "https://github.com/tensorflow/tensorflow/issues/5289#issuecomment-280950556", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5289", "id": 280950556, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDk1MDU1Ng==", "user": {"login": "nroth1", "id": 4693141, "node_id": "MDQ6VXNlcjQ2OTMxNDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4693141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nroth1", "html_url": "https://github.com/nroth1", "followers_url": "https://api.github.com/users/nroth1/followers", "following_url": "https://api.github.com/users/nroth1/following{/other_user}", "gists_url": "https://api.github.com/users/nroth1/gists{/gist_id}", "starred_url": "https://api.github.com/users/nroth1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nroth1/subscriptions", "organizations_url": "https://api.github.com/users/nroth1/orgs", "repos_url": "https://api.github.com/users/nroth1/repos", "events_url": "https://api.github.com/users/nroth1/events{/privacy}", "received_events_url": "https://api.github.com/users/nroth1/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-19T21:26:59Z", "updated_at": "2017-02-19T21:26:59Z", "author_association": "NONE", "body_html": "<p>I'm able to reproduce the hangs on a smaller example, and have logs attached. I'm not sure if this is necessarily the same hang as I was seeing above, as the freeze seems to happen in a numpy call, not at the tensorflow level. The example is pretty silly, but I basically just make and evaluate two graphs, while saving them in the background on separate threads. Eventually, it hangs at:</p>\n<p>***  File: \"crash_eg.py\", line 93, in   main_thread2()<br>\n***  File: \"crash_eg.py\", line 79, in main_thread2  session2.run(outputs2,feed_dict = fd)<br>\n***  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run  run_metadata_ptr)<br>\n***  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 938, in _run  np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)<br>\n***  File: \"/usr/local/lib/python2.7/site-packages/numpy/core/numeric.py\", line 531, in asarray  return array(a, dtype, copy=False, order=order)</p>\n<p>I have attached the full logs below. Need to investigate more about whether this is the same issue as reported above, but thought I would leave the info just in case. I should mention I am on a mac, and have read that there can be some issues with numpy and threading on Mac OS, so maybe that is involved. I am not sure if anyone else has ever seen a similar issue? Thanks again for all your time!</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/786206/hang_logs.txt\">hang_logs.txt</a></p>\n<pre><code>import tensorflow as tf\nimport threading\nimport numpy as np\nimport time\nimport sys\nimport traceback\n\ng1 = tf.Graph()\ng2 = tf.Graph()\n\nembedding_size = 150\npad_len = 100\n\nwith g1.as_default():\n    batch_sentence = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]\n    cell = tf.contrib.rnn.GRUCell(embedding_size) \n    output1, states1 = tf.contrib.rnn.static_rnn(cell, batch_sentence, dtype = tf.float32)\n    loss = tf.reduce_sum(output1[0] - output1[99])\n    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)\n    optimizer.minimize(loss)\n    session1 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))\n    session1.run(tf.initialize_all_variables())\n    saver1 = tf.train.Saver()\n\nwith g2.as_default():\n    batch_sentence2 = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]\n    cell = tf.contrib.rnn.GRUCell(embedding_size) \n    outputs2, states2 = tf.contrib.rnn.static_rnn(cell, batch_sentence2, dtype = tf.float32)\n    loss = tf.reduce_sum(outputs2[0] - outputs2[99])\n    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)\n    optimizer.minimize(loss)\n    session2 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))\n    session2.run(tf.initialize_all_variables())\n    saver2 = tf.train.Saver()\n\nlock_map = [threading.Lock(),threading.Lock()]\n\n\ndef report_thread():\n    while(True):\n        def stacktraces():\n            code = []\n            for threadId, stack in sys._current_frames().items():\n                code.append(\"\\n thread: %s\" % threadId)\n                for filename, lineno, name, line in traceback.extract_stack(stack):\n                    code.append('\\n***  File: \"%s\", line %d, in %s' % (filename, lineno, name))\n                    if line:\n                        code.append(\"  %s\" % (line.strip()))\n            for line in code:\n                sys.stderr.write(line)\n            sys.stderr.write(\"\\n\")\n        stacktraces()\n        time.sleep(300)\n\n\ndef save_wrapper():\n    print time.time()\n    curr_lock = lock_map[0]\n    with curr_lock:\n        saver1.save(session1,'out1')\n    print time.time()\n\ndef save_wrapper2():\n    print time.time()\n    curr_lock = lock_map[1]\n    with curr_lock:\n        saver2.save(session2,'out2')\n    print time.time()\n\ndef main_thread2():\n    X = np.random.random((5,150))\n    while(True):\n        print '------'\n        for _ in range(60):\n            fd = {}\n\n            for i in range(pad_len):\n                fd[batch_sentence2[i]] = X\n            session2.run(outputs2,feed_dict = fd)\n        save_thread = threading.Thread(target = save_wrapper2)\n        save_thread.start()\n\n        for _ in range(60):\n            fd = {}\n            for i in range(pad_len):\n                fd[batch_sentence[i]] = X\n            session1.run(output1,feed_dict = fd)\n        save_thread = threading.Thread(target = save_wrapper)\n        save_thread.start()\n        \nt1 = threading.Thread(target = report_thread)\nt1.start()\nmain_thread2()\n\n\n</code></pre>", "body_text": "I'm able to reproduce the hangs on a smaller example, and have logs attached. I'm not sure if this is necessarily the same hang as I was seeing above, as the freeze seems to happen in a numpy call, not at the tensorflow level. The example is pretty silly, but I basically just make and evaluate two graphs, while saving them in the background on separate threads. Eventually, it hangs at:\n***  File: \"crash_eg.py\", line 93, in   main_thread2()\n***  File: \"crash_eg.py\", line 79, in main_thread2  session2.run(outputs2,feed_dict = fd)\n***  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run  run_metadata_ptr)\n***  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 938, in _run  np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\n***  File: \"/usr/local/lib/python2.7/site-packages/numpy/core/numeric.py\", line 531, in asarray  return array(a, dtype, copy=False, order=order)\nI have attached the full logs below. Need to investigate more about whether this is the same issue as reported above, but thought I would leave the info just in case. I should mention I am on a mac, and have read that there can be some issues with numpy and threading on Mac OS, so maybe that is involved. I am not sure if anyone else has ever seen a similar issue? Thanks again for all your time!\nhang_logs.txt\nimport tensorflow as tf\nimport threading\nimport numpy as np\nimport time\nimport sys\nimport traceback\n\ng1 = tf.Graph()\ng2 = tf.Graph()\n\nembedding_size = 150\npad_len = 100\n\nwith g1.as_default():\n    batch_sentence = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]\n    cell = tf.contrib.rnn.GRUCell(embedding_size) \n    output1, states1 = tf.contrib.rnn.static_rnn(cell, batch_sentence, dtype = tf.float32)\n    loss = tf.reduce_sum(output1[0] - output1[99])\n    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)\n    optimizer.minimize(loss)\n    session1 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))\n    session1.run(tf.initialize_all_variables())\n    saver1 = tf.train.Saver()\n\nwith g2.as_default():\n    batch_sentence2 = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]\n    cell = tf.contrib.rnn.GRUCell(embedding_size) \n    outputs2, states2 = tf.contrib.rnn.static_rnn(cell, batch_sentence2, dtype = tf.float32)\n    loss = tf.reduce_sum(outputs2[0] - outputs2[99])\n    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)\n    optimizer.minimize(loss)\n    session2 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))\n    session2.run(tf.initialize_all_variables())\n    saver2 = tf.train.Saver()\n\nlock_map = [threading.Lock(),threading.Lock()]\n\n\ndef report_thread():\n    while(True):\n        def stacktraces():\n            code = []\n            for threadId, stack in sys._current_frames().items():\n                code.append(\"\\n thread: %s\" % threadId)\n                for filename, lineno, name, line in traceback.extract_stack(stack):\n                    code.append('\\n***  File: \"%s\", line %d, in %s' % (filename, lineno, name))\n                    if line:\n                        code.append(\"  %s\" % (line.strip()))\n            for line in code:\n                sys.stderr.write(line)\n            sys.stderr.write(\"\\n\")\n        stacktraces()\n        time.sleep(300)\n\n\ndef save_wrapper():\n    print time.time()\n    curr_lock = lock_map[0]\n    with curr_lock:\n        saver1.save(session1,'out1')\n    print time.time()\n\ndef save_wrapper2():\n    print time.time()\n    curr_lock = lock_map[1]\n    with curr_lock:\n        saver2.save(session2,'out2')\n    print time.time()\n\ndef main_thread2():\n    X = np.random.random((5,150))\n    while(True):\n        print '------'\n        for _ in range(60):\n            fd = {}\n\n            for i in range(pad_len):\n                fd[batch_sentence2[i]] = X\n            session2.run(outputs2,feed_dict = fd)\n        save_thread = threading.Thread(target = save_wrapper2)\n        save_thread.start()\n\n        for _ in range(60):\n            fd = {}\n            for i in range(pad_len):\n                fd[batch_sentence[i]] = X\n            session1.run(output1,feed_dict = fd)\n        save_thread = threading.Thread(target = save_wrapper)\n        save_thread.start()\n        \nt1 = threading.Thread(target = report_thread)\nt1.start()\nmain_thread2()", "body": "I'm able to reproduce the hangs on a smaller example, and have logs attached. I'm not sure if this is necessarily the same hang as I was seeing above, as the freeze seems to happen in a numpy call, not at the tensorflow level. The example is pretty silly, but I basically just make and evaluate two graphs, while saving them in the background on separate threads. Eventually, it hangs at:\r\n\r\n***  File: \"crash_eg.py\", line 93, in <module>  main_thread2()\r\n***  File: \"crash_eg.py\", line 79, in main_thread2  session2.run(outputs2,feed_dict = fd)\r\n***  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run  run_metadata_ptr)\r\n***  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 938, in _run  np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\r\n***  File: \"/usr/local/lib/python2.7/site-packages/numpy/core/numeric.py\", line 531, in asarray  return array(a, dtype, copy=False, order=order)\r\n\r\nI have attached the full logs below. Need to investigate more about whether this is the same issue as reported above, but thought I would leave the info just in case. I should mention I am on a mac, and have read that there can be some issues with numpy and threading on Mac OS, so maybe that is involved. I am not sure if anyone else has ever seen a similar issue? Thanks again for all your time!\r\n\r\n[hang_logs.txt](https://github.com/tensorflow/tensorflow/files/786206/hang_logs.txt)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport threading\r\nimport numpy as np\r\nimport time\r\nimport sys\r\nimport traceback\r\n\r\ng1 = tf.Graph()\r\ng2 = tf.Graph()\r\n\r\nembedding_size = 150\r\npad_len = 100\r\n\r\nwith g1.as_default():\r\n    batch_sentence = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]\r\n    cell = tf.contrib.rnn.GRUCell(embedding_size) \r\n    output1, states1 = tf.contrib.rnn.static_rnn(cell, batch_sentence, dtype = tf.float32)\r\n    loss = tf.reduce_sum(output1[0] - output1[99])\r\n    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)\r\n    optimizer.minimize(loss)\r\n    session1 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))\r\n    session1.run(tf.initialize_all_variables())\r\n    saver1 = tf.train.Saver()\r\n\r\nwith g2.as_default():\r\n    batch_sentence2 = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]\r\n    cell = tf.contrib.rnn.GRUCell(embedding_size) \r\n    outputs2, states2 = tf.contrib.rnn.static_rnn(cell, batch_sentence2, dtype = tf.float32)\r\n    loss = tf.reduce_sum(outputs2[0] - outputs2[99])\r\n    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)\r\n    optimizer.minimize(loss)\r\n    session2 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))\r\n    session2.run(tf.initialize_all_variables())\r\n    saver2 = tf.train.Saver()\r\n\r\nlock_map = [threading.Lock(),threading.Lock()]\r\n\r\n\r\ndef report_thread():\r\n    while(True):\r\n        def stacktraces():\r\n            code = []\r\n            for threadId, stack in sys._current_frames().items():\r\n                code.append(\"\\n thread: %s\" % threadId)\r\n                for filename, lineno, name, line in traceback.extract_stack(stack):\r\n                    code.append('\\n***  File: \"%s\", line %d, in %s' % (filename, lineno, name))\r\n                    if line:\r\n                        code.append(\"  %s\" % (line.strip()))\r\n            for line in code:\r\n                sys.stderr.write(line)\r\n            sys.stderr.write(\"\\n\")\r\n        stacktraces()\r\n        time.sleep(300)\r\n\r\n\r\ndef save_wrapper():\r\n    print time.time()\r\n    curr_lock = lock_map[0]\r\n    with curr_lock:\r\n        saver1.save(session1,'out1')\r\n    print time.time()\r\n\r\ndef save_wrapper2():\r\n    print time.time()\r\n    curr_lock = lock_map[1]\r\n    with curr_lock:\r\n        saver2.save(session2,'out2')\r\n    print time.time()\r\n\r\ndef main_thread2():\r\n    X = np.random.random((5,150))\r\n    while(True):\r\n        print '------'\r\n        for _ in range(60):\r\n            fd = {}\r\n\r\n            for i in range(pad_len):\r\n                fd[batch_sentence2[i]] = X\r\n            session2.run(outputs2,feed_dict = fd)\r\n        save_thread = threading.Thread(target = save_wrapper2)\r\n        save_thread.start()\r\n\r\n        for _ in range(60):\r\n            fd = {}\r\n            for i in range(pad_len):\r\n                fd[batch_sentence[i]] = X\r\n            session1.run(output1,feed_dict = fd)\r\n        save_thread = threading.Thread(target = save_wrapper)\r\n        save_thread.start()\r\n        \r\nt1 = threading.Thread(target = report_thread)\r\nt1.start()\r\nmain_thread2()\r\n\r\n\r\n```\r\n\r\n\r\n"}