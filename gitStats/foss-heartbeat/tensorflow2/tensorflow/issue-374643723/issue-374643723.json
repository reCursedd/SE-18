{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23316", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23316/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23316/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23316/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23316", "id": 374643723, "node_id": "MDU6SXNzdWUzNzQ2NDM3MjM=", "number": 23316, "title": "Wrong gradients in combination with placeholders.", "user": {"login": "pesser", "id": 2175508, "node_id": "MDQ6VXNlcjIxNzU1MDg=", "avatar_url": "https://avatars3.githubusercontent.com/u/2175508?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pesser", "html_url": "https://github.com/pesser", "followers_url": "https://api.github.com/users/pesser/followers", "following_url": "https://api.github.com/users/pesser/following{/other_user}", "gists_url": "https://api.github.com/users/pesser/gists{/gist_id}", "starred_url": "https://api.github.com/users/pesser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pesser/subscriptions", "organizations_url": "https://api.github.com/users/pesser/orgs", "repos_url": "https://api.github.com/users/pesser/repos", "events_url": "https://api.github.com/users/pesser/events{/privacy}", "received_events_url": "https://api.github.com/users/pesser/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097547147, "node_id": "MDU6TGFiZWwxMDk3NTQ3MTQ3", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:ops", "name": "comp:ops", "color": "0052cc", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-10-27T13:11:31Z", "updated_at": "2018-11-21T18:58:21Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): <code>Ubuntu 14.04.5 LTS</code>, <code>Debian 4.18.10-2</code>, <code>Archlinux</code></li>\n<li>TensorFlow installed from (source or binary): binary</li>\n<li>TensorFlow version (use command below): <code>v1.4.0-19-ga52c8d9 1.4.1</code>, <code>v1.10.1-0-g4dcfddc5d1 1.10.1</code>, <code>1.10.0</code>,</li>\n<li>Python version: <code>3.6.0</code>, <code>3.4.3</code>, <code>3.7.0</code></li>\n<li>CUDA/cuDNN version: <code>9.0/7.1.2</code>, <code>8.0.61/6.0.21</code>, <code>CPU only</code></li>\n<li>GPU model and memory: Titan 12GB</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nIncorrect gradient computation when placeholder weights are involved. In a situation where two variables are involved, optimization of one of the variables leads to wrong gradients for the other variable when placeholders weights are used.</p>\n<p><strong>Describe the expected behavior</strong><br>\nGradients should be correct and there should be no difference in gradients whether a placeholder or another variable is used.</p>\n<p><strong>Code to reproduce the issue</strong><br>\nHere is a minimal example to reproduce the issue (the case of <code>indirect = True</code> can be used as a workaround and shows the expected output in contrast of <code>indirect = False</code>):</p>\n<pre><code>import tensorflow as tf                                                                                   \n                                                                                                          \ndef test(indirect):                                                                                       \n    print(\"indirect:\", indirect)                                                                          \n    a = tf.Variable(1.0)                                                                                  \n    b = tf.Variable(2.0)                                                                                  \n    weight_var = tf.Variable(3.0, trainable = False)                                                      \n                                                                                                          \n    weight_placeholder = tf.placeholder(tf.float32)                                                       \n    if indirect:                                                                                          \n        weight = tf.assign(weight_var, weight_placeholder)                                                \n    else:                                                                                                 \n        weight = weight_placeholder                                                                       \n                                                                                                          \n    common_loss = tf.square(a*b)                                                                          \n    b_only_loss = tf.square(b)                                                                            \n                                                                                                          \n    common_grads = tf.gradients(                                                                          \n            common_loss,                                                                                  \n            [b])                                                                                          \n    b_only_grads = tf.gradients(                                                                          \n            weight*b_only_loss,                                                                           \n            [b])                                                                                          \n    summed_grads = tf.gradients(                                                                          \n            common_loss + weight*b_only_loss,                                                             \n            [b])                                                                                          \n    grad_diff = tf.abs(common_grads[0] + b_only_grads[0] - summed_grads[0])                               \n                                                                                                          \n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)                                      \n    opt_a = optimizer.minimize(common_loss, var_list=[a])                                                 \n                                                                                                          \n    feed={weight_placeholder: 0.0}                                                                        \n    fetch={\"common_grads\": common_grads, \"b_only_grads\": b_only_grads,                                    \n           \"summed_grads\": summed_grads, \"grad_diff\": grad_diff,                                          \n           \"opt_a\": opt_a}                                                                                \n    s = tf.Session()                                                                                      \n    s.run(tf.global_variables_initializer())                                                              \n    r = s.run(fetch, feed)                                                                                \n    for k in sorted(r):                                                                                   \n        print(\"{:16}\".format(k), r[k])                                                                    \n    assert r[\"grad_diff\"] &lt; 1e-11, r[\"grad_diff\"]\n                                                                                                          \n                                                                                                          \nif __name__ == \"__main__\":                                                                                \n    print(\"tensorflow\", tf.GIT_VERSION, tf.VERSION)                                                       \n    test(True)                                                                                            \n    test(False)\n</code></pre>\n<p>Example output:</p>\n<pre><code>tensorflow b'unknown' 1.10.0\nindirect: True\nb_only_grads     [0.0]\ncommon_grads     [4.0]\ngrad_diff        0.0\nopt_a            None\nsummed_grads     [4.0]\nindirect: False\nb_only_grads     [0.0]\ncommon_grads     [4.0]\ngrad_diff        32.0\nopt_a            None\nsummed_grads     [-28.0]\nTraceback (most recent call last):\n  File \"minimal_example.py\", line 47, in &lt;module&gt;\n    test(False)\n  File \"minimal_example.py\", line 41, in test\n    assert r[\"grad_diff\"] &lt; 1e-11, r[\"grad_diff\"]\nAssertionError: 32.0\n</code></pre>\n<p><strong>Other info / logs</strong><br>\nThe output is not deterministic and sometimes produces correct results so the code might have to be run multiple times. The code also works fine without the <code>opt_a</code> call. Constant values for the weight also work fine, just in combination with a placeholder it seems to be problematic. The problem remains the same for other values of the weight and thus in general violates commutativity of the gradient but the case of a weight of zero illustrates the problem best because the zero weighted summand really should not have any influence on the gradients. Finally, the gradients are not just reported falsely but optimization of <code>b</code>, e.g. <code>optimizer.minimize(common_loss + weight*b_only_loss, var_list=[b])</code>, produces wrong results (i.e. gradient descent uses wrong gradients).</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04.5 LTS, Debian 4.18.10-2, Archlinux\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.4.0-19-ga52c8d9 1.4.1, v1.10.1-0-g4dcfddc5d1 1.10.1, 1.10.0,\nPython version: 3.6.0, 3.4.3, 3.7.0\nCUDA/cuDNN version: 9.0/7.1.2, 8.0.61/6.0.21, CPU only\nGPU model and memory: Titan 12GB\n\nDescribe the current behavior\nIncorrect gradient computation when placeholder weights are involved. In a situation where two variables are involved, optimization of one of the variables leads to wrong gradients for the other variable when placeholders weights are used.\nDescribe the expected behavior\nGradients should be correct and there should be no difference in gradients whether a placeholder or another variable is used.\nCode to reproduce the issue\nHere is a minimal example to reproduce the issue (the case of indirect = True can be used as a workaround and shows the expected output in contrast of indirect = False):\nimport tensorflow as tf                                                                                   \n                                                                                                          \ndef test(indirect):                                                                                       \n    print(\"indirect:\", indirect)                                                                          \n    a = tf.Variable(1.0)                                                                                  \n    b = tf.Variable(2.0)                                                                                  \n    weight_var = tf.Variable(3.0, trainable = False)                                                      \n                                                                                                          \n    weight_placeholder = tf.placeholder(tf.float32)                                                       \n    if indirect:                                                                                          \n        weight = tf.assign(weight_var, weight_placeholder)                                                \n    else:                                                                                                 \n        weight = weight_placeholder                                                                       \n                                                                                                          \n    common_loss = tf.square(a*b)                                                                          \n    b_only_loss = tf.square(b)                                                                            \n                                                                                                          \n    common_grads = tf.gradients(                                                                          \n            common_loss,                                                                                  \n            [b])                                                                                          \n    b_only_grads = tf.gradients(                                                                          \n            weight*b_only_loss,                                                                           \n            [b])                                                                                          \n    summed_grads = tf.gradients(                                                                          \n            common_loss + weight*b_only_loss,                                                             \n            [b])                                                                                          \n    grad_diff = tf.abs(common_grads[0] + b_only_grads[0] - summed_grads[0])                               \n                                                                                                          \n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)                                      \n    opt_a = optimizer.minimize(common_loss, var_list=[a])                                                 \n                                                                                                          \n    feed={weight_placeholder: 0.0}                                                                        \n    fetch={\"common_grads\": common_grads, \"b_only_grads\": b_only_grads,                                    \n           \"summed_grads\": summed_grads, \"grad_diff\": grad_diff,                                          \n           \"opt_a\": opt_a}                                                                                \n    s = tf.Session()                                                                                      \n    s.run(tf.global_variables_initializer())                                                              \n    r = s.run(fetch, feed)                                                                                \n    for k in sorted(r):                                                                                   \n        print(\"{:16}\".format(k), r[k])                                                                    \n    assert r[\"grad_diff\"] < 1e-11, r[\"grad_diff\"]\n                                                                                                          \n                                                                                                          \nif __name__ == \"__main__\":                                                                                \n    print(\"tensorflow\", tf.GIT_VERSION, tf.VERSION)                                                       \n    test(True)                                                                                            \n    test(False)\n\nExample output:\ntensorflow b'unknown' 1.10.0\nindirect: True\nb_only_grads     [0.0]\ncommon_grads     [4.0]\ngrad_diff        0.0\nopt_a            None\nsummed_grads     [4.0]\nindirect: False\nb_only_grads     [0.0]\ncommon_grads     [4.0]\ngrad_diff        32.0\nopt_a            None\nsummed_grads     [-28.0]\nTraceback (most recent call last):\n  File \"minimal_example.py\", line 47, in <module>\n    test(False)\n  File \"minimal_example.py\", line 41, in test\n    assert r[\"grad_diff\"] < 1e-11, r[\"grad_diff\"]\nAssertionError: 32.0\n\nOther info / logs\nThe output is not deterministic and sometimes produces correct results so the code might have to be run multiple times. The code also works fine without the opt_a call. Constant values for the weight also work fine, just in combination with a placeholder it seems to be problematic. The problem remains the same for other values of the weight and thus in general violates commutativity of the gradient but the case of a weight of zero illustrates the problem best because the zero weighted summand really should not have any influence on the gradients. Finally, the gradients are not just reported falsely but optimization of b, e.g. optimizer.minimize(common_loss + weight*b_only_loss, var_list=[b]), produces wrong results (i.e. gradient descent uses wrong gradients).", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 14.04.5 LTS`, `Debian 4.18.10-2`, `Archlinux`\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v1.4.0-19-ga52c8d9 1.4.1`, `v1.10.1-0-g4dcfddc5d1 1.10.1`, `1.10.0`, \r\n- Python version: `3.6.0`, `3.4.3`, `3.7.0`\r\n- CUDA/cuDNN version: `9.0/7.1.2`, `8.0.61/6.0.21`, `CPU only`\r\n- GPU model and memory: Titan 12GB\r\n\r\n**Describe the current behavior**\r\nIncorrect gradient computation when placeholder weights are involved. In a situation where two variables are involved, optimization of one of the variables leads to wrong gradients for the other variable when placeholders weights are used.\r\n\r\n**Describe the expected behavior**\r\nGradients should be correct and there should be no difference in gradients whether a placeholder or another variable is used.\r\n\r\n**Code to reproduce the issue**\r\nHere is a minimal example to reproduce the issue (the case of `indirect = True` can be used as a workaround and shows the expected output in contrast of `indirect = False`):\r\n\r\n```\r\nimport tensorflow as tf                                                                                   \r\n                                                                                                          \r\ndef test(indirect):                                                                                       \r\n    print(\"indirect:\", indirect)                                                                          \r\n    a = tf.Variable(1.0)                                                                                  \r\n    b = tf.Variable(2.0)                                                                                  \r\n    weight_var = tf.Variable(3.0, trainable = False)                                                      \r\n                                                                                                          \r\n    weight_placeholder = tf.placeholder(tf.float32)                                                       \r\n    if indirect:                                                                                          \r\n        weight = tf.assign(weight_var, weight_placeholder)                                                \r\n    else:                                                                                                 \r\n        weight = weight_placeholder                                                                       \r\n                                                                                                          \r\n    common_loss = tf.square(a*b)                                                                          \r\n    b_only_loss = tf.square(b)                                                                            \r\n                                                                                                          \r\n    common_grads = tf.gradients(                                                                          \r\n            common_loss,                                                                                  \r\n            [b])                                                                                          \r\n    b_only_grads = tf.gradients(                                                                          \r\n            weight*b_only_loss,                                                                           \r\n            [b])                                                                                          \r\n    summed_grads = tf.gradients(                                                                          \r\n            common_loss + weight*b_only_loss,                                                             \r\n            [b])                                                                                          \r\n    grad_diff = tf.abs(common_grads[0] + b_only_grads[0] - summed_grads[0])                               \r\n                                                                                                          \r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)                                      \r\n    opt_a = optimizer.minimize(common_loss, var_list=[a])                                                 \r\n                                                                                                          \r\n    feed={weight_placeholder: 0.0}                                                                        \r\n    fetch={\"common_grads\": common_grads, \"b_only_grads\": b_only_grads,                                    \r\n           \"summed_grads\": summed_grads, \"grad_diff\": grad_diff,                                          \r\n           \"opt_a\": opt_a}                                                                                \r\n    s = tf.Session()                                                                                      \r\n    s.run(tf.global_variables_initializer())                                                              \r\n    r = s.run(fetch, feed)                                                                                \r\n    for k in sorted(r):                                                                                   \r\n        print(\"{:16}\".format(k), r[k])                                                                    \r\n    assert r[\"grad_diff\"] < 1e-11, r[\"grad_diff\"]\r\n                                                                                                          \r\n                                                                                                          \r\nif __name__ == \"__main__\":                                                                                \r\n    print(\"tensorflow\", tf.GIT_VERSION, tf.VERSION)                                                       \r\n    test(True)                                                                                            \r\n    test(False)\r\n```\r\n\r\nExample output:\r\n\r\n```\r\ntensorflow b'unknown' 1.10.0\r\nindirect: True\r\nb_only_grads     [0.0]\r\ncommon_grads     [4.0]\r\ngrad_diff        0.0\r\nopt_a            None\r\nsummed_grads     [4.0]\r\nindirect: False\r\nb_only_grads     [0.0]\r\ncommon_grads     [4.0]\r\ngrad_diff        32.0\r\nopt_a            None\r\nsummed_grads     [-28.0]\r\nTraceback (most recent call last):\r\n  File \"minimal_example.py\", line 47, in <module>\r\n    test(False)\r\n  File \"minimal_example.py\", line 41, in test\r\n    assert r[\"grad_diff\"] < 1e-11, r[\"grad_diff\"]\r\nAssertionError: 32.0\r\n```\r\n\r\n**Other info / logs**\r\nThe output is not deterministic and sometimes produces correct results so the code might have to be run multiple times. The code also works fine without the `opt_a` call. Constant values for the weight also work fine, just in combination with a placeholder it seems to be problematic. The problem remains the same for other values of the weight and thus in general violates commutativity of the gradient but the case of a weight of zero illustrates the problem best because the zero weighted summand really should not have any influence on the gradients. Finally, the gradients are not just reported falsely but optimization of `b`, e.g. `optimizer.minimize(common_loss + weight*b_only_loss, var_list=[b])`, produces wrong results (i.e. gradient descent uses wrong gradients).\r\n"}