{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/368755494", "html_url": "https://github.com/tensorflow/tensorflow/issues/16803#issuecomment-368755494", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16803", "id": 368755494, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODc1NTQ5NA==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-27T05:49:51Z", "updated_at": "2018-02-27T05:49:51Z", "author_association": "MEMBER", "body_html": "<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=161459\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petewarden\">@petewarden</a></p>\n<p>transform_graph has had some hard to debug issue for various models in the past. From your error, it seems that some transformation is screwing up shape information in the graph, but i am really not sure.</p>\n<p>We have focused our 8-bit quantization efforts for mobile models on TensorFlow Lite : <a href=\"https://www.tensorflow.org/mobile/tflite/\" rel=\"nofollow\">https://www.tensorflow.org/mobile/tflite/</a>. Currently TFLite quantizations requires quantizing both weights and activations, though this may change.</p>\n<p>We use fake quantized training (<a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize\">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize</a>) to get models that are compatible with TensorFlow Lite's converter, TOCO. TOCO can then convert these models to fully quantized models. We find that for many models, quantizing with training in the loop is necessary for good accuracy.<br>\nWe are working on making these rewriters more general over various models, so expect improvements in this department.</p>\n<p>Here are some examples for how we train mobilenet_v1 for quantization : <a href=\"https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\">https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md</a></p>\n<p>The graph resulting from <a href=\"https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_eval.py\">https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_eval.py</a> is compatible with TensorFlow Lite.</p>\n<p>Hope that helps!</p>\n<p>What model architecture are you trying to quantize? Are you ok with quantizing both weights and activations?</p>", "body_text": "cc @petewarden\ntransform_graph has had some hard to debug issue for various models in the past. From your error, it seems that some transformation is screwing up shape information in the graph, but i am really not sure.\nWe have focused our 8-bit quantization efforts for mobile models on TensorFlow Lite : https://www.tensorflow.org/mobile/tflite/. Currently TFLite quantizations requires quantizing both weights and activations, though this may change.\nWe use fake quantized training (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize) to get models that are compatible with TensorFlow Lite's converter, TOCO. TOCO can then convert these models to fully quantized models. We find that for many models, quantizing with training in the loop is necessary for good accuracy.\nWe are working on making these rewriters more general over various models, so expect improvements in this department.\nHere are some examples for how we train mobilenet_v1 for quantization : https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\nThe graph resulting from https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_eval.py is compatible with TensorFlow Lite.\nHope that helps!\nWhat model architecture are you trying to quantize? Are you ok with quantizing both weights and activations?", "body": "cc @petewarden \r\n\r\ntransform_graph has had some hard to debug issue for various models in the past. From your error, it seems that some transformation is screwing up shape information in the graph, but i am really not sure. \r\n\r\nWe have focused our 8-bit quantization efforts for mobile models on TensorFlow Lite : https://www.tensorflow.org/mobile/tflite/. Currently TFLite quantizations requires quantizing both weights and activations, though this may change.\r\n\r\nWe use fake quantized training (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize) to get models that are compatible with TensorFlow Lite's converter, TOCO. TOCO can then convert these models to fully quantized models. We find that for many models, quantizing with training in the loop is necessary for good accuracy.\r\nWe are working on making these rewriters more general over various models, so expect improvements in this department.\r\n\r\nHere are some examples for how we train mobilenet_v1 for quantization : https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\n\r\nThe graph resulting from https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_eval.py is compatible with TensorFlow Lite.\r\n\r\nHope that helps!\r\n\r\nWhat model architecture are you trying to quantize? Are you ok with quantizing both weights and activations? "}