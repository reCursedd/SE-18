{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9433", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9433/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9433/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9433/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9433", "id": 224065407, "node_id": "MDU6SXNzdWUyMjQwNjU0MDc=", "number": 9433, "title": "scan in theano and tensorflow", "user": {"login": "saitarslanboun", "id": 9799395, "node_id": "MDQ6VXNlcjk3OTkzOTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9799395?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saitarslanboun", "html_url": "https://github.com/saitarslanboun", "followers_url": "https://api.github.com/users/saitarslanboun/followers", "following_url": "https://api.github.com/users/saitarslanboun/following{/other_user}", "gists_url": "https://api.github.com/users/saitarslanboun/gists{/gist_id}", "starred_url": "https://api.github.com/users/saitarslanboun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saitarslanboun/subscriptions", "organizations_url": "https://api.github.com/users/saitarslanboun/orgs", "repos_url": "https://api.github.com/users/saitarslanboun/repos", "events_url": "https://api.github.com/users/saitarslanboun/events{/privacy}", "received_events_url": "https://api.github.com/users/saitarslanboun/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-25T08:48:54Z", "updated_at": "2017-04-25T16:38:46Z", "closed_at": "2017-04-25T16:38:43Z", "author_association": "NONE", "body_html": "<p>There is a function written with theano:</p>\n<pre><code>`import numpy as np\nimport theano as theano\nimport theano.tensor as T\n\ndef forward_prop_step(x_t, s_t1_prev, s_t2_prev):\n            # This is how we calculated the hidden state in a simple RNN. No longer!\n            # s_t = T.tanh(U[:,x_t] + W.dot(s_t1_prev))\n\n            # Word embedding layer\n            x_e = E[:,x_t]\n\n            # GRU Layer 1\n            z_t1 = T.nnet.hard_sigmoid(U[0].dot(x_e) + W[0].dot(s_t1_prev) + b[0])\n            r_t1 = T.nnet.hard_sigmoid(U[1].dot(x_e) + W[1].dot(s_t1_prev) + b[1])\n            c_t1 = T.tanh(U[2].dot(x_e) + W[2].dot(s_t1_prev * r_t1) + b[2])\n            s_t1 = (T.ones_like(z_t1) - z_t1) * c_t1 + z_t1 * s_t1_prev\n\n            # GRU Layer 2\n            z_t2 = T.nnet.hard_sigmoid(U[3].dot(s_t1) + W[3].dot(s_t2_prev) + b[3])\n            r_t2 = T.nnet.hard_sigmoid(U[4].dot(s_t1) + W[4].dot(s_t2_prev) + b[4])\n            c_t2 = T.tanh(U[5].dot(s_t1) + W[5].dot(s_t2_prev * r_t2) + b[5])\n            s_t2 = (T.ones_like(z_t2) - z_t2) * c_t2 + z_t2 * s_t2_prev\n\n            # Final output calculation\n            # Theano's softmax returns a matrix with one row, we only need the row\n            o_t = T.nnet.softmax(V.dot(s_t2) + c)[0]\n\nreturn [o_t, s_t1, s_t2]`\n</code></pre>\n<p>I have tried to rewrite it with tensorflow:</p>\n<pre><code>`import numpy as np, tensorflow as tf, operator\n\ndef forward_prop_step(x_t, s_t1_prev, s_t2_prev):\n                        # This is how we calculated the hidden state in a simple RNN. No longer!\n                        # s_t = T.tanh(U[:,x_t] + W.dot(s_t1_prev))\n\n                        # Word embedding layer\n                        x_e = E[:,x_t]\n\n                        # GRU Layer 1\n                        z_t1 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[0], x_e)) + tf.reduce_sum(tf.multiply(W[0], s_t1_prev)) + b[0])\n                        r_t1 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[1], x_e)) + tf.reduce_sum(tf.multiply(W[1], s_t1_prev)) + b[1])\n                        c_t1 = tf.nn.tanh(tf.reduce_sum(tf.multiply(U[2], x_e)) + tf.reduce_sum(tf.multiply(W[2], (s_t1_prev * r_t1))) + b[2])\n                        s_t1 = (tf.ones_like(z_t1) - z_t1) * c_t1 + z_t1 * s_t1_prev\n\n                        # GRU Layer 2\n                        z_t2 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[3], s_t1)) + tf.reduce_sum(tf.multiply(W[3], s_t2_prev)) + b[3])\n                        r_t2 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[4], s_t1)) + tf.reduce_sum(tf.multiply(W[4], s_t2_prev)) + b[4])\n                        c_t2 = tf.nn.tanh(tf.reduce_sum(tf.multiply(U[5], s_t1)) + tf.reduce_sum(tf.multiply(W[5], (s_t2_prev * r_t2))) + b[5])\n                        s_t2 = (tf.ones_like(z_t2) - z_t2) * c_t2 + z_t2 * s_t2_prev\n\n                        # Final output calculation\n                        # Tensorflow's softmax returns a matrix with one row, we only need the row\n                        o_t = tf.nn.softmax(tf.reduce_sum(tf.multiply(V, s_t2)) + c)[0]\n\n                        return [o_t, s_t1, s_t2]`\n</code></pre>\n<p>In theano, scan function is called to perform \"forward_prep_step\" function in a loop:</p>\n<pre><code>`[o, s, s2], updates = theano.scan(\n            forward_prop_step,\n            sequences=x,\n            truncate_gradient=self.bptt_truncate,\n            outputs_info=[None, \n                          dict(initial=T.zeros(self.hidden_dim)),\ndict(initial=T.zeros(self.hidden_dim))])`\n</code></pre>\n<p>There is a scan function in tensorflow as well, but they don't get the same parameters. How could be the transformation of scan function above, into tensorflow scan function?</p>", "body_text": "There is a function written with theano:\n`import numpy as np\nimport theano as theano\nimport theano.tensor as T\n\ndef forward_prop_step(x_t, s_t1_prev, s_t2_prev):\n            # This is how we calculated the hidden state in a simple RNN. No longer!\n            # s_t = T.tanh(U[:,x_t] + W.dot(s_t1_prev))\n\n            # Word embedding layer\n            x_e = E[:,x_t]\n\n            # GRU Layer 1\n            z_t1 = T.nnet.hard_sigmoid(U[0].dot(x_e) + W[0].dot(s_t1_prev) + b[0])\n            r_t1 = T.nnet.hard_sigmoid(U[1].dot(x_e) + W[1].dot(s_t1_prev) + b[1])\n            c_t1 = T.tanh(U[2].dot(x_e) + W[2].dot(s_t1_prev * r_t1) + b[2])\n            s_t1 = (T.ones_like(z_t1) - z_t1) * c_t1 + z_t1 * s_t1_prev\n\n            # GRU Layer 2\n            z_t2 = T.nnet.hard_sigmoid(U[3].dot(s_t1) + W[3].dot(s_t2_prev) + b[3])\n            r_t2 = T.nnet.hard_sigmoid(U[4].dot(s_t1) + W[4].dot(s_t2_prev) + b[4])\n            c_t2 = T.tanh(U[5].dot(s_t1) + W[5].dot(s_t2_prev * r_t2) + b[5])\n            s_t2 = (T.ones_like(z_t2) - z_t2) * c_t2 + z_t2 * s_t2_prev\n\n            # Final output calculation\n            # Theano's softmax returns a matrix with one row, we only need the row\n            o_t = T.nnet.softmax(V.dot(s_t2) + c)[0]\n\nreturn [o_t, s_t1, s_t2]`\n\nI have tried to rewrite it with tensorflow:\n`import numpy as np, tensorflow as tf, operator\n\ndef forward_prop_step(x_t, s_t1_prev, s_t2_prev):\n                        # This is how we calculated the hidden state in a simple RNN. No longer!\n                        # s_t = T.tanh(U[:,x_t] + W.dot(s_t1_prev))\n\n                        # Word embedding layer\n                        x_e = E[:,x_t]\n\n                        # GRU Layer 1\n                        z_t1 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[0], x_e)) + tf.reduce_sum(tf.multiply(W[0], s_t1_prev)) + b[0])\n                        r_t1 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[1], x_e)) + tf.reduce_sum(tf.multiply(W[1], s_t1_prev)) + b[1])\n                        c_t1 = tf.nn.tanh(tf.reduce_sum(tf.multiply(U[2], x_e)) + tf.reduce_sum(tf.multiply(W[2], (s_t1_prev * r_t1))) + b[2])\n                        s_t1 = (tf.ones_like(z_t1) - z_t1) * c_t1 + z_t1 * s_t1_prev\n\n                        # GRU Layer 2\n                        z_t2 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[3], s_t1)) + tf.reduce_sum(tf.multiply(W[3], s_t2_prev)) + b[3])\n                        r_t2 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[4], s_t1)) + tf.reduce_sum(tf.multiply(W[4], s_t2_prev)) + b[4])\n                        c_t2 = tf.nn.tanh(tf.reduce_sum(tf.multiply(U[5], s_t1)) + tf.reduce_sum(tf.multiply(W[5], (s_t2_prev * r_t2))) + b[5])\n                        s_t2 = (tf.ones_like(z_t2) - z_t2) * c_t2 + z_t2 * s_t2_prev\n\n                        # Final output calculation\n                        # Tensorflow's softmax returns a matrix with one row, we only need the row\n                        o_t = tf.nn.softmax(tf.reduce_sum(tf.multiply(V, s_t2)) + c)[0]\n\n                        return [o_t, s_t1, s_t2]`\n\nIn theano, scan function is called to perform \"forward_prep_step\" function in a loop:\n`[o, s, s2], updates = theano.scan(\n            forward_prop_step,\n            sequences=x,\n            truncate_gradient=self.bptt_truncate,\n            outputs_info=[None, \n                          dict(initial=T.zeros(self.hidden_dim)),\ndict(initial=T.zeros(self.hidden_dim))])`\n\nThere is a scan function in tensorflow as well, but they don't get the same parameters. How could be the transformation of scan function above, into tensorflow scan function?", "body": "There is a function written with theano:\r\n\r\n```\r\n`import numpy as np\r\nimport theano as theano\r\nimport theano.tensor as T\r\n\r\ndef forward_prop_step(x_t, s_t1_prev, s_t2_prev):\r\n            # This is how we calculated the hidden state in a simple RNN. No longer!\r\n            # s_t = T.tanh(U[:,x_t] + W.dot(s_t1_prev))\r\n\r\n            # Word embedding layer\r\n            x_e = E[:,x_t]\r\n\r\n            # GRU Layer 1\r\n            z_t1 = T.nnet.hard_sigmoid(U[0].dot(x_e) + W[0].dot(s_t1_prev) + b[0])\r\n            r_t1 = T.nnet.hard_sigmoid(U[1].dot(x_e) + W[1].dot(s_t1_prev) + b[1])\r\n            c_t1 = T.tanh(U[2].dot(x_e) + W[2].dot(s_t1_prev * r_t1) + b[2])\r\n            s_t1 = (T.ones_like(z_t1) - z_t1) * c_t1 + z_t1 * s_t1_prev\r\n\r\n            # GRU Layer 2\r\n            z_t2 = T.nnet.hard_sigmoid(U[3].dot(s_t1) + W[3].dot(s_t2_prev) + b[3])\r\n            r_t2 = T.nnet.hard_sigmoid(U[4].dot(s_t1) + W[4].dot(s_t2_prev) + b[4])\r\n            c_t2 = T.tanh(U[5].dot(s_t1) + W[5].dot(s_t2_prev * r_t2) + b[5])\r\n            s_t2 = (T.ones_like(z_t2) - z_t2) * c_t2 + z_t2 * s_t2_prev\r\n\r\n            # Final output calculation\r\n            # Theano's softmax returns a matrix with one row, we only need the row\r\n            o_t = T.nnet.softmax(V.dot(s_t2) + c)[0]\r\n\r\nreturn [o_t, s_t1, s_t2]`\r\n```\r\n\r\nI have tried to rewrite it with tensorflow:\r\n\r\n```\r\n`import numpy as np, tensorflow as tf, operator\r\n\r\ndef forward_prop_step(x_t, s_t1_prev, s_t2_prev):\r\n                        # This is how we calculated the hidden state in a simple RNN. No longer!\r\n                        # s_t = T.tanh(U[:,x_t] + W.dot(s_t1_prev))\r\n\r\n                        # Word embedding layer\r\n                        x_e = E[:,x_t]\r\n\r\n                        # GRU Layer 1\r\n                        z_t1 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[0], x_e)) + tf.reduce_sum(tf.multiply(W[0], s_t1_prev)) + b[0])\r\n                        r_t1 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[1], x_e)) + tf.reduce_sum(tf.multiply(W[1], s_t1_prev)) + b[1])\r\n                        c_t1 = tf.nn.tanh(tf.reduce_sum(tf.multiply(U[2], x_e)) + tf.reduce_sum(tf.multiply(W[2], (s_t1_prev * r_t1))) + b[2])\r\n                        s_t1 = (tf.ones_like(z_t1) - z_t1) * c_t1 + z_t1 * s_t1_prev\r\n\r\n                        # GRU Layer 2\r\n                        z_t2 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[3], s_t1)) + tf.reduce_sum(tf.multiply(W[3], s_t2_prev)) + b[3])\r\n                        r_t2 = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(U[4], s_t1)) + tf.reduce_sum(tf.multiply(W[4], s_t2_prev)) + b[4])\r\n                        c_t2 = tf.nn.tanh(tf.reduce_sum(tf.multiply(U[5], s_t1)) + tf.reduce_sum(tf.multiply(W[5], (s_t2_prev * r_t2))) + b[5])\r\n                        s_t2 = (tf.ones_like(z_t2) - z_t2) * c_t2 + z_t2 * s_t2_prev\r\n\r\n                        # Final output calculation\r\n                        # Tensorflow's softmax returns a matrix with one row, we only need the row\r\n                        o_t = tf.nn.softmax(tf.reduce_sum(tf.multiply(V, s_t2)) + c)[0]\r\n\r\n                        return [o_t, s_t1, s_t2]`\r\n```\r\n\r\nIn theano, scan function is called to perform \"forward_prep_step\" function in a loop:\r\n\r\n```\r\n`[o, s, s2], updates = theano.scan(\r\n            forward_prop_step,\r\n            sequences=x,\r\n            truncate_gradient=self.bptt_truncate,\r\n            outputs_info=[None, \r\n                          dict(initial=T.zeros(self.hidden_dim)),\r\ndict(initial=T.zeros(self.hidden_dim))])`\r\n```\r\n\r\nThere is a scan function in tensorflow as well, but they don't get the same parameters. How could be the transformation of scan function above, into tensorflow scan function? "}