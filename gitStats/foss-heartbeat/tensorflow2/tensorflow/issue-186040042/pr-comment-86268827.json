{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/86268827", "pull_request_review_id": 6935082, "id": 86268827, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDg2MjY4ODI3", "diff_hunk": "@@ -65,10 +65,10 @@ void IntraProcessRendezvous::SameWorkerRecvDone(\n     StatusCallback done) {\n   // Do a quick copy (sharing the underlying buffer) if both tensors\n   // are on host memory.\n-  const bool src_host =\n-      (send_args.alloc_attrs.on_host() || parsed.src.type == \"CPU\");\n-  const bool dst_host =\n-      (recv_args.alloc_attrs.on_host() || parsed.dst.type == \"CPU\");\n+  const bool src_host = (send_args.alloc_attrs.on_host() ||\n+                         parsed.src.type == \"CPU\" || parsed.src.type == \"SYCL\");", "path": "tensorflow/core/common_runtime/rendezvous_mgr.cc", "position": null, "original_position": 9, "commit_id": "1f094bccdb1a607fcf8f67cacb4b614c7fc618f1", "original_commit_id": "174c6edb7dd5f99b4e9b0f45a0eaf978b578e85c", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "body": "Yeah, you probably need to have a device-specific allocator.\n\nThe change here is basically saying that from the perspective of the framework, whenever you try to copy to a SYCL device, the destination should be treated as \"host memory\" (e.g., on the CPU).  This probably isn't what you want: you likely want the data to be copied into \"device memory\" which will have been allocated using a custom device allocator for SYCL.\n\nIn the CUDA case, when you have a send/recv pair between a CPU and GPU subgraph, you would want to transfer from CPU memory into GPU memory allocated via cuMalloc.  Our GPU allocator implementation for CUDA does this allocation, so you can see that our existing GPUDevice uses a GPUBFCAllocator implementation to allocate device memory via cuMalloc.\n\nThere are some cases where a kernel is written such that some of its inputs are not really needed in \"device memory\" -- they are more like 'control' bits (e.g., the index arguments to reshape).  Those are marked as \"HostMemory\" in their kernel registrations.  So this piece of code checks whether the source and destination are in 'host/cpu' memory.  If so, no copy needs to be made because the memory address spaces are the same.  Does that make sense?\n", "created_at": "2016-11-03T00:05:51Z", "updated_at": "2016-11-03T23:52:34Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/5267#discussion_r86268827", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/5267", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/86268827"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/5267#discussion_r86268827"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/5267"}}, "body_html": "<p>Yeah, you probably need to have a device-specific allocator.</p>\n<p>The change here is basically saying that from the perspective of the framework, whenever you try to copy to a SYCL device, the destination should be treated as \"host memory\" (e.g., on the CPU).  This probably isn't what you want: you likely want the data to be copied into \"device memory\" which will have been allocated using a custom device allocator for SYCL.</p>\n<p>In the CUDA case, when you have a send/recv pair between a CPU and GPU subgraph, you would want to transfer from CPU memory into GPU memory allocated via cuMalloc.  Our GPU allocator implementation for CUDA does this allocation, so you can see that our existing GPUDevice uses a GPUBFCAllocator implementation to allocate device memory via cuMalloc.</p>\n<p>There are some cases where a kernel is written such that some of its inputs are not really needed in \"device memory\" -- they are more like 'control' bits (e.g., the index arguments to reshape).  Those are marked as \"HostMemory\" in their kernel registrations.  So this piece of code checks whether the source and destination are in 'host/cpu' memory.  If so, no copy needs to be made because the memory address spaces are the same.  Does that make sense?</p>", "body_text": "Yeah, you probably need to have a device-specific allocator.\nThe change here is basically saying that from the perspective of the framework, whenever you try to copy to a SYCL device, the destination should be treated as \"host memory\" (e.g., on the CPU).  This probably isn't what you want: you likely want the data to be copied into \"device memory\" which will have been allocated using a custom device allocator for SYCL.\nIn the CUDA case, when you have a send/recv pair between a CPU and GPU subgraph, you would want to transfer from CPU memory into GPU memory allocated via cuMalloc.  Our GPU allocator implementation for CUDA does this allocation, so you can see that our existing GPUDevice uses a GPUBFCAllocator implementation to allocate device memory via cuMalloc.\nThere are some cases where a kernel is written such that some of its inputs are not really needed in \"device memory\" -- they are more like 'control' bits (e.g., the index arguments to reshape).  Those are marked as \"HostMemory\" in their kernel registrations.  So this piece of code checks whether the source and destination are in 'host/cpu' memory.  If so, no copy needs to be made because the memory address spaces are the same.  Does that make sense?", "in_reply_to_id": 85631669}