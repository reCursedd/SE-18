{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20218", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20218/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20218/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20218/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20218", "id": 334793888, "node_id": "MDU6SXNzdWUzMzQ3OTM4ODg=", "number": 20218, "title": "ran out of memory in eager execution", "user": {"login": "qijimrc", "id": 28889175, "node_id": "MDQ6VXNlcjI4ODg5MTc1", "avatar_url": "https://avatars1.githubusercontent.com/u/28889175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qijimrc", "html_url": "https://github.com/qijimrc", "followers_url": "https://api.github.com/users/qijimrc/followers", "following_url": "https://api.github.com/users/qijimrc/following{/other_user}", "gists_url": "https://api.github.com/users/qijimrc/gists{/gist_id}", "starred_url": "https://api.github.com/users/qijimrc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qijimrc/subscriptions", "organizations_url": "https://api.github.com/users/qijimrc/orgs", "repos_url": "https://api.github.com/users/qijimrc/repos", "events_url": "https://api.github.com/users/qijimrc/events{/privacy}", "received_events_url": "https://api.github.com/users/qijimrc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 736653459, "node_id": "MDU6TGFiZWw3MzY2NTM0NTk=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:eager", "name": "comp:eager", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "akshaym", "id": 122911, "node_id": "MDQ6VXNlcjEyMjkxMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/122911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akshaym", "html_url": "https://github.com/akshaym", "followers_url": "https://api.github.com/users/akshaym/followers", "following_url": "https://api.github.com/users/akshaym/following{/other_user}", "gists_url": "https://api.github.com/users/akshaym/gists{/gist_id}", "starred_url": "https://api.github.com/users/akshaym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akshaym/subscriptions", "organizations_url": "https://api.github.com/users/akshaym/orgs", "repos_url": "https://api.github.com/users/akshaym/repos", "events_url": "https://api.github.com/users/akshaym/events{/privacy}", "received_events_url": "https://api.github.com/users/akshaym/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akshaym", "id": 122911, "node_id": "MDQ6VXNlcjEyMjkxMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/122911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akshaym", "html_url": "https://github.com/akshaym", "followers_url": "https://api.github.com/users/akshaym/followers", "following_url": "https://api.github.com/users/akshaym/following{/other_user}", "gists_url": "https://api.github.com/users/akshaym/gists{/gist_id}", "starred_url": "https://api.github.com/users/akshaym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akshaym/subscriptions", "organizations_url": "https://api.github.com/users/akshaym/orgs", "repos_url": "https://api.github.com/users/akshaym/repos", "events_url": "https://api.github.com/users/akshaym/events{/privacy}", "received_events_url": "https://api.github.com/users/akshaym/received_events", "type": "User", "site_admin": false}, {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 19, "created_at": "2018-06-22T08:39:35Z", "updated_at": "2018-08-31T01:36:20Z", "closed_at": "2018-08-31T00:31:38Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>:Yes</li>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from</strong>:source</li>\n<li><strong>TensorFlow version</strong>:1.8.0-gpu</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0/7.0</li>\n<li><strong>GPU model and memory</strong>:GeForce GTX 1060 / 6G</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nGeneral training model steps</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I wrote a Neural Language Model using TF eager mode, which has only one LSTM cell.<br>\nSome of these hyperparameters are:</p>\n<p>hidden_dim : 128<br>\nvocab_size : 7393<br>\nseq_len : &lt;=50<br>\nbatch_size : 32</p>\n<p>In particular, I did not use the inherited keras base class to write the model, but instead used tfe.Variable to gradually implement some of the details of the model. The model was normal during the early training and performed well.  However, a memory exhaustion error occurs every time the model runs to 960 steps or so.  I think it's because some variables haven't released this error that has been accumulated during training.  Because I still have this error in 960 steps after adjusting batch-size.<br>\nThe following is the error log, I think this is a TF eager mode memory usage problem, however the custom model is a necessary choice for some attempts.<br>\nHow can we avoid this problem and let the model train smoothly?<br>\nThanks a lot.</p>\n<h3>Source code / logs</h3>\n<p>loss at epoch 0 step 954: 6.665831<br>\nloss at epoch 0 step 955: 6.716213<br>\nloss at epoch 0 step 956: 6.761374<br>\nloss at epoch 0 step 957: 6.712702<br>\nloss at epoch 0 step 958: 6.753476<br>\nloss at epoch 0 step 959: 6.481436<br>\nloss at epoch 0 step 960: 6.544363<br>\nloss at epoch 0 step 961: 6.562449<br>\nloss at epoch 0 step 962: 6.701943<br>\nloss at epoch 0 step 963: 6.415178<br>\nloss at epoch 0 step 964: 6.630089<br>\nloss at epoch 0 step 965: 6.580114<br>\n2018-06-22 11:53:38.547278: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 92.05MiB.  Current allocation summary follows.<br>\n2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-requested in<br>\nuse in bin.<br>\n2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-r[1767/1952]<br>\nuse in bin.<br>\n2018-06-22 11:53:38.547459: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-req<br>\nuested in use in bin.<br>\n2018-06-22 11:53:38.547482: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-reque<br>\nsted in use in bin.<br>\n2018-06-22 11:53:38.547504: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use<br>\nin bin.<br>\n2018-06-22 11:53:38.547525: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 1, Chunks in use: 0. 4.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in<br>\nuse in bin.<br>\n2018-06-22 11:53:38.547544: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use<br>\nin bin.<br>\n2018-06-22 11:53:38.547568: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 5, Chunks in use: 4. 135.5KiB allocated for chunks. 110.0KiB in use in bin. 108.8Ki<br>\nB client-requested in use in bin.<br>\n2018-06-22 11:53:38.547593: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 920, Chunks in use: 918. 28.87MiB allocated for chunks. 28.81MiB in use in bin. 28.<br>\n69MiB client-requested in use in bin.<br>\n2018-06-22 11:53:38.547617: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 410, Chunks in use: 410. 26.12MiB allocated for chunks. 26.12MiB in use in bin. 25.<br>\n66MiB client-requested in use in bin.<br>\n2018-06-22 11:53:38.547640: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 20, Chunks in use: 20. 2.69MiB allocated for chunks. 2.69MiB in use in bin. 2.69MiB<br>\nclient-requested in use in bin.<br>\n.<br>\n.<br>\n.<br>\n.<br>\n.<br>\n.<br>\n.<br>\n2018-06-22 11:53:38.548461: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc700 of size 512                                                                         [1691/1952]<br>\n2018-06-22 11:53:38.548478: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc900 of size 131072<br>\n2018-06-22 11:53:38.548494: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dc900 of size 512<br>\n2018-06-22 11:53:38.548510: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dcb00 of size 131072<br>\n2018-06-22 11:53:38.548526: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcb00 of size 512<br>\n2018-06-22 11:53:38.548542: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcd00 of size 131072<br>\n2018-06-22 11:53:38.548558: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cd00 of size 512<br>\n2018-06-22 11:53:38.548574: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cf00 of size 131072<br>\n2018-06-22 11:53:38.548589: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3cf00 of size 512<br>\n2018-06-22 11:53:38.548605: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3d100 of size 131072<br>\n2018-06-22 11:53:38.548622: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d100 of size 512<br>\n2018-06-22 11:53:38.548638: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d300 of size 131072<br>\n2018-06-22 11:53:38.548654: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d300 of size 512<br>\n2018-06-22 11:53:38.548669: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d500 of size 131072<br>\n2018-06-22 11:53:38.548685: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d500 of size 512<br>\n2018-06-22 11:53:38.548702: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d700 of size 7570432<br>\n2018-06-22 11:53:38.548718: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091d5b00 of size 29696<br>\n2018-06-22 11:53:38.548735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091dcf00 of size 32768<br>\n2018-06-22 11:53:38.548752: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091e4f00 of size 32768<br>\n2018-06-22 11:53:38.548768: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091ecf00 of size 32768<br>\n2018-06-22 11:53:38.548783: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091f4f00 of size 32768<br>\n.<br>\n.<br>\n.<br>\n.<br>\n.<br>\n.<br>\n.<br>\n.<br>\n.<br>\n2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:<br>\nLimit:                  5272961024<br>\n2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:                                                                                                       [19/1952]<br>\nLimit:                  5272961024<br>\nInUse:                  3087996928<br>\nMaxInUse:               3174426368<br>\nNumAllocs:                 2214674<br>\nMaxAllocSize:            140052992</p>\n<p>2018-06-22 11:53:38.796832: W tensorflow/core/common_runtime/bfc_allocator.cc:279] <em><strong><strong><strong><strong><strong><strong>_</strong></strong></strong></strong></strong></strong></em><strong><strong><strong><strong><strong><strong><em><strong><strong><strong><strong><strong>_</strong></strong></strong></strong></strong></em>_</strong>_</strong>_</strong>_</strong>_</strong><em>******</em></strong><em>***</em><strong>_</strong>***<br>\n2018-06-22 11:53:38.796866: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at bias_op.cc:341 : Resource exhausted: OOM when allocating tensor with shape[3200,7393] and type float on<br>\n/job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc</p>", "body_text": "System information\n\nHave I written custom code:Yes\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from:source\nTensorFlow version:1.8.0-gpu\nPython version: 3.5\nCUDA/cuDNN version: 9.0/7.0\nGPU model and memory:GeForce GTX 1060 / 6G\nBazel version (if compiling from source): N/A\nExact command to reproduce:\nGeneral training model steps\n\nDescribe the problem\nI wrote a Neural Language Model using TF eager mode, which has only one LSTM cell.\nSome of these hyperparameters are:\nhidden_dim : 128\nvocab_size : 7393\nseq_len : <=50\nbatch_size : 32\nIn particular, I did not use the inherited keras base class to write the model, but instead used tfe.Variable to gradually implement some of the details of the model. The model was normal during the early training and performed well.  However, a memory exhaustion error occurs every time the model runs to 960 steps or so.  I think it's because some variables haven't released this error that has been accumulated during training.  Because I still have this error in 960 steps after adjusting batch-size.\nThe following is the error log, I think this is a TF eager mode memory usage problem, however the custom model is a necessary choice for some attempts.\nHow can we avoid this problem and let the model train smoothly?\nThanks a lot.\nSource code / logs\nloss at epoch 0 step 954: 6.665831\nloss at epoch 0 step 955: 6.716213\nloss at epoch 0 step 956: 6.761374\nloss at epoch 0 step 957: 6.712702\nloss at epoch 0 step 958: 6.753476\nloss at epoch 0 step 959: 6.481436\nloss at epoch 0 step 960: 6.544363\nloss at epoch 0 step 961: 6.562449\nloss at epoch 0 step 962: 6.701943\nloss at epoch 0 step 963: 6.415178\nloss at epoch 0 step 964: 6.630089\nloss at epoch 0 step 965: 6.580114\n2018-06-22 11:53:38.547278: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 92.05MiB.  Current allocation summary follows.\n2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-requested in\nuse in bin.\n2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-r[1767/1952]\nuse in bin.\n2018-06-22 11:53:38.547459: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-req\nuested in use in bin.\n2018-06-22 11:53:38.547482: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-reque\nsted in use in bin.\n2018-06-22 11:53:38.547504: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use\nin bin.\n2018-06-22 11:53:38.547525: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 1, Chunks in use: 0. 4.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in\nuse in bin.\n2018-06-22 11:53:38.547544: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use\nin bin.\n2018-06-22 11:53:38.547568: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 5, Chunks in use: 4. 135.5KiB allocated for chunks. 110.0KiB in use in bin. 108.8Ki\nB client-requested in use in bin.\n2018-06-22 11:53:38.547593: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 920, Chunks in use: 918. 28.87MiB allocated for chunks. 28.81MiB in use in bin. 28.\n69MiB client-requested in use in bin.\n2018-06-22 11:53:38.547617: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 410, Chunks in use: 410. 26.12MiB allocated for chunks. 26.12MiB in use in bin. 25.\n66MiB client-requested in use in bin.\n2018-06-22 11:53:38.547640: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 20, Chunks in use: 20. 2.69MiB allocated for chunks. 2.69MiB in use in bin. 2.69MiB\nclient-requested in use in bin.\n.\n.\n.\n.\n.\n.\n.\n2018-06-22 11:53:38.548461: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc700 of size 512                                                                         [1691/1952]\n2018-06-22 11:53:38.548478: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc900 of size 131072\n2018-06-22 11:53:38.548494: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dc900 of size 512\n2018-06-22 11:53:38.548510: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dcb00 of size 131072\n2018-06-22 11:53:38.548526: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcb00 of size 512\n2018-06-22 11:53:38.548542: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcd00 of size 131072\n2018-06-22 11:53:38.548558: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cd00 of size 512\n2018-06-22 11:53:38.548574: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cf00 of size 131072\n2018-06-22 11:53:38.548589: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3cf00 of size 512\n2018-06-22 11:53:38.548605: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3d100 of size 131072\n2018-06-22 11:53:38.548622: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d100 of size 512\n2018-06-22 11:53:38.548638: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d300 of size 131072\n2018-06-22 11:53:38.548654: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d300 of size 512\n2018-06-22 11:53:38.548669: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d500 of size 131072\n2018-06-22 11:53:38.548685: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d500 of size 512\n2018-06-22 11:53:38.548702: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d700 of size 7570432\n2018-06-22 11:53:38.548718: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091d5b00 of size 29696\n2018-06-22 11:53:38.548735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091dcf00 of size 32768\n2018-06-22 11:53:38.548752: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091e4f00 of size 32768\n2018-06-22 11:53:38.548768: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091ecf00 of size 32768\n2018-06-22 11:53:38.548783: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091f4f00 of size 32768\n.\n.\n.\n.\n.\n.\n.\n.\n.\n2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:\nLimit:                  5272961024\n2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:                                                                                                       [19/1952]\nLimit:                  5272961024\nInUse:                  3087996928\nMaxInUse:               3174426368\nNumAllocs:                 2214674\nMaxAllocSize:            140052992\n2018-06-22 11:53:38.796832: W tensorflow/core/common_runtime/bfc_allocator.cc:279] _______*********_***\n2018-06-22 11:53:38.796866: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at bias_op.cc:341 : Resource exhausted: OOM when allocating tensor with shape[3200,7393] and type float on\n/job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc", "body": "### System information\r\n- **Have I written custom code**:Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**:source\r\n- **TensorFlow version**:1.8.0-gpu\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**:GeForce GTX 1060 / 6G\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **Exact command to reproduce**:\r\nGeneral training model steps\r\n\r\n### Describe the problem\r\nI wrote a Neural Language Model using TF eager mode, which has only one LSTM cell.\r\nSome of these hyperparameters are:\r\n\r\nhidden_dim : 128\r\nvocab_size : 7393\r\nseq_len : <=50\r\nbatch_size : 32\r\n\r\nIn particular, I did not use the inherited keras base class to write the model, but instead used tfe.Variable to gradually implement some of the details of the model. The model was normal during the early training and performed well.  However, a memory exhaustion error occurs every time the model runs to 960 steps or so.  I think it's because some variables haven't released this error that has been accumulated during training.  Because I still have this error in 960 steps after adjusting batch-size. \r\nThe following is the error log, I think this is a TF eager mode memory usage problem, however the custom model is a necessary choice for some attempts.\r\nHow can we avoid this problem and let the model train smoothly?\r\nThanks a lot.\r\n\r\n### Source code / logs\r\nloss at epoch 0 step 954: 6.665831\r\nloss at epoch 0 step 955: 6.716213\r\nloss at epoch 0 step 956: 6.761374\r\nloss at epoch 0 step 957: 6.712702\r\nloss at epoch 0 step 958: 6.753476\r\nloss at epoch 0 step 959: 6.481436\r\nloss at epoch 0 step 960: 6.544363\r\nloss at epoch 0 step 961: 6.562449\r\nloss at epoch 0 step 962: 6.701943\r\nloss at epoch 0 step 963: 6.415178\r\nloss at epoch 0 step 964: 6.630089\r\nloss at epoch 0 step 965: 6.580114\r\n2018-06-22 11:53:38.547278: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 92.05MiB.  Current allocation summary follows.\r\n2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-requested in\r\n use in bin.\r\n2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-r[1767/1952]\r\n use in bin.\r\n2018-06-22 11:53:38.547459: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-req\r\nuested in use in bin.\r\n2018-06-22 11:53:38.547482: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-reque\r\nsted in use in bin.\r\n2018-06-22 11:53:38.547504: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use \r\nin bin.\r\n2018-06-22 11:53:38.547525: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 1, Chunks in use: 0. 4.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in \r\nuse in bin.\r\n2018-06-22 11:53:38.547544: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use \r\nin bin.\r\n2018-06-22 11:53:38.547568: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 5, Chunks in use: 4. 135.5KiB allocated for chunks. 110.0KiB in use in bin. 108.8Ki\r\nB client-requested in use in bin.\r\n2018-06-22 11:53:38.547593: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 920, Chunks in use: 918. 28.87MiB allocated for chunks. 28.81MiB in use in bin. 28.\r\n69MiB client-requested in use in bin.\r\n2018-06-22 11:53:38.547617: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 410, Chunks in use: 410. 26.12MiB allocated for chunks. 26.12MiB in use in bin. 25.\r\n66MiB client-requested in use in bin.\r\n2018-06-22 11:53:38.547640: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 20, Chunks in use: 20. 2.69MiB allocated for chunks. 2.69MiB in use in bin. 2.69MiB\r\n client-requested in use in bin.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n2018-06-22 11:53:38.548461: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc700 of size 512                                                                         [1691/1952]\r\n2018-06-22 11:53:38.548478: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc900 of size 131072\r\n2018-06-22 11:53:38.548494: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dc900 of size 512\r\n2018-06-22 11:53:38.548510: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dcb00 of size 131072\r\n2018-06-22 11:53:38.548526: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcb00 of size 512\r\n2018-06-22 11:53:38.548542: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcd00 of size 131072\r\n2018-06-22 11:53:38.548558: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cd00 of size 512\r\n2018-06-22 11:53:38.548574: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cf00 of size 131072\r\n2018-06-22 11:53:38.548589: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3cf00 of size 512\r\n2018-06-22 11:53:38.548605: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3d100 of size 131072\r\n2018-06-22 11:53:38.548622: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d100 of size 512\r\n2018-06-22 11:53:38.548638: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d300 of size 131072\r\n2018-06-22 11:53:38.548654: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d300 of size 512\r\n2018-06-22 11:53:38.548669: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d500 of size 131072\r\n2018-06-22 11:53:38.548685: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d500 of size 512\r\n2018-06-22 11:53:38.548702: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d700 of size 7570432\r\n2018-06-22 11:53:38.548718: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091d5b00 of size 29696\r\n2018-06-22 11:53:38.548735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091dcf00 of size 32768\r\n2018-06-22 11:53:38.548752: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091e4f00 of size 32768\r\n2018-06-22 11:53:38.548768: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091ecf00 of size 32768\r\n2018-06-22 11:53:38.548783: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091f4f00 of size 32768\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: \r\nLimit:                  5272961024\r\n2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:                                                                                                       [19/1952]\r\nLimit:                  5272961024\r\nInUse:                  3087996928\r\nMaxInUse:               3174426368\r\nNumAllocs:                 2214674\r\nMaxAllocSize:            140052992\r\n\r\n2018-06-22 11:53:38.796832: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *************_************************************_***********_**_**_**_**_**_******_**_***_**_*****\r\n2018-06-22 11:53:38.796866: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at bias_op.cc:341 : Resource exhausted: OOM when allocating tensor with shape[3200,7393] and type float on \r\n/job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n"}