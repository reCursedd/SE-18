{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/399873140", "html_url": "https://github.com/tensorflow/tensorflow/issues/20218#issuecomment-399873140", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20218", "id": 399873140, "node_id": "MDEyOklzc3VlQ29tbWVudDM5OTg3MzE0MA==", "user": {"login": "qijimrc", "id": 28889175, "node_id": "MDQ6VXNlcjI4ODg5MTc1", "avatar_url": "https://avatars1.githubusercontent.com/u/28889175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qijimrc", "html_url": "https://github.com/qijimrc", "followers_url": "https://api.github.com/users/qijimrc/followers", "following_url": "https://api.github.com/users/qijimrc/following{/other_user}", "gists_url": "https://api.github.com/users/qijimrc/gists{/gist_id}", "starred_url": "https://api.github.com/users/qijimrc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qijimrc/subscriptions", "organizations_url": "https://api.github.com/users/qijimrc/orgs", "repos_url": "https://api.github.com/users/qijimrc/repos", "events_url": "https://api.github.com/users/qijimrc/events{/privacy}", "received_events_url": "https://api.github.com/users/qijimrc/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-25T08:27:45Z", "updated_at": "2018-06-25T08:40:45Z", "author_association": "NONE", "body_html": "<p>Hi alextp, thank you very much for you helping. The following is the code of part of my model:</p>\n<p>`<br>\nimport tensorflow as tf<br>\nimport tensorflow.contrib.eager as tfe<br>\nimport numpy as np</p>\n<pre><code>class Model():\n    def __init__(self,params,vocab_size):\n\n        # Hy-parameters\n        self.vocab_size = vocab_size\n        self.embedding_dim = params.embedding_dim\n        self.hidden_dim = params.hidden_dim\n        self.num_layers = params.num_layers\n        self.keep_ratio = params.keep_ratio\n        self.time_major = params.time_major\n\n        # Embeddings\n        self.embedding = Embedding(self.vocab_size,self.embedding_dim,self.time_major)\n\n        # Dynamic RNN\n        self.rnn = RNN(self.hidden_dim, self.embedding_dim,self.num_layers,\n                               self.keep_ratio,self.time_major, name='Forward')\n\n        # Fully Connections Layer\n        self.fc = FC(self.embedding_dim,self.vocab_size,self.time_major)\n\n        # Build all trainable variables\n        self.build()\n\n        # Add saver\n        self.saver = tfe.Saver(self.variables)\n\n    def build(self):\n        # initialize trainable variables\n        self.variables = []\n\n        self.variables.append(self.embedding.embedding)\n        for variable in self.rnn.variables:\n            self.variables.append(variable)\n        self.variables.append(self.fc.weights)\n        self.variables.append(self.fc.bias)\n\n    def __call__(self,inputs,training):\n        # Parallel computing\n        if self.time_major:\n            inputs = tf.transpose(inputs,(1,0))\n        # Get word embeddings for all\n        embed = self.embedding(inputs)\n        # moving\n        rnn_outputs = self.rnn(embed,training=training)\n\n        # Fully connections\n        logits = self.fc(rnn_outputs)\n        if self.time_major:\n            seq_len,batch_size,last_dim=logits.shape.as_list()\n            logits = tf.reshape(logits,(batch_size,seq_len,-1))\n\n        return logits\nclass Embedding():\n    # Embedding with different lengths.\n    def __init__(self, vocab_size, embedding_dim,time_major=True):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.time_major = time_major\n\n        self.embedding = tfe.Variable(initial_value=tf.random_normal(shape=(vocab_size,embedding_dim)),dtype=tf.float32,name='embedding',trainable=True)\n\n    def __call__(self, x):\n        # implement word embedding\n        # whether parallel computing\n        if self.time_major:\n                #all_embedded=tf.nn.embedding_lookup(self.embedding,x)\n                seq_len, batch_size = x.shape.as_list()\n                x = tf.one_hot(x, depth=self.vocab_size)\n                flat_x = tf.reshape(x, shape=(-1,self.vocab_size))\n                all_embedded = tf.matmul(flat_x,self.embedding)\n                all_embedded = tf.reshape(all_embedded,(seq_len,batch_size,-1))\n        else:\n                all_embedded = []\n                for seq in x:\n                      #embedded = tf.nn.embedding_lookup(self.embedding,seq)\n                      seq = tf.one_hot(seq, depth=self.vocab_size)\n                      embedded = tf.matmul(seq, self.embedding)\n                      all_embedded.append(embedded)\n\n        return all_embedded\nclass LSTMCell():\n    def __init__(self,inputs_dim, hidden_dim, name):\n\n        self.inputs_dim = inputs_dim\n        self.hidden_dim = hidden_dim\n\n        with tf.name_scope(name=name):\n            # Forget gate\n            self.W_f = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n                                    dtype=tf.float32, name='forget_weights', trainable=True)\n            self.b_f = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n                                    dtype=tf.float32, name='forget_bias', trainable=True)\n            # Input gate\n            self.W_i = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n                                    dtype=tf.float32, name='input_weights', trainable=True)\n            self.b_i = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n                                    dtype=tf.float32, name='input_bias', trainable=True)\n            self.W_c = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n                                    dtype=tf.float32, name='input_filter_weights', trainable=True)\n            self.b_c = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n                                    dtype=tf.float32, name='input_filter_bias', trainable=True)\n            # Output gate\n            self.W_o = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n                                    dtype=tf.float32, name='output_weights', trainable=True)\n            self.b_o = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n                                dtype=tf.float32, name='output_bias', trainable=True)\n\n            self.build()\n\n    def build(self,):\n        # build all variables\n        self.variables = [self.W_f, self.b_f,self.W_i, self.b_i,\n                          self.W_c, self.b_c,self.W_o, self.b_o]\n\n    def zero_state(self,batch_size):\n        c_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='c_t')\n        h_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='h_t')\n        return c_t, h_t\n\n    def __call__(self,x_t,c_t,h_t):\n        # Define LSTM forward propagation\n\n        # Forget\n        f_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_f,self.b_f),name='Forget_Gate')\n        # Input\n        i_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_i,self.b_i),name='Input_Gate')\n        uc_t=tf.tanh(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_c,self.b_c),name='Input_Filter')\n        # Update Cell State\n        c_t = f_t*c_t + i_t*uc_t\n        # Output\n        o_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_o,self.b_o),name='Output_Gate')\n        h_t = o_t * tf.tanh(c_t)\n\n        return o_t,c_t,h_t\nclass RNN():\n    # A static RNN. Similar to tf.nn.static_rnn, implemented as a class.\n    def __init__(self, hidden_dim,inputs_dim, num_layers, keep_ratio,time_major=True,name=''):\n        self.keep_ratio = keep_ratio\n        self.time_major = time_major\n        self.inputs_dim=inputs_dim\n        self.hidden_dim=hidden_dim\n\n        self.cells = [\n            LSTMCell(inputs_dim,hidden_dim,name=name+'_LSTMCell'+str(idx))\n            for idx in range(num_layers)\n        ]\n\n        self.build()\n\n    def build(self):\n        # build all trainable variables\n        self.variables = []\n\n        for cell in self.cells:\n            for variable in cell.variables:\n                self.variables.append(variable)\n\n\n    def __call__(self, inputs_seq, training=False):\n        # rnn with different length sequences, inputs : [batch, (embedded tensor)]\n\n        # parallel computing\n        if self.time_major:\n            seq_len,batch_size,last_dim = inputs_seq.shape.as_list()\n            for cell in self.cells:\n                c_t, h_t = cell.zero_state(batch_size)\n                outputs = []\n                for inp in inputs_seq:\n                    output, c_t, h_t = cell(inp, c_t, h_t)\n                    outputs.append(output)\n\n                inputs_seq = tf.stack(outputs, axis=0)\n                if training:\n                    inputs_seq = tf.nn.dropout(inputs_seq, self.keep_ratio)\n            batch_outputs = inputs_seq\n        # linear computing\n        else:\n            for cell in self.cells:\n                c_t,h_t = cell.zero_state(1)\n                outputs = []\n                for seq in inputs_seq:\n                    seq_outputs = []\n                    for word in seq:\n                        word = tf.reshape(word,(1,-1))\n                        output, c_t,h_t = cell(word, c_t,h_t)\n                        if training:\n                            output = tf.nn.dropout(output, self.keep_ratio)\n                        seq_outputs.append(output)\n                    outputs.append(seq_outputs)\n                inputs_seq = outputs\n            batch_outputs = inputs_seq\n        return batch_outputs\nclass FC():\n    # A Fully Connection Layer.\n    def __init__(self, inputs_dim, outputs_dim,time_major=True,active=tf.nn.relu,name=None):\n        self.inputs_dim = inputs_dim\n        self.outputs_dim= outputs_dim\n        self.time_major = time_major\n        self.active = active\n\n        # Build trainable variables\n        self.weights = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim,outputs_dim)),\n                                  dtype=tf.float32,name=name,trainable=True)\n        self.bias = tfe.Variable(initial_value=tf.random_normal(shape=(outputs_dim,)),\n                                  dtype=tf.float32,name=name,trainable=True)\n\n    def __call__(self, x):\n        # different length sequences, inputs : [batch, (RNN outputs tensor)]\n        # whether to parallel computing\n        if self.time_major:\n            seq_len,batch_size,last_dim = x.shape.as_list()\n            # flatten\n            flat = tf.reshape(x,(-1,last_dim))\n            fc = tf.nn.xw_plus_b(flat,self.weights,self.bias)\n            outputs = self.active(fc)\n            outputs = tf.reshape(outputs,(seq_len,batch_size,-1))\n        else:\n            outputs = []\n            for seq in x:\n                fc = tf.nn.xw_plus_b(seq, self.weights, self.bias)\n                fc = self.active(fc)\n                outputs.append(fc)\n        return outputs\n</code></pre>\n<p>`</p>\n<p>Part of the training code is as follows:</p>\n<p><code>with tf.GradientTape() as tape: logits = model(batch_forward,batch_backward,training=True) loss = training_helper.loss_fn_time_major(batch_labels,logits,batch_lengths) # add loss summary tf.contrib.summary.scalar('loss', loss) grads = tape.gradient(loss, model.variables) optimizer.apply_gradients(zip(grads,model.variables)) print('loss at epoch %d step %d: %f' % (epoch,train_step,loss))</code></p>\n<p>I used your recommended settings turning log_device_placement=True, then I observed an interesting phenomenon. At every certain training step, there are three operation logs that are cycled, and the interval between the training steps is gradually decreasing.<br>\nThe following is the log after the setting log_device_placement=True is turned on. It can be seen, at the beginning, a log of three operations is printed every 16 training steps. As the training process progresses, a log of three operations is printed every three training steps until the OOM problem happened. I don't know why and what these three operations represent respectively. I hope this information will help solve the problem. Thank you again for your reply.</p>\n<h3>some logs</h3>\n<p>loss at epoch 0 step 805: 6.740626<br>\nloss at epoch 0 step 806: 7.095036<br>\nloss at epoch 0 step 807: 7.000319<br>\nloss at epoch 0 step 808: 6.904347<br>\nloss at epoch 0 step 809: 6.922751<br>\nloss at epoch 0 step 810: 6.838681<br>\nloss at epoch 0 step 811: 6.905680<br>\nloss at epoch 0 step 812: 7.053514<br>\nloss at epoch 0 step 813: 6.855484<br>\nloss at epoch 0 step 814: 6.755508<br>\nloss at epoch 0 step 815: 7.050679<br>\nloss at epoch 0 step 816: 7.023479<br>\nloss at epoch 0 step 817: 6.827959<br>\nloss at epoch 0 step 818: 6.776998<br>\nloss at epoch 0 step 819: 6.765797<br>\nloss at epoch 0 step 820: 6.759283<br>\n2018-06-25 14:57:15.878743: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:15.935075: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:15.970760: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 821: 6.984869<br>\nloss at epoch 0 step 822: 6.748662<br>\nloss at epoch 0 step 823: 6.772003<br>\nloss at epoch 0 step 824: 6.942709<br>\nloss at epoch 0 step 825: 6.892979<br>\nloss at epoch 0 step 826: 6.680350<br>\nloss at epoch 0 step 827: 6.867269<br>\nloss at epoch 0 step 828: 7.118308<br>\nloss at epoch 0 step 829: 6.601588<br>\nloss at epoch 0 step 830: 6.652170<br>\nloss at epoch 0 step 831: 7.263901<br>\nloss at epoch 0 step 832: 6.902826<br>\nloss at epoch 0 step 833: 6.791662<br>\nloss at epoch 0 step 834: 7.087551<br>\nloss at epoch 0 step 835: 6.820101<br>\n2018-06-25 14:57:18.650329: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:18.710324: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:18.746490: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 836: 7.031796<br>\nloss at epoch 0 step 837: 6.905032<br>\nloss at epoch 0 step 838: 6.889287<br>\nloss at epoch 0 step 839: 6.732944<br>\nloss at epoch 0 step 840: 6.715842<br>\nloss at epoch 0 step 841: 6.757300<br>\nloss at epoch 0 step 842: 6.882929<br>\nloss at epoch 0 step 843: 6.658679<br>\nloss at epoch 0 step 844: 6.788300<br>\nloss at epoch 0 step 845: 6.952966<br>\nloss at epoch 0 step 846: 7.061527<br>\nloss at epoch 0 step 847: 6.603632<br>\nloss at epoch 0 step 848: 7.163596<br>\nloss at epoch 0 step 849: 7.038760<br>\n2018-06-25 14:57:21.325933: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:21.386127: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:21.423910: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 850: 6.885034<br>\nloss at epoch 0 step 851: 6.931499<br>\nloss at epoch 0 step 852: 6.713628<br>\nloss at epoch 0 step 853: 6.777599<br>\nloss at epoch 0 step 854: 6.799969<br>\nloss at epoch 0 step 855: 6.985339<br>\nloss at epoch 0 step 856: 6.620006<br>\nloss at epoch 0 step 857: 6.878152<br>\nloss at epoch 0 step 858: 6.783222<br>\nloss at epoch 0 step 859: 6.712417<br>\nloss at epoch 0 step 860: 6.734530<br>\nloss at epoch 0 step 861: 7.009552<br>\n2018-06-25 14:57:23.706079: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:23.769647: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:23.809172: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 862: 6.570765<br>\nloss at epoch 0 step 863: 6.818643<br>\nloss at epoch 0 step 864: 6.707229<br>\nloss at epoch 0 step 865: 6.860226<br>\nloss at epoch 0 step 866: 6.832708<br>\nloss at epoch 0 step 867: 6.746671<br>\nloss at epoch 0 step 868: 6.925843<br>\nloss at epoch 0 step 869: 6.845281<br>\nloss at epoch 0 step 870: 6.901897<br>\nloss at epoch 0 step 871: 6.936728<br>\nloss at epoch 0 step 872: 6.793565<br>\n2018-06-25 14:57:25.894904: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:25.958699: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:25.998658: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 873: 6.795174<br>\nloss at epoch 0 step 874: 6.897914<br>\nloss at epoch 0 step 875: 6.964663<br>\nloss at epoch 0 step 876: 6.933444<br>\nloss at epoch 0 step 877: 6.889087<br>\nloss at epoch 0 step 878: 6.728238<br>\nloss at epoch 0 step 879: 6.543335<br>\nloss at epoch 0 step 880: 6.785100<br>\nloss at epoch 0 step 881: 6.867868<br>\nloss at epoch 0 step 882: 6.626728<br>\n2018-06-25 14:57:28.011803: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:28.078783: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:28.121272: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 883: 6.632171<br>\nloss at epoch 0 step 884: 6.638273<br>\nloss at epoch 0 step 885: 6.727106<br>\nloss at epoch 0 step 886: 6.755214<br>\nloss at epoch 0 step 887: 6.788674<br>\nloss at epoch 0 step 888: 6.618323<br>\nloss at epoch 0 step 889: 6.802135<br>\nloss at epoch 0 step 890: 6.812809<br>\nloss at epoch 0 step 891: 6.905511<br>\n2018-06-25 14:57:29.982565: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:30.048872: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:30.089943: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 892: 6.660409<br>\nloss at epoch 0 step 893: 6.817567<br>\nloss at epoch 0 step 894: 6.782594<br>\nloss at epoch 0 step 895: 6.746886<br>\nloss at epoch 0 step 896: 6.679479<br>\nloss at epoch 0 step 897: 6.696771<br>\nloss at epoch 0 step 898: 6.642287<br>\nloss at epoch 0 step 899: 6.870078<br>\n2018-06-25 14:57:31.693334: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:31.761710: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:31.805339: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 900: 6.774796<br>\nloss at epoch 0 step 901: 6.841524<br>\nloss at epoch 0 step 902: 6.682564<br>\nloss at epoch 0 step 903: 6.693571<br>\nloss at epoch 0 step 904: 6.603614<br>\nloss at epoch 0 step 905: 6.705801<br>\nloss at epoch 0 step 906: 6.713558<br>\nloss at epoch 0 step 907: 6.889330<br>\n2018-06-25 14:57:33.459800: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:33.528563: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:33.573187: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 908: 6.726191<br>\nloss at epoch 0 step 909: 6.945642<br>\nloss at epoch 0 step 910: 6.768640<br>\nloss at epoch 0 step 911: 6.801554<br>\nloss at epoch 0 step 912: 6.867862<br>\nloss at epoch 0 step 913: 6.563491<br>\nloss at epoch 0 step 914: 6.853379<br>\nloss at epoch 0 step 915: 6.724048<br>\n2018-06-25 14:57:35.341966: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:35.412082: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:35.456774: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 916: 6.639415<br>\nloss at epoch 0 step 917: 6.881902<br>\nloss at epoch 0 step 918: 6.735335<br>\nloss at epoch 0 step 919: 6.688423<br>\nloss at epoch 0 step 920: 6.636367<br>\nloss at epoch 0 step 921: 6.717305<br>\nloss at epoch 0 step 922: 6.629582<br>\n2018-06-25 14:57:36.960963: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:37.033898: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:37.079990: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 923: 6.682681<br>\nloss at epoch 0 step 924: 6.882014<br>\nloss at epoch 0 step 925: 6.731138<br>\nloss at epoch 0 step 926: 6.588252<br>\nloss at epoch 0 step 927: 6.824276<br>\nloss at epoch 0 step 928: 6.707603<br>\nloss at epoch 0 step 929: 6.748030<br>\n2018-06-25 14:57:38.690123: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:38.767782: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:38.816409: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 930: 6.658401<br>\nloss at epoch 0 step 931: 6.748807<br>\nloss at epoch 0 step 932: 6.802845<br>\nloss at epoch 0 step 933: 6.590845<br>\nloss at epoch 0 step 934: 6.749659<br>\nloss at epoch 0 step 935: 6.701960<br>\n2018-06-25 14:57:40.158598: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:40.234263: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:40.282647: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 936: 6.775355<br>\nloss at epoch 0 step 937: 6.781133<br>\nloss at epoch 0 step 938: 6.600548<br>\nloss at epoch 0 step 939: 6.747424<br>\nloss at epoch 0 step 940: 6.605201<br>\n2018-06-25 14:57:41.416798: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:41.496970: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:41.550523: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 941: 6.672260<br>\nloss at epoch 0 step 942: 6.614303<br>\nloss at epoch 0 step 943: 6.726663<br>\nloss at epoch 0 step 944: 6.694652<br>\nloss at epoch 0 step 945: 6.590765<br>\n2018-06-25 14:57:42.720835: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:42.800870: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:42.852718: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 946: 6.639668<br>\nloss at epoch 0 step 947: 6.581154<br>\nloss at epoch 0 step 948: 6.780156<br>\nloss at epoch 0 step 949: 6.619957<br>\nloss at epoch 0 step 950: 6.680293<br>\n2018-06-25 14:57:44.034047: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:44.116253: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:44.167625: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 951: 6.767905<br>\nloss at epoch 0 step 952: 6.694592<br>\nloss at epoch 0 step 953: 6.586446<br>\nloss at epoch 0 step 954: 6.691846<br>\n2018-06-25 14:57:45.122539: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:45.204973: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:45.258423: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 955: 6.994371<br>\nloss at epoch 0 step 956: 6.858140<br>\nloss at epoch 0 step 957: 6.646007<br>\nloss at epoch 0 step 958: 6.721205<br>\n2018-06-25 14:57:46.238911: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:46.325236: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:46.380011: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 959: 6.589066<br>\nloss at epoch 0 step 960: 6.683684<br>\nloss at epoch 0 step 961: 6.672047<br>\n2018-06-25 14:57:47.046723: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:47.203337: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:47.260355: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 962: 6.725653<br>\nloss at epoch 0 step 963: 6.720684<br>\nloss at epoch 0 step 964: 6.636925<br>\n2018-06-25 14:57:47.932850: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:48.021669: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:48.077942: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0<br>\nloss at epoch 0 step 965: 6.461943<br>\nloss at epoch 0 step 966: 6.716685<br>\nloss at epoch 0 step 967: 6.541767<br>\n2018-06-25 14:57:48.839285: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0<br>\n2018-06-25 14:57:58.895255: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 90.19MiB.  Current allocation summary follows.<br>\n2018-06-25 14:57:58.895406: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-requested in use in bin.<br>\n2018-06-25 14:57:58.895440: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.<br>\n2018-06-25 14:57:58.895466: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.<br>\n2018-06-25 14:57:58.895488: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.<br>\n2018-06-25 14:57:58.895517: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.<br>\n2018-06-25 14:57:58.895538: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.<br>\n2018-06-25 14:57:58.895565: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 5, Chunks in use: 3. 124.0KiB allocated for chunks. 79.0KiB in use in bin. 78.9KiB client-requested in use in bin.<br>\n2018-06-25 14:57:58.895592: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 903, Chunks in use: 901. 28.30MiB allocated for chunks. 28.24MiB in use in bin. 28.15MiB client-requested in use in bin.<br>\n2018-06-25 14:57:58.895618: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 402, Chunks in use: 402. 25.56MiB allocated for chunks. 25.56MiB in use in bin. 25.16MiB client-requested in use in bin.<br>\n2018-06-25 14:57:58.895639: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 19, Chunks in use: 19. 2.51MiB allocated for chunks. 2.51MiB in use in bin. 2.47MiB client-requested in use in bin.<br>\n2018-06-25 14:57:58.895673: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 8, Chunks in use: 8. 2.97MiB allocated for chunks. 2.97MiB in use in bin. 2.75MiB client-requested in use in bin.</p>", "body_text": "Hi alextp, thank you very much for you helping. The following is the code of part of my model:\n`\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nimport numpy as np\nclass Model():\n    def __init__(self,params,vocab_size):\n\n        # Hy-parameters\n        self.vocab_size = vocab_size\n        self.embedding_dim = params.embedding_dim\n        self.hidden_dim = params.hidden_dim\n        self.num_layers = params.num_layers\n        self.keep_ratio = params.keep_ratio\n        self.time_major = params.time_major\n\n        # Embeddings\n        self.embedding = Embedding(self.vocab_size,self.embedding_dim,self.time_major)\n\n        # Dynamic RNN\n        self.rnn = RNN(self.hidden_dim, self.embedding_dim,self.num_layers,\n                               self.keep_ratio,self.time_major, name='Forward')\n\n        # Fully Connections Layer\n        self.fc = FC(self.embedding_dim,self.vocab_size,self.time_major)\n\n        # Build all trainable variables\n        self.build()\n\n        # Add saver\n        self.saver = tfe.Saver(self.variables)\n\n    def build(self):\n        # initialize trainable variables\n        self.variables = []\n\n        self.variables.append(self.embedding.embedding)\n        for variable in self.rnn.variables:\n            self.variables.append(variable)\n        self.variables.append(self.fc.weights)\n        self.variables.append(self.fc.bias)\n\n    def __call__(self,inputs,training):\n        # Parallel computing\n        if self.time_major:\n            inputs = tf.transpose(inputs,(1,0))\n        # Get word embeddings for all\n        embed = self.embedding(inputs)\n        # moving\n        rnn_outputs = self.rnn(embed,training=training)\n\n        # Fully connections\n        logits = self.fc(rnn_outputs)\n        if self.time_major:\n            seq_len,batch_size,last_dim=logits.shape.as_list()\n            logits = tf.reshape(logits,(batch_size,seq_len,-1))\n\n        return logits\nclass Embedding():\n    # Embedding with different lengths.\n    def __init__(self, vocab_size, embedding_dim,time_major=True):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.time_major = time_major\n\n        self.embedding = tfe.Variable(initial_value=tf.random_normal(shape=(vocab_size,embedding_dim)),dtype=tf.float32,name='embedding',trainable=True)\n\n    def __call__(self, x):\n        # implement word embedding\n        # whether parallel computing\n        if self.time_major:\n                #all_embedded=tf.nn.embedding_lookup(self.embedding,x)\n                seq_len, batch_size = x.shape.as_list()\n                x = tf.one_hot(x, depth=self.vocab_size)\n                flat_x = tf.reshape(x, shape=(-1,self.vocab_size))\n                all_embedded = tf.matmul(flat_x,self.embedding)\n                all_embedded = tf.reshape(all_embedded,(seq_len,batch_size,-1))\n        else:\n                all_embedded = []\n                for seq in x:\n                      #embedded = tf.nn.embedding_lookup(self.embedding,seq)\n                      seq = tf.one_hot(seq, depth=self.vocab_size)\n                      embedded = tf.matmul(seq, self.embedding)\n                      all_embedded.append(embedded)\n\n        return all_embedded\nclass LSTMCell():\n    def __init__(self,inputs_dim, hidden_dim, name):\n\n        self.inputs_dim = inputs_dim\n        self.hidden_dim = hidden_dim\n\n        with tf.name_scope(name=name):\n            # Forget gate\n            self.W_f = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n                                    dtype=tf.float32, name='forget_weights', trainable=True)\n            self.b_f = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n                                    dtype=tf.float32, name='forget_bias', trainable=True)\n            # Input gate\n            self.W_i = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n                                    dtype=tf.float32, name='input_weights', trainable=True)\n            self.b_i = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n                                    dtype=tf.float32, name='input_bias', trainable=True)\n            self.W_c = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n                                    dtype=tf.float32, name='input_filter_weights', trainable=True)\n            self.b_c = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n                                    dtype=tf.float32, name='input_filter_bias', trainable=True)\n            # Output gate\n            self.W_o = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n                                    dtype=tf.float32, name='output_weights', trainable=True)\n            self.b_o = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n                                dtype=tf.float32, name='output_bias', trainable=True)\n\n            self.build()\n\n    def build(self,):\n        # build all variables\n        self.variables = [self.W_f, self.b_f,self.W_i, self.b_i,\n                          self.W_c, self.b_c,self.W_o, self.b_o]\n\n    def zero_state(self,batch_size):\n        c_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='c_t')\n        h_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='h_t')\n        return c_t, h_t\n\n    def __call__(self,x_t,c_t,h_t):\n        # Define LSTM forward propagation\n\n        # Forget\n        f_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_f,self.b_f),name='Forget_Gate')\n        # Input\n        i_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_i,self.b_i),name='Input_Gate')\n        uc_t=tf.tanh(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_c,self.b_c),name='Input_Filter')\n        # Update Cell State\n        c_t = f_t*c_t + i_t*uc_t\n        # Output\n        o_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_o,self.b_o),name='Output_Gate')\n        h_t = o_t * tf.tanh(c_t)\n\n        return o_t,c_t,h_t\nclass RNN():\n    # A static RNN. Similar to tf.nn.static_rnn, implemented as a class.\n    def __init__(self, hidden_dim,inputs_dim, num_layers, keep_ratio,time_major=True,name=''):\n        self.keep_ratio = keep_ratio\n        self.time_major = time_major\n        self.inputs_dim=inputs_dim\n        self.hidden_dim=hidden_dim\n\n        self.cells = [\n            LSTMCell(inputs_dim,hidden_dim,name=name+'_LSTMCell'+str(idx))\n            for idx in range(num_layers)\n        ]\n\n        self.build()\n\n    def build(self):\n        # build all trainable variables\n        self.variables = []\n\n        for cell in self.cells:\n            for variable in cell.variables:\n                self.variables.append(variable)\n\n\n    def __call__(self, inputs_seq, training=False):\n        # rnn with different length sequences, inputs : [batch, (embedded tensor)]\n\n        # parallel computing\n        if self.time_major:\n            seq_len,batch_size,last_dim = inputs_seq.shape.as_list()\n            for cell in self.cells:\n                c_t, h_t = cell.zero_state(batch_size)\n                outputs = []\n                for inp in inputs_seq:\n                    output, c_t, h_t = cell(inp, c_t, h_t)\n                    outputs.append(output)\n\n                inputs_seq = tf.stack(outputs, axis=0)\n                if training:\n                    inputs_seq = tf.nn.dropout(inputs_seq, self.keep_ratio)\n            batch_outputs = inputs_seq\n        # linear computing\n        else:\n            for cell in self.cells:\n                c_t,h_t = cell.zero_state(1)\n                outputs = []\n                for seq in inputs_seq:\n                    seq_outputs = []\n                    for word in seq:\n                        word = tf.reshape(word,(1,-1))\n                        output, c_t,h_t = cell(word, c_t,h_t)\n                        if training:\n                            output = tf.nn.dropout(output, self.keep_ratio)\n                        seq_outputs.append(output)\n                    outputs.append(seq_outputs)\n                inputs_seq = outputs\n            batch_outputs = inputs_seq\n        return batch_outputs\nclass FC():\n    # A Fully Connection Layer.\n    def __init__(self, inputs_dim, outputs_dim,time_major=True,active=tf.nn.relu,name=None):\n        self.inputs_dim = inputs_dim\n        self.outputs_dim= outputs_dim\n        self.time_major = time_major\n        self.active = active\n\n        # Build trainable variables\n        self.weights = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim,outputs_dim)),\n                                  dtype=tf.float32,name=name,trainable=True)\n        self.bias = tfe.Variable(initial_value=tf.random_normal(shape=(outputs_dim,)),\n                                  dtype=tf.float32,name=name,trainable=True)\n\n    def __call__(self, x):\n        # different length sequences, inputs : [batch, (RNN outputs tensor)]\n        # whether to parallel computing\n        if self.time_major:\n            seq_len,batch_size,last_dim = x.shape.as_list()\n            # flatten\n            flat = tf.reshape(x,(-1,last_dim))\n            fc = tf.nn.xw_plus_b(flat,self.weights,self.bias)\n            outputs = self.active(fc)\n            outputs = tf.reshape(outputs,(seq_len,batch_size,-1))\n        else:\n            outputs = []\n            for seq in x:\n                fc = tf.nn.xw_plus_b(seq, self.weights, self.bias)\n                fc = self.active(fc)\n                outputs.append(fc)\n        return outputs\n\n`\nPart of the training code is as follows:\nwith tf.GradientTape() as tape: logits = model(batch_forward,batch_backward,training=True) loss = training_helper.loss_fn_time_major(batch_labels,logits,batch_lengths) # add loss summary tf.contrib.summary.scalar('loss', loss) grads = tape.gradient(loss, model.variables) optimizer.apply_gradients(zip(grads,model.variables)) print('loss at epoch %d step %d: %f' % (epoch,train_step,loss))\nI used your recommended settings turning log_device_placement=True, then I observed an interesting phenomenon. At every certain training step, there are three operation logs that are cycled, and the interval between the training steps is gradually decreasing.\nThe following is the log after the setting log_device_placement=True is turned on. It can be seen, at the beginning, a log of three operations is printed every 16 training steps. As the training process progresses, a log of three operations is printed every three training steps until the OOM problem happened. I don't know why and what these three operations represent respectively. I hope this information will help solve the problem. Thank you again for your reply.\nsome logs\nloss at epoch 0 step 805: 6.740626\nloss at epoch 0 step 806: 7.095036\nloss at epoch 0 step 807: 7.000319\nloss at epoch 0 step 808: 6.904347\nloss at epoch 0 step 809: 6.922751\nloss at epoch 0 step 810: 6.838681\nloss at epoch 0 step 811: 6.905680\nloss at epoch 0 step 812: 7.053514\nloss at epoch 0 step 813: 6.855484\nloss at epoch 0 step 814: 6.755508\nloss at epoch 0 step 815: 7.050679\nloss at epoch 0 step 816: 7.023479\nloss at epoch 0 step 817: 6.827959\nloss at epoch 0 step 818: 6.776998\nloss at epoch 0 step 819: 6.765797\nloss at epoch 0 step 820: 6.759283\n2018-06-25 14:57:15.878743: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:15.935075: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:15.970760: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 821: 6.984869\nloss at epoch 0 step 822: 6.748662\nloss at epoch 0 step 823: 6.772003\nloss at epoch 0 step 824: 6.942709\nloss at epoch 0 step 825: 6.892979\nloss at epoch 0 step 826: 6.680350\nloss at epoch 0 step 827: 6.867269\nloss at epoch 0 step 828: 7.118308\nloss at epoch 0 step 829: 6.601588\nloss at epoch 0 step 830: 6.652170\nloss at epoch 0 step 831: 7.263901\nloss at epoch 0 step 832: 6.902826\nloss at epoch 0 step 833: 6.791662\nloss at epoch 0 step 834: 7.087551\nloss at epoch 0 step 835: 6.820101\n2018-06-25 14:57:18.650329: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:18.710324: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:18.746490: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 836: 7.031796\nloss at epoch 0 step 837: 6.905032\nloss at epoch 0 step 838: 6.889287\nloss at epoch 0 step 839: 6.732944\nloss at epoch 0 step 840: 6.715842\nloss at epoch 0 step 841: 6.757300\nloss at epoch 0 step 842: 6.882929\nloss at epoch 0 step 843: 6.658679\nloss at epoch 0 step 844: 6.788300\nloss at epoch 0 step 845: 6.952966\nloss at epoch 0 step 846: 7.061527\nloss at epoch 0 step 847: 6.603632\nloss at epoch 0 step 848: 7.163596\nloss at epoch 0 step 849: 7.038760\n2018-06-25 14:57:21.325933: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:21.386127: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:21.423910: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 850: 6.885034\nloss at epoch 0 step 851: 6.931499\nloss at epoch 0 step 852: 6.713628\nloss at epoch 0 step 853: 6.777599\nloss at epoch 0 step 854: 6.799969\nloss at epoch 0 step 855: 6.985339\nloss at epoch 0 step 856: 6.620006\nloss at epoch 0 step 857: 6.878152\nloss at epoch 0 step 858: 6.783222\nloss at epoch 0 step 859: 6.712417\nloss at epoch 0 step 860: 6.734530\nloss at epoch 0 step 861: 7.009552\n2018-06-25 14:57:23.706079: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:23.769647: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:23.809172: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 862: 6.570765\nloss at epoch 0 step 863: 6.818643\nloss at epoch 0 step 864: 6.707229\nloss at epoch 0 step 865: 6.860226\nloss at epoch 0 step 866: 6.832708\nloss at epoch 0 step 867: 6.746671\nloss at epoch 0 step 868: 6.925843\nloss at epoch 0 step 869: 6.845281\nloss at epoch 0 step 870: 6.901897\nloss at epoch 0 step 871: 6.936728\nloss at epoch 0 step 872: 6.793565\n2018-06-25 14:57:25.894904: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:25.958699: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:25.998658: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 873: 6.795174\nloss at epoch 0 step 874: 6.897914\nloss at epoch 0 step 875: 6.964663\nloss at epoch 0 step 876: 6.933444\nloss at epoch 0 step 877: 6.889087\nloss at epoch 0 step 878: 6.728238\nloss at epoch 0 step 879: 6.543335\nloss at epoch 0 step 880: 6.785100\nloss at epoch 0 step 881: 6.867868\nloss at epoch 0 step 882: 6.626728\n2018-06-25 14:57:28.011803: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:28.078783: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:28.121272: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 883: 6.632171\nloss at epoch 0 step 884: 6.638273\nloss at epoch 0 step 885: 6.727106\nloss at epoch 0 step 886: 6.755214\nloss at epoch 0 step 887: 6.788674\nloss at epoch 0 step 888: 6.618323\nloss at epoch 0 step 889: 6.802135\nloss at epoch 0 step 890: 6.812809\nloss at epoch 0 step 891: 6.905511\n2018-06-25 14:57:29.982565: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:30.048872: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:30.089943: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 892: 6.660409\nloss at epoch 0 step 893: 6.817567\nloss at epoch 0 step 894: 6.782594\nloss at epoch 0 step 895: 6.746886\nloss at epoch 0 step 896: 6.679479\nloss at epoch 0 step 897: 6.696771\nloss at epoch 0 step 898: 6.642287\nloss at epoch 0 step 899: 6.870078\n2018-06-25 14:57:31.693334: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:31.761710: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:31.805339: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 900: 6.774796\nloss at epoch 0 step 901: 6.841524\nloss at epoch 0 step 902: 6.682564\nloss at epoch 0 step 903: 6.693571\nloss at epoch 0 step 904: 6.603614\nloss at epoch 0 step 905: 6.705801\nloss at epoch 0 step 906: 6.713558\nloss at epoch 0 step 907: 6.889330\n2018-06-25 14:57:33.459800: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:33.528563: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:33.573187: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 908: 6.726191\nloss at epoch 0 step 909: 6.945642\nloss at epoch 0 step 910: 6.768640\nloss at epoch 0 step 911: 6.801554\nloss at epoch 0 step 912: 6.867862\nloss at epoch 0 step 913: 6.563491\nloss at epoch 0 step 914: 6.853379\nloss at epoch 0 step 915: 6.724048\n2018-06-25 14:57:35.341966: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:35.412082: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:35.456774: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 916: 6.639415\nloss at epoch 0 step 917: 6.881902\nloss at epoch 0 step 918: 6.735335\nloss at epoch 0 step 919: 6.688423\nloss at epoch 0 step 920: 6.636367\nloss at epoch 0 step 921: 6.717305\nloss at epoch 0 step 922: 6.629582\n2018-06-25 14:57:36.960963: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:37.033898: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:37.079990: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 923: 6.682681\nloss at epoch 0 step 924: 6.882014\nloss at epoch 0 step 925: 6.731138\nloss at epoch 0 step 926: 6.588252\nloss at epoch 0 step 927: 6.824276\nloss at epoch 0 step 928: 6.707603\nloss at epoch 0 step 929: 6.748030\n2018-06-25 14:57:38.690123: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:38.767782: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:38.816409: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 930: 6.658401\nloss at epoch 0 step 931: 6.748807\nloss at epoch 0 step 932: 6.802845\nloss at epoch 0 step 933: 6.590845\nloss at epoch 0 step 934: 6.749659\nloss at epoch 0 step 935: 6.701960\n2018-06-25 14:57:40.158598: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:40.234263: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:40.282647: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 936: 6.775355\nloss at epoch 0 step 937: 6.781133\nloss at epoch 0 step 938: 6.600548\nloss at epoch 0 step 939: 6.747424\nloss at epoch 0 step 940: 6.605201\n2018-06-25 14:57:41.416798: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:41.496970: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:41.550523: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 941: 6.672260\nloss at epoch 0 step 942: 6.614303\nloss at epoch 0 step 943: 6.726663\nloss at epoch 0 step 944: 6.694652\nloss at epoch 0 step 945: 6.590765\n2018-06-25 14:57:42.720835: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:42.800870: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:42.852718: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 946: 6.639668\nloss at epoch 0 step 947: 6.581154\nloss at epoch 0 step 948: 6.780156\nloss at epoch 0 step 949: 6.619957\nloss at epoch 0 step 950: 6.680293\n2018-06-25 14:57:44.034047: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:44.116253: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:44.167625: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 951: 6.767905\nloss at epoch 0 step 952: 6.694592\nloss at epoch 0 step 953: 6.586446\nloss at epoch 0 step 954: 6.691846\n2018-06-25 14:57:45.122539: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:45.204973: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:45.258423: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 955: 6.994371\nloss at epoch 0 step 956: 6.858140\nloss at epoch 0 step 957: 6.646007\nloss at epoch 0 step 958: 6.721205\n2018-06-25 14:57:46.238911: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:46.325236: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:46.380011: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 959: 6.589066\nloss at epoch 0 step 960: 6.683684\nloss at epoch 0 step 961: 6.672047\n2018-06-25 14:57:47.046723: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:47.203337: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:47.260355: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 962: 6.725653\nloss at epoch 0 step 963: 6.720684\nloss at epoch 0 step 964: 6.636925\n2018-06-25 14:57:47.932850: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:48.021669: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:48.077942: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\nloss at epoch 0 step 965: 6.461943\nloss at epoch 0 step 966: 6.716685\nloss at epoch 0 step 967: 6.541767\n2018-06-25 14:57:48.839285: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n2018-06-25 14:57:58.895255: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 90.19MiB.  Current allocation summary follows.\n2018-06-25 14:57:58.895406: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-requested in use in bin.\n2018-06-25 14:57:58.895440: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.\n2018-06-25 14:57:58.895466: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n2018-06-25 14:57:58.895488: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-06-25 14:57:58.895517: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-06-25 14:57:58.895538: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-06-25 14:57:58.895565: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 5, Chunks in use: 3. 124.0KiB allocated for chunks. 79.0KiB in use in bin. 78.9KiB client-requested in use in bin.\n2018-06-25 14:57:58.895592: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 903, Chunks in use: 901. 28.30MiB allocated for chunks. 28.24MiB in use in bin. 28.15MiB client-requested in use in bin.\n2018-06-25 14:57:58.895618: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 402, Chunks in use: 402. 25.56MiB allocated for chunks. 25.56MiB in use in bin. 25.16MiB client-requested in use in bin.\n2018-06-25 14:57:58.895639: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 19, Chunks in use: 19. 2.51MiB allocated for chunks. 2.51MiB in use in bin. 2.47MiB client-requested in use in bin.\n2018-06-25 14:57:58.895673: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 8, Chunks in use: 8. 2.97MiB allocated for chunks. 2.97MiB in use in bin. 2.75MiB client-requested in use in bin.", "body": "Hi alextp, thank you very much for you helping. The following is the code of part of my model:\r\n\r\n`\r\n    import tensorflow as tf\r\n    import tensorflow.contrib.eager as tfe\r\n    import numpy as np\r\n\r\n    class Model():\r\n        def __init__(self,params,vocab_size):\r\n\r\n            # Hy-parameters\r\n            self.vocab_size = vocab_size\r\n            self.embedding_dim = params.embedding_dim\r\n            self.hidden_dim = params.hidden_dim\r\n            self.num_layers = params.num_layers\r\n            self.keep_ratio = params.keep_ratio\r\n            self.time_major = params.time_major\r\n\r\n            # Embeddings\r\n            self.embedding = Embedding(self.vocab_size,self.embedding_dim,self.time_major)\r\n\r\n            # Dynamic RNN\r\n            self.rnn = RNN(self.hidden_dim, self.embedding_dim,self.num_layers,\r\n                                   self.keep_ratio,self.time_major, name='Forward')\r\n\r\n            # Fully Connections Layer\r\n            self.fc = FC(self.embedding_dim,self.vocab_size,self.time_major)\r\n\r\n            # Build all trainable variables\r\n            self.build()\r\n\r\n            # Add saver\r\n            self.saver = tfe.Saver(self.variables)\r\n\r\n        def build(self):\r\n            # initialize trainable variables\r\n            self.variables = []\r\n\r\n            self.variables.append(self.embedding.embedding)\r\n            for variable in self.rnn.variables:\r\n                self.variables.append(variable)\r\n            self.variables.append(self.fc.weights)\r\n            self.variables.append(self.fc.bias)\r\n\r\n        def __call__(self,inputs,training):\r\n            # Parallel computing\r\n            if self.time_major:\r\n                inputs = tf.transpose(inputs,(1,0))\r\n            # Get word embeddings for all\r\n            embed = self.embedding(inputs)\r\n            # moving\r\n            rnn_outputs = self.rnn(embed,training=training)\r\n\r\n            # Fully connections\r\n            logits = self.fc(rnn_outputs)\r\n            if self.time_major:\r\n                seq_len,batch_size,last_dim=logits.shape.as_list()\r\n                logits = tf.reshape(logits,(batch_size,seq_len,-1))\r\n\r\n            return logits\r\n    class Embedding():\r\n        # Embedding with different lengths.\r\n        def __init__(self, vocab_size, embedding_dim,time_major=True):\r\n            self.vocab_size = vocab_size\r\n            self.embedding_dim = embedding_dim\r\n            self.time_major = time_major\r\n\r\n            self.embedding = tfe.Variable(initial_value=tf.random_normal(shape=(vocab_size,embedding_dim)),dtype=tf.float32,name='embedding',trainable=True)\r\n\r\n        def __call__(self, x):\r\n            # implement word embedding\r\n            # whether parallel computing\r\n            if self.time_major:\r\n                    #all_embedded=tf.nn.embedding_lookup(self.embedding,x)\r\n                    seq_len, batch_size = x.shape.as_list()\r\n                    x = tf.one_hot(x, depth=self.vocab_size)\r\n                    flat_x = tf.reshape(x, shape=(-1,self.vocab_size))\r\n                    all_embedded = tf.matmul(flat_x,self.embedding)\r\n                    all_embedded = tf.reshape(all_embedded,(seq_len,batch_size,-1))\r\n            else:\r\n                    all_embedded = []\r\n                    for seq in x:\r\n                          #embedded = tf.nn.embedding_lookup(self.embedding,seq)\r\n                          seq = tf.one_hot(seq, depth=self.vocab_size)\r\n                          embedded = tf.matmul(seq, self.embedding)\r\n                          all_embedded.append(embedded)\r\n\r\n            return all_embedded\r\n    class LSTMCell():\r\n        def __init__(self,inputs_dim, hidden_dim, name):\r\n\r\n            self.inputs_dim = inputs_dim\r\n            self.hidden_dim = hidden_dim\r\n\r\n            with tf.name_scope(name=name):\r\n                # Forget gate\r\n                self.W_f = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\r\n                                        dtype=tf.float32, name='forget_weights', trainable=True)\r\n                self.b_f = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\r\n                                        dtype=tf.float32, name='forget_bias', trainable=True)\r\n                # Input gate\r\n                self.W_i = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\r\n                                        dtype=tf.float32, name='input_weights', trainable=True)\r\n                self.b_i = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\r\n                                        dtype=tf.float32, name='input_bias', trainable=True)\r\n                self.W_c = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\r\n                                        dtype=tf.float32, name='input_filter_weights', trainable=True)\r\n                self.b_c = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\r\n                                        dtype=tf.float32, name='input_filter_bias', trainable=True)\r\n                # Output gate\r\n                self.W_o = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\r\n                                        dtype=tf.float32, name='output_weights', trainable=True)\r\n                self.b_o = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\r\n                                    dtype=tf.float32, name='output_bias', trainable=True)\r\n    \r\n                self.build()\r\n\r\n        def build(self,):\r\n            # build all variables\r\n            self.variables = [self.W_f, self.b_f,self.W_i, self.b_i,\r\n                              self.W_c, self.b_c,self.W_o, self.b_o]\r\n\r\n        def zero_state(self,batch_size):\r\n            c_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='c_t')\r\n            h_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='h_t')\r\n            return c_t, h_t\r\n\r\n        def __call__(self,x_t,c_t,h_t):\r\n            # Define LSTM forward propagation\r\n\r\n            # Forget\r\n            f_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_f,self.b_f),name='Forget_Gate')\r\n            # Input\r\n            i_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_i,self.b_i),name='Input_Gate')\r\n            uc_t=tf.tanh(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_c,self.b_c),name='Input_Filter')\r\n            # Update Cell State\r\n            c_t = f_t*c_t + i_t*uc_t\r\n            # Output\r\n            o_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_o,self.b_o),name='Output_Gate')\r\n            h_t = o_t * tf.tanh(c_t)\r\n\r\n            return o_t,c_t,h_t\r\n    class RNN():\r\n        # A static RNN. Similar to tf.nn.static_rnn, implemented as a class.\r\n        def __init__(self, hidden_dim,inputs_dim, num_layers, keep_ratio,time_major=True,name=''):\r\n            self.keep_ratio = keep_ratio\r\n            self.time_major = time_major\r\n            self.inputs_dim=inputs_dim\r\n            self.hidden_dim=hidden_dim\r\n\r\n            self.cells = [\r\n                LSTMCell(inputs_dim,hidden_dim,name=name+'_LSTMCell'+str(idx))\r\n                for idx in range(num_layers)\r\n            ]\r\n\r\n            self.build()\r\n\r\n        def build(self):\r\n            # build all trainable variables\r\n            self.variables = []\r\n\r\n            for cell in self.cells:\r\n                for variable in cell.variables:\r\n                    self.variables.append(variable)\r\n\r\n\r\n        def __call__(self, inputs_seq, training=False):\r\n            # rnn with different length sequences, inputs : [batch, (embedded tensor)]\r\n\r\n            # parallel computing\r\n            if self.time_major:\r\n                seq_len,batch_size,last_dim = inputs_seq.shape.as_list()\r\n                for cell in self.cells:\r\n                    c_t, h_t = cell.zero_state(batch_size)\r\n                    outputs = []\r\n                    for inp in inputs_seq:\r\n                        output, c_t, h_t = cell(inp, c_t, h_t)\r\n                        outputs.append(output)\r\n    \r\n                    inputs_seq = tf.stack(outputs, axis=0)\r\n                    if training:\r\n                        inputs_seq = tf.nn.dropout(inputs_seq, self.keep_ratio)\r\n                batch_outputs = inputs_seq\r\n            # linear computing\r\n            else:\r\n                for cell in self.cells:\r\n                    c_t,h_t = cell.zero_state(1)\r\n                    outputs = []\r\n                    for seq in inputs_seq:\r\n                        seq_outputs = []\r\n                        for word in seq:\r\n                            word = tf.reshape(word,(1,-1))\r\n                            output, c_t,h_t = cell(word, c_t,h_t)\r\n                            if training:\r\n                                output = tf.nn.dropout(output, self.keep_ratio)\r\n                            seq_outputs.append(output)\r\n                        outputs.append(seq_outputs)\r\n                    inputs_seq = outputs\r\n                batch_outputs = inputs_seq\r\n            return batch_outputs\r\n    class FC():\r\n        # A Fully Connection Layer.\r\n        def __init__(self, inputs_dim, outputs_dim,time_major=True,active=tf.nn.relu,name=None):\r\n            self.inputs_dim = inputs_dim\r\n            self.outputs_dim= outputs_dim\r\n            self.time_major = time_major\r\n            self.active = active\r\n\r\n            # Build trainable variables\r\n            self.weights = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim,outputs_dim)),\r\n                                      dtype=tf.float32,name=name,trainable=True)\r\n            self.bias = tfe.Variable(initial_value=tf.random_normal(shape=(outputs_dim,)),\r\n                                      dtype=tf.float32,name=name,trainable=True)\r\n    \r\n        def __call__(self, x):\r\n            # different length sequences, inputs : [batch, (RNN outputs tensor)]\r\n            # whether to parallel computing\r\n            if self.time_major:\r\n                seq_len,batch_size,last_dim = x.shape.as_list()\r\n                # flatten\r\n                flat = tf.reshape(x,(-1,last_dim))\r\n                fc = tf.nn.xw_plus_b(flat,self.weights,self.bias)\r\n                outputs = self.active(fc)\r\n                outputs = tf.reshape(outputs,(seq_len,batch_size,-1))\r\n            else:\r\n                outputs = []\r\n                for seq in x:\r\n                    fc = tf.nn.xw_plus_b(seq, self.weights, self.bias)\r\n                    fc = self.active(fc)\r\n                    outputs.append(fc)\r\n            return outputs\r\n`\r\n\r\n\r\nPart of the training code is as follows:\r\n\r\n`\r\n     with tf.GradientTape() as tape:\r\n             logits = model(batch_forward,batch_backward,training=True)\r\n             loss = training_helper.loss_fn_time_major(batch_labels,logits,batch_lengths)\r\n             # add loss summary\r\n             tf.contrib.summary.scalar('loss', loss)\r\n             grads = tape.gradient(loss, model.variables)\r\n             optimizer.apply_gradients(zip(grads,model.variables))\r\n             print('loss at epoch %d step %d: %f' % (epoch,train_step,loss))\r\n`\r\n\r\nI used your recommended settings turning log_device_placement=True, then I observed an interesting phenomenon. At every certain training step, there are three operation logs that are cycled, and the interval between the training steps is gradually decreasing. \r\nThe following is the log after the setting log_device_placement=True is turned on. It can be seen, at the beginning, a log of three operations is printed every 16 training steps. As the training process progresses, a log of three operations is printed every three training steps until the OOM problem happened. I don't know why and what these three operations represent respectively. I hope this information will help solve the problem. Thank you again for your reply.\r\n\r\n### some logs\r\n\r\nloss at epoch 0 step 805: 6.740626\r\nloss at epoch 0 step 806: 7.095036\r\nloss at epoch 0 step 807: 7.000319\r\nloss at epoch 0 step 808: 6.904347\r\nloss at epoch 0 step 809: 6.922751\r\nloss at epoch 0 step 810: 6.838681\r\nloss at epoch 0 step 811: 6.905680\r\nloss at epoch 0 step 812: 7.053514\r\nloss at epoch 0 step 813: 6.855484\r\nloss at epoch 0 step 814: 6.755508\r\nloss at epoch 0 step 815: 7.050679\r\nloss at epoch 0 step 816: 7.023479\r\nloss at epoch 0 step 817: 6.827959\r\nloss at epoch 0 step 818: 6.776998\r\nloss at epoch 0 step 819: 6.765797\r\nloss at epoch 0 step 820: 6.759283\r\n2018-06-25 14:57:15.878743: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:15.935075: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:15.970760: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 821: 6.984869\r\nloss at epoch 0 step 822: 6.748662\r\nloss at epoch 0 step 823: 6.772003\r\nloss at epoch 0 step 824: 6.942709\r\nloss at epoch 0 step 825: 6.892979\r\nloss at epoch 0 step 826: 6.680350\r\nloss at epoch 0 step 827: 6.867269\r\nloss at epoch 0 step 828: 7.118308\r\nloss at epoch 0 step 829: 6.601588\r\nloss at epoch 0 step 830: 6.652170\r\nloss at epoch 0 step 831: 7.263901\r\nloss at epoch 0 step 832: 6.902826\r\nloss at epoch 0 step 833: 6.791662\r\nloss at epoch 0 step 834: 7.087551\r\nloss at epoch 0 step 835: 6.820101\r\n2018-06-25 14:57:18.650329: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:18.710324: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:18.746490: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 836: 7.031796\r\nloss at epoch 0 step 837: 6.905032\r\nloss at epoch 0 step 838: 6.889287\r\nloss at epoch 0 step 839: 6.732944\r\nloss at epoch 0 step 840: 6.715842\r\nloss at epoch 0 step 841: 6.757300\r\nloss at epoch 0 step 842: 6.882929\r\nloss at epoch 0 step 843: 6.658679\r\nloss at epoch 0 step 844: 6.788300\r\nloss at epoch 0 step 845: 6.952966\r\nloss at epoch 0 step 846: 7.061527\r\nloss at epoch 0 step 847: 6.603632\r\nloss at epoch 0 step 848: 7.163596\r\nloss at epoch 0 step 849: 7.038760\r\n2018-06-25 14:57:21.325933: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:21.386127: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:21.423910: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 850: 6.885034\r\nloss at epoch 0 step 851: 6.931499\r\nloss at epoch 0 step 852: 6.713628\r\nloss at epoch 0 step 853: 6.777599\r\nloss at epoch 0 step 854: 6.799969\r\nloss at epoch 0 step 855: 6.985339\r\nloss at epoch 0 step 856: 6.620006\r\nloss at epoch 0 step 857: 6.878152\r\nloss at epoch 0 step 858: 6.783222\r\nloss at epoch 0 step 859: 6.712417\r\nloss at epoch 0 step 860: 6.734530\r\nloss at epoch 0 step 861: 7.009552\r\n2018-06-25 14:57:23.706079: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:23.769647: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:23.809172: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 862: 6.570765\r\nloss at epoch 0 step 863: 6.818643\r\nloss at epoch 0 step 864: 6.707229\r\nloss at epoch 0 step 865: 6.860226\r\nloss at epoch 0 step 866: 6.832708\r\nloss at epoch 0 step 867: 6.746671\r\nloss at epoch 0 step 868: 6.925843\r\nloss at epoch 0 step 869: 6.845281\r\nloss at epoch 0 step 870: 6.901897\r\nloss at epoch 0 step 871: 6.936728\r\nloss at epoch 0 step 872: 6.793565\r\n2018-06-25 14:57:25.894904: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:25.958699: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:25.998658: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 873: 6.795174\r\nloss at epoch 0 step 874: 6.897914\r\nloss at epoch 0 step 875: 6.964663\r\nloss at epoch 0 step 876: 6.933444\r\nloss at epoch 0 step 877: 6.889087\r\nloss at epoch 0 step 878: 6.728238\r\nloss at epoch 0 step 879: 6.543335\r\nloss at epoch 0 step 880: 6.785100\r\nloss at epoch 0 step 881: 6.867868\r\nloss at epoch 0 step 882: 6.626728\r\n2018-06-25 14:57:28.011803: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:28.078783: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:28.121272: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 883: 6.632171\r\nloss at epoch 0 step 884: 6.638273\r\nloss at epoch 0 step 885: 6.727106\r\nloss at epoch 0 step 886: 6.755214\r\nloss at epoch 0 step 887: 6.788674\r\nloss at epoch 0 step 888: 6.618323\r\nloss at epoch 0 step 889: 6.802135\r\nloss at epoch 0 step 890: 6.812809\r\nloss at epoch 0 step 891: 6.905511\r\n2018-06-25 14:57:29.982565: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:30.048872: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:30.089943: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 892: 6.660409\r\nloss at epoch 0 step 893: 6.817567\r\nloss at epoch 0 step 894: 6.782594\r\nloss at epoch 0 step 895: 6.746886\r\nloss at epoch 0 step 896: 6.679479\r\nloss at epoch 0 step 897: 6.696771\r\nloss at epoch 0 step 898: 6.642287\r\nloss at epoch 0 step 899: 6.870078\r\n2018-06-25 14:57:31.693334: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:31.761710: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:31.805339: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 900: 6.774796\r\nloss at epoch 0 step 901: 6.841524\r\nloss at epoch 0 step 902: 6.682564\r\nloss at epoch 0 step 903: 6.693571\r\nloss at epoch 0 step 904: 6.603614\r\nloss at epoch 0 step 905: 6.705801\r\nloss at epoch 0 step 906: 6.713558\r\nloss at epoch 0 step 907: 6.889330\r\n2018-06-25 14:57:33.459800: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:33.528563: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:33.573187: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 908: 6.726191\r\nloss at epoch 0 step 909: 6.945642\r\nloss at epoch 0 step 910: 6.768640\r\nloss at epoch 0 step 911: 6.801554\r\nloss at epoch 0 step 912: 6.867862\r\nloss at epoch 0 step 913: 6.563491\r\nloss at epoch 0 step 914: 6.853379\r\nloss at epoch 0 step 915: 6.724048\r\n2018-06-25 14:57:35.341966: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:35.412082: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:35.456774: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 916: 6.639415\r\nloss at epoch 0 step 917: 6.881902\r\nloss at epoch 0 step 918: 6.735335\r\nloss at epoch 0 step 919: 6.688423\r\nloss at epoch 0 step 920: 6.636367\r\nloss at epoch 0 step 921: 6.717305\r\nloss at epoch 0 step 922: 6.629582\r\n2018-06-25 14:57:36.960963: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:37.033898: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:37.079990: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 923: 6.682681\r\nloss at epoch 0 step 924: 6.882014\r\nloss at epoch 0 step 925: 6.731138\r\nloss at epoch 0 step 926: 6.588252\r\nloss at epoch 0 step 927: 6.824276\r\nloss at epoch 0 step 928: 6.707603\r\nloss at epoch 0 step 929: 6.748030\r\n2018-06-25 14:57:38.690123: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:38.767782: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:38.816409: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 930: 6.658401\r\nloss at epoch 0 step 931: 6.748807\r\nloss at epoch 0 step 932: 6.802845\r\nloss at epoch 0 step 933: 6.590845\r\nloss at epoch 0 step 934: 6.749659\r\nloss at epoch 0 step 935: 6.701960\r\n2018-06-25 14:57:40.158598: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:40.234263: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:40.282647: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 936: 6.775355\r\nloss at epoch 0 step 937: 6.781133\r\nloss at epoch 0 step 938: 6.600548\r\nloss at epoch 0 step 939: 6.747424\r\nloss at epoch 0 step 940: 6.605201\r\n2018-06-25 14:57:41.416798: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:41.496970: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:41.550523: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 941: 6.672260\r\nloss at epoch 0 step 942: 6.614303\r\nloss at epoch 0 step 943: 6.726663\r\nloss at epoch 0 step 944: 6.694652\r\nloss at epoch 0 step 945: 6.590765\r\n2018-06-25 14:57:42.720835: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:42.800870: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:42.852718: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 946: 6.639668\r\nloss at epoch 0 step 947: 6.581154\r\nloss at epoch 0 step 948: 6.780156\r\nloss at epoch 0 step 949: 6.619957\r\nloss at epoch 0 step 950: 6.680293\r\n2018-06-25 14:57:44.034047: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:44.116253: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:44.167625: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 951: 6.767905\r\nloss at epoch 0 step 952: 6.694592\r\nloss at epoch 0 step 953: 6.586446\r\nloss at epoch 0 step 954: 6.691846\r\n2018-06-25 14:57:45.122539: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:45.204973: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:45.258423: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 955: 6.994371\r\nloss at epoch 0 step 956: 6.858140\r\nloss at epoch 0 step 957: 6.646007\r\nloss at epoch 0 step 958: 6.721205\r\n2018-06-25 14:57:46.238911: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:46.325236: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:46.380011: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 959: 6.589066\r\nloss at epoch 0 step 960: 6.683684\r\nloss at epoch 0 step 961: 6.672047\r\n2018-06-25 14:57:47.046723: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:47.203337: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:47.260355: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 962: 6.725653\r\nloss at epoch 0 step 963: 6.720684\r\nloss at epoch 0 step 964: 6.636925\r\n2018-06-25 14:57:47.932850: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:48.021669: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:48.077942: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 965: 6.461943\r\nloss at epoch 0 step 966: 6.716685\r\nloss at epoch 0 step 967: 6.541767\r\n2018-06-25 14:57:48.839285: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:58.895255: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 90.19MiB.  Current allocation summary follows.\r\n2018-06-25 14:57:58.895406: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-requested in use in bin.\r\n2018-06-25 14:57:58.895440: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895466: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895488: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-06-25 14:57:58.895517: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-06-25 14:57:58.895538: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-06-25 14:57:58.895565: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 5, Chunks in use: 3. 124.0KiB allocated for chunks. 79.0KiB in use in bin. 78.9KiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895592: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 903, Chunks in use: 901. 28.30MiB allocated for chunks. 28.24MiB in use in bin. 28.15MiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895618: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 402, Chunks in use: 402. 25.56MiB allocated for chunks. 25.56MiB in use in bin. 25.16MiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895639: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 19, Chunks in use: 19. 2.51MiB allocated for chunks. 2.51MiB in use in bin. 2.47MiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895673: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 8, Chunks in use: 8. 2.97MiB allocated for chunks. 2.97MiB in use in bin. 2.75MiB client-requested in use in bin."}