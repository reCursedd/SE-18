{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/416815428", "html_url": "https://github.com/tensorflow/tensorflow/issues/20218#issuecomment-416815428", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20218", "id": 416815428, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjgxNTQyOA==", "user": {"login": "LeoLai930603", "id": 15106070, "node_id": "MDQ6VXNlcjE1MTA2MDcw", "avatar_url": "https://avatars0.githubusercontent.com/u/15106070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LeoLai930603", "html_url": "https://github.com/LeoLai930603", "followers_url": "https://api.github.com/users/LeoLai930603/followers", "following_url": "https://api.github.com/users/LeoLai930603/following{/other_user}", "gists_url": "https://api.github.com/users/LeoLai930603/gists{/gist_id}", "starred_url": "https://api.github.com/users/LeoLai930603/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LeoLai930603/subscriptions", "organizations_url": "https://api.github.com/users/LeoLai930603/orgs", "repos_url": "https://api.github.com/users/LeoLai930603/repos", "events_url": "https://api.github.com/users/LeoLai930603/events{/privacy}", "received_events_url": "https://api.github.com/users/LeoLai930603/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-29T03:51:57Z", "updated_at": "2018-08-29T03:51:57Z", "author_association": "NONE", "body_html": "<p>Agree, and I was wondering whether it is possible to expose this setting (caching size or entry limit) to the user, so we can manage the training and deployment setting by ourselves? And I believe it deserves more attention to the RNN model implementation with Eager mode, for example, it is very inconvenient to process the trainable variables in LSTMCells during training, because 1) it is lazy-initialized, and I was expecting to have a explicit way to initialize the weights in it like Conv2d, 2) the trainable variables are nested list, which makes the following pair-matching with gradients more difficult (you have to flatten the list by your own in stead of just a zip function in tutorial). I hope such cells could provide more features that make them more eager-friendly. At least that would help debugging when facing memory leak.</p>", "body_text": "Agree, and I was wondering whether it is possible to expose this setting (caching size or entry limit) to the user, so we can manage the training and deployment setting by ourselves? And I believe it deserves more attention to the RNN model implementation with Eager mode, for example, it is very inconvenient to process the trainable variables in LSTMCells during training, because 1) it is lazy-initialized, and I was expecting to have a explicit way to initialize the weights in it like Conv2d, 2) the trainable variables are nested list, which makes the following pair-matching with gradients more difficult (you have to flatten the list by your own in stead of just a zip function in tutorial). I hope such cells could provide more features that make them more eager-friendly. At least that would help debugging when facing memory leak.", "body": "Agree, and I was wondering whether it is possible to expose this setting (caching size or entry limit) to the user, so we can manage the training and deployment setting by ourselves? And I believe it deserves more attention to the RNN model implementation with Eager mode, for example, it is very inconvenient to process the trainable variables in LSTMCells during training, because 1) it is lazy-initialized, and I was expecting to have a explicit way to initialize the weights in it like Conv2d, 2) the trainable variables are nested list, which makes the following pair-matching with gradients more difficult (you have to flatten the list by your own in stead of just a zip function in tutorial). I hope such cells could provide more features that make them more eager-friendly. At least that would help debugging when facing memory leak."}