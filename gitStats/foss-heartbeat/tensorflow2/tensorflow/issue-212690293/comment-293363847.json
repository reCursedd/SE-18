{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293363847", "html_url": "https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-293363847", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191", "id": 293363847, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MzM2Mzg0Nw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-11T18:46:33Z", "updated_at": "2017-04-11T18:46:33Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Thanks for the feedback!  Seems there's something different between the TF\non pypi and at that tag? Gunhan, is that possible?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Mon, Apr 10, 2017 at 9:05 PM, prashantserai ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/oxwsds\">@oxwsds</a> &lt;<a href=\"https://github.com/oxwsds\">https://github.com/oxwsds</a>&gt; <a class=\"user-mention\" href=\"https://github.com/robmsylvester\">@robmsylvester</a>\n &lt;<a href=\"https://github.com/robmsylvester\">https://github.com/robmsylvester</a>&gt; <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt;\n I finally have something that's working now (I mean, results for my single\n layer 256 unit network are kind of appalling, but that's probably just\n because the network is ultra light weight and I didn't tune params AT ALL)\n\n Here's my bottomline:\n\n <a class=\"user-mention\" href=\"https://github.com/oxwsds\">@oxwsds</a> &lt;<a href=\"https://github.com/oxwsds\">https://github.com/oxwsds</a>&gt; comment *that the tutorial (in it's\n current form) works without any need for modification when Tensorflow is\n compiled from the branch remotes/origin/r1.0 was TRUE*. The sad bit\n although being that the version of Tensorflow I had for which modifications\n within Tensorflow code were needed, and the version in remotes/origin/r1.0\n were both identically labelled.\n\n <a class=\"user-mention\" href=\"https://github.com/robmsylvester\">@robmsylvester</a> &lt;<a href=\"https://github.com/robmsylvester\">https://github.com/robmsylvester</a>&gt; 's fix in the comment\n (copied below) DID WORK for my version of Tensorflow where the Tutorial\n didn't work out of the box (and should work for TF 1.1 too I guess). It is\n slightly messy to implement, but I could do it, which is saying something\n :-P\n The error in my last two comments before this was due to my mistake. Like\n a dummy, I was specifying the layers and hidden units parameters only\n during training, I was leaving the code to use defaults during decoding. *(this\n portion of the tutorial is could be slightly more dummy proof:\n <a href=\"https://www.tensorflow.org/tutorials/seq2seq#lets_run_it\">https://www.tensorflow.org/tutorials/seq2seq#lets_run_it</a>\n &lt;<a href=\"https://www.tensorflow.org/tutorials/seq2seq#lets_run_it\">https://www.tensorflow.org/tutorials/seq2seq#lets_run_it</a>&gt; )*\n\n Hmmm. One thing that stands out to me is in the referenced legacy seq2seq\n file:\n\n encoder_cell = copy.deepcopy(cell)\n\n This line appears to be used because the same architecture is used on both\n the encoder and decoder side. They make a copy of the cell, then pass the\n cell argument along to the attention decoder embedding function, then to\n the attention decoder itself.\n\n What happens if you explicitly create the encoder cell AND the decoder\n cell in your seq2seq model file and pass both along to the legacy library\n file, making the small adjustments to the functions and their arguments?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"212690293\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8191\" href=\"https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-293143828\">#8191 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimxvcfFnbWbpj7aUs3BUjwGEFj6p5ks5ruvvygaJpZM4MWl4f\">https://github.com/notifications/unsubscribe-auth/ABtimxvcfFnbWbpj7aUs3BUjwGEFj6p5ks5ruvvygaJpZM4MWl4f</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Thanks for the feedback!  Seems there's something different between the TF\non pypi and at that tag? Gunhan, is that possible?\n\u2026\nOn Mon, Apr 10, 2017 at 9:05 PM, prashantserai ***@***.***> wrote:\n @oxwsds <https://github.com/oxwsds> @robmsylvester\n <https://github.com/robmsylvester> @ebrevdo <https://github.com/ebrevdo>\n I finally have something that's working now (I mean, results for my single\n layer 256 unit network are kind of appalling, but that's probably just\n because the network is ultra light weight and I didn't tune params AT ALL)\n\n Here's my bottomline:\n\n @oxwsds <https://github.com/oxwsds> comment *that the tutorial (in it's\n current form) works without any need for modification when Tensorflow is\n compiled from the branch remotes/origin/r1.0 was TRUE*. The sad bit\n although being that the version of Tensorflow I had for which modifications\n within Tensorflow code were needed, and the version in remotes/origin/r1.0\n were both identically labelled.\n\n @robmsylvester <https://github.com/robmsylvester> 's fix in the comment\n (copied below) DID WORK for my version of Tensorflow where the Tutorial\n didn't work out of the box (and should work for TF 1.1 too I guess). It is\n slightly messy to implement, but I could do it, which is saying something\n :-P\n The error in my last two comments before this was due to my mistake. Like\n a dummy, I was specifying the layers and hidden units parameters only\n during training, I was leaving the code to use defaults during decoding. *(this\n portion of the tutorial is could be slightly more dummy proof:\n https://www.tensorflow.org/tutorials/seq2seq#lets_run_it\n <https://www.tensorflow.org/tutorials/seq2seq#lets_run_it> )*\n\n Hmmm. One thing that stands out to me is in the referenced legacy seq2seq\n file:\n\n encoder_cell = copy.deepcopy(cell)\n\n This line appears to be used because the same architecture is used on both\n the encoder and decoder side. They make a copy of the cell, then pass the\n cell argument along to the attention decoder embedding function, then to\n the attention decoder itself.\n\n What happens if you explicitly create the encoder cell AND the decoder\n cell in your seq2seq model file and pass both along to the legacy library\n file, making the small adjustments to the functions and their arguments?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#8191 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtimxvcfFnbWbpj7aUs3BUjwGEFj6p5ks5ruvvygaJpZM4MWl4f>\n .", "body": "Thanks for the feedback!  Seems there's something different between the TF\non pypi and at that tag? Gunhan, is that possible?\n\nOn Mon, Apr 10, 2017 at 9:05 PM, prashantserai <notifications@github.com>\nwrote:\n\n> @oxwsds <https://github.com/oxwsds> @robmsylvester\n> <https://github.com/robmsylvester> @ebrevdo <https://github.com/ebrevdo>\n> I finally have something that's working now (I mean, results for my single\n> layer 256 unit network are kind of appalling, but that's probably just\n> because the network is ultra light weight and I didn't tune params AT ALL)\n>\n> Here's my bottomline:\n>\n> @oxwsds <https://github.com/oxwsds> comment *that the tutorial (in it's\n> current form) works without any need for modification when Tensorflow is\n> compiled from the branch remotes/origin/r1.0 was TRUE*. The sad bit\n> although being that the version of Tensorflow I had for which modifications\n> within Tensorflow code were needed, and the version in remotes/origin/r1.0\n> were both identically labelled.\n>\n> @robmsylvester <https://github.com/robmsylvester> 's fix in the comment\n> (copied below) DID WORK for my version of Tensorflow where the Tutorial\n> didn't work out of the box (and should work for TF 1.1 too I guess). It is\n> slightly messy to implement, but I could do it, which is saying something\n> :-P\n> The error in my last two comments before this was due to my mistake. Like\n> a dummy, I was specifying the layers and hidden units parameters only\n> during training, I was leaving the code to use defaults during decoding. *(this\n> portion of the tutorial is could be slightly more dummy proof:\n> https://www.tensorflow.org/tutorials/seq2seq#lets_run_it\n> <https://www.tensorflow.org/tutorials/seq2seq#lets_run_it> )*\n>\n> Hmmm. One thing that stands out to me is in the referenced legacy seq2seq\n> file:\n>\n> encoder_cell = copy.deepcopy(cell)\n>\n> This line appears to be used because the same architecture is used on both\n> the encoder and decoder side. They make a copy of the cell, then pass the\n> cell argument along to the attention decoder embedding function, then to\n> the attention decoder itself.\n>\n> What happens if you explicitly create the encoder cell AND the decoder\n> cell in your seq2seq model file and pass both along to the legacy library\n> file, making the small adjustments to the functions and their arguments?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-293143828>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxvcfFnbWbpj7aUs3BUjwGEFj6p5ks5ruvvygaJpZM4MWl4f>\n> .\n>\n"}