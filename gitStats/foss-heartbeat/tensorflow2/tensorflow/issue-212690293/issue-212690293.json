{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8191", "id": 212690293, "node_id": "MDU6SXNzdWUyMTI2OTAyOTM=", "number": 8191, "title": "ValueError: Attempt to reuse RNNCell with a different variable scope than its first use.", "user": {"login": "doncat99", "id": 4565981, "node_id": "MDQ6VXNlcjQ1NjU5ODE=", "avatar_url": "https://avatars1.githubusercontent.com/u/4565981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/doncat99", "html_url": "https://github.com/doncat99", "followers_url": "https://api.github.com/users/doncat99/followers", "following_url": "https://api.github.com/users/doncat99/following{/other_user}", "gists_url": "https://api.github.com/users/doncat99/gists{/gist_id}", "starred_url": "https://api.github.com/users/doncat99/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/doncat99/subscriptions", "organizations_url": "https://api.github.com/users/doncat99/orgs", "repos_url": "https://api.github.com/users/doncat99/repos", "events_url": "https://api.github.com/users/doncat99/events{/privacy}", "received_events_url": "https://api.github.com/users/doncat99/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 102, "created_at": "2017-03-08T10:06:12Z", "updated_at": "2018-04-13T08:28:21Z", "closed_at": "2018-01-05T19:32:31Z", "author_association": "NONE", "body_html": "<p>I am not sure if I am the first who met the following error:</p>\n<p>ValueError: Attempt to reuse RNNCell &lt;tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x10210d5c0&gt; with a different variable scope than its first use.  First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)</p>\n<p>with the code fragment:</p>\n<pre><code>  import tensorflow as tf\n  from tensorflow.contrib import rnn\n\n  hidden_size = 100\n  batch_size  = 100\n  num_steps   = 100\n  num_layers  = 100\n  is_training = True\n  keep_prob   = 0.4\n\n  input_data = tf.placeholder(tf.float32, [batch_size, num_steps])\n  lstm_cell = rnn.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)\n\n  if is_training and keep_prob &lt; 1:\n      lstm_cell = rnn.DropoutWrapper(lstm_cell)\n  cell = rnn.MultiRNNCell([lstm_cell for _ in range(num_layers)], state_is_tuple=True)\n\n  _initial_state = cell.zero_state(batch_size, tf.float32)\n\n  iw = tf.get_variable(\"input_w\", [1, hidden_size])\n  ib = tf.get_variable(\"input_b\", [hidden_size])\n  inputs = [tf.nn.xw_plus_b(i_, iw, ib) for i_ in tf.split(input_data, num_steps, 1)]\n\n  if is_training and keep_prob &lt; 1:\n      inputs = [tf.nn.dropout(input_, keep_prob) for input_ in inputs]\n\n  outputs, states = rnn.static_rnn(cell, inputs, initial_state=_initial_state)\n</code></pre>\n<p>I had googled around with no luck, can anyone show me a way out?</p>", "body_text": "I am not sure if I am the first who met the following error:\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x10210d5c0> with a different variable scope than its first use.  First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\nwith the code fragment:\n  import tensorflow as tf\n  from tensorflow.contrib import rnn\n\n  hidden_size = 100\n  batch_size  = 100\n  num_steps   = 100\n  num_layers  = 100\n  is_training = True\n  keep_prob   = 0.4\n\n  input_data = tf.placeholder(tf.float32, [batch_size, num_steps])\n  lstm_cell = rnn.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)\n\n  if is_training and keep_prob < 1:\n      lstm_cell = rnn.DropoutWrapper(lstm_cell)\n  cell = rnn.MultiRNNCell([lstm_cell for _ in range(num_layers)], state_is_tuple=True)\n\n  _initial_state = cell.zero_state(batch_size, tf.float32)\n\n  iw = tf.get_variable(\"input_w\", [1, hidden_size])\n  ib = tf.get_variable(\"input_b\", [hidden_size])\n  inputs = [tf.nn.xw_plus_b(i_, iw, ib) for i_ in tf.split(input_data, num_steps, 1)]\n\n  if is_training and keep_prob < 1:\n      inputs = [tf.nn.dropout(input_, keep_prob) for input_ in inputs]\n\n  outputs, states = rnn.static_rnn(cell, inputs, initial_state=_initial_state)\n\nI had googled around with no luck, can anyone show me a way out?", "body": "I am not sure if I am the first who met the following error:\r\n\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x10210d5c0> with a different variable scope than its first use.  First use of cell was with scope 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell', this attempt is with scope 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n\r\nwith the code fragment:\r\n\r\n      import tensorflow as tf\r\n      from tensorflow.contrib import rnn\r\n\r\n      hidden_size = 100\r\n      batch_size  = 100\r\n      num_steps   = 100\r\n      num_layers  = 100\r\n      is_training = True\r\n      keep_prob   = 0.4\r\n\r\n      input_data = tf.placeholder(tf.float32, [batch_size, num_steps])\r\n      lstm_cell = rnn.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)\r\n\r\n      if is_training and keep_prob < 1:\r\n          lstm_cell = rnn.DropoutWrapper(lstm_cell)\r\n      cell = rnn.MultiRNNCell([lstm_cell for _ in range(num_layers)], state_is_tuple=True)\r\n\r\n      _initial_state = cell.zero_state(batch_size, tf.float32)\r\n\r\n      iw = tf.get_variable(\"input_w\", [1, hidden_size])\r\n      ib = tf.get_variable(\"input_b\", [hidden_size])\r\n      inputs = [tf.nn.xw_plus_b(i_, iw, ib) for i_ in tf.split(input_data, num_steps, 1)]\r\n\r\n      if is_training and keep_prob < 1:\r\n          inputs = [tf.nn.dropout(input_, keep_prob) for input_ in inputs]\r\n    \r\n      outputs, states = rnn.static_rnn(cell, inputs, initial_state=_initial_state)\r\n\r\nI had googled around with no luck, can anyone show me a way out?"}