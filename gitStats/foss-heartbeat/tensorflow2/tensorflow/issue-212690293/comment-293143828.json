{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293143828", "html_url": "https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-293143828", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191", "id": 293143828, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MzE0MzgyOA==", "user": {"login": "prashantserai", "id": 10653125, "node_id": "MDQ6VXNlcjEwNjUzMTI1", "avatar_url": "https://avatars3.githubusercontent.com/u/10653125?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prashantserai", "html_url": "https://github.com/prashantserai", "followers_url": "https://api.github.com/users/prashantserai/followers", "following_url": "https://api.github.com/users/prashantserai/following{/other_user}", "gists_url": "https://api.github.com/users/prashantserai/gists{/gist_id}", "starred_url": "https://api.github.com/users/prashantserai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prashantserai/subscriptions", "organizations_url": "https://api.github.com/users/prashantserai/orgs", "repos_url": "https://api.github.com/users/prashantserai/repos", "events_url": "https://api.github.com/users/prashantserai/events{/privacy}", "received_events_url": "https://api.github.com/users/prashantserai/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-11T04:04:23Z", "updated_at": "2017-04-11T22:57:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2883580\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oxwsds\">@oxwsds</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3009901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/robmsylvester\">@robmsylvester</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a><br>\nI finally have something that's working now (I mean, results for my single layer 256 unit network are kind of appalling, but that's probably just because the network is ultra light weight and I didn't tune params AT ALL)<br>\nThank you so much everyone...!!!!!</p>\n<p><em>Here's my thoughts at the end of this:</em></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2883580\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oxwsds\">@oxwsds</a> comment <strong>that the tutorial (in it's current form) works without any need for modification when Tensorflow is compiled from the branch remotes/origin/r1.0 was TRUE</strong>. Although, the sad bit was that the version of Tensorflow I had for which modifications within Tensorflow code were needed, and the version in remotes/origin/r1.0 were both identically labelled.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3009901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/robmsylvester\">@robmsylvester</a> 's fix in the comment (copied below) DID WORK for my version of Tensorflow where the Tutorial didn't work out of the box (and should work for TF 1.1 too I guess). It is slightly messy to implement, but I could do it, which is saying something :-P<br>\nThe error in my last two comments before this was due to my mistake. Like a dummy, I was specifying the layers and hidden units parameters only during training, I was leaving the code to use defaults during decoding. <strong>(this portion of the tutorial could be slightly more dummy proof: <a href=\"https://www.tensorflow.org/tutorials/seq2seq#lets_run_it\" rel=\"nofollow\">https://www.tensorflow.org/tutorials/seq2seq#lets_run_it</a> )</strong></p>\n<blockquote>\n<p>Hmmm. One thing that stands out to me is in the referenced legacy seq2seq file:</p>\n<p>encoder_cell = copy.deepcopy(cell)</p>\n<p>This line appears to be used because the same architecture is used on both the encoder and decoder side. They make a copy of the cell, then pass the cell argument along to the attention decoder embedding function, then to the attention decoder itself.</p>\n<p>What happens if you explicitly create the encoder cell AND the decoder cell in your seq2seq model file and pass both along to the legacy library file, making the small adjustments to the functions and their arguments?</p>\n</blockquote>", "body_text": "@oxwsds @robmsylvester @ebrevdo\nI finally have something that's working now (I mean, results for my single layer 256 unit network are kind of appalling, but that's probably just because the network is ultra light weight and I didn't tune params AT ALL)\nThank you so much everyone...!!!!!\nHere's my thoughts at the end of this:\n@oxwsds comment that the tutorial (in it's current form) works without any need for modification when Tensorflow is compiled from the branch remotes/origin/r1.0 was TRUE. Although, the sad bit was that the version of Tensorflow I had for which modifications within Tensorflow code were needed, and the version in remotes/origin/r1.0 were both identically labelled.\n@robmsylvester 's fix in the comment (copied below) DID WORK for my version of Tensorflow where the Tutorial didn't work out of the box (and should work for TF 1.1 too I guess). It is slightly messy to implement, but I could do it, which is saying something :-P\nThe error in my last two comments before this was due to my mistake. Like a dummy, I was specifying the layers and hidden units parameters only during training, I was leaving the code to use defaults during decoding. (this portion of the tutorial could be slightly more dummy proof: https://www.tensorflow.org/tutorials/seq2seq#lets_run_it )\n\nHmmm. One thing that stands out to me is in the referenced legacy seq2seq file:\nencoder_cell = copy.deepcopy(cell)\nThis line appears to be used because the same architecture is used on both the encoder and decoder side. They make a copy of the cell, then pass the cell argument along to the attention decoder embedding function, then to the attention decoder itself.\nWhat happens if you explicitly create the encoder cell AND the decoder cell in your seq2seq model file and pass both along to the legacy library file, making the small adjustments to the functions and their arguments?", "body": "@oxwsds @robmsylvester @ebrevdo \r\nI finally have something that's working now (I mean, results for my single layer 256 unit network are kind of appalling, but that's probably just because the network is ultra light weight and I didn't tune params AT ALL)\r\nThank you so much everyone...!!!!!\r\n\r\n_Here's my thoughts at the end of this:_\r\n\r\n@oxwsds comment **that the tutorial (in it's current form) works without any need for modification when Tensorflow is compiled from the branch remotes/origin/r1.0 was TRUE**. Although, the sad bit was that the version of Tensorflow I had for which modifications within Tensorflow code were needed, and the version in remotes/origin/r1.0 were both identically labelled.\r\n\r\n@robmsylvester 's fix in the comment (copied below) DID WORK for my version of Tensorflow where the Tutorial didn't work out of the box (and should work for TF 1.1 too I guess). It is slightly messy to implement, but I could do it, which is saying something :-P\r\nThe error in my last two comments before this was due to my mistake. Like a dummy, I was specifying the layers and hidden units parameters only during training, I was leaving the code to use defaults during decoding. **(this portion of the tutorial could be slightly more dummy proof: https://www.tensorflow.org/tutorials/seq2seq#lets_run_it )**\r\n\r\n> Hmmm. One thing that stands out to me is in the referenced legacy seq2seq file:\r\n> \r\n> encoder_cell = copy.deepcopy(cell)\r\n> \r\n> This line appears to be used because the same architecture is used on both the encoder and decoder side. They make a copy of the cell, then pass the cell argument along to the attention decoder embedding function, then to the attention decoder itself.\r\n> \r\n> What happens if you explicitly create the encoder cell AND the decoder cell in your seq2seq model file and pass both along to the legacy library file, making the small adjustments to the functions and their arguments?"}