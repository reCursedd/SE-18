{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323439646", "html_url": "https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-323439646", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191", "id": 323439646, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzQzOTY0Ng==", "user": {"login": "comsian106", "id": 27868903, "node_id": "MDQ6VXNlcjI3ODY4OTAz", "avatar_url": "https://avatars0.githubusercontent.com/u/27868903?v=4", "gravatar_id": "", "url": "https://api.github.com/users/comsian106", "html_url": "https://github.com/comsian106", "followers_url": "https://api.github.com/users/comsian106/followers", "following_url": "https://api.github.com/users/comsian106/following{/other_user}", "gists_url": "https://api.github.com/users/comsian106/gists{/gist_id}", "starred_url": "https://api.github.com/users/comsian106/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/comsian106/subscriptions", "organizations_url": "https://api.github.com/users/comsian106/orgs", "repos_url": "https://api.github.com/users/comsian106/repos", "events_url": "https://api.github.com/users/comsian106/events{/privacy}", "received_events_url": "https://api.github.com/users/comsian106/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-18T19:21:17Z", "updated_at": "2017-08-18T19:30:03Z", "author_association": "NONE", "body_html": "<p>I'm facing this error:<br>\n<code>TypeError: embedding_attention_seq2seq() missing 1 required positional argument: 'dec_cell' </code></p>\n<p>The error points to this function in seq2seq_model.py which is line 142 in seq2seq_model.py:</p>\n<pre lang=\"def\" data-meta=\"seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\"><code>    return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n        encoder_inputs,\n        decoder_inputs,\n        cell,\n        num_encoder_symbols=source_vocab_size,\n        num_decoder_symbols=target_vocab_size,\n        embedding_size=size,\n        output_projection=output_projection,\n        feed_previous=do_decode,\n        dtype=dtype) \n</code></pre>\n<p>Anyone who came across with this error and managed  to solve this, please help me correct this issue.</p>", "body_text": "I'm facing this error:\nTypeError: embedding_attention_seq2seq() missing 1 required positional argument: 'dec_cell' \nThe error points to this function in seq2seq_model.py which is line 142 in seq2seq_model.py:\n    return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n        encoder_inputs,\n        decoder_inputs,\n        cell,\n        num_encoder_symbols=source_vocab_size,\n        num_decoder_symbols=target_vocab_size,\n        embedding_size=size,\n        output_projection=output_projection,\n        feed_previous=do_decode,\n        dtype=dtype) \n\nAnyone who came across with this error and managed  to solve this, please help me correct this issue.", "body": "I'm facing this error:\r\n```TypeError: embedding_attention_seq2seq() missing 1 required positional argument: 'dec_cell' ```\r\n\r\nThe error points to this function in seq2seq_model.py which is line 142 in seq2seq_model.py:\r\n\r\n  ```def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\r\n          encoder_inputs,\r\n          decoder_inputs,\r\n          cell,\r\n          num_encoder_symbols=source_vocab_size,\r\n          num_decoder_symbols=target_vocab_size,\r\n          embedding_size=size,\r\n          output_projection=output_projection,\r\n          feed_previous=do_decode,\r\n          dtype=dtype) \r\n```\r\n\r\n\r\nAnyone who came across with this error and managed  to solve this, please help me correct this issue."}