{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308729142", "html_url": "https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-308729142", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191", "id": 308729142, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODcyOTE0Mg==", "user": {"login": "ypruan", "id": 20205901, "node_id": "MDQ6VXNlcjIwMjA1OTAx", "avatar_url": "https://avatars1.githubusercontent.com/u/20205901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ypruan", "html_url": "https://github.com/ypruan", "followers_url": "https://api.github.com/users/ypruan/followers", "following_url": "https://api.github.com/users/ypruan/following{/other_user}", "gists_url": "https://api.github.com/users/ypruan/gists{/gist_id}", "starred_url": "https://api.github.com/users/ypruan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ypruan/subscriptions", "organizations_url": "https://api.github.com/users/ypruan/orgs", "repos_url": "https://api.github.com/users/ypruan/repos", "events_url": "https://api.github.com/users/ypruan/events{/privacy}", "received_events_url": "https://api.github.com/users/ypruan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-15T13:28:28Z", "updated_at": "2017-06-15T13:28:28Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4565981\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/doncat99\">@doncat99</a><br>\nIt seems that <code>copy.deepcopy(cell)</code>  in <code>seq2seq.py</code>  doesn't make effect.<br>\nSo I change the related part in <code>seq2seq_model.py</code> to</p>\n<pre><code>if num_layers &gt; 1:\n      cell_enc = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\n      cell_dec = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\n\n    # The seq2seq function: we use embedding for the input and attention.\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      return seq2seq.embedding_attention_seq2seq(\n          encoder_inputs,\n          decoder_inputs,\n          cell_enc,\n          cell_dec,\n          num_encoder_symbols=source_vocab_size,\n          num_decoder_symbols=target_vocab_size,\n          embedding_size=size,\n          output_projection=output_projection,\n          feed_previous=do_decode,\n          dtype=dtype)\n</code></pre>", "body_text": "@doncat99\nIt seems that copy.deepcopy(cell)  in seq2seq.py  doesn't make effect.\nSo I change the related part in seq2seq_model.py to\nif num_layers > 1:\n      cell_enc = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\n      cell_dec = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\n\n    # The seq2seq function: we use embedding for the input and attention.\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      return seq2seq.embedding_attention_seq2seq(\n          encoder_inputs,\n          decoder_inputs,\n          cell_enc,\n          cell_dec,\n          num_encoder_symbols=source_vocab_size,\n          num_decoder_symbols=target_vocab_size,\n          embedding_size=size,\n          output_projection=output_projection,\n          feed_previous=do_decode,\n          dtype=dtype)", "body": "@doncat99 \r\nIt seems that `copy.deepcopy(cell)`  in `seq2seq.py`  doesn't make effect.\r\nSo I change the related part in `seq2seq_model.py` to \r\n\r\n```\r\nif num_layers > 1:\r\n      cell_enc = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n      cell_dec = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n\r\n    # The seq2seq function: we use embedding for the input and attention.\r\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n      return seq2seq.embedding_attention_seq2seq(\r\n          encoder_inputs,\r\n          decoder_inputs,\r\n          cell_enc,\r\n          cell_dec,\r\n          num_encoder_symbols=source_vocab_size,\r\n          num_decoder_symbols=target_vocab_size,\r\n          embedding_size=size,\r\n          output_projection=output_projection,\r\n          feed_previous=do_decode,\r\n          dtype=dtype)\r\n```"}