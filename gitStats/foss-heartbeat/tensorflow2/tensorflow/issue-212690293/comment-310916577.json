{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/310916577", "html_url": "https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-310916577", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191", "id": 310916577, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDkxNjU3Nw==", "user": {"login": "fabiofumarola", "id": 1550672, "node_id": "MDQ6VXNlcjE1NTA2NzI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1550672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fabiofumarola", "html_url": "https://github.com/fabiofumarola", "followers_url": "https://api.github.com/users/fabiofumarola/followers", "following_url": "https://api.github.com/users/fabiofumarola/following{/other_user}", "gists_url": "https://api.github.com/users/fabiofumarola/gists{/gist_id}", "starred_url": "https://api.github.com/users/fabiofumarola/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fabiofumarola/subscriptions", "organizations_url": "https://api.github.com/users/fabiofumarola/orgs", "repos_url": "https://api.github.com/users/fabiofumarola/repos", "events_url": "https://api.github.com/users/fabiofumarola/events{/privacy}", "received_events_url": "https://api.github.com/users/fabiofumarola/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-25T17:37:25Z", "updated_at": "2017-06-25T17:37:25Z", "author_association": "NONE", "body_html": "<p>Hi guys, I don't know if you're still interested on it, but I found that the problem is related to the operation of copying the cell passed as params to the <code>embedding_attention_seq2seq</code> function. This is because the same cell definition is used both for encoder and decoder. I think the tutorial is deprecated since it uses a seq2seq model with bucketing in contrast to a dynamic seq2seq. But, I'm pasting a modified function that works. The function is updated in the file <code>tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py</code>.</p>\n<p>thanks,<br>\nFabio</p>\n<pre lang=\"!python\"><code>def embedding_attention_seq2seq(encoder_inputs,\n                                decoder_inputs,\n                                enc_cell,\n                                dec_cell,\n                                num_encoder_symbols,\n                                num_decoder_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_projection=None,\n                                feed_previous=False,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False):\n  \"\"\"Embedding sequence-to-sequence model with attention.\n\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\n  embedded encoder_inputs into a state vector. It keeps the outputs of this\n  RNN at every step to use for attention later. Next, it embeds decoder_inputs\n  by another newly created embedding (of shape [num_decoder_symbols x\n  input_size]). Then it runs attention decoder, initialized with the last\n  encoder state, on embedded decoder_inputs and attending to encoder outputs.\n\n  Warning: when output_projection is None, the size of the attention vectors\n  and variables will be made proportional to num_decoder_symbols, can be large.\n\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\n    num_decoder_symbols: Integer; number of symbols on the decoder side.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_heads: Number of attention heads that read from attention_states.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_decoder_symbols] and B has\n      shape [num_decoder_symbols]; if provided and feed_previous=True, each\n      fed previous output will first be multiplied by W and added B.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\n      of decoder_inputs will be used (the \"GO\" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype of the initial RNN state (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      \"embedding_attention_seq2seq\".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x num_decoder_symbols] containing the generated\n        outputs.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  \"\"\"\n  with variable_scope.variable_scope(\n      scope or \"embedding_attention_seq2seq\", dtype=dtype) as scope:\n    dtype = scope.dtype\n    # Encoder.\n\n    encoder_cell = enc_cell\n\n    encoder_cell = core_rnn_cell.EmbeddingWrapper(\n        encoder_cell,\n        embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    encoder_outputs, encoder_state = rnn.static_rnn(\n        encoder_cell, encoder_inputs, dtype=dtype)\n\n    # First calculate a concatenation of encoder outputs to put attention on.\n    top_states = [\n        array_ops.reshape(e, [-1, 1, encoder_cell.output_size]) for e in encoder_outputs\n    ]\n    attention_states = array_ops.concat(top_states, 1)\n\n    # Decoder.\n    output_size = None\n    if output_projection is None:\n      dec_cell = core_rnn_cell.OutputProjectionWrapper(dec_cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n    if isinstance(feed_previous, bool):\n      return embedding_attention_decoder(\n          decoder_inputs,\n          encoder_state,\n          attention_states,\n          dec_cell,\n          num_decoder_symbols,\n          embedding_size,\n          num_heads=num_heads,\n          output_size=output_size,\n          output_projection=output_projection,\n          feed_previous=feed_previous,\n          initial_state_attention=initial_state_attention)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse):\n        outputs, state = embedding_attention_decoder(\n            decoder_inputs,\n            encoder_state,\n            attention_states,\n            dec_cell,\n            num_decoder_symbols,\n            embedding_size,\n            num_heads=num_heads,\n            output_size=output_size,\n            output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False,\n            initial_state_attention=initial_state_attention)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(\n          structure=encoder_state, flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state\n</code></pre>", "body_text": "Hi guys, I don't know if you're still interested on it, but I found that the problem is related to the operation of copying the cell passed as params to the embedding_attention_seq2seq function. This is because the same cell definition is used both for encoder and decoder. I think the tutorial is deprecated since it uses a seq2seq model with bucketing in contrast to a dynamic seq2seq. But, I'm pasting a modified function that works. The function is updated in the file tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py.\nthanks,\nFabio\ndef embedding_attention_seq2seq(encoder_inputs,\n                                decoder_inputs,\n                                enc_cell,\n                                dec_cell,\n                                num_encoder_symbols,\n                                num_decoder_symbols,\n                                embedding_size,\n                                num_heads=1,\n                                output_projection=None,\n                                feed_previous=False,\n                                dtype=None,\n                                scope=None,\n                                initial_state_attention=False):\n  \"\"\"Embedding sequence-to-sequence model with attention.\n\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\n  embedded encoder_inputs into a state vector. It keeps the outputs of this\n  RNN at every step to use for attention later. Next, it embeds decoder_inputs\n  by another newly created embedding (of shape [num_decoder_symbols x\n  input_size]). Then it runs attention decoder, initialized with the last\n  encoder state, on embedded decoder_inputs and attending to encoder outputs.\n\n  Warning: when output_projection is None, the size of the attention vectors\n  and variables will be made proportional to num_decoder_symbols, can be large.\n\n  Args:\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\n    num_decoder_symbols: Integer; number of symbols on the decoder side.\n    embedding_size: Integer, the length of the embedding vector for each symbol.\n    num_heads: Number of attention heads that read from attention_states.\n    output_projection: None or a pair (W, B) of output projection weights and\n      biases; W has shape [output_size x num_decoder_symbols] and B has\n      shape [num_decoder_symbols]; if provided and feed_previous=True, each\n      fed previous output will first be multiplied by W and added B.\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\n      of decoder_inputs will be used (the \"GO\" symbol), and all other decoder\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\n      If False, decoder_inputs are used as given (the standard decoder case).\n    dtype: The dtype of the initial RNN state (default: tf.float32).\n    scope: VariableScope for the created subgraph; defaults to\n      \"embedding_attention_seq2seq\".\n    initial_state_attention: If False (default), initial attentions are zero.\n      If True, initialize the attentions from the initial state and attention\n      states.\n\n  Returns:\n    A tuple of the form (outputs, state), where:\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\n        shape [batch_size x num_decoder_symbols] containing the generated\n        outputs.\n      state: The state of each decoder cell at the final time-step.\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\n  \"\"\"\n  with variable_scope.variable_scope(\n      scope or \"embedding_attention_seq2seq\", dtype=dtype) as scope:\n    dtype = scope.dtype\n    # Encoder.\n\n    encoder_cell = enc_cell\n\n    encoder_cell = core_rnn_cell.EmbeddingWrapper(\n        encoder_cell,\n        embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    encoder_outputs, encoder_state = rnn.static_rnn(\n        encoder_cell, encoder_inputs, dtype=dtype)\n\n    # First calculate a concatenation of encoder outputs to put attention on.\n    top_states = [\n        array_ops.reshape(e, [-1, 1, encoder_cell.output_size]) for e in encoder_outputs\n    ]\n    attention_states = array_ops.concat(top_states, 1)\n\n    # Decoder.\n    output_size = None\n    if output_projection is None:\n      dec_cell = core_rnn_cell.OutputProjectionWrapper(dec_cell, num_decoder_symbols)\n      output_size = num_decoder_symbols\n\n    if isinstance(feed_previous, bool):\n      return embedding_attention_decoder(\n          decoder_inputs,\n          encoder_state,\n          attention_states,\n          dec_cell,\n          num_decoder_symbols,\n          embedding_size,\n          num_heads=num_heads,\n          output_size=output_size,\n          output_projection=output_projection,\n          feed_previous=feed_previous,\n          initial_state_attention=initial_state_attention)\n\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n    def decoder(feed_previous_bool):\n      reuse = None if feed_previous_bool else True\n      with variable_scope.variable_scope(\n          variable_scope.get_variable_scope(), reuse=reuse):\n        outputs, state = embedding_attention_decoder(\n            decoder_inputs,\n            encoder_state,\n            attention_states,\n            dec_cell,\n            num_decoder_symbols,\n            embedding_size,\n            num_heads=num_heads,\n            output_size=output_size,\n            output_projection=output_projection,\n            feed_previous=feed_previous_bool,\n            update_embedding_for_previous=False,\n            initial_state_attention=initial_state_attention)\n        state_list = [state]\n        if nest.is_sequence(state):\n          state_list = nest.flatten(state)\n        return outputs + state_list\n\n    outputs_and_state = control_flow_ops.cond(feed_previous,\n                                              lambda: decoder(True),\n                                              lambda: decoder(False))\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n    state_list = outputs_and_state[outputs_len:]\n    state = state_list[0]\n    if nest.is_sequence(encoder_state):\n      state = nest.pack_sequence_as(\n          structure=encoder_state, flat_sequence=state_list)\n    return outputs_and_state[:outputs_len], state", "body": "Hi guys, I don't know if you're still interested on it, but I found that the problem is related to the operation of copying the cell passed as params to the `embedding_attention_seq2seq` function. This is because the same cell definition is used both for encoder and decoder. I think the tutorial is deprecated since it uses a seq2seq model with bucketing in contrast to a dynamic seq2seq. But, I'm pasting a modified function that works. The function is updated in the file `tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py`.\r\n\r\nthanks,\r\nFabio\r\n\r\n```!python\r\ndef embedding_attention_seq2seq(encoder_inputs,\r\n                                decoder_inputs,\r\n                                enc_cell,\r\n                                dec_cell,\r\n                                num_encoder_symbols,\r\n                                num_decoder_symbols,\r\n                                embedding_size,\r\n                                num_heads=1,\r\n                                output_projection=None,\r\n                                feed_previous=False,\r\n                                dtype=None,\r\n                                scope=None,\r\n                                initial_state_attention=False):\r\n  \"\"\"Embedding sequence-to-sequence model with attention.\r\n\r\n  This model first embeds encoder_inputs by a newly created embedding (of shape\r\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\r\n  embedded encoder_inputs into a state vector. It keeps the outputs of this\r\n  RNN at every step to use for attention later. Next, it embeds decoder_inputs\r\n  by another newly created embedding (of shape [num_decoder_symbols x\r\n  input_size]). Then it runs attention decoder, initialized with the last\r\n  encoder state, on embedded decoder_inputs and attending to encoder outputs.\r\n\r\n  Warning: when output_projection is None, the size of the attention vectors\r\n  and variables will be made proportional to num_decoder_symbols, can be large.\r\n\r\n  Args:\r\n    encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\r\n    decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].\r\n    cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.\r\n    num_encoder_symbols: Integer; number of symbols on the encoder side.\r\n    num_decoder_symbols: Integer; number of symbols on the decoder side.\r\n    embedding_size: Integer, the length of the embedding vector for each symbol.\r\n    num_heads: Number of attention heads that read from attention_states.\r\n    output_projection: None or a pair (W, B) of output projection weights and\r\n      biases; W has shape [output_size x num_decoder_symbols] and B has\r\n      shape [num_decoder_symbols]; if provided and feed_previous=True, each\r\n      fed previous output will first be multiplied by W and added B.\r\n    feed_previous: Boolean or scalar Boolean Tensor; if True, only the first\r\n      of decoder_inputs will be used (the \"GO\" symbol), and all other decoder\r\n      inputs will be taken from previous outputs (as in embedding_rnn_decoder).\r\n      If False, decoder_inputs are used as given (the standard decoder case).\r\n    dtype: The dtype of the initial RNN state (default: tf.float32).\r\n    scope: VariableScope for the created subgraph; defaults to\r\n      \"embedding_attention_seq2seq\".\r\n    initial_state_attention: If False (default), initial attentions are zero.\r\n      If True, initialize the attentions from the initial state and attention\r\n      states.\r\n\r\n  Returns:\r\n    A tuple of the form (outputs, state), where:\r\n      outputs: A list of the same length as decoder_inputs of 2D Tensors with\r\n        shape [batch_size x num_decoder_symbols] containing the generated\r\n        outputs.\r\n      state: The state of each decoder cell at the final time-step.\r\n        It is a 2D Tensor of shape [batch_size x cell.state_size].\r\n  \"\"\"\r\n  with variable_scope.variable_scope(\r\n      scope or \"embedding_attention_seq2seq\", dtype=dtype) as scope:\r\n    dtype = scope.dtype\r\n    # Encoder.\r\n\r\n    encoder_cell = enc_cell\r\n\r\n    encoder_cell = core_rnn_cell.EmbeddingWrapper(\r\n        encoder_cell,\r\n        embedding_classes=num_encoder_symbols,\r\n        embedding_size=embedding_size)\r\n    encoder_outputs, encoder_state = rnn.static_rnn(\r\n        encoder_cell, encoder_inputs, dtype=dtype)\r\n\r\n    # First calculate a concatenation of encoder outputs to put attention on.\r\n    top_states = [\r\n        array_ops.reshape(e, [-1, 1, encoder_cell.output_size]) for e in encoder_outputs\r\n    ]\r\n    attention_states = array_ops.concat(top_states, 1)\r\n\r\n    # Decoder.\r\n    output_size = None\r\n    if output_projection is None:\r\n      dec_cell = core_rnn_cell.OutputProjectionWrapper(dec_cell, num_decoder_symbols)\r\n      output_size = num_decoder_symbols\r\n\r\n    if isinstance(feed_previous, bool):\r\n      return embedding_attention_decoder(\r\n          decoder_inputs,\r\n          encoder_state,\r\n          attention_states,\r\n          dec_cell,\r\n          num_decoder_symbols,\r\n          embedding_size,\r\n          num_heads=num_heads,\r\n          output_size=output_size,\r\n          output_projection=output_projection,\r\n          feed_previous=feed_previous,\r\n          initial_state_attention=initial_state_attention)\r\n\r\n    # If feed_previous is a Tensor, we construct 2 graphs and use cond.\r\n    def decoder(feed_previous_bool):\r\n      reuse = None if feed_previous_bool else True\r\n      with variable_scope.variable_scope(\r\n          variable_scope.get_variable_scope(), reuse=reuse):\r\n        outputs, state = embedding_attention_decoder(\r\n            decoder_inputs,\r\n            encoder_state,\r\n            attention_states,\r\n            dec_cell,\r\n            num_decoder_symbols,\r\n            embedding_size,\r\n            num_heads=num_heads,\r\n            output_size=output_size,\r\n            output_projection=output_projection,\r\n            feed_previous=feed_previous_bool,\r\n            update_embedding_for_previous=False,\r\n            initial_state_attention=initial_state_attention)\r\n        state_list = [state]\r\n        if nest.is_sequence(state):\r\n          state_list = nest.flatten(state)\r\n        return outputs + state_list\r\n\r\n    outputs_and_state = control_flow_ops.cond(feed_previous,\r\n                                              lambda: decoder(True),\r\n                                              lambda: decoder(False))\r\n    outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\r\n    state_list = outputs_and_state[outputs_len:]\r\n    state = state_list[0]\r\n    if nest.is_sequence(encoder_state):\r\n      state = nest.pack_sequence_as(\r\n          structure=encoder_state, flat_sequence=state_list)\r\n    return outputs_and_state[:outputs_len], state\r\n```"}