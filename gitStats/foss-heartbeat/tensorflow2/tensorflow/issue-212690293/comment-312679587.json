{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312679587", "html_url": "https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-312679587", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191", "id": 312679587, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjY3OTU4Nw==", "user": {"login": "sachinh35", "id": 21972708, "node_id": "MDQ6VXNlcjIxOTcyNzA4", "avatar_url": "https://avatars2.githubusercontent.com/u/21972708?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sachinh35", "html_url": "https://github.com/sachinh35", "followers_url": "https://api.github.com/users/sachinh35/followers", "following_url": "https://api.github.com/users/sachinh35/following{/other_user}", "gists_url": "https://api.github.com/users/sachinh35/gists{/gist_id}", "starred_url": "https://api.github.com/users/sachinh35/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sachinh35/subscriptions", "organizations_url": "https://api.github.com/users/sachinh35/orgs", "repos_url": "https://api.github.com/users/sachinh35/repos", "events_url": "https://api.github.com/users/sachinh35/events{/privacy}", "received_events_url": "https://api.github.com/users/sachinh35/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-03T15:45:37Z", "updated_at": "2017-07-03T15:48:12Z", "author_association": "NONE", "body_html": "<p>I am totally new to this. Maybe this a pretty basic question but could you tell what argument to be passed as the decoder cell in this code? I am trying to develop the seq2seq as shown in the tensorflow tutorial using own dataset.</p>\n<p>`<br>\nfrom <strong>future</strong> import absolute_import<br>\nfrom <strong>future</strong> import division<br>\nfrom <strong>future</strong> import print_function</p>\n<p>import random</p>\n<p>import numpy as np<br>\nfrom six.moves import xrange  # pylint: disable=redefined-builtin<br>\nimport tensorflow as tf</p>\n<p>import data_utils</p>\n<p>class Seq2SeqModel(object):<br>\ndef <strong>init</strong>(self,<br>\nsource_vocab_size,<br>\ntarget_vocab_size,<br>\nbuckets,<br>\nsize,<br>\nnum_layers,<br>\nmax_gradient_norm,<br>\nbatch_size,<br>\nlearning_rate,<br>\nlearning_rate_decay_factor,<br>\nuse_lstm=False,<br>\nnum_samples=512,<br>\nforward_only=False,<br>\ndtype=tf.float32):</p>\n<pre><code>self.source_vocab_size = source_vocab_size\nself.target_vocab_size = target_vocab_size\nself.buckets = buckets\nself.batch_size = batch_size\nself.learning_rate = tf.Variable(\n    float(learning_rate), trainable=False, dtype=dtype)\nself.learning_rate_decay_op = self.learning_rate.assign(\n    self.learning_rate * learning_rate_decay_factor)\nself.global_step = tf.Variable(0, trainable=False)\n\n\noutput_projection = None\nsoftmax_loss_function = None\n\nif num_samples &gt; 0 and num_samples &lt; self.target_vocab_size:\n  w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n  w = tf.transpose(w_t)\n  b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n  output_projection = (w, b)\n\n  def sampled_loss(labels, inputs):\n    labels = tf.reshape(labels, [-1, 1])\n    \n    local_w_t = tf.cast(w_t, tf.float32)\n    local_b = tf.cast(b, tf.float32)\n    local_inputs = tf.cast(inputs, tf.float32)\n    return tf.cast(\n        tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n                                   num_samples, self.target_vocab_size),\n        dtype)\n  softmax_loss_function = sampled_loss\n\n\ndef single_cell():\n  return tf.nn.rnn_cell.GRUCell(size)\nif use_lstm:\n  def single_cell():\n    return tf.nn.rnn_cell.BasicLSTMCell(size)\ncell = single_cell()\nif num_layers &gt; 1:\n  cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])\n\n\ndef seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n  return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n      encoder_inputs,\n      decoder_inputs,\n      cell,\n      num_encoder_symbols=source_vocab_size,\n      num_decoder_symbols=target_vocab_size,\n      embedding_size=size,\n      output_projection=output_projection,\n      feed_previous=do_decode,\n      dtype=dtype)\n\n\nself.encoder_inputs = []\nself.decoder_inputs = []\nself.target_weights = []\nfor i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n  self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                            name=\"encoder{0}\".format(i)))\nfor i in xrange(buckets[-1][1] + 1):\n  self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                            name=\"decoder{0}\".format(i)))\n  self.target_weights.append(tf.placeholder(dtype, shape=[None],\n                                            name=\"weight{0}\".format(i)))\n\n# Our targets are decoder inputs shifted by one.\ntargets = [self.decoder_inputs[i + 1]\n           for i in xrange(len(self.decoder_inputs) - 1)]\n\n# Training outputs and losses.\nif forward_only:\n  self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n      self.encoder_inputs, self.decoder_inputs, targets,\n      self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n      softmax_loss_function=softmax_loss_function)\n  # If we use output projection, we need to project outputs for decoding.\n  if output_projection is not None:\n    for b in xrange(len(buckets)):\n      self.outputs[b] = [\n          tf.matmul(output, output_projection[0]) + output_projection[1]\n          for output in self.outputs[b]\n      ]\nelse:\n  self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n      self.encoder_inputs, self.decoder_inputs, targets,\n      self.target_weights, buckets,\n      lambda x, y: seq2seq_f(x, y, False),\n      softmax_loss_function=softmax_loss_function)\n\n# Gradients and SGD update operation for training the model.\nparams = tf.trainable_variables()\nif not forward_only:\n  self.gradient_norms = []\n  self.updates = []\n  opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n  for b in xrange(len(buckets)):\n    gradients = tf.gradients(self.losses[b], params)\n    clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                     max_gradient_norm)\n    self.gradient_norms.append(norm)\n    self.updates.append(opt.apply_gradients(\n        zip(clipped_gradients, params), global_step=self.global_step))\n\nself.saver = tf.train.Saver(tf.global_variables())\n</code></pre>\n<p>def step(self, session, encoder_inputs, decoder_inputs, target_weights,<br>\nbucket_id, forward_only):</p>\n<pre><code># Check if the sizes match.\nencoder_size, decoder_size = self.buckets[bucket_id]\nif len(encoder_inputs) != encoder_size:\n  raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(encoder_inputs), encoder_size))\nif len(decoder_inputs) != decoder_size:\n  raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(decoder_inputs), decoder_size))\nif len(target_weights) != decoder_size:\n  raise ValueError(\"Weights length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(target_weights), decoder_size))\n\n# Input feed: encoder inputs, decoder inputs, target_weights, as provided.\ninput_feed = {}\nfor l in xrange(encoder_size):\n  input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\nfor l in xrange(decoder_size):\n  input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n  input_feed[self.target_weights[l].name] = target_weights[l]\n\n# Since our targets are decoder inputs shifted by one, we need one more.\nlast_target = self.decoder_inputs[decoder_size].name\ninput_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n# Output feed: depends on whether we do a backward step or not.\nif not forward_only:\n  output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                 self.gradient_norms[bucket_id],  # Gradient norm.\n                 self.losses[bucket_id]]  # Loss for this batch.\nelse:\n  output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n  for l in xrange(decoder_size):  # Output logits.\n    output_feed.append(self.outputs[bucket_id][l])\n\noutputs = session.run(output_feed, input_feed)\nif not forward_only:\n  return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\nelse:\n  return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n</code></pre>\n<p>def get_batch(self, data, bucket_id):</p>\n<pre><code>encoder_size, decoder_size = self.buckets[bucket_id]\nencoder_inputs, decoder_inputs = [], []\n\n# Get a random batch of encoder and decoder inputs from data,\n# pad them if needed, reverse encoder inputs and add GO to decoder.\nfor _ in xrange(self.batch_size):\n  encoder_input, decoder_input = random.choice(data[bucket_id])\n\n  # Encoder inputs are padded and then reversed.\n  encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n  encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n  # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n  decoder_pad_size = decoder_size - len(decoder_input) - 1\n  decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n                        [data_utils.PAD_ID] * decoder_pad_size)\n\n# Now we create batch-major vectors from the data selected above.\nbatch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n# Batch encoder inputs are just re-indexed encoder_inputs.\nfor length_idx in xrange(encoder_size):\n  batch_encoder_inputs.append(\n      np.array([encoder_inputs[batch_idx][length_idx]\n                for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n# Batch decoder inputs are re-indexed decoder_inputs, we create weights.\nfor length_idx in xrange(decoder_size):\n  batch_decoder_inputs.append(\n      np.array([decoder_inputs[batch_idx][length_idx]\n                for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n  # Create target_weights to be 0 for targets that are padding.\n  batch_weight = np.ones(self.batch_size, dtype=np.float32)\n  for batch_idx in xrange(self.batch_size):\n    # We set weight to 0 if the corresponding target is a PAD symbol.\n    # The corresponding target is decoder_input shifted by 1 forward.\n    if length_idx &lt; decoder_size - 1:\n      target = decoder_inputs[batch_idx][length_idx + 1]\n    if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n      batch_weight[batch_idx] = 0.0\n  batch_weights.append(batch_weight)\nreturn batch_encoder_inputs, batch_decoder_inputs, batch_weights`\n</code></pre>", "body_text": "I am totally new to this. Maybe this a pretty basic question but could you tell what argument to be passed as the decoder cell in this code? I am trying to develop the seq2seq as shown in the tensorflow tutorial using own dataset.\n`\nfrom future import absolute_import\nfrom future import division\nfrom future import print_function\nimport random\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport data_utils\nclass Seq2SeqModel(object):\ndef init(self,\nsource_vocab_size,\ntarget_vocab_size,\nbuckets,\nsize,\nnum_layers,\nmax_gradient_norm,\nbatch_size,\nlearning_rate,\nlearning_rate_decay_factor,\nuse_lstm=False,\nnum_samples=512,\nforward_only=False,\ndtype=tf.float32):\nself.source_vocab_size = source_vocab_size\nself.target_vocab_size = target_vocab_size\nself.buckets = buckets\nself.batch_size = batch_size\nself.learning_rate = tf.Variable(\n    float(learning_rate), trainable=False, dtype=dtype)\nself.learning_rate_decay_op = self.learning_rate.assign(\n    self.learning_rate * learning_rate_decay_factor)\nself.global_step = tf.Variable(0, trainable=False)\n\n\noutput_projection = None\nsoftmax_loss_function = None\n\nif num_samples > 0 and num_samples < self.target_vocab_size:\n  w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n  w = tf.transpose(w_t)\n  b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n  output_projection = (w, b)\n\n  def sampled_loss(labels, inputs):\n    labels = tf.reshape(labels, [-1, 1])\n    \n    local_w_t = tf.cast(w_t, tf.float32)\n    local_b = tf.cast(b, tf.float32)\n    local_inputs = tf.cast(inputs, tf.float32)\n    return tf.cast(\n        tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n                                   num_samples, self.target_vocab_size),\n        dtype)\n  softmax_loss_function = sampled_loss\n\n\ndef single_cell():\n  return tf.nn.rnn_cell.GRUCell(size)\nif use_lstm:\n  def single_cell():\n    return tf.nn.rnn_cell.BasicLSTMCell(size)\ncell = single_cell()\nif num_layers > 1:\n  cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])\n\n\ndef seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n  return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n      encoder_inputs,\n      decoder_inputs,\n      cell,\n      num_encoder_symbols=source_vocab_size,\n      num_decoder_symbols=target_vocab_size,\n      embedding_size=size,\n      output_projection=output_projection,\n      feed_previous=do_decode,\n      dtype=dtype)\n\n\nself.encoder_inputs = []\nself.decoder_inputs = []\nself.target_weights = []\nfor i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n  self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                            name=\"encoder{0}\".format(i)))\nfor i in xrange(buckets[-1][1] + 1):\n  self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                            name=\"decoder{0}\".format(i)))\n  self.target_weights.append(tf.placeholder(dtype, shape=[None],\n                                            name=\"weight{0}\".format(i)))\n\n# Our targets are decoder inputs shifted by one.\ntargets = [self.decoder_inputs[i + 1]\n           for i in xrange(len(self.decoder_inputs) - 1)]\n\n# Training outputs and losses.\nif forward_only:\n  self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n      self.encoder_inputs, self.decoder_inputs, targets,\n      self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n      softmax_loss_function=softmax_loss_function)\n  # If we use output projection, we need to project outputs for decoding.\n  if output_projection is not None:\n    for b in xrange(len(buckets)):\n      self.outputs[b] = [\n          tf.matmul(output, output_projection[0]) + output_projection[1]\n          for output in self.outputs[b]\n      ]\nelse:\n  self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n      self.encoder_inputs, self.decoder_inputs, targets,\n      self.target_weights, buckets,\n      lambda x, y: seq2seq_f(x, y, False),\n      softmax_loss_function=softmax_loss_function)\n\n# Gradients and SGD update operation for training the model.\nparams = tf.trainable_variables()\nif not forward_only:\n  self.gradient_norms = []\n  self.updates = []\n  opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n  for b in xrange(len(buckets)):\n    gradients = tf.gradients(self.losses[b], params)\n    clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                     max_gradient_norm)\n    self.gradient_norms.append(norm)\n    self.updates.append(opt.apply_gradients(\n        zip(clipped_gradients, params), global_step=self.global_step))\n\nself.saver = tf.train.Saver(tf.global_variables())\n\ndef step(self, session, encoder_inputs, decoder_inputs, target_weights,\nbucket_id, forward_only):\n# Check if the sizes match.\nencoder_size, decoder_size = self.buckets[bucket_id]\nif len(encoder_inputs) != encoder_size:\n  raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(encoder_inputs), encoder_size))\nif len(decoder_inputs) != decoder_size:\n  raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(decoder_inputs), decoder_size))\nif len(target_weights) != decoder_size:\n  raise ValueError(\"Weights length must be equal to the one in bucket,\"\n                   \" %d != %d.\" % (len(target_weights), decoder_size))\n\n# Input feed: encoder inputs, decoder inputs, target_weights, as provided.\ninput_feed = {}\nfor l in xrange(encoder_size):\n  input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\nfor l in xrange(decoder_size):\n  input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n  input_feed[self.target_weights[l].name] = target_weights[l]\n\n# Since our targets are decoder inputs shifted by one, we need one more.\nlast_target = self.decoder_inputs[decoder_size].name\ninput_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n# Output feed: depends on whether we do a backward step or not.\nif not forward_only:\n  output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                 self.gradient_norms[bucket_id],  # Gradient norm.\n                 self.losses[bucket_id]]  # Loss for this batch.\nelse:\n  output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n  for l in xrange(decoder_size):  # Output logits.\n    output_feed.append(self.outputs[bucket_id][l])\n\noutputs = session.run(output_feed, input_feed)\nif not forward_only:\n  return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\nelse:\n  return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n\ndef get_batch(self, data, bucket_id):\nencoder_size, decoder_size = self.buckets[bucket_id]\nencoder_inputs, decoder_inputs = [], []\n\n# Get a random batch of encoder and decoder inputs from data,\n# pad them if needed, reverse encoder inputs and add GO to decoder.\nfor _ in xrange(self.batch_size):\n  encoder_input, decoder_input = random.choice(data[bucket_id])\n\n  # Encoder inputs are padded and then reversed.\n  encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n  encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n  # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n  decoder_pad_size = decoder_size - len(decoder_input) - 1\n  decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n                        [data_utils.PAD_ID] * decoder_pad_size)\n\n# Now we create batch-major vectors from the data selected above.\nbatch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n# Batch encoder inputs are just re-indexed encoder_inputs.\nfor length_idx in xrange(encoder_size):\n  batch_encoder_inputs.append(\n      np.array([encoder_inputs[batch_idx][length_idx]\n                for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n# Batch decoder inputs are re-indexed decoder_inputs, we create weights.\nfor length_idx in xrange(decoder_size):\n  batch_decoder_inputs.append(\n      np.array([decoder_inputs[batch_idx][length_idx]\n                for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n  # Create target_weights to be 0 for targets that are padding.\n  batch_weight = np.ones(self.batch_size, dtype=np.float32)\n  for batch_idx in xrange(self.batch_size):\n    # We set weight to 0 if the corresponding target is a PAD symbol.\n    # The corresponding target is decoder_input shifted by 1 forward.\n    if length_idx < decoder_size - 1:\n      target = decoder_inputs[batch_idx][length_idx + 1]\n    if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n      batch_weight[batch_idx] = 0.0\n  batch_weights.append(batch_weight)\nreturn batch_encoder_inputs, batch_decoder_inputs, batch_weights`", "body": "I am totally new to this. Maybe this a pretty basic question but could you tell what argument to be passed as the decoder cell in this code? I am trying to develop the seq2seq as shown in the tensorflow tutorial using own dataset. \r\n\r\n`\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport random\r\n\r\nimport numpy as np\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\nimport data_utils\r\n\r\n\r\nclass Seq2SeqModel(object):\r\n  def __init__(self,\r\n               source_vocab_size,\r\n               target_vocab_size,\r\n               buckets,\r\n               size,\r\n               num_layers,\r\n               max_gradient_norm,\r\n               batch_size,\r\n               learning_rate,\r\n               learning_rate_decay_factor,\r\n               use_lstm=False,\r\n               num_samples=512,\r\n               forward_only=False,\r\n               dtype=tf.float32):\r\n   \r\n    self.source_vocab_size = source_vocab_size\r\n    self.target_vocab_size = target_vocab_size\r\n    self.buckets = buckets\r\n    self.batch_size = batch_size\r\n    self.learning_rate = tf.Variable(\r\n        float(learning_rate), trainable=False, dtype=dtype)\r\n    self.learning_rate_decay_op = self.learning_rate.assign(\r\n        self.learning_rate * learning_rate_decay_factor)\r\n    self.global_step = tf.Variable(0, trainable=False)\r\n\r\n    \r\n    output_projection = None\r\n    softmax_loss_function = None\r\n    \r\n    if num_samples > 0 and num_samples < self.target_vocab_size:\r\n      w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\r\n      w = tf.transpose(w_t)\r\n      b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\r\n      output_projection = (w, b)\r\n\r\n      def sampled_loss(labels, inputs):\r\n        labels = tf.reshape(labels, [-1, 1])\r\n        \r\n        local_w_t = tf.cast(w_t, tf.float32)\r\n        local_b = tf.cast(b, tf.float32)\r\n        local_inputs = tf.cast(inputs, tf.float32)\r\n        return tf.cast(\r\n            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\r\n                                       num_samples, self.target_vocab_size),\r\n            dtype)\r\n      softmax_loss_function = sampled_loss\r\n\r\n    \r\n    def single_cell():\r\n      return tf.nn.rnn_cell.GRUCell(size)\r\n    if use_lstm:\r\n      def single_cell():\r\n        return tf.nn.rnn_cell.BasicLSTMCell(size)\r\n    cell = single_cell()\r\n    if num_layers > 1:\r\n      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\n\r\n   \r\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\r\n      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\r\n          encoder_inputs,\r\n          decoder_inputs,\r\n          cell,\r\n          num_encoder_symbols=source_vocab_size,\r\n          num_decoder_symbols=target_vocab_size,\r\n          embedding_size=size,\r\n          output_projection=output_projection,\r\n          feed_previous=do_decode,\r\n          dtype=dtype)\r\n\r\n    \r\n    self.encoder_inputs = []\r\n    self.decoder_inputs = []\r\n    self.target_weights = []\r\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\r\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                name=\"encoder{0}\".format(i)))\r\n    for i in xrange(buckets[-1][1] + 1):\r\n      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                name=\"decoder{0}\".format(i)))\r\n      self.target_weights.append(tf.placeholder(dtype, shape=[None],\r\n                                                name=\"weight{0}\".format(i)))\r\n\r\n    # Our targets are decoder inputs shifted by one.\r\n    targets = [self.decoder_inputs[i + 1]\r\n               for i in xrange(len(self.decoder_inputs) - 1)]\r\n\r\n    # Training outputs and losses.\r\n    if forward_only:\r\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\r\n          self.encoder_inputs, self.decoder_inputs, targets,\r\n          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\r\n          softmax_loss_function=softmax_loss_function)\r\n      # If we use output projection, we need to project outputs for decoding.\r\n      if output_projection is not None:\r\n        for b in xrange(len(buckets)):\r\n          self.outputs[b] = [\r\n              tf.matmul(output, output_projection[0]) + output_projection[1]\r\n              for output in self.outputs[b]\r\n          ]\r\n    else:\r\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\r\n          self.encoder_inputs, self.decoder_inputs, targets,\r\n          self.target_weights, buckets,\r\n          lambda x, y: seq2seq_f(x, y, False),\r\n          softmax_loss_function=softmax_loss_function)\r\n\r\n    # Gradients and SGD update operation for training the model.\r\n    params = tf.trainable_variables()\r\n    if not forward_only:\r\n      self.gradient_norms = []\r\n      self.updates = []\r\n      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\r\n      for b in xrange(len(buckets)):\r\n        gradients = tf.gradients(self.losses[b], params)\r\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\r\n                                                         max_gradient_norm)\r\n        self.gradient_norms.append(norm)\r\n        self.updates.append(opt.apply_gradients(\r\n            zip(clipped_gradients, params), global_step=self.global_step))\r\n\r\n    self.saver = tf.train.Saver(tf.global_variables())\r\n\r\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\r\n           bucket_id, forward_only):\r\n   \r\n    # Check if the sizes match.\r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    if len(encoder_inputs) != encoder_size:\r\n      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\r\n    if len(decoder_inputs) != decoder_size:\r\n      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\r\n    if len(target_weights) != decoder_size:\r\n      raise ValueError(\"Weights length must be equal to the one in bucket,\"\r\n                       \" %d != %d.\" % (len(target_weights), decoder_size))\r\n\r\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\r\n    input_feed = {}\r\n    for l in xrange(encoder_size):\r\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\r\n    for l in xrange(decoder_size):\r\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\r\n      input_feed[self.target_weights[l].name] = target_weights[l]\r\n\r\n    # Since our targets are decoder inputs shifted by one, we need one more.\r\n    last_target = self.decoder_inputs[decoder_size].name\r\n    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\r\n\r\n    # Output feed: depends on whether we do a backward step or not.\r\n    if not forward_only:\r\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\r\n                     self.gradient_norms[bucket_id],  # Gradient norm.\r\n                     self.losses[bucket_id]]  # Loss for this batch.\r\n    else:\r\n      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\r\n      for l in xrange(decoder_size):  # Output logits.\r\n        output_feed.append(self.outputs[bucket_id][l])\r\n\r\n    outputs = session.run(output_feed, input_feed)\r\n    if not forward_only:\r\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\r\n    else:\r\n      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\r\n\r\n  def get_batch(self, data, bucket_id):\r\n   \r\n    encoder_size, decoder_size = self.buckets[bucket_id]\r\n    encoder_inputs, decoder_inputs = [], []\r\n\r\n    # Get a random batch of encoder and decoder inputs from data,\r\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\r\n    for _ in xrange(self.batch_size):\r\n      encoder_input, decoder_input = random.choice(data[bucket_id])\r\n\r\n      # Encoder inputs are padded and then reversed.\r\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\r\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\r\n\r\n      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\r\n      decoder_pad_size = decoder_size - len(decoder_input) - 1\r\n      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\r\n                            [data_utils.PAD_ID] * decoder_pad_size)\r\n\r\n    # Now we create batch-major vectors from the data selected above.\r\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\r\n\r\n    # Batch encoder inputs are just re-indexed encoder_inputs.\r\n    for length_idx in xrange(encoder_size):\r\n      batch_encoder_inputs.append(\r\n          np.array([encoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\r\n    for length_idx in xrange(decoder_size):\r\n      batch_decoder_inputs.append(\r\n          np.array([decoder_inputs[batch_idx][length_idx]\r\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\r\n\r\n      # Create target_weights to be 0 for targets that are padding.\r\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\r\n      for batch_idx in xrange(self.batch_size):\r\n        # We set weight to 0 if the corresponding target is a PAD symbol.\r\n        # The corresponding target is decoder_input shifted by 1 forward.\r\n        if length_idx < decoder_size - 1:\r\n          target = decoder_inputs[batch_idx][length_idx + 1]\r\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\r\n          batch_weight[batch_idx] = 0.0\r\n      batch_weights.append(batch_weight)\r\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights`"}