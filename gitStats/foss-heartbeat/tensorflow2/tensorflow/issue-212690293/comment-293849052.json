{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293849052", "html_url": "https://github.com/tensorflow/tensorflow/issues/8191#issuecomment-293849052", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8191", "id": 293849052, "node_id": "MDEyOklzc3VlQ29tbWVudDI5Mzg0OTA1Mg==", "user": {"login": "pltrdy", "id": 6375843, "node_id": "MDQ6VXNlcjYzNzU4NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6375843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pltrdy", "html_url": "https://github.com/pltrdy", "followers_url": "https://api.github.com/users/pltrdy/followers", "following_url": "https://api.github.com/users/pltrdy/following{/other_user}", "gists_url": "https://api.github.com/users/pltrdy/gists{/gist_id}", "starred_url": "https://api.github.com/users/pltrdy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pltrdy/subscriptions", "organizations_url": "https://api.github.com/users/pltrdy/orgs", "repos_url": "https://api.github.com/users/pltrdy/repos", "events_url": "https://api.github.com/users/pltrdy/events{/privacy}", "received_events_url": "https://api.github.com/users/pltrdy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-13T10:11:20Z", "updated_at": "2017-04-18T09:47:57Z", "author_association": "NONE", "body_html": "<p>For information I had this issue while trying to stack LSTM cells:<br>\nMy orginial code was:</p>\n<pre><code>    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)\n    if is_training and keep_prob &lt; 1:\n      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n          lstm_cell, output_keep_prob=keep_prob)\n    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers, state_is_tuple=True)\n</code></pre>\n<p>Then, with the following code, creating the model was ok, but I couldn't share the variable with another model. (for instance if you create a train_model and a valid_model supposed to share tensors, it will fail)</p>\n<pre><code>    lstm_creator = lambda: tf.contrib.rnn.BasicLSTMCell(\n                                        hidden_size, \n                                        forget_bias=0.0, state_is_tuple=True)\n    if is_training and keep_prob &lt; 1:\n      cell_creator = lambda:tf.contrib.rnn.DropoutWrapper(\n          lstm_creator(), output_keep_prob=keep_prob)\n    else:\n      cell_creator = lstm_creator\n\n    cell = tf.contrib.rnn.MultiRNNCell([cell_creator() for _ in range(num_layers)], state_is_tuple=True)\n\n</code></pre>\n<p>So finally I used <code>lstm_creator</code> to be the function like <code>lstm_cell</code> in <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L112\">tensorflow/models/tutorials/rnn/ptb/ptb_word_lm.py#L112</a>. I now have:</p>\n<pre><code>def lstm_cell():\n      # With the latest TensorFlow source code (as of Mar 27, 2017),\n      # the BasicLSTMCell will need a reuse parameter which is unfortunately not\n      # defined in TensorFlow 1.0. To maintain backwards compatibility, we add\n      # an argument check here:\n      if 'reuse' in inspect.getargspec(\n          tf.contrib.rnn.BasicLSTMCell.__init__).args:\n        return tf.contrib.rnn.BasicLSTMCell(\n            size, forget_bias=0.0, state_is_tuple=True,\n            reuse=tf.get_variable_scope().reuse)\n      else:\n        return tf.contrib.rnn.BasicLSTMCell(\n            size, forget_bias=0.0, state_is_tuple=True)\n    attn_cell = lstm_cell\n    \n    lstm_creator = lstm_cell\n    if is_training and keep_prob &lt; 1:\n      cell_creator = lambda:tf.contrib.rnn.DropoutWrapper(\n          lstm_creator(), output_keep_prob=keep_prob)\n    else:\n      cell_creator = lstm_creator\n\n    cell = tf.contrib.rnn.MultiRNNCell([cell_creator() for _ in range(num_layers)], state_is_tuple=True)\n</code></pre>\n<p>It is now fully working</p>", "body_text": "For information I had this issue while trying to stack LSTM cells:\nMy orginial code was:\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)\n    if is_training and keep_prob < 1:\n      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n          lstm_cell, output_keep_prob=keep_prob)\n    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers, state_is_tuple=True)\n\nThen, with the following code, creating the model was ok, but I couldn't share the variable with another model. (for instance if you create a train_model and a valid_model supposed to share tensors, it will fail)\n    lstm_creator = lambda: tf.contrib.rnn.BasicLSTMCell(\n                                        hidden_size, \n                                        forget_bias=0.0, state_is_tuple=True)\n    if is_training and keep_prob < 1:\n      cell_creator = lambda:tf.contrib.rnn.DropoutWrapper(\n          lstm_creator(), output_keep_prob=keep_prob)\n    else:\n      cell_creator = lstm_creator\n\n    cell = tf.contrib.rnn.MultiRNNCell([cell_creator() for _ in range(num_layers)], state_is_tuple=True)\n\n\nSo finally I used lstm_creator to be the function like lstm_cell in tensorflow/models/tutorials/rnn/ptb/ptb_word_lm.py#L112. I now have:\ndef lstm_cell():\n      # With the latest TensorFlow source code (as of Mar 27, 2017),\n      # the BasicLSTMCell will need a reuse parameter which is unfortunately not\n      # defined in TensorFlow 1.0. To maintain backwards compatibility, we add\n      # an argument check here:\n      if 'reuse' in inspect.getargspec(\n          tf.contrib.rnn.BasicLSTMCell.__init__).args:\n        return tf.contrib.rnn.BasicLSTMCell(\n            size, forget_bias=0.0, state_is_tuple=True,\n            reuse=tf.get_variable_scope().reuse)\n      else:\n        return tf.contrib.rnn.BasicLSTMCell(\n            size, forget_bias=0.0, state_is_tuple=True)\n    attn_cell = lstm_cell\n    \n    lstm_creator = lstm_cell\n    if is_training and keep_prob < 1:\n      cell_creator = lambda:tf.contrib.rnn.DropoutWrapper(\n          lstm_creator(), output_keep_prob=keep_prob)\n    else:\n      cell_creator = lstm_creator\n\n    cell = tf.contrib.rnn.MultiRNNCell([cell_creator() for _ in range(num_layers)], state_is_tuple=True)\n\nIt is now fully working", "body": "For information I had this issue while trying to stack LSTM cells:\r\nMy orginial code was:\r\n```\r\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)\r\n    if is_training and keep_prob < 1:\r\n      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\r\n          lstm_cell, output_keep_prob=keep_prob)\r\n    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers, state_is_tuple=True)\r\n```\r\n\r\nThen, with the following code, creating the model was ok, but I couldn't share the variable with another model. (for instance if you create a train_model and a valid_model supposed to share tensors, it will fail)\r\n```\r\n    lstm_creator = lambda: tf.contrib.rnn.BasicLSTMCell(\r\n                                        hidden_size, \r\n                                        forget_bias=0.0, state_is_tuple=True)\r\n    if is_training and keep_prob < 1:\r\n      cell_creator = lambda:tf.contrib.rnn.DropoutWrapper(\r\n          lstm_creator(), output_keep_prob=keep_prob)\r\n    else:\r\n      cell_creator = lstm_creator\r\n\r\n    cell = tf.contrib.rnn.MultiRNNCell([cell_creator() for _ in range(num_layers)], state_is_tuple=True)\r\n\r\n```\r\n\r\nSo finally I used `lstm_creator` to be the function like `lstm_cell` in [tensorflow/models/tutorials/rnn/ptb/ptb_word_lm.py#L112](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L112). I now have:\r\n```\r\ndef lstm_cell():\r\n      # With the latest TensorFlow source code (as of Mar 27, 2017),\r\n      # the BasicLSTMCell will need a reuse parameter which is unfortunately not\r\n      # defined in TensorFlow 1.0. To maintain backwards compatibility, we add\r\n      # an argument check here:\r\n      if 'reuse' in inspect.getargspec(\r\n          tf.contrib.rnn.BasicLSTMCell.__init__).args:\r\n        return tf.contrib.rnn.BasicLSTMCell(\r\n            size, forget_bias=0.0, state_is_tuple=True,\r\n            reuse=tf.get_variable_scope().reuse)\r\n      else:\r\n        return tf.contrib.rnn.BasicLSTMCell(\r\n            size, forget_bias=0.0, state_is_tuple=True)\r\n    attn_cell = lstm_cell\r\n    \r\n    lstm_creator = lstm_cell\r\n    if is_training and keep_prob < 1:\r\n      cell_creator = lambda:tf.contrib.rnn.DropoutWrapper(\r\n          lstm_creator(), output_keep_prob=keep_prob)\r\n    else:\r\n      cell_creator = lstm_creator\r\n\r\n    cell = tf.contrib.rnn.MultiRNNCell([cell_creator() for _ in range(num_layers)], state_is_tuple=True)\r\n```\r\n\r\nIt is now fully working"}