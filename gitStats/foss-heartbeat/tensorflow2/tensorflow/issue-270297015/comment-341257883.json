{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341257883", "html_url": "https://github.com/tensorflow/tensorflow/issues/14154#issuecomment-341257883", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14154", "id": 341257883, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTI1Nzg4Mw==", "user": {"login": "protoget", "id": 5117188, "node_id": "MDQ6VXNlcjUxMTcxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5117188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/protoget", "html_url": "https://github.com/protoget", "followers_url": "https://api.github.com/users/protoget/followers", "following_url": "https://api.github.com/users/protoget/following{/other_user}", "gists_url": "https://api.github.com/users/protoget/gists{/gist_id}", "starred_url": "https://api.github.com/users/protoget/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/protoget/subscriptions", "organizations_url": "https://api.github.com/users/protoget/orgs", "repos_url": "https://api.github.com/users/protoget/repos", "events_url": "https://api.github.com/users/protoget/events{/privacy}", "received_events_url": "https://api.github.com/users/protoget/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-01T22:07:49Z", "updated_at": "2017-11-01T22:07:49Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5205204\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alquraishi\">@alquraishi</a><br>\nFor TF Cudnn layers, think TF as a wrapper on top of Nvidia Cudnn library. There is a very limited set of config TF can do. So you probably can refer to Nvidia's CuDNN manual if the doc is missing something.</p>\n<p>To answer ur question:</p>\n<blockquote>\n<p>when they are bidirectional and configured with multiple layers, integrate the outputs from both directions at a given layer n before sending it to the next layer n+1, or whether each direction works independently of the other?</p>\n</blockquote>\n<p>At the end of each layer, it concates output from both directions (forward goes first), which becomes the input for next layer, for both directions.</p>\n<p>Thanks for your input. It would be appreciated if you would like to improve the doc as well. Else we can make the change. :)</p>", "body_text": "@alquraishi\nFor TF Cudnn layers, think TF as a wrapper on top of Nvidia Cudnn library. There is a very limited set of config TF can do. So you probably can refer to Nvidia's CuDNN manual if the doc is missing something.\nTo answer ur question:\n\nwhen they are bidirectional and configured with multiple layers, integrate the outputs from both directions at a given layer n before sending it to the next layer n+1, or whether each direction works independently of the other?\n\nAt the end of each layer, it concates output from both directions (forward goes first), which becomes the input for next layer, for both directions.\nThanks for your input. It would be appreciated if you would like to improve the doc as well. Else we can make the change. :)", "body": "@alquraishi \r\nFor TF Cudnn layers, think TF as a wrapper on top of Nvidia Cudnn library. There is a very limited set of config TF can do. So you probably can refer to Nvidia's CuDNN manual if the doc is missing something.\r\n\r\nTo answer ur question:\r\n> when they are bidirectional and configured with multiple layers, integrate the outputs from both directions at a given layer n before sending it to the next layer n+1, or whether each direction works independently of the other?\r\n\r\nAt the end of each layer, it concates output from both directions (forward goes first), which becomes the input for next layer, for both directions.\r\n\r\nThanks for your input. It would be appreciated if you would like to improve the doc as well. Else we can make the change. :)\r\n"}