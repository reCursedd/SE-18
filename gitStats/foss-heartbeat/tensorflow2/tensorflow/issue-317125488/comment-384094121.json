{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/384094121", "html_url": "https://github.com/tensorflow/tensorflow/issues/18823#issuecomment-384094121", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18823", "id": 384094121, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDA5NDEyMQ==", "user": {"login": "mdanatg", "id": 26628547, "node_id": "MDQ6VXNlcjI2NjI4NTQ3", "avatar_url": "https://avatars1.githubusercontent.com/u/26628547?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mdanatg", "html_url": "https://github.com/mdanatg", "followers_url": "https://api.github.com/users/mdanatg/followers", "following_url": "https://api.github.com/users/mdanatg/following{/other_user}", "gists_url": "https://api.github.com/users/mdanatg/gists{/gist_id}", "starred_url": "https://api.github.com/users/mdanatg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mdanatg/subscriptions", "organizations_url": "https://api.github.com/users/mdanatg/orgs", "repos_url": "https://api.github.com/users/mdanatg/repos", "events_url": "https://api.github.com/users/mdanatg/events{/privacy}", "received_events_url": "https://api.github.com/users/mdanatg/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-24T22:01:09Z", "updated_at": "2018-04-25T05:05:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The answer largely depends on how you need to use the variable - after the call to to_graph, do you just want a variable initialized with what's inside v, or do you have another purpose in mind? In general, I tend to stay away from variables because they require separate initialization and I have to watch for concurrency. But if you need a trainable variable there is obviously no other choice.</p>\n<p>In our case, calling <code>t()</code> will indeed return a Tensor, which can be used in an initializer or assignment. You could also create the variable inside the compiled code, as it's possible to mix TF calls in there (the rule of thumb is that Python variables are Tensors, and Python lists are TensorArrays). See the snippet below for a handful of examples.</p>\n<p>Side note: we haven't yet added support for extend(), so that won't work, and I changed it to appends.</p>\n<pre><code>import tensorflow as tf\n\nfrom tensorflow.contrib import autograph\n\ndef func():\n  v = []\n  autograph.utils.set_element_type(v, tf.float32)\n  v.append(0.5)\n  v.append(0.1)\n  return v.stack()\n\ndef func_with_var():\n  v = []\n  autograph.utils.set_element_type(v, tf.float32)\n  v.append(0.5)\n  v.append(0.1)\n  return tf.get_variable('foo', initializer=v.stack())\n\ndef func_with_var_assign(my_var):\n  v = []\n  autograph.utils.set_element_type(v, tf.float32)\n  v.append(0.5)\n  v.append(0.1)\n  # This is trickier, assign returns an op, but we can use it.\n  return my_var.assign(v.stack())\n\n# Original code:\nt = autograph.to_graph(func, verbose=True)\n\n# Mix tf.get_variable in:\nt_var = autograph.to_graph(func, verbose=True)\n\n# Pass a variable separately:\nt_assign = autograph.to_graph(func_with_var_assign, verbose=True)\n\nwith tf.Graph().as_default():\n  with tf.Session() as sess:\n    \n    # Output of t:\n    print(sess.run(t()))\n    \n    # Output of t_var:\n    print(sess.run(t_var()))\n\n  # Setup to use t_assign:\n  my_var = tf.get_variable('foo', shape=(2,))\n  assign_op = t_assign(my_var)\n  \n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(assign_op)\n    print(sess.run(my_var))\n</code></pre>", "body_text": "The answer largely depends on how you need to use the variable - after the call to to_graph, do you just want a variable initialized with what's inside v, or do you have another purpose in mind? In general, I tend to stay away from variables because they require separate initialization and I have to watch for concurrency. But if you need a trainable variable there is obviously no other choice.\nIn our case, calling t() will indeed return a Tensor, which can be used in an initializer or assignment. You could also create the variable inside the compiled code, as it's possible to mix TF calls in there (the rule of thumb is that Python variables are Tensors, and Python lists are TensorArrays). See the snippet below for a handful of examples.\nSide note: we haven't yet added support for extend(), so that won't work, and I changed it to appends.\nimport tensorflow as tf\n\nfrom tensorflow.contrib import autograph\n\ndef func():\n  v = []\n  autograph.utils.set_element_type(v, tf.float32)\n  v.append(0.5)\n  v.append(0.1)\n  return v.stack()\n\ndef func_with_var():\n  v = []\n  autograph.utils.set_element_type(v, tf.float32)\n  v.append(0.5)\n  v.append(0.1)\n  return tf.get_variable('foo', initializer=v.stack())\n\ndef func_with_var_assign(my_var):\n  v = []\n  autograph.utils.set_element_type(v, tf.float32)\n  v.append(0.5)\n  v.append(0.1)\n  # This is trickier, assign returns an op, but we can use it.\n  return my_var.assign(v.stack())\n\n# Original code:\nt = autograph.to_graph(func, verbose=True)\n\n# Mix tf.get_variable in:\nt_var = autograph.to_graph(func, verbose=True)\n\n# Pass a variable separately:\nt_assign = autograph.to_graph(func_with_var_assign, verbose=True)\n\nwith tf.Graph().as_default():\n  with tf.Session() as sess:\n    \n    # Output of t:\n    print(sess.run(t()))\n    \n    # Output of t_var:\n    print(sess.run(t_var()))\n\n  # Setup to use t_assign:\n  my_var = tf.get_variable('foo', shape=(2,))\n  assign_op = t_assign(my_var)\n  \n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(assign_op)\n    print(sess.run(my_var))", "body": "The answer largely depends on how you need to use the variable - after the call to to_graph, do you just want a variable initialized with what's inside v, or do you have another purpose in mind? In general, I tend to stay away from variables because they require separate initialization and I have to watch for concurrency. But if you need a trainable variable there is obviously no other choice.\r\n\r\nIn our case, calling `t()` will indeed return a Tensor, which can be used in an initializer or assignment. You could also create the variable inside the compiled code, as it's possible to mix TF calls in there (the rule of thumb is that Python variables are Tensors, and Python lists are TensorArrays). See the snippet below for a handful of examples.\r\n\r\nSide note: we haven't yet added support for extend(), so that won't work, and I changed it to appends.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib import autograph\r\n\r\ndef func():\r\n  v = []\r\n  autograph.utils.set_element_type(v, tf.float32)\r\n  v.append(0.5)\r\n  v.append(0.1)\r\n  return v.stack()\r\n\r\ndef func_with_var():\r\n  v = []\r\n  autograph.utils.set_element_type(v, tf.float32)\r\n  v.append(0.5)\r\n  v.append(0.1)\r\n  return tf.get_variable('foo', initializer=v.stack())\r\n\r\ndef func_with_var_assign(my_var):\r\n  v = []\r\n  autograph.utils.set_element_type(v, tf.float32)\r\n  v.append(0.5)\r\n  v.append(0.1)\r\n  # This is trickier, assign returns an op, but we can use it.\r\n  return my_var.assign(v.stack())\r\n\r\n# Original code:\r\nt = autograph.to_graph(func, verbose=True)\r\n\r\n# Mix tf.get_variable in:\r\nt_var = autograph.to_graph(func, verbose=True)\r\n\r\n# Pass a variable separately:\r\nt_assign = autograph.to_graph(func_with_var_assign, verbose=True)\r\n\r\nwith tf.Graph().as_default():\r\n  with tf.Session() as sess:\r\n    \r\n    # Output of t:\r\n    print(sess.run(t()))\r\n    \r\n    # Output of t_var:\r\n    print(sess.run(t_var()))\r\n\r\n  # Setup to use t_assign:\r\n  my_var = tf.get_variable('foo', shape=(2,))\r\n  assign_op = t_assign(my_var)\r\n  \r\n  with tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(assign_op)\r\n    print(sess.run(my_var))\r\n```\r\n"}