{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335415078", "html_url": "https://github.com/tensorflow/tensorflow/issues/13499#issuecomment-335415078", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13499", "id": 335415078, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTQxNTA3OA==", "user": {"login": "laoreja", "id": 9369143, "node_id": "MDQ6VXNlcjkzNjkxNDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/9369143?v=4", "gravatar_id": "", "url": "https://api.github.com/users/laoreja", "html_url": "https://github.com/laoreja", "followers_url": "https://api.github.com/users/laoreja/followers", "following_url": "https://api.github.com/users/laoreja/following{/other_user}", "gists_url": "https://api.github.com/users/laoreja/gists{/gist_id}", "starred_url": "https://api.github.com/users/laoreja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/laoreja/subscriptions", "organizations_url": "https://api.github.com/users/laoreja/orgs", "repos_url": "https://api.github.com/users/laoreja/repos", "events_url": "https://api.github.com/users/laoreja/events{/privacy}", "received_events_url": "https://api.github.com/users/laoreja/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-10T09:26:55Z", "updated_at": "2017-10-10T09:26:55Z", "author_association": "NONE", "body_html": "<p>I would like to add some observations:</p>\n<p>If plug in my customized op into a large scale deep neural network, I will get the following error message (I do not do any CUDA memory manipulation, but CUDPP does):</p>\n<pre><code>2017-10-10 02:04:52.138035: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ER\nROR_ILLEGAL_ADDRESS\n2017-10-10 02:04:52.138076: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\n</code></pre>\n<p>If I write some CudaMemcpy statements in the kernel implementation, the error I receive is \"invalid argument\" (for the CudaMemcpy function call).</p>\n<p>But I also use the following script to test my customized op, it's really strange that it works well even if I use many GPU memory. (<code>query_square_point</code> is my customized op implemented on GPU)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/python</span>\n<span class=\"pl-k\">import</span> os, sys\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-c1\">BASE_DIR</span> <span class=\"pl-k\">=</span> os.path.dirname(os.path.abspath(<span class=\"pl-c1\">__file__</span>))\nsys.path.append(os.path.join(<span class=\"pl-c1\">BASE_DIR</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tf_ops/grouping<span class=\"pl-pds\">'</span></span>))\n\n<span class=\"pl-k\">from</span> tf_grouping <span class=\"pl-k\">import</span> query_square_point\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">gather_point</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">idx</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    input:</span>\n<span class=\"pl-s\">            batch_size * ndataset * x   float32</span>\n<span class=\"pl-s\">            batch_size * npoints        int32</span>\n<span class=\"pl-s\">    returns:</span>\n<span class=\"pl-s\">            batch_size * npoints * x    float32</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    features_shape <span class=\"pl-k\">=</span> features.get_shape()\n    batch_size <span class=\"pl-k\">=</span> features_shape[<span class=\"pl-c1\">0</span>].value\n    ndatasets <span class=\"pl-k\">=</span> features_shape[<span class=\"pl-c1\">1</span>].value\n    nfeatures <span class=\"pl-k\">=</span> features_shape[<span class=\"pl-c1\">2</span>].value\n    npoint <span class=\"pl-k\">=</span> idx.get_shape()[<span class=\"pl-c1\">1</span>].value\n    \n    offsets <span class=\"pl-k\">=</span> tf.tile(tf.range(batch_size)[:, <span class=\"pl-c1\">None</span>], [<span class=\"pl-c1\">1</span>, npoint]) <span class=\"pl-k\">*</span> ndatasets <span class=\"pl-k\">+</span> idx\n    offsets <span class=\"pl-k\">=</span> tf.reshape(offsets, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n    reshaped_features <span class=\"pl-k\">=</span> tf.reshape(features, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, nfeatures])\n    new_features <span class=\"pl-k\">=</span> tf.reshape(tf.gather(reshaped_features, offsets), [batch_size, npoint, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])  \n    \n    <span class=\"pl-k\">return</span> new_features\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">query_square_point_wrapper</span>(<span class=\"pl-smi\">grid_size</span>, <span class=\"pl-smi\">nsample</span>, <span class=\"pl-smi\">xyz</span>, <span class=\"pl-smi\">centroids_xyz</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    Input:</span>\n<span class=\"pl-s\">        grid_size: float32, ball search radius / 3.0</span>\n<span class=\"pl-s\">        nsample: int32, number of points selected in each ball region</span>\n<span class=\"pl-s\">        xyz: (batch_size, ndataset, 3) float32 array, input points</span>\n<span class=\"pl-s\">        centroids_xyz: (batch_size, npoint, 3) float32 array, query points</span>\n<span class=\"pl-s\">    Output:</span>\n<span class=\"pl-s\">        idx: (batch_size, npoint, nsample) int32 array, indices to input points</span>\n<span class=\"pl-s\">        pts_cnt: (batch_size, npoint) int32 array, number of unique points in each local region</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    batch_size <span class=\"pl-k\">=</span> centroids_xyz.get_shape()[<span class=\"pl-c1\">0</span>].value\n    npoint <span class=\"pl-k\">=</span> centroids_xyz.get_shape()[<span class=\"pl-c1\">1</span>].value\n    ndataset <span class=\"pl-k\">=</span> xyz.get_shape()[<span class=\"pl-c1\">1</span>].value\n    \n    min_x <span class=\"pl-k\">=</span> tf.reduce_min(xyz[:, :, <span class=\"pl-c1\">0</span>])\n    max_x <span class=\"pl-k\">=</span> tf.reduce_max(xyz[:, :, <span class=\"pl-c1\">0</span>])\n    min_y <span class=\"pl-k\">=</span> tf.reduce_min(xyz[:, :, <span class=\"pl-c1\">1</span>])\n    max_y <span class=\"pl-k\">=</span> tf.reduce_max(xyz[:, :, <span class=\"pl-c1\">1</span>])\n    min_z <span class=\"pl-k\">=</span> tf.reduce_min(xyz[:, :, <span class=\"pl-c1\">2</span>])\n    max_z <span class=\"pl-k\">=</span> tf.reduce_max(xyz[:, :, <span class=\"pl-c1\">2</span>])\n    limits <span class=\"pl-k\">=</span> tf.stack([min_x, max_x, min_y, max_y, min_z, max_z])\n        \n    x_size <span class=\"pl-k\">=</span> tf.cast(tf.ceil((max_x <span class=\"pl-k\">-</span> min_x) <span class=\"pl-k\">/</span> grid_size), tf.int32) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n    y_size <span class=\"pl-k\">=</span> tf.cast(tf.ceil((max_y <span class=\"pl-k\">-</span> min_y) <span class=\"pl-k\">/</span> grid_size), tf.int32) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n    z_size <span class=\"pl-k\">=</span> tf.cast(tf.ceil((max_z <span class=\"pl-k\">-</span> min_z) <span class=\"pl-k\">/</span> grid_size), tf.int32) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n    sizes <span class=\"pl-k\">=</span> tf.stack([x_size, y_size, z_size])\n    \n    idx, pts_cnt <span class=\"pl-k\">=</span> query_square_point(grid_size, nsample, xyz, centroids_xyz, limits, sizes)\n    \n    <span class=\"pl-k\">return</span> idx, pts_cnt\n\nchoice <span class=\"pl-k\">=</span> sys.argv[<span class=\"pl-c1\">2</span>]\nradius <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.4</span>\ngrid_size <span class=\"pl-k\">=</span> radius <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2.0</span> <span class=\"pl-k\">/</span> <span class=\"pl-c1\">3.0</span>\nnsample <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nnpoint <span class=\"pl-k\">=</span> <span class=\"pl-c1\">512</span>\nndataset <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1024</span>\n\nxyz <span class=\"pl-k\">=</span> tf.random_uniform(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[batch_size, ndataset, <span class=\"pl-c1\">3</span>], <span class=\"pl-v\">minval</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">maxval</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>xyz<span class=\"pl-pds\">\"</span></span>)\ncentroids_idx <span class=\"pl-k\">=</span> tf.constant(np.random.choice(ndataset, [batch_size, npoint]), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>centroids_idx<span class=\"pl-pds\">\"</span></span>)\ncentroids_xyz <span class=\"pl-k\">=</span> gather_point(xyz, centroids_idx)\na, b <span class=\"pl-k\">=</span> query_square_point_wrapper(grid_size, nsample, xyz, centroids_xyz)\n\nconfig <span class=\"pl-k\">=</span> tf.ConfigProto()\nconfig.gpu_options.allow_growth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\nconfig.log_device_placement <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\nsess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config)\nsess.run(xyz)\n\nstart <span class=\"pl-k\">=</span> time.time()\nsess.run(a)\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>first epoch<span class=\"pl-pds\">'</span></span>, time.time() <span class=\"pl-k\">-</span> start\n\nstart <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">1000</span>):\n\n    sess.run(a)\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>next 1000 epoch<span class=\"pl-pds\">'</span></span>, time.time() <span class=\"pl-k\">-</span> start</pre></div>", "body_text": "I would like to add some observations:\nIf plug in my customized op into a large scale deep neural network, I will get the following error message (I do not do any CUDA memory manipulation, but CUDPP does):\n2017-10-10 02:04:52.138035: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ER\nROR_ILLEGAL_ADDRESS\n2017-10-10 02:04:52.138076: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\n\nIf I write some CudaMemcpy statements in the kernel implementation, the error I receive is \"invalid argument\" (for the CudaMemcpy function call).\nBut I also use the following script to test my customized op, it's really strange that it works well even if I use many GPU memory. (query_square_point is my customized op implemented on GPU)\n#!/usr/bin/python\nimport os, sys\nimport tensorflow as tf\nimport numpy as np\nimport time\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.join(BASE_DIR, 'tf_ops/grouping'))\n\nfrom tf_grouping import query_square_point\n\ndef gather_point(features, idx):\n    '''\n    input:\n            batch_size * ndataset * x   float32\n            batch_size * npoints        int32\n    returns:\n            batch_size * npoints * x    float32\n    '''\n    features_shape = features.get_shape()\n    batch_size = features_shape[0].value\n    ndatasets = features_shape[1].value\n    nfeatures = features_shape[2].value\n    npoint = idx.get_shape()[1].value\n    \n    offsets = tf.tile(tf.range(batch_size)[:, None], [1, npoint]) * ndatasets + idx\n    offsets = tf.reshape(offsets, [-1])\n    reshaped_features = tf.reshape(features, [-1, nfeatures])\n    new_features = tf.reshape(tf.gather(reshaped_features, offsets), [batch_size, npoint, -1])  \n    \n    return new_features\n\ndef query_square_point_wrapper(grid_size, nsample, xyz, centroids_xyz):\n    '''\n    Input:\n        grid_size: float32, ball search radius / 3.0\n        nsample: int32, number of points selected in each ball region\n        xyz: (batch_size, ndataset, 3) float32 array, input points\n        centroids_xyz: (batch_size, npoint, 3) float32 array, query points\n    Output:\n        idx: (batch_size, npoint, nsample) int32 array, indices to input points\n        pts_cnt: (batch_size, npoint) int32 array, number of unique points in each local region\n    '''\n    batch_size = centroids_xyz.get_shape()[0].value\n    npoint = centroids_xyz.get_shape()[1].value\n    ndataset = xyz.get_shape()[1].value\n    \n    min_x = tf.reduce_min(xyz[:, :, 0])\n    max_x = tf.reduce_max(xyz[:, :, 0])\n    min_y = tf.reduce_min(xyz[:, :, 1])\n    max_y = tf.reduce_max(xyz[:, :, 1])\n    min_z = tf.reduce_min(xyz[:, :, 2])\n    max_z = tf.reduce_max(xyz[:, :, 2])\n    limits = tf.stack([min_x, max_x, min_y, max_y, min_z, max_z])\n        \n    x_size = tf.cast(tf.ceil((max_x - min_x) / grid_size), tf.int32) + 1\n    y_size = tf.cast(tf.ceil((max_y - min_y) / grid_size), tf.int32) + 1\n    z_size = tf.cast(tf.ceil((max_z - min_z) / grid_size), tf.int32) + 1\n    sizes = tf.stack([x_size, y_size, z_size])\n    \n    idx, pts_cnt = query_square_point(grid_size, nsample, xyz, centroids_xyz, limits, sizes)\n    \n    return idx, pts_cnt\n\nchoice = sys.argv[2]\nradius = 0.4\ngrid_size = radius * 2.0 / 3.0\nnsample = 64\nbatch_size = 32\nnpoint = 512\nndataset = 1024\n\nxyz = tf.random_uniform(shape=[batch_size, ndataset, 3], minval=-1.0, maxval=1.0, dtype=tf.float32, name=\"xyz\")\ncentroids_idx = tf.constant(np.random.choice(ndataset, [batch_size, npoint]), dtype=tf.int32, name=\"centroids_idx\")\ncentroids_xyz = gather_point(xyz, centroids_idx)\na, b = query_square_point_wrapper(grid_size, nsample, xyz, centroids_xyz)\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nconfig.log_device_placement = False\nsess = tf.Session(config=config)\nsess.run(xyz)\n\nstart = time.time()\nsess.run(a)\nprint 'first epoch', time.time() - start\n\nstart = time.time()\nfor i in xrange(1000):\n\n    sess.run(a)\nprint 'next 1000 epoch', time.time() - start", "body": "I would like to add some observations:\r\n\r\nIf plug in my customized op into a large scale deep neural network, I will get the following error message (I do not do any CUDA memory manipulation, but CUDPP does):\r\n```\r\n2017-10-10 02:04:52.138035: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ER\r\nROR_ILLEGAL_ADDRESS\r\n2017-10-10 02:04:52.138076: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n```\r\n\r\nIf I write some CudaMemcpy statements in the kernel implementation, the error I receive is \"invalid argument\" (for the CudaMemcpy function call).\r\n\r\nBut I also use the following script to test my customized op, it's really strange that it works well even if I use many GPU memory. (`query_square_point` is my customized op implemented on GPU)\r\n\r\n```python\r\n#!/usr/bin/python\r\nimport os, sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\r\nsys.path.append(os.path.join(BASE_DIR, 'tf_ops/grouping'))\r\n\r\nfrom tf_grouping import query_square_point\r\n\r\ndef gather_point(features, idx):\r\n    '''\r\n    input:\r\n            batch_size * ndataset * x   float32\r\n            batch_size * npoints        int32\r\n    returns:\r\n            batch_size * npoints * x    float32\r\n    '''\r\n    features_shape = features.get_shape()\r\n    batch_size = features_shape[0].value\r\n    ndatasets = features_shape[1].value\r\n    nfeatures = features_shape[2].value\r\n    npoint = idx.get_shape()[1].value\r\n    \r\n    offsets = tf.tile(tf.range(batch_size)[:, None], [1, npoint]) * ndatasets + idx\r\n    offsets = tf.reshape(offsets, [-1])\r\n    reshaped_features = tf.reshape(features, [-1, nfeatures])\r\n    new_features = tf.reshape(tf.gather(reshaped_features, offsets), [batch_size, npoint, -1])  \r\n    \r\n    return new_features\r\n\r\ndef query_square_point_wrapper(grid_size, nsample, xyz, centroids_xyz):\r\n    '''\r\n    Input:\r\n        grid_size: float32, ball search radius / 3.0\r\n        nsample: int32, number of points selected in each ball region\r\n        xyz: (batch_size, ndataset, 3) float32 array, input points\r\n        centroids_xyz: (batch_size, npoint, 3) float32 array, query points\r\n    Output:\r\n        idx: (batch_size, npoint, nsample) int32 array, indices to input points\r\n        pts_cnt: (batch_size, npoint) int32 array, number of unique points in each local region\r\n    '''\r\n    batch_size = centroids_xyz.get_shape()[0].value\r\n    npoint = centroids_xyz.get_shape()[1].value\r\n    ndataset = xyz.get_shape()[1].value\r\n    \r\n    min_x = tf.reduce_min(xyz[:, :, 0])\r\n    max_x = tf.reduce_max(xyz[:, :, 0])\r\n    min_y = tf.reduce_min(xyz[:, :, 1])\r\n    max_y = tf.reduce_max(xyz[:, :, 1])\r\n    min_z = tf.reduce_min(xyz[:, :, 2])\r\n    max_z = tf.reduce_max(xyz[:, :, 2])\r\n    limits = tf.stack([min_x, max_x, min_y, max_y, min_z, max_z])\r\n        \r\n    x_size = tf.cast(tf.ceil((max_x - min_x) / grid_size), tf.int32) + 1\r\n    y_size = tf.cast(tf.ceil((max_y - min_y) / grid_size), tf.int32) + 1\r\n    z_size = tf.cast(tf.ceil((max_z - min_z) / grid_size), tf.int32) + 1\r\n    sizes = tf.stack([x_size, y_size, z_size])\r\n    \r\n    idx, pts_cnt = query_square_point(grid_size, nsample, xyz, centroids_xyz, limits, sizes)\r\n    \r\n    return idx, pts_cnt\r\n\r\nchoice = sys.argv[2]\r\nradius = 0.4\r\ngrid_size = radius * 2.0 / 3.0\r\nnsample = 64\r\nbatch_size = 32\r\nnpoint = 512\r\nndataset = 1024\r\n\r\nxyz = tf.random_uniform(shape=[batch_size, ndataset, 3], minval=-1.0, maxval=1.0, dtype=tf.float32, name=\"xyz\")\r\ncentroids_idx = tf.constant(np.random.choice(ndataset, [batch_size, npoint]), dtype=tf.int32, name=\"centroids_idx\")\r\ncentroids_xyz = gather_point(xyz, centroids_idx)\r\na, b = query_square_point_wrapper(grid_size, nsample, xyz, centroids_xyz)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nconfig.log_device_placement = False\r\nsess = tf.Session(config=config)\r\nsess.run(xyz)\r\n\r\nstart = time.time()\r\nsess.run(a)\r\nprint 'first epoch', time.time() - start\r\n\r\nstart = time.time()\r\nfor i in xrange(1000):\r\n\r\n    sess.run(a)\r\nprint 'next 1000 epoch', time.time() - start\r\n```"}