{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11456", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11456/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11456/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11456/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11456", "id": 242373679, "node_id": "MDU6SXNzdWUyNDIzNzM2Nzk=", "number": 11456, "title": "GPU-Isolated docker containers fail in a distributed training with more than one worker.", "user": {"login": "MtDersvan", "id": 7069222, "node_id": "MDQ6VXNlcjcwNjkyMjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7069222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MtDersvan", "html_url": "https://github.com/MtDersvan", "followers_url": "https://api.github.com/users/MtDersvan/followers", "following_url": "https://api.github.com/users/MtDersvan/following{/other_user}", "gists_url": "https://api.github.com/users/MtDersvan/gists{/gist_id}", "starred_url": "https://api.github.com/users/MtDersvan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MtDersvan/subscriptions", "organizations_url": "https://api.github.com/users/MtDersvan/orgs", "repos_url": "https://api.github.com/users/MtDersvan/repos", "events_url": "https://api.github.com/users/MtDersvan/events{/privacy}", "received_events_url": "https://api.github.com/users/MtDersvan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-07-12T13:07:13Z", "updated_at": "2018-01-03T06:40:38Z", "closed_at": "2018-01-03T06:40:38Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: Yes</li>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: No</li>\n<li><strong>TensorFlow version</strong>: v1.2.0-rc2-21-g12f033d 1.2.0</li>\n<li><strong>Python version</strong>: Python 3.5.2</li>\n<li><strong>Bazel version</strong>: -</li>\n<li><strong>CUDA/cuDNN version</strong>: 'CUDA_VERSION': '8.0.61' / 'CUDNN_VERSION': '5.1.10'</li>\n<li><strong>GPU model and memory</strong>: 8x GCE K80 Tesla</li>\n</ul>\n<h3>Context:</h3>\n<ol>\n<li>\n<p>I have created a Cloud Compute Engine instance with 8 GPUs.</p>\n</li>\n<li>\n<p>I have started a couple of GPU-isolated containers with intention to run each as a separate worker:<br>\n<code>sudo NV_GPU=0 nvidia-docker run -it --name tf_worker_0 -p 8000:8000 -v /$(pwd)/repo:/notebooks/repo gcr.io/tensorflow/tensorflow:latest-gpu-py3 /bin/bash</code></p>\n</li>\n<li>\n<p>To create a loopback to the host I run following:<br>\n<code>echo $(netstat -nr | grep '^0\\.0\\.0\\.0' | awk '{print $2}') dockerhost &gt;&gt; /etc/hosts</code><br>\nThrough <code>dockerhost</code> it properly detects and pings all the open ports on the host including processes running in other containers that are exposed with <code>-p hostport:containerport</code>.</p>\n</li>\n</ol>\n<h3>Problem:</h3>\n<p><em>Case 1 - Normal</em>: When I run a distributed TensorFlow script using <code>dockerhost</code> with only one <code>worker</code> and any number of <code>ps</code> servers (e.g.  <code>--ps_hosts=dockerhost:7000,dockerhost:7001 --worker_hosts=dockerhost:8000</code>) everything works as intended - local grcp servers are launched, worker and ps communicate well.</p>\n<p><em>Case 2 - Issue</em>: When I increase the number of worker jobs (e.g.  <code>--worker_hosts=dockerhost:8000,dockerhost:8001</code>) i get <code>Master init: Unavailable</code> and <code>Master init: Internal</code> Errors.</p>\n<p><em>Case 3 - Normal</em>: BTW, everything works smoothly when I run all the workers inside a single container isolating GPUs with <code>CUDA_VISIBLE_DEVICES</code>.</p>\n<p><em>Case 4 - Issue</em>: Also, it is worth mentioning here, that using queues solution to shutdown <code>ps</code> proposed in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"180475976\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4713\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4713/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4713\">#4713</a> fails in both Case 1 and Case 2. This results in immediate <code>tensorflow.python.framework.errors_impl.UnavailableError</code> when starting <code>ps</code>.</p>\n<h3>Statement:</h3>\n<p>Is this an intended behaviour? Or does such setting is insufficient for a distributed training and a special <code>master</code> server or a cluster manager is needed? It just seems interesting that it would work for one worker but not for more.</p>\n<p><strong>Reproducible Toy Example</strong>:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1142032/repr.txt\">repr.txt</a></p>\n<p><strong>Log</strong>:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1142041/worker_log.txt\">worker_log.txt</a></p>", "body_text": "System information\n\nHave I written custom code: Yes\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): No\nTensorFlow version: v1.2.0-rc2-21-g12f033d 1.2.0\nPython version: Python 3.5.2\nBazel version: -\nCUDA/cuDNN version: 'CUDA_VERSION': '8.0.61' / 'CUDNN_VERSION': '5.1.10'\nGPU model and memory: 8x GCE K80 Tesla\n\nContext:\n\n\nI have created a Cloud Compute Engine instance with 8 GPUs.\n\n\nI have started a couple of GPU-isolated containers with intention to run each as a separate worker:\nsudo NV_GPU=0 nvidia-docker run -it --name tf_worker_0 -p 8000:8000 -v /$(pwd)/repo:/notebooks/repo gcr.io/tensorflow/tensorflow:latest-gpu-py3 /bin/bash\n\n\nTo create a loopback to the host I run following:\necho $(netstat -nr | grep '^0\\.0\\.0\\.0' | awk '{print $2}') dockerhost >> /etc/hosts\nThrough dockerhost it properly detects and pings all the open ports on the host including processes running in other containers that are exposed with -p hostport:containerport.\n\n\nProblem:\nCase 1 - Normal: When I run a distributed TensorFlow script using dockerhost with only one worker and any number of ps servers (e.g.  --ps_hosts=dockerhost:7000,dockerhost:7001 --worker_hosts=dockerhost:8000) everything works as intended - local grcp servers are launched, worker and ps communicate well.\nCase 2 - Issue: When I increase the number of worker jobs (e.g.  --worker_hosts=dockerhost:8000,dockerhost:8001) i get Master init: Unavailable and Master init: Internal Errors.\nCase 3 - Normal: BTW, everything works smoothly when I run all the workers inside a single container isolating GPUs with CUDA_VISIBLE_DEVICES.\nCase 4 - Issue: Also, it is worth mentioning here, that using queues solution to shutdown ps proposed in #4713 fails in both Case 1 and Case 2. This results in immediate tensorflow.python.framework.errors_impl.UnavailableError when starting ps.\nStatement:\nIs this an intended behaviour? Or does such setting is insufficient for a distributed training and a special master server or a cluster manager is needed? It just seems interesting that it would work for one worker but not for more.\nReproducible Toy Example:\nrepr.txt\nLog:\nworker_log.txt", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: No\r\n- **TensorFlow version**: v1.2.0-rc2-21-g12f033d 1.2.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version**: -\r\n- **CUDA/cuDNN version**: 'CUDA_VERSION': '8.0.61' / 'CUDNN_VERSION': '5.1.10'\r\n- **GPU model and memory**: 8x GCE K80 Tesla\r\n\r\n### Context:\r\n1) I have created a Cloud Compute Engine instance with 8 GPUs.\r\n2) I have started a couple of GPU-isolated containers with intention to run each as a separate worker:\r\n`sudo NV_GPU=0 nvidia-docker run -it --name tf_worker_0 -p 8000:8000 -v /$(pwd)/repo:/notebooks/repo gcr.io/tensorflow/tensorflow:latest-gpu-py3 /bin/bash` \r\n\r\n3) To create a loopback to the host I run following:\r\n`echo $(netstat -nr | grep '^0\\.0\\.0\\.0' | awk '{print $2}') dockerhost >> /etc/hosts`\r\nThrough `dockerhost` it properly detects and pings all the open ports on the host including processes running in other containers that are exposed with `-p hostport:containerport`.\r\n\r\n### Problem:\r\n*Case 1 - Normal*: When I run a distributed TensorFlow script using `dockerhost` with only one `worker` and any number of `ps` servers (e.g.  `--ps_hosts=dockerhost:7000,dockerhost:7001 --worker_hosts=dockerhost:8000`) everything works as intended - local grcp servers are launched, worker and ps communicate well. \r\n\r\n*Case 2 - Issue*: When I increase the number of worker jobs (e.g.  `--worker_hosts=dockerhost:8000,dockerhost:8001`) i get `Master init: Unavailable` and `Master init: Internal` Errors. \r\n\r\n*Case 3 - Normal*: BTW, everything works smoothly when I run all the workers inside a single container isolating GPUs with `CUDA_VISIBLE_DEVICES`.\r\n\r\n*Case 4 - Issue*: Also, it is worth mentioning here, that using queues solution to shutdown `ps` proposed in https://github.com/tensorflow/tensorflow/issues/4713 fails in both Case 1 and Case 2. This results in immediate `tensorflow.python.framework.errors_impl.UnavailableError` when starting `ps`.\r\n\r\n### Statement:\r\nIs this an intended behaviour? Or does such setting is insufficient for a distributed training and a special `master` server or a cluster manager is needed? It just seems interesting that it would work for one worker but not for more.\r\n\r\n**Reproducible Toy Example**:\r\n[repr.txt](https://github.com/tensorflow/tensorflow/files/1142032/repr.txt)\r\n\r\n**Log**:\r\n[worker_log.txt](https://github.com/tensorflow/tensorflow/files/1142041/worker_log.txt)\r\n\r\n"}