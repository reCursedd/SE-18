{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/373195173", "html_url": "https://github.com/tensorflow/tensorflow/pull/17261#issuecomment-373195173", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17261", "id": 373195173, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzE5NTE3Mw==", "user": {"login": "zasdfgbnm", "id": 1032377, "node_id": "MDQ6VXNlcjEwMzIzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1032377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zasdfgbnm", "html_url": "https://github.com/zasdfgbnm", "followers_url": "https://api.github.com/users/zasdfgbnm/followers", "following_url": "https://api.github.com/users/zasdfgbnm/following{/other_user}", "gists_url": "https://api.github.com/users/zasdfgbnm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zasdfgbnm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zasdfgbnm/subscriptions", "organizations_url": "https://api.github.com/users/zasdfgbnm/orgs", "repos_url": "https://api.github.com/users/zasdfgbnm/repos", "events_url": "https://api.github.com/users/zasdfgbnm/events{/privacy}", "received_events_url": "https://api.github.com/users/zasdfgbnm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-14T22:24:15Z", "updated_at": "2018-03-14T22:24:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19293677\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ispirmustafa\">@ispirmustafa</a> I have updated the files. For the convenience of your review, below is the diff between recall and specificity obtained using the following command (on fish shell):</p>\n<div class=\"highlight highlight-source-fish\"><pre>git <span class=\"pl-c1\">diff</span> <span class=\"pl-s\">-w</span> (<span class=\"pl-c1\">sed</span> <span class=\"pl-s\">-n</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>1978,2067p<span class=\"pl-pds\">'</span></span> metrics_impl.py<span class=\"pl-k\">|</span><span class=\"pl-c1\">psub</span>) (<span class=\"pl-c1\">sed</span> <span class=\"pl-s\">-n</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2084,2176p<span class=\"pl-pds\">'</span></span> metrics_impl.py<span class=\"pl-k\">|</span><span class=\"pl-c1\">psub</span>)</pre></div>\n<div class=\"highlight highlight-source-diff\"><pre><span class=\"pl-mdr\">@@ -1,23 +1,26 @@</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>@tf_export('metrics.recall')</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>def recall(labels,</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>@tf_export('metrics.specificity')</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>def specificity(labels,</span>\n                 predictions,\n                 weights=None,\n                 metrics_collections=None,\n                 updates_collections=None,\n                 name=None):\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  \"\"\"Computes the recall of the predictions with respect to the labels.</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  \"\"\"Computes the specificity of the predictions with respect to the labels.</span>\n \n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  The `recall` function creates two local variables, `true_positives`</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  and `false_negatives`, that are used to compute the recall. This value is</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  ultimately returned as `recall`, an idempotent operation that simply divides</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  `true_positives` by the sum of `true_positives`  and `false_negatives`.</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  The `specificity` function creates two local variables, `true_negatives`</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  and `false_positives`, that are used to compute the specificity. This value</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  is ultimately returned as `specificity`, an idempotent operation that simply</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  divides `true_negatives` by the sum of `true_negatives` and `false_positives`.</span>\n \n   For estimation of the metric over a stream of data, the function creates an\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  `update_op` that updates these variables and returns the `recall`. `update_op`</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  weights each prediction by the corresponding value in `weights`.</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  `update_op` that updates these variables and returns the `specificity`.</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  `update_op` weights each prediction by the corresponding value in `weights`.</span>\n \n   If `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n \n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  For additional information about specificity and sensitivity, see the</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span></span>\n   Args:\n     labels: The ground truth values, a `Tensor` whose dimensions must match\n       `predictions`. Will be cast to `bool`.\n<span class=\"pl-mdr\">@@ -26,18 +29,18 @@</span> def recall(labels,\n     weights: Optional `Tensor` whose rank is either 0, or the same rank as\n       `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n       be either `1`, or the same as the corresponding `labels` dimension).\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    metrics_collections: An optional list of collections that `recall` should</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>      be added to.</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    metrics_collections: An optional list of collections that `specificity`</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>      should be added to.</span>\n     updates_collections: An optional list of collections that `update_op` should\n       be added to.\n     name: An optional variable_scope name.\n \n   Returns:\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    recall: Scalar float `Tensor` with the value of `true_positives` divided</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>      by the sum of `true_positives` and `false_negatives`.</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    update_op: `Operation` that increments `true_positives` and</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>      `false_negatives` variables appropriately and whose value matches</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>      `recall`.</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    specificity: Scalar float `Tensor` with the value of `true_negatives`</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>      divided by the sum of `true_negatives` and `false_positives`.</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    update_op: `Operation` that increments `true_negatives` and</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>      `false_positives` variables appropriately and whose value matches</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>      `specificity`.</span>\n \n   Raises:\n     ValueError: If `predictions` and `labels` have mismatched shapes, or if\n<span class=\"pl-mdr\">@@ -50,21 +53,21 @@</span> def recall(labels,\n     raise RuntimeError('tf.metrics.recall is not supported is not '\n                        'supported when eager execution is enabled.')\n \n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  with variable_scope.variable_scope(name, 'recall',</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  with variable_scope.variable_scope(name, 'specificity',</span>\n                                      (predictions, labels, weights)):\n     predictions, labels, weights = _remove_squeezable_dimensions(\n         predictions=math_ops.cast(predictions, dtype=dtypes.bool),\n         labels=math_ops.cast(labels, dtype=dtypes.bool),\n         weights=weights)\n \n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    true_p, true_positives_update_op = true_positives(</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    true_n, true_negatives_update_op = true_negatives(</span>\n         labels,\n         predictions,\n         weights,\n         metrics_collections=None,\n         updates_collections=None,\n         name=None)\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    false_n, false_negatives_update_op = false_negatives(</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    false_p, false_positives_update_op = false_positives(</span>\n         labels,\n         predictions,\n         weights,\n<span class=\"pl-mdr\">@@ -72,19 +75,19 @@</span> def recall(labels,\n         updates_collections=None,\n         name=None)\n \n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    def compute_recall(true_p, false_n, name):</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    def compute_specificity(true_n, false_p, name):</span>\n       return array_ops.where(\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>          math_ops.greater(true_p + false_n, 0),</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>          math_ops.div(true_p, true_p + false_n), 0, name)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>          math_ops.greater(true_n + false_p, 0),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>          math_ops.div(true_n, true_n + false_p), 0, name)</span>\n \n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    rec = compute_recall(true_p, false_n, 'value')</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    update_op = compute_recall(true_positives_update_op,</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                               false_negatives_update_op, 'update_op')</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    specificity = compute_specificity(true_n, false_p, 'value')</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    update_op = compute_specificity(true_negatives_update_op,</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                    false_positives_update_op, 'update_op')</span>\n \n     if metrics_collections:\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>      ops.add_to_collections(metrics_collections, rec)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>      ops.add_to_collections(metrics_collections, specificity)</span>\n \n     if updates_collections:\n       ops.add_to_collections(updates_collections, update_op)\n \n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    return rec, update_op</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    return specificity, update_op</span></pre></div>", "body_text": "@ispirmustafa I have updated the files. For the convenience of your review, below is the diff between recall and specificity obtained using the following command (on fish shell):\ngit diff -w (sed -n '1978,2067p' metrics_impl.py|psub) (sed -n '2084,2176p' metrics_impl.py|psub)\n@@ -1,23 +1,26 @@\n-@tf_export('metrics.recall')\n-def recall(labels,\n+@tf_export('metrics.specificity')\n+def specificity(labels,\n                 predictions,\n                 weights=None,\n                 metrics_collections=None,\n                 updates_collections=None,\n                 name=None):\n-  \"\"\"Computes the recall of the predictions with respect to the labels.\n+  \"\"\"Computes the specificity of the predictions with respect to the labels.\n \n-  The `recall` function creates two local variables, `true_positives`\n-  and `false_negatives`, that are used to compute the recall. This value is\n-  ultimately returned as `recall`, an idempotent operation that simply divides\n-  `true_positives` by the sum of `true_positives`  and `false_negatives`.\n+  The `specificity` function creates two local variables, `true_negatives`\n+  and `false_positives`, that are used to compute the specificity. This value\n+  is ultimately returned as `specificity`, an idempotent operation that simply\n+  divides `true_negatives` by the sum of `true_negatives` and `false_positives`.\n \n   For estimation of the metric over a stream of data, the function creates an\n-  `update_op` that updates these variables and returns the `recall`. `update_op`\n-  weights each prediction by the corresponding value in `weights`.\n+  `update_op` that updates these variables and returns the `specificity`.\n+  `update_op` weights each prediction by the corresponding value in `weights`.\n \n   If `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n \n+  For additional information about specificity and sensitivity, see the\n+  following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n+\n   Args:\n     labels: The ground truth values, a `Tensor` whose dimensions must match\n       `predictions`. Will be cast to `bool`.\n@@ -26,18 +29,18 @@ def recall(labels,\n     weights: Optional `Tensor` whose rank is either 0, or the same rank as\n       `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n       be either `1`, or the same as the corresponding `labels` dimension).\n-    metrics_collections: An optional list of collections that `recall` should\n-      be added to.\n+    metrics_collections: An optional list of collections that `specificity`\n+      should be added to.\n     updates_collections: An optional list of collections that `update_op` should\n       be added to.\n     name: An optional variable_scope name.\n \n   Returns:\n-    recall: Scalar float `Tensor` with the value of `true_positives` divided\n-      by the sum of `true_positives` and `false_negatives`.\n-    update_op: `Operation` that increments `true_positives` and\n-      `false_negatives` variables appropriately and whose value matches\n-      `recall`.\n+    specificity: Scalar float `Tensor` with the value of `true_negatives`\n+      divided by the sum of `true_negatives` and `false_positives`.\n+    update_op: `Operation` that increments `true_negatives` and\n+      `false_positives` variables appropriately and whose value matches\n+      `specificity`.\n \n   Raises:\n     ValueError: If `predictions` and `labels` have mismatched shapes, or if\n@@ -50,21 +53,21 @@ def recall(labels,\n     raise RuntimeError('tf.metrics.recall is not supported is not '\n                        'supported when eager execution is enabled.')\n \n-  with variable_scope.variable_scope(name, 'recall',\n+  with variable_scope.variable_scope(name, 'specificity',\n                                      (predictions, labels, weights)):\n     predictions, labels, weights = _remove_squeezable_dimensions(\n         predictions=math_ops.cast(predictions, dtype=dtypes.bool),\n         labels=math_ops.cast(labels, dtype=dtypes.bool),\n         weights=weights)\n \n-    true_p, true_positives_update_op = true_positives(\n+    true_n, true_negatives_update_op = true_negatives(\n         labels,\n         predictions,\n         weights,\n         metrics_collections=None,\n         updates_collections=None,\n         name=None)\n-    false_n, false_negatives_update_op = false_negatives(\n+    false_p, false_positives_update_op = false_positives(\n         labels,\n         predictions,\n         weights,\n@@ -72,19 +75,19 @@ def recall(labels,\n         updates_collections=None,\n         name=None)\n \n-    def compute_recall(true_p, false_n, name):\n+    def compute_specificity(true_n, false_p, name):\n       return array_ops.where(\n-          math_ops.greater(true_p + false_n, 0),\n-          math_ops.div(true_p, true_p + false_n), 0, name)\n+          math_ops.greater(true_n + false_p, 0),\n+          math_ops.div(true_n, true_n + false_p), 0, name)\n \n-    rec = compute_recall(true_p, false_n, 'value')\n-    update_op = compute_recall(true_positives_update_op,\n-                               false_negatives_update_op, 'update_op')\n+    specificity = compute_specificity(true_n, false_p, 'value')\n+    update_op = compute_specificity(true_negatives_update_op,\n+                                    false_positives_update_op, 'update_op')\n \n     if metrics_collections:\n-      ops.add_to_collections(metrics_collections, rec)\n+      ops.add_to_collections(metrics_collections, specificity)\n \n     if updates_collections:\n       ops.add_to_collections(updates_collections, update_op)\n \n-    return rec, update_op\n+    return specificity, update_op", "body": "@ispirmustafa I have updated the files. For the convenience of your review, below is the diff between recall and specificity obtained using the following command (on fish shell):\r\n```fish\r\ngit diff -w (sed -n '1978,2067p' metrics_impl.py|psub) (sed -n '2084,2176p' metrics_impl.py|psub)\r\n```\r\n\r\n```diff\r\n@@ -1,23 +1,26 @@\r\n-@tf_export('metrics.recall')\r\n-def recall(labels,\r\n+@tf_export('metrics.specificity')\r\n+def specificity(labels,\r\n                 predictions,\r\n                 weights=None,\r\n                 metrics_collections=None,\r\n                 updates_collections=None,\r\n                 name=None):\r\n-  \"\"\"Computes the recall of the predictions with respect to the labels.\r\n+  \"\"\"Computes the specificity of the predictions with respect to the labels.\r\n \r\n-  The `recall` function creates two local variables, `true_positives`\r\n-  and `false_negatives`, that are used to compute the recall. This value is\r\n-  ultimately returned as `recall`, an idempotent operation that simply divides\r\n-  `true_positives` by the sum of `true_positives`  and `false_negatives`.\r\n+  The `specificity` function creates two local variables, `true_negatives`\r\n+  and `false_positives`, that are used to compute the specificity. This value\r\n+  is ultimately returned as `specificity`, an idempotent operation that simply\r\n+  divides `true_negatives` by the sum of `true_negatives` and `false_positives`.\r\n \r\n   For estimation of the metric over a stream of data, the function creates an\r\n-  `update_op` that updates these variables and returns the `recall`. `update_op`\r\n-  weights each prediction by the corresponding value in `weights`.\r\n+  `update_op` that updates these variables and returns the `specificity`.\r\n+  `update_op` weights each prediction by the corresponding value in `weights`.\r\n \r\n   If `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\r\n \r\n+  For additional information about specificity and sensitivity, see the\r\n+  following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\r\n+\r\n   Args:\r\n     labels: The ground truth values, a `Tensor` whose dimensions must match\r\n       `predictions`. Will be cast to `bool`.\r\n@@ -26,18 +29,18 @@ def recall(labels,\r\n     weights: Optional `Tensor` whose rank is either 0, or the same rank as\r\n       `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\r\n       be either `1`, or the same as the corresponding `labels` dimension).\r\n-    metrics_collections: An optional list of collections that `recall` should\r\n-      be added to.\r\n+    metrics_collections: An optional list of collections that `specificity`\r\n+      should be added to.\r\n     updates_collections: An optional list of collections that `update_op` should\r\n       be added to.\r\n     name: An optional variable_scope name.\r\n \r\n   Returns:\r\n-    recall: Scalar float `Tensor` with the value of `true_positives` divided\r\n-      by the sum of `true_positives` and `false_negatives`.\r\n-    update_op: `Operation` that increments `true_positives` and\r\n-      `false_negatives` variables appropriately and whose value matches\r\n-      `recall`.\r\n+    specificity: Scalar float `Tensor` with the value of `true_negatives`\r\n+      divided by the sum of `true_negatives` and `false_positives`.\r\n+    update_op: `Operation` that increments `true_negatives` and\r\n+      `false_positives` variables appropriately and whose value matches\r\n+      `specificity`.\r\n \r\n   Raises:\r\n     ValueError: If `predictions` and `labels` have mismatched shapes, or if\r\n@@ -50,21 +53,21 @@ def recall(labels,\r\n     raise RuntimeError('tf.metrics.recall is not supported is not '\r\n                        'supported when eager execution is enabled.')\r\n \r\n-  with variable_scope.variable_scope(name, 'recall',\r\n+  with variable_scope.variable_scope(name, 'specificity',\r\n                                      (predictions, labels, weights)):\r\n     predictions, labels, weights = _remove_squeezable_dimensions(\r\n         predictions=math_ops.cast(predictions, dtype=dtypes.bool),\r\n         labels=math_ops.cast(labels, dtype=dtypes.bool),\r\n         weights=weights)\r\n \r\n-    true_p, true_positives_update_op = true_positives(\r\n+    true_n, true_negatives_update_op = true_negatives(\r\n         labels,\r\n         predictions,\r\n         weights,\r\n         metrics_collections=None,\r\n         updates_collections=None,\r\n         name=None)\r\n-    false_n, false_negatives_update_op = false_negatives(\r\n+    false_p, false_positives_update_op = false_positives(\r\n         labels,\r\n         predictions,\r\n         weights,\r\n@@ -72,19 +75,19 @@ def recall(labels,\r\n         updates_collections=None,\r\n         name=None)\r\n \r\n-    def compute_recall(true_p, false_n, name):\r\n+    def compute_specificity(true_n, false_p, name):\r\n       return array_ops.where(\r\n-          math_ops.greater(true_p + false_n, 0),\r\n-          math_ops.div(true_p, true_p + false_n), 0, name)\r\n+          math_ops.greater(true_n + false_p, 0),\r\n+          math_ops.div(true_n, true_n + false_p), 0, name)\r\n \r\n-    rec = compute_recall(true_p, false_n, 'value')\r\n-    update_op = compute_recall(true_positives_update_op,\r\n-                               false_negatives_update_op, 'update_op')\r\n+    specificity = compute_specificity(true_n, false_p, 'value')\r\n+    update_op = compute_specificity(true_negatives_update_op,\r\n+                                    false_positives_update_op, 'update_op')\r\n \r\n     if metrics_collections:\r\n-      ops.add_to_collections(metrics_collections, rec)\r\n+      ops.add_to_collections(metrics_collections, specificity)\r\n \r\n     if updates_collections:\r\n       ops.add_to_collections(updates_collections, update_op)\r\n \r\n-    return rec, update_op\r\n+    return specificity, update_op\r\n```"}