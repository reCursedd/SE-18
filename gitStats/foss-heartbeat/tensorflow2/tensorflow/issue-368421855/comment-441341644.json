{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/441341644", "html_url": "https://github.com/tensorflow/tensorflow/issues/22854#issuecomment-441341644", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22854", "id": 441341644, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MTM0MTY0NA==", "user": {"login": "benjamintanweihao", "id": 861236, "node_id": "MDQ6VXNlcjg2MTIzNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/861236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benjamintanweihao", "html_url": "https://github.com/benjamintanweihao", "followers_url": "https://api.github.com/users/benjamintanweihao/followers", "following_url": "https://api.github.com/users/benjamintanweihao/following{/other_user}", "gists_url": "https://api.github.com/users/benjamintanweihao/gists{/gist_id}", "starred_url": "https://api.github.com/users/benjamintanweihao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benjamintanweihao/subscriptions", "organizations_url": "https://api.github.com/users/benjamintanweihao/orgs", "repos_url": "https://api.github.com/users/benjamintanweihao/repos", "events_url": "https://api.github.com/users/benjamintanweihao/events{/privacy}", "received_events_url": "https://api.github.com/users/benjamintanweihao/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-24T03:57:24Z", "updated_at": "2018-11-24T04:00:12Z", "author_association": "NONE", "body_html": "<p>I tried to explicitly split the creation of the calibration graph and the actual calibration into two sessions but I get<br>\n<code>AttributeError: 'NoneType' object has no attribute 'SerializeToString'</code>. Here's the code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">get_frozen_graph</span>(<span class=\"pl-smi\">graph_file</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Read Frozen Graph file from disk.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.gfile.GFile(graph_file, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rb<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> f:\n        graph_def <span class=\"pl-k\">=</span> tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    <span class=\"pl-k\">return</span> graph_def\n\n\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>gpu_options)) <span class=\"pl-k\">as</span> sess:\n    graph_file <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>frozen/optimized_freespace_mask_rcnn.pb<span class=\"pl-pds\">'</span></span>\n    graph <span class=\"pl-k\">=</span> get_frozen_graph(graph_file)\n    nodes <span class=\"pl-k\">=</span> [node.name <span class=\"pl-k\">for</span> node <span class=\"pl-k\">in</span> graph.node]\n    <span class=\"pl-c1\">print</span>(nodes)\n    <span class=\"pl-c1\">print</span>(out_names)\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">%d</span> ops in the final graph.<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">len</span>(graph.node))\n\n    trt_graph <span class=\"pl-k\">=</span> trt.create_inference_graph(\n        <span class=\"pl-v\">input_graph_def</span><span class=\"pl-k\">=</span>graph,\n        <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>out_names,\n        <span class=\"pl-v\">max_batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n        <span class=\"pl-v\">max_workspace_size_bytes</span><span class=\"pl-k\">=</span>max_workspace_size_bytes,\n        <span class=\"pl-v\">precision_mode</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">PRECISION</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> FP32 | FP16 | INT8</span>\n        <span class=\"pl-v\">minimum_segment_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">50</span>,\n        <span class=\"pl-v\">maximum_cached_engines</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>\n    )\n\n    tf.import_graph_def(trt_graph, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>)\n\n    filename <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>frozen/optimized_freespace_mask_rcnn_TRT<span class=\"pl-c1\">{}</span>Calib.pb<span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-c1\">PRECISION</span>)\n\n    <span class=\"pl-k\">with</span> gfile.GFile(filename, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>wb<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n        f.write(trt_graph.SerializeToString())\n\n\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>gpu_options)) <span class=\"pl-k\">as</span> sess2:\n    graph_file <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>frozen/optimized_freespace_mask_rcnn_TRT<span class=\"pl-c1\">{}</span>Calib.pb<span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-c1\">PRECISION</span>)\n    graph <span class=\"pl-k\">=</span> get_frozen_graph(graph_file)\n\n    nodes <span class=\"pl-k\">=</span> [node.name <span class=\"pl-k\">for</span> node <span class=\"pl-k\">in</span> graph.node]\n    <span class=\"pl-c1\">print</span>(nodes)\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">PRECISION</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>INT8<span class=\"pl-pds\">'</span></span>:\n        return_elements <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_image<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_image_meta<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_anchors<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">+</span> out_names\n\n        return_tensors <span class=\"pl-k\">=</span> tf.import_graph_def(\n            <span class=\"pl-v\">graph_def</span><span class=\"pl-k\">=</span>graph,\n            <span class=\"pl-v\">return_elements</span><span class=\"pl-k\">=</span>return_elements\n        )\n\n        inp0 <span class=\"pl-k\">=</span> return_tensors[<span class=\"pl-c1\">0</span>].outputs[<span class=\"pl-c1\">0</span>]\n        inp1 <span class=\"pl-k\">=</span> return_tensors[<span class=\"pl-c1\">1</span>].outputs[<span class=\"pl-c1\">0</span>]\n        inp2 <span class=\"pl-k\">=</span> return_tensors[<span class=\"pl-c1\">2</span>].outputs[<span class=\"pl-c1\">0</span>]\n\n        out <span class=\"pl-k\">=</span> [rt.outputs[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">for</span> rt <span class=\"pl-k\">in</span> return_tensors[<span class=\"pl-c1\">3</span>:]]\n\n        <span class=\"pl-k\">for</span> file_name <span class=\"pl-k\">in</span> <span class=\"pl-c1\">list</span>(paths.list_images(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>samples<span class=\"pl-pds\">'</span></span>))[<span class=\"pl-c1\">0</span>:<span class=\"pl-c1\">30</span>]:\n            <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> utils.read_tensor_from_image_file(file_name,\n                                                      <span class=\"pl-v\">input_height</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">HEIGHT</span>,\n                                                      <span class=\"pl-v\">input_width</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">WIDTH</span>)\n            start_time <span class=\"pl-k\">=</span> time.time()\n            _ <span class=\"pl-k\">=</span> sess2.run(out, {inp0: <span class=\"pl-c1\">input</span>,\n                                      inp1: np.random.random((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>)),\n                                      inp2: np.random.random((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>))})\n            stop_time <span class=\"pl-k\">=</span> time.time()\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>[Calibration] Inference time is <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(stop_time <span class=\"pl-k\">-</span> start_time))\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>[Calibration] Running <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(file_name))\n\n        trt_graph <span class=\"pl-k\">=</span> trt.calib_graph_to_infer_graph(graph)\n\n        filename <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>frozen/optimized_freespace_mask_rcnn_TRTINT8.pb<span class=\"pl-pds\">\"</span></span>\n\n        <span class=\"pl-k\">with</span> gfile.GFile(filename, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>wb<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n            f.write(trt_graph.SerializeToString())</pre></div>\n<p>What am I doing wrong?</p>", "body_text": "I tried to explicitly split the creation of the calibration graph and the actual calibration into two sessions but I get\nAttributeError: 'NoneType' object has no attribute 'SerializeToString'. Here's the code:\ndef get_frozen_graph(graph_file):\n    \"\"\"Read Frozen Graph file from disk.\"\"\"\n    with tf.gfile.GFile(graph_file, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    return graph_def\n\n\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n    graph_file = 'frozen/optimized_freespace_mask_rcnn.pb'\n    graph = get_frozen_graph(graph_file)\n    nodes = [node.name for node in graph.node]\n    print(nodes)\n    print(out_names)\n\n    print(\"%d ops in the final graph.\" % len(graph.node))\n\n    trt_graph = trt.create_inference_graph(\n        input_graph_def=graph,\n        outputs=out_names,\n        max_batch_size=1,\n        max_workspace_size_bytes=max_workspace_size_bytes,\n        precision_mode=PRECISION,  # FP32 | FP16 | INT8\n        minimum_segment_size=50,\n        maximum_cached_engines=1\n    )\n\n    tf.import_graph_def(trt_graph, name='')\n\n    filename = \"frozen/optimized_freespace_mask_rcnn_TRT{}Calib.pb\".format(PRECISION)\n\n    with gfile.GFile(filename, 'wb') as f:\n        f.write(trt_graph.SerializeToString())\n\n\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess2:\n    graph_file = \"frozen/optimized_freespace_mask_rcnn_TRT{}Calib.pb\".format(PRECISION)\n    graph = get_frozen_graph(graph_file)\n\n    nodes = [node.name for node in graph.node]\n    print(nodes)\n\n    if PRECISION == 'INT8':\n        return_elements = ['input_image', 'input_image_meta', 'input_anchors'] + out_names\n\n        return_tensors = tf.import_graph_def(\n            graph_def=graph,\n            return_elements=return_elements\n        )\n\n        inp0 = return_tensors[0].outputs[0]\n        inp1 = return_tensors[1].outputs[0]\n        inp2 = return_tensors[2].outputs[0]\n\n        out = [rt.outputs[0] for rt in return_tensors[3:]]\n\n        for file_name in list(paths.list_images('samples'))[0:30]:\n            input = utils.read_tensor_from_image_file(file_name,\n                                                      input_height=HEIGHT,\n                                                      input_width=WIDTH)\n            start_time = time.time()\n            _ = sess2.run(out, {inp0: input,\n                                      inp1: np.random.random((1, 20)),\n                                      inp2: np.random.random((1, 1, 4))})\n            stop_time = time.time()\n            print('[Calibration] Inference time is {}'.format(stop_time - start_time))\n            print('[Calibration] Running {}'.format(file_name))\n\n        trt_graph = trt.calib_graph_to_infer_graph(graph)\n\n        filename = \"frozen/optimized_freespace_mask_rcnn_TRTINT8.pb\"\n\n        with gfile.GFile(filename, 'wb') as f:\n            f.write(trt_graph.SerializeToString())\nWhat am I doing wrong?", "body": "I tried to explicitly split the creation of the calibration graph and the actual calibration into two sessions but I get  \r\n`AttributeError: 'NoneType' object has no attribute 'SerializeToString'`. Here's the code:\r\n\r\n```python\r\n\r\ndef get_frozen_graph(graph_file):\r\n    \"\"\"Read Frozen Graph file from disk.\"\"\"\r\n    with tf.gfile.GFile(graph_file, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n    return graph_def\r\n\r\n\r\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\r\n    graph_file = 'frozen/optimized_freespace_mask_rcnn.pb'\r\n    graph = get_frozen_graph(graph_file)\r\n    nodes = [node.name for node in graph.node]\r\n    print(nodes)\r\n    print(out_names)\r\n\r\n    print(\"%d ops in the final graph.\" % len(graph.node))\r\n\r\n    trt_graph = trt.create_inference_graph(\r\n        input_graph_def=graph,\r\n        outputs=out_names,\r\n        max_batch_size=1,\r\n        max_workspace_size_bytes=max_workspace_size_bytes,\r\n        precision_mode=PRECISION,  # FP32 | FP16 | INT8\r\n        minimum_segment_size=50,\r\n        maximum_cached_engines=1\r\n    )\r\n\r\n    tf.import_graph_def(trt_graph, name='')\r\n\r\n    filename = \"frozen/optimized_freespace_mask_rcnn_TRT{}Calib.pb\".format(PRECISION)\r\n\r\n    with gfile.GFile(filename, 'wb') as f:\r\n        f.write(trt_graph.SerializeToString())\r\n\r\n\r\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess2:\r\n    graph_file = \"frozen/optimized_freespace_mask_rcnn_TRT{}Calib.pb\".format(PRECISION)\r\n    graph = get_frozen_graph(graph_file)\r\n\r\n    nodes = [node.name for node in graph.node]\r\n    print(nodes)\r\n\r\n    if PRECISION == 'INT8':\r\n        return_elements = ['input_image', 'input_image_meta', 'input_anchors'] + out_names\r\n\r\n        return_tensors = tf.import_graph_def(\r\n            graph_def=graph,\r\n            return_elements=return_elements\r\n        )\r\n\r\n        inp0 = return_tensors[0].outputs[0]\r\n        inp1 = return_tensors[1].outputs[0]\r\n        inp2 = return_tensors[2].outputs[0]\r\n\r\n        out = [rt.outputs[0] for rt in return_tensors[3:]]\r\n\r\n        for file_name in list(paths.list_images('samples'))[0:30]:\r\n            input = utils.read_tensor_from_image_file(file_name,\r\n                                                      input_height=HEIGHT,\r\n                                                      input_width=WIDTH)\r\n            start_time = time.time()\r\n            _ = sess2.run(out, {inp0: input,\r\n                                      inp1: np.random.random((1, 20)),\r\n                                      inp2: np.random.random((1, 1, 4))})\r\n            stop_time = time.time()\r\n            print('[Calibration] Inference time is {}'.format(stop_time - start_time))\r\n            print('[Calibration] Running {}'.format(file_name))\r\n\r\n        trt_graph = trt.calib_graph_to_infer_graph(graph)\r\n\r\n        filename = \"frozen/optimized_freespace_mask_rcnn_TRTINT8.pb\"\r\n\r\n        with gfile.GFile(filename, 'wb') as f:\r\n            f.write(trt_graph.SerializeToString())\r\n```\r\n\r\nWhat am I doing wrong?"}