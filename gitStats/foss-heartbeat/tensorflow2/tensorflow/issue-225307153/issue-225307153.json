{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9548", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9548/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9548/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9548/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9548", "id": 225307153, "node_id": "MDU6SXNzdWUyMjUzMDcxNTM=", "number": 9548, "title": "distributed alexnet error : alexnet_v2/pool1/MaxPool : tensor_in must be 4-dimensional", "user": {"login": "Agoniii", "id": 18502794, "node_id": "MDQ6VXNlcjE4NTAyNzk0", "avatar_url": "https://avatars1.githubusercontent.com/u/18502794?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Agoniii", "html_url": "https://github.com/Agoniii", "followers_url": "https://api.github.com/users/Agoniii/followers", "following_url": "https://api.github.com/users/Agoniii/following{/other_user}", "gists_url": "https://api.github.com/users/Agoniii/gists{/gist_id}", "starred_url": "https://api.github.com/users/Agoniii/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Agoniii/subscriptions", "organizations_url": "https://api.github.com/users/Agoniii/orgs", "repos_url": "https://api.github.com/users/Agoniii/repos", "events_url": "https://api.github.com/users/Agoniii/events{/privacy}", "received_events_url": "https://api.github.com/users/Agoniii/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2017-04-30T06:07:21Z", "updated_at": "2017-05-25T23:42:00Z", "closed_at": "2017-05-25T23:41:59Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: linux Centos 7.1</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow (0.11.0)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When I try to run distributed alexnet training in TensorFlow, the alexnet model definition is referred to <a href=\"https://github.com/tensorflow/models/blob/master/slim/nets/alexnet.py\">here</a>. The Error occurred at 'alexnet_v2/pool1/MaxPool' as follows</p>\n<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: tensor_in must be 4-dimensional\n         [[Node: alexnet_v2/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:worker/replica:0/task:0/cpu:0\"](alexnet_v2/conv1/Relu)]]\n</code></pre>\n<h3>Source code / logs</h3>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nfrom datetime import datetime\nimport math\nimport sys\nimport time\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\n\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_integer(\"batch_size\", 100, \"Training batch size\")\ntf.app.flags.DEFINE_integer('num_batches', 100, \"Number of batches to run.\")\n\nFLAGS = tf.app.flags.FLAGS\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0, 0, stddev)\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding='SAME'):\n      with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope='alexnet_v2'):\n  with tf.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:\n    end_points_collection = sc.name + '_end_points'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',\n                        scope='conv1')\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool1')\n      net = slim.conv2d(net, 192, [5, 5], scope='conv2')\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool2')\n      net = slim.conv2d(net, 384, [3, 3], scope='conv3')\n      net = slim.conv2d(net, 384, [3, 3], scope='conv4')\n      net = slim.conv2d(net, 256, [3, 3], scope='conv5')\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool5')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding='VALID',\n                          scope='fc6')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope='dropout6')\n        net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope='dropout7')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer,\n                          scope='fc8')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n        end_points[sc.name + '/fc8'] = net\n      return net, end_points\n\ndef main(_):\n\n  #Construct the cluster and start the server\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)  \n\n\n  if FLAGS.job_name == \"ps\":\n    server.join()\n\n  elif FLAGS.job_name == \"worker\":\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index, cluster=cluster)):\n\n      image_size = 224\n      images = tf.Variable(tf.random_normal([FLAGS.batch_size, image_size, image_size, 3], dtype=tf.float32, stddev=1e-1))\n      \n      with slim.arg_scope(alexnet_v2_arg_scope()):\n        logits, end_points = alexnet_v2(images, is_training = False)    \n      saver = tf.train.Saver()\n      summary_op = tf.merge_all_summaries()\n      #summary_op = tf.summary.merge_all()\n      init_op = tf.global_variables_initializer()\n\n    # Create a Supervisor that will checkpoint the model and computes summaries\u3002\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir=\"./alexnet_train_logs\", init_op=init_op, summary_op=summary_op, saver=saver, save_model_secs=600)\n\n    # Get a TensorFlow session managed by the supervisor.\n    with sv.managed_session(server.target) as sess:\n      num_steps_burn_in = 10\n      total_duration = 0.0\n      total_duration_squared = 0.0\n      for i in xrange(FLAGS.num_batches + num_steps_burn_in):\n        start_time = time.time()\n        _ = sess.run(logits)\n        duration = time.time() - start_time\n        if i &gt;= num_steps_burn_in:\n          if not i % 10:\n            print ('%s: step %d, duration = %.3f' % (datetime.now(), i - num_steps_burn_in, duration))\n        total_duration += duration\n        total_duration_squared += duration * duration\n      mn = total_duration / FLAGS.num_batches\n      vr = total_duration_squared / FLAGS.num_batches - mn * mn\n      sd = math.sqrt(vr)\n      print ('%s: across %d steps, %.3f +/- %.3f sec / batch' % (datetime.now(), FLAGS.num_batches, mn, sd))\n         \n    # Stop TensorFlow Session\n    sv.stop()\n\nif __name__ == \"__main__\":\n  tf.app.run()\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux Centos 7.1\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): tensorflow (0.11.0)\n\nDescribe the problem\nWhen I try to run distributed alexnet training in TensorFlow, the alexnet model definition is referred to here. The Error occurred at 'alexnet_v2/pool1/MaxPool' as follows\ntensorflow.python.framework.errors_impl.InvalidArgumentError: tensor_in must be 4-dimensional\n         [[Node: alexnet_v2/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:worker/replica:0/task:0/cpu:0\"](alexnet_v2/conv1/Relu)]]\n\nSource code / logs\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nfrom datetime import datetime\nimport math\nimport sys\nimport time\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\n\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\ntf.app.flags.DEFINE_integer(\"batch_size\", 100, \"Training batch size\")\ntf.app.flags.DEFINE_integer('num_batches', 100, \"Number of batches to run.\")\n\nFLAGS = tf.app.flags.FLAGS\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0, 0, stddev)\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding='SAME'):\n      with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope='alexnet_v2'):\n  with tf.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:\n    end_points_collection = sc.name + '_end_points'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',\n                        scope='conv1')\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool1')\n      net = slim.conv2d(net, 192, [5, 5], scope='conv2')\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool2')\n      net = slim.conv2d(net, 384, [3, 3], scope='conv3')\n      net = slim.conv2d(net, 384, [3, 3], scope='conv4')\n      net = slim.conv2d(net, 256, [3, 3], scope='conv5')\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool5')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding='VALID',\n                          scope='fc6')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope='dropout6')\n        net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope='dropout7')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer,\n                          scope='fc8')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n        end_points[sc.name + '/fc8'] = net\n      return net, end_points\n\ndef main(_):\n\n  #Construct the cluster and start the server\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)  \n\n\n  if FLAGS.job_name == \"ps\":\n    server.join()\n\n  elif FLAGS.job_name == \"worker\":\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index, cluster=cluster)):\n\n      image_size = 224\n      images = tf.Variable(tf.random_normal([FLAGS.batch_size, image_size, image_size, 3], dtype=tf.float32, stddev=1e-1))\n      \n      with slim.arg_scope(alexnet_v2_arg_scope()):\n        logits, end_points = alexnet_v2(images, is_training = False)    \n      saver = tf.train.Saver()\n      summary_op = tf.merge_all_summaries()\n      #summary_op = tf.summary.merge_all()\n      init_op = tf.global_variables_initializer()\n\n    # Create a Supervisor that will checkpoint the model and computes summaries\u3002\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir=\"./alexnet_train_logs\", init_op=init_op, summary_op=summary_op, saver=saver, save_model_secs=600)\n\n    # Get a TensorFlow session managed by the supervisor.\n    with sv.managed_session(server.target) as sess:\n      num_steps_burn_in = 10\n      total_duration = 0.0\n      total_duration_squared = 0.0\n      for i in xrange(FLAGS.num_batches + num_steps_burn_in):\n        start_time = time.time()\n        _ = sess.run(logits)\n        duration = time.time() - start_time\n        if i >= num_steps_burn_in:\n          if not i % 10:\n            print ('%s: step %d, duration = %.3f' % (datetime.now(), i - num_steps_burn_in, duration))\n        total_duration += duration\n        total_duration_squared += duration * duration\n      mn = total_duration / FLAGS.num_batches\n      vr = total_duration_squared / FLAGS.num_batches - mn * mn\n      sd = math.sqrt(vr)\n      print ('%s: across %d steps, %.3f +/- %.3f sec / batch' % (datetime.now(), FLAGS.num_batches, mn, sd))\n         \n    # Stop TensorFlow Session\n    sv.stop()\n\nif __name__ == \"__main__\":\n  tf.app.run()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux Centos 7.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tensorflow (0.11.0)\r\n\r\n\r\n### Describe the problem\r\nWhen I try to run distributed alexnet training in TensorFlow, the alexnet model definition is referred to [here](https://github.com/tensorflow/models/blob/master/slim/nets/alexnet.py). The Error occurred at 'alexnet_v2/pool1/MaxPool' as follows\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: tensor_in must be 4-dimensional\r\n         [[Node: alexnet_v2/pool1/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:worker/replica:0/task:0/cpu:0\"](alexnet_v2/conv1/Relu)]]\r\n```\r\n\r\n### Source code / logs\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nfrom datetime import datetime\r\nimport math\r\nimport sys\r\nimport time\r\n\r\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\r\nimport tensorflow as tf\r\n\r\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n\r\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\ntf.app.flags.DEFINE_integer(\"batch_size\", 100, \"Training batch size\")\r\ntf.app.flags.DEFINE_integer('num_batches', 100, \"Number of batches to run.\")\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\nslim = tf.contrib.slim\r\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0, 0, stddev)\r\n\r\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\r\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                      activation_fn=tf.nn.relu,\r\n                      biases_initializer=tf.constant_initializer(0.1),\r\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\r\n    with slim.arg_scope([slim.conv2d], padding='SAME'):\r\n      with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:\r\n        return arg_sc\r\n\r\n\r\ndef alexnet_v2(inputs,\r\n               num_classes=1000,\r\n               is_training=True,\r\n               dropout_keep_prob=0.5,\r\n               spatial_squeeze=True,\r\n               scope='alexnet_v2'):\r\n  with tf.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:\r\n    end_points_collection = sc.name + '_end_points'\r\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\r\n                        outputs_collections=[end_points_collection]):\r\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',\r\n                        scope='conv1')\r\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool1')\r\n      net = slim.conv2d(net, 192, [5, 5], scope='conv2')\r\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool2')\r\n      net = slim.conv2d(net, 384, [3, 3], scope='conv3')\r\n      net = slim.conv2d(net, 384, [3, 3], scope='conv4')\r\n      net = slim.conv2d(net, 256, [3, 3], scope='conv5')\r\n      net = slim.max_pool2d(net, [3, 3], 2, scope='pool5')\r\n\r\n      # Use conv2d instead of fully_connected layers.\r\n      with slim.arg_scope([slim.conv2d],\r\n                          weights_initializer=trunc_normal(0.005),\r\n                          biases_initializer=tf.constant_initializer(0.1)):\r\n        net = slim.conv2d(net, 4096, [5, 5], padding='VALID',\r\n                          scope='fc6')\r\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\r\n                           scope='dropout6')\r\n        net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\r\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\r\n                           scope='dropout7')\r\n        net = slim.conv2d(net, num_classes, [1, 1],\r\n                          activation_fn=None,\r\n                          normalizer_fn=None,\r\n                          biases_initializer=tf.zeros_initializer,\r\n                          scope='fc8')\r\n\r\n      # Convert end_points_collection into a end_point dict.\r\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\r\n      if spatial_squeeze:\r\n        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\r\n        end_points[sc.name + '/fc8'] = net\r\n      return net, end_points\r\n\r\ndef main(_):\r\n\r\n  #Construct the cluster and start the server\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)  \r\n\r\n\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n\r\n  elif FLAGS.job_name == \"worker\":\r\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index, cluster=cluster)):\r\n\r\n      image_size = 224\r\n      images = tf.Variable(tf.random_normal([FLAGS.batch_size, image_size, image_size, 3], dtype=tf.float32, stddev=1e-1))\r\n      \r\n      with slim.arg_scope(alexnet_v2_arg_scope()):\r\n        logits, end_points = alexnet_v2(images, is_training = False)    \r\n      saver = tf.train.Saver()\r\n      summary_op = tf.merge_all_summaries()\r\n      #summary_op = tf.summary.merge_all()\r\n      init_op = tf.global_variables_initializer()\r\n\r\n    # Create a Supervisor that will checkpoint the model and computes summaries\u3002\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir=\"./alexnet_train_logs\", init_op=init_op, summary_op=summary_op, saver=saver, save_model_secs=600)\r\n\r\n    # Get a TensorFlow session managed by the supervisor.\r\n    with sv.managed_session(server.target) as sess:\r\n      num_steps_burn_in = 10\r\n      total_duration = 0.0\r\n      total_duration_squared = 0.0\r\n      for i in xrange(FLAGS.num_batches + num_steps_burn_in):\r\n        start_time = time.time()\r\n        _ = sess.run(logits)\r\n        duration = time.time() - start_time\r\n        if i >= num_steps_burn_in:\r\n          if not i % 10:\r\n            print ('%s: step %d, duration = %.3f' % (datetime.now(), i - num_steps_burn_in, duration))\r\n        total_duration += duration\r\n        total_duration_squared += duration * duration\r\n      mn = total_duration / FLAGS.num_batches\r\n      vr = total_duration_squared / FLAGS.num_batches - mn * mn\r\n      sd = math.sqrt(vr)\r\n      print ('%s: across %d steps, %.3f +/- %.3f sec / batch' % (datetime.now(), FLAGS.num_batches, mn, sd))\r\n         \r\n    # Stop TensorFlow Session\r\n    sv.stop()\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n```\r\n\r\n"}