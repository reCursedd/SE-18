{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19853", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19853/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19853/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19853/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19853", "id": 330598088, "node_id": "MDU6SXNzdWUzMzA1OTgwODg=", "number": 19853, "title": "There seems to be a wrong network implementation in tensorflow/example/speech_commands/models.py", "user": {"login": "smallbal", "id": 6365754, "node_id": "MDQ6VXNlcjYzNjU3NTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/6365754?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smallbal", "html_url": "https://github.com/smallbal", "followers_url": "https://api.github.com/users/smallbal/followers", "following_url": "https://api.github.com/users/smallbal/following{/other_user}", "gists_url": "https://api.github.com/users/smallbal/gists{/gist_id}", "starred_url": "https://api.github.com/users/smallbal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smallbal/subscriptions", "organizations_url": "https://api.github.com/users/smallbal/orgs", "repos_url": "https://api.github.com/users/smallbal/repos", "events_url": "https://api.github.com/users/smallbal/events{/privacy}", "received_events_url": "https://api.github.com/users/smallbal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-06-08T10:05:40Z", "updated_at": "2018-07-17T20:20:00Z", "closed_at": "2018-07-17T20:20:00Z", "author_association": "NONE", "body_html": "<p>There may be a mistake I found in <code>tensorflow/tensorflow/example/speech_commands/models.py</code>:<br>\nThe function <code>create_conv_model()</code> is to create a \"cnn-trad-fpool3\" network with 2 max pool layers, but the second max pool layer is missing now.  The code can run normally, but I'm not sure if it is  the right \"cnn-trad-fpool3\" network.</p>\n<p>I put part of the code of <code>tensorflow/tensorflow/exmaple/speech_commands/models.py</code> here below and added some comments  to explain my points.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">create_conv_model</span>(<span class=\"pl-smi\">fingerprint_input</span>, <span class=\"pl-smi\">model_settings</span>, <span class=\"pl-smi\">is_training</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Builds a standard convolutional model.</span>\n<span class=\"pl-s\">  This is roughly the network labeled as 'cnn-trad-fpool3' in the</span>\n<span class=\"pl-s\">  'Convolutional Neural Networks for Small-footprint Keyword Spotting' paper:</span>\n<span class=\"pl-s\">  http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf</span>\n<span class=\"pl-s\">  Here's the layout of the graph:</span>\n<span class=\"pl-s\">  (fingerprint_input)</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">      [Conv2D]&lt;-(weights)</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">      [BiasAdd]&lt;-(bias)</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">        [Relu]</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">      [MaxPool]</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">      [Conv2D]&lt;-(weights)</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">      [BiasAdd]&lt;-(bias)</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">        [Relu]</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">      [MaxPool]</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">      [MatMul]&lt;-(weights)</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">      [BiasAdd]&lt;-(bias)</span>\n<span class=\"pl-s\">          v</span>\n<span class=\"pl-s\">  This produces fairly good quality results, but can involve a large number of</span>\n<span class=\"pl-s\">  weight parameters and computations. For a cheaper alternative from the same</span>\n<span class=\"pl-s\">  paper with slightly less accuracy, see 'low_latency_conv' below.</span>\n<span class=\"pl-s\">  During training, dropout nodes are introduced after each relu, controlled by a</span>\n<span class=\"pl-s\">  placeholder.</span>\n<span class=\"pl-s\">  Args:</span>\n<span class=\"pl-s\">    fingerprint_input: TensorFlow node that will output audio feature vectors.</span>\n<span class=\"pl-s\">    model_settings: Dictionary of information about the model.</span>\n<span class=\"pl-s\">    is_training: Whether the model is going to be used for training.</span>\n<span class=\"pl-s\">  Returns:</span>\n<span class=\"pl-s\">    TensorFlow node outputting logits results, and optionally a dropout</span>\n<span class=\"pl-s\">    placeholder.</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-k\">if</span> is_training:\n    dropout_prob <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>dropout_prob<span class=\"pl-pds\">'</span></span>)\n  input_frequency_size <span class=\"pl-k\">=</span> model_settings[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dct_coefficient_count<span class=\"pl-pds\">'</span></span>]\n  input_time_size <span class=\"pl-k\">=</span> model_settings[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>spectrogram_length<span class=\"pl-pds\">'</span></span>]\n  fingerprint_4d <span class=\"pl-k\">=</span> tf.reshape(fingerprint_input,\n                              [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, input_time_size, input_frequency_size, <span class=\"pl-c1\">1</span>])\n  first_filter_width <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\n  first_filter_height <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\n  first_filter_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\n  first_weights <span class=\"pl-k\">=</span> tf.Variable(\n      tf.truncated_normal(\n          [first_filter_height, first_filter_width, <span class=\"pl-c1\">1</span>, first_filter_count],\n          <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>))\n  first_bias <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([first_filter_count]))\n  first_conv <span class=\"pl-k\">=</span> tf.nn.conv2d(fingerprint_4d, first_weights, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>],\n                            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">+</span> first_bias\n  first_relu <span class=\"pl-k\">=</span> tf.nn.relu(first_conv)\n  <span class=\"pl-k\">if</span> is_training:\n    first_dropout <span class=\"pl-k\">=</span> tf.nn.dropout(first_relu, dropout_prob)\n  <span class=\"pl-k\">else</span>:\n    first_dropout <span class=\"pl-k\">=</span> first_relu\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> the first and only max pool operation in the whole network.</span>\n  max_pool <span class=\"pl-k\">=</span> tf.nn.max_pool(first_dropout, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>) \n  second_filter_width <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n  second_filter_height <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n  second_filter_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\n  second_weights <span class=\"pl-k\">=</span> tf.Variable(\n      tf.truncated_normal(\n          [\n              second_filter_height, second_filter_width, first_filter_count,\n              second_filter_count\n          ],\n          <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>))\n  second_bias <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([second_filter_count]))\n  second_conv <span class=\"pl-k\">=</span> tf.nn.conv2d(max_pool, second_weights, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>],\n                             <span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">+</span> second_bias\n  second_relu <span class=\"pl-k\">=</span> tf.nn.relu(second_conv)\n  <span class=\"pl-k\">if</span> is_training:\n    second_dropout <span class=\"pl-k\">=</span> tf.nn.dropout(second_relu, dropout_prob)\n  <span class=\"pl-k\">else</span>:\n    second_dropout <span class=\"pl-k\">=</span> second_relu\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> there could be another one max pool operation as the comments said in the beginning:</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> another_max_pool =tf.nn.max_pool(second_dropout, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> and the variable name \"second_dropout\" below should be changed to \"another_max_pool\".</span>\n  second_conv_shape <span class=\"pl-k\">=</span> second_dropout.get_shape()\n  second_conv_output_width <span class=\"pl-k\">=</span> second_conv_shape[<span class=\"pl-c1\">2</span>]\n  second_conv_output_height <span class=\"pl-k\">=</span> second_conv_shape[<span class=\"pl-c1\">1</span>]\n  second_conv_element_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(\n      second_conv_output_width <span class=\"pl-k\">*</span> second_conv_output_height <span class=\"pl-k\">*</span>\n      second_filter_count)\n  flattened_second_conv <span class=\"pl-k\">=</span> tf.reshape(second_dropout,\n                                     [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, second_conv_element_count])\n  label_count <span class=\"pl-k\">=</span> model_settings[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>label_count<span class=\"pl-pds\">'</span></span>]\n  final_fc_weights <span class=\"pl-k\">=</span> tf.Variable(\n      tf.truncated_normal(\n          [second_conv_element_count, label_count], <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>))\n  final_fc_bias <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([label_count]))\n  final_fc <span class=\"pl-k\">=</span> tf.matmul(flattened_second_conv, final_fc_weights) <span class=\"pl-k\">+</span> final_fc_bias\n  <span class=\"pl-k\">if</span> is_training:\n    <span class=\"pl-k\">return</span> final_fc, dropout_prob\n  <span class=\"pl-k\">else</span>:\n    <span class=\"pl-k\">return</span> final_fc</pre></div>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:  No. I read the source code and run the it on my own computer.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nN/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nN/A</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nv1.9.0-rc0</li>\n<li><strong>Python version</strong>:<br>\nN/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nN/A</li>\n<li><strong>GPU model and memory</strong>:<br>\nN/A</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nN/A</li>\n</ul>", "body_text": "There may be a mistake I found in tensorflow/tensorflow/example/speech_commands/models.py:\nThe function create_conv_model() is to create a \"cnn-trad-fpool3\" network with 2 max pool layers, but the second max pool layer is missing now.  The code can run normally, but I'm not sure if it is  the right \"cnn-trad-fpool3\" network.\nI put part of the code of tensorflow/tensorflow/exmaple/speech_commands/models.py here below and added some comments  to explain my points.\ndef create_conv_model(fingerprint_input, model_settings, is_training):\n  \"\"\"Builds a standard convolutional model.\n  This is roughly the network labeled as 'cnn-trad-fpool3' in the\n  'Convolutional Neural Networks for Small-footprint Keyword Spotting' paper:\n  http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf\n  Here's the layout of the graph:\n  (fingerprint_input)\n          v\n      [Conv2D]<-(weights)\n          v\n      [BiasAdd]<-(bias)\n          v\n        [Relu]\n          v\n      [MaxPool]\n          v\n      [Conv2D]<-(weights)\n          v\n      [BiasAdd]<-(bias)\n          v\n        [Relu]\n          v\n      [MaxPool]\n          v\n      [MatMul]<-(weights)\n          v\n      [BiasAdd]<-(bias)\n          v\n  This produces fairly good quality results, but can involve a large number of\n  weight parameters and computations. For a cheaper alternative from the same\n  paper with slightly less accuracy, see 'low_latency_conv' below.\n  During training, dropout nodes are introduced after each relu, controlled by a\n  placeholder.\n  Args:\n    fingerprint_input: TensorFlow node that will output audio feature vectors.\n    model_settings: Dictionary of information about the model.\n    is_training: Whether the model is going to be used for training.\n  Returns:\n    TensorFlow node outputting logits results, and optionally a dropout\n    placeholder.\n  \"\"\"\n  if is_training:\n    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n  input_frequency_size = model_settings['dct_coefficient_count']\n  input_time_size = model_settings['spectrogram_length']\n  fingerprint_4d = tf.reshape(fingerprint_input,\n                              [-1, input_time_size, input_frequency_size, 1])\n  first_filter_width = 8\n  first_filter_height = 20\n  first_filter_count = 64\n  first_weights = tf.Variable(\n      tf.truncated_normal(\n          [first_filter_height, first_filter_width, 1, first_filter_count],\n          stddev=0.01))\n  first_bias = tf.Variable(tf.zeros([first_filter_count]))\n  first_conv = tf.nn.conv2d(fingerprint_4d, first_weights, [1, 1, 1, 1],\n                            'SAME') + first_bias\n  first_relu = tf.nn.relu(first_conv)\n  if is_training:\n    first_dropout = tf.nn.dropout(first_relu, dropout_prob)\n  else:\n    first_dropout = first_relu\n  # the first and only max pool operation in the whole network.\n  max_pool = tf.nn.max_pool(first_dropout, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME') \n  second_filter_width = 4\n  second_filter_height = 10\n  second_filter_count = 64\n  second_weights = tf.Variable(\n      tf.truncated_normal(\n          [\n              second_filter_height, second_filter_width, first_filter_count,\n              second_filter_count\n          ],\n          stddev=0.01))\n  second_bias = tf.Variable(tf.zeros([second_filter_count]))\n  second_conv = tf.nn.conv2d(max_pool, second_weights, [1, 1, 1, 1],\n                             'SAME') + second_bias\n  second_relu = tf.nn.relu(second_conv)\n  if is_training:\n    second_dropout = tf.nn.dropout(second_relu, dropout_prob)\n  else:\n    second_dropout = second_relu\n  # there could be another one max pool operation as the comments said in the beginning:\n  # another_max_pool =tf.nn.max_pool(second_dropout, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n  # and the variable name \"second_dropout\" below should be changed to \"another_max_pool\".\n  second_conv_shape = second_dropout.get_shape()\n  second_conv_output_width = second_conv_shape[2]\n  second_conv_output_height = second_conv_shape[1]\n  second_conv_element_count = int(\n      second_conv_output_width * second_conv_output_height *\n      second_filter_count)\n  flattened_second_conv = tf.reshape(second_dropout,\n                                     [-1, second_conv_element_count])\n  label_count = model_settings['label_count']\n  final_fc_weights = tf.Variable(\n      tf.truncated_normal(\n          [second_conv_element_count, label_count], stddev=0.01))\n  final_fc_bias = tf.Variable(tf.zeros([label_count]))\n  final_fc = tf.matmul(flattened_second_conv, final_fc_weights) + final_fc_bias\n  if is_training:\n    return final_fc, dropout_prob\n  else:\n    return final_fc\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):  No. I read the source code and run the it on my own computer.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nN/A\nTensorFlow installed from (source or binary):\nN/A\nTensorFlow version (use command below):\nv1.9.0-rc0\nPython version:\nN/A\nBazel version (if compiling from source):\nN/A\nGCC/Compiler version (if compiling from source):\nN/A\nCUDA/cuDNN version:\nN/A\nGPU model and memory:\nN/A\nExact command to reproduce:\nN/A", "body": "There may be a mistake I found in `tensorflow/tensorflow/example/speech_commands/models.py`:\r\nThe function `create_conv_model()` is to create a \"cnn-trad-fpool3\" network with 2 max pool layers, but the second max pool layer is missing now.  The code can run normally, but I'm not sure if it is  the right \"cnn-trad-fpool3\" network.  \r\n\r\nI put part of the code of `tensorflow/tensorflow/exmaple/speech_commands/models.py` here below and added some comments  to explain my points.\r\n```python\r\ndef create_conv_model(fingerprint_input, model_settings, is_training):\r\n  \"\"\"Builds a standard convolutional model.\r\n  This is roughly the network labeled as 'cnn-trad-fpool3' in the\r\n  'Convolutional Neural Networks for Small-footprint Keyword Spotting' paper:\r\n  http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf\r\n  Here's the layout of the graph:\r\n  (fingerprint_input)\r\n          v\r\n      [Conv2D]<-(weights)\r\n          v\r\n      [BiasAdd]<-(bias)\r\n          v\r\n        [Relu]\r\n          v\r\n      [MaxPool]\r\n          v\r\n      [Conv2D]<-(weights)\r\n          v\r\n      [BiasAdd]<-(bias)\r\n          v\r\n        [Relu]\r\n          v\r\n      [MaxPool]\r\n          v\r\n      [MatMul]<-(weights)\r\n          v\r\n      [BiasAdd]<-(bias)\r\n          v\r\n  This produces fairly good quality results, but can involve a large number of\r\n  weight parameters and computations. For a cheaper alternative from the same\r\n  paper with slightly less accuracy, see 'low_latency_conv' below.\r\n  During training, dropout nodes are introduced after each relu, controlled by a\r\n  placeholder.\r\n  Args:\r\n    fingerprint_input: TensorFlow node that will output audio feature vectors.\r\n    model_settings: Dictionary of information about the model.\r\n    is_training: Whether the model is going to be used for training.\r\n  Returns:\r\n    TensorFlow node outputting logits results, and optionally a dropout\r\n    placeholder.\r\n  \"\"\"\r\n  if is_training:\r\n    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\r\n  input_frequency_size = model_settings['dct_coefficient_count']\r\n  input_time_size = model_settings['spectrogram_length']\r\n  fingerprint_4d = tf.reshape(fingerprint_input,\r\n                              [-1, input_time_size, input_frequency_size, 1])\r\n  first_filter_width = 8\r\n  first_filter_height = 20\r\n  first_filter_count = 64\r\n  first_weights = tf.Variable(\r\n      tf.truncated_normal(\r\n          [first_filter_height, first_filter_width, 1, first_filter_count],\r\n          stddev=0.01))\r\n  first_bias = tf.Variable(tf.zeros([first_filter_count]))\r\n  first_conv = tf.nn.conv2d(fingerprint_4d, first_weights, [1, 1, 1, 1],\r\n                            'SAME') + first_bias\r\n  first_relu = tf.nn.relu(first_conv)\r\n  if is_training:\r\n    first_dropout = tf.nn.dropout(first_relu, dropout_prob)\r\n  else:\r\n    first_dropout = first_relu\r\n  # the first and only max pool operation in the whole network.\r\n  max_pool = tf.nn.max_pool(first_dropout, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME') \r\n  second_filter_width = 4\r\n  second_filter_height = 10\r\n  second_filter_count = 64\r\n  second_weights = tf.Variable(\r\n      tf.truncated_normal(\r\n          [\r\n              second_filter_height, second_filter_width, first_filter_count,\r\n              second_filter_count\r\n          ],\r\n          stddev=0.01))\r\n  second_bias = tf.Variable(tf.zeros([second_filter_count]))\r\n  second_conv = tf.nn.conv2d(max_pool, second_weights, [1, 1, 1, 1],\r\n                             'SAME') + second_bias\r\n  second_relu = tf.nn.relu(second_conv)\r\n  if is_training:\r\n    second_dropout = tf.nn.dropout(second_relu, dropout_prob)\r\n  else:\r\n    second_dropout = second_relu\r\n  # there could be another one max pool operation as the comments said in the beginning:\r\n  # another_max_pool =tf.nn.max_pool(second_dropout, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\r\n  # and the variable name \"second_dropout\" below should be changed to \"another_max_pool\".\r\n  second_conv_shape = second_dropout.get_shape()\r\n  second_conv_output_width = second_conv_shape[2]\r\n  second_conv_output_height = second_conv_shape[1]\r\n  second_conv_element_count = int(\r\n      second_conv_output_width * second_conv_output_height *\r\n      second_filter_count)\r\n  flattened_second_conv = tf.reshape(second_dropout,\r\n                                     [-1, second_conv_element_count])\r\n  label_count = model_settings['label_count']\r\n  final_fc_weights = tf.Variable(\r\n      tf.truncated_normal(\r\n          [second_conv_element_count, label_count], stddev=0.01))\r\n  final_fc_bias = tf.Variable(tf.zeros([label_count]))\r\n  final_fc = tf.matmul(flattened_second_conv, final_fc_weights) + final_fc_bias\r\n  if is_training:\r\n    return final_fc, dropout_prob\r\n  else:\r\n    return final_fc\r\n```\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No. I read the source code and run the it on my own computer.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  \r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nN/A\r\n- **TensorFlow version (use command below)**:  \r\nv1.9.0-rc0\r\n- **Python version**: \r\nN/A\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nN/A"}