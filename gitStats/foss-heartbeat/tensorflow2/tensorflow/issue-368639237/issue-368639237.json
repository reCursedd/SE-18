{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22863", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22863/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22863/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22863/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22863", "id": 368639237, "node_id": "MDU6SXNzdWUzNjg2MzkyMzc=", "number": 22863, "title": "Tensorflow C++ 1.9.0 ~1.11.0 failed call to cuInit: CUresult(-1)  but libcuda.so is ok, where is the problem?", "user": {"login": "TsinghuaTop", "id": 8545208, "node_id": "MDQ6VXNlcjg1NDUyMDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/8545208?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TsinghuaTop", "html_url": "https://github.com/TsinghuaTop", "followers_url": "https://api.github.com/users/TsinghuaTop/followers", "following_url": "https://api.github.com/users/TsinghuaTop/following{/other_user}", "gists_url": "https://api.github.com/users/TsinghuaTop/gists{/gist_id}", "starred_url": "https://api.github.com/users/TsinghuaTop/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TsinghuaTop/subscriptions", "organizations_url": "https://api.github.com/users/TsinghuaTop/orgs", "repos_url": "https://api.github.com/users/TsinghuaTop/repos", "events_url": "https://api.github.com/users/TsinghuaTop/events{/privacy}", "received_events_url": "https://api.github.com/users/TsinghuaTop/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173351, "node_id": "MDU6TGFiZWw0NzMxNzMzNTE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:build/install", "name": "type:build/install", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-10-10T12:23:29Z", "updated_at": "2018-10-24T18:27:52Z", "closed_at": "2018-10-15T05:11:32Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</p>\n</li>\n<li>\n<p>**OS Platform and Distribution Linux kernel 3.10.104:</p>\n</li>\n<li>\n<p>**TensorFlow installed from source 1.9.0 build with gpu<br>\nconfigure with open cuda support and build with command as<br>\n\"bazel build -c opt --config=mkl --config=cuda --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 //tensorflow:libtensorflow_cc.so\"</p>\n</li>\n<li>\n<p>**TensorFlow version 1.9.0:</p>\n</li>\n<li>\n<p><strong>Python version</strong>:</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nUsing built-in specs.<br>\nCOLLECT_GCC=gcc<br>\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper<br>\nTarget: x86_64-redhat-linux<br>\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=<a href=\"http://bugzilla.redhat.com/bugzilla\" rel=\"nofollow\">http://bugzilla.redhat.com/bugzilla</a> --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux<br>\nThread model: posix<br>\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)</p>\n</li>\n<li>\n<p>**CUDA/cuDNN version:/usr/local/cuda-9.0/</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nWed Oct 10 20:14:44 2018<br>\n+-----------------------------------------------------------------------------+<br>\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================+======================|<br>\n|   0  Tesla P40           On   | 00000000:04:00.0 Off |                    0 |<br>\n| N/A   51C    P0   136W / 250W |  16817MiB / 22912MiB |    100%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   1  Tesla P40           On   | 00000000:06:00.0 Off |                    0 |<br>\n| N/A   50C    P0   143W / 250W |  16817MiB / 22912MiB |    100%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   2  Tesla P40           On   | 00000000:07:00.0 Off |                    0 |<br>\n| N/A   49C    P0    93W / 250W |  16817MiB / 22912MiB |    100%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   3  Tesla P40           On   | 00000000:08:00.0 Off |                    0 |<br>\n| N/A   50C    P0   138W / 250W |  16817MiB / 22912MiB |     98%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   4  Tesla P40           On   | 00000000:0C:00.0 Off |                    0 |<br>\n| N/A   40C    P0    56W / 250W |   2435MiB / 22912MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   5  Tesla P40           On   | 00000000:0D:00.0 Off |                    0 |<br>\n| N/A   26C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   6  Tesla P40           On   | 00000000:0E:00.0 Off |                    0 |<br>\n| N/A   25C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   7  Tesla P40           On   | 00000000:0F:00.0 Off |                    0 |<br>\n| N/A   28C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n</li>\n</ul>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory |<br>\n|  GPU       PID   Type   Process name                             Usage      |<br>\n|=============================================================================|</p>\n<p>my problem is  that when i use c++ api  to inference\uff0cbut get error as follows:<br>\n2018-10-10 20:19:47.332240: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1)<br>\n2018-10-10 20:19:47.332371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: 100-88-66-85<br>\n2018-10-10 20:19:47.332402: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: 100-88-66-85<br>\n2018-10-10 20:19:47.332545: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 384.81.0<br>\n2018-10-10 20:19:47.332661: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.81.0<br>\n2018-10-10 20:19:47.332691: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 384.81.0<br>\n2018-10-10 20:19:47.349695: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.</p>\n<p>the c++ code as follows:</p>\n<p>int main(int argc, char const <em>argv[]) {<br>\nconst char</em> devices = getenv(\"CUDA_VISIBLE_DEVICES\");<br>\nif(devices!=NULL)<br>\nLOG_INFO(\"devices:%s\", devices);<br>\nstd::string model_path = \"wavenet.pb\";<br>\ntensorflow::GraphDef graph;<br>\ntensorflow::Status load_graph_status = ReadBinaryProto(tensorflow::Env::Default(), model_path.c_str(), &amp;graph);<br>\nif (!load_graph_status.ok()) {<br>\nLOG_ERROR(\"Failed to load pb graph:%s\", load_graph_status.error_message().c_str());<br>\nreturn load_graph_status.code();<br>\n}<br>\n// config<br>\ntensorflow::SessionOptions options;<br>\nint num_threads_1 = 4;<br>\nint num_threads_2 = 4;<br>\nif (num_threads_1 &gt; 0) {<br>\noptions.config.set_intra_op_parallelism_threads(num_threads_1);<br>\noptions.config.set_inter_op_parallelism_threads(num_threads_2);<br>\noptions.config.mutable_gpu_options()-&gt;set_visible_device_list(\"0\");<br>\n}<br>\n// create session<br>\nstd::unique_ptrtensorflow::Session session;<br>\nsession.reset(tensorflow::NewSession(options));<br>\ntensorflow::Status session_create_status = session-&gt;Create(graph);<br>\nif (!session_create_status.ok()) {<br>\nLOG_ERROR(\"Failed to create tensorflow graph: %s\", session_create_status.error_message().c_str());<br>\nreturn load_graph_status.code();<br>\n}</p>\n<pre><code>const std::string input_tensor_name = \"Predict/mel:0\";\nconst std::string output_tensor_name = \"Predict/wav:0\";\n\nint32_t batch_size = 1;\n//int32_t frame_nums = (int)(24000 / 256);\nint32_t frame_nums = 1;\nint32_t dims = 80;\nstd::vector&lt;float&gt; spectrum;\nstd::vector&lt;float&gt; wave;\nspectrum.resize(frame_nums, 0);\n\nLOG_INFO(\"fill data to input tensor begin\");\ntensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({batch_size, frame_nums, dims}));\nauto input_tensor_matrix = input_tensor.tensor&lt;float, 3&gt;();\nfor (int32_t x = 0; x &lt; batch_size; ++x) {\n    for (int y = 0; y &lt; frame_nums; ++y) {\n        for (int z = 0; z &lt; dims; ++z) {\n            int32_t index = x * frame_nums * dims + y * frame_nums + z;\n            input_tensor_matrix(x, y, z) = spectrum[index];\n        }\n    }\n}\nLOG_INFO(\"fill data to input tensor end\");\nstd::vector&lt;std::pair&lt;std::string, tensorflow::Tensor&gt;&gt; inputs;\nstd::vector&lt;tensorflow::Tensor&gt; outputs;\ninputs.push_back(std::pair&lt;std::string, tensorflow::Tensor&gt;(input_tensor_name, input_tensor));\nLOG_INFO(\"session run begin\");\ntensorflow::Status run_status = session-&gt;Run(inputs, {output_tensor_name}, {}, &amp;outputs);\nLOG_INFO(\"session run end\");\nif (!run_status.ok()) {\n    LOG_ERROR(\"Running model failed: %s\", run_status.error_message().c_str());\n    return load_graph_status.code();\n}\nauto output_c = outputs[0].tensor&lt;float, 3&gt;();\nint32_t length = outputs[0].dim_size(2);\nwave.resize(batch_size * 1 * length);\nfor (int32_t x = 0; x &lt; batch_size; ++x) {\n    for (int y = 0; y &lt; 1; ++y) {\n        for (int z = 0; z &lt; length; ++z) {\n            int32_t index = x * 1 * length + y * 1 + z;\n            wave[index] = output_c(x, y, z);\n        }\n    }\n}\nsession-&gt;Close();\nreturn 0;\n</code></pre>\n<p>}</p>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\n\n**OS Platform and Distribution Linux kernel 3.10.104:\n\n\n**TensorFlow installed from source 1.9.0 build with gpu\nconfigure with open cuda support and build with command as\n\"bazel build -c opt --config=mkl --config=cuda --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 //tensorflow:libtensorflow_cc.so\"\n\n\n**TensorFlow version 1.9.0:\n\n\nPython version:\n\n\nBazel version (if compiling from source):\n\n\nGCC/Compiler version (if compiling from source):\nUsing built-in specs.\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper\nTarget: x86_64-redhat-linux\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux\nThread model: posix\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)\n\n\n**CUDA/cuDNN version:/usr/local/cuda-9.0/\n\n\nGPU model and memory:\nWed Oct 10 20:14:44 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:04:00.0 Off |                    0 |\n| N/A   51C    P0   136W / 250W |  16817MiB / 22912MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla P40           On   | 00000000:06:00.0 Off |                    0 |\n| N/A   50C    P0   143W / 250W |  16817MiB / 22912MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla P40           On   | 00000000:07:00.0 Off |                    0 |\n| N/A   49C    P0    93W / 250W |  16817MiB / 22912MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla P40           On   | 00000000:08:00.0 Off |                    0 |\n| N/A   50C    P0   138W / 250W |  16817MiB / 22912MiB |     98%      Default |\n+-------------------------------+----------------------+----------------------+\n|   4  Tesla P40           On   | 00000000:0C:00.0 Off |                    0 |\n| N/A   40C    P0    56W / 250W |   2435MiB / 22912MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   5  Tesla P40           On   | 00000000:0D:00.0 Off |                    0 |\n| N/A   26C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   6  Tesla P40           On   | 00000000:0E:00.0 Off |                    0 |\n| N/A   25C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   7  Tesla P40           On   | 00000000:0F:00.0 Off |                    0 |\n| N/A   28C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\nmy problem is  that when i use c++ api  to inference\uff0cbut get error as follows:\n2018-10-10 20:19:47.332240: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1)\n2018-10-10 20:19:47.332371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: 100-88-66-85\n2018-10-10 20:19:47.332402: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: 100-88-66-85\n2018-10-10 20:19:47.332545: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 384.81.0\n2018-10-10 20:19:47.332661: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.81.0\n2018-10-10 20:19:47.332691: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 384.81.0\n2018-10-10 20:19:47.349695: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nthe c++ code as follows:\nint main(int argc, char const argv[]) {\nconst char devices = getenv(\"CUDA_VISIBLE_DEVICES\");\nif(devices!=NULL)\nLOG_INFO(\"devices:%s\", devices);\nstd::string model_path = \"wavenet.pb\";\ntensorflow::GraphDef graph;\ntensorflow::Status load_graph_status = ReadBinaryProto(tensorflow::Env::Default(), model_path.c_str(), &graph);\nif (!load_graph_status.ok()) {\nLOG_ERROR(\"Failed to load pb graph:%s\", load_graph_status.error_message().c_str());\nreturn load_graph_status.code();\n}\n// config\ntensorflow::SessionOptions options;\nint num_threads_1 = 4;\nint num_threads_2 = 4;\nif (num_threads_1 > 0) {\noptions.config.set_intra_op_parallelism_threads(num_threads_1);\noptions.config.set_inter_op_parallelism_threads(num_threads_2);\noptions.config.mutable_gpu_options()->set_visible_device_list(\"0\");\n}\n// create session\nstd::unique_ptrtensorflow::Session session;\nsession.reset(tensorflow::NewSession(options));\ntensorflow::Status session_create_status = session->Create(graph);\nif (!session_create_status.ok()) {\nLOG_ERROR(\"Failed to create tensorflow graph: %s\", session_create_status.error_message().c_str());\nreturn load_graph_status.code();\n}\nconst std::string input_tensor_name = \"Predict/mel:0\";\nconst std::string output_tensor_name = \"Predict/wav:0\";\n\nint32_t batch_size = 1;\n//int32_t frame_nums = (int)(24000 / 256);\nint32_t frame_nums = 1;\nint32_t dims = 80;\nstd::vector<float> spectrum;\nstd::vector<float> wave;\nspectrum.resize(frame_nums, 0);\n\nLOG_INFO(\"fill data to input tensor begin\");\ntensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({batch_size, frame_nums, dims}));\nauto input_tensor_matrix = input_tensor.tensor<float, 3>();\nfor (int32_t x = 0; x < batch_size; ++x) {\n    for (int y = 0; y < frame_nums; ++y) {\n        for (int z = 0; z < dims; ++z) {\n            int32_t index = x * frame_nums * dims + y * frame_nums + z;\n            input_tensor_matrix(x, y, z) = spectrum[index];\n        }\n    }\n}\nLOG_INFO(\"fill data to input tensor end\");\nstd::vector<std::pair<std::string, tensorflow::Tensor>> inputs;\nstd::vector<tensorflow::Tensor> outputs;\ninputs.push_back(std::pair<std::string, tensorflow::Tensor>(input_tensor_name, input_tensor));\nLOG_INFO(\"session run begin\");\ntensorflow::Status run_status = session->Run(inputs, {output_tensor_name}, {}, &outputs);\nLOG_INFO(\"session run end\");\nif (!run_status.ok()) {\n    LOG_ERROR(\"Running model failed: %s\", run_status.error_message().c_str());\n    return load_graph_status.code();\n}\nauto output_c = outputs[0].tensor<float, 3>();\nint32_t length = outputs[0].dim_size(2);\nwave.resize(batch_size * 1 * length);\nfor (int32_t x = 0; x < batch_size; ++x) {\n    for (int y = 0; y < 1; ++y) {\n        for (int z = 0; z < length; ++z) {\n            int32_t index = x * 1 * length + y * 1 + z;\n            wave[index] = output_c(x, y, z);\n        }\n    }\n}\nsession->Close();\nreturn 0;\n\n}", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution Linux kernel 3.10.104:\r\n- **TensorFlow installed from source 1.9.0 build with gpu\r\n       configure with open cuda support and build with command as \r\n      \"bazel build -c opt --config=mkl --config=cuda --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 //tensorflow:libtensorflow_cc.so\"\r\n- **TensorFlow version 1.9.0:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux\r\nThread model: posix\r\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n\r\n- **CUDA/cuDNN version:/usr/local/cuda-9.0/\r\n- **GPU model and memory**:\r\nWed Oct 10 20:14:44 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P40           On   | 00000000:04:00.0 Off |                    0 |\r\n| N/A   51C    P0   136W / 250W |  16817MiB / 22912MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P40           On   | 00000000:06:00.0 Off |                    0 |\r\n| N/A   50C    P0   143W / 250W |  16817MiB / 22912MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla P40           On   | 00000000:07:00.0 Off |                    0 |\r\n| N/A   49C    P0    93W / 250W |  16817MiB / 22912MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla P40           On   | 00000000:08:00.0 Off |                    0 |\r\n| N/A   50C    P0   138W / 250W |  16817MiB / 22912MiB |     98%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla P40           On   | 00000000:0C:00.0 Off |                    0 |\r\n| N/A   40C    P0    56W / 250W |   2435MiB / 22912MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla P40           On   | 00000000:0D:00.0 Off |                    0 |\r\n| N/A   26C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla P40           On   | 00000000:0E:00.0 Off |                    0 |\r\n| N/A   25C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla P40           On   | 00000000:0F:00.0 Off |                    0 |\r\n| N/A   28C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n\r\nmy problem is  that when i use c++ api  to inference\uff0cbut get error as follows:\r\n2018-10-10 20:19:47.332240: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1)\r\n2018-10-10 20:19:47.332371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: 100-88-66-85\r\n2018-10-10 20:19:47.332402: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: 100-88-66-85\r\n2018-10-10 20:19:47.332545: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 384.81.0\r\n2018-10-10 20:19:47.332661: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.81.0\r\n2018-10-10 20:19:47.332691: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 384.81.0\r\n2018-10-10 20:19:47.349695: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n\r\nthe c++ code as follows:\r\n\r\nint main(int argc, char const *argv[]) {\r\n    const char* devices = getenv(\"CUDA_VISIBLE_DEVICES\");\r\n    if(devices!=NULL)\r\n        LOG_INFO(\"devices:%s\", devices);\r\n    std::string model_path = \"wavenet.pb\";\r\n    tensorflow::GraphDef graph;\r\n    tensorflow::Status load_graph_status = ReadBinaryProto(tensorflow::Env::Default(), model_path.c_str(), &graph);\r\n    if (!load_graph_status.ok()) {\r\n        LOG_ERROR(\"Failed to load pb graph:%s\", load_graph_status.error_message().c_str());\r\n        return load_graph_status.code();\r\n    }\r\n    // config\r\n    tensorflow::SessionOptions options;\r\n    int num_threads_1 = 4;\r\n    int num_threads_2 = 4;\r\n    if (num_threads_1 > 0) {\r\n        options.config.set_intra_op_parallelism_threads(num_threads_1);\r\n        options.config.set_inter_op_parallelism_threads(num_threads_2);\r\n\toptions.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\n    }\r\n    // create session\r\n    std::unique_ptr<tensorflow::Session> session;\r\n    session.reset(tensorflow::NewSession(options));\r\n    tensorflow::Status session_create_status = session->Create(graph);\r\n    if (!session_create_status.ok()) {\r\n        LOG_ERROR(\"Failed to create tensorflow graph: %s\", session_create_status.error_message().c_str());\r\n        return load_graph_status.code();\r\n    }\r\n\r\n    const std::string input_tensor_name = \"Predict/mel:0\";\r\n    const std::string output_tensor_name = \"Predict/wav:0\";\r\n\r\n    int32_t batch_size = 1;\r\n    //int32_t frame_nums = (int)(24000 / 256);\r\n    int32_t frame_nums = 1;\r\n    int32_t dims = 80;\r\n    std::vector<float> spectrum;\r\n    std::vector<float> wave;\r\n    spectrum.resize(frame_nums, 0);\r\n\r\n    LOG_INFO(\"fill data to input tensor begin\");\r\n    tensorflow::Tensor input_tensor(tensorflow::DT_FLOAT, tensorflow::TensorShape({batch_size, frame_nums, dims}));\r\n    auto input_tensor_matrix = input_tensor.tensor<float, 3>();\r\n    for (int32_t x = 0; x < batch_size; ++x) {\r\n        for (int y = 0; y < frame_nums; ++y) {\r\n            for (int z = 0; z < dims; ++z) {\r\n                int32_t index = x * frame_nums * dims + y * frame_nums + z;\r\n                input_tensor_matrix(x, y, z) = spectrum[index];\r\n            }\r\n        }\r\n    }\r\n    LOG_INFO(\"fill data to input tensor end\");\r\n    std::vector<std::pair<std::string, tensorflow::Tensor>> inputs;\r\n    std::vector<tensorflow::Tensor> outputs;\r\n    inputs.push_back(std::pair<std::string, tensorflow::Tensor>(input_tensor_name, input_tensor));\r\n    LOG_INFO(\"session run begin\");\r\n    tensorflow::Status run_status = session->Run(inputs, {output_tensor_name}, {}, &outputs);\r\n    LOG_INFO(\"session run end\");\r\n    if (!run_status.ok()) {\r\n        LOG_ERROR(\"Running model failed: %s\", run_status.error_message().c_str());\r\n        return load_graph_status.code();\r\n    }\r\n    auto output_c = outputs[0].tensor<float, 3>();\r\n    int32_t length = outputs[0].dim_size(2);\r\n    wave.resize(batch_size * 1 * length);\r\n    for (int32_t x = 0; x < batch_size; ++x) {\r\n        for (int y = 0; y < 1; ++y) {\r\n            for (int z = 0; z < length; ++z) {\r\n                int32_t index = x * 1 * length + y * 1 + z;\r\n                wave[index] = output_c(x, y, z);\r\n            }\r\n        }\r\n    }\r\n    session->Close();\r\n    return 0;\r\n}\r\n\r\n\r\n\r\n"}