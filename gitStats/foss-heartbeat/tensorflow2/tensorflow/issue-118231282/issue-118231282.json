{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/320", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/320/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/320/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/320/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/320", "id": 118231282, "node_id": "MDU6SXNzdWUxMTgyMzEyODI=", "number": 320, "title": "__ldg intrinsic unavailable on compute capability <3.2", "user": {"login": "levskaya", "id": 501805, "node_id": "MDQ6VXNlcjUwMTgwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/501805?v=4", "gravatar_id": "", "url": "https://api.github.com/users/levskaya", "html_url": "https://github.com/levskaya", "followers_url": "https://api.github.com/users/levskaya/followers", "following_url": "https://api.github.com/users/levskaya/following{/other_user}", "gists_url": "https://api.github.com/users/levskaya/gists{/gist_id}", "starred_url": "https://api.github.com/users/levskaya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/levskaya/subscriptions", "organizations_url": "https://api.github.com/users/levskaya/orgs", "repos_url": "https://api.github.com/users/levskaya/repos", "events_url": "https://api.github.com/users/levskaya/events{/privacy}", "received_events_url": "https://api.github.com/users/levskaya/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 284463744, "node_id": "MDU6TGFiZWwyODQ0NjM3NDQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cuda", "name": "cuda", "color": "f7c6c7", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2015-11-22T00:59:06Z", "updated_at": "2015-12-23T19:26:38Z", "closed_at": "2015-12-23T19:26:38Z", "author_association": "NONE", "body_html": "<p>I'm not sure if supporting compute 3.0 capability will ever be a priority for the project, it's mainly useful for people who want to train small convnets on AWS g2.2x and g2.8x boxes as they're learning the framework.</p>\n<p>A very recent commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529\"><tt>9c3043f</tt></a> added the read-only data cache load function intrinsic __ldg to two operation defs: conv_ops_gpu_3.cu.cc and bias_op_gpu.cu.cc</p>\n<p>All that's needed to maintain 3.0 support is to pass through __ldg(x) as a pointer --&gt;  *x (I've confirmed this compiles and runs fine on the AWS GPU instances)  One could do this properly in theory with e.g. a generic wrapper:  (this is way too simplistic I'm sure...)</p>\n<pre><code>template&lt;typename T&gt;\n__device__ __forceinline__ T ldg(const T* ptr) {\n#if __CUDA_ARCH__ &gt;= 320\n    return __ldg(ptr);\n#else\n    return *ptr;\n#endif\n}\n</code></pre>\n<p>Again, it's mainly an issue if you want to keep the unsupported TF_UNOFFICIAL_SETTING=1 option working for people who happen to be working with slightly older GPUs.  (BTW thanks for all the amazing work!)</p>", "body_text": "I'm not sure if supporting compute 3.0 capability will ever be a priority for the project, it's mainly useful for people who want to train small convnets on AWS g2.2x and g2.8x boxes as they're learning the framework.\nA very recent commit 9c3043f added the read-only data cache load function intrinsic __ldg to two operation defs: conv_ops_gpu_3.cu.cc and bias_op_gpu.cu.cc\nAll that's needed to maintain 3.0 support is to pass through __ldg(x) as a pointer -->  *x (I've confirmed this compiles and runs fine on the AWS GPU instances)  One could do this properly in theory with e.g. a generic wrapper:  (this is way too simplistic I'm sure...)\ntemplate<typename T>\n__device__ __forceinline__ T ldg(const T* ptr) {\n#if __CUDA_ARCH__ >= 320\n    return __ldg(ptr);\n#else\n    return *ptr;\n#endif\n}\n\nAgain, it's mainly an issue if you want to keep the unsupported TF_UNOFFICIAL_SETTING=1 option working for people who happen to be working with slightly older GPUs.  (BTW thanks for all the amazing work!)", "body": "I'm not sure if supporting compute 3.0 capability will ever be a priority for the project, it's mainly useful for people who want to train small convnets on AWS g2.2x and g2.8x boxes as they're learning the framework.\n\nA very recent commit 9c3043ff added the read-only data cache load function intrinsic __ldg to two operation defs: conv_ops_gpu_3.cu.cc and bias_op_gpu.cu.cc\n\nAll that's needed to maintain 3.0 support is to pass through __ldg(x) as a pointer -->  *x (I've confirmed this compiles and runs fine on the AWS GPU instances)  One could do this properly in theory with e.g. a generic wrapper:  (this is way too simplistic I'm sure...)\n\n```\ntemplate<typename T>\n__device__ __forceinline__ T ldg(const T* ptr) {\n#if __CUDA_ARCH__ >= 320\n    return __ldg(ptr);\n#else\n    return *ptr;\n#endif\n}\n```\n\nAgain, it's mainly an issue if you want to keep the unsupported TF_UNOFFICIAL_SETTING=1 option working for people who happen to be working with slightly older GPUs.  (BTW thanks for all the amazing work!)\n"}