{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296829550", "html_url": "https://github.com/tensorflow/tensorflow/issues/9387#issuecomment-296829550", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9387", "id": 296829550, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjgyOTU1MA==", "user": {"login": "juntingzh", "id": 12653506, "node_id": "MDQ6VXNlcjEyNjUzNTA2", "avatar_url": "https://avatars2.githubusercontent.com/u/12653506?v=4", "gravatar_id": "", "url": "https://api.github.com/users/juntingzh", "html_url": "https://github.com/juntingzh", "followers_url": "https://api.github.com/users/juntingzh/followers", "following_url": "https://api.github.com/users/juntingzh/following{/other_user}", "gists_url": "https://api.github.com/users/juntingzh/gists{/gist_id}", "starred_url": "https://api.github.com/users/juntingzh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/juntingzh/subscriptions", "organizations_url": "https://api.github.com/users/juntingzh/orgs", "repos_url": "https://api.github.com/users/juntingzh/repos", "events_url": "https://api.github.com/users/juntingzh/events{/privacy}", "received_events_url": "https://api.github.com/users/juntingzh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-24T21:37:17Z", "updated_at": "2017-04-24T21:37:17Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1766524\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sguada\">@sguada</a> Thank you for the great explanation. Another doubt about the ResNet implementation I have is that in the tf slim resnet_v1_50, resnet_v1_101 and resnet_v1_152 etc. implementations, downsampling is performed by block<strong>1</strong>, block<strong>2</strong>, and block<strong>3</strong>; while in the original ResNet paper, downsampling is performed by conv3_1(block<strong>2</strong> in your notation), conv4_1(block<strong>3</strong>), and conv5_1(block<strong>4</strong>) with a stride of 2. With the skip connections between the blocks involved, the equivalence of two implementations is not clear to me, could you please explain more? Thanks!</p>", "body_text": "@sguada Thank you for the great explanation. Another doubt about the ResNet implementation I have is that in the tf slim resnet_v1_50, resnet_v1_101 and resnet_v1_152 etc. implementations, downsampling is performed by block1, block2, and block3; while in the original ResNet paper, downsampling is performed by conv3_1(block2 in your notation), conv4_1(block3), and conv5_1(block4) with a stride of 2. With the skip connections between the blocks involved, the equivalence of two implementations is not clear to me, could you please explain more? Thanks!", "body": "@sguada Thank you for the great explanation. Another doubt about the ResNet implementation I have is that in the tf slim resnet_v1_50, resnet_v1_101 and resnet_v1_152 etc. implementations, downsampling is performed by block**1**, block**2**, and block**3**; while in the original ResNet paper, downsampling is performed by conv3_1(block**2** in your notation), conv4_1(block**3**), and conv5_1(block**4**) with a stride of 2. With the skip connections between the blocks involved, the equivalence of two implementations is not clear to me, could you please explain more? Thanks!"}