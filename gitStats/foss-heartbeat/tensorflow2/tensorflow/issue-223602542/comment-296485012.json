{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296485012", "html_url": "https://github.com/tensorflow/tensorflow/issues/9387#issuecomment-296485012", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9387", "id": 296485012, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjQ4NTAxMg==", "user": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-23T20:08:20Z", "updated_at": "2017-04-23T20:17:06Z", "author_association": "MEMBER", "body_html": "<p>Both implementation are equivalent, but the current one is more efficient.</p>\n<p>Applying<br>\n<code>conv([3, 3], stride=1, scope='conv_3x3a') followed by conv([1,1], stride=2, scope='conv_1x1a')</code><br>\nis the same as<br>\n<code>conv([3, 3], stride=2, scope='conv_3x3b') followed by conv([1,1], stride=1, scope='conv_1x1b')</code></p>\n<p>However, the first approach the conv_3x3 computes a lot of activations that the conv_1x1 is going to skip.</p>\n<p>Let's assume that the output of 'conv_3x3a' is <code>[a, x, b, x, c]</code> then 'conv_1x1a' is going to skip all the 'x' due to stride=2 and going to produce <code>[a2, b2, c2]</code>. So all the memory and computation used to generate the 'x's is wasted.</p>\n<p>While the output of 'conv_3x3b' is <code>[a, b, c]</code> due to stride=2, then 'conv_1x1a' is going to still produce produce <code>[a2, b2, c2]</code></p>", "body_text": "Both implementation are equivalent, but the current one is more efficient.\nApplying\nconv([3, 3], stride=1, scope='conv_3x3a') followed by conv([1,1], stride=2, scope='conv_1x1a')\nis the same as\nconv([3, 3], stride=2, scope='conv_3x3b') followed by conv([1,1], stride=1, scope='conv_1x1b')\nHowever, the first approach the conv_3x3 computes a lot of activations that the conv_1x1 is going to skip.\nLet's assume that the output of 'conv_3x3a' is [a, x, b, x, c] then 'conv_1x1a' is going to skip all the 'x' due to stride=2 and going to produce [a2, b2, c2]. So all the memory and computation used to generate the 'x's is wasted.\nWhile the output of 'conv_3x3b' is [a, b, c] due to stride=2, then 'conv_1x1a' is going to still produce produce [a2, b2, c2]", "body": "Both implementation are equivalent, but the current one is more efficient.\r\n\r\nApplying \r\n`conv([3, 3], stride=1, scope='conv_3x3a') followed by conv([1,1], stride=2, scope='conv_1x1a')`\r\nis the same as\r\n`conv([3, 3], stride=2, scope='conv_3x3b') followed by conv([1,1], stride=1, scope='conv_1x1b')`\r\n\r\nHowever, the first approach the conv_3x3 computes a lot of activations that the conv_1x1 is going to skip.\r\n\r\nLet's assume that the output of 'conv_3x3a' is `[a, x, b, x, c]` then 'conv_1x1a' is going to skip all the 'x' due to stride=2 and going to produce `[a2, b2, c2]`. So all the memory and computation used to generate the 'x's is wasted.\r\n\r\nWhile the output of 'conv_3x3b' is `[a, b, c]` due to stride=2, then 'conv_1x1a' is going to still produce produce `[a2, b2, c2]`\r\n"}