{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14940", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14940/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14940/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14940/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14940", "id": 277389831, "node_id": "MDU6SXNzdWUyNzczODk4MzE=", "number": 14940, "title": "The keys in end_points of slim are not unified for different layers.", "user": {"login": "gauss-clb", "id": 11674304, "node_id": "MDQ6VXNlcjExNjc0MzA0", "avatar_url": "https://avatars2.githubusercontent.com/u/11674304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gauss-clb", "html_url": "https://github.com/gauss-clb", "followers_url": "https://api.github.com/users/gauss-clb/followers", "following_url": "https://api.github.com/users/gauss-clb/following{/other_user}", "gists_url": "https://api.github.com/users/gauss-clb/gists{/gist_id}", "starred_url": "https://api.github.com/users/gauss-clb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gauss-clb/subscriptions", "organizations_url": "https://api.github.com/users/gauss-clb/orgs", "repos_url": "https://api.github.com/users/gauss-clb/repos", "events_url": "https://api.github.com/users/gauss-clb/events{/privacy}", "received_events_url": "https://api.github.com/users/gauss-clb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2017-11-28T13:45:35Z", "updated_at": "2018-01-24T09:04:05Z", "closed_at": "2018-01-24T09:04:05Z", "author_association": "NONE", "body_html": "<p>Look at the code:</p>\n<div class=\"highlight highlight-source-python\"><pre>  <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>xx<span class=\"pl-pds\">'</span></span>):\n    end_points_collection <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dd<span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-k\">with</span> slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                      <span class=\"pl-v\">outputs_collections</span><span class=\"pl-k\">=</span>end_points_collection):\n      y <span class=\"pl-k\">=</span> slim.conv2d(np.zeros([<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">3</span>],<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32), <span class=\"pl-c1\">10</span>, [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>])\n      x <span class=\"pl-k\">=</span> slim.max_pool2d(np.zeros([<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">20</span>,<span class=\"pl-c1\">3</span>],<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32), [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>])\n      end_points <span class=\"pl-k\">=</span> slim.utils.convert_collection_to_dict(end_points_collection)\n  <span class=\"pl-c1\">print</span>(end_points)</pre></div>\n<p>It output</p>\n<pre><code>OrderedDict([('Conv', &lt;tf.Tensor 'xx/Conv/Relu:0' shape=(1, 20, 20, 10) dtype=fl\noat32&gt;), ('xx/MaxPool2D', &lt;tf.Tensor 'xx/MaxPool2D/MaxPool:0' shape=(1, 10, 10,\n3) dtype=float32&gt;)])\n</code></pre>\n<p>For <code>max_pool2d</code> layer, the key has prefix <code>xx</code>, but for <code>conv2d</code> layer, it don't have prefix <code>xx</code>. Because in <code>conv2d</code> it uses <code>variable_scope</code> but in <code>max_pool2d</code> it uses <code>name_scope</code>. So the behavior looks inconsistent, and may cause the code make mistake. Do we need to unify the behavior ?</p>\n<p>For example, if we use multi-gpu to train the network, and we have many clones of network, which name_scope's prefix  are <code>clone_1</code>,  <code>clone_2</code> and so on. If we want to use the key of <code>end_points</code> to get the output of layer on different gpu. We should deal with <code>max_pool2d</code> and <code>conv2d</code>  differently.</p>", "body_text": "Look at the code:\n  with tf.name_scope('xx'):\n    end_points_collection = 'dd'\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                      outputs_collections=end_points_collection):\n      y = slim.conv2d(np.zeros([1,20,20,3],dtype=np.float32), 10, [2, 2])\n      x = slim.max_pool2d(np.zeros([1,20,20,3],dtype=np.float32), [2, 2])\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n  print(end_points)\nIt output\nOrderedDict([('Conv', <tf.Tensor 'xx/Conv/Relu:0' shape=(1, 20, 20, 10) dtype=fl\noat32>), ('xx/MaxPool2D', <tf.Tensor 'xx/MaxPool2D/MaxPool:0' shape=(1, 10, 10,\n3) dtype=float32>)])\n\nFor max_pool2d layer, the key has prefix xx, but for conv2d layer, it don't have prefix xx. Because in conv2d it uses variable_scope but in max_pool2d it uses name_scope. So the behavior looks inconsistent, and may cause the code make mistake. Do we need to unify the behavior ?\nFor example, if we use multi-gpu to train the network, and we have many clones of network, which name_scope's prefix  are clone_1,  clone_2 and so on. If we want to use the key of end_points to get the output of layer on different gpu. We should deal with max_pool2d and conv2d  differently.", "body": "Look at the code:\r\n\r\n```python\r\n  with tf.name_scope('xx'):\r\n    end_points_collection = 'dd'\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\r\n                      outputs_collections=end_points_collection):\r\n      y = slim.conv2d(np.zeros([1,20,20,3],dtype=np.float32), 10, [2, 2])\r\n      x = slim.max_pool2d(np.zeros([1,20,20,3],dtype=np.float32), [2, 2])\r\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\r\n  print(end_points)\r\n```\r\n\r\nIt output\r\n```\r\nOrderedDict([('Conv', <tf.Tensor 'xx/Conv/Relu:0' shape=(1, 20, 20, 10) dtype=fl\r\noat32>), ('xx/MaxPool2D', <tf.Tensor 'xx/MaxPool2D/MaxPool:0' shape=(1, 10, 10,\r\n3) dtype=float32>)])\r\n```\r\n\r\nFor `max_pool2d` layer, the key has prefix `xx`, but for `conv2d` layer, it don't have prefix `xx`. Because in `conv2d` it uses `variable_scope` but in `max_pool2d` it uses `name_scope`. So the behavior looks inconsistent, and may cause the code make mistake. Do we need to unify the behavior ?\r\n\r\n\r\nFor example, if we use multi-gpu to train the network, and we have many clones of network, which name_scope's prefix  are `clone_1`,  `clone_2` and so on. If we want to use the key of `end_points` to get the output of layer on different gpu. We should deal with `max_pool2d` and `conv2d`  differently."}