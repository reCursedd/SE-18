{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/413081269", "html_url": "https://github.com/tensorflow/tensorflow/issues/21594#issuecomment-413081269", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21594", "id": 413081269, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzA4MTI2OQ==", "user": {"login": "sabhiram", "id": 2693144, "node_id": "MDQ6VXNlcjI2OTMxNDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2693144?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sabhiram", "html_url": "https://github.com/sabhiram", "followers_url": "https://api.github.com/users/sabhiram/followers", "following_url": "https://api.github.com/users/sabhiram/following{/other_user}", "gists_url": "https://api.github.com/users/sabhiram/gists{/gist_id}", "starred_url": "https://api.github.com/users/sabhiram/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sabhiram/subscriptions", "organizations_url": "https://api.github.com/users/sabhiram/orgs", "repos_url": "https://api.github.com/users/sabhiram/repos", "events_url": "https://api.github.com/users/sabhiram/events{/privacy}", "received_events_url": "https://api.github.com/users/sabhiram/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-15T03:10:37Z", "updated_at": "2018-08-15T03:10:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here is a maybe patch:</p>\n<pre><code>diff --git a/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py b/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\nindex 1c9d179e3c..d1d191691a 100644\n--- a/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\n+++ b/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\n@@ -185,6 +185,9 @@ class _BaseAttentionMechanism(AttentionMechanism):\n         `memory_sequence_length` is not None.\n       name: Name to use when creating ops.\n     \"\"\"\n+    if (query_layer is None and memory_layer is None):\n+      raise TypeError(\n+        \"either \\\"query_layer\\\" or \\\"memory_layer\\\" should be specified\")\n     if (query_layer is not None\n         and not isinstance(query_layer, layers_base.Layer)):\n       raise TypeError(\n@@ -195,13 +198,12 @@ class _BaseAttentionMechanism(AttentionMechanism):\n           \"memory_layer is not a Layer: %s\" % type(memory_layer).__name__)\n     self._query_layer = query_layer\n     self._memory_layer = memory_layer\n-    self.dtype = memory_layer.dtype\n+    self.dtype = memory_layer.dtype if memory_layer else query_layer.dtype\n     if not callable(probability_fn):\n       raise TypeError(\"probability_fn must be callable, saw type: %s\" %\n                       type(probability_fn).__name__)\n     if score_mask_value is None:\n-      score_mask_value = dtypes.as_dtype(\n-          self._memory_layer.dtype).as_numpy_dtype(-np.inf)\n+      score_mask_value = dtypes.as_dtype(self.dtype).as_numpy_dtype(-np.inf)\n     self._probability_fn = lambda score, prev: (  # pylint:disable=g-long-lambda\n         probability_fn(\n             _maybe_mask_score(score, memory_sequence_length, score_mask_value),\n</code></pre>", "body_text": "Here is a maybe patch:\ndiff --git a/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py b/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\nindex 1c9d179e3c..d1d191691a 100644\n--- a/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\n+++ b/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\n@@ -185,6 +185,9 @@ class _BaseAttentionMechanism(AttentionMechanism):\n         `memory_sequence_length` is not None.\n       name: Name to use when creating ops.\n     \"\"\"\n+    if (query_layer is None and memory_layer is None):\n+      raise TypeError(\n+        \"either \\\"query_layer\\\" or \\\"memory_layer\\\" should be specified\")\n     if (query_layer is not None\n         and not isinstance(query_layer, layers_base.Layer)):\n       raise TypeError(\n@@ -195,13 +198,12 @@ class _BaseAttentionMechanism(AttentionMechanism):\n           \"memory_layer is not a Layer: %s\" % type(memory_layer).__name__)\n     self._query_layer = query_layer\n     self._memory_layer = memory_layer\n-    self.dtype = memory_layer.dtype\n+    self.dtype = memory_layer.dtype if memory_layer else query_layer.dtype\n     if not callable(probability_fn):\n       raise TypeError(\"probability_fn must be callable, saw type: %s\" %\n                       type(probability_fn).__name__)\n     if score_mask_value is None:\n-      score_mask_value = dtypes.as_dtype(\n-          self._memory_layer.dtype).as_numpy_dtype(-np.inf)\n+      score_mask_value = dtypes.as_dtype(self.dtype).as_numpy_dtype(-np.inf)\n     self._probability_fn = lambda score, prev: (  # pylint:disable=g-long-lambda\n         probability_fn(\n             _maybe_mask_score(score, memory_sequence_length, score_mask_value),", "body": "Here is a maybe patch:\r\n```\r\ndiff --git a/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py b/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\r\nindex 1c9d179e3c..d1d191691a 100644\r\n--- a/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\r\n+++ b/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\r\n@@ -185,6 +185,9 @@ class _BaseAttentionMechanism(AttentionMechanism):\r\n         `memory_sequence_length` is not None.\r\n       name: Name to use when creating ops.\r\n     \"\"\"\r\n+    if (query_layer is None and memory_layer is None):\r\n+      raise TypeError(\r\n+        \"either \\\"query_layer\\\" or \\\"memory_layer\\\" should be specified\")\r\n     if (query_layer is not None\r\n         and not isinstance(query_layer, layers_base.Layer)):\r\n       raise TypeError(\r\n@@ -195,13 +198,12 @@ class _BaseAttentionMechanism(AttentionMechanism):\r\n           \"memory_layer is not a Layer: %s\" % type(memory_layer).__name__)\r\n     self._query_layer = query_layer\r\n     self._memory_layer = memory_layer\r\n-    self.dtype = memory_layer.dtype\r\n+    self.dtype = memory_layer.dtype if memory_layer else query_layer.dtype\r\n     if not callable(probability_fn):\r\n       raise TypeError(\"probability_fn must be callable, saw type: %s\" %\r\n                       type(probability_fn).__name__)\r\n     if score_mask_value is None:\r\n-      score_mask_value = dtypes.as_dtype(\r\n-          self._memory_layer.dtype).as_numpy_dtype(-np.inf)\r\n+      score_mask_value = dtypes.as_dtype(self.dtype).as_numpy_dtype(-np.inf)\r\n     self._probability_fn = lambda score, prev: (  # pylint:disable=g-long-lambda\r\n         probability_fn(\r\n             _maybe_mask_score(score, memory_sequence_length, score_mask_value),\r\n```"}