{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3813", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3813/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3813/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3813/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3813", "id": 171115563, "node_id": "MDU6SXNzdWUxNzExMTU1NjM=", "number": 3813, "title": "wide_n_deep model deal with large dataset got ValueError: GraphDef cannot be larger than 2GB.", "user": {"login": "guozhizou", "id": 7837436, "node_id": "MDQ6VXNlcjc4Mzc0MzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7837436?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guozhizou", "html_url": "https://github.com/guozhizou", "followers_url": "https://api.github.com/users/guozhizou/followers", "following_url": "https://api.github.com/users/guozhizou/following{/other_user}", "gists_url": "https://api.github.com/users/guozhizou/gists{/gist_id}", "starred_url": "https://api.github.com/users/guozhizou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guozhizou/subscriptions", "organizations_url": "https://api.github.com/users/guozhizou/orgs", "repos_url": "https://api.github.com/users/guozhizou/repos", "events_url": "https://api.github.com/users/guozhizou/events{/privacy}", "received_events_url": "https://api.github.com/users/guozhizou/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2016-08-15T06:29:30Z", "updated_at": "2017-01-01T15:43:08Z", "closed_at": "2016-08-28T05:56:19Z", "author_association": "NONE", "body_html": "<p>wide_n_deep seems like unable to fit large dataset, when I put 1 million data, I got:</p>\n<pre><code>Traceback (most recent call last):\n  File \"search_click.py\", line 207, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"search_click.py\", line 204, in main\n    train_and_eval()\n  File \"search_click.py\", line 181, in train_and_eval\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 182, in fit\n    monitors=monitors)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 458, in _train_model\n    summary_writer=graph_actions.get_summary_writer(self._model_dir))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 76, in get_summary_writer\n    graph=ops.get_default_graph())\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py\", line 113, in __init__\n    self.add_graph(graph=graph, graph_def=graph_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py\", line 204, in add_graph\n    true_graph_def = graph.as_graph_def(add_shapes=True)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2117, in as_graph_def\n    raise ValueError(\"GraphDef cannot be larger than 2GB.\")\nValueError: GraphDef cannot be larger than 2GB.\n</code></pre>\n<pre><code>def input_fn(df):\n  \"\"\"Input builder function.\"\"\"\n  # Creates a dictionary mapping from each continuous feature column name (k) to\n  # the values of that column stored in a constant Tensor.\n  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n  # Creates a dictionary mapping from each categorical feature column name (k)\n  # to the values of that column stored in a tf.SparseTensor.\n  categorical_cols = {k: tf.SparseTensor(\n      indices=[[i, 0] for i in range(df[k].size)],\n      values=df[k].values,\n      shape=[df[k].size, 1])\n                      for k in CATEGORICAL_COLUMNS}\n  # Merges the two dictionaries into one.\n  feature_cols = dict(continuous_cols)\n  feature_cols.update(categorical_cols)\n  # Converts the label column into a constant Tensor.\n  label = tf.constant(df[LABEL_COLUMN].values)\n  # Returns the feature columns and the label.\n  return feature_cols, label\n</code></pre>\n<p>Is there a replacement of tf.constant and tf.SparseTensor, so we can load a batch once a time?<br>\nAny help would be appreciated!</p>", "body_text": "wide_n_deep seems like unable to fit large dataset, when I put 1 million data, I got:\nTraceback (most recent call last):\n  File \"search_click.py\", line 207, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"search_click.py\", line 204, in main\n    train_and_eval()\n  File \"search_click.py\", line 181, in train_and_eval\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 182, in fit\n    monitors=monitors)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 458, in _train_model\n    summary_writer=graph_actions.get_summary_writer(self._model_dir))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 76, in get_summary_writer\n    graph=ops.get_default_graph())\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py\", line 113, in __init__\n    self.add_graph(graph=graph, graph_def=graph_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py\", line 204, in add_graph\n    true_graph_def = graph.as_graph_def(add_shapes=True)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2117, in as_graph_def\n    raise ValueError(\"GraphDef cannot be larger than 2GB.\")\nValueError: GraphDef cannot be larger than 2GB.\n\ndef input_fn(df):\n  \"\"\"Input builder function.\"\"\"\n  # Creates a dictionary mapping from each continuous feature column name (k) to\n  # the values of that column stored in a constant Tensor.\n  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n  # Creates a dictionary mapping from each categorical feature column name (k)\n  # to the values of that column stored in a tf.SparseTensor.\n  categorical_cols = {k: tf.SparseTensor(\n      indices=[[i, 0] for i in range(df[k].size)],\n      values=df[k].values,\n      shape=[df[k].size, 1])\n                      for k in CATEGORICAL_COLUMNS}\n  # Merges the two dictionaries into one.\n  feature_cols = dict(continuous_cols)\n  feature_cols.update(categorical_cols)\n  # Converts the label column into a constant Tensor.\n  label = tf.constant(df[LABEL_COLUMN].values)\n  # Returns the feature columns and the label.\n  return feature_cols, label\n\nIs there a replacement of tf.constant and tf.SparseTensor, so we can load a batch once a time?\nAny help would be appreciated!", "body": "wide_n_deep seems like unable to fit large dataset, when I put 1 million data, I got:\n\n```\nTraceback (most recent call last):\n  File \"search_click.py\", line 207, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"search_click.py\", line 204, in main\n    train_and_eval()\n  File \"search_click.py\", line 181, in train_and_eval\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 182, in fit\n    monitors=monitors)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 458, in _train_model\n    summary_writer=graph_actions.get_summary_writer(self._model_dir))\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 76, in get_summary_writer\n    graph=ops.get_default_graph())\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py\", line 113, in __init__\n    self.add_graph(graph=graph, graph_def=graph_def)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/summary_io.py\", line 204, in add_graph\n    true_graph_def = graph.as_graph_def(add_shapes=True)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2117, in as_graph_def\n    raise ValueError(\"GraphDef cannot be larger than 2GB.\")\nValueError: GraphDef cannot be larger than 2GB.\n```\n\n```\ndef input_fn(df):\n  \"\"\"Input builder function.\"\"\"\n  # Creates a dictionary mapping from each continuous feature column name (k) to\n  # the values of that column stored in a constant Tensor.\n  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n  # Creates a dictionary mapping from each categorical feature column name (k)\n  # to the values of that column stored in a tf.SparseTensor.\n  categorical_cols = {k: tf.SparseTensor(\n      indices=[[i, 0] for i in range(df[k].size)],\n      values=df[k].values,\n      shape=[df[k].size, 1])\n                      for k in CATEGORICAL_COLUMNS}\n  # Merges the two dictionaries into one.\n  feature_cols = dict(continuous_cols)\n  feature_cols.update(categorical_cols)\n  # Converts the label column into a constant Tensor.\n  label = tf.constant(df[LABEL_COLUMN].values)\n  # Returns the feature columns and the label.\n  return feature_cols, label\n```\n\nIs there a replacement of tf.constant and tf.SparseTensor, so we can load a batch once a time?\nAny help would be appreciated!\n"}