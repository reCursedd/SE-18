{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16072", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16072/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16072/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16072/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16072", "id": 288168842, "node_id": "MDU6SXNzdWUyODgxNjg4NDI=", "number": 16072, "title": "Dynamic Bi-directional RNN vs Dynamic RNN.- Not working as expected.", "user": {"login": "hkhatod", "id": 23573076, "node_id": "MDQ6VXNlcjIzNTczMDc2", "avatar_url": "https://avatars0.githubusercontent.com/u/23573076?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hkhatod", "html_url": "https://github.com/hkhatod", "followers_url": "https://api.github.com/users/hkhatod/followers", "following_url": "https://api.github.com/users/hkhatod/following{/other_user}", "gists_url": "https://api.github.com/users/hkhatod/gists{/gist_id}", "starred_url": "https://api.github.com/users/hkhatod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hkhatod/subscriptions", "organizations_url": "https://api.github.com/users/hkhatod/orgs", "repos_url": "https://api.github.com/users/hkhatod/repos", "events_url": "https://api.github.com/users/hkhatod/events{/privacy}", "received_events_url": "https://api.github.com/users/hkhatod/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-01-12T16:12:57Z", "updated_at": "2018-02-06T12:55:04Z", "closed_at": "2018-02-06T12:55:04Z", "author_association": "NONE", "body_html": "<p>I am trying to use Bidirectional RNN and pass the output through a CNN for text classification. However, I am getting all sorts of shape errors with bidirectional RNN. Although, If I use two dynamic rnn with reverse op in the second layer, it appears to work fine:</p>\n<p>Here is bidirectional RNN code that DOES NOT work for me:</p>\n<pre><code>    # Bidirectional LSTM layer\n    with tf.name_scope(\"bidirectional-lstm\"):\n        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\n        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\n\n        self.lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n            lstm_fw_cell, \n            lstm_bw_cell, \n            self.embedded_chars, \n            sequence_length=self.seqlen, \n            dtype=tf.float32)\n        self.lstm_outputs = tf.concat(self.lstm_outputs, axis=2)\n</code></pre>\n<p>Here is the two layer dynamic rnn that DOES work for me:</p>\n<pre><code>  # Bidirectional LSTM layer\n    with tf.name_scope(\"bidirectional-lstm\"):\n        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\n        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\n    with tf.variable_scope(\"lstm-output-fw\"):\n        self.lstm_outputs_fw, _ = tf.nn.dynamic_rnn(\n            lstm_fw_cell, \n            self.embedded_chars, \n            sequence_length=self.seqlen, \n            dtype=tf.float32)\n\n    with tf.variable_scope(\"lstm-output-bw\"):\n        self.embedded_chars_rev = array_ops.reverse_sequence(self.embedded_chars, seq_lengths=self.seqlen, seq_dim=1)\n        tmp, _ = tf.nn.dynamic_rnn(\n            lstm_bw_cell, \n            self.embedded_chars_rev, \n            sequence_length=self.seqlen, \n            dtype=tf.float32)\n        self.lstm_outputs_bw = array_ops.reverse_sequence(tmp, seq_lengths=self.seqlen, seq_dim=1)\n\n    Concatenate outputs\n    self.lstm_outputs = tf.add(self.lstm_outputs_fw, self.lstm_outputs_bw, name=\"lstm_outputs\")\n</code></pre>\n<p>I am passing the output of this to CNN and error occurs when computing the</p>\n<p>Here is the rest of the code:</p>\n<h1>Convolution + maxpool layer for each filter size</h1>\n<pre><code>    pooled_outputs = []\n    for i, filter_size in enumerate(filter_sizes):\n        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n            # Convolution Layer\n            filter_shape = [filter_size, hidden_size, 1, num_filters]\n            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n\n            conv = tf.nn.conv2d(\n                self.lstm_outputs_expanded, \n                W,\n                strides=[1, 1, 1, 1], \n                padding=\"VALID\",\n                name=\"conv\")\n\n            # Apply nonlinearity\n            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n\n            # Maxpooling over the outputs\n            pooled = tf.nn.max_pool(\n                h, \n                ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                strides=[1, 1, 1, 1], \n                padding='VALID',\n                name=\"pool\")\n            pooled_outputs.append(pooled)\n\n    # Combine all the pooled features\n    num_filters_total = num_filters * len(filter_sizes)\n    self.h_pool = tf.concat(axis=3, values=pooled_outputs)\n    self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n\n\n    # Dropout layer\n    with tf.name_scope(\"dropout\"):\n        self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n</code></pre>\n<pre><code>        # Final (unnormalized) scores and predictions\n        with tf.name_scope(\"output\"):\n            # Standard output weights initialization\n            W = tf.get_variable(\n                \"W\", \n                shape=[num_filters_total, num_classes], \n                initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n\n            # # Initialized output weights to 0.0, might improve accuracy\n            # W = tf.Variable(tf.constant(0.0, shape=[num_filters_total, num_classes]), name=\"W\")\n            # b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\n\n            l2_loss += tf.nn.l2_loss(W)\n            l2_loss += tf.nn.l2_loss(b)\n\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n\n            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n\n        # Calculate mean cross-entropy loss\n        with tf.name_scope(\"loss\"):\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n        # Accuracy\n        with tf.name_scope(\"accuracy\"):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n</code></pre>\n<p>here are the errors I am getting.</p>\n<pre><code>During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"train_upgraded.py\", line 209, in &lt;module&gt;\n    train_step(x_batch, seqlen_batch, y_batch)\n  File \"train_upgraded.py\", line 177, in train_step\n    feed_dict)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\n\nCaused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:\n  File \"train_upgraded.py\", line 87, in &lt;module&gt;\n    l2_reg_lambda=FLAGS.l2_reg_lambda)\n  File \"/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py\", line 138, in __init__\n    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\n    name=name)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"train_upgraded.py\", line 209, in &lt;module&gt;\n    train_step(x_batch, seqlen_batch, y_batch)\n  File \"train_upgraded.py\", line 177, in train_step\n    feed_dict)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\n\nCaused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:\n  File \"train_upgraded.py\", line 87, in &lt;module&gt;\n    l2_reg_lambda=FLAGS.l2_reg_lambda)\n  File \"/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py\", line 138, in __init__\n    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\n    name=name)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\n</code></pre>", "body_text": "I am trying to use Bidirectional RNN and pass the output through a CNN for text classification. However, I am getting all sorts of shape errors with bidirectional RNN. Although, If I use two dynamic rnn with reverse op in the second layer, it appears to work fine:\nHere is bidirectional RNN code that DOES NOT work for me:\n    # Bidirectional LSTM layer\n    with tf.name_scope(\"bidirectional-lstm\"):\n        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\n        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\n\n        self.lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n            lstm_fw_cell, \n            lstm_bw_cell, \n            self.embedded_chars, \n            sequence_length=self.seqlen, \n            dtype=tf.float32)\n        self.lstm_outputs = tf.concat(self.lstm_outputs, axis=2)\n\nHere is the two layer dynamic rnn that DOES work for me:\n  # Bidirectional LSTM layer\n    with tf.name_scope(\"bidirectional-lstm\"):\n        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\n        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\n    with tf.variable_scope(\"lstm-output-fw\"):\n        self.lstm_outputs_fw, _ = tf.nn.dynamic_rnn(\n            lstm_fw_cell, \n            self.embedded_chars, \n            sequence_length=self.seqlen, \n            dtype=tf.float32)\n\n    with tf.variable_scope(\"lstm-output-bw\"):\n        self.embedded_chars_rev = array_ops.reverse_sequence(self.embedded_chars, seq_lengths=self.seqlen, seq_dim=1)\n        tmp, _ = tf.nn.dynamic_rnn(\n            lstm_bw_cell, \n            self.embedded_chars_rev, \n            sequence_length=self.seqlen, \n            dtype=tf.float32)\n        self.lstm_outputs_bw = array_ops.reverse_sequence(tmp, seq_lengths=self.seqlen, seq_dim=1)\n\n    Concatenate outputs\n    self.lstm_outputs = tf.add(self.lstm_outputs_fw, self.lstm_outputs_bw, name=\"lstm_outputs\")\n\nI am passing the output of this to CNN and error occurs when computing the\nHere is the rest of the code:\nConvolution + maxpool layer for each filter size\n    pooled_outputs = []\n    for i, filter_size in enumerate(filter_sizes):\n        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n            # Convolution Layer\n            filter_shape = [filter_size, hidden_size, 1, num_filters]\n            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n\n            conv = tf.nn.conv2d(\n                self.lstm_outputs_expanded, \n                W,\n                strides=[1, 1, 1, 1], \n                padding=\"VALID\",\n                name=\"conv\")\n\n            # Apply nonlinearity\n            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n\n            # Maxpooling over the outputs\n            pooled = tf.nn.max_pool(\n                h, \n                ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                strides=[1, 1, 1, 1], \n                padding='VALID',\n                name=\"pool\")\n            pooled_outputs.append(pooled)\n\n    # Combine all the pooled features\n    num_filters_total = num_filters * len(filter_sizes)\n    self.h_pool = tf.concat(axis=3, values=pooled_outputs)\n    self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n\n\n    # Dropout layer\n    with tf.name_scope(\"dropout\"):\n        self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n        # Final (unnormalized) scores and predictions\n        with tf.name_scope(\"output\"):\n            # Standard output weights initialization\n            W = tf.get_variable(\n                \"W\", \n                shape=[num_filters_total, num_classes], \n                initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n\n            # # Initialized output weights to 0.0, might improve accuracy\n            # W = tf.Variable(tf.constant(0.0, shape=[num_filters_total, num_classes]), name=\"W\")\n            # b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\n\n            l2_loss += tf.nn.l2_loss(W)\n            l2_loss += tf.nn.l2_loss(b)\n\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n\n            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n\n        # Calculate mean cross-entropy loss\n        with tf.name_scope(\"loss\"):\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n        # Accuracy\n        with tf.name_scope(\"accuracy\"):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n\nhere are the errors I am getting.\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"train_upgraded.py\", line 209, in <module>\n    train_step(x_batch, seqlen_batch, y_batch)\n  File \"train_upgraded.py\", line 177, in train_step\n    feed_dict)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\n\nCaused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:\n  File \"train_upgraded.py\", line 87, in <module>\n    l2_reg_lambda=FLAGS.l2_reg_lambda)\n  File \"/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py\", line 138, in __init__\n    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\n    name=name)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"train_upgraded.py\", line 209, in <module>\n    train_step(x_batch, seqlen_batch, y_batch)\n  File \"train_upgraded.py\", line 177, in train_step\n    feed_dict)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\n\nCaused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:\n  File \"train_upgraded.py\", line 87, in <module>\n    l2_reg_lambda=FLAGS.l2_reg_lambda)\n  File \"/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py\", line 138, in __init__\n    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\n    name=name)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]", "body": "I am trying to use Bidirectional RNN and pass the output through a CNN for text classification. However, I am getting all sorts of shape errors with bidirectional RNN. Although, If I use two dynamic rnn with reverse op in the second layer, it appears to work fine:\r\n\r\nHere is bidirectional RNN code that DOES NOT work for me:\r\n\r\n```\r\n    # Bidirectional LSTM layer\r\n    with tf.name_scope(\"bidirectional-lstm\"):\r\n        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\r\n        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\r\n\r\n        self.lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\r\n            lstm_fw_cell, \r\n            lstm_bw_cell, \r\n            self.embedded_chars, \r\n            sequence_length=self.seqlen, \r\n            dtype=tf.float32)\r\n        self.lstm_outputs = tf.concat(self.lstm_outputs, axis=2)\r\n```\r\n\r\n\r\nHere is the two layer dynamic rnn that DOES work for me:\r\n\r\n\r\n```\r\n  # Bidirectional LSTM layer\r\n    with tf.name_scope(\"bidirectional-lstm\"):\r\n        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\r\n        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\r\n    with tf.variable_scope(\"lstm-output-fw\"):\r\n        self.lstm_outputs_fw, _ = tf.nn.dynamic_rnn(\r\n            lstm_fw_cell, \r\n            self.embedded_chars, \r\n            sequence_length=self.seqlen, \r\n            dtype=tf.float32)\r\n\r\n    with tf.variable_scope(\"lstm-output-bw\"):\r\n        self.embedded_chars_rev = array_ops.reverse_sequence(self.embedded_chars, seq_lengths=self.seqlen, seq_dim=1)\r\n        tmp, _ = tf.nn.dynamic_rnn(\r\n            lstm_bw_cell, \r\n            self.embedded_chars_rev, \r\n            sequence_length=self.seqlen, \r\n            dtype=tf.float32)\r\n        self.lstm_outputs_bw = array_ops.reverse_sequence(tmp, seq_lengths=self.seqlen, seq_dim=1)\r\n\r\n    Concatenate outputs\r\n    self.lstm_outputs = tf.add(self.lstm_outputs_fw, self.lstm_outputs_bw, name=\"lstm_outputs\")\r\n```\r\n\r\nI am passing the output of this to CNN and error occurs when computing the\r\n\r\nHere is the rest of the code:\r\n\r\n# Convolution + maxpool layer for each filter size\r\n        pooled_outputs = []\r\n        for i, filter_size in enumerate(filter_sizes):\r\n            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\r\n                # Convolution Layer\r\n                filter_shape = [filter_size, hidden_size, 1, num_filters]\r\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\r\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\r\n\r\n                conv = tf.nn.conv2d(\r\n                    self.lstm_outputs_expanded, \r\n                    W,\r\n                    strides=[1, 1, 1, 1], \r\n                    padding=\"VALID\",\r\n                    name=\"conv\")\r\n\r\n                # Apply nonlinearity\r\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\r\n\r\n                # Maxpooling over the outputs\r\n                pooled = tf.nn.max_pool(\r\n                    h, \r\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\r\n                    strides=[1, 1, 1, 1], \r\n                    padding='VALID',\r\n                    name=\"pool\")\r\n                pooled_outputs.append(pooled)\r\n\r\n        # Combine all the pooled features\r\n        num_filters_total = num_filters * len(filter_sizes)\r\n        self.h_pool = tf.concat(axis=3, values=pooled_outputs)\r\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\r\n\r\n\r\n        # Dropout layer\r\n        with tf.name_scope(\"dropout\"):\r\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\r\n\r\n\r\n```\r\n        # Final (unnormalized) scores and predictions\r\n        with tf.name_scope(\"output\"):\r\n            # Standard output weights initialization\r\n            W = tf.get_variable(\r\n                \"W\", \r\n                shape=[num_filters_total, num_classes], \r\n                initializer=tf.contrib.layers.xavier_initializer())\r\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\r\n\r\n            # # Initialized output weights to 0.0, might improve accuracy\r\n            # W = tf.Variable(tf.constant(0.0, shape=[num_filters_total, num_classes]), name=\"W\")\r\n            # b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\r\n\r\n            l2_loss += tf.nn.l2_loss(W)\r\n            l2_loss += tf.nn.l2_loss(b)\r\n\r\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\r\n\r\n            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\r\n\r\n        # Calculate mean cross-entropy loss\r\n        with tf.name_scope(\"loss\"):\r\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\r\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\r\n\r\n        # Accuracy\r\n        with tf.name_scope(\"accuracy\"):\r\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\r\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\r\n```\r\n\r\n\r\nhere are the errors I am getting.\r\n\r\n```\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_upgraded.py\", line 209, in <module>\r\n    train_step(x_batch, seqlen_batch, y_batch)\r\n  File \"train_upgraded.py\", line 177, in train_step\r\n    feed_dict)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\r\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\r\n\r\nCaused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:\r\n  File \"train_upgraded.py\", line 87, in <module>\r\n    l2_reg_lambda=FLAGS.l2_reg_lambda)\r\n  File \"/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py\", line 138, in __init__\r\n    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\r\n    name=name)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\r\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_upgraded.py\", line 209, in <module>\r\n    train_step(x_batch, seqlen_batch, y_batch)\r\n  File \"train_upgraded.py\", line 177, in train_step\r\n    feed_dict)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\r\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\r\n\r\nCaused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:\r\n  File \"train_upgraded.py\", line 87, in <module>\r\n    l2_reg_lambda=FLAGS.l2_reg_lambda)\r\n  File \"/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py\", line 138, in __init__\r\n    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\r\n    name=name)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\r\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\r\n```"}