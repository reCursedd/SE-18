{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20844", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20844/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20844/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20844/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20844", "id": 341580501, "node_id": "MDU6SXNzdWUzNDE1ODA1MDE=", "number": 20844, "title": "MonitoredTrainingSession is not handling the dummy_QueueRunner cleanly?", "user": {"login": "tsoi2", "id": 13420915, "node_id": "MDQ6VXNlcjEzNDIwOTE1", "avatar_url": "https://avatars0.githubusercontent.com/u/13420915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tsoi2", "html_url": "https://github.com/tsoi2", "followers_url": "https://api.github.com/users/tsoi2/followers", "following_url": "https://api.github.com/users/tsoi2/following{/other_user}", "gists_url": "https://api.github.com/users/tsoi2/gists{/gist_id}", "starred_url": "https://api.github.com/users/tsoi2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tsoi2/subscriptions", "organizations_url": "https://api.github.com/users/tsoi2/orgs", "repos_url": "https://api.github.com/users/tsoi2/repos", "events_url": "https://api.github.com/users/tsoi2/events{/privacy}", "received_events_url": "https://api.github.com/users/tsoi2/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-07-16T16:02:51Z", "updated_at": "2018-11-20T07:51:20Z", "closed_at": "2018-10-26T21:22:07Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  Linux Fedora Red Hat Enterprise Linux Server release 7.5 (Maipo)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.7.0</li>\n<li><strong>Python version</strong>: 3.6.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA9.0/ CUDNN7.0</li>\n<li><strong>GPU model and memory</strong>:  P100 16GB</li>\n<li><strong>Exact command to reproduce</strong>:  bash run.sh</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am using the MonitoredTrainingSession to test a simple script (base on <a href=\"https://github.com/tmulc18/Distributed-TensorFlow-Guide/blob/master/Synchronous-SGD/ssgd.py\">this</a> of synchronized distributed training/multi_gpu training. When the training is finished, the session always throws out Exception (CancelledError) in thread QueueRunnerThread.  I suspect this is related to QueueRunner is not handled cleanly when sess.close() is called.</p>\n<h3>Source code / logs</h3>\n<p>`REPLICAS_TO_AGGREGATE = 1<br>\ndef main():<br>\n# Configure<br>\nconfig=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)<br>\nconfig.gpu_options.allow_growth = True</p>\n<pre><code># Server Setup\ncluster = tf.train.ClusterSpec({\n    'ps':['localhost:2222'],\n    'worker':['localhost:2223']\n    }) #allows this node know about all other nodes\nif FLAGS.job_name == 'ps': #checks if parameter server\n    with tf.device('/job:ps/task:0/cpu:0'):\n        server = tf.train.Server(cluster,\n                        job_name=\"ps\",\n                        task_index=FLAGS.task_index,\n                        config=config)\n        server.join()\nelse: #it must be a worker server\n    is_chief = (FLAGS.task_index == 0) #checks if this is the chief node\n    with tf.device('/gpu:%d' % (FLAGS.task_index)):\n        server = tf.train.Server(cluster,\n                job_name=\"worker\",\n                task_index=FLAGS.task_index,\n                config=config)\n\n    # Graph\n    # ps_device = \"/job:ps/task:0/cpu:0\"\n    worker_device = \"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, FLAGS.task_index)\n    with tf.device(tf.train.replica_device_setter(ps_tasks=1,\n                   #ps_device=ps_device,\n                   worker_device=worker_device)):\n\n        a = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)\n        b = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)\n        c= a+b\n\n        global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n        target = tf.constant(100.,shape=[2],dtype=tf.float32)\n        loss = tf.reduce_mean(tf.square(c-target))\n\n        # create an optimizer then wrap it with SynceReplicasOptimizer\n        optimizer = tf.train.GradientDescentOptimizer(.0001)\n        optimizer1 = tf.train.SyncReplicasOptimizer(optimizer,\n                 replicas_to_aggregate=REPLICAS_TO_AGGREGATE, total_num_replicas=REPLICAS_TO_AGGREGATE)\n  \n        opt = optimizer1.minimize(loss,global_step=global_step) # averages gradients\n        #opt = optimizer1.minimize(REPLICAS_TO_AGGREGATE*loss,\n        #                           global_step=global_step) # hackily sums gradients\n\n    # Session\n    sync_replicas_hook = optimizer1.make_session_run_hook(is_chief)\n    stop_hook = tf.train.StopAtStepHook(last_step=10)\n    hooks = [sync_replicas_hook,stop_hook]\n\n    # Monitored Training Session\n    with tf.train.MonitoredTrainingSession(master = server.target, \n          is_chief=is_chief,\n          config=config,\n          hooks=hooks,\n          stop_grace_period_secs=10) as sess:\n\n        print('Starting training on worker %d'%FLAGS.task_index)\n        while not sess.should_stop():\n            _,r,gs=sess.run([opt,c,global_step])\n            print(r,'step: ',gs,'worker: ',FLAGS.task_index)\n            # if is_chief: time.sleep(1)\n            time.sleep(1)\n        print('Done',FLAGS.task_index)\n\n        time.sleep(10) #grace period to wait before closing session\n        #sess.close() # if uncomment this, it will raise error 'Session is already closed' after the CanceledError \n    print('Session from worker %d closed cleanly'%FLAGS.task_index)\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\nparser = argparse.ArgumentParser()<br>\n# Flags for defining the tf.train.ClusterSpec<br>\nparser.add_argument(<br>\n\"--job_name\",<br>\ntype=str,<br>\ndefault=\"\",<br>\nhelp=\"One of 'ps', 'worker'\"<br>\n)<br>\n# Flags for defining the tf.train.Server<br>\nparser.add_argument(<br>\n\"--task_index\",<br>\ntype=int,<br>\ndefault=0,<br>\nhelp=\"Index of task within the job\"<br>\n)<br>\nFLAGS, unparsed = parser.parse_known_args()<br>\nprint(FLAGS.task_index)<br>\nmain()`</p>\n<p>**************** I use the following script to run the code<br>\n<code>#!/bin/bash -e export CUDA_VISIBLE_DEVICES=''  python text_multi_gpu.py --job_name=\"ps\" --task_index=0 &amp; export CUDA_VISIBLE_DEVICES='0'  python text_multi_gpu.py --job_name=\"worker\" --task_index=0</code></p>\n<p>****************** Below is part of the log<br>\n`[0.15988803 0.15988803] step:  8 worker:  0<br>\n[0.17985605 0.17985605] step:  9 worker:  0<br>\nDone 0<br>\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:<br>\nTraceback (most recent call last):<br>\nFile \"/usr/lib64/python3.6/threading.py\", line 916, in _bootstrap_inner<br>\nself.run()<br>\nFile \"/usr/lib64/python3.6/threading.py\", line 864, in run<br>\nself._target(*self._args, **self._kwargs)<br>\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run<br>\ncoord.request_stop(e)<br>\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop<br>\nsix.reraise(*sys.exc_info())<br>\nFile \"/home/python3_env/lib64/python3.6/site-packages/six.py\", line 693, in reraise<br>\nraise value<br>\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run<br>\nenqueue_callable()<br>\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1249, in _single_operation_run<br>\nself._call_tf_sessionrun(None, {}, [], target_list, None)<br>\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1420, in _call_tf_sessionrun<br>\nstatus, run_metadata)<br>\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in <strong>exit</strong><br>\nc_api.TF_GetCode(self.status.status))<br>\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to \"Session::Close()\".</p>\n<p>Session from worker 0 closed cleanly`</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Fedora Red Hat Enterprise Linux Server release 7.5 (Maipo)\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.7.0\nPython version: 3.6.3\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: CUDA9.0/ CUDNN7.0\nGPU model and memory:  P100 16GB\nExact command to reproduce:  bash run.sh\n\nDescribe the problem\nI am using the MonitoredTrainingSession to test a simple script (base on this of synchronized distributed training/multi_gpu training. When the training is finished, the session always throws out Exception (CancelledError) in thread QueueRunnerThread.  I suspect this is related to QueueRunner is not handled cleanly when sess.close() is called.\nSource code / logs\n`REPLICAS_TO_AGGREGATE = 1\ndef main():\n# Configure\nconfig=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\nconfig.gpu_options.allow_growth = True\n# Server Setup\ncluster = tf.train.ClusterSpec({\n    'ps':['localhost:2222'],\n    'worker':['localhost:2223']\n    }) #allows this node know about all other nodes\nif FLAGS.job_name == 'ps': #checks if parameter server\n    with tf.device('/job:ps/task:0/cpu:0'):\n        server = tf.train.Server(cluster,\n                        job_name=\"ps\",\n                        task_index=FLAGS.task_index,\n                        config=config)\n        server.join()\nelse: #it must be a worker server\n    is_chief = (FLAGS.task_index == 0) #checks if this is the chief node\n    with tf.device('/gpu:%d' % (FLAGS.task_index)):\n        server = tf.train.Server(cluster,\n                job_name=\"worker\",\n                task_index=FLAGS.task_index,\n                config=config)\n\n    # Graph\n    # ps_device = \"/job:ps/task:0/cpu:0\"\n    worker_device = \"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, FLAGS.task_index)\n    with tf.device(tf.train.replica_device_setter(ps_tasks=1,\n                   #ps_device=ps_device,\n                   worker_device=worker_device)):\n\n        a = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)\n        b = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)\n        c= a+b\n\n        global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n        target = tf.constant(100.,shape=[2],dtype=tf.float32)\n        loss = tf.reduce_mean(tf.square(c-target))\n\n        # create an optimizer then wrap it with SynceReplicasOptimizer\n        optimizer = tf.train.GradientDescentOptimizer(.0001)\n        optimizer1 = tf.train.SyncReplicasOptimizer(optimizer,\n                 replicas_to_aggregate=REPLICAS_TO_AGGREGATE, total_num_replicas=REPLICAS_TO_AGGREGATE)\n  \n        opt = optimizer1.minimize(loss,global_step=global_step) # averages gradients\n        #opt = optimizer1.minimize(REPLICAS_TO_AGGREGATE*loss,\n        #                           global_step=global_step) # hackily sums gradients\n\n    # Session\n    sync_replicas_hook = optimizer1.make_session_run_hook(is_chief)\n    stop_hook = tf.train.StopAtStepHook(last_step=10)\n    hooks = [sync_replicas_hook,stop_hook]\n\n    # Monitored Training Session\n    with tf.train.MonitoredTrainingSession(master = server.target, \n          is_chief=is_chief,\n          config=config,\n          hooks=hooks,\n          stop_grace_period_secs=10) as sess:\n\n        print('Starting training on worker %d'%FLAGS.task_index)\n        while not sess.should_stop():\n            _,r,gs=sess.run([opt,c,global_step])\n            print(r,'step: ',gs,'worker: ',FLAGS.task_index)\n            # if is_chief: time.sleep(1)\n            time.sleep(1)\n        print('Done',FLAGS.task_index)\n\n        time.sleep(10) #grace period to wait before closing session\n        #sess.close() # if uncomment this, it will raise error 'Session is already closed' after the CanceledError \n    print('Session from worker %d closed cleanly'%FLAGS.task_index)\n\nif name == 'main':\nparser = argparse.ArgumentParser()\n# Flags for defining the tf.train.ClusterSpec\nparser.add_argument(\n\"--job_name\",\ntype=str,\ndefault=\"\",\nhelp=\"One of 'ps', 'worker'\"\n)\n# Flags for defining the tf.train.Server\nparser.add_argument(\n\"--task_index\",\ntype=int,\ndefault=0,\nhelp=\"Index of task within the job\"\n)\nFLAGS, unparsed = parser.parse_known_args()\nprint(FLAGS.task_index)\nmain()`\n**************** I use the following script to run the code\n#!/bin/bash -e export CUDA_VISIBLE_DEVICES=''  python text_multi_gpu.py --job_name=\"ps\" --task_index=0 & export CUDA_VISIBLE_DEVICES='0'  python text_multi_gpu.py --job_name=\"worker\" --task_index=0\n****************** Below is part of the log\n`[0.15988803 0.15988803] step:  8 worker:  0\n[0.17985605 0.17985605] step:  9 worker:  0\nDone 0\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\nTraceback (most recent call last):\nFile \"/usr/lib64/python3.6/threading.py\", line 916, in _bootstrap_inner\nself.run()\nFile \"/usr/lib64/python3.6/threading.py\", line 864, in run\nself._target(*self._args, **self._kwargs)\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\ncoord.request_stop(e)\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\nsix.reraise(*sys.exc_info())\nFile \"/home/python3_env/lib64/python3.6/site-packages/six.py\", line 693, in reraise\nraise value\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\nenqueue_callable()\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1249, in _single_operation_run\nself._call_tf_sessionrun(None, {}, [], target_list, None)\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1420, in _call_tf_sessionrun\nstatus, run_metadata)\nFile \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in exit\nc_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to \"Session::Close()\".\nSession from worker 0 closed cleanly`", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Fedora Red Hat Enterprise Linux Server release 7.5 (Maipo)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA9.0/ CUDNN7.0\r\n- **GPU model and memory**:  P100 16GB\r\n- **Exact command to reproduce**:  bash run.sh\r\n\r\n### Describe the problem\r\nI am using the MonitoredTrainingSession to test a simple script (base on [this](https://github.com/tmulc18/Distributed-TensorFlow-Guide/blob/master/Synchronous-SGD/ssgd.py) of synchronized distributed training/multi_gpu training. When the training is finished, the session always throws out Exception (CancelledError) in thread QueueRunnerThread.  I suspect this is related to QueueRunner is not handled cleanly when sess.close() is called. \r\n\r\n### Source code / logs\r\n`REPLICAS_TO_AGGREGATE = 1\r\ndef main():\r\n    # Configure\r\n    config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\r\n    config.gpu_options.allow_growth = True\r\n\r\n    # Server Setup\r\n    cluster = tf.train.ClusterSpec({\r\n        'ps':['localhost:2222'],\r\n        'worker':['localhost:2223']\r\n        }) #allows this node know about all other nodes\r\n    if FLAGS.job_name == 'ps': #checks if parameter server\r\n        with tf.device('/job:ps/task:0/cpu:0'):\r\n            server = tf.train.Server(cluster,\r\n                            job_name=\"ps\",\r\n                            task_index=FLAGS.task_index,\r\n                            config=config)\r\n            server.join()\r\n    else: #it must be a worker server\r\n        is_chief = (FLAGS.task_index == 0) #checks if this is the chief node\r\n        with tf.device('/gpu:%d' % (FLAGS.task_index)):\r\n            server = tf.train.Server(cluster,\r\n                    job_name=\"worker\",\r\n                    task_index=FLAGS.task_index,\r\n                    config=config)\r\n    \r\n        # Graph\r\n        # ps_device = \"/job:ps/task:0/cpu:0\"\r\n        worker_device = \"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, FLAGS.task_index)\r\n        with tf.device(tf.train.replica_device_setter(ps_tasks=1,\r\n                       #ps_device=ps_device,\r\n                       worker_device=worker_device)):\r\n\r\n            a = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)\r\n            b = tf.Variable(tf.constant(0.,shape=[2]),dtype=tf.float32)\r\n            c= a+b\r\n\r\n            global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\r\n            target = tf.constant(100.,shape=[2],dtype=tf.float32)\r\n            loss = tf.reduce_mean(tf.square(c-target))\r\n\r\n            # create an optimizer then wrap it with SynceReplicasOptimizer\r\n            optimizer = tf.train.GradientDescentOptimizer(.0001)\r\n            optimizer1 = tf.train.SyncReplicasOptimizer(optimizer,\r\n                     replicas_to_aggregate=REPLICAS_TO_AGGREGATE, total_num_replicas=REPLICAS_TO_AGGREGATE)\r\n      \r\n            opt = optimizer1.minimize(loss,global_step=global_step) # averages gradients\r\n            #opt = optimizer1.minimize(REPLICAS_TO_AGGREGATE*loss,\r\n            #                           global_step=global_step) # hackily sums gradients\r\n\r\n        # Session\r\n        sync_replicas_hook = optimizer1.make_session_run_hook(is_chief)\r\n        stop_hook = tf.train.StopAtStepHook(last_step=10)\r\n        hooks = [sync_replicas_hook,stop_hook]\r\n\r\n        # Monitored Training Session\r\n        with tf.train.MonitoredTrainingSession(master = server.target, \r\n              is_chief=is_chief,\r\n              config=config,\r\n              hooks=hooks,\r\n              stop_grace_period_secs=10) as sess:\r\n\r\n            print('Starting training on worker %d'%FLAGS.task_index)\r\n            while not sess.should_stop():\r\n                _,r,gs=sess.run([opt,c,global_step])\r\n                print(r,'step: ',gs,'worker: ',FLAGS.task_index)\r\n                # if is_chief: time.sleep(1)\r\n                time.sleep(1)\r\n            print('Done',FLAGS.task_index)\r\n  \r\n            time.sleep(10) #grace period to wait before closing session\r\n            #sess.close() # if uncomment this, it will raise error 'Session is already closed' after the CanceledError \r\n        print('Session from worker %d closed cleanly'%FLAGS.task_index)\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    # Flags for defining the tf.train.ClusterSpec\r\n    parser.add_argument(\r\n        \"--job_name\",\r\n        type=str,\r\n        default=\"\",\r\n        help=\"One of 'ps', 'worker'\"\r\n      )\r\n    # Flags for defining the tf.train.Server\r\n    parser.add_argument(\r\n        \"--task_index\",\r\n        type=int,\r\n        default=0,\r\n        help=\"Index of task within the job\"\r\n      )\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    print(FLAGS.task_index)\r\n    main()`\r\n\r\n\r\n\r\n**************** I use the following script to run the code \r\n`#!/bin/bash -e\r\nexport CUDA_VISIBLE_DEVICES='' \r\npython text_multi_gpu.py --job_name=\"ps\" --task_index=0 &\r\nexport CUDA_VISIBLE_DEVICES='0' \r\npython text_multi_gpu.py --job_name=\"worker\" --task_index=0`\r\n\r\n\r\n\r\n****************** Below is part of the log\r\n`[0.15988803 0.15988803] step:  8 worker:  0\r\n[0.17985605 0.17985605] step:  9 worker:  0\r\nDone 0\r\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\r\n    coord.request_stop(e)\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1249, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1420, in _call_tf_sessionrun\r\n    status, run_metadata)\r\n  File \"/home/python3_env/lib64/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to \"Session::Close()\".\r\n\r\nSession from worker 0 closed cleanly`"}