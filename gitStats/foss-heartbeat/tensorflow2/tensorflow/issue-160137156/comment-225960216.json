{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225960216", "html_url": "https://github.com/tensorflow/tensorflow/issues/2848#issuecomment-225960216", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2848", "id": 225960216, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTk2MDIxNg==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-14T17:46:49Z", "updated_at": "2016-06-14T17:47:33Z", "author_association": "MEMBER", "body_html": "<p>This is indeed the expected behavior.   Control dependencies only constrain the order of evaluation of the compute ops in your program.  The device to host memcpy operations are executed as _Send/_Recv ops which are automatically inserted when the graph is partitioned between cpu and gpu devices.</p>\n<p>The _Recv ops  have zero inputs and so are schedulable immediately.  The _Send ops (and hence the transfers) are schedulable as soon as the input data has been produced (in this case by Identity ops with names like 'variable/read' which take a single \"snapshot\" of each variable).</p>\n<p>The order that the transfers happen <em>should</em> have no influence on the result of the computation - but, in this case, do appear to have a significant impact on latency.  (You appear to have some <em>huge</em> transfers!)</p>\n<p>In the distributed setting, GraphOptions.enable_recv_scheduling can be used to constrain the order of _Recv nodes.  However, this doesn't currently influence intra-machine transfers.</p>\n<p>If you really care about hand-constraining the order of the GPU transfers, you could <em>probably</em> write something using a bunch of <em>additional</em> identity ops on the cpu and gpu device and explicitly put control dependencies on these.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2342391\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuanbyu\">@yuanbyu</a>  might have more to say on this issue?</p>", "body_text": "This is indeed the expected behavior.   Control dependencies only constrain the order of evaluation of the compute ops in your program.  The device to host memcpy operations are executed as _Send/_Recv ops which are automatically inserted when the graph is partitioned between cpu and gpu devices.\nThe _Recv ops  have zero inputs and so are schedulable immediately.  The _Send ops (and hence the transfers) are schedulable as soon as the input data has been produced (in this case by Identity ops with names like 'variable/read' which take a single \"snapshot\" of each variable).\nThe order that the transfers happen should have no influence on the result of the computation - but, in this case, do appear to have a significant impact on latency.  (You appear to have some huge transfers!)\nIn the distributed setting, GraphOptions.enable_recv_scheduling can be used to constrain the order of _Recv nodes.  However, this doesn't currently influence intra-machine transfers.\nIf you really care about hand-constraining the order of the GPU transfers, you could probably write something using a bunch of additional identity ops on the cpu and gpu device and explicitly put control dependencies on these.\n@yuanbyu  might have more to say on this issue?", "body": "This is indeed the expected behavior.   Control dependencies only constrain the order of evaluation of the compute ops in your program.  The device to host memcpy operations are executed as _Send/_Recv ops which are automatically inserted when the graph is partitioned between cpu and gpu devices.  \n\nThe _Recv ops  have zero inputs and so are schedulable immediately.  The _Send ops (and hence the transfers) are schedulable as soon as the input data has been produced (in this case by Identity ops with names like 'variable/read' which take a single \"snapshot\" of each variable).  \n\nThe order that the transfers happen _should_ have no influence on the result of the computation - but, in this case, do appear to have a significant impact on latency.  (You appear to have some _huge_ transfers!)\n\nIn the distributed setting, GraphOptions.enable_recv_scheduling can be used to constrain the order of _Recv nodes.  However, this doesn't currently influence intra-machine transfers.\n\nIf you really care about hand-constraining the order of the GPU transfers, you could _probably_ write something using a bunch of _additional_ identity ops on the cpu and gpu device and explicitly put control dependencies on these.   \n\n@yuanbyu  might have more to say on this issue?\n"}