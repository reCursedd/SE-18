{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/226009845", "html_url": "https://github.com/tensorflow/tensorflow/issues/2848#issuecomment-226009845", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2848", "id": 226009845, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNjAwOTg0NQ==", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-14T20:42:40Z", "updated_at": "2016-06-14T20:50:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a> Thanks for the detailed feedback, its very helpful for understanding Tensorflow's transfer handling.</p>\n<blockquote>\n<p>The order that the transfers happen should have no influence on the result of the computation - but, in this case, do appear to have a significant impact on latency. (You appear to have some huge transfers!)</p>\n</blockquote>\n<p>I think it's highly dependent on an algorithm's compute to I/O ratio -- Transfers don't matter much if one is performing a lot of GPU compute for a small amount of input data. As this ratio decreases one wants to ensure that one's data transfers overlap compute in order to maximise device usage. For instance, Gregg and Hazelwood have devised a <a href=\"http://www.cs.virginia.edu/kim/docs/ispass11.pdf\" rel=\"nofollow\">taxonomy</a> for classifying GPU algorithms according to these properties.</p>\n<p>But I suspect I'm preaching to the converted here ;-). Similar principles probably apply  to Google's bread and butter -- scheduling RPC network transfers -- and its encouraging to hear about the <strong>enable_recv_scheduling</strong> option. I'll have a look at <code>tf.identity</code> -- there is a nice  <a href=\"http://stackoverflow.com/a/34877802/1611416\" rel=\"nofollow\">stackoverflow</a> answer for other's looking at this issue.</p>\n<p>Having said that,  would there be scope for improving the scheduling of intra-machine send/recv ops?</p>", "body_text": "@prb12 Thanks for the detailed feedback, its very helpful for understanding Tensorflow's transfer handling.\n\nThe order that the transfers happen should have no influence on the result of the computation - but, in this case, do appear to have a significant impact on latency. (You appear to have some huge transfers!)\n\nI think it's highly dependent on an algorithm's compute to I/O ratio -- Transfers don't matter much if one is performing a lot of GPU compute for a small amount of input data. As this ratio decreases one wants to ensure that one's data transfers overlap compute in order to maximise device usage. For instance, Gregg and Hazelwood have devised a taxonomy for classifying GPU algorithms according to these properties.\nBut I suspect I'm preaching to the converted here ;-). Similar principles probably apply  to Google's bread and butter -- scheduling RPC network transfers -- and its encouraging to hear about the enable_recv_scheduling option. I'll have a look at tf.identity -- there is a nice  stackoverflow answer for other's looking at this issue.\nHaving said that,  would there be scope for improving the scheduling of intra-machine send/recv ops?", "body": "@prb12 Thanks for the detailed feedback, its very helpful for understanding Tensorflow's transfer handling.\n\n> The order that the transfers happen should have no influence on the result of the computation - but, in this case, do appear to have a significant impact on latency. (You appear to have some huge transfers!)\n\nI think it's highly dependent on an algorithm's compute to I/O ratio -- Transfers don't matter much if one is performing a lot of GPU compute for a small amount of input data. As this ratio decreases one wants to ensure that one's data transfers overlap compute in order to maximise device usage. For instance, Gregg and Hazelwood have devised a [taxonomy](http://www.cs.virginia.edu/kim/docs/ispass11.pdf) for classifying GPU algorithms according to these properties.\n\nBut I suspect I'm preaching to the converted here ;-). Similar principles probably apply  to Google's bread and butter -- scheduling RPC network transfers -- and its encouraging to hear about the **enable_recv_scheduling** option. I'll have a look at `tf.identity` -- there is a nice  [stackoverflow](http://stackoverflow.com/a/34877802/1611416) answer for other's looking at this issue.\n\nHaving said that,  would there be scope for improving the scheduling of intra-machine send/recv ops?\n"}