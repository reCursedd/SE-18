{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2848", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2848/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2848/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2848/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2848", "id": 160137156, "node_id": "MDU6SXNzdWUxNjAxMzcxNTY=", "number": 2848, "title": "GPU Host To Device copies don't appear to respect GPU operation dependencies", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2016-06-14T09:34:12Z", "updated_at": "2017-02-09T22:37:10Z", "closed_at": "2017-01-23T23:32:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I've been trying out the fancy new <a href=\"http://stackoverflow.com/a/37774430/1611416\" rel=\"nofollow\">tracing</a> functionality in the Tensorflow nightlies. It's really great, thanks for providing it!</p>\n<p>One thing I noticed while <a href=\"https://github.com/ska-sa/montblanc/blob/29b264ee114a6ef63159808125b1a4505cc5ffa2/montblanc/tensorflow/rime_ops/test_all.py#L66-L127\">enqueueing</a> multiple GPU operations with <code>tf.control_dependencies</code> is that Host to Device memory copies for the inputs of consecutive GPU ops can be scheduled in an interleaved pattern, instead of consecutively. In my case I have something like this:</p>\n<ol>\n<li>op EBeam with inputs: ebeam</li>\n<li>op SumCoherencies with inputs: gterm, ant2, model_vis, flag, ant1, ant2</li>\n</ol>\n<p>but in the <a href=\"https://github.com/tensorflow/tensorflow/files/313540/timeline.json.zip\">trace</a> I see the memory copies scheduled as:</p>\n<ul>\n<li>ant1, flag, <strong>ebeam</strong>, gterm, ant2, model_vis</li>\n</ul>\n<p>when I would expect, due to the the consecutive scheduling of EBeam and SumCoherencies ops, that the memory copies would be scheduled as:</p>\n<ul>\n<li><strong>ebeam</strong>, ant1, flag,  gterm, ant2, model_vis</li>\n</ul>\n<p>I also notice that the EBeam op only starts executing on the GPU after all the SumCoherencies inputs have been copied to the GPU (rather than just the ebeam input) so the GPU is idle. There is no dependency on SumCoherencies by EBeam (Its the other way round).</p>\n<p><em>There are several other ops and inputs that I haven't mentioned for the sake of brevity</em></p>\n<h3>Environment info</h3>\n<p>Operating System: <code>Ubuntu 14.04.4</code></p>\n<p>Installed version of CUDA and cuDNN: <code>CUDA 7.5 and cuDNN 4.0.7</code><br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">~</span>$ ls -l /usr/local/cuda/lib64/libcud<span class=\"pl-k\">*</span>\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -<span class=\"pl-k\">&gt;</span> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -<span class=\"pl-k\">&gt;</span> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 19:48 /usr/local/cuda/lib64/libcudnn.so -<span class=\"pl-k\">&gt;</span> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 19:48 /usr/local/cuda/lib64/libcudnn.so.4 -<span class=\"pl-k\">&gt;</span> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  9 00:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  9 00:12 /usr/local/cuda/lib64/libcudnn_static.a</pre></div>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed. <code>python 2 GPU nightly</code></li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>. <code>0.8.0</code></li>\n</ol>\n<p>If installed from sources, provide the commit hash: <strong>N/A</strong></p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>Ran <a href=\"https://github.com/ska-sa/montblanc/blob/29b264ee114a6ef63159808125b1a4505cc5ffa2/montblanc/tensorflow/rime_ops/test_all.py#L66-L127\">this</a> script. (If necessary, source with Makefile is <a href=\"https://github.com/ska-sa/montblanc/tree/29b264ee114a6ef63159808125b1a4505cc5ffa2/montblanc/tensorflow/rime_ops\">here</a> and relevent commit is <a href=\"https://github.com/ska-sa/montblanc/tree/29b264ee114a6ef63159808125b1a4505cc5ffa2\">29b264ee</a>)</li>\n<li>Inspected the timeline (<a href=\"https://github.com/tensorflow/tensorflow/files/313540/timeline.json.zip\">timeline.json.zip</a>) in <code>chrome://tracing/</code></li>\n</ol>\n<h3>What have you tried?</h3>\n<ol>\n<li>Using <code>tf.control_dependencies</code> to modify op execution order</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment).</p>", "body_text": "I've been trying out the fancy new tracing functionality in the Tensorflow nightlies. It's really great, thanks for providing it!\nOne thing I noticed while enqueueing multiple GPU operations with tf.control_dependencies is that Host to Device memory copies for the inputs of consecutive GPU ops can be scheduled in an interleaved pattern, instead of consecutively. In my case I have something like this:\n\nop EBeam with inputs: ebeam\nop SumCoherencies with inputs: gterm, ant2, model_vis, flag, ant1, ant2\n\nbut in the trace I see the memory copies scheduled as:\n\nant1, flag, ebeam, gterm, ant2, model_vis\n\nwhen I would expect, due to the the consecutive scheduling of EBeam and SumCoherencies ops, that the memory copies would be scheduled as:\n\nebeam, ant1, flag,  gterm, ant2, model_vis\n\nI also notice that the EBeam op only starts executing on the GPU after all the SumCoherencies inputs have been copied to the GPU (rather than just the ebeam input) so the GPU is idle. There is no dependency on SumCoherencies by EBeam (Its the other way round).\nThere are several other ops and inputs that I haven't mentioned for the sake of brevity\nEnvironment info\nOperating System: Ubuntu 14.04.4\nInstalled version of CUDA and cuDNN: CUDA 7.5 and cuDNN 4.0.7\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 19:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 19:48 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  9 00:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  9 00:12 /usr/local/cuda/lib64/libcudnn_static.a\nIf installed from binary pip package, provide:\n\nWhich pip package you installed. python 2 GPU nightly\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\". 0.8.0\n\nIf installed from sources, provide the commit hash: N/A\nSteps to reproduce\n\nRan this script. (If necessary, source with Makefile is here and relevent commit is 29b264ee)\nInspected the timeline (timeline.json.zip) in chrome://tracing/\n\nWhat have you tried?\n\nUsing tf.control_dependencies to modify op execution order\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).", "body": "I've been trying out the fancy new [tracing](http://stackoverflow.com/a/37774430/1611416) functionality in the Tensorflow nightlies. It's really great, thanks for providing it!\n\nOne thing I noticed while [enqueueing](https://github.com/ska-sa/montblanc/blob/29b264ee114a6ef63159808125b1a4505cc5ffa2/montblanc/tensorflow/rime_ops/test_all.py#L66-L127) multiple GPU operations with `tf.control_dependencies` is that Host to Device memory copies for the inputs of consecutive GPU ops can be scheduled in an interleaved pattern, instead of consecutively. In my case I have something like this:\n1. op EBeam with inputs: ebeam\n2. op SumCoherencies with inputs: gterm, ant2, model_vis, flag, ant1, ant2\n\nbut in the [trace](https://github.com/tensorflow/tensorflow/files/313540/timeline.json.zip) I see the memory copies scheduled as:\n- ant1, flag, **ebeam**, gterm, ant2, model_vis\n\nwhen I would expect, due to the the consecutive scheduling of EBeam and SumCoherencies ops, that the memory copies would be scheduled as:\n- **ebeam**, ant1, flag,  gterm, ant2, model_vis\n\nI also notice that the EBeam op only starts executing on the GPU after all the SumCoherencies inputs have been copied to the GPU (rather than just the ebeam input) so the GPU is idle. There is no dependency on SumCoherencies by EBeam (Its the other way round).\n\n_There are several other ops and inputs that I haven't mentioned for the sake of brevity_\n### Environment info\n\nOperating System: `Ubuntu 14.04.4`\n\nInstalled version of CUDA and cuDNN: `CUDA 7.5 and cuDNN 4.0.7`\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n``` bash\n~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 19:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 19:48 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  9 00:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  9 00:12 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. `python 2 GPU nightly`\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. `0.8.0`\n\nIf installed from sources, provide the commit hash: **N/A**\n### Steps to reproduce\n1. Ran [this](https://github.com/ska-sa/montblanc/blob/29b264ee114a6ef63159808125b1a4505cc5ffa2/montblanc/tensorflow/rime_ops/test_all.py#L66-L127) script. (If necessary, source with Makefile is [here](https://github.com/ska-sa/montblanc/tree/29b264ee114a6ef63159808125b1a4505cc5ffa2/montblanc/tensorflow/rime_ops) and relevent commit is [29b264ee](https://github.com/ska-sa/montblanc/tree/29b264ee114a6ef63159808125b1a4505cc5ffa2))\n2. Inspected the timeline ([timeline.json.zip](https://github.com/tensorflow/tensorflow/files/313540/timeline.json.zip)) in `chrome://tracing/`\n### What have you tried?\n1. Using `tf.control_dependencies` to modify op execution order\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n"}