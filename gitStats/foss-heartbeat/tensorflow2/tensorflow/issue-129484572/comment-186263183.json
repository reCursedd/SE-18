{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/186263183", "html_url": "https://github.com/tensorflow/tensorflow/issues/917#issuecomment-186263183", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/917", "id": 186263183, "node_id": "MDEyOklzc3VlQ29tbWVudDE4NjI2MzE4Mw==", "user": {"login": "vincentvanhoucke", "id": 15737127, "node_id": "MDQ6VXNlcjE1NzM3MTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/15737127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vincentvanhoucke", "html_url": "https://github.com/vincentvanhoucke", "followers_url": "https://api.github.com/users/vincentvanhoucke/followers", "following_url": "https://api.github.com/users/vincentvanhoucke/following{/other_user}", "gists_url": "https://api.github.com/users/vincentvanhoucke/gists{/gist_id}", "starred_url": "https://api.github.com/users/vincentvanhoucke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vincentvanhoucke/subscriptions", "organizations_url": "https://api.github.com/users/vincentvanhoucke/orgs", "repos_url": "https://api.github.com/users/vincentvanhoucke/repos", "events_url": "https://api.github.com/users/vincentvanhoucke/events{/privacy}", "received_events_url": "https://api.github.com/users/vincentvanhoucke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-19T15:37:47Z", "updated_at": "2016-02-19T15:37:47Z", "author_association": "MEMBER", "body_html": "<p>@LeavesBreathe in the grand scheme of things, it will not affect memory consumption much: as soon as the copy is performed, the input buffers will no longer be needed for the backward pass, and be releasable to the pool. Packing things into one tensor also potentially enables more efficient contractions to take place. What matters most to memory consumption is long-lived tensors, e.g. ones that have to be preserved between the forward and the backward pass. One could still consider a different API if there is evidence that this is a problem.<br>\nI'm also looking into separating the collection of sufficient statistics (which can be performed online) from the actual aggregation, which would give users the freedom to make the reductions more granular if they wanted. I'll be closing this bug now, and treat the other items as opportunities for optimization.</p>", "body_text": "@LeavesBreathe in the grand scheme of things, it will not affect memory consumption much: as soon as the copy is performed, the input buffers will no longer be needed for the backward pass, and be releasable to the pool. Packing things into one tensor also potentially enables more efficient contractions to take place. What matters most to memory consumption is long-lived tensors, e.g. ones that have to be preserved between the forward and the backward pass. One could still consider a different API if there is evidence that this is a problem.\nI'm also looking into separating the collection of sufficient statistics (which can be performed online) from the actual aggregation, which would give users the freedom to make the reductions more granular if they wanted. I'll be closing this bug now, and treat the other items as opportunities for optimization.", "body": "@LeavesBreathe in the grand scheme of things, it will not affect memory consumption much: as soon as the copy is performed, the input buffers will no longer be needed for the backward pass, and be releasable to the pool. Packing things into one tensor also potentially enables more efficient contractions to take place. What matters most to memory consumption is long-lived tensors, e.g. ones that have to be preserved between the forward and the backward pass. One could still consider a different API if there is evidence that this is a problem.\nI'm also looking into separating the collection of sufficient statistics (which can be performed online) from the actual aggregation, which would give users the freedom to make the reductions more granular if they wanted. I'll be closing this bug now, and treat the other items as opportunities for optimization.\n"}