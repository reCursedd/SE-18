{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280723902", "html_url": "https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280723902", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7595", "id": 280723902, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDcyMzkwMg==", "user": {"login": "shtoshni", "id": 1709427, "node_id": "MDQ6VXNlcjE3MDk0Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1709427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shtoshni", "html_url": "https://github.com/shtoshni", "followers_url": "https://api.github.com/users/shtoshni/followers", "following_url": "https://api.github.com/users/shtoshni/following{/other_user}", "gists_url": "https://api.github.com/users/shtoshni/gists{/gist_id}", "starred_url": "https://api.github.com/users/shtoshni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shtoshni/subscriptions", "organizations_url": "https://api.github.com/users/shtoshni/orgs", "repos_url": "https://api.github.com/users/shtoshni/repos", "events_url": "https://api.github.com/users/shtoshni/events{/privacy}", "received_events_url": "https://api.github.com/users/shtoshni/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-17T18:08:10Z", "updated_at": "2017-02-17T18:08:10Z", "author_association": "NONE", "body_html": "<p>I will just describe the parts I modified and the code snippet for that piece.</p>\n<ul>\n<li>\n<p>attention_states has shape (batch_size, attn_length, attn_size) where attn_length is enc_inp_len<br>\nThe first 2 dimensions are unknown and that will cause issue with reshaping ops trying to preserve the first 2 dimensions. However, the reshaping operation used in seq2seq below was easily replaceable by the next line.<br>\n<code>hidden = array_ops.reshape( attention_states, [-1, attn_length, 1, attn_size])</code><br>\n<code>hidden = tf.expand_dims(attention_states, 2)</code></p>\n</li>\n<li>\n<p>Multiplying attention weights <code>alpha</code> with encoder hidden states, <code>hidden</code>, requires another reshape in the original code<br>\nalpha has shape (batch_size, attn_length) and is calculated via<br>\n<code>alpha = nn_ops.softmax(s)</code><br>\nThe original code uses the following line to get context vector <strong>c</strong><br>\n<code>c = math_ops.reduce_sum(  array_ops.reshape(alpha, [-1, attn_length, 1, 1]) * hidden,                                        [1, 2]) </code><br>\nI replaced it via<br>\n<code>alpha = tf.expand_dims(alpha, 2)</code><br>\n<code>alpha = tf.expand_dims(alpha, 3)</code><br>\n<code>c = math_ops.reduce_sum(alpha * hidden, [1, 2])</code></p>\n</li>\n</ul>\n<p>Hope this helps!</p>", "body_text": "I will just describe the parts I modified and the code snippet for that piece.\n\n\nattention_states has shape (batch_size, attn_length, attn_size) where attn_length is enc_inp_len\nThe first 2 dimensions are unknown and that will cause issue with reshaping ops trying to preserve the first 2 dimensions. However, the reshaping operation used in seq2seq below was easily replaceable by the next line.\nhidden = array_ops.reshape( attention_states, [-1, attn_length, 1, attn_size])\nhidden = tf.expand_dims(attention_states, 2)\n\n\nMultiplying attention weights alpha with encoder hidden states, hidden, requires another reshape in the original code\nalpha has shape (batch_size, attn_length) and is calculated via\nalpha = nn_ops.softmax(s)\nThe original code uses the following line to get context vector c\nc = math_ops.reduce_sum(  array_ops.reshape(alpha, [-1, attn_length, 1, 1]) * hidden,                                        [1, 2]) \nI replaced it via\nalpha = tf.expand_dims(alpha, 2)\nalpha = tf.expand_dims(alpha, 3)\nc = math_ops.reduce_sum(alpha * hidden, [1, 2])\n\n\nHope this helps!", "body": "I will just describe the parts I modified and the code snippet for that piece.\r\n\r\n- attention_states has shape (batch_size, attn_length, attn_size) where attn_length is enc_inp_len\r\nThe first 2 dimensions are unknown and that will cause issue with reshaping ops trying to preserve the first 2 dimensions. However, the reshaping operation used in seq2seq below was easily replaceable by the next line. \r\n`hidden = array_ops.reshape( attention_states, [-1, attn_length, 1, attn_size])` \r\n`hidden = tf.expand_dims(attention_states, 2)`\r\n\r\n- Multiplying attention weights `alpha` with encoder hidden states, `hidden`, requires another reshape in the original code\r\nalpha has shape (batch_size, attn_length) and is calculated via\r\n`alpha = nn_ops.softmax(s)` \r\nThe original code uses the following line to get context vector **c**\r\n`c = math_ops.reduce_sum( \r\n                        array_ops.reshape(alpha, [-1, attn_length, 1, 1]) * hidden,                                       \r\n                        [1, 2])\r\n`\r\nI replaced it via\r\n`alpha = tf.expand_dims(alpha, 2)` \r\n`alpha = tf.expand_dims(alpha, 3)` \r\n`c = math_ops.reduce_sum(alpha * hidden, [1, 2])`\r\n\r\nHope this helps!\r\n\r\n\r\n\r\n"}