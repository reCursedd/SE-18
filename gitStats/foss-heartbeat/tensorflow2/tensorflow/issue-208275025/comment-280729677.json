{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280729677", "html_url": "https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280729677", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7595", "id": 280729677, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDcyOTY3Nw==", "user": {"login": "shtoshni", "id": 1709427, "node_id": "MDQ6VXNlcjE3MDk0Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1709427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shtoshni", "html_url": "https://github.com/shtoshni", "followers_url": "https://api.github.com/users/shtoshni/followers", "following_url": "https://api.github.com/users/shtoshni/following{/other_user}", "gists_url": "https://api.github.com/users/shtoshni/gists{/gist_id}", "starred_url": "https://api.github.com/users/shtoshni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shtoshni/subscriptions", "organizations_url": "https://api.github.com/users/shtoshni/orgs", "repos_url": "https://api.github.com/users/shtoshni/repos", "events_url": "https://api.github.com/users/shtoshni/events{/privacy}", "received_events_url": "https://api.github.com/users/shtoshni/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-17T18:32:17Z", "updated_at": "2017-02-17T18:32:17Z", "author_association": "NONE", "body_html": "<p>By the way, since you guys are looking at improving the seq2seq library, there is one mathematical inaccuracy in current implementation.</p>\n<ul>\n<li>The attention mechanism currently considers the whole padded input.</li>\n<li>Even though the padded input won't contribute to context vector due to <strong>0</strong> output vector corresponding to it, it does reduce the probability mass of the actual inputs as it is part of the <code>softmax</code> calculation.</li>\n<li>Ideally, there should be a mask applied to the <code>softmax</code> output and subsequent re-normalization.  Something along these lines:<br>\n<code>attn_mask = tf.sequence_mask(seq_len_inp, dtype=tf.float32)</code><br>\n<code>alpha = nn_ops.softmax(s) * attn_mask</code><br>\n<code>alpha = alpha / (tf.reduce_sum(alpha, reduction_indices=[1]) + 1e-12)</code></li>\n<li>However, I must say that things work out fine even without it but it's needed to match the math described in research papers.</li>\n</ul>", "body_text": "By the way, since you guys are looking at improving the seq2seq library, there is one mathematical inaccuracy in current implementation.\n\nThe attention mechanism currently considers the whole padded input.\nEven though the padded input won't contribute to context vector due to 0 output vector corresponding to it, it does reduce the probability mass of the actual inputs as it is part of the softmax calculation.\nIdeally, there should be a mask applied to the softmax output and subsequent re-normalization.  Something along these lines:\nattn_mask = tf.sequence_mask(seq_len_inp, dtype=tf.float32)\nalpha = nn_ops.softmax(s) * attn_mask\nalpha = alpha / (tf.reduce_sum(alpha, reduction_indices=[1]) + 1e-12)\nHowever, I must say that things work out fine even without it but it's needed to match the math described in research papers.", "body": "By the way, since you guys are looking at improving the seq2seq library, there is one mathematical inaccuracy in current implementation. \r\n- The attention mechanism currently considers the whole padded input. \r\n- Even though the padded input won't contribute to context vector due to **0** output vector corresponding to it, it does reduce the probability mass of the actual inputs as it is part of the `softmax` calculation.\r\n- Ideally, there should be a mask applied to the `softmax` output and subsequent re-normalization.  Something along these lines:\r\n`attn_mask = tf.sequence_mask(seq_len_inp, dtype=tf.float32)`\r\n`alpha = nn_ops.softmax(s) * attn_mask`\r\n`alpha = alpha / (tf.reduce_sum(alpha, reduction_indices=[1]) + 1e-12)`\r\n- However, I must say that things work out fine even without it but it's needed to match the math described in research papers."}