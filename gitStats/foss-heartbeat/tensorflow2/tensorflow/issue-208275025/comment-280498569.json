{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280498569", "html_url": "https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280498569", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7595", "id": 280498569, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDQ5ODU2OQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-16T23:30:07Z", "updated_at": "2017-02-16T23:30:07Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Sounds like your encoder inputs have unknown batch_size and time_steps,\nright?  But you expect dynamic_rnn to \"fill in\" the unknown time_steps?\nSince time_steps are not known at graph build time, you cannot fill it into\nthe static shape, even if you use reduce_max.  the result of reduce_max is\na Tensor, which you cannot use to set static shape.\n\nIt sounds like the attention mechanism wants a static number of time steps,\nand is unable to handle variable length outputs.  That or you're somehow\nnot using the attention correctly.\n\nThat said, we hope to have a better decoder + attention mechanism coming\nsoon (next week or two); in tf.contrib.seq2seq, on tensorflow's github\nmaster branch.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Thu, Feb 16, 2017 at 3:17 PM, Yaroslav Bulatov ***@***.***&gt; wrote:\n cc <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; who has substantial experience\n with it (he'll probably ask you to upgrade to TF 1.0 first)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"208275025\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7595\" href=\"https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280495558\">#7595 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimwOB6j2q_SNkIpu1No9nzufWbCiMks5rdNkGgaJpZM4MDp5g\">https://github.com/notifications/unsubscribe-auth/ABtimwOB6j2q_SNkIpu1No9nzufWbCiMks5rdNkGgaJpZM4MDp5g</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Sounds like your encoder inputs have unknown batch_size and time_steps,\nright?  But you expect dynamic_rnn to \"fill in\" the unknown time_steps?\nSince time_steps are not known at graph build time, you cannot fill it into\nthe static shape, even if you use reduce_max.  the result of reduce_max is\na Tensor, which you cannot use to set static shape.\n\nIt sounds like the attention mechanism wants a static number of time steps,\nand is unable to handle variable length outputs.  That or you're somehow\nnot using the attention correctly.\n\nThat said, we hope to have a better decoder + attention mechanism coming\nsoon (next week or two); in tf.contrib.seq2seq, on tensorflow's github\nmaster branch.\n\u2026\nOn Thu, Feb 16, 2017 at 3:17 PM, Yaroslav Bulatov ***@***.***> wrote:\n cc @ebrevdo <https://github.com/ebrevdo> who has substantial experience\n with it (he'll probably ask you to upgrade to TF 1.0 first)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#7595 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtimwOB6j2q_SNkIpu1No9nzufWbCiMks5rdNkGgaJpZM4MDp5g>\n .", "body": "Sounds like your encoder inputs have unknown batch_size and time_steps,\nright?  But you expect dynamic_rnn to \"fill in\" the unknown time_steps?\nSince time_steps are not known at graph build time, you cannot fill it into\nthe static shape, even if you use reduce_max.  the result of reduce_max is\na Tensor, which you cannot use to set static shape.\n\nIt sounds like the attention mechanism wants a static number of time steps,\nand is unable to handle variable length outputs.  That or you're somehow\nnot using the attention correctly.\n\nThat said, we hope to have a better decoder + attention mechanism coming\nsoon (next week or two); in tf.contrib.seq2seq, on tensorflow's github\nmaster branch.\n\nOn Thu, Feb 16, 2017 at 3:17 PM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> cc @ebrevdo <https://github.com/ebrevdo> who has substantial experience\n> with it (he'll probably ask you to upgrade to TF 1.0 first)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280495558>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwOB6j2q_SNkIpu1No9nzufWbCiMks5rdNkGgaJpZM4MDp5g>\n> .\n>\n"}