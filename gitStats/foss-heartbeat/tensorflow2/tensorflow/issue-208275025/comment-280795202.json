{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280795202", "html_url": "https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280795202", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7595", "id": 280795202, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDc5NTIwMg==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-17T23:30:46Z", "updated_at": "2017-02-17T23:30:46Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Thanks!  Forwarded these two notes (one on slicing to andrew; the other to\na collaborator working on attention).</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Feb 17, 2017 at 10:32 AM, Shubham Toshniwal &lt; ***@***.***&gt; wrote:\n By the way, since you guys are looking at improving the seq2seq library,\n there is one mathematical inaccuracy in current implementation.\n\n    - The attention mechanism currently considers the whole padded input.\n    - Even though the padded input won't contribute to context vector due\n    to *0* output vector corresponding to it, it does reduce the\n    probability mass of the actual inputs as it is part of the softmax\n    calculation.\n    - Ideally, there should be a mask applied to the softmax output and\n    subsequent re-normalization. Something along these lines:\n    attn_mask = tf.sequence_mask(seq_len_inp, dtype=tf.float32)\n    alpha = nn_ops.softmax(s) * attn_mask\n    alpha = alpha / (tf.reduce_sum(alpha, reduction_indices=[1]) + 1e-12)\n    - However, I must say that things work out fine even without it but\n    it's needed to match the math described in research papers.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"208275025\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7595\" href=\"https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280729677\">#7595 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim5ppHUp3GS1Cckrfwcj5g7ln1I3dks5rdefIgaJpZM4MDp5g\">https://github.com/notifications/unsubscribe-auth/ABtim5ppHUp3GS1Cckrfwcj5g7ln1I3dks5rdefIgaJpZM4MDp5g</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Thanks!  Forwarded these two notes (one on slicing to andrew; the other to\na collaborator working on attention).\n\u2026\nOn Fri, Feb 17, 2017 at 10:32 AM, Shubham Toshniwal < ***@***.***> wrote:\n By the way, since you guys are looking at improving the seq2seq library,\n there is one mathematical inaccuracy in current implementation.\n\n    - The attention mechanism currently considers the whole padded input.\n    - Even though the padded input won't contribute to context vector due\n    to *0* output vector corresponding to it, it does reduce the\n    probability mass of the actual inputs as it is part of the softmax\n    calculation.\n    - Ideally, there should be a mask applied to the softmax output and\n    subsequent re-normalization. Something along these lines:\n    attn_mask = tf.sequence_mask(seq_len_inp, dtype=tf.float32)\n    alpha = nn_ops.softmax(s) * attn_mask\n    alpha = alpha / (tf.reduce_sum(alpha, reduction_indices=[1]) + 1e-12)\n    - However, I must say that things work out fine even without it but\n    it's needed to match the math described in research papers.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#7595 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim5ppHUp3GS1Cckrfwcj5g7ln1I3dks5rdefIgaJpZM4MDp5g>\n .", "body": "Thanks!  Forwarded these two notes (one on slicing to andrew; the other to\na collaborator working on attention).\n\nOn Fri, Feb 17, 2017 at 10:32 AM, Shubham Toshniwal <\nnotifications@github.com> wrote:\n\n> By the way, since you guys are looking at improving the seq2seq library,\n> there is one mathematical inaccuracy in current implementation.\n>\n>    - The attention mechanism currently considers the whole padded input.\n>    - Even though the padded input won't contribute to context vector due\n>    to *0* output vector corresponding to it, it does reduce the\n>    probability mass of the actual inputs as it is part of the softmax\n>    calculation.\n>    - Ideally, there should be a mask applied to the softmax output and\n>    subsequent re-normalization. Something along these lines:\n>    attn_mask = tf.sequence_mask(seq_len_inp, dtype=tf.float32)\n>    alpha = nn_ops.softmax(s) * attn_mask\n>    alpha = alpha / (tf.reduce_sum(alpha, reduction_indices=[1]) + 1e-12)\n>    - However, I must say that things work out fine even without it but\n>    it's needed to match the math described in research papers.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7595#issuecomment-280729677>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5ppHUp3GS1Cckrfwcj5g7ln1I3dks5rdefIgaJpZM4MDp5g>\n> .\n>\n"}