{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15768", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15768/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15768/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15768/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15768", "id": 285302510, "node_id": "MDU6SXNzdWUyODUzMDI1MTA=", "number": 15768, "title": "Training broke with ResourceExausted error", "user": {"login": "sbmaruf", "id": 32699797, "node_id": "MDQ6VXNlcjMyNjk5Nzk3", "avatar_url": "https://avatars0.githubusercontent.com/u/32699797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sbmaruf", "html_url": "https://github.com/sbmaruf", "followers_url": "https://api.github.com/users/sbmaruf/followers", "following_url": "https://api.github.com/users/sbmaruf/following{/other_user}", "gists_url": "https://api.github.com/users/sbmaruf/gists{/gist_id}", "starred_url": "https://api.github.com/users/sbmaruf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sbmaruf/subscriptions", "organizations_url": "https://api.github.com/users/sbmaruf/orgs", "repos_url": "https://api.github.com/users/sbmaruf/repos", "events_url": "https://api.github.com/users/sbmaruf/events{/privacy}", "received_events_url": "https://api.github.com/users/sbmaruf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-01T11:31:01Z", "updated_at": "2018-01-03T02:13:59Z", "closed_at": "2018-01-03T02:13:59Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.0</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>CUDA/cuDNN version</strong>: 8</li>\n<li><strong>GPU model and memory</strong>: TITAN X, 12207MiB</li>\n</ul>\n<hr>\n<p>Most Probably everyone will be asking about this is a question for StackOverflow. Here is the link to the StackOverflow question,  But please take a look at the problem. There could be some bug in tensorflow as the error occurs after 32 epoch.<br>\n<a href=\"https://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error\" rel=\"nofollow\">https://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error</a></p>\n<p>Here is the code of the model, <a href=\"https://paste.ubuntu.com/26298336/\" rel=\"nofollow\">https://paste.ubuntu.com/26298336/</a><br>\nA short description of the model would be,</p>\n<ul>\n<li>Character level Embedding Vector -&gt; Embedding lookup -&gt; LSTM1</li>\n<li>Word level Embedding Vector-&gt;Embedding lookup -&gt; LSTM2</li>\n<li>[LSTM1+LSTM2] -&gt; single layer MLP-&gt; softmax layer/CRF layer</li>\n<li>[LSTM1+LSTM2] -&gt; Single layer MLP-&gt; WGAN discriminator</li>\n</ul>\n<p>While running the code it produces the following error output at the epoch 32,</p>\n<p><code>ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24760,100] [[Node: chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split = Split[T=DT_FLOAT, num_split=4, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y, chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] [[Node: bi-lstm/bidirectional_rnn/bw/bw/stack/_167 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]</code></p>\n<p>My question is if there is any error then it should occur in the first epoch why at 32 epoch?</p>\n<p>I am using embedding_lookup in following way,</p>\n<pre><code>_word_embeddings = tf.Variable(\n                        embeddings,\n                        name=\"_word_embeddings\",\n                        dtype=tf.float32,\n                        trainable=False)\n            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=\"word_embeddings\")\n\n</code></pre>\n<p>Where <code>embeddings</code> is a <code>(61698, 100)</code> sized vector. which is only 24 MB. However in the error message, it showed the error with, <code>(24760, 100)</code> sized vector which is only 10MB. It also produces warning while declaring optimizers for the model. it was suggested as below,</p>\n<blockquote>\n<p>gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.</p>\n</blockquote>", "body_text": "System information\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4.0\nPython version: 3.5\nCUDA/cuDNN version: 8\nGPU model and memory: TITAN X, 12207MiB\n\n\nMost Probably everyone will be asking about this is a question for StackOverflow. Here is the link to the StackOverflow question,  But please take a look at the problem. There could be some bug in tensorflow as the error occurs after 32 epoch.\nhttps://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error\nHere is the code of the model, https://paste.ubuntu.com/26298336/\nA short description of the model would be,\n\nCharacter level Embedding Vector -> Embedding lookup -> LSTM1\nWord level Embedding Vector->Embedding lookup -> LSTM2\n[LSTM1+LSTM2] -> single layer MLP-> softmax layer/CRF layer\n[LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator\n\nWhile running the code it produces the following error output at the epoch 32,\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24760,100] [[Node: chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split = Split[T=DT_FLOAT, num_split=4, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y, chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] [[Node: bi-lstm/bidirectional_rnn/bw/bw/stack/_167 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\nMy question is if there is any error then it should occur in the first epoch why at 32 epoch?\nI am using embedding_lookup in following way,\n_word_embeddings = tf.Variable(\n                        embeddings,\n                        name=\"_word_embeddings\",\n                        dtype=tf.float32,\n                        trainable=False)\n            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=\"word_embeddings\")\n\n\nWhere embeddings is a (61698, 100) sized vector. which is only 24 MB. However in the error message, it showed the error with, (24760, 100) sized vector which is only 10MB. It also produces warning while declaring optimizers for the model. it was suggested as below,\n\ngradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: TITAN X, 12207MiB\r\n\r\n----------------------\r\n\r\nMost Probably everyone will be asking about this is a question for StackOverflow. Here is the link to the StackOverflow question,  But please take a look at the problem. There could be some bug in tensorflow as the error occurs after 32 epoch.\r\nhttps://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error\r\n\r\nHere is the code of the model, https://paste.ubuntu.com/26298336/\r\nA short description of the model would be,\r\n\r\n- Character level Embedding Vector -> Embedding lookup -> LSTM1\r\n- Word level Embedding Vector->Embedding lookup -> LSTM2\r\n- [LSTM1+LSTM2] -> single layer MLP-> softmax layer/CRF layer\r\n- [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator\r\n\r\nWhile running the code it produces the following error output at the epoch 32,\r\n\r\n`ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24760,100] [[Node: chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split = Split[T=DT_FLOAT, num_split=4, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y, chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] [[Node: bi-lstm/bidirectional_rnn/bw/bw/stack/_167 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]`\r\n\r\nMy question is if there is any error then it should occur in the first epoch why at 32 epoch?\r\n\r\nI am using embedding_lookup in following way,\r\n\r\n```\r\n_word_embeddings = tf.Variable(\r\n                        embeddings,\r\n                        name=\"_word_embeddings\",\r\n                        dtype=tf.float32,\r\n                        trainable=False)\r\n            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=\"word_embeddings\")\r\n\r\n```\r\n\r\nWhere `embeddings` is a `(61698, 100)` sized vector. which is only 24 MB. However in the error message, it showed the error with, `(24760, 100)` sized vector which is only 10MB. It also produces warning while declaring optimizers for the model. it was suggested as below,\r\n\r\n> gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n\r\n"}