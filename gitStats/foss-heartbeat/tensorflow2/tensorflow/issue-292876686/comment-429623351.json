{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/429623351", "html_url": "https://github.com/tensorflow/tensorflow/issues/16593#issuecomment-429623351", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16593", "id": 429623351, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTYyMzM1MQ==", "user": {"login": "Cheng-XJTU", "id": 24236503, "node_id": "MDQ6VXNlcjI0MjM2NTAz", "avatar_url": "https://avatars1.githubusercontent.com/u/24236503?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Cheng-XJTU", "html_url": "https://github.com/Cheng-XJTU", "followers_url": "https://api.github.com/users/Cheng-XJTU/followers", "following_url": "https://api.github.com/users/Cheng-XJTU/following{/other_user}", "gists_url": "https://api.github.com/users/Cheng-XJTU/gists{/gist_id}", "starred_url": "https://api.github.com/users/Cheng-XJTU/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Cheng-XJTU/subscriptions", "organizations_url": "https://api.github.com/users/Cheng-XJTU/orgs", "repos_url": "https://api.github.com/users/Cheng-XJTU/repos", "events_url": "https://api.github.com/users/Cheng-XJTU/events{/privacy}", "received_events_url": "https://api.github.com/users/Cheng-XJTU/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-14T12:42:23Z", "updated_at": "2018-10-14T12:42:23Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>After testing this more it looks like <code>tfgan</code> might do the right thing. I misread the DCGAN implementation mentioned above: They're using the same behaviour as <code>tfgan</code>.</p>\n<p>Some quick tests on our side suggest that it is indeed better to not run batch norm in inference mode during adverserial training.</p>\n<p>In any case it would be great to get some feedback from GAN experts.</p>\n</blockquote>\n<p>Same observation here. It might be fine to hard code <code>is_training = True</code>. I think it works more like a weakened instance normalization.</p>", "body_text": "After testing this more it looks like tfgan might do the right thing. I misread the DCGAN implementation mentioned above: They're using the same behaviour as tfgan.\nSome quick tests on our side suggest that it is indeed better to not run batch norm in inference mode during adverserial training.\nIn any case it would be great to get some feedback from GAN experts.\n\nSame observation here. It might be fine to hard code is_training = True. I think it works more like a weakened instance normalization.", "body": "> After testing this more it looks like `tfgan` might do the right thing. I misread the DCGAN implementation mentioned above: They're using the same behaviour as `tfgan`.\r\n> \r\n> Some quick tests on our side suggest that it is indeed better to not run batch norm in inference mode during adverserial training.\r\n> \r\n> In any case it would be great to get some feedback from GAN experts.\r\n\r\nSame observation here. It might be fine to hard code `is_training = True`. I think it works more like a weakened instance normalization."}