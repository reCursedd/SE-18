{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/377793617", "html_url": "https://github.com/tensorflow/tensorflow/issues/16593#issuecomment-377793617", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16593", "id": 377793617, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Nzc5MzYxNw==", "user": {"login": "lgeiger", "id": 13285808, "node_id": "MDQ6VXNlcjEzMjg1ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/13285808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lgeiger", "html_url": "https://github.com/lgeiger", "followers_url": "https://api.github.com/users/lgeiger/followers", "following_url": "https://api.github.com/users/lgeiger/following{/other_user}", "gists_url": "https://api.github.com/users/lgeiger/gists{/gist_id}", "starred_url": "https://api.github.com/users/lgeiger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lgeiger/subscriptions", "organizations_url": "https://api.github.com/users/lgeiger/orgs", "repos_url": "https://api.github.com/users/lgeiger/repos", "events_url": "https://api.github.com/users/lgeiger/events{/privacy}", "received_events_url": "https://api.github.com/users/lgeiger/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-01T15:15:51Z", "updated_at": "2018-04-01T15:15:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6020988\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/joel-shor\">@joel-shor</a> Thanks for the fast response and the updated link.</p>\n<p>Sorry, I should've been more clear in my comment above. I was talking about the distinction between training and inference of the different models <strong>during</strong> adversarial training.<br>\nMeaning the generator should work in inference mode during the discriminator updates and vice versa. This is also how <a href=\"https://towardsdatascience.com/implementing-a-generative-adversarial-network-gan-dcgan-to-draw-human-faces-8291616904a\" rel=\"nofollow\">this DCGAN implementation</a> handles batch norm.</p>\n<p>Should TFGAN handle this the same way? I think it's easiest to explain this with some (pseudo) code:</p>\n<div class=\"highlight highlight-source-diff\"><pre>commit b2b6fbc857325298de402eef589281dcebd5ae5c\nAuthor: Lukas Geiger &lt;lukas.geiger94@gmail.com&gt;\nDate:   Sun Apr 1 14:39:53 2018 +0200\n\n    is_training\n\n<span class=\"pl-c1\">diff --git a/tensorflow/contrib/gan/python/train.py b/tensorflow/contrib/gan/python/train.py</span>\nindex 73acd05b60..8cf2b316aa 100644\n<span class=\"pl-md\">--- a/tensorflow/contrib/gan/python/train.py</span>\n<span class=\"pl-mi1\">+++ b/tensorflow/contrib/gan/python/train.py</span>\n<span class=\"pl-mdr\">@@ -31,21 +31,26 @@</span> from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>import functools</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span></span>\n from tensorflow.contrib.framework.python.ops import variables as variables_lib\n from tensorflow.contrib.gan.python import losses as tfgan_losses\n from tensorflow.contrib.gan.python import namedtuples\n from tensorflow.contrib.slim.python.slim import learning as slim_learning\n from tensorflow.contrib.training.python.training import training\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>from tensorflow.python.framework import dtypes</span>\n from tensorflow.python.framework import ops\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import check_ops\n from tensorflow.python.ops import init_ops\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>from tensorflow.python.ops import math_ops</span>\n from tensorflow.python.ops import variable_scope\n from tensorflow.python.ops.distributions import distribution as ds\n from tensorflow.python.ops.losses import losses\n from tensorflow.python.training import session_run_hook\n from tensorflow.python.training import sync_replicas_optimizer\n from tensorflow.python.training import training_util\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>from tensorflow.python.util import tf_inspect as inspect</span>\n\n\n __all__ = [\n<span class=\"pl-mdr\">@@ -101,6 +106,14 @@</span> def gan_model(\n     ValueError: If the generator outputs a Tensor that isn't the same shape as\n       `real_data`.\n   \"\"\"\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  is_generator_training = array_ops.placeholder(</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>      dtype=dtypes.bool, name='IS_GENERATOR_TRAINING')</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  if 'is_training' in inspect.getargspec(generator_fn).args:</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    generator_fn = functools.partial(</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        generator_fn, is_training=is_generator_training)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  if 'is_training' in inspect.getargspec(discriminator_fn).args:</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    discriminator_fn = functools.partial(</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        discriminator_fn, is_training=math_ops.logical_not(is_generator_training))</span>\n   # Create models\n   with variable_scope.variable_scope(generator_scope) as gen_scope:\n     generator_inputs = _convert_tensor_or_l_or_d(generator_inputs)\n<span class=\"pl-mdr\">@@ -792,7 +805,7 @@</span> def gan_train_ops(\n class RunTrainOpsHook(session_run_hook.SessionRunHook):\n   \"\"\"A hook to run train ops a fixed number of times.\"\"\"\n\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  def __init__(self, train_ops, train_steps):</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  def __init__(self, train_ops, train_steps, is_generator=True):</span>\n     \"\"\"Run train ops a certain number of times.\n\n     Args:\n<span class=\"pl-mdr\">@@ -803,10 +816,11 @@</span> class RunTrainOpsHook(session_run_hook.SessionRunHook):\n       train_ops = [train_ops]\n     self._train_ops = train_ops\n     self._train_steps = train_steps\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    self._is_generator = is_generator</span>\n\n   def before_run(self, run_context):\n     for _ in range(self._train_steps):\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>      run_context.session.run(self._train_ops)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>      run_context.session.run(self._train_ops, feed_dict={'IS_GENERATOR_TRAINING:0': self._is_generator})</span>\n\n\n def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1)):\n<span class=\"pl-mdr\">@@ -821,9 +835,11 @@</span> def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1)):\n   \"\"\"\n   def get_hooks(train_ops):\n     generator_hook = RunTrainOpsHook(train_ops.generator_train_op,\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                                     train_steps.generator_train_steps)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                     train_steps.generator_train_steps,</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                     is_generator=True)</span>\n     discriminator_hook = RunTrainOpsHook(train_ops.discriminator_train_op,\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                                         train_steps.discriminator_train_steps)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                         train_steps.discriminator_train_steps,</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                         is_generator=False)</span>\n     return [generator_hook, discriminator_hook]\n   return get_hooks\n</pre></div>", "body_text": "@joel-shor Thanks for the fast response and the updated link.\nSorry, I should've been more clear in my comment above. I was talking about the distinction between training and inference of the different models during adversarial training.\nMeaning the generator should work in inference mode during the discriminator updates and vice versa. This is also how this DCGAN implementation handles batch norm.\nShould TFGAN handle this the same way? I think it's easiest to explain this with some (pseudo) code:\ncommit b2b6fbc857325298de402eef589281dcebd5ae5c\nAuthor: Lukas Geiger <lukas.geiger94@gmail.com>\nDate:   Sun Apr 1 14:39:53 2018 +0200\n\n    is_training\n\ndiff --git a/tensorflow/contrib/gan/python/train.py b/tensorflow/contrib/gan/python/train.py\nindex 73acd05b60..8cf2b316aa 100644\n--- a/tensorflow/contrib/gan/python/train.py\n+++ b/tensorflow/contrib/gan/python/train.py\n@@ -31,21 +31,26 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n\n+import functools\n+\n from tensorflow.contrib.framework.python.ops import variables as variables_lib\n from tensorflow.contrib.gan.python import losses as tfgan_losses\n from tensorflow.contrib.gan.python import namedtuples\n from tensorflow.contrib.slim.python.slim import learning as slim_learning\n from tensorflow.contrib.training.python.training import training\n+from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import ops\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import check_ops\n from tensorflow.python.ops import init_ops\n+from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import variable_scope\n from tensorflow.python.ops.distributions import distribution as ds\n from tensorflow.python.ops.losses import losses\n from tensorflow.python.training import session_run_hook\n from tensorflow.python.training import sync_replicas_optimizer\n from tensorflow.python.training import training_util\n+from tensorflow.python.util import tf_inspect as inspect\n\n\n __all__ = [\n@@ -101,6 +106,14 @@ def gan_model(\n     ValueError: If the generator outputs a Tensor that isn't the same shape as\n       `real_data`.\n   \"\"\"\n+  is_generator_training = array_ops.placeholder(\n+      dtype=dtypes.bool, name='IS_GENERATOR_TRAINING')\n+  if 'is_training' in inspect.getargspec(generator_fn).args:\n+    generator_fn = functools.partial(\n+        generator_fn, is_training=is_generator_training)\n+  if 'is_training' in inspect.getargspec(discriminator_fn).args:\n+    discriminator_fn = functools.partial(\n+        discriminator_fn, is_training=math_ops.logical_not(is_generator_training))\n   # Create models\n   with variable_scope.variable_scope(generator_scope) as gen_scope:\n     generator_inputs = _convert_tensor_or_l_or_d(generator_inputs)\n@@ -792,7 +805,7 @@ def gan_train_ops(\n class RunTrainOpsHook(session_run_hook.SessionRunHook):\n   \"\"\"A hook to run train ops a fixed number of times.\"\"\"\n\n-  def __init__(self, train_ops, train_steps):\n+  def __init__(self, train_ops, train_steps, is_generator=True):\n     \"\"\"Run train ops a certain number of times.\n\n     Args:\n@@ -803,10 +816,11 @@ class RunTrainOpsHook(session_run_hook.SessionRunHook):\n       train_ops = [train_ops]\n     self._train_ops = train_ops\n     self._train_steps = train_steps\n+    self._is_generator = is_generator\n\n   def before_run(self, run_context):\n     for _ in range(self._train_steps):\n-      run_context.session.run(self._train_ops)\n+      run_context.session.run(self._train_ops, feed_dict={'IS_GENERATOR_TRAINING:0': self._is_generator})\n\n\n def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1)):\n@@ -821,9 +835,11 @@ def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1)):\n   \"\"\"\n   def get_hooks(train_ops):\n     generator_hook = RunTrainOpsHook(train_ops.generator_train_op,\n-                                     train_steps.generator_train_steps)\n+                                     train_steps.generator_train_steps,\n+                                     is_generator=True)\n     discriminator_hook = RunTrainOpsHook(train_ops.discriminator_train_op,\n-                                         train_steps.discriminator_train_steps)\n+                                         train_steps.discriminator_train_steps,\n+                                         is_generator=False)\n     return [generator_hook, discriminator_hook]\n   return get_hooks", "body": "@joel-shor Thanks for the fast response and the updated link.\r\n\r\nSorry, I should've been more clear in my comment above. I was talking about the distinction between training and inference of the different models **during** adversarial training.\r\nMeaning the generator should work in inference mode during the discriminator updates and vice versa. This is also how [this DCGAN implementation](https://towardsdatascience.com/implementing-a-generative-adversarial-network-gan-dcgan-to-draw-human-faces-8291616904a) handles batch norm.\r\n\r\nShould TFGAN handle this the same way? I think it's easiest to explain this with some (pseudo) code:\r\n```patch\r\ncommit b2b6fbc857325298de402eef589281dcebd5ae5c\r\nAuthor: Lukas Geiger <lukas.geiger94@gmail.com>\r\nDate:   Sun Apr 1 14:39:53 2018 +0200\r\n\r\n    is_training\r\n\r\ndiff --git a/tensorflow/contrib/gan/python/train.py b/tensorflow/contrib/gan/python/train.py\r\nindex 73acd05b60..8cf2b316aa 100644\r\n--- a/tensorflow/contrib/gan/python/train.py\r\n+++ b/tensorflow/contrib/gan/python/train.py\r\n@@ -31,21 +31,26 @@ from __future__ import absolute_import\r\n from __future__ import division\r\n from __future__ import print_function\r\n\r\n+import functools\r\n+\r\n from tensorflow.contrib.framework.python.ops import variables as variables_lib\r\n from tensorflow.contrib.gan.python import losses as tfgan_losses\r\n from tensorflow.contrib.gan.python import namedtuples\r\n from tensorflow.contrib.slim.python.slim import learning as slim_learning\r\n from tensorflow.contrib.training.python.training import training\r\n+from tensorflow.python.framework import dtypes\r\n from tensorflow.python.framework import ops\r\n from tensorflow.python.ops import array_ops\r\n from tensorflow.python.ops import check_ops\r\n from tensorflow.python.ops import init_ops\r\n+from tensorflow.python.ops import math_ops\r\n from tensorflow.python.ops import variable_scope\r\n from tensorflow.python.ops.distributions import distribution as ds\r\n from tensorflow.python.ops.losses import losses\r\n from tensorflow.python.training import session_run_hook\r\n from tensorflow.python.training import sync_replicas_optimizer\r\n from tensorflow.python.training import training_util\r\n+from tensorflow.python.util import tf_inspect as inspect\r\n\r\n\r\n __all__ = [\r\n@@ -101,6 +106,14 @@ def gan_model(\r\n     ValueError: If the generator outputs a Tensor that isn't the same shape as\r\n       `real_data`.\r\n   \"\"\"\r\n+  is_generator_training = array_ops.placeholder(\r\n+      dtype=dtypes.bool, name='IS_GENERATOR_TRAINING')\r\n+  if 'is_training' in inspect.getargspec(generator_fn).args:\r\n+    generator_fn = functools.partial(\r\n+        generator_fn, is_training=is_generator_training)\r\n+  if 'is_training' in inspect.getargspec(discriminator_fn).args:\r\n+    discriminator_fn = functools.partial(\r\n+        discriminator_fn, is_training=math_ops.logical_not(is_generator_training))\r\n   # Create models\r\n   with variable_scope.variable_scope(generator_scope) as gen_scope:\r\n     generator_inputs = _convert_tensor_or_l_or_d(generator_inputs)\r\n@@ -792,7 +805,7 @@ def gan_train_ops(\r\n class RunTrainOpsHook(session_run_hook.SessionRunHook):\r\n   \"\"\"A hook to run train ops a fixed number of times.\"\"\"\r\n\r\n-  def __init__(self, train_ops, train_steps):\r\n+  def __init__(self, train_ops, train_steps, is_generator=True):\r\n     \"\"\"Run train ops a certain number of times.\r\n\r\n     Args:\r\n@@ -803,10 +816,11 @@ class RunTrainOpsHook(session_run_hook.SessionRunHook):\r\n       train_ops = [train_ops]\r\n     self._train_ops = train_ops\r\n     self._train_steps = train_steps\r\n+    self._is_generator = is_generator\r\n\r\n   def before_run(self, run_context):\r\n     for _ in range(self._train_steps):\r\n-      run_context.session.run(self._train_ops)\r\n+      run_context.session.run(self._train_ops, feed_dict={'IS_GENERATOR_TRAINING:0': self._is_generator})\r\n\r\n\r\n def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1)):\r\n@@ -821,9 +835,11 @@ def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1)):\r\n   \"\"\"\r\n   def get_hooks(train_ops):\r\n     generator_hook = RunTrainOpsHook(train_ops.generator_train_op,\r\n-                                     train_steps.generator_train_steps)\r\n+                                     train_steps.generator_train_steps,\r\n+                                     is_generator=True)\r\n     discriminator_hook = RunTrainOpsHook(train_ops.discriminator_train_op,\r\n-                                         train_steps.discriminator_train_steps)\r\n+                                         train_steps.discriminator_train_steps,\r\n+                                         is_generator=False)\r\n     return [generator_hook, discriminator_hook]\r\n   return get_hooks\r\n\r\n```"}