{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/189319848", "html_url": "https://github.com/tensorflow/tensorflow/issues/435#issuecomment-189319848", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/435", "id": 189319848, "node_id": "MDEyOklzc3VlQ29tbWVudDE4OTMxOTg0OA==", "user": {"login": "fxsuper", "id": 16196995, "node_id": "MDQ6VXNlcjE2MTk2OTk1", "avatar_url": "https://avatars1.githubusercontent.com/u/16196995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fxsuper", "html_url": "https://github.com/fxsuper", "followers_url": "https://api.github.com/users/fxsuper/followers", "following_url": "https://api.github.com/users/fxsuper/following{/other_user}", "gists_url": "https://api.github.com/users/fxsuper/gists{/gist_id}", "starred_url": "https://api.github.com/users/fxsuper/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fxsuper/subscriptions", "organizations_url": "https://api.github.com/users/fxsuper/orgs", "repos_url": "https://api.github.com/users/fxsuper/repos", "events_url": "https://api.github.com/users/fxsuper/events{/privacy}", "received_events_url": "https://api.github.com/users/fxsuper/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-26T15:18:55Z", "updated_at": "2016-02-26T15:18:55Z", "author_association": "NONE", "body_html": "<p>I benchmarked the new <code>tf.cross</code> function against my own implementation written in python TensorFlow (see my earlier comment on Jan 12). The performance is actually worse on the CPU, which is rather surprising. Using this snippet of code:</p>\n<pre><code>NUM_DIMENSIONS = 3\nBATCH_SIZE = 128\nNUM_STEPS = 900\n\nnpr.seed(1)\nmats = npr.rand(NUM_STEPS, BATCH_SIZE, NUM_DIMENSIONS).astype('float32')\nbase = npr.rand(BATCH_SIZE, NUM_DIMENSIONS).astype('float32')\n\ncrosses = [base]\nfor mat in mats:\n    with tf.device('/cpu:0'):\n        new_cross = tf.cross(mat, crosses[-1])\n    crosses.append(new_cross)\n</code></pre>\n<p>And evaluating the last element of <code>crosses</code>, i.e. <code>crosses[-1].eval()</code>, takes about 0.1 seconds on a Xeon E5-2643 v3 with 4 threads enabled. In comparison my own implementation runs in 0.06 seconds despite being written in high-level TF. This is benchmarked on multiple runs and averaged, with very small standard error.</p>\n<p>On the GPU (Titan X) the situation is different. The new <code>tf.cross</code> runs in 0.025 seconds, while my implementation runs in 0.22 seconds.</p>\n<p>The poor CPU performance seems surprising. Any thoughts on why and if it may be improved?</p>", "body_text": "I benchmarked the new tf.cross function against my own implementation written in python TensorFlow (see my earlier comment on Jan 12). The performance is actually worse on the CPU, which is rather surprising. Using this snippet of code:\nNUM_DIMENSIONS = 3\nBATCH_SIZE = 128\nNUM_STEPS = 900\n\nnpr.seed(1)\nmats = npr.rand(NUM_STEPS, BATCH_SIZE, NUM_DIMENSIONS).astype('float32')\nbase = npr.rand(BATCH_SIZE, NUM_DIMENSIONS).astype('float32')\n\ncrosses = [base]\nfor mat in mats:\n    with tf.device('/cpu:0'):\n        new_cross = tf.cross(mat, crosses[-1])\n    crosses.append(new_cross)\n\nAnd evaluating the last element of crosses, i.e. crosses[-1].eval(), takes about 0.1 seconds on a Xeon E5-2643 v3 with 4 threads enabled. In comparison my own implementation runs in 0.06 seconds despite being written in high-level TF. This is benchmarked on multiple runs and averaged, with very small standard error.\nOn the GPU (Titan X) the situation is different. The new tf.cross runs in 0.025 seconds, while my implementation runs in 0.22 seconds.\nThe poor CPU performance seems surprising. Any thoughts on why and if it may be improved?", "body": "I benchmarked the new `tf.cross` function against my own implementation written in python TensorFlow (see my earlier comment on Jan 12). The performance is actually worse on the CPU, which is rather surprising. Using this snippet of code:\n\n```\nNUM_DIMENSIONS = 3\nBATCH_SIZE = 128\nNUM_STEPS = 900\n\nnpr.seed(1)\nmats = npr.rand(NUM_STEPS, BATCH_SIZE, NUM_DIMENSIONS).astype('float32')\nbase = npr.rand(BATCH_SIZE, NUM_DIMENSIONS).astype('float32')\n\ncrosses = [base]\nfor mat in mats:\n    with tf.device('/cpu:0'):\n        new_cross = tf.cross(mat, crosses[-1])\n    crosses.append(new_cross)\n```\n\nAnd evaluating the last element of `crosses`, i.e. `crosses[-1].eval()`, takes about 0.1 seconds on a Xeon E5-2643 v3 with 4 threads enabled. In comparison my own implementation runs in 0.06 seconds despite being written in high-level TF. This is benchmarked on multiple runs and averaged, with very small standard error.\n\nOn the GPU (Titan X) the situation is different. The new `tf.cross` runs in 0.025 seconds, while my implementation runs in 0.22 seconds.\n\nThe poor CPU performance seems surprising. Any thoughts on why and if it may be improved?\n"}