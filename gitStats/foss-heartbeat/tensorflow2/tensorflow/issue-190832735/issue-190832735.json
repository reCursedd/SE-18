{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5757", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5757/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5757/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5757/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5757", "id": 190832735, "node_id": "MDU6SXNzdWUxOTA4MzI3MzU=", "number": 5757, "title": "CPU slowdown with Quantized Eight bit graphs", "user": {"login": "vade", "id": 65011, "node_id": "MDQ6VXNlcjY1MDEx", "avatar_url": "https://avatars1.githubusercontent.com/u/65011?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vade", "html_url": "https://github.com/vade", "followers_url": "https://api.github.com/users/vade/followers", "following_url": "https://api.github.com/users/vade/following{/other_user}", "gists_url": "https://api.github.com/users/vade/gists{/gist_id}", "starred_url": "https://api.github.com/users/vade/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vade/subscriptions", "organizations_url": "https://api.github.com/users/vade/orgs", "repos_url": "https://api.github.com/users/vade/repos", "events_url": "https://api.github.com/users/vade/events{/privacy}", "received_events_url": "https://api.github.com/users/vade/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2016-11-21T20:53:19Z", "updated_at": "2018-01-29T22:37:53Z", "closed_at": "2018-01-29T22:37:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In attempts to highly optimize my Tensorflow client application (and TF install) for consumer desktop hardware i've noticed (and noticed in other bug reports) that quantized eight bit graphs appear to run very slow.  My goal is to match the realtime 1 batch (1 x 299 x 299 x3 ) iOS performance that the Camera Example gets, yet I can't get a Desktop CPU compile of TF to get lower than roughly 150ms per frame, where in reality close to 16ms per frame is needed for roughly 60hz, or 33ms for 30hz performance. It appears somehow the iOS / ArmV7 build is able to achieve this performance unless I am missing something!</p>\n<p>From the discussion group, I was asked by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=161459\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petewarden\">@petewarden</a> to start a bug based on findings</p>\n<p>Thread here: <a href=\"https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&amp;utm_source=footer#!msg/discuss/PJwgfoeNIKs/jiegynxLBAAJ\" rel=\"nofollow\">https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&amp;utm_source=footer#!msg/discuss/PJwgfoeNIKs/jiegynxLBAAJ</a></p>\n<p>Briefly, I've added Stat Tracing to the Image Label example.  Modified source is here:</p>\n<p><a href=\"https://gist.github.com/vade/18d7e72f633f9479c5080a251661ebd9\">https://gist.github.com/vade/18d7e72f633f9479c5080a251661ebd9</a></p>\n<p>Ive downloaded compiled tensor flow with the following bazel build commands, referenced from the makefile for iOS which speeds things up just a bit more than the standard compile:</p>\n<p><code>bazel build -c opt --copt=-mavx --cxxopt=-fno-exceptions --cxxopt=--std=c++11 --cxxopt=-DNDEBUG --cxxopt=-DNOTFDBG --cxxopt=-O2 --cxxopt=-DUSE_GEMM_FOR_CONV //tensorflow:libtensorflow_cc.so</code></p>\n<p>I then compiled Image Label via the standard bazel command:</p>\n<p><code> bazel build tensorflow/examples/label_image/...</code></p>\n<p>And ran it with 4 graphs:</p>\n<ul>\n<li>Standard InceptionV3</li>\n<li>InceptionV3 run through Inference Optimizer Script</li>\n<li>InceptionV3 run through Inference Optimizer and Quantizer in Weighted Rounding mode</li>\n<li>InceptionV3 run through Inference Optimizer and Quantizer in eight bit mode</li>\n</ul>\n<p>The output of the runs are documented here, in order:</p>\n<ul>\n<li><a href=\"https://gist.github.com/vade/a7d95da155c25dc8134f7cda8168e540\">https://gist.github.com/vade/a7d95da155c25dc8134f7cda8168e540</a></li>\n<li><a href=\"https://gist.github.com/vade/71af1cbd38864cb176ce64bcafb934de\">https://gist.github.com/vade/71af1cbd38864cb176ce64bcafb934de</a></li>\n<li><a href=\"https://gist.github.com/vade/e1923d7e7a9abfe1d8c912cc5d36a763\">https://gist.github.com/vade/e1923d7e7a9abfe1d8c912cc5d36a763</a></li>\n<li><a href=\"https://gist.github.com/vade/1d9d5e102878cfb42f9c02a4200b3a50\">https://gist.github.com/vade/1d9d5e102878cfb42f9c02a4200b3a50</a></li>\n</ul>\n<p>Note the time for the Quantized Eight bit mode is roughly 2x longer than previous runs.</p>\n<p>As a second set of data, my custom C++ app which uses the same lib_tensorfow_cc.so nets similar results to the benchmark :\u00a0</p>\n<ul>\n<li>\n<p>InceptionV3 (no optimizations or quantizations)</p>\n</li>\n<li>\n<p>222 frames took 32.598143 seconds</p>\n</li>\n<li>\n<p><a href=\"https://gist.github.com/vade/77a9314a5c7a5bda9b4a2c90f691a98e\">https://gist.github.com/vade/77a9314a5c7a5bda9b4a2c90f691a98e</a></p>\n</li>\n<li>\n<p>InceptionV3 (Inference Optimizations - no quantizations)</p>\n</li>\n<li>\n<p>222 frames took 28.129690 seconds</p>\n</li>\n<li>\n<p><a href=\"https://gist.github.com/vade/1c5dc51015f5a0fa24f4e0a7209cabf9\">https://gist.github.com/vade/1c5dc51015f5a0fa24f4e0a7209cabf9</a></p>\n</li>\n<li>\n<p>InceptionV3 (Inference Optimizations &amp; Quantizations Rounded)</p>\n</li>\n<li>\n<p>222 frames took 25.201791 seconds</p>\n</li>\n<li>\n<p><a href=\"https://gist.github.com/vade/ad8a2c42c5fbcf9be9f074c95d2e95ae\">https://gist.github.com/vade/ad8a2c42c5fbcf9be9f074c95d2e95ae</a></p>\n</li>\n<li>\n<p>InceptionV3 (Inference Optimizations &amp; Quantizations Eightbit)</p>\n</li>\n<li>\n<p>222 frames took 63.174700 seconds</p>\n</li>\n<li>\n<p><a href=\"https://gist.github.com/vade/d6dcce06861bf8932446ae5ed33d93bb\">https://gist.github.com/vade/d6dcce06861bf8932446ae5ed33d93bb</a></p>\n</li>\n</ul>\n<p>Operating System:</p>\n<p><code>Mac OS X 10.12.1 2.8 GHz Intel Core i7 16GB Ram Xcode 8.1 / Command Line Tools from 7.3.1 and enabled via xcselect </code><br>\nIf installed from source, provide</p>\n<ol>\n<li>The commit hash</li>\n</ol>\n<p><code>41285cf7a11fa3a2c2ead6b6e9adcec4232b18ad</code></p>\n<ol start=\"2\">\n<li>The output of</li>\n</ol>\n<p><code>Build label: 0.4.0-homebrew Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar Build time: Wed Nov 2 19:18:00 2016 (1478114280) Build timestamp: 1478114280 Build timestamp as int: 1478114280</code></p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)`</h3>\n<p>See above for source code for minimally modified label image source.</p>\n<h3>What other attempted solutions have you tried?</h3>\n<p>Attempted to run cuda but am targeting consumer desktop systems and would like similar performance to iOS targets for realtime or better than realtime performance for InceptionV3 / pool_3 feature vector determination and possibly labelling / classification.</p>\n<p>For my system, cuda compilation netted similar results to CPU, although admittedly I did not enable batch sizes larger than 1. However, I think this is moot because iOS appears to be able to get realtime labelling and desktop cant, (is roughly 10x slower)</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>Logs and links provided in preamble  / description</p>", "body_text": "In attempts to highly optimize my Tensorflow client application (and TF install) for consumer desktop hardware i've noticed (and noticed in other bug reports) that quantized eight bit graphs appear to run very slow.  My goal is to match the realtime 1 batch (1 x 299 x 299 x3 ) iOS performance that the Camera Example gets, yet I can't get a Desktop CPU compile of TF to get lower than roughly 150ms per frame, where in reality close to 16ms per frame is needed for roughly 60hz, or 33ms for 30hz performance. It appears somehow the iOS / ArmV7 build is able to achieve this performance unless I am missing something!\nFrom the discussion group, I was asked by @petewarden to start a bug based on findings\nThread here: https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/PJwgfoeNIKs/jiegynxLBAAJ\nBriefly, I've added Stat Tracing to the Image Label example.  Modified source is here:\nhttps://gist.github.com/vade/18d7e72f633f9479c5080a251661ebd9\nIve downloaded compiled tensor flow with the following bazel build commands, referenced from the makefile for iOS which speeds things up just a bit more than the standard compile:\nbazel build -c opt --copt=-mavx --cxxopt=-fno-exceptions --cxxopt=--std=c++11 --cxxopt=-DNDEBUG --cxxopt=-DNOTFDBG --cxxopt=-O2 --cxxopt=-DUSE_GEMM_FOR_CONV //tensorflow:libtensorflow_cc.so\nI then compiled Image Label via the standard bazel command:\n bazel build tensorflow/examples/label_image/...\nAnd ran it with 4 graphs:\n\nStandard InceptionV3\nInceptionV3 run through Inference Optimizer Script\nInceptionV3 run through Inference Optimizer and Quantizer in Weighted Rounding mode\nInceptionV3 run through Inference Optimizer and Quantizer in eight bit mode\n\nThe output of the runs are documented here, in order:\n\nhttps://gist.github.com/vade/a7d95da155c25dc8134f7cda8168e540\nhttps://gist.github.com/vade/71af1cbd38864cb176ce64bcafb934de\nhttps://gist.github.com/vade/e1923d7e7a9abfe1d8c912cc5d36a763\nhttps://gist.github.com/vade/1d9d5e102878cfb42f9c02a4200b3a50\n\nNote the time for the Quantized Eight bit mode is roughly 2x longer than previous runs.\nAs a second set of data, my custom C++ app which uses the same lib_tensorfow_cc.so nets similar results to the benchmark :\u00a0\n\n\nInceptionV3 (no optimizations or quantizations)\n\n\n222 frames took 32.598143 seconds\n\n\nhttps://gist.github.com/vade/77a9314a5c7a5bda9b4a2c90f691a98e\n\n\nInceptionV3 (Inference Optimizations - no quantizations)\n\n\n222 frames took 28.129690 seconds\n\n\nhttps://gist.github.com/vade/1c5dc51015f5a0fa24f4e0a7209cabf9\n\n\nInceptionV3 (Inference Optimizations & Quantizations Rounded)\n\n\n222 frames took 25.201791 seconds\n\n\nhttps://gist.github.com/vade/ad8a2c42c5fbcf9be9f074c95d2e95ae\n\n\nInceptionV3 (Inference Optimizations & Quantizations Eightbit)\n\n\n222 frames took 63.174700 seconds\n\n\nhttps://gist.github.com/vade/d6dcce06861bf8932446ae5ed33d93bb\n\n\nOperating System:\nMac OS X 10.12.1 2.8 GHz Intel Core i7 16GB Ram Xcode 8.1 / Command Line Tools from 7.3.1 and enabled via xcselect \nIf installed from source, provide\n\nThe commit hash\n\n41285cf7a11fa3a2c2ead6b6e9adcec4232b18ad\n\nThe output of\n\nBuild label: 0.4.0-homebrew Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar Build time: Wed Nov 2 19:18:00 2016 (1478114280) Build timestamp: 1478114280 Build timestamp as int: 1478114280\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)`\nSee above for source code for minimally modified label image source.\nWhat other attempted solutions have you tried?\nAttempted to run cuda but am targeting consumer desktop systems and would like similar performance to iOS targets for realtime or better than realtime performance for InceptionV3 / pool_3 feature vector determination and possibly labelling / classification.\nFor my system, cuda compilation netted similar results to CPU, although admittedly I did not enable batch sizes larger than 1. However, I think this is moot because iOS appears to be able to get realtime labelling and desktop cant, (is roughly 10x slower)\nLogs or other output that would be helpful\nLogs and links provided in preamble  / description", "body": "In attempts to highly optimize my Tensorflow client application (and TF install) for consumer desktop hardware i've noticed (and noticed in other bug reports) that quantized eight bit graphs appear to run very slow.  My goal is to match the realtime 1 batch (1 x 299 x 299 x3 ) iOS performance that the Camera Example gets, yet I can't get a Desktop CPU compile of TF to get lower than roughly 150ms per frame, where in reality close to 16ms per frame is needed for roughly 60hz, or 33ms for 30hz performance. It appears somehow the iOS / ArmV7 build is able to achieve this performance unless I am missing something!\r\n\r\nFrom the discussion group, I was asked by @petewarden to start a bug based on findings\r\n\r\nThread here: https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/PJwgfoeNIKs/jiegynxLBAAJ\r\n\r\nBriefly, I've added Stat Tracing to the Image Label example.  Modified source is here:\r\n\r\nhttps://gist.github.com/vade/18d7e72f633f9479c5080a251661ebd9\r\n\r\nIve downloaded compiled tensor flow with the following bazel build commands, referenced from the makefile for iOS which speeds things up just a bit more than the standard compile:\r\n\r\n` bazel build -c opt --copt=-mavx --cxxopt=-fno-exceptions --cxxopt=--std=c++11 --cxxopt=-DNDEBUG --cxxopt=-DNOTFDBG --cxxopt=-O2 --cxxopt=-DUSE_GEMM_FOR_CONV //tensorflow:libtensorflow_cc.so `\r\n\r\nI then compiled Image Label via the standard bazel command:\r\n\r\n` bazel build tensorflow/examples/label_image/...`\r\n\r\nAnd ran it with 4 graphs:\r\n\r\n* Standard InceptionV3\r\n* InceptionV3 run through Inference Optimizer Script\r\n* InceptionV3 run through Inference Optimizer and Quantizer in Weighted Rounding mode\r\n* InceptionV3 run through Inference Optimizer and Quantizer in eight bit mode\r\n\r\nThe output of the runs are documented here, in order:\r\n\r\n* https://gist.github.com/vade/a7d95da155c25dc8134f7cda8168e540\r\n* https://gist.github.com/vade/71af1cbd38864cb176ce64bcafb934de\r\n* https://gist.github.com/vade/e1923d7e7a9abfe1d8c912cc5d36a763\r\n* https://gist.github.com/vade/1d9d5e102878cfb42f9c02a4200b3a50\r\n\r\nNote the time for the Quantized Eight bit mode is roughly 2x longer than previous runs.\r\n\r\nAs a second set of data, my custom C++ app which uses the same lib_tensorfow_cc.so nets similar results to the benchmark :\u00a0\r\n\r\n\r\n* InceptionV3 (no optimizations or quantizations) \r\n* 222 frames took 32.598143 seconds\r\n* https://gist.github.com/vade/77a9314a5c7a5bda9b4a2c90f691a98e\r\n\r\n\r\n* InceptionV3 (Inference Optimizations - no quantizations)\r\n* 222 frames took 28.129690 seconds\r\n* https://gist.github.com/vade/1c5dc51015f5a0fa24f4e0a7209cabf9\r\n\r\n\r\n* InceptionV3 (Inference Optimizations & Quantizations Rounded)\r\n* 222 frames took 25.201791 seconds\r\n* https://gist.github.com/vade/ad8a2c42c5fbcf9be9f074c95d2e95ae\r\n\r\n\r\n* InceptionV3 (Inference Optimizations & Quantizations Eightbit)\r\n* 222 frames took 63.174700 seconds\r\n* https://gist.github.com/vade/d6dcce06861bf8932446ae5ed33d93bb\r\n\r\n\r\nOperating System:\r\n\r\n`Mac OS X 10.12.1\r\n2.8 GHz Intel Core i7\r\n16GB Ram\r\nXcode 8.1 / Command Line Tools from 7.3.1 and enabled via xcselect\r\n`\r\nIf installed from source, provide \r\n\r\n1. The commit hash\r\n\r\n`41285cf7a11fa3a2c2ead6b6e9adcec4232b18ad`\r\n\r\n2. The output of\r\n\r\n`Build label: 0.4.0-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 19:18:00 2016 (1478114280)\r\nBuild timestamp: 1478114280\r\nBuild timestamp as int: 1478114280`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)`\r\n\r\nSee above for source code for minimally modified label image source.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nAttempted to run cuda but am targeting consumer desktop systems and would like similar performance to iOS targets for realtime or better than realtime performance for InceptionV3 / pool_3 feature vector determination and possibly labelling / classification.\r\n\r\nFor my system, cuda compilation netted similar results to CPU, although admittedly I did not enable batch sizes larger than 1. However, I think this is moot because iOS appears to be able to get realtime labelling and desktop cant, (is roughly 10x slower)\r\n\r\n### Logs or other output that would be helpful\r\nLogs and links provided in preamble  / description"}