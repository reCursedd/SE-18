{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/346596454", "html_url": "https://github.com/tensorflow/tensorflow/issues/14809#issuecomment-346596454", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14809", "id": 346596454, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NjU5NjQ1NA==", "user": {"login": "nikonikolov", "id": 11044035, "node_id": "MDQ6VXNlcjExMDQ0MDM1", "avatar_url": "https://avatars3.githubusercontent.com/u/11044035?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikonikolov", "html_url": "https://github.com/nikonikolov", "followers_url": "https://api.github.com/users/nikonikolov/followers", "following_url": "https://api.github.com/users/nikonikolov/following{/other_user}", "gists_url": "https://api.github.com/users/nikonikolov/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikonikolov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikonikolov/subscriptions", "organizations_url": "https://api.github.com/users/nikonikolov/orgs", "repos_url": "https://api.github.com/users/nikonikolov/repos", "events_url": "https://api.github.com/users/nikonikolov/events{/privacy}", "received_events_url": "https://api.github.com/users/nikonikolov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-23T11:41:40Z", "updated_at": "2017-11-23T11:41:40Z", "author_association": "NONE", "body_html": "<p>Thanks for the quick reply!</p>\n<ol>\n<li>It does not seem to work for me.</li>\n</ol>\n<pre><code>&gt;&gt;&gt; data = tf.placeholder(tf.float32, shape=[None, 2, 3])\n&gt;&gt;&gt; norm = tf.layers.batch_normalization(data, axis=0)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 586, in batch_normalization\n    return layer.apply(inputs, training=training)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/base.py\", line 559, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 179, in build\n    input_shape)\nValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(None), Dimension(2), Dimension(3)]))\n</code></pre>\n<p>In the source code, it seems that the batch dimension must be fixed: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/b5df90f91cde6eb12af9cbe818bd2cf4a9bcc687/tensorflow/python/layers/normalization.py#L176-L179\">tensorflow/tensorflow/python/layers/normalization.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 176 to 179\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/b5df90f91cde6eb12af9cbe818bd2cf4a9bcc687\">b5df90f</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L176\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"176\"></td>\n          <td id=\"LC176\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> param_dim <span class=\"pl-k\">=</span> input_shape[axis] </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L177\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"177\"></td>\n          <td id=\"LC177\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> param_dim.value: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L178\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"178\"></td>\n          <td id=\"LC178\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Input has undefined `axis` dimension. Input shape: <span class=\"pl-pds\">'</span></span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L179\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"179\"></td>\n          <td id=\"LC179\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                    input_shape) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n Am I doing something wrong?</p>\n<ol start=\"2\">\n<li>Out of curiosity, is it slower because computing the input for the next layer takes more time when you do the update internally? In any case, there is often a trade-off between code readability and performance, so in my opinion it would definitely be much better if there is some parameter that controls if the update is done internally or not, rather than leaving no choice for the programmer. It would also be a good, if possible, to provide some approximate metric of the speed up, when the update is not done internally, so the programmer can choose for himself.</li>\n</ol>", "body_text": "Thanks for the quick reply!\n\nIt does not seem to work for me.\n\n>>> data = tf.placeholder(tf.float32, shape=[None, 2, 3])\n>>> norm = tf.layers.batch_normalization(data, axis=0)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 586, in batch_normalization\n    return layer.apply(inputs, training=training)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/base.py\", line 559, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 179, in build\n    input_shape)\nValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(None), Dimension(2), Dimension(3)]))\n\nIn the source code, it seems that the batch dimension must be fixed: \n  \n    \n      tensorflow/tensorflow/python/layers/normalization.py\n    \n    \n        Lines 176 to 179\n      in\n      b5df90f\n    \n    \n    \n    \n\n        \n          \n           param_dim = input_shape[axis] \n        \n\n        \n          \n           if not param_dim.value: \n        \n\n        \n          \n             raise ValueError('Input has undefined `axis` dimension. Input shape: ', \n        \n\n        \n          \n                              input_shape) \n        \n    \n  \n\n Am I doing something wrong?\n\nOut of curiosity, is it slower because computing the input for the next layer takes more time when you do the update internally? In any case, there is often a trade-off between code readability and performance, so in my opinion it would definitely be much better if there is some parameter that controls if the update is done internally or not, rather than leaving no choice for the programmer. It would also be a good, if possible, to provide some approximate metric of the speed up, when the update is not done internally, so the programmer can choose for himself.", "body": "Thanks for the quick reply!\r\n\r\n1. It does not seem to work for me. \r\n```\r\n>>> data = tf.placeholder(tf.float32, shape=[None, 2, 3])\r\n>>> norm = tf.layers.batch_normalization(data, axis=0)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 586, in batch_normalization\r\n    return layer.apply(inputs, training=training)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/base.py\", line 671, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/base.py\", line 559, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 179, in build\r\n    input_shape)\r\nValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(None), Dimension(2), Dimension(3)]))\r\n```\r\nIn the source code, it seems that the batch dimension must be fixed: https://github.com/tensorflow/tensorflow/blob/b5df90f91cde6eb12af9cbe818bd2cf4a9bcc687/tensorflow/python/layers/normalization.py#L176-L179 Am I doing something wrong?\r\n\r\n\r\n2. Out of curiosity, is it slower because computing the input for the next layer takes more time when you do the update internally? In any case, there is often a trade-off between code readability and performance, so in my opinion it would definitely be much better if there is some parameter that controls if the update is done internally or not, rather than leaving no choice for the programmer. It would also be a good, if possible, to provide some approximate metric of the speed up, when the update is not done internally, so the programmer can choose for himself. "}