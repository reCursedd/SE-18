{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11540", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11540/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11540/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11540/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11540", "id": 243287195, "node_id": "MDU6SXNzdWUyNDMyODcxOTU=", "number": 11540, "title": "Unclear about how to make AttentionWrapper work", "user": {"login": "zhedongzheng", "id": 16261331, "node_id": "MDQ6VXNlcjE2MjYxMzMx", "avatar_url": "https://avatars2.githubusercontent.com/u/16261331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhedongzheng", "html_url": "https://github.com/zhedongzheng", "followers_url": "https://api.github.com/users/zhedongzheng/followers", "following_url": "https://api.github.com/users/zhedongzheng/following{/other_user}", "gists_url": "https://api.github.com/users/zhedongzheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhedongzheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhedongzheng/subscriptions", "organizations_url": "https://api.github.com/users/zhedongzheng/orgs", "repos_url": "https://api.github.com/users/zhedongzheng/repos", "events_url": "https://api.github.com/users/zhedongzheng/events{/privacy}", "received_events_url": "https://api.github.com/users/zhedongzheng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-07-17T02:55:54Z", "updated_at": "2017-07-18T14:48:31Z", "closed_at": "2017-07-18T08:50:06Z", "author_association": "NONE", "body_html": "<p>Hello, I am using the TensorFlow 1.2.1 and I want to implement a Seq2Seq model with Attention through tf.contrib.seq2seq. I didn't find enough information of how to use <code>tf.contrib.seq2seq.AttentionWrapper</code> on documentation, however I successfully find how to use that in the <a href=\"https://github.com/tensorflow/nmt#attention-wrapper-api\">NMT tutorial</a>.</p>\n<p>Before adding Attention, I already make the Seq2Seq model work, however, it seems very difficult to insert the AttentionWrapper by simply wrapping the decoder cell. It is not fair to ask anyone to read my own code, which is quite long. So here I provide the minimal code to demonstrate the problem:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.layers.core <span class=\"pl-k\">import</span> Dense\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> INPUT</span>\nX <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>])\nY <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>])\nX_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>])\nY_seq_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ENCODER         </span>\nencoder_out, encoder_state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>), \n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.contrib.layers.embed_sequence(X, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len,\n    <span class=\"pl-v\">dtype</span> <span class=\"pl-k\">=</span> tf.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ATTENTION</span>\nattention_mechanism <span class=\"pl-k\">=</span> tf.contrib.seq2seq.LuongAttention(\n    <span class=\"pl-v\">num_units</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, \n    <span class=\"pl-v\">memory</span> <span class=\"pl-k\">=</span> encoder_out,\n    <span class=\"pl-v\">memory_sequence_length</span> <span class=\"pl-k\">=</span> X_seq_len)\n\ndecoder_cell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">128</span>),\n    <span class=\"pl-v\">attention_mechanism</span> <span class=\"pl-k\">=</span> attention_mechanism,\n    <span class=\"pl-v\">attention_layer_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DECODER</span>\nY_vocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\ndecoder_embedding <span class=\"pl-k\">=</span> tf.Variable(tf.random_uniform([Y_vocab_size, <span class=\"pl-c1\">128</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>))\nprojection_layer <span class=\"pl-k\">=</span> Dense(Y_vocab_size)\n\ntraining_helper <span class=\"pl-k\">=</span> tf.contrib.seq2seq.TrainingHelper(\n    <span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(decoder_embedding, Y),\n    <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> Y_seq_len,\n    <span class=\"pl-v\">time_major</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)\ntraining_decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BasicDecoder(\n    <span class=\"pl-v\">cell</span> <span class=\"pl-k\">=</span> decoder_cell,\n    <span class=\"pl-v\">helper</span> <span class=\"pl-k\">=</span> training_helper,\n    <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> encoder_state,\n    <span class=\"pl-v\">output_layer</span> <span class=\"pl-k\">=</span> projection_layer)\ntraining_decoder_output, _, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n    <span class=\"pl-v\">decoder</span> <span class=\"pl-k\">=</span> training_decoder,\n    <span class=\"pl-v\">impute_finished</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">maximum_iterations</span> <span class=\"pl-k\">=</span> tf.reduce_max(Y_seq_len))\ntraining_logits <span class=\"pl-k\">=</span> training_decoder_output.rnn_output\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> LOSS</span>\nmasks <span class=\"pl-k\">=</span> tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nloss <span class=\"pl-k\">=</span> tf.contrib.seq2seq.sequence_loss(<span class=\"pl-v\">logits</span> <span class=\"pl-k\">=</span> training_logits, <span class=\"pl-v\">targets</span> <span class=\"pl-k\">=</span> Y, <span class=\"pl-v\">weights</span> <span class=\"pl-k\">=</span> masks)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> BACKWARD</span>\nparams <span class=\"pl-k\">=</span> tf.trainable_variables()\ngradients <span class=\"pl-k\">=</span> tf.gradients(loss, params)\nclipped_gradients, _ <span class=\"pl-k\">=</span> tf.clip_by_global_norm(gradients, <span class=\"pl-c1\">5.0</span>)\ntrain_op <span class=\"pl-k\">=</span> tf.train.AdamOptimizer().apply_gradients(<span class=\"pl-c1\">zip</span>(clipped_gradients, params))</pre></div>\n<p>The graph cannot be built, due to error message</p>\n<pre><code>Traceback (most recent call last):\n  File \"seq2seq_attn.py\", line 44, in &lt;module&gt;\n    maximum_iterations = tf.reduce_max(Y_seq_len))\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2766, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2595, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2545, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 234, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 139, in step\n    cell_outputs, cell_state = self._cell(inputs, state)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 180, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 439, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 677, in call\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\nAttributeError: 'LSTMStateTuple' object has no attribute 'attention'\n</code></pre>\n<p>I have also tried using GRU instead of LSTM, the error still exists</p>", "body_text": "Hello, I am using the TensorFlow 1.2.1 and I want to implement a Seq2Seq model with Attention through tf.contrib.seq2seq. I didn't find enough information of how to use tf.contrib.seq2seq.AttentionWrapper on documentation, however I successfully find how to use that in the NMT tutorial.\nBefore adding Attention, I already make the Seq2Seq model work, however, it seems very difficult to insert the AttentionWrapper by simply wrapping the decoder cell. It is not fair to ask anyone to read my own code, which is quite long. So here I provide the minimal code to demonstrate the problem:\nimport tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\n\n# INPUT\nX = tf.placeholder(tf.int32, [None, None])\nY = tf.placeholder(tf.int32, [None, None])\nX_seq_len = tf.placeholder(tf.int32, [None])\nY_seq_len = tf.placeholder(tf.int32, [None])\n\n# ENCODER         \nencoder_out, encoder_state = tf.nn.dynamic_rnn(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\n    sequence_length = X_seq_len,\n    dtype = tf.float32)\n\n# ATTENTION\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n    num_units = 128, \n    memory = encoder_out,\n    memory_sequence_length = X_seq_len)\n\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\n    attention_mechanism = attention_mechanism,\n    attention_layer_size = 128)\n\n# DECODER\nY_vocab_size = 10000\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\nprojection_layer = Dense(Y_vocab_size)\n\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\n    sequence_length = Y_seq_len,\n    time_major = False)\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\n    cell = decoder_cell,\n    helper = training_helper,\n    initial_state = encoder_state,\n    output_layer = projection_layer)\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n    decoder = training_decoder,\n    impute_finished = True,\n    maximum_iterations = tf.reduce_max(Y_seq_len))\ntraining_logits = training_decoder_output.rnn_output\n\n# LOSS\nmasks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)\nloss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)\n\n# BACKWARD\nparams = tf.trainable_variables()\ngradients = tf.gradients(loss, params)\nclipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\ntrain_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))\nThe graph cannot be built, due to error message\nTraceback (most recent call last):\n  File \"seq2seq_attn.py\", line 44, in <module>\n    maximum_iterations = tf.reduce_max(Y_seq_len))\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2766, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2595, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2545, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 234, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 139, in step\n    cell_outputs, cell_state = self._cell(inputs, state)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 180, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 439, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 677, in call\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\nAttributeError: 'LSTMStateTuple' object has no attribute 'attention'\n\nI have also tried using GRU instead of LSTM, the error still exists", "body": "Hello, I am using the TensorFlow 1.2.1 and I want to implement a Seq2Seq model with Attention through tf.contrib.seq2seq. I didn't find enough information of how to use ```tf.contrib.seq2seq.AttentionWrapper``` on documentation, however I successfully find how to use that in the [NMT tutorial](https://github.com/tensorflow/nmt#attention-wrapper-api).\r\n\r\nBefore adding Attention, I already make the Seq2Seq model work, however, it seems very difficult to insert the AttentionWrapper by simply wrapping the decoder cell. It is not fair to ask anyone to read my own code, which is quite long. So here I provide the minimal code to demonstrate the problem:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n# INPUT\r\nX = tf.placeholder(tf.int32, [None, None])\r\nY = tf.placeholder(tf.int32, [None, None])\r\nX_seq_len = tf.placeholder(tf.int32, [None])\r\nY_seq_len = tf.placeholder(tf.int32, [None])\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n# ATTENTION\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    num_units = 128, \r\n    memory = encoder_out,\r\n    memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n# DECODER\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\n\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = encoder_state,\r\n    output_layer = projection_layer)\r\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = training_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n# LOSS\r\nmasks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)\r\nloss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)\r\n\r\n# BACKWARD\r\nparams = tf.trainable_variables()\r\ngradients = tf.gradients(loss, params)\r\nclipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\r\ntrain_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))\r\n```\r\nThe graph cannot be built, due to error message\r\n```\r\nTraceback (most recent call last):\r\n  File \"seq2seq_attn.py\", line 44, in <module>\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\r\n    swap_memory=swap_memory)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2766, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2595, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2545, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 234, in body\r\n    decoder_finished) = decoder.step(time, inputs, state)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 139, in step\r\n    cell_outputs, cell_state = self._cell(inputs, state)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 180, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 439, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 677, in call\r\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\r\nAttributeError: 'LSTMStateTuple' object has no attribute 'attention'\r\n```\r\nI have also tried using GRU instead of LSTM, the error still exists"}