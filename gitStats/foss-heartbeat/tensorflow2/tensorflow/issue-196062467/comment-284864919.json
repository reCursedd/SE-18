{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284864919", "html_url": "https://github.com/tensorflow/tensorflow/issues/6360#issuecomment-284864919", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6360", "id": 284864919, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDg2NDkxOQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-07T21:28:07Z", "updated_at": "2017-03-07T21:29:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Can you get a backtrace to see where it's stuck? (gdb attach, then <code>bt</code> or <code>thread apply all bt</code> for all threads)  You can get stuck for reasons unrelated to tensorflow, for instance, I've had a model stuck because a GPU op was calling Nvidia drivers, which was trying to store some temporary files on NFS which was hanging (solution was to set <code>CUDA_CACHE_PATH</code> to local directory). You can get more logging with <code>export TF_CPP_MIN_VLOG_LEVEL=1</code>, although it's going to be a deluge. Also, I'd recommend trying to reproduce with CPU-only.</p>", "body_text": "Can you get a backtrace to see where it's stuck? (gdb attach, then bt or thread apply all bt for all threads)  You can get stuck for reasons unrelated to tensorflow, for instance, I've had a model stuck because a GPU op was calling Nvidia drivers, which was trying to store some temporary files on NFS which was hanging (solution was to set CUDA_CACHE_PATH to local directory). You can get more logging with export TF_CPP_MIN_VLOG_LEVEL=1, although it's going to be a deluge. Also, I'd recommend trying to reproduce with CPU-only.", "body": "Can you get a backtrace to see where it's stuck? (gdb attach, then `bt` or `thread apply all bt` for all threads)  You can get stuck for reasons unrelated to tensorflow, for instance, I've had a model stuck because a GPU op was calling Nvidia drivers, which was trying to store some temporary files on NFS which was hanging (solution was to set `CUDA_CACHE_PATH` to local directory). You can get more logging with `export TF_CPP_MIN_VLOG_LEVEL=1`, although it's going to be a deluge. Also, I'd recommend trying to reproduce with CPU-only."}