{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/269543896", "html_url": "https://github.com/tensorflow/tensorflow/issues/6360#issuecomment-269543896", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6360", "id": 269543896, "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTU0Mzg5Ng==", "user": {"login": "danijar", "id": 2111293, "node_id": "MDQ6VXNlcjIxMTEyOTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2111293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danijar", "html_url": "https://github.com/danijar", "followers_url": "https://api.github.com/users/danijar/followers", "following_url": "https://api.github.com/users/danijar/following{/other_user}", "gists_url": "https://api.github.com/users/danijar/gists{/gist_id}", "starred_url": "https://api.github.com/users/danijar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danijar/subscriptions", "organizations_url": "https://api.github.com/users/danijar/orgs", "repos_url": "https://api.github.com/users/danijar/repos", "events_url": "https://api.github.com/users/danijar/events{/privacy}", "received_events_url": "https://api.github.com/users/danijar/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-28T21:30:29Z", "updated_at": "2016-12-28T21:30:55Z", "author_association": "MEMBER", "body_html": "<p>You're right, here is a slightly more compact example to reproduce the issue. The writer thread increases the elements of the matrix exponentially using locking. The reader thread checks for all elements of the matrix being identical at any point, which fails. Isn't this a bug in TensorFlow's <code>use_locking</code>?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> threading\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nvariable <span class=\"pl-k\">=</span> tf.Variable(np.ones((<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)), <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nwrite_op <span class=\"pl-k\">=</span> variable.assign_add(variable, <span class=\"pl-v\">use_locking</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nsess <span class=\"pl-k\">=</span> tf.Session()\nrunning <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">writing</span>():\n  <span class=\"pl-k\">global</span> running\n  <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n    sess.run(write_op)\n    time.sleep(<span class=\"pl-c1\">0.001</span>)\n  running <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">reading</span>():\n  <span class=\"pl-k\">while</span> running:\n    value <span class=\"pl-k\">=</span> sess.run(variable)\n    <span class=\"pl-k\">assert</span> (value <span class=\"pl-k\">==</span> value.flat[<span class=\"pl-c1\">0</span>]).all()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> This fails!</span>\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n  sess.run(tf.global_variables_initializer())\n  threading.Thread(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>writing).start()\n  threading.Thread(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>reading).start()</pre></div>", "body_text": "You're right, here is a slightly more compact example to reproduce the issue. The writer thread increases the elements of the matrix exponentially using locking. The reader thread checks for all elements of the matrix being identical at any point, which fails. Isn't this a bug in TensorFlow's use_locking?\nimport threading\nimport time\nimport numpy as np\nimport tensorflow as tf\n\nvariable = tf.Variable(np.ones((100, 100)), trainable=False)\nwrite_op = variable.assign_add(variable, use_locking=True)\nsess = tf.Session()\nrunning = True\n\ndef writing():\n  global running\n  for _ in range(1000):\n    sess.run(write_op)\n    time.sleep(0.001)\n  running = False\n\ndef reading():\n  while running:\n    value = sess.run(variable)\n    assert (value == value.flat[0]).all()  # This fails!\n\nif __name__ == '__main__':\n  sess.run(tf.global_variables_initializer())\n  threading.Thread(target=writing).start()\n  threading.Thread(target=reading).start()", "body": "You're right, here is a slightly more compact example to reproduce the issue. The writer thread increases the elements of the matrix exponentially using locking. The reader thread checks for all elements of the matrix being identical at any point, which fails. Isn't this a bug in TensorFlow's `use_locking`?\r\n\r\n```python\r\nimport threading\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nvariable = tf.Variable(np.ones((100, 100)), trainable=False)\r\nwrite_op = variable.assign_add(variable, use_locking=True)\r\nsess = tf.Session()\r\nrunning = True\r\n\r\ndef writing():\r\n  global running\r\n  for _ in range(1000):\r\n    sess.run(write_op)\r\n    time.sleep(0.001)\r\n  running = False\r\n\r\ndef reading():\r\n  while running:\r\n    value = sess.run(variable)\r\n    assert (value == value.flat[0]).all()  # This fails!\r\n\r\nif __name__ == '__main__':\r\n  sess.run(tf.global_variables_initializer())\r\n  threading.Thread(target=writing).start()\r\n  threading.Thread(target=reading).start()\r\n```"}