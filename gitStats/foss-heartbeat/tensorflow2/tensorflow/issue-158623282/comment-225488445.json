{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225488445", "html_url": "https://github.com/tensorflow/tensorflow/issues/2682#issuecomment-225488445", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2682", "id": 225488445, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTQ4ODQ0NQ==", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-13T04:46:26Z", "updated_at": "2016-06-13T04:46:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4291728\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ajaybhat\">@ajaybhat</a> Search for classes which inherit from <code>Optimizer</code>.  However, note that for Hessian free methods the standard split into <code>compute_gradients</code> and <code>apply_gradients</code> won't work, since you need to compute partial second order gradients.  There are a few different ways one could handle that; the simplest would be to compute gradients as normal during <code>compute_gradients</code> and do the higher order stuff in <code>apply_gradients</code>.</p>", "body_text": "@ajaybhat Search for classes which inherit from Optimizer.  However, note that for Hessian free methods the standard split into compute_gradients and apply_gradients won't work, since you need to compute partial second order gradients.  There are a few different ways one could handle that; the simplest would be to compute gradients as normal during compute_gradients and do the higher order stuff in apply_gradients.", "body": "@ajaybhat Search for classes which inherit from `Optimizer`.  However, note that for Hessian free methods the standard split into `compute_gradients` and `apply_gradients` won't work, since you need to compute partial second order gradients.  There are a few different ways one could handle that; the simplest would be to compute gradients as normal during `compute_gradients` and do the higher order stuff in `apply_gradients`.\n"}