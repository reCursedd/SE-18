{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/380619386", "html_url": "https://github.com/tensorflow/tensorflow/issues/2682#issuecomment-380619386", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2682", "id": 380619386, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDYxOTM4Ng==", "user": {"login": "lixilinx", "id": 37510137, "node_id": "MDQ6VXNlcjM3NTEwMTM3", "avatar_url": "https://avatars2.githubusercontent.com/u/37510137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lixilinx", "html_url": "https://github.com/lixilinx", "followers_url": "https://api.github.com/users/lixilinx/followers", "following_url": "https://api.github.com/users/lixilinx/following{/other_user}", "gists_url": "https://api.github.com/users/lixilinx/gists{/gist_id}", "starred_url": "https://api.github.com/users/lixilinx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lixilinx/subscriptions", "organizations_url": "https://api.github.com/users/lixilinx/orgs", "repos_url": "https://api.github.com/users/lixilinx/repos", "events_url": "https://api.github.com/users/lixilinx/events{/privacy}", "received_events_url": "https://api.github.com/users/lixilinx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-11T22:45:54Z", "updated_at": "2018-04-12T22:22:49Z", "author_association": "NONE", "body_html": "<p>Pardon me to promote my second-order optimization methods here. If interested, please check my tensorflow package at <a href=\"https://github.com/lixilinx/psgd_tf\">https://github.com/lixilinx/psgd_tf</a><br>\nSecond-order optimization with five different preconditioners and rnn/cnn examples are provided. It works for both FNN and RNN with while loop. We know that tf.while_loop still does not support second-order derivative. To work around it, you can just use perturbation of gradient to approximate the Hessian-vector product you want.</p>\n<p>As for HF optimization, its damping factor and step size in line search are obtained by trial-and-error, and this could cause further troubles for its tensorflow implementation. A second-order method without line search is preferred, and methods in the above link are such examples.</p>", "body_text": "Pardon me to promote my second-order optimization methods here. If interested, please check my tensorflow package at https://github.com/lixilinx/psgd_tf\nSecond-order optimization with five different preconditioners and rnn/cnn examples are provided. It works for both FNN and RNN with while loop. We know that tf.while_loop still does not support second-order derivative. To work around it, you can just use perturbation of gradient to approximate the Hessian-vector product you want.\nAs for HF optimization, its damping factor and step size in line search are obtained by trial-and-error, and this could cause further troubles for its tensorflow implementation. A second-order method without line search is preferred, and methods in the above link are such examples.", "body": "Pardon me to promote my second-order optimization methods here. If interested, please check my tensorflow package at https://github.com/lixilinx/psgd_tf  \r\nSecond-order optimization with five different preconditioners and rnn/cnn examples are provided. It works for both FNN and RNN with while loop. We know that tf.while_loop still does not support second-order derivative. To work around it, you can just use perturbation of gradient to approximate the Hessian-vector product you want.\r\n\r\nAs for HF optimization, its damping factor and step size in line search are obtained by trial-and-error, and this could cause further troubles for its tensorflow implementation. A second-order method without line search is preferred, and methods in the above link are such examples."}