{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23710", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23710/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23710/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23710/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23710", "id": 380106207, "node_id": "MDU6SXNzdWUzODAxMDYyMDc=", "number": 23710, "title": "the performance of tensorflow distributed", "user": {"login": "dingevin", "id": 16797858, "node_id": "MDQ6VXNlcjE2Nzk3ODU4", "avatar_url": "https://avatars2.githubusercontent.com/u/16797858?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dingevin", "html_url": "https://github.com/dingevin", "followers_url": "https://api.github.com/users/dingevin/followers", "following_url": "https://api.github.com/users/dingevin/following{/other_user}", "gists_url": "https://api.github.com/users/dingevin/gists{/gist_id}", "starred_url": "https://api.github.com/users/dingevin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dingevin/subscriptions", "organizations_url": "https://api.github.com/users/dingevin/orgs", "repos_url": "https://api.github.com/users/dingevin/repos", "events_url": "https://api.github.com/users/dingevin/events{/privacy}", "received_events_url": "https://api.github.com/users/dingevin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 996845227, "node_id": "MDU6TGFiZWw5OTY4NDUyMjc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:dist-strat", "name": "comp:dist-strat", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-11-13T08:15:20Z", "updated_at": "2018-11-18T11:35:37Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>Describe the current behavior</strong><br>\nI have trained a speech recognition network using tensorflow both on multi-gpu single machine version(multi-gpu) and multi-gpu multi-machine(distributed) version. the training speed was ok in multi-GPU but the speed on distributed is slow and accuracy is not same as multi-GPU version at same steps. I watched the timeline and found RecvTensor is waste so long time. i\u2018m wondering how to improve the performance of distributed training.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/16797858/48396541-c0f02480-e755-11e8-97fd-8dad532956fa.png\"><img src=\"https://user-images.githubusercontent.com/16797858/48396541-c0f02480-e755-11e8-97fd-8dad532956fa.png\" alt=\"c2e456a6 17eda71d 1b93e65b\" style=\"max-width:100%;\"></a></p>\n<p><strong>log info</strong><br>\nthe start script for each worker (3 workers in total and 3 ps) is:</p>\n<pre><code>CUDA_VISIBLE_DEVICES='' nohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=ps --task_index=0 &amp;\n\nnohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=worker --task_index=0 --gpu_num=4 --learning_rate=0.00001 --batch_size=128&amp;\n</code></pre>\n<p>the training log:<br>\nmulti-gpu:</p>\n<pre><code>INFO:tensorflow:Step #312: rate 0.000010, accuracy 0.08653%, cross entropy 7.359044(192.9 examples/sec; 0.166 sec/batch)\nINFO:tensorflow:Step #313: rate 0.000010, accuracy 0.09099%, cross entropy 7.384690(218.1 examples/sec; 0.147 sec/batch)\nINFO:tensorflow:Step #314: rate 0.000010, accuracy 0.09412%, cross entropy 7.325777(241.5 examples/sec; 0.132 sec/batch)\nINFO:tensorflow:Step #315: rate 0.000010, accuracy 0.08608%, cross entropy 7.381987(225.8 examples/sec; 0.142 sec/batch)\n</code></pre>\n<p>distributed:</p>\n<pre><code>INFO:tensorflow:Step #768: learning_rate:0.00001, accuracy 0.49589%, cross entropy 6.968968 (62.8 examples/sec; 0.509 sec/batch)\nINFO:tensorflow:Step #769: learning_rate:0.00001, accuracy 0.60303%, cross entropy 7.082638 (87.0 examples/sec; 0.368 sec/batch)\nINFO:tensorflow:Step #770: learning_rate:0.00001, accuracy 0.77971%, cross entropy 7.041758 (54.9 examples/sec; 0.583 sec/batch)\nINFO:tensorflow:Step #771: learning_rate:0.00001, accuracy 1.00002%, cross entropy 6.996723 (41.4 examples/sec; 0.773 sec/batch)\n</code></pre>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code: yes    (code is <a href=\"https://github.com/dingevin/distributed-training/blob/master/distributed.py\">here</a>)</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.8</li>\n<li>TensorFlow installed from (source or binary): binary</li>\n<li>TensorFlow version (use command below): 1.10.1</li>\n<li>Python version: 2.7.13</li>\n<li>GPU model and memory: TITAN X 12G</li>\n<li>Network bandwidt: 125M/s</li>\n</ul>\n<p><strong>What I have tried</strong></p>\n<ol>\n<li>the <strong>RecvTensor</strong> is so long time , so i increase the number of ps, the speed more fast than before but accuracy don't improve. i wonder how to confirm the number of ps and worker\uff1fif i increate the numer of ps the speed can be faster?  my network is 1CNN+5LSTM+1Fully Connected Layer\u3002</li>\n<li>the accuracy was 10% in multi-gpu 4000 steps; so i set 2000 steps to each worer(2 ps 2 worker), but the accuracy is 5%\uff0c it looks don't sync updata parameter? Is there something wrong with my <a href=\"https://github.com/dingevin/distributed-training/blob/master/distributed.py\">code</a>?</li>\n</ol>", "body_text": "Describe the current behavior\nI have trained a speech recognition network using tensorflow both on multi-gpu single machine version(multi-gpu) and multi-gpu multi-machine(distributed) version. the training speed was ok in multi-GPU but the speed on distributed is slow and accuracy is not same as multi-GPU version at same steps. I watched the timeline and found RecvTensor is waste so long time. i\u2018m wondering how to improve the performance of distributed training.\n\nlog info\nthe start script for each worker (3 workers in total and 3 ps) is:\nCUDA_VISIBLE_DEVICES='' nohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=ps --task_index=0 &\n\nnohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=worker --task_index=0 --gpu_num=4 --learning_rate=0.00001 --batch_size=128&\n\nthe training log:\nmulti-gpu:\nINFO:tensorflow:Step #312: rate 0.000010, accuracy 0.08653%, cross entropy 7.359044(192.9 examples/sec; 0.166 sec/batch)\nINFO:tensorflow:Step #313: rate 0.000010, accuracy 0.09099%, cross entropy 7.384690(218.1 examples/sec; 0.147 sec/batch)\nINFO:tensorflow:Step #314: rate 0.000010, accuracy 0.09412%, cross entropy 7.325777(241.5 examples/sec; 0.132 sec/batch)\nINFO:tensorflow:Step #315: rate 0.000010, accuracy 0.08608%, cross entropy 7.381987(225.8 examples/sec; 0.142 sec/batch)\n\ndistributed:\nINFO:tensorflow:Step #768: learning_rate:0.00001, accuracy 0.49589%, cross entropy 6.968968 (62.8 examples/sec; 0.509 sec/batch)\nINFO:tensorflow:Step #769: learning_rate:0.00001, accuracy 0.60303%, cross entropy 7.082638 (87.0 examples/sec; 0.368 sec/batch)\nINFO:tensorflow:Step #770: learning_rate:0.00001, accuracy 0.77971%, cross entropy 7.041758 (54.9 examples/sec; 0.583 sec/batch)\nINFO:tensorflow:Step #771: learning_rate:0.00001, accuracy 1.00002%, cross entropy 6.996723 (41.4 examples/sec; 0.773 sec/batch)\n\nSystem information\n\nHave I written custom code: yes    (code is here)\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.8\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.10.1\nPython version: 2.7.13\nGPU model and memory: TITAN X 12G\nNetwork bandwidt: 125M/s\n\nWhat I have tried\n\nthe RecvTensor is so long time , so i increase the number of ps, the speed more fast than before but accuracy don't improve. i wonder how to confirm the number of ps and worker\uff1fif i increate the numer of ps the speed can be faster?  my network is 1CNN+5LSTM+1Fully Connected Layer\u3002\nthe accuracy was 10% in multi-gpu 4000 steps; so i set 2000 steps to each worer(2 ps 2 worker), but the accuracy is 5%\uff0c it looks don't sync updata parameter? Is there something wrong with my code?", "body": "**Describe the current behavior**\r\nI have trained a speech recognition network using tensorflow both on multi-gpu single machine version(multi-gpu) and multi-gpu multi-machine(distributed) version. the training speed was ok in multi-GPU but the speed on distributed is slow and accuracy is not same as multi-GPU version at same steps. I watched the timeline and found RecvTensor is waste so long time. i\u2018m wondering how to improve the performance of distributed training.\r\n![c2e456a6 17eda71d 1b93e65b](https://user-images.githubusercontent.com/16797858/48396541-c0f02480-e755-11e8-97fd-8dad532956fa.png)\r\n\r\n**log info**\r\nthe start script for each worker (3 workers in total and 3 ps) is:\r\n```\r\nCUDA_VISIBLE_DEVICES='' nohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=ps --task_index=0 &\r\n\r\nnohup python distributed.py --ps_hosts=gpu47-hca:2220,gpu42-hca:2221,gpu35-hca:2222 --worker_hosts=gpu47-hca:3330,gpu42-hca:3331,gpu35-hca:3332 --job_name=worker --task_index=0 --gpu_num=4 --learning_rate=0.00001 --batch_size=128&\r\n```\r\nthe training log:\r\nmulti-gpu:\r\n```\r\nINFO:tensorflow:Step #312: rate 0.000010, accuracy 0.08653%, cross entropy 7.359044(192.9 examples/sec; 0.166 sec/batch)\r\nINFO:tensorflow:Step #313: rate 0.000010, accuracy 0.09099%, cross entropy 7.384690(218.1 examples/sec; 0.147 sec/batch)\r\nINFO:tensorflow:Step #314: rate 0.000010, accuracy 0.09412%, cross entropy 7.325777(241.5 examples/sec; 0.132 sec/batch)\r\nINFO:tensorflow:Step #315: rate 0.000010, accuracy 0.08608%, cross entropy 7.381987(225.8 examples/sec; 0.142 sec/batch)\r\n```\r\ndistributed:\r\n```\r\nINFO:tensorflow:Step #768: learning_rate:0.00001, accuracy 0.49589%, cross entropy 6.968968 (62.8 examples/sec; 0.509 sec/batch)\r\nINFO:tensorflow:Step #769: learning_rate:0.00001, accuracy 0.60303%, cross entropy 7.082638 (87.0 examples/sec; 0.368 sec/batch)\r\nINFO:tensorflow:Step #770: learning_rate:0.00001, accuracy 0.77971%, cross entropy 7.041758 (54.9 examples/sec; 0.583 sec/batch)\r\nINFO:tensorflow:Step #771: learning_rate:0.00001, accuracy 1.00002%, cross entropy 6.996723 (41.4 examples/sec; 0.773 sec/batch)\r\n```\r\n\r\n\r\n**System information**\r\n- Have I written custom code: yes    (code is [here](https://github.com/dingevin/distributed-training/blob/master/distributed.py))\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.8\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): 1.10.1\r\n- Python version: 2.7.13\r\n- GPU model and memory: TITAN X 12G\r\n- Network bandwidt: 125M/s\r\n\r\n**What I have tried**\r\n1. the **RecvTensor** is so long time , so i increase the number of ps, the speed more fast than before but accuracy don't improve. i wonder how to confirm the number of ps and worker\uff1fif i increate the numer of ps the speed can be faster?  my network is 1CNN+5LSTM+1Fully Connected Layer\u3002\r\n2. the accuracy was 10% in multi-gpu 4000 steps; so i set 2000 steps to each worer(2 ps 2 worker), but the accuracy is 5%\uff0c it looks don't sync updata parameter? Is there something wrong with my [code](https://github.com/dingevin/distributed-training/blob/master/distributed.py)?"}