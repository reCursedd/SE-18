{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17783", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17783/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17783/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17783/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17783", "id": 306068048, "node_id": "MDU6SXNzdWUzMDYwNjgwNDg=", "number": 17783, "title": "Segmentation fault in Eigen::internal::InnerMostDimReducer<...>::reduce when passing large tensor to sparse_softmax_cross_entropy_with_logits", "user": {"login": "qtdaniel", "id": 21170884, "node_id": "MDQ6VXNlcjIxMTcwODg0", "avatar_url": "https://avatars2.githubusercontent.com/u/21170884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qtdaniel", "html_url": "https://github.com/qtdaniel", "followers_url": "https://api.github.com/users/qtdaniel/followers", "following_url": "https://api.github.com/users/qtdaniel/following{/other_user}", "gists_url": "https://api.github.com/users/qtdaniel/gists{/gist_id}", "starred_url": "https://api.github.com/users/qtdaniel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qtdaniel/subscriptions", "organizations_url": "https://api.github.com/users/qtdaniel/orgs", "repos_url": "https://api.github.com/users/qtdaniel/repos", "events_url": "https://api.github.com/users/qtdaniel/events{/privacy}", "received_events_url": "https://api.github.com/users/qtdaniel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-03-16T20:33:57Z", "updated_at": "2018-11-22T18:54:38Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: I have written custom code. A simple reproduction script is included below.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04 (also occurring on Linux Ubuntu 14.04)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: TensorFlow installed from binary (also occurring after building from source)</li>\n<li><strong>TensorFlow version (use command below)</strong>: TensorFlow v1.6 (also occurring on v1.4, v1.5, v1.7rc0)</li>\n<li><strong>Python version</strong>: Python 2.7 (Ubuntu base and also occurring on Anaconda Python 2.7)</li>\n<li><strong>Bazel version (if compiling from source)</strong>: Bazel version: 0.11.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: GCC 4.9.4</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA not used, CPU only</li>\n<li><strong>GPU model and memory</strong>: GPU not used, CPU only (Intel(R) Xeon(R) CPU E5-2650 and Intel(R) Xeon(R) Platinum 8175M)</li>\n<li><strong>Exact command to reproduce</strong>: Command to reproduce using script given below: \"python sfi.py 300000\"</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>A segmentation fault is occurring with the following gdb backtrace when a \"logits\" tensor of sufficient size is passed to <code>sparse_softmax_cross_entropy_with_logits</code>. The single argument to the demonstration code below adjusts the size. I have found that there is a point below which the SegFault does not seem to ever occur and above which the SegFault always seems to occur, but around that point (e.g. within +/- 2) the SegFault behaviour is intermittent. Right on the change point I can run the same code with the same argument and it will sometimes generate a SegFault and sometimes not (though the random data generated in the demo code may be causing this randomness).</p>\n<pre><code>Program received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7fffcdffb700 (LWP 2440)]\n0x00007fffef12a590 in Eigen::internal::InnerMostDimReducer&lt;Eigen::TensorEvaluator&lt;Eigen::TensorReductionOp&lt;Eigen::internal::MaxReducer&lt;float&gt;, Eigen::IndexList&lt;Eigen::type2index&lt;1l&gt;&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, int&gt;, 16, Eigen::MakePointer&gt; const, Eigen::MakePointer&gt; const, Eigen::ThreadPoolDevice&gt;, Eigen::internal::MaxReducer&lt;float&gt;, true&gt;::reduce(Eigen::TensorEvaluator&lt;Eigen::TensorReductionOp&lt;Eigen::internal::MaxReducer&lt;float&gt;, Eigen::IndexList&lt;Eigen::type2index&lt;1l&gt;&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, int&gt;, 16, Eigen::MakePointer&gt; const, Eigen::MakePointer&gt; const, Eigen::ThreadPoolDevice&gt; const&amp;, int, int, Eigen::internal::MaxReducer&lt;float&gt;&amp;) ()\n   from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n(gdb) bt\n#0  0x00007fffef12a590 in Eigen::internal::InnerMostDimReducer&lt;Eigen::TensorEvaluator&lt;Eigen::TensorReductionOp&lt;Eigen::internal::MaxReducer&lt;float&gt;, Eigen::IndexList&lt;Eigen::type2index&lt;1l&gt;&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, int&gt;, 16, Eigen::MakePointer&gt; const, Eigen::MakePointer&gt; const, Eigen::ThreadPoolDevice&gt;, Eigen::internal::MaxReducer&lt;float&gt;, true&gt;::reduce(Eigen::TensorEvaluator&lt;Eigen::TensorReductionOp&lt;Eigen::internal::MaxReducer&lt;float&gt;, Eigen::IndexList&lt;Eigen::type2index&lt;1l&gt;&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, int&gt;, 16, Eigen::MakePointer&gt; const, Eigen::MakePointer&gt; const, Eigen::ThreadPoolDevice&gt; const&amp;, int, int, Eigen::internal::MaxReducer&lt;float&gt;&amp;) ()\n   from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#1  0x00007fffef12a90f in Eigen::internal::EvalRange&lt;Eigen::TensorEvaluator&lt;Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, int&gt;, 16, Eigen::MakePointer&gt;, Eigen::TensorReductionOp&lt;Eigen::internal::MaxReducer&lt;float&gt;, Eigen::IndexList&lt;Eigen::type2index&lt;1l&gt;&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, int&gt;, 16, Eigen::MakePointer&gt; const, Eigen::MakePointer&gt; const&gt; const, Eigen::ThreadPoolDevice&gt;, int, true&gt;::run(Eigen::TensorEvaluator&lt;Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, int&gt;, 16, Eigen::MakePointer&gt;, Eigen::TensorReductionOp&lt;Eigen::internal::MaxReducer&lt;float&gt;, Eigen::IndexList&lt;Eigen::type2index&lt;1l&gt;&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, int&gt;, 16, Eigen::MakePointer&gt; const, Eigen::MakePointer&gt; const&gt; const, Eigen::ThreadPoolDevice&gt;*, int, int) () from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#2  0x00007fffedcd5541 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}::operator()(long, long) const () from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#3  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}::operator()(long, long) const () from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#4  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}::operator()(long, long) const () from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#5  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}::operator()(long, long) const () from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#6  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}::operator()(long, long) const () from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#7  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}::operator()(long, long) const () from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#8  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}::operator()(long, long) const () from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#9  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}::operator()(long, long) const () from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#10 0x00007fffebcecb70 in Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) ()\n   from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\n#11 0x00007fffebceb8e2 in std::_Function_handler&lt;void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function&lt;void ()&gt;)::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;) ()\n   from &lt;home_dir&gt;/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\n#12 0x00007fffe20355b0 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#13 0x00007ffff77e7184 in start_thread (arg=0x7fffcdffb700) at pthread_create.c:312\n#14 0x00007ffff6e0703d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\n</code></pre>\n<p>The problem occurs in all of these configurations:</p>\n<ul>\n<li>Binary CPU-only install of TF: v1.4, v1.5, v1.6, and v1.7</li>\n<li>Build from source code: v1.6, v1.7</li>\n<li>Building with MKL and without MKL</li>\n<li>Ubuntu Python and Anaconda Python (but only Python 2.7 in both cases)</li>\n<li>In clean virtual/conda envs with only the minimum TF dependencies installed</li>\n</ul>\n<h3>Source code / logs</h3>\n<p>I've been able to distil the problem down to the following code which reliably reproduces the problem for me on my local hardware and also on a m5.2xlarge EC2 instance running Ubuntu 16.04 Server or Amazon Linux. The following code has no external data or code dependencies other than tensorflow.</p>\n<p>The script has a single argument which sets the \"vocabulary size\" (this was originally an RNN LM); if this value is large enough a SegFault occurs. The only operation of note is the <code>sparse_softmax_cross_entropy_with_logits</code>.</p>\n<pre><code>#!/usr/bin/env python\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport sys\nimport tensorflow as tf\n\ndef main():\n    vocabulary_size = int(sys.argv[1])\n    batch_size = 256\n    step_size = 32\n\n    print(\"Vocabulary size:\", vocabulary_size)\n\n    labels = tf.get_variable(\"labels\", shape=[batch_size, step_size], dtype=tf.int32)\n    logits = tf.get_variable(\"logits\", shape=[batch_size, step_size, vocabulary_size])\n    costs = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n\n        print(\"Executing...\")\n        session.run(costs)\n        print(\"SUCCESS!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): I have written custom code. A simple reproduction script is included below.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 (also occurring on Linux Ubuntu 14.04)\nTensorFlow installed from (source or binary): TensorFlow installed from binary (also occurring after building from source)\nTensorFlow version (use command below): TensorFlow v1.6 (also occurring on v1.4, v1.5, v1.7rc0)\nPython version: Python 2.7 (Ubuntu base and also occurring on Anaconda Python 2.7)\nBazel version (if compiling from source): Bazel version: 0.11.1\nGCC/Compiler version (if compiling from source): GCC 4.9.4\nCUDA/cuDNN version: CUDA not used, CPU only\nGPU model and memory: GPU not used, CPU only (Intel(R) Xeon(R) CPU E5-2650 and Intel(R) Xeon(R) Platinum 8175M)\nExact command to reproduce: Command to reproduce using script given below: \"python sfi.py 300000\"\n\nDescribe the problem\nA segmentation fault is occurring with the following gdb backtrace when a \"logits\" tensor of sufficient size is passed to sparse_softmax_cross_entropy_with_logits. The single argument to the demonstration code below adjusts the size. I have found that there is a point below which the SegFault does not seem to ever occur and above which the SegFault always seems to occur, but around that point (e.g. within +/- 2) the SegFault behaviour is intermittent. Right on the change point I can run the same code with the same argument and it will sometimes generate a SegFault and sometimes not (though the random data generated in the demo code may be causing this randomness).\nProgram received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7fffcdffb700 (LWP 2440)]\n0x00007fffef12a590 in Eigen::internal::InnerMostDimReducer<Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::internal::MaxReducer<float>, true>::reduce(Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice> const&, int, int, Eigen::internal::MaxReducer<float>&) ()\n   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n(gdb) bt\n#0  0x00007fffef12a590 in Eigen::internal::InnerMostDimReducer<Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::internal::MaxReducer<float>, true>::reduce(Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice> const&, int, int, Eigen::internal::MaxReducer<float>&) ()\n   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#1  0x00007fffef12a90f in Eigen::internal::EvalRange<Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, int, true>::run(Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>*, int, int) () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#2  0x00007fffedcd5541 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#3  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#4  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#5  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#6  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#7  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#8  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#9  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#10 0x00007fffebcecb70 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\n   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\n#11 0x00007fffebceb8e2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\n   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\n#12 0x00007fffe20355b0 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#13 0x00007ffff77e7184 in start_thread (arg=0x7fffcdffb700) at pthread_create.c:312\n#14 0x00007ffff6e0703d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\n\nThe problem occurs in all of these configurations:\n\nBinary CPU-only install of TF: v1.4, v1.5, v1.6, and v1.7\nBuild from source code: v1.6, v1.7\nBuilding with MKL and without MKL\nUbuntu Python and Anaconda Python (but only Python 2.7 in both cases)\nIn clean virtual/conda envs with only the minimum TF dependencies installed\n\nSource code / logs\nI've been able to distil the problem down to the following code which reliably reproduces the problem for me on my local hardware and also on a m5.2xlarge EC2 instance running Ubuntu 16.04 Server or Amazon Linux. The following code has no external data or code dependencies other than tensorflow.\nThe script has a single argument which sets the \"vocabulary size\" (this was originally an RNN LM); if this value is large enough a SegFault occurs. The only operation of note is the sparse_softmax_cross_entropy_with_logits.\n#!/usr/bin/env python\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport sys\nimport tensorflow as tf\n\ndef main():\n    vocabulary_size = int(sys.argv[1])\n    batch_size = 256\n    step_size = 32\n\n    print(\"Vocabulary size:\", vocabulary_size)\n\n    labels = tf.get_variable(\"labels\", shape=[batch_size, step_size], dtype=tf.int32)\n    logits = tf.get_variable(\"logits\", shape=[batch_size, step_size, vocabulary_size])\n    costs = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n\n        print(\"Executing...\")\n        session.run(costs)\n        print(\"SUCCESS!\")\n\n\nif __name__ == \"__main__\":\n    main()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have written custom code. A simple reproduction script is included below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 (also occurring on Linux Ubuntu 14.04)\r\n- **TensorFlow installed from (source or binary)**: TensorFlow installed from binary (also occurring after building from source)\r\n- **TensorFlow version (use command below)**: TensorFlow v1.6 (also occurring on v1.4, v1.5, v1.7rc0)\r\n- **Python version**: Python 2.7 (Ubuntu base and also occurring on Anaconda Python 2.7)\r\n- **Bazel version (if compiling from source)**: Bazel version: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: GCC 4.9.4\r\n- **CUDA/cuDNN version**: CUDA not used, CPU only\r\n- **GPU model and memory**: GPU not used, CPU only (Intel(R) Xeon(R) CPU E5-2650 and Intel(R) Xeon(R) Platinum 8175M)\r\n- **Exact command to reproduce**: Command to reproduce using script given below: \"python sfi.py 300000\"\r\n\r\n### Describe the problem\r\nA segmentation fault is occurring with the following gdb backtrace when a \"logits\" tensor of sufficient size is passed to `sparse_softmax_cross_entropy_with_logits`. The single argument to the demonstration code below adjusts the size. I have found that there is a point below which the SegFault does not seem to ever occur and above which the SegFault always seems to occur, but around that point (e.g. within +/- 2) the SegFault behaviour is intermittent. Right on the change point I can run the same code with the same argument and it will sometimes generate a SegFault and sometimes not (though the random data generated in the demo code may be causing this randomness).\r\n\r\n```\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fffcdffb700 (LWP 2440)]\r\n0x00007fffef12a590 in Eigen::internal::InnerMostDimReducer<Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::internal::MaxReducer<float>, true>::reduce(Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice> const&, int, int, Eigen::internal::MaxReducer<float>&) ()\r\n   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n(gdb) bt\r\n#0  0x00007fffef12a590 in Eigen::internal::InnerMostDimReducer<Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::internal::MaxReducer<float>, true>::reduce(Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const, Eigen::ThreadPoolDevice> const&, int, int, Eigen::internal::MaxReducer<float>&) ()\r\n   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007fffef12a90f in Eigen::internal::EvalRange<Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, int, true>::run(Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, Eigen::IndexList<Eigen::type2index<1l>> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, int>, 16, Eigen::MakePointer> const, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>*, int, int) () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fffedcd5541 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fffedcd5511 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}::operator()(long, long) const () from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fffebcecb70 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#11 0x00007fffebceb8e2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from <home_dir>/.conda/envs/research/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#12 0x00007fffe20355b0 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#13 0x00007ffff77e7184 in start_thread (arg=0x7fffcdffb700) at pthread_create.c:312\r\n#14 0x00007ffff6e0703d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\r\n```\r\n\r\nThe problem occurs in all of these configurations:\r\n\r\n- Binary CPU-only install of TF: v1.4, v1.5, v1.6, and v1.7\r\n- Build from source code: v1.6, v1.7\r\n- Building with MKL and without MKL\r\n- Ubuntu Python and Anaconda Python (but only Python 2.7 in both cases)\r\n- In clean virtual/conda envs with only the minimum TF dependencies installed\r\n\r\n### Source code / logs\r\n\r\nI've been able to distil the problem down to the following code which reliably reproduces the problem for me on my local hardware and also on a m5.2xlarge EC2 instance running Ubuntu 16.04 Server or Amazon Linux. The following code has no external data or code dependencies other than tensorflow.\r\n\r\nThe script has a single argument which sets the \"vocabulary size\" (this was originally an RNN LM); if this value is large enough a SegFault occurs. The only operation of note is the `sparse_softmax_cross_entropy_with_logits`.\r\n\r\n```\r\n#!/usr/bin/env python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n    vocabulary_size = int(sys.argv[1])\r\n    batch_size = 256\r\n    step_size = 32\r\n\r\n    print(\"Vocabulary size:\", vocabulary_size)\r\n\r\n    labels = tf.get_variable(\"labels\", shape=[batch_size, step_size], dtype=tf.int32)\r\n    logits = tf.get_variable(\"logits\", shape=[batch_size, step_size, vocabulary_size])\r\n    costs = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\r\n\r\n    with tf.Session() as session:\r\n        session.run(tf.global_variables_initializer())\r\n\r\n        print(\"Executing...\")\r\n        session.run(costs)\r\n        print(\"SUCCESS!\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n"}