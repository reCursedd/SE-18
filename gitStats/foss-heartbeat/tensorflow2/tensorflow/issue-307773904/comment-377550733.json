{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/377550733", "html_url": "https://github.com/tensorflow/tensorflow/issues/17930#issuecomment-377550733", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17930", "id": 377550733, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzU1MDczMw==", "user": {"login": "chrisrsipes", "id": 5429005, "node_id": "MDQ6VXNlcjU0MjkwMDU=", "avatar_url": "https://avatars2.githubusercontent.com/u/5429005?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrisrsipes", "html_url": "https://github.com/chrisrsipes", "followers_url": "https://api.github.com/users/chrisrsipes/followers", "following_url": "https://api.github.com/users/chrisrsipes/following{/other_user}", "gists_url": "https://api.github.com/users/chrisrsipes/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrisrsipes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrisrsipes/subscriptions", "organizations_url": "https://api.github.com/users/chrisrsipes/orgs", "repos_url": "https://api.github.com/users/chrisrsipes/repos", "events_url": "https://api.github.com/users/chrisrsipes/events{/privacy}", "received_events_url": "https://api.github.com/users/chrisrsipes/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T15:32:59Z", "updated_at": "2018-03-30T15:32:59Z", "author_association": "NONE", "body_html": "<p>My memory footprint actually does continually increase and the process would eventually run out of memory.  My guess is that perhaps on each iteration, instead of reusing that memory that was allocated and left for future use, it is allocating indeed allocating new memory that never gets freed.</p>\n<p>The screenshots from above are successive invocations of performing the inference.  No other processes (as least not with significant memory footprints) were started in between invocations.  I now realize that the process shown using htop isn't the same process that was doing the invocation, which was unintentional.</p>\n<p>Still, after performing the first inference, the RAM usage is at 3.73G.  after the second inference, the RAM usage is at 4.14G.  After the third inference, the RAM usage is at 4.66G.  Each time the inference runs, it uses a significant amount of RAM that is not released after the inference finishes.  If I was to kill the program, all the memory allocated is free.  However, I am wrapping the capability of performing the inference in an API, and as such can not shut down the application each time an inference is performed.</p>\n<p>If you have any tips of how I can debug / profile the native libraries or see where the memory blocks are being allocated, I would be happy to try it out.</p>", "body_text": "My memory footprint actually does continually increase and the process would eventually run out of memory.  My guess is that perhaps on each iteration, instead of reusing that memory that was allocated and left for future use, it is allocating indeed allocating new memory that never gets freed.\nThe screenshots from above are successive invocations of performing the inference.  No other processes (as least not with significant memory footprints) were started in between invocations.  I now realize that the process shown using htop isn't the same process that was doing the invocation, which was unintentional.\nStill, after performing the first inference, the RAM usage is at 3.73G.  after the second inference, the RAM usage is at 4.14G.  After the third inference, the RAM usage is at 4.66G.  Each time the inference runs, it uses a significant amount of RAM that is not released after the inference finishes.  If I was to kill the program, all the memory allocated is free.  However, I am wrapping the capability of performing the inference in an API, and as such can not shut down the application each time an inference is performed.\nIf you have any tips of how I can debug / profile the native libraries or see where the memory blocks are being allocated, I would be happy to try it out.", "body": "My memory footprint actually does continually increase and the process would eventually run out of memory.  My guess is that perhaps on each iteration, instead of reusing that memory that was allocated and left for future use, it is allocating indeed allocating new memory that never gets freed.\r\n\r\nThe screenshots from above are successive invocations of performing the inference.  No other processes (as least not with significant memory footprints) were started in between invocations.  I now realize that the process shown using htop isn't the same process that was doing the invocation, which was unintentional.  \r\n\r\nStill, after performing the first inference, the RAM usage is at 3.73G.  after the second inference, the RAM usage is at 4.14G.  After the third inference, the RAM usage is at 4.66G.  Each time the inference runs, it uses a significant amount of RAM that is not released after the inference finishes.  If I was to kill the program, all the memory allocated is free.  However, I am wrapping the capability of performing the inference in an API, and as such can not shut down the application each time an inference is performed.\r\n\r\nIf you have any tips of how I can debug / profile the native libraries or see where the memory blocks are being allocated, I would be happy to try it out."}