{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/233174084", "html_url": "https://github.com/tensorflow/tensorflow/issues/175#issuecomment-233174084", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/175", "id": 233174084, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMzE3NDA4NA==", "user": {"login": "tscohen", "id": 617085, "node_id": "MDQ6VXNlcjYxNzA4NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/617085?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tscohen", "html_url": "https://github.com/tscohen", "followers_url": "https://api.github.com/users/tscohen/followers", "following_url": "https://api.github.com/users/tscohen/following{/other_user}", "gists_url": "https://api.github.com/users/tscohen/gists{/gist_id}", "starred_url": "https://api.github.com/users/tscohen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tscohen/subscriptions", "organizations_url": "https://api.github.com/users/tscohen/orgs", "repos_url": "https://api.github.com/users/tscohen/repos", "events_url": "https://api.github.com/users/tscohen/events{/privacy}", "received_events_url": "https://api.github.com/users/tscohen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-17T09:53:43Z", "updated_at": "2016-07-17T09:53:43Z", "author_association": "NONE", "body_html": "<p>Would also like to see this. One tensor op to rule them all.</p>\n<p>If anyone wants to have a go at this, note that the gradient of einsum can be implemented neatly as another einsum with swapped arguments / indices. See the autograd implementation make_grad_einsum: <a href=\"https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_grads.py#L447\">https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_grads.py#L447</a></p>", "body_text": "Would also like to see this. One tensor op to rule them all.\nIf anyone wants to have a go at this, note that the gradient of einsum can be implemented neatly as another einsum with swapped arguments / indices. See the autograd implementation make_grad_einsum: https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_grads.py#L447", "body": "Would also like to see this. One tensor op to rule them all.\n\nIf anyone wants to have a go at this, note that the gradient of einsum can be implemented neatly as another einsum with swapped arguments / indices. See the autograd implementation make_grad_einsum: https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_grads.py#L447 \n"}