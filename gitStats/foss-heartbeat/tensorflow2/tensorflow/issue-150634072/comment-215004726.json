{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/215004726", "html_url": "https://github.com/tensorflow/tensorflow/pull/2079#issuecomment-215004726", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2079", "id": 215004726, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNTAwNDcyNg==", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-27T08:07:47Z", "updated_at": "2016-04-27T08:07:47Z", "author_association": "NONE", "body_html": "<p>I agree in general your idea works. Just some minor things...</p>\n<blockquote>\n<p>The tensor s has shape [batch_size x attn_length], where attn_length is<br>\nthe size of the attention_states (length of the encoder in a seq2seq<br>\nmodel). Let me focus on the basic case when we want mask_f to mask an input<br>\niff the decoder input symbol is a padding, which means that it is all<br>\nzeros. We can do this in two ways, I think. If we're sure the decoder input<br>\nis all 0s only if it's padded and has a reasonably large norm otherwise,<br>\nthen we can do mask_f(_, dec_inp, <em>) = tf.minimum(1.0,<br>\ntf.nn.l2_loss(dec_inp) * LARGE). If we're not sure of that, then we can<br>\ndo mask_f(</em>, _, step) = self.decoder_mask[i] when declaring the mask_f<br>\nfunction somewhere in our model code. In this case decoder_mask are<br>\nplaceholders of the same kind as decoder_inputs that are 1 in case of<br>\npadding and 0 otherwise -- and we need to make sure to feed the appropriate<br>\nvalues.</p>\n<p>The problem w/ this approach is (where self.decoder_inputs is generated by<br>\nthe input layer), is that the encoder may have a pyramid architecture<br>\n(i.e., time subsampling) which is very common in speech. i.e., depending on<br>\nthe graph, we may reduce it by 4/8/16 multipliers. so we need to know this<br>\nahead of time (the time factor reduction) and pass that in to the input op<br>\nthat is generating the mask.</p>\n</blockquote>\n<p>Where as w/ a op that does the masking for you where you only need to pass<br>\nin the sequence_length, every time we do hierarchical subsampling in the<br>\ngraph, we can just do sequence_length_tensor = sequence_length_tensor / 2;<br>\nenergies = attention_mask(energies, sequence_length);</p>\n<p>However, I think this problem can be alleviated by tf.cond(), where we can<br>\ndo something like tf.cond(sequence_length &lt; encoder_idx, 0, -FLT_MAX),<br>\nsomething like that... however we need to do this T times where T is the<br>\nlength of the encoder and encoder_idx \\in range(0, T).</p>\n<p>But I agree w/ you, I think (havent completely tested it out) you are right<br>\nthis can be done almost completely w/ the existing python ops. Personally,<br>\nI'll still be using my op since I already wrote it and it works and it<br>\nshould be slightly faster (and simplier graph). But we can close this PR if<br>\nyou think this is adding too much code to maintain.</p>", "body_text": "I agree in general your idea works. Just some minor things...\n\nThe tensor s has shape [batch_size x attn_length], where attn_length is\nthe size of the attention_states (length of the encoder in a seq2seq\nmodel). Let me focus on the basic case when we want mask_f to mask an input\niff the decoder input symbol is a padding, which means that it is all\nzeros. We can do this in two ways, I think. If we're sure the decoder input\nis all 0s only if it's padded and has a reasonably large norm otherwise,\nthen we can do mask_f(_, dec_inp, ) = tf.minimum(1.0,\ntf.nn.l2_loss(dec_inp) * LARGE). If we're not sure of that, then we can\ndo mask_f(, _, step) = self.decoder_mask[i] when declaring the mask_f\nfunction somewhere in our model code. In this case decoder_mask are\nplaceholders of the same kind as decoder_inputs that are 1 in case of\npadding and 0 otherwise -- and we need to make sure to feed the appropriate\nvalues.\nThe problem w/ this approach is (where self.decoder_inputs is generated by\nthe input layer), is that the encoder may have a pyramid architecture\n(i.e., time subsampling) which is very common in speech. i.e., depending on\nthe graph, we may reduce it by 4/8/16 multipliers. so we need to know this\nahead of time (the time factor reduction) and pass that in to the input op\nthat is generating the mask.\n\nWhere as w/ a op that does the masking for you where you only need to pass\nin the sequence_length, every time we do hierarchical subsampling in the\ngraph, we can just do sequence_length_tensor = sequence_length_tensor / 2;\nenergies = attention_mask(energies, sequence_length);\nHowever, I think this problem can be alleviated by tf.cond(), where we can\ndo something like tf.cond(sequence_length < encoder_idx, 0, -FLT_MAX),\nsomething like that... however we need to do this T times where T is the\nlength of the encoder and encoder_idx \\in range(0, T).\nBut I agree w/ you, I think (havent completely tested it out) you are right\nthis can be done almost completely w/ the existing python ops. Personally,\nI'll still be using my op since I already wrote it and it works and it\nshould be slightly faster (and simplier graph). But we can close this PR if\nyou think this is adding too much code to maintain.", "body": "I agree in general your idea works. Just some minor things...\n\n> The tensor s has shape [batch_size x attn_length], where attn_length is\n> the size of the attention_states (length of the encoder in a seq2seq\n> model). Let me focus on the basic case when we want mask_f to mask an input\n> iff the decoder input symbol is a padding, which means that it is all\n> zeros. We can do this in two ways, I think. If we're sure the decoder input\n> is all 0s only if it's padded and has a reasonably large norm otherwise,\n> then we can do mask_f(_, dec_inp, _) = tf.minimum(1.0,\n> tf.nn.l2_loss(dec_inp) \\* LARGE). If we're not sure of that, then we can\n> do mask_f(_, _, step) = self.decoder_mask[i] when declaring the mask_f\n> function somewhere in our model code. In this case decoder_mask are\n> placeholders of the same kind as decoder_inputs that are 1 in case of\n> padding and 0 otherwise -- and we need to make sure to feed the appropriate\n> values.\n> \n> The problem w/ this approach is (where self.decoder_inputs is generated by\n> the input layer), is that the encoder may have a pyramid architecture\n> (i.e., time subsampling) which is very common in speech. i.e., depending on\n> the graph, we may reduce it by 4/8/16 multipliers. so we need to know this\n> ahead of time (the time factor reduction) and pass that in to the input op\n> that is generating the mask.\n\nWhere as w/ a op that does the masking for you where you only need to pass\nin the sequence_length, every time we do hierarchical subsampling in the\ngraph, we can just do sequence_length_tensor = sequence_length_tensor / 2;\nenergies = attention_mask(energies, sequence_length);\n\nHowever, I think this problem can be alleviated by tf.cond(), where we can\ndo something like tf.cond(sequence_length < encoder_idx, 0, -FLT_MAX),\nsomething like that... however we need to do this T times where T is the\nlength of the encoder and encoder_idx \\in range(0, T).\n\nBut I agree w/ you, I think (havent completely tested it out) you are right\nthis can be done almost completely w/ the existing python ops. Personally,\nI'll still be using my op since I already wrote it and it works and it\nshould be slightly faster (and simplier graph). But we can close this PR if\nyou think this is adding too much code to maintain.\n"}