{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/214979869", "html_url": "https://github.com/tensorflow/tensorflow/pull/2079#issuecomment-214979869", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2079", "id": 214979869, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNDk3OTg2OQ==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-27T06:05:20Z", "updated_at": "2016-04-27T06:05:20Z", "author_association": "MEMBER", "body_html": "<p>I was thinking about it a bit and I still don't see the problem. Here is how I'd do it:</p>\n<ul>\n<li>add a parameter to attention_decoder called mask_function</li>\n<li>this parameter is a python function that takes the step number and the previous attention mask tensor<br>\nand outputs a new attention mask tensor</li>\n<li>every attention is multiplied by mask_function(prev_attention, step_number) (one-liner in python)</li>\n</ul>\n<p>Would that work? If you want to feed-in attention, you'd just provide mask_function(_, i) = length-tensor[i] so you can feed the example lengths. And if you want to average by previous (or move it with a step), you just need to write a lambda that does it.</p>\n<p>Or am I missing something crucial here?</p>", "body_text": "I was thinking about it a bit and I still don't see the problem. Here is how I'd do it:\n\nadd a parameter to attention_decoder called mask_function\nthis parameter is a python function that takes the step number and the previous attention mask tensor\nand outputs a new attention mask tensor\nevery attention is multiplied by mask_function(prev_attention, step_number) (one-liner in python)\n\nWould that work? If you want to feed-in attention, you'd just provide mask_function(_, i) = length-tensor[i] so you can feed the example lengths. And if you want to average by previous (or move it with a step), you just need to write a lambda that does it.\nOr am I missing something crucial here?", "body": "I was thinking about it a bit and I still don't see the problem. Here is how I'd do it:\n- add a parameter to attention_decoder called mask_function\n- this parameter is a python function that takes the step number and the previous attention mask tensor\n  and outputs a new attention mask tensor\n- every attention is multiplied by mask_function(prev_attention, step_number) (one-liner in python)\n\nWould that work? If you want to feed-in attention, you'd just provide mask_function(_, i) = length-tensor[i] so you can feed the example lengths. And if you want to average by previous (or move it with a step), you just need to write a lambda that does it.\n\nOr am I missing something crucial here?\n"}