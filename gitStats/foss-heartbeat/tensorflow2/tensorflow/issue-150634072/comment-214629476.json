{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/214629476", "html_url": "https://github.com/tensorflow/tensorflow/pull/2079#issuecomment-214629476", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2079", "id": 214629476, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNDYyOTQ3Ng==", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-26T06:32:02Z", "updated_at": "2016-04-26T06:32:02Z", "author_association": "NONE", "body_html": "<p>sometimes the mask is generated dynamically (i.e., the mask is a function<br>\nof the previous attention)...</p>\n<p>for example in speech recognition: Bahdanau/Chorowski et. al.,<br>\n<a href=\"http://arxiv.org/pdf/1508.04395.pdf\" rel=\"nofollow\">http://arxiv.org/pdf/1508.04395.pdf</a><br>\n<a href=\"http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf\" rel=\"nofollow\">http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf</a></p>\n<h2></h2>\n<p>William Chan<br>\nCarnegie Mellon University<br>\n(650) 450-9455<br>\nwilliamchan.ca</p>\n<p>On Mon, Apr 25, 2016 at 11:28 PM, Lukasz Kaiser <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>I agree that it would be good to add masking to the attention model and<br>\nAPI. But I think adding an op and a separate cuda kernel just for that is a<br>\nhuge overkill. Why not just multiply pointwise by the mask in python before<br>\nthe softmax? Without a good reason, I find a PR like this not acceptable,<br>\nthe code complexity and maintenance cost looks much larger than the simple<br>\nproblem it solves.</p>\n<p>\u2014<br>\nYou are receiving this because you authored the thread.<br>\nReply to this email directly or view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"150634072\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2079\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/2079/hovercard?comment_id=214628517&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/pull/2079#issuecomment-214628517\">#2079 (comment)</a></p>\n</blockquote>", "body_text": "sometimes the mask is generated dynamically (i.e., the mask is a function\nof the previous attention)...\nfor example in speech recognition: Bahdanau/Chorowski et. al.,\nhttp://arxiv.org/pdf/1508.04395.pdf\nhttp://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf\n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\nOn Mon, Apr 25, 2016 at 11:28 PM, Lukasz Kaiser notifications@github.com\nwrote:\n\nI agree that it would be good to add masking to the attention model and\nAPI. But I think adding an op and a separate cuda kernel just for that is a\nhuge overkill. Why not just multiply pointwise by the mask in python before\nthe softmax? Without a good reason, I find a PR like this not acceptable,\nthe code complexity and maintenance cost looks much larger than the simple\nproblem it solves.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n#2079 (comment)", "body": "sometimes the mask is generated dynamically (i.e., the mask is a function\nof the previous attention)...\n\nfor example in speech recognition: Bahdanau/Chorowski et. al.,\nhttp://arxiv.org/pdf/1508.04395.pdf\nhttp://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Mon, Apr 25, 2016 at 11:28 PM, Lukasz Kaiser notifications@github.com\nwrote:\n\n> I agree that it would be good to add masking to the attention model and\n> API. But I think adding an op and a separate cuda kernel just for that is a\n> huge overkill. Why not just multiply pointwise by the mask in python before\n> the softmax? Without a good reason, I find a PR like this not acceptable,\n> the code complexity and maintenance cost looks much larger than the simple\n> problem it solves.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2079#issuecomment-214628517\n"}