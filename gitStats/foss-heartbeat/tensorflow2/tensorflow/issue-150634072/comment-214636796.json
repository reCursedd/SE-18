{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/214636796", "html_url": "https://github.com/tensorflow/tensorflow/pull/2079#issuecomment-214636796", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2079", "id": 214636796, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNDYzNjc5Ng==", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-26T06:51:33Z", "updated_at": "2016-04-26T06:51:33Z", "author_association": "NONE", "body_html": "<p>Well, the python side does not know what mask to generate (i.e., it doesnt<br>\nknow the sequence_len of each sample in the minibatch). This is especially<br>\ntrue in speech where your minibatch has a high variance in sequence_len.<br>\nHowever, this problem can be somewhat alleviated to generate the mask of 1<br>\nand -FLT_MAX (not 1 and 0), if our input layer generates that for us.</p>\n<p>However, this is not true for all attention masks, for example in speech<br>\nrecognition where the encoder length can be thousands of frames, we want<br>\nthe attention mask to be a function of the previous attention alignment,<br>\ni.e., in the papers I cited above, they generate the mask by looking at the<br>\nmedian of the previous attention and then use a window around it (in<br>\nessence, you have a somewhat monotonic sliding window on where to attend<br>\nto). Correct me if I am wrong, but I believe this can not be generated<br>\npython side?</p>\n<p>Its not a speed advantage, (correct me if I am wrong), but I don't think<br>\nyou can mask properly w/o an op like this.</p>\n<h2></h2>\n<p>William Chan<br>\nCarnegie Mellon University<br>\n(650) 450-9455<br>\nwilliamchan.ca</p>\n<p>On Mon, Apr 25, 2016 at 11:45 PM, Lukasz Kaiser <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>It is possible that I don't understand the problem, so please, clarify.<br>\nBut why would it need a special op to make the mask a function? If it's a<br>\ntensor of 0s and 1s, it can be changed at every step, why can't this be<br>\ndone the simple way in python? Is there a very big speed advantage to<br>\nhaving an extra op?</p>\n<p>\u2014<br>\nYou are receiving this because you authored the thread.<br>\nReply to this email directly or view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"150634072\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2079\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/2079/hovercard?comment_id=214635456&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/pull/2079#issuecomment-214635456\">#2079 (comment)</a></p>\n</blockquote>", "body_text": "Well, the python side does not know what mask to generate (i.e., it doesnt\nknow the sequence_len of each sample in the minibatch). This is especially\ntrue in speech where your minibatch has a high variance in sequence_len.\nHowever, this problem can be somewhat alleviated to generate the mask of 1\nand -FLT_MAX (not 1 and 0), if our input layer generates that for us.\nHowever, this is not true for all attention masks, for example in speech\nrecognition where the encoder length can be thousands of frames, we want\nthe attention mask to be a function of the previous attention alignment,\ni.e., in the papers I cited above, they generate the mask by looking at the\nmedian of the previous attention and then use a window around it (in\nessence, you have a somewhat monotonic sliding window on where to attend\nto). Correct me if I am wrong, but I believe this can not be generated\npython side?\nIts not a speed advantage, (correct me if I am wrong), but I don't think\nyou can mask properly w/o an op like this.\n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\nOn Mon, Apr 25, 2016 at 11:45 PM, Lukasz Kaiser notifications@github.com\nwrote:\n\nIt is possible that I don't understand the problem, so please, clarify.\nBut why would it need a special op to make the mask a function? If it's a\ntensor of 0s and 1s, it can be changed at every step, why can't this be\ndone the simple way in python? Is there a very big speed advantage to\nhaving an extra op?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n#2079 (comment)", "body": "Well, the python side does not know what mask to generate (i.e., it doesnt\nknow the sequence_len of each sample in the minibatch). This is especially\ntrue in speech where your minibatch has a high variance in sequence_len.\nHowever, this problem can be somewhat alleviated to generate the mask of 1\nand -FLT_MAX (not 1 and 0), if our input layer generates that for us.\n\nHowever, this is not true for all attention masks, for example in speech\nrecognition where the encoder length can be thousands of frames, we want\nthe attention mask to be a function of the previous attention alignment,\ni.e., in the papers I cited above, they generate the mask by looking at the\nmedian of the previous attention and then use a window around it (in\nessence, you have a somewhat monotonic sliding window on where to attend\nto). Correct me if I am wrong, but I believe this can not be generated\npython side?\n\nIts not a speed advantage, (correct me if I am wrong), but I don't think\nyou can mask properly w/o an op like this.\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Mon, Apr 25, 2016 at 11:45 PM, Lukasz Kaiser notifications@github.com\nwrote:\n\n> It is possible that I don't understand the problem, so please, clarify.\n> But why would it need a special op to make the mask a function? If it's a\n> tensor of 0s and 1s, it can be changed at every step, why can't this be\n> done the simple way in python? Is there a very big speed advantage to\n> having an extra op?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2079#issuecomment-214635456\n"}