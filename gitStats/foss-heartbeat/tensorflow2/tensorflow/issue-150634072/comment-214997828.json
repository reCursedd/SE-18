{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/214997828", "html_url": "https://github.com/tensorflow/tensorflow/pull/2079#issuecomment-214997828", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2079", "id": 214997828, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNDk5NzgyOA==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-27T07:50:07Z", "updated_at": "2016-04-27T07:50:07Z", "author_association": "MEMBER", "body_html": "<p>To answer your question, let me be a little bit more precise. What I suggest is to modify the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py\">seq2seq.py</a> in two ways.</p>\n<p>(1) on lines 546, 565, 567 instead of <code>attns = attention(state)</code> let's do <code>attns = attention(state, attns, inp, i)</code> so the <code>attention()</code> function now takes the arguments <code>attention(query, previous_attention, decoder_input, step_number)</code></p>\n<p>(2) on line 530 instead of <code>a = nn_ops.softmax(s)</code> let's do <code>a = nn_ops.softmax(s + mask_f(previous_attention, decoder_input, step_number) * -LARGE_NUMBER</code> where <code>mask_f</code> is an extra argument to the decoder.</p>\n<p>In this way the mask_f function gets all the current information in the step. Its task is to generate 1.0 (or anything positive) if we want to mask something and 0.0 otherwise. The 1.0 will be turned into a -LARGE_NUMBER bias in the softmax and effectively disable that field for attention, right?</p>\n<p>The tensor s has shape [batch_size x attn_length], where attn_length is the size of the attention_states (length of the encoder in a seq2seq model). Let me focus on the basic case when we want mask_f to mask an input iff the decoder input symbol is a padding, which means that it is all zeros. We can do this in two ways, I think. If we're sure the decoder input is all 0s only if it's padded and has a reasonably large norm otherwise, then we can do <code>mask_f(_, dec_inp, _) = tf.minimum(1.0, tf.nn.l2_loss(dec_inp) * LARGE)</code>. If we're not sure of that, then we can do <code>mask_f(_, _, step) = self.decoder_mask[i]</code> when declaring the <code>mask_f</code> function somewhere in our model code. In this case <code>decoder_mask</code> are placeholders of the same kind as <code>decoder_inputs</code> that are 1 in case of padding and 0 otherwise -- and we need to make sure to feed the appropriate values.</p>\n<p>I'm sure I missed some things. In particular, <code>previous_attention</code> coming to the <code>attention</code> function should probably be the previous attention mask, not the previous attention vector as I suggested above (so the new mask needs to be returned in the function and the pointer saved in the loop, a few more lines). But does the general idea sound reasonable and flexible enough?</p>", "body_text": "To answer your question, let me be a little bit more precise. What I suggest is to modify the seq2seq.py in two ways.\n(1) on lines 546, 565, 567 instead of attns = attention(state) let's do attns = attention(state, attns, inp, i) so the attention() function now takes the arguments attention(query, previous_attention, decoder_input, step_number)\n(2) on line 530 instead of a = nn_ops.softmax(s) let's do a = nn_ops.softmax(s + mask_f(previous_attention, decoder_input, step_number) * -LARGE_NUMBER where mask_f is an extra argument to the decoder.\nIn this way the mask_f function gets all the current information in the step. Its task is to generate 1.0 (or anything positive) if we want to mask something and 0.0 otherwise. The 1.0 will be turned into a -LARGE_NUMBER bias in the softmax and effectively disable that field for attention, right?\nThe tensor s has shape [batch_size x attn_length], where attn_length is the size of the attention_states (length of the encoder in a seq2seq model). Let me focus on the basic case when we want mask_f to mask an input iff the decoder input symbol is a padding, which means that it is all zeros. We can do this in two ways, I think. If we're sure the decoder input is all 0s only if it's padded and has a reasonably large norm otherwise, then we can do mask_f(_, dec_inp, _) = tf.minimum(1.0, tf.nn.l2_loss(dec_inp) * LARGE). If we're not sure of that, then we can do mask_f(_, _, step) = self.decoder_mask[i] when declaring the mask_f function somewhere in our model code. In this case decoder_mask are placeholders of the same kind as decoder_inputs that are 1 in case of padding and 0 otherwise -- and we need to make sure to feed the appropriate values.\nI'm sure I missed some things. In particular, previous_attention coming to the attention function should probably be the previous attention mask, not the previous attention vector as I suggested above (so the new mask needs to be returned in the function and the pointer saved in the loop, a few more lines). But does the general idea sound reasonable and flexible enough?", "body": "To answer your question, let me be a little bit more precise. What I suggest is to modify the [seq2seq.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py) in two ways.\n\n(1) on lines 546, 565, 567 instead of `attns = attention(state)` let's do `attns = attention(state, attns, inp, i)` so the `attention()` function now takes the arguments `attention(query, previous_attention, decoder_input, step_number)`\n\n(2) on line 530 instead of `a = nn_ops.softmax(s)` let's do `a = nn_ops.softmax(s + mask_f(previous_attention, decoder_input, step_number) * -LARGE_NUMBER` where `mask_f` is an extra argument to the decoder.\n\nIn this way the mask_f function gets all the current information in the step. Its task is to generate 1.0 (or anything positive) if we want to mask something and 0.0 otherwise. The 1.0 will be turned into a -LARGE_NUMBER bias in the softmax and effectively disable that field for attention, right?\n\nThe tensor s has shape [batch_size x attn_length], where attn_length is the size of the attention_states (length of the encoder in a seq2seq model). Let me focus on the basic case when we want mask_f to mask an input iff the decoder input symbol is a padding, which means that it is all zeros. We can do this in two ways, I think. If we're sure the decoder input is all 0s only if it's padded and has a reasonably large norm otherwise, then we can do `mask_f(_, dec_inp, _) = tf.minimum(1.0, tf.nn.l2_loss(dec_inp) * LARGE)`. If we're not sure of that, then we can do `mask_f(_, _, step) = self.decoder_mask[i]` when declaring the `mask_f` function somewhere in our model code. In this case `decoder_mask` are placeholders of the same kind as `decoder_inputs` that are 1 in case of padding and 0 otherwise -- and we need to make sure to feed the appropriate values.\n\nI'm sure I missed some things. In particular, `previous_attention` coming to the `attention` function should probably be the previous attention mask, not the previous attention vector as I suggested above (so the new mask needs to be returned in the function and the pointer saved in the loop, a few more lines). But does the general idea sound reasonable and flexible enough?\n"}