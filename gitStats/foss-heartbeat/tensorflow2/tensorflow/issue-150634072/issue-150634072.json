{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2079", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2079/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2079/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2079/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/2079", "id": 150634072, "node_id": "MDExOlB1bGxSZXF1ZXN0Njc2MjkxMzc=", "number": 2079, "title": "Attention Mask Ops", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2016-04-24T06:35:41Z", "updated_at": "2016-05-12T16:42:10Z", "closed_at": "2016-05-12T16:42:10Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2079", "html_url": "https://github.com/tensorflow/tensorflow/pull/2079", "diff_url": "https://github.com/tensorflow/tensorflow/pull/2079.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/2079.patch"}, "body_html": "<p>The current attention seq2seq example (i.e., in seq2seq.py function attention_decoder) has a bug. The code (quoted below):</p>\n<p>520     def attention(query):<br>\n521       \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"<br>\n522       ds = []  # Results of attention reads will be stored here.<br>\n523       for a in xrange(num_heads):<br>\n524         with variable_scope.variable_scope(\"Attention_%d\" % a):<br>\n525           y = rnn_cell.linear(query, attention_vec_size, True)<br>\n526           y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])<br>\n527           # Attention mask is a softmax of v^T * tanh(...).<br>\n528           s = math_ops.reduce_sum(<br>\n529               v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])<br>\n530           a = nn_ops.softmax(s)<br>\n531           # Now calculate the attention-weighted vector d.<br>\n532           d = math_ops.reduce_sum(<br>\n533               array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,<br>\n534               [1, 2])<br>\n535           ds.append(array_ops.reshape(d, [-1, attn_size]))<br>\n536       return ds</p>\n<p>does not do a masking op before the softmax. This git pull request will fix this problem, i.e., we should mask the attention energies before the softmax based on the encoder sequence length.</p>", "body_text": "The current attention seq2seq example (i.e., in seq2seq.py function attention_decoder) has a bug. The code (quoted below):\n520     def attention(query):\n521       \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n522       ds = []  # Results of attention reads will be stored here.\n523       for a in xrange(num_heads):\n524         with variable_scope.variable_scope(\"Attention_%d\" % a):\n525           y = rnn_cell.linear(query, attention_vec_size, True)\n526           y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n527           # Attention mask is a softmax of v^T * tanh(...).\n528           s = math_ops.reduce_sum(\n529               v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n530           a = nn_ops.softmax(s)\n531           # Now calculate the attention-weighted vector d.\n532           d = math_ops.reduce_sum(\n533               array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n534               [1, 2])\n535           ds.append(array_ops.reshape(d, [-1, attn_size]))\n536       return ds\ndoes not do a masking op before the softmax. This git pull request will fix this problem, i.e., we should mask the attention energies before the softmax based on the encoder sequence length.", "body": "The current attention seq2seq example (i.e., in seq2seq.py function attention_decoder) has a bug. The code (quoted below):\n\n520     def attention(query):\n521       \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n522       ds = []  # Results of attention reads will be stored here.\n523       for a in xrange(num_heads):\n524         with variable_scope.variable_scope(\"Attention_%d\" % a):\n525           y = rnn_cell.linear(query, attention_vec_size, True)\n526           y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n527           # Attention mask is a softmax of v^T \\* tanh(...).\n528           s = math_ops.reduce_sum(\n529               v[a] \\* math_ops.tanh(hidden_features[a] + y), [2, 3])\n530           a = nn_ops.softmax(s)\n531           # Now calculate the attention-weighted vector d.\n532           d = math_ops.reduce_sum(\n533               array_ops.reshape(a, [-1, attn_length, 1, 1]) \\* hidden,\n534               [1, 2])\n535           ds.append(array_ops.reshape(d, [-1, attn_size]))\n536       return ds\n\ndoes not do a masking op before the softmax. This git pull request will fix this problem, i.e., we should mask the attention energies before the softmax based on the encoder sequence length.\n"}