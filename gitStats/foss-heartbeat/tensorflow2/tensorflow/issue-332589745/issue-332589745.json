{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20041", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20041/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20041/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20041/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20041", "id": 332589745, "node_id": "MDU6SXNzdWUzMzI1ODk3NDU=", "number": 20041, "title": "Failed to create CUPTI subcriber when profiling on servers with no GPU's", "user": {"login": "Majoros", "id": 56639, "node_id": "MDQ6VXNlcjU2NjM5", "avatar_url": "https://avatars3.githubusercontent.com/u/56639?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Majoros", "html_url": "https://github.com/Majoros", "followers_url": "https://api.github.com/users/Majoros/followers", "following_url": "https://api.github.com/users/Majoros/following{/other_user}", "gists_url": "https://api.github.com/users/Majoros/gists{/gist_id}", "starred_url": "https://api.github.com/users/Majoros/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Majoros/subscriptions", "organizations_url": "https://api.github.com/users/Majoros/orgs", "repos_url": "https://api.github.com/users/Majoros/repos", "events_url": "https://api.github.com/users/Majoros/events{/privacy}", "received_events_url": "https://api.github.com/users/Majoros/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-06-14T22:43:59Z", "updated_at": "2018-11-14T19:20:35Z", "closed_at": null, "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n</ul>\n<p>Custom i guess? I took the first example on the <a href=\"https://www.tensorflow.org/programmers_guide/using_gpu\" rel=\"nofollow\">Using GPUs</a> page and added profiling to it (code below).</p>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n</ul>\n<p>Red Hat Enterprise Linux Server release 6.7 (Santiago)</p>\n<ul>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n</ul>\n<p>Source</p>\n<ul>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n</ul>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>import tensorflow as tf<br>\nprint(tf.GIT_VERSION, tf.VERSION)<br>\n('unknown', '1.8.0')</p>\n</blockquote>\n</blockquote>\n</blockquote>\n<ul>\n<li><strong>Python version</strong>:</li>\n</ul>\n<p>Built for both version of python same outcome for both<br>\n2.7.14<br>\n3.6.5</p>\n<ul>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n</ul>\n<p>0.11.1</p>\n<ul>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n</ul>\n<p>4.9.3</p>\n<ul>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n</ul>\n<p>CUDA: 9.1.85<br>\ncuDNN: 7.0.5</p>\n<ul>\n<li><strong>GPU model and memory</strong>:</li>\n</ul>\n<p>My GPU test server has the following GPU's and driver version</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ nvidia-smi\nThu Jun 14 12:58:15 2018\n+-----------------------------------------------------------------------------+\n<span class=\"pl-k\">|</span> NVIDIA-SMI 390.30                 Driver Version: 390.30                    <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span> GPU  Name        Persistence-M<span class=\"pl-k\">|</span> Bus-Id        Disp.A <span class=\"pl-k\">|</span> Volatile Uncorr. ECC <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> Fan  Temp  Perf  Pwr:Usage/Cap<span class=\"pl-k\">|</span>         Memory-Usage <span class=\"pl-k\">|</span> GPU-Util  Compute M. <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>===============================+======================+======================<span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>   0  Tesla K40m          On   <span class=\"pl-k\">|</span> 00000000:0B:00.0 Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   24C    P8    19W / 235W <span class=\"pl-k\">|</span>    207MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n<span class=\"pl-k\">|</span>   1  Tesla K40m          On   <span class=\"pl-k\">|</span> 00000000:81:00.0 Off <span class=\"pl-k\">|</span>                    0 <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> N/A   25C    P8    20W / 235W <span class=\"pl-k\">|</span>    207MiB / 11441MiB <span class=\"pl-k\">|</span>      0%      Default <span class=\"pl-k\">|</span>\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n<span class=\"pl-k\">|</span> Processes:                                                       GPU Memory <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>  GPU       PID   Type   Process name                             Usage      <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>=============================================================================<span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span>  No running processes found                                                 <span class=\"pl-k\">|</span>\n+-----------------------------------------------------------------------------+</pre></div>\n<ul>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>Run the following script with TF built with GPU support on a server without GPU's with the cuda stub and cupti extras library path's in your LD_LIBRARY_PATH environment variable.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> os\ntrace_dir <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tmp/tf_trace<span class=\"pl-pds\">'</span></span>\n\nbuilder <span class=\"pl-k\">=</span> tf.profiler.ProfileOptionBuilder\nopts <span class=\"pl-k\">=</span> builder(builder.time_and_memory()).order_by(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>micros<span class=\"pl-pds\">'</span></span>).build()\n\n<span class=\"pl-k\">with</span> tf.contrib.tfprof.ProfileContext(trace_dir, <span class=\"pl-v\">trace_steps</span><span class=\"pl-k\">=</span>[], <span class=\"pl-v\">dump_steps</span><span class=\"pl-k\">=</span>[]) <span class=\"pl-k\">as</span> pctx:\n    a <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">2.0</span>, <span class=\"pl-c1\">3.0</span>, <span class=\"pl-c1\">4.0</span>, <span class=\"pl-c1\">5.0</span>, <span class=\"pl-c1\">6.0</span>], <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>a<span class=\"pl-pds\">'</span></span>)\n    b <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">2.0</span>, <span class=\"pl-c1\">3.0</span>, <span class=\"pl-c1\">4.0</span>, <span class=\"pl-c1\">5.0</span>, <span class=\"pl-c1\">6.0</span>], <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>b<span class=\"pl-pds\">'</span></span>)\n    c <span class=\"pl-k\">=</span> tf.matmul(a, b)\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        pctx.trace_next_step()\n        pctx.dump_next_step()\n        _ <span class=\"pl-k\">=</span> sess.run(c)\n        pctx.profiler.profile_operations(<span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>opts)\n</pre></div>\n<p>Works perfect on servers with GPU's fails on servers without GPU's</p>\n<h3>Describe the problem</h3>\n<p>I am attempting to make a single, portable version of TensorFlow 1.8.0 with XLA and GPU support. Everything works as expected on servers with GPU's.<br>\nThe only thing that\u2019s not working on servers without GPU's is when you run a session in a profiler context, you get the below message.</p>\n<p>The error seems fairly straight forward. The calls to the libcupti are failing since there are no GPU's or cuda drivers installed.</p>\n<p>How feasible would it be to have TensorFlow fallback(or do a pre-check) to CPU only profiling if TensorFlow was built with GPU support but there are no GPU's on the servers?</p>\n<h3>Source code / logs</h3>\n<p>Note: I manually truncated the paths of the log files up until the lib directory.</p>\n<p>$ python ~/tmp/tfprof.py<br>\n2018-06-14 17:32:07.348377: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<br>\n2018-06-14 17:32:07.354865: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUresult(-1)<br>\n2018-06-14 17:32:07.354921: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (HOST_NAME): /proc/driver/nvidia/version does not exist<br>\nNone<br>\n2018-06-14 17:32:07.360597: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcupti.so.9.1 locally<br>\n2018-06-14 17:32:07.460813: E tensorflow/core/platform/default/device_tracer.cc:134] cuda call ActivityRegisterCallbacks(BufferRequested, BufferCompleted) failed 15<br>\nFailed to create CUPTI subcriber.<br>\nTraceback (most recent call last):<br>\nFile \"lib/tensorflow/python/client/session.py\", line 1323, in _do_call<br>\nreturn fn(*args)<br>\nFile \"/home/o594256/tmp/tf18/lib/tensorflow/python/client/session.py\", line 1308, in _run_fn<br>\noptions, feed_dict, fetch_list, target_list, run_metadata)<br>\nFile \"lib/tensorflow/python/client/session.py\", line 1411, in _call_tf_sessionrun<br>\nrun_metadata)<br>\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.</p>\n<p>During handling of the above exception, another exception occurred:</p>\n<p>Traceback (most recent call last):<br>\nFile \"/tmp/tfprof.py\", line 24, in <br>\n_ = sess.run(c)<br>\nFile \"lib/tensorflow/python/profiler/profile_context.py\", line 74, in _profiled_run<br>\nfetches, feed_dict, options, run_metadata)<br>\nFile \"lib/tensorflow/python/client/session.py\", line 900, in run<br>\nrun_metadata_ptr)<br>\nFile \"lib/tensorflow/python/client/session.py\", line 1136, in _run<br>\nfeed_dict_tensor, options, run_metadata)<br>\nFile \"lib/tensorflow/python/client/session.py\", line 1317, in _do_run<br>\nrun_metadata)<br>\nFile \"lib/tensorflow/python/client/session.py\", line 1337, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nCustom i guess? I took the first example on the Using GPUs page and added profiling to it (code below).\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\nRed Hat Enterprise Linux Server release 6.7 (Santiago)\n\nTensorFlow installed from (source or binary):\n\nSource\n\nTensorFlow version (use command below):\n\n\n\n\nimport tensorflow as tf\nprint(tf.GIT_VERSION, tf.VERSION)\n('unknown', '1.8.0')\n\n\n\n\nPython version:\n\nBuilt for both version of python same outcome for both\n2.7.14\n3.6.5\n\nBazel version (if compiling from source):\n\n0.11.1\n\nGCC/Compiler version (if compiling from source):\n\n4.9.3\n\nCUDA/cuDNN version:\n\nCUDA: 9.1.85\ncuDNN: 7.0.5\n\nGPU model and memory:\n\nMy GPU test server has the following GPU's and driver version\n$ nvidia-smi\nThu Jun 14 12:58:15 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40m          On   | 00000000:0B:00.0 Off |                    0 |\n| N/A   24C    P8    19W / 235W |    207MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K40m          On   | 00000000:81:00.0 Off |                    0 |\n| N/A   25C    P8    20W / 235W |    207MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\nExact command to reproduce:\n\nRun the following script with TF built with GPU support on a server without GPU's with the cuda stub and cupti extras library path's in your LD_LIBRARY_PATH environment variable.\nimport tensorflow as tf\nimport os\ntrace_dir = '/tmp/tf_trace'\n\nbuilder = tf.profiler.ProfileOptionBuilder\nopts = builder(builder.time_and_memory()).order_by('micros').build()\n\nwith tf.contrib.tfprof.ProfileContext(trace_dir, trace_steps=[], dump_steps=[]) as pctx:\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n    c = tf.matmul(a, b)\n    with tf.Session() as sess:\n        pctx.trace_next_step()\n        pctx.dump_next_step()\n        _ = sess.run(c)\n        pctx.profiler.profile_operations(options=opts)\n\nWorks perfect on servers with GPU's fails on servers without GPU's\nDescribe the problem\nI am attempting to make a single, portable version of TensorFlow 1.8.0 with XLA and GPU support. Everything works as expected on servers with GPU's.\nThe only thing that\u2019s not working on servers without GPU's is when you run a session in a profiler context, you get the below message.\nThe error seems fairly straight forward. The calls to the libcupti are failing since there are no GPU's or cuda drivers installed.\nHow feasible would it be to have TensorFlow fallback(or do a pre-check) to CPU only profiling if TensorFlow was built with GPU support but there are no GPU's on the servers?\nSource code / logs\nNote: I manually truncated the paths of the log files up until the lib directory.\n$ python ~/tmp/tfprof.py\n2018-06-14 17:32:07.348377: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-06-14 17:32:07.354865: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUresult(-1)\n2018-06-14 17:32:07.354921: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (HOST_NAME): /proc/driver/nvidia/version does not exist\nNone\n2018-06-14 17:32:07.360597: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcupti.so.9.1 locally\n2018-06-14 17:32:07.460813: E tensorflow/core/platform/default/device_tracer.cc:134] cuda call ActivityRegisterCallbacks(BufferRequested, BufferCompleted) failed 15\nFailed to create CUPTI subcriber.\nTraceback (most recent call last):\nFile \"lib/tensorflow/python/client/session.py\", line 1323, in _do_call\nreturn fn(*args)\nFile \"/home/o594256/tmp/tf18/lib/tensorflow/python/client/session.py\", line 1308, in _run_fn\noptions, feed_dict, fetch_list, target_list, run_metadata)\nFile \"lib/tensorflow/python/client/session.py\", line 1411, in _call_tf_sessionrun\nrun_metadata)\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \"/tmp/tfprof.py\", line 24, in \n_ = sess.run(c)\nFile \"lib/tensorflow/python/profiler/profile_context.py\", line 74, in _profiled_run\nfetches, feed_dict, options, run_metadata)\nFile \"lib/tensorflow/python/client/session.py\", line 900, in run\nrun_metadata_ptr)\nFile \"lib/tensorflow/python/client/session.py\", line 1136, in _run\nfeed_dict_tensor, options, run_metadata)\nFile \"lib/tensorflow/python/client/session.py\", line 1317, in _do_run\nrun_metadata)\nFile \"lib/tensorflow/python/client/session.py\", line 1337, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nCustom i guess? I took the first example on the [Using GPUs](https://www.tensorflow.org/programmers_guide/using_gpu) page and added profiling to it (code below).\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nRed Hat Enterprise Linux Server release 6.7 (Santiago)\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n>>> import tensorflow as tf\r\n>>> print(tf.GIT_VERSION, tf.VERSION)\r\n('unknown', '1.8.0')\r\n\r\n\r\n- **Python version**:\r\n\r\nBuilt for both version of python same outcome for both\r\n2.7.14\r\n3.6.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\n0.11.1\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\n4.9.3\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nCUDA: 9.1.85\r\ncuDNN: 7.0.5\r\n\r\n- **GPU model and memory**:\r\n\r\nMy GPU test server has the following GPU's and driver version\r\n\r\n```bash\r\n$ nvidia-smi\r\nThu Jun 14 12:58:15 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K40m          On   | 00000000:0B:00.0 Off |                    0 |\r\n| N/A   24C    P8    19W / 235W |    207MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K40m          On   | 00000000:81:00.0 Off |                    0 |\r\n| N/A   25C    P8    20W / 235W |    207MiB / 11441MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n- **Exact command to reproduce**:\r\n\r\nRun the following script with TF built with GPU support on a server without GPU's with the cuda stub and cupti extras library path's in your LD_LIBRARY_PATH environment variable.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\ntrace_dir = '/tmp/tf_trace'\r\n\r\nbuilder = tf.profiler.ProfileOptionBuilder\r\nopts = builder(builder.time_and_memory()).order_by('micros').build()\r\n\r\nwith tf.contrib.tfprof.ProfileContext(trace_dir, trace_steps=[], dump_steps=[]) as pctx:\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\n    with tf.Session() as sess:\r\n        pctx.trace_next_step()\r\n        pctx.dump_next_step()\r\n        _ = sess.run(c)\r\n        pctx.profiler.profile_operations(options=opts)\r\n\r\n```\r\n\r\nWorks perfect on servers with GPU's fails on servers without GPU's\r\n\r\n### Describe the problem\r\n\r\nI am attempting to make a single, portable version of TensorFlow 1.8.0 with XLA and GPU support. Everything works as expected on servers with GPU's.\r\nThe only thing that\u2019s not working on servers without GPU's is when you run a session in a profiler context, you get the below message.\r\n\r\nThe error seems fairly straight forward. The calls to the libcupti are failing since there are no GPU's or cuda drivers installed.\r\n\r\nHow feasible would it be to have TensorFlow fallback(or do a pre-check) to CPU only profiling if TensorFlow was built with GPU support but there are no GPU's on the servers?\r\n\r\n\r\n### Source code / logs\r\n\r\nNote: I manually truncated the paths of the log files up until the lib directory.\r\n\r\n$ python ~/tmp/tfprof.py\r\n2018-06-14 17:32:07.348377: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-06-14 17:32:07.354865: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUresult(-1)\r\n2018-06-14 17:32:07.354921: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (HOST_NAME): /proc/driver/nvidia/version does not exist\r\nNone\r\n2018-06-14 17:32:07.360597: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcupti.so.9.1 locally\r\n2018-06-14 17:32:07.460813: E tensorflow/core/platform/default/device_tracer.cc:134] cuda call ActivityRegisterCallbacks(BufferRequested, BufferCompleted) failed 15\r\nFailed to create CUPTI subcriber.\r\nTraceback (most recent call last):\r\n  File \"lib/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/o594256/tmp/tf18/lib/tensorflow/python/client/session.py\", line 1308, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"lib/tensorflow/python/client/session.py\", line 1411, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp/tfprof.py\", line 24, in <module>\r\n    _ = sess.run(c)\r\n  File \"lib/tensorflow/python/profiler/profile_context.py\", line 74, in _profiled_run\r\n    fetches, feed_dict, options, run_metadata)\r\n  File \"lib/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"lib/tensorflow/python/client/session.py\", line 1136, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"lib/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    run_metadata)\r\n  File \"lib/tensorflow/python/client/session.py\", line 1337, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create CUPTI subcriber.\r\n"}