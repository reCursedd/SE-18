{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11574", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11574/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11574/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11574/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11574", "id": 243692871, "node_id": "MDU6SXNzdWUyNDM2OTI4NzE=", "number": 11574, "title": "The same code for tensorboard test generate .mm file which should be .user file", "user": {"login": "LaiPiXiong", "id": 15033269, "node_id": "MDQ6VXNlcjE1MDMzMjY5", "avatar_url": "https://avatars2.githubusercontent.com/u/15033269?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LaiPiXiong", "html_url": "https://github.com/LaiPiXiong", "followers_url": "https://api.github.com/users/LaiPiXiong/followers", "following_url": "https://api.github.com/users/LaiPiXiong/following{/other_user}", "gists_url": "https://api.github.com/users/LaiPiXiong/gists{/gist_id}", "starred_url": "https://api.github.com/users/LaiPiXiong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LaiPiXiong/subscriptions", "organizations_url": "https://api.github.com/users/LaiPiXiong/orgs", "repos_url": "https://api.github.com/users/LaiPiXiong/repos", "events_url": "https://api.github.com/users/LaiPiXiong/events{/privacy}", "received_events_url": "https://api.github.com/users/LaiPiXiong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-07-18T12:04:05Z", "updated_at": "2017-07-18T12:43:28Z", "closed_at": "2017-07-18T12:43:28Z", "author_association": "NONE", "body_html": "<p>I use my classmate's file for tensorboard test. But, this .py file generate .mm file not a .user file.</p>\n<p>Codes are as follows:<br>\nimport tensorflow as tf<br>\nimport math<br>\nfrom tensorflow.examples.tutorials.mnist import input_data<br>\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)</p>\n<p>def conv_layer(input,channels_in,channels_out,strides, name=\"conv\",):<br>\nwith tf.name_scope(name):<br>\nw = tf.Variable(tf.truncated_normal([5,5,channels_in,channels_out],stddev=0.1),name=\"W\")<br>\nb = tf.Variable(tf.ones([channels_out])/10,name=\"B\")<br>\nconv = tf.nn.conv2d(input=input,filter=w,strides=[1,strides,strides,1],padding=\"SAME\")<br>\n# print conv<br>\nact = tf.nn.relu(conv + b)<br>\ntf.summary.histogram(\"weight\",w)<br>\ntf.summary.histogram(\"bias\",b)<br>\ntf.summary.histogram(\"activations\",act)<br>\n# print act<br>\nreturn act</p>\n<p>def fc_layer(input,channels_in,channels_out, name=\"fc\"):<br>\nwith tf.name_scope(name):<br>\nw = tf.Variable(tf.truncated_normal([channels_in,channels_out],stddev=0.1),name=\"W\")<br>\nb = tf.Variable(tf.ones([channels_out])/10,name=\"B\")<br>\nprint input<br>\nact = tf.nn.relu(tf.matmul(input, w) + b)<br>\n# act = tf.matmul(input, w) + b<br>\nreturn act</p>\n<p>def compatible_convolutional_noise_shape(Y):<br>\nnoiseshape = tf.shape(Y)<br>\nnoiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])<br>\nreturn noiseshape</p>\n<p>x = tf.placeholder(dtype=tf.float32,shape=[None,784],name=\"x\")<br>\ny = tf.placeholder(dtype=tf.float32,shape=[None,10],name=\"lables\")</p>\n<h1>learning rate</h1>\n<p>lr = tf.placeholder(tf.float32)</p>\n<h1>dropout</h1>\n<p>pkeep = tf.placeholder(tf.float32)<br>\npkeep_conv = tf.placeholder(tf.float32)</p>\n<h1>batch_normal</h1>\n<p>tst = tf.placeholder(tf.bool)<br>\niter = tf.placeholder(tf.float32)</p>\n<p>x_image = tf.reshape(x,shape=[-1,28,28,1])<br>\ntf.summary.image(\"input\",x_image,6)</p>\n<h1>creat net</h1>\n<p>conv1 = conv_layer(x_image,1,4,1,\"conv_1\")</p>\n<h1>conv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))</h1>\n<h1>pool1 = tf.nn.max_pool(value=conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")</h1>\n<p>conv2 = conv_layer(conv1,4,8,2,\"conv_2\")</p>\n<h1>conv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))</h1>\n<h1>pool2 = tf.nn.max_pool(value=conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")</h1>\n<p>conv3 = conv_layer(conv2,8,16,2,\"conv_3\")</p>\n<p>flattened = tf.reshape(conv3,[-1, 7 * 7 * 16])<br>\nfc1 = fc_layer(flattened,7 * 7 * 16, 200,\"fc1\")</p>\n<h1>fc1_dr = tf.nn.dropout(fc1,pkeep)</h1>\n<p>w5 = tf.Variable(tf.truncated_normal([200,10],stddev=0.1))<br>\nb5 = tf.Variable(tf.ones([10])/10)<br>\nlogits = tf.matmul(fc1,w5)+b5</p>\n<p>with tf.name_scope(\"loss\"):<br>\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits))<br>\ntf.summary.scalar(\"loss\",cross_entropy)<br>\nwith tf.name_scope(\"BP\"):<br>\ntrain_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)</p>\n<p>with tf.name_scope(\"accuracy\"):<br>\ncorrect_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))<br>\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))<br>\ntf.summary.scalar(\"accuracy\",accuracy)</p>\n<h1>writer = tf.summary.FileWriter(\"~/workspace/TensorFlow_Test/1\")</h1>\n<p>with tf.Session() as sess:<br>\nsess.run(tf.global_variables_initializer())<br>\nmerged_summary = tf.summary.merge_all()<br>\nwriter = tf.summary.FileWriter(\"/tmp/mnist_demo/\")<br>\nwriter.add_graph(sess.graph)</p>\n<pre><code># learning rate decay\nmax_learning_rate = 0.02\nmin_learning_rate = 0.001\ndecay_speed = 2000\nfor i in range(2001):\n    batch = mnist.train.next_batch(100)\n\n    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-1/decay_speed)\n    if i % 5 == 0:\n        s = sess.run(merged_summary,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})\n        writer.add_summary(s,i)\n\n    if i  % 100 == 0:\n        [train_accuracy] = sess.run([accuracy], feed_dict={x: mnist.test.images, y: mnist.test.labels,lr:learning_rate,pkeep:1.0,pkeep_conv:1.0})\n        print(\"step %d, test accuracy is %g\" %(i,train_accuracy))\n    sess.run(train_step,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})\n</code></pre>\n<p>This file runs well on my classmate's machine.........</p>", "body_text": "I use my classmate's file for tensorboard test. But, this .py file generate .mm file not a .user file.\nCodes are as follows:\nimport tensorflow as tf\nimport math\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\ndef conv_layer(input,channels_in,channels_out,strides, name=\"conv\",):\nwith tf.name_scope(name):\nw = tf.Variable(tf.truncated_normal([5,5,channels_in,channels_out],stddev=0.1),name=\"W\")\nb = tf.Variable(tf.ones([channels_out])/10,name=\"B\")\nconv = tf.nn.conv2d(input=input,filter=w,strides=[1,strides,strides,1],padding=\"SAME\")\n# print conv\nact = tf.nn.relu(conv + b)\ntf.summary.histogram(\"weight\",w)\ntf.summary.histogram(\"bias\",b)\ntf.summary.histogram(\"activations\",act)\n# print act\nreturn act\ndef fc_layer(input,channels_in,channels_out, name=\"fc\"):\nwith tf.name_scope(name):\nw = tf.Variable(tf.truncated_normal([channels_in,channels_out],stddev=0.1),name=\"W\")\nb = tf.Variable(tf.ones([channels_out])/10,name=\"B\")\nprint input\nact = tf.nn.relu(tf.matmul(input, w) + b)\n# act = tf.matmul(input, w) + b\nreturn act\ndef compatible_convolutional_noise_shape(Y):\nnoiseshape = tf.shape(Y)\nnoiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\nreturn noiseshape\nx = tf.placeholder(dtype=tf.float32,shape=[None,784],name=\"x\")\ny = tf.placeholder(dtype=tf.float32,shape=[None,10],name=\"lables\")\nlearning rate\nlr = tf.placeholder(tf.float32)\ndropout\npkeep = tf.placeholder(tf.float32)\npkeep_conv = tf.placeholder(tf.float32)\nbatch_normal\ntst = tf.placeholder(tf.bool)\niter = tf.placeholder(tf.float32)\nx_image = tf.reshape(x,shape=[-1,28,28,1])\ntf.summary.image(\"input\",x_image,6)\ncreat net\nconv1 = conv_layer(x_image,1,4,1,\"conv_1\")\nconv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))\npool1 = tf.nn.max_pool(value=conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\nconv2 = conv_layer(conv1,4,8,2,\"conv_2\")\nconv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))\npool2 = tf.nn.max_pool(value=conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\nconv3 = conv_layer(conv2,8,16,2,\"conv_3\")\nflattened = tf.reshape(conv3,[-1, 7 * 7 * 16])\nfc1 = fc_layer(flattened,7 * 7 * 16, 200,\"fc1\")\nfc1_dr = tf.nn.dropout(fc1,pkeep)\nw5 = tf.Variable(tf.truncated_normal([200,10],stddev=0.1))\nb5 = tf.Variable(tf.ones([10])/10)\nlogits = tf.matmul(fc1,w5)+b5\nwith tf.name_scope(\"loss\"):\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits))\ntf.summary.scalar(\"loss\",cross_entropy)\nwith tf.name_scope(\"BP\"):\ntrain_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\nwith tf.name_scope(\"accuracy\"):\ncorrect_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\ntf.summary.scalar(\"accuracy\",accuracy)\nwriter = tf.summary.FileWriter(\"~/workspace/TensorFlow_Test/1\")\nwith tf.Session() as sess:\nsess.run(tf.global_variables_initializer())\nmerged_summary = tf.summary.merge_all()\nwriter = tf.summary.FileWriter(\"/tmp/mnist_demo/\")\nwriter.add_graph(sess.graph)\n# learning rate decay\nmax_learning_rate = 0.02\nmin_learning_rate = 0.001\ndecay_speed = 2000\nfor i in range(2001):\n    batch = mnist.train.next_batch(100)\n\n    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-1/decay_speed)\n    if i % 5 == 0:\n        s = sess.run(merged_summary,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})\n        writer.add_summary(s,i)\n\n    if i  % 100 == 0:\n        [train_accuracy] = sess.run([accuracy], feed_dict={x: mnist.test.images, y: mnist.test.labels,lr:learning_rate,pkeep:1.0,pkeep_conv:1.0})\n        print(\"step %d, test accuracy is %g\" %(i,train_accuracy))\n    sess.run(train_step,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})\n\nThis file runs well on my classmate's machine.........", "body": "I use my classmate's file for tensorboard test. But, this .py file generate .mm file not a .user file.\r\n\r\nCodes are as follows:\r\nimport tensorflow as tf\r\nimport math\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\ndef conv_layer(input,channels_in,channels_out,strides, name=\"conv\",):\r\n    with tf.name_scope(name):\r\n        w = tf.Variable(tf.truncated_normal([5,5,channels_in,channels_out],stddev=0.1),name=\"W\")\r\n        b = tf.Variable(tf.ones([channels_out])/10,name=\"B\")\r\n        conv = tf.nn.conv2d(input=input,filter=w,strides=[1,strides,strides,1],padding=\"SAME\")\r\n        # print conv\r\n        act = tf.nn.relu(conv + b)\r\n        tf.summary.histogram(\"weight\",w)\r\n        tf.summary.histogram(\"bias\",b)\r\n        tf.summary.histogram(\"activations\",act)\r\n        # print act\r\n        return act\r\n\r\ndef fc_layer(input,channels_in,channels_out, name=\"fc\"):\r\n    with tf.name_scope(name):\r\n        w = tf.Variable(tf.truncated_normal([channels_in,channels_out],stddev=0.1),name=\"W\")\r\n        b = tf.Variable(tf.ones([channels_out])/10,name=\"B\")\r\n        print input\r\n        act = tf.nn.relu(tf.matmul(input, w) + b)\r\n        # act = tf.matmul(input, w) + b\r\n        return act\r\n\r\ndef compatible_convolutional_noise_shape(Y):\r\n    noiseshape = tf.shape(Y)\r\n    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\r\n    return noiseshape\r\n\r\nx = tf.placeholder(dtype=tf.float32,shape=[None,784],name=\"x\")\r\ny = tf.placeholder(dtype=tf.float32,shape=[None,10],name=\"lables\")\r\n\r\n# learning rate\r\nlr = tf.placeholder(tf.float32)\r\n# dropout\r\npkeep = tf.placeholder(tf.float32)\r\npkeep_conv = tf.placeholder(tf.float32)\r\n# batch_normal\r\ntst = tf.placeholder(tf.bool)\r\niter = tf.placeholder(tf.float32)\r\n\r\n\r\nx_image = tf.reshape(x,shape=[-1,28,28,1])\r\ntf.summary.image(\"input\",x_image,6)\r\n\r\n# creat net\r\nconv1 = conv_layer(x_image,1,4,1,\"conv_1\")\r\n# conv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))\r\n# pool1 = tf.nn.max_pool(value=conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\r\n\r\nconv2 = conv_layer(conv1,4,8,2,\"conv_2\")\r\n# conv1_dr = tf.nn.dropout(conv1,pkeep_conv,compatible_convolutional_noise_shape(conv1))\r\n# pool2 = tf.nn.max_pool(value=conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\r\nconv3 = conv_layer(conv2,8,16,2,\"conv_3\")\r\n\r\nflattened = tf.reshape(conv3,[-1, 7 * 7 * 16])\r\nfc1 = fc_layer(flattened,7 * 7 * 16, 200,\"fc1\")\r\n# fc1_dr = tf.nn.dropout(fc1,pkeep)\r\n\r\nw5 = tf.Variable(tf.truncated_normal([200,10],stddev=0.1))\r\nb5 = tf.Variable(tf.ones([10])/10)\r\nlogits = tf.matmul(fc1,w5)+b5\r\n\r\nwith tf.name_scope(\"loss\"):\r\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits))\r\n    tf.summary.scalar(\"loss\",cross_entropy)\r\nwith tf.name_scope(\"BP\"):\r\n    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\r\n\r\nwith tf.name_scope(\"accuracy\"):\r\n    correct_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\r\n    tf.summary.scalar(\"accuracy\",accuracy)\r\n\r\n# writer = tf.summary.FileWriter(\"~/workspace/TensorFlow_Test/1\")\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    merged_summary = tf.summary.merge_all()\r\n    writer = tf.summary.FileWriter(\"/tmp/mnist_demo/\")\r\n    writer.add_graph(sess.graph)\r\n\r\n    # learning rate decay\r\n    max_learning_rate = 0.02\r\n    min_learning_rate = 0.001\r\n    decay_speed = 2000\r\n    for i in range(2001):\r\n        batch = mnist.train.next_batch(100)\r\n\r\n        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-1/decay_speed)\r\n        if i % 5 == 0:\r\n            s = sess.run(merged_summary,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})\r\n            writer.add_summary(s,i)\r\n\r\n        if i  % 100 == 0:\r\n            [train_accuracy] = sess.run([accuracy], feed_dict={x: mnist.test.images, y: mnist.test.labels,lr:learning_rate,pkeep:1.0,pkeep_conv:1.0})\r\n            print(\"step %d, test accuracy is %g\" %(i,train_accuracy))\r\n        sess.run(train_step,feed_dict={x:batch[0],y:batch[1],lr:learning_rate,pkeep:0.9,pkeep_conv:0.9})\r\n\r\nThis file runs well on my classmate's machine........."}