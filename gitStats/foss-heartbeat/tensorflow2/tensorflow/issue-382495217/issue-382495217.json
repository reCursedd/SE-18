{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23868", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23868/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23868/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23868/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23868", "id": 382495217, "node_id": "MDU6SXNzdWUzODI0OTUyMTc=", "number": 23868, "title": "Exporting GraphDef from File and using the resulting TFLite model in the TFLite Android App doesn't work.", "user": {"login": "Zeit42", "id": 1715673, "node_id": "MDQ6VXNlcjE3MTU2NzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1715673?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zeit42", "html_url": "https://github.com/Zeit42", "followers_url": "https://api.github.com/users/Zeit42/followers", "following_url": "https://api.github.com/users/Zeit42/following{/other_user}", "gists_url": "https://api.github.com/users/Zeit42/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zeit42/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zeit42/subscriptions", "organizations_url": "https://api.github.com/users/Zeit42/orgs", "repos_url": "https://api.github.com/users/Zeit42/repos", "events_url": "https://api.github.com/users/Zeit42/events{/privacy}", "received_events_url": "https://api.github.com/users/Zeit42/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-20T04:24:31Z", "updated_at": "2018-11-20T05:42:53Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a bug. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra 10.13.4</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Lenovo Tab 10</li>\n<li>TensorFlow installed from (source or binary): source I think</li>\n<li>TensorFlow version (use command below): 1.11.0</li>\n<li>Python version: 3.6.5</li>\n<li>Bazel version (if compiling from source): 0.16.1</li>\n<li>GCC/Compiler version (if compiling from source): 4.2.1</li>\n<li>CUDA/cuDNN version: I don't have CUDA</li>\n<li>GPU model and memory: I'm not running on a GPU</li>\n</ul>\n<p>You can collect some of this information using our environment capture <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">script</a><br>\nYou can also obtain the TensorFlow version with<br>\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<p>I have downloaded the tensorflow Release 1.12.0.<br>\nUsing Android Studio, I imported the Tensorflow-Lite sample for Android located in tensorflow/tensorflow/lite/examples/android folder. I edited the Manifest so that only the DetectActivity would be installed and would run. I deployed it in my Lenovo Tab 10 Tablet and it ran well.</p>\n<p>I wanted to use my own model, and while checking <a href=\"https://www.tensorflow.org/lite/convert/python_api#exporting_a_graphdef_from_file_\" rel=\"nofollow\">this TensorFlow Guide</a>, I downloaded the Mobilenet_1.0_224 to see how the conversion would go. As I was using TensorFlow 1.11, my Python Script looked like:</p>\n<pre><code>import tensorflow as tf\n\ngraph_def_file = \"mobilenet_v1_1.0_224/frozen_graph.pb\"\ninput_arrays = [\"input\"]\noutput_arrays = [\"MobilenetV1/Predictions/Softmax\"]\n\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(\n  graph_def_file, \n  input_arrays, \n  output_arrays)\ntflite_model = converter.convert()\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\n</code></pre>\n<p>I got the converted_model.tflite and the labels.txt and copied it to the assets folder. I ran the app again, expecting it to run like before, but I hit an error:</p>\n<pre><code>java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 270000 bytes.\n</code></pre>\n<p>The line at fault is this line from TFLiteObejctDetectionAPIModel:<br>\ntfLite.runForMultipleInputsOutputs(inputArray, outputMap);</p>\n<p>I had expected the app to run again and use my new model since I'm feeding the converted model from what TensorFlow showed in their example. But it seems that this is not the case.</p>\n<p>I also changed the variable TF_OD_API_IS_QUANTIZED to false.</p>\n<p><strong>Code to reproduce the issue</strong><br>\nTo reproduce the issue I had encountered, download the TensorFlow-Lite example. Check the manifest and comment out the other activities and just leave the DetectorActivity. Run it and it should run as expected. Now, use the guide linked above to convert a GraphDef to a TFLite example, and then copy the resulting tflite model to the assets folder of the Android Sample code. Update the value of TF_OD_API_IS_QUANTIZED to false, and then run it.</p>\n<p><strong>Other info / logs</strong><br>\njava.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 270000 bytes.<br>\nat org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:175)<br>\nat org.tensorflow.lite.Tensor.setTo(Tensor.java:65)<br>\nat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:126)<br>\nat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:168)<br>\nat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:216)<br>\nat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:249)<br>\nat android.os.Handler.handleCallback(Handler.java:751)<br>\nat android.os.Handler.dispatchMessage(Handler.java:95)<br>\nat android.os.Looper.loop(Looper.java:154)<br>\nat android.os.HandlerThread.run(HandlerThread.java:61)</p>", "body_text": "Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra 10.13.4\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Lenovo Tab 10\nTensorFlow installed from (source or binary): source I think\nTensorFlow version (use command below): 1.11.0\nPython version: 3.6.5\nBazel version (if compiling from source): 0.16.1\nGCC/Compiler version (if compiling from source): 4.2.1\nCUDA/cuDNN version: I don't have CUDA\nGPU model and memory: I'm not running on a GPU\n\nYou can collect some of this information using our environment capture script\nYou can also obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nI have downloaded the tensorflow Release 1.12.0.\nUsing Android Studio, I imported the Tensorflow-Lite sample for Android located in tensorflow/tensorflow/lite/examples/android folder. I edited the Manifest so that only the DetectActivity would be installed and would run. I deployed it in my Lenovo Tab 10 Tablet and it ran well.\nI wanted to use my own model, and while checking this TensorFlow Guide, I downloaded the Mobilenet_1.0_224 to see how the conversion would go. As I was using TensorFlow 1.11, my Python Script looked like:\nimport tensorflow as tf\n\ngraph_def_file = \"mobilenet_v1_1.0_224/frozen_graph.pb\"\ninput_arrays = [\"input\"]\noutput_arrays = [\"MobilenetV1/Predictions/Softmax\"]\n\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(\n  graph_def_file, \n  input_arrays, \n  output_arrays)\ntflite_model = converter.convert()\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\n\nI got the converted_model.tflite and the labels.txt and copied it to the assets folder. I ran the app again, expecting it to run like before, but I hit an error:\njava.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 270000 bytes.\n\nThe line at fault is this line from TFLiteObejctDetectionAPIModel:\ntfLite.runForMultipleInputsOutputs(inputArray, outputMap);\nI had expected the app to run again and use my new model since I'm feeding the converted model from what TensorFlow showed in their example. But it seems that this is not the case.\nI also changed the variable TF_OD_API_IS_QUANTIZED to false.\nCode to reproduce the issue\nTo reproduce the issue I had encountered, download the TensorFlow-Lite example. Check the manifest and comment out the other activities and just leave the DetectorActivity. Run it and it should run as expected. Now, use the guide linked above to convert a GraphDef to a TFLite example, and then copy the resulting tflite model to the assets folder of the Android Sample code. Update the value of TF_OD_API_IS_QUANTIZED to false, and then run it.\nOther info / logs\njava.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 270000 bytes.\nat org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:175)\nat org.tensorflow.lite.Tensor.setTo(Tensor.java:65)\nat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:126)\nat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:168)\nat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:216)\nat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:249)\nat android.os.Handler.handleCallback(Handler.java:751)\nat android.os.Handler.dispatchMessage(Handler.java:95)\nat android.os.Looper.loop(Looper.java:154)\nat android.os.HandlerThread.run(HandlerThread.java:61)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra 10.13.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Lenovo Tab 10\r\n- TensorFlow installed from (source or binary): source I think\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): 4.2.1\r\n- CUDA/cuDNN version: I don't have CUDA\r\n- GPU model and memory: I'm not running on a GPU\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nI have downloaded the tensorflow Release 1.12.0.\r\nUsing Android Studio, I imported the Tensorflow-Lite sample for Android located in tensorflow/tensorflow/lite/examples/android folder. I edited the Manifest so that only the DetectActivity would be installed and would run. I deployed it in my Lenovo Tab 10 Tablet and it ran well.\r\n\r\nI wanted to use my own model, and while checking [this TensorFlow Guide](https://www.tensorflow.org/lite/convert/python_api#exporting_a_graphdef_from_file_), I downloaded the Mobilenet_1.0_224 to see how the conversion would go. As I was using TensorFlow 1.11, my Python Script looked like:\r\n\r\n\timport tensorflow as tf\r\n\r\n\tgraph_def_file = \"mobilenet_v1_1.0_224/frozen_graph.pb\"\r\n\tinput_arrays = [\"input\"]\r\n\toutput_arrays = [\"MobilenetV1/Predictions/Softmax\"]\r\n\r\n\tconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(\r\n\t  graph_def_file, \r\n\t  input_arrays, \r\n\t  output_arrays)\r\n\ttflite_model = converter.convert()\r\n\topen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\nI got the converted_model.tflite and the labels.txt and copied it to the assets folder. I ran the app again, expecting it to run like before, but I hit an error:\r\n\r\n    java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 270000 bytes.\r\n\r\nThe line at fault is this line from TFLiteObejctDetectionAPIModel:\r\n    tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n\r\nI had expected the app to run again and use my new model since I'm feeding the converted model from what TensorFlow showed in their example. But it seems that this is not the case.\r\n\r\nI also changed the variable TF_OD_API_IS_QUANTIZED to false.\r\n\r\n**Code to reproduce the issue**\r\nTo reproduce the issue I had encountered, download the TensorFlow-Lite example. Check the manifest and comment out the other activities and just leave the DetectorActivity. Run it and it should run as expected. Now, use the guide linked above to convert a GraphDef to a TFLite example, and then copy the resulting tflite model to the assets folder of the Android Sample code. Update the value of TF_OD_API_IS_QUANTIZED to false, and then run it. \r\n\r\n\r\n**Other info / logs**\r\n    java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 270000 bytes.\r\n        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:175)\r\n        at org.tensorflow.lite.Tensor.setTo(Tensor.java:65)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:126)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:168)\r\n        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:216)\r\n        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:249)\r\n        at android.os.Handler.handleCallback(Handler.java:751)\r\n        at android.os.Handler.dispatchMessage(Handler.java:95)\r\n        at android.os.Looper.loop(Looper.java:154)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n"}