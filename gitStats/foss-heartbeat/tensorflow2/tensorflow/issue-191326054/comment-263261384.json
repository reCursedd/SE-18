{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263261384", "html_url": "https://github.com/tensorflow/tensorflow/pull/5813#issuecomment-263261384", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5813", "id": 263261384, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzI2MTM4NA==", "user": {"login": "Syndrome777", "id": 6788909, "node_id": "MDQ6VXNlcjY3ODg5MDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/6788909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Syndrome777", "html_url": "https://github.com/Syndrome777", "followers_url": "https://api.github.com/users/Syndrome777/followers", "following_url": "https://api.github.com/users/Syndrome777/following{/other_user}", "gists_url": "https://api.github.com/users/Syndrome777/gists{/gist_id}", "starred_url": "https://api.github.com/users/Syndrome777/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Syndrome777/subscriptions", "organizations_url": "https://api.github.com/users/Syndrome777/orgs", "repos_url": "https://api.github.com/users/Syndrome777/repos", "events_url": "https://api.github.com/users/Syndrome777/events{/privacy}", "received_events_url": "https://api.github.com/users/Syndrome777/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-28T12:40:04Z", "updated_at": "2016-11-30T05:27:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>BTW,<br>\nI use the code below, get different prints between the master's implementation and ours.<br>\nWe use <code>variable_scope.variable_scope(None, primary.op.name + '/' + name)</code> and master's implementation uses <code>ops.name_scope(primary.op.name + \"/\" + name) as scope</code> as scope name.</p>\n<pre><code>Master's print:\n['global_step', 'linear//weight', 'linear//weight/Momentum', 'linear/bias_weight', 'linear/bias_weight/Momentum']\nOur print:\n['global_step', 'linear//weight', 'linear//weight/', 'linear/bias_weight', 'linear/bias_weight/']\n</code></pre>\n<p>Meanwhile, I find the scope name in slot_creator.py of master' implementation is <code>linear//weight/part_0/Momentum</code>, but in <code>LinearRegressor.get_variable_names()</code>, it is <code>linear//weight/Momentum</code>, <code>part_0</code> is removed somewhere.<br>\nThe interesting thing is the scope name in our implementation is <code>linear//weight/part_0/Momentum</code>, but in <code>LinearRegressor.get_variable_names()</code>, it is <code>linear//weight/</code></p>\n<pre><code>#scope name in slot_creator.py\nlinear//weight/part_0/Momentum\nlinear/bias_weight/part_0/Momentum\n\n#variable name in LinearRegressor.get_variable_names()\nlinear//weight/Momentum\nlinear//bias_weight/Momentum\n</code></pre>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">boston_input_fn</span>():\n  boston <span class=\"pl-k\">=</span> tf.contrib.learn.datasets.load_boston()\n  features <span class=\"pl-k\">=</span> tf.cast(tf.reshape(tf.constant(boston.data), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">13</span>]), tf.float32)\n  labels <span class=\"pl-k\">=</span> tf.cast(tf.reshape(tf.constant(boston.target), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>]), tf.float32)\n  <span class=\"pl-k\">return</span> features, labels\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">FeatureColumnTest</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">test</span>.<span class=\"pl-e\">TestCase</span>):\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">testTrain</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    feature_columns <span class=\"pl-k\">=</span> tf.contrib.learn.infer_real_valued_columns_from_input_fn(\n        boston_input_fn)\n    optimizer <span class=\"pl-k\">=</span> tf.train.MomentumOptimizer(<span class=\"pl-c1\">1e-3</span>, <span class=\"pl-c1\">0.9</span>)\n    est <span class=\"pl-k\">=</span> tf.contrib.learn.LinearRegressor(<span class=\"pl-v\">feature_columns</span><span class=\"pl-k\">=</span>feature_columns, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>optimizer)\n    est.fit(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>boston_input_fn, <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>------------------------------------<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">print</span> (est.get_variable_names())\n    _ <span class=\"pl-k\">=</span> est.evaluate(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>boston_input_fn, <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n  tf.test.main()</pre></div>", "body_text": "BTW,\nI use the code below, get different prints between the master's implementation and ours.\nWe use variable_scope.variable_scope(None, primary.op.name + '/' + name) and master's implementation uses ops.name_scope(primary.op.name + \"/\" + name) as scope as scope name.\nMaster's print:\n['global_step', 'linear//weight', 'linear//weight/Momentum', 'linear/bias_weight', 'linear/bias_weight/Momentum']\nOur print:\n['global_step', 'linear//weight', 'linear//weight/', 'linear/bias_weight', 'linear/bias_weight/']\n\nMeanwhile, I find the scope name in slot_creator.py of master' implementation is linear//weight/part_0/Momentum, but in LinearRegressor.get_variable_names(), it is linear//weight/Momentum, part_0 is removed somewhere.\nThe interesting thing is the scope name in our implementation is linear//weight/part_0/Momentum, but in LinearRegressor.get_variable_names(), it is linear//weight/\n#scope name in slot_creator.py\nlinear//weight/part_0/Momentum\nlinear/bias_weight/part_0/Momentum\n\n#variable name in LinearRegressor.get_variable_names()\nlinear//weight/Momentum\nlinear//bias_weight/Momentum\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef boston_input_fn():\n  boston = tf.contrib.learn.datasets.load_boston()\n  features = tf.cast(tf.reshape(tf.constant(boston.data), [-1, 13]), tf.float32)\n  labels = tf.cast(tf.reshape(tf.constant(boston.target), [-1, 1]), tf.float32)\n  return features, labels\n\n\nclass FeatureColumnTest(tf.test.TestCase):\n\n  def testTrain(self):\n    feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input_fn(\n        boston_input_fn)\n    optimizer = tf.train.MomentumOptimizer(1e-3, 0.9)\n    est = tf.contrib.learn.LinearRegressor(feature_columns=feature_columns, optimizer=optimizer)\n    est.fit(input_fn=boston_input_fn, steps=1)\n    print ('------------------------------------')\n    print (est.get_variable_names())\n    _ = est.evaluate(input_fn=boston_input_fn, steps=1)\n\n\nif __name__ == '__main__':\n  tf.test.main()", "body": "BTW, \r\nI use the code below, get different prints between the master's implementation and ours.\r\nWe use `variable_scope.variable_scope(None, primary.op.name + '/' + name)` and master's implementation uses `ops.name_scope(primary.op.name + \"/\" + name) as scope` as scope name.\r\n```\r\nMaster's print:\r\n['global_step', 'linear//weight', 'linear//weight/Momentum', 'linear/bias_weight', 'linear/bias_weight/Momentum']\r\nOur print:\r\n['global_step', 'linear//weight', 'linear//weight/', 'linear/bias_weight', 'linear/bias_weight/']\r\n```\r\nMeanwhile, I find the scope name in slot_creator.py of master' implementation is `linear//weight/part_0/Momentum`, but in `LinearRegressor.get_variable_names()`, it is `linear//weight/Momentum`, `part_0` is removed somewhere. \r\nThe interesting thing is the scope name in our implementation is `linear//weight/part_0/Momentum`, but in `LinearRegressor.get_variable_names()`, it is `linear//weight/`\r\n```\r\n#scope name in slot_creator.py\r\nlinear//weight/part_0/Momentum\r\nlinear/bias_weight/part_0/Momentum\r\n\r\n#variable name in LinearRegressor.get_variable_names()\r\nlinear//weight/Momentum\r\nlinear//bias_weight/Momentum\r\n```\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef boston_input_fn():\r\n  boston = tf.contrib.learn.datasets.load_boston()\r\n  features = tf.cast(tf.reshape(tf.constant(boston.data), [-1, 13]), tf.float32)\r\n  labels = tf.cast(tf.reshape(tf.constant(boston.target), [-1, 1]), tf.float32)\r\n  return features, labels\r\n\r\n\r\nclass FeatureColumnTest(tf.test.TestCase):\r\n\r\n  def testTrain(self):\r\n    feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input_fn(\r\n        boston_input_fn)\r\n    optimizer = tf.train.MomentumOptimizer(1e-3, 0.9)\r\n    est = tf.contrib.learn.LinearRegressor(feature_columns=feature_columns, optimizer=optimizer)\r\n    est.fit(input_fn=boston_input_fn, steps=1)\r\n    print ('------------------------------------')\r\n    print (est.get_variable_names())\r\n    _ = est.evaluate(input_fn=boston_input_fn, steps=1)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.test.main()\r\n```"}