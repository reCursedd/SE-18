{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/375075229", "html_url": "https://github.com/tensorflow/tensorflow/issues/17886#issuecomment-375075229", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17886", "id": 375075229, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTA3NTIyOQ==", "user": {"login": "haasdo95", "id": 20941623, "node_id": "MDQ6VXNlcjIwOTQxNjIz", "avatar_url": "https://avatars2.githubusercontent.com/u/20941623?v=4", "gravatar_id": "", "url": "https://api.github.com/users/haasdo95", "html_url": "https://github.com/haasdo95", "followers_url": "https://api.github.com/users/haasdo95/followers", "following_url": "https://api.github.com/users/haasdo95/following{/other_user}", "gists_url": "https://api.github.com/users/haasdo95/gists{/gist_id}", "starred_url": "https://api.github.com/users/haasdo95/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/haasdo95/subscriptions", "organizations_url": "https://api.github.com/users/haasdo95/orgs", "repos_url": "https://api.github.com/users/haasdo95/repos", "events_url": "https://api.github.com/users/haasdo95/events{/privacy}", "received_events_url": "https://api.github.com/users/haasdo95/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-21T19:56:16Z", "updated_at": "2018-03-22T03:16:19Z", "author_association": "NONE", "body_html": "<p>The research project is ongoing so it's kinda tough to come up with a new reproducible case.<br>\nBut the code looks kinda like the following:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> lookup index</span>\ndefault <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">sum</span>, <span class=\"pl-c1\">HIDDEN_SIZE</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>)) \\\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">CELL_TYPE</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>LSTM<span class=\"pl-pds\">\"</span></span> \\\n    <span class=\"pl-k\">else</span> np.zeros((<span class=\"pl-c1\">sum</span>, <span class=\"pl-c1\">HIDDEN_SIZE</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>))\nall_hiddens <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> gpu_idx <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">NUM_GPUS</span>):\n    actual_index_of_gpu <span class=\"pl-k\">=</span> actual_index[gpu_idx]\n    all_hiddens_of_gpu <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> b <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">BATCH_SIZE</span>):\n        idx_long <span class=\"pl-k\">=</span> actual_index_of_gpu[b][<span class=\"pl-c1\">0</span>]\n        idx_short <span class=\"pl-k\">=</span> actual_index_of_gpu[b][<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> minus one is important!...</span>\n        hidden_of_index <span class=\"pl-k\">=</span> hidden_dict.get((idx_long, idx_short), default)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> THIS IS THE ACTUAL LOOKUP</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> hidden_of_index: (2, sum, hidden_size) if lstm</span>\n        all_hiddens_of_gpu.append(hidden_of_index)\n    all_hiddens_of_gpu <span class=\"pl-k\">=</span> np.stack(all_hiddens_of_gpu, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (2, sum, batch_size, hidden_size)</span>\n    all_hiddens.append(all_hiddens_of_gpu)\nall_hiddens <span class=\"pl-k\">=</span> np.array(all_hiddens)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (NUM_GPUS, 2, sum, batch_size, hidden_size)</span>\n\nfeed_dict <span class=\"pl-k\">=</span> {\n    hidden_entries: all_hiddens,\n    data_entries: actual_data,\n    index_entries: actual_index <span class=\"pl-c\"><span class=\"pl-c\">#</span> IDEA: feed data_index in as well</span>\n}  <span class=\"pl-c\"><span class=\"pl-c\">#</span> BIG BATCH fed here</span>\n_, loss_value, run_states, run_indices <span class=\"pl-k\">=</span> sess.run([train_op, loss, tower_states, tower_indices],\n                                                  <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed_dict)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> run_states: ([2], sum, batch_size, dim)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> write hidden state back to dict</span>\n<span class=\"pl-k\">for</span> tower_state, tower_index <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(run_states, run_indices):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> a NUM_GPUS loop</span>\n    tower_state <span class=\"pl-k\">=</span> np.copy(tower_state) <span class=\"pl-c\"><span class=\"pl-c\">#</span> A deep copy that may or may not fix the problem???</span>\n    <span class=\"pl-k\">for</span> batch_idx <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">BATCH_SIZE</span>):\n        idx_long <span class=\"pl-k\">=</span> tower_index[batch_idx][<span class=\"pl-c1\">0</span>]\n        idx_short <span class=\"pl-k\">=</span> tower_index[batch_idx][<span class=\"pl-c1\">1</span>]\n        curr_hidden <span class=\"pl-k\">=</span> tower_state[\n                      :, <span class=\"pl-c\"><span class=\"pl-c\">#</span> LSTM-specific</span>\n                      :,\n                      batch_idx, <span class=\"pl-c\"><span class=\"pl-c\">#</span> current token</span>\n                      :] \\\n                    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">CELL_TYPE</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>LSTM<span class=\"pl-pds\">\"</span></span> \\\n                    <span class=\"pl-k\">else</span> tower_state[\n                         :,\n                         batch_idx,\n                         :]\n        hidden_dict[\n            (idx_long, idx_short)\n        ] <span class=\"pl-k\">=</span> curr_hidden  <span class=\"pl-c\"><span class=\"pl-c\">#</span> curr_hidden: (2, sum, input_dim) if LSTM</span></pre></div>\n<p>Basically we look up hidden state from the hash table, sess.run, and write the retrieved hidden state back into the hash table.<br>\nAfter commenting out the write-back part of the code, the memory issue disappears.</p>", "body_text": "The research project is ongoing so it's kinda tough to come up with a new reproducible case.\nBut the code looks kinda like the following:\n# lookup index\ndefault = np.zeros((2, sum, HIDDEN_SIZE * 2)) \\\n    if CELL_TYPE == \"LSTM\" \\\n    else np.zeros((sum, HIDDEN_SIZE * 2))\nall_hiddens = []\nfor gpu_idx in range(NUM_GPUS):\n    actual_index_of_gpu = actual_index[gpu_idx]\n    all_hiddens_of_gpu = []\n    for b in xrange(BATCH_SIZE):\n        idx_long = actual_index_of_gpu[b][0]\n        idx_short = actual_index_of_gpu[b][1] - 1  # minus one is important!...\n        hidden_of_index = hidden_dict.get((idx_long, idx_short), default)  # THIS IS THE ACTUAL LOOKUP\n        # hidden_of_index: (2, sum, hidden_size) if lstm\n        all_hiddens_of_gpu.append(hidden_of_index)\n    all_hiddens_of_gpu = np.stack(all_hiddens_of_gpu, axis=-2)  # (2, sum, batch_size, hidden_size)\n    all_hiddens.append(all_hiddens_of_gpu)\nall_hiddens = np.array(all_hiddens)  # (NUM_GPUS, 2, sum, batch_size, hidden_size)\n\nfeed_dict = {\n    hidden_entries: all_hiddens,\n    data_entries: actual_data,\n    index_entries: actual_index # IDEA: feed data_index in as well\n}  # BIG BATCH fed here\n_, loss_value, run_states, run_indices = sess.run([train_op, loss, tower_states, tower_indices],\n                                                  feed_dict=feed_dict)\n# run_states: ([2], sum, batch_size, dim)\n# write hidden state back to dict\nfor tower_state, tower_index in zip(run_states, run_indices):  # a NUM_GPUS loop\n    tower_state = np.copy(tower_state) # A deep copy that may or may not fix the problem???\n    for batch_idx in range(BATCH_SIZE):\n        idx_long = tower_index[batch_idx][0]\n        idx_short = tower_index[batch_idx][1]\n        curr_hidden = tower_state[\n                      :, # LSTM-specific\n                      :,\n                      batch_idx, # current token\n                      :] \\\n                    if CELL_TYPE == \"LSTM\" \\\n                    else tower_state[\n                         :,\n                         batch_idx,\n                         :]\n        hidden_dict[\n            (idx_long, idx_short)\n        ] = curr_hidden  # curr_hidden: (2, sum, input_dim) if LSTM\nBasically we look up hidden state from the hash table, sess.run, and write the retrieved hidden state back into the hash table.\nAfter commenting out the write-back part of the code, the memory issue disappears.", "body": "The research project is ongoing so it's kinda tough to come up with a new reproducible case.\r\nBut the code looks kinda like the following:\r\n```python\r\n# lookup index\r\ndefault = np.zeros((2, sum, HIDDEN_SIZE * 2)) \\\r\n    if CELL_TYPE == \"LSTM\" \\\r\n    else np.zeros((sum, HIDDEN_SIZE * 2))\r\nall_hiddens = []\r\nfor gpu_idx in range(NUM_GPUS):\r\n    actual_index_of_gpu = actual_index[gpu_idx]\r\n    all_hiddens_of_gpu = []\r\n    for b in xrange(BATCH_SIZE):\r\n        idx_long = actual_index_of_gpu[b][0]\r\n        idx_short = actual_index_of_gpu[b][1] - 1  # minus one is important!...\r\n        hidden_of_index = hidden_dict.get((idx_long, idx_short), default)  # THIS IS THE ACTUAL LOOKUP\r\n        # hidden_of_index: (2, sum, hidden_size) if lstm\r\n        all_hiddens_of_gpu.append(hidden_of_index)\r\n    all_hiddens_of_gpu = np.stack(all_hiddens_of_gpu, axis=-2)  # (2, sum, batch_size, hidden_size)\r\n    all_hiddens.append(all_hiddens_of_gpu)\r\nall_hiddens = np.array(all_hiddens)  # (NUM_GPUS, 2, sum, batch_size, hidden_size)\r\n\r\nfeed_dict = {\r\n    hidden_entries: all_hiddens,\r\n    data_entries: actual_data,\r\n    index_entries: actual_index # IDEA: feed data_index in as well\r\n}  # BIG BATCH fed here\r\n_, loss_value, run_states, run_indices = sess.run([train_op, loss, tower_states, tower_indices],\r\n                                                  feed_dict=feed_dict)\r\n# run_states: ([2], sum, batch_size, dim)\r\n# write hidden state back to dict\r\nfor tower_state, tower_index in zip(run_states, run_indices):  # a NUM_GPUS loop\r\n    tower_state = np.copy(tower_state) # A deep copy that may or may not fix the problem???\r\n    for batch_idx in range(BATCH_SIZE):\r\n        idx_long = tower_index[batch_idx][0]\r\n        idx_short = tower_index[batch_idx][1]\r\n        curr_hidden = tower_state[\r\n                      :, # LSTM-specific\r\n                      :,\r\n                      batch_idx, # current token\r\n                      :] \\\r\n                    if CELL_TYPE == \"LSTM\" \\\r\n                    else tower_state[\r\n                         :,\r\n                         batch_idx,\r\n                         :]\r\n        hidden_dict[\r\n            (idx_long, idx_short)\r\n        ] = curr_hidden  # curr_hidden: (2, sum, input_dim) if LSTM\r\n```\r\nBasically we look up hidden state from the hash table, sess.run, and write the retrieved hidden state back into the hash table. \r\nAfter commenting out the write-back part of the code, the memory issue disappears. "}