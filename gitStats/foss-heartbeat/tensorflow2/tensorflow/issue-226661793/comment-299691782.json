{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299691782", "html_url": "https://github.com/tensorflow/tensorflow/issues/9699#issuecomment-299691782", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9699", "id": 299691782, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTY5MTc4Mg==", "user": {"login": "kevinashaw", "id": 7141343, "node_id": "MDQ6VXNlcjcxNDEzNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/7141343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kevinashaw", "html_url": "https://github.com/kevinashaw", "followers_url": "https://api.github.com/users/kevinashaw/followers", "following_url": "https://api.github.com/users/kevinashaw/following{/other_user}", "gists_url": "https://api.github.com/users/kevinashaw/gists{/gist_id}", "starred_url": "https://api.github.com/users/kevinashaw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kevinashaw/subscriptions", "organizations_url": "https://api.github.com/users/kevinashaw/orgs", "repos_url": "https://api.github.com/users/kevinashaw/repos", "events_url": "https://api.github.com/users/kevinashaw/events{/privacy}", "received_events_url": "https://api.github.com/users/kevinashaw/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-07T09:03:37Z", "updated_at": "2017-05-07T09:03:37Z", "author_association": "NONE", "body_html": "<p>Okay, after about five hours on this, I've ditched the old <code>static_rnn</code> approach and switched to a <code>dynamic_rnn</code> which eliminated those issues about input shape.  If I understand correctly, with the <code>dynamic_rnn</code>, I can feed an input Tensor of shape <code>[b x n x f]</code> where <code>b</code> is the <code>batch_size</code>, <code>n</code> is <code>n_steps</code>, and <code>f</code> is the number of features, <code>n_feat</code>, directly into the RNN object.<br>\nLong story short, there is nothing like a little \"confessional debugging\" (as a friend likes to say) to solve a problem.  I wrote up a long answer here documenting everything I'd found and happened to notice that the error was showing up only on the test data, not the training data.  I'm using a smaller data set for this graph testing working and the test set is smaller than the 2500 element batch size that I'm using for the training.  Hence, for the test set, when the tensor count is smaller than the batch size, the initial state tensor will not match the state result from the RNN.<br>\nThe solution is to use a variable <code>batch_size</code> based on the size of the <code>Xin</code> value, and to then generate a variable <code>initial_state</code> dynamically based on this:</p>\n<pre><code>batch_size_T  = tf.shape(Xin)[0]\ninitial_state = lstm_cells.zero_state(batch_size_T, tf.float32) \n</code></pre>\n<p>So, problem solved.  Thanks for the help!!!</p>", "body_text": "Okay, after about five hours on this, I've ditched the old static_rnn approach and switched to a dynamic_rnn which eliminated those issues about input shape.  If I understand correctly, with the dynamic_rnn, I can feed an input Tensor of shape [b x n x f] where b is the batch_size, n is n_steps, and f is the number of features, n_feat, directly into the RNN object.\nLong story short, there is nothing like a little \"confessional debugging\" (as a friend likes to say) to solve a problem.  I wrote up a long answer here documenting everything I'd found and happened to notice that the error was showing up only on the test data, not the training data.  I'm using a smaller data set for this graph testing working and the test set is smaller than the 2500 element batch size that I'm using for the training.  Hence, for the test set, when the tensor count is smaller than the batch size, the initial state tensor will not match the state result from the RNN.\nThe solution is to use a variable batch_size based on the size of the Xin value, and to then generate a variable initial_state dynamically based on this:\nbatch_size_T  = tf.shape(Xin)[0]\ninitial_state = lstm_cells.zero_state(batch_size_T, tf.float32) \n\nSo, problem solved.  Thanks for the help!!!", "body": "Okay, after about five hours on this, I've ditched the old `static_rnn` approach and switched to a `dynamic_rnn` which eliminated those issues about input shape.  If I understand correctly, with the `dynamic_rnn`, I can feed an input Tensor of shape `[b x n x f]` where `b` is the `batch_size`, `n` is `n_steps`, and `f` is the number of features, `n_feat`, directly into the RNN object.\r\nLong story short, there is nothing like a little \"confessional debugging\" (as a friend likes to say) to solve a problem.  I wrote up a long answer here documenting everything I'd found and happened to notice that the error was showing up only on the test data, not the training data.  I'm using a smaller data set for this graph testing working and the test set is smaller than the 2500 element batch size that I'm using for the training.  Hence, for the test set, when the tensor count is smaller than the batch size, the initial state tensor will not match the state result from the RNN.  \r\nThe solution is to use a variable `batch_size` based on the size of the `Xin` value, and to then generate a variable `initial_state` dynamically based on this:\r\n```\r\nbatch_size_T  = tf.shape(Xin)[0]\r\ninitial_state = lstm_cells.zero_state(batch_size_T, tf.float32) \r\n```\r\nSo, problem solved.  Thanks for the help!!!\r\n"}