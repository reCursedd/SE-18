{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4601", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4601/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4601/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4601/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4601", "id": 179560998, "node_id": "MDU6SXNzdWUxNzk1NjA5OTg=", "number": 4601, "title": "convert Inception v1 model .pb file into 8bit precision fail", "user": {"login": "civilman628", "id": 8059551, "node_id": "MDQ6VXNlcjgwNTk1NTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/8059551?v=4", "gravatar_id": "", "url": "https://api.github.com/users/civilman628", "html_url": "https://github.com/civilman628", "followers_url": "https://api.github.com/users/civilman628/followers", "following_url": "https://api.github.com/users/civilman628/following{/other_user}", "gists_url": "https://api.github.com/users/civilman628/gists{/gist_id}", "starred_url": "https://api.github.com/users/civilman628/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/civilman628/subscriptions", "organizations_url": "https://api.github.com/users/civilman628/orgs", "repos_url": "https://api.github.com/users/civilman628/repos", "events_url": "https://api.github.com/users/civilman628/events{/privacy}", "received_events_url": "https://api.github.com/users/civilman628/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-09-27T17:33:47Z", "updated_at": "2016-09-27T22:47:27Z", "closed_at": "2016-09-27T22:45:23Z", "author_association": "NONE", "body_html": "<p>I use the guide below to download a v3 model and convert it to 8 bit precision without no issue.</p>\n<p><a href=\"https://www.tensorflow.org/versions/master/how_tos/quantization/index.html\" rel=\"nofollow\">https://www.tensorflow.org/versions/master/how_tos/quantization/index.html</a></p>\n<p>curl <a href=\"http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\" rel=\"nofollow\">http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz</a> -o /tmp/inceptionv3.tgz<br>\ntar xzf /tmp/inceptionv3.tgz -C /tmp/<br>\nbazel build tensorflow/contrib/quantization/tools:quantize_graph<br>\n<strong>bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph <br>\n--input=/tmp/classify_image_graph_def.pb <br>\n--output_node_names=\"softmax\" --output=/tmp/quantized_graph.pb <br>\n--mode=eightbit</strong></p>\n<p>However, i download a v1 model from below and try to convert to 8 bit precision fail.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/\">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/</a></p>\n<p><a href=\"https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\" rel=\"nofollow\">https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip</a></p>\n<p>the model file is: \"tensorflow_inception_graph.pb\"</p>\n<h2>I get error:</h2>\n<p>File \"/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py\", line 319, in rewrite<br>\nfor output_node_name in output_node_names]</p>\n<h2>KeyError: 'softmax'</h2>\n<p>it says key error: softmax. is this caused by v1 model use something other than softmax or it has different name? What should i input for --output_node_names parameter when I convert v1? Or how can i get the output node name? If i try to convert other model .pb file how to input this parameter?</p>\n<p>Thanks</p>", "body_text": "I use the guide below to download a v3 model and convert it to 8 bit precision without no issue.\nhttps://www.tensorflow.org/versions/master/how_tos/quantization/index.html\ncurl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz\ntar xzf /tmp/inceptionv3.tgz -C /tmp/\nbazel build tensorflow/contrib/quantization/tools:quantize_graph\nbazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \n--input=/tmp/classify_image_graph_def.pb \n--output_node_names=\"softmax\" --output=/tmp/quantized_graph.pb \n--mode=eightbit\nHowever, i download a v1 model from below and try to convert to 8 bit precision fail.\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/\nhttps://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\nthe model file is: \"tensorflow_inception_graph.pb\"\nI get error:\nFile \"/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py\", line 319, in rewrite\nfor output_node_name in output_node_names]\nKeyError: 'softmax'\nit says key error: softmax. is this caused by v1 model use something other than softmax or it has different name? What should i input for --output_node_names parameter when I convert v1? Or how can i get the output node name? If i try to convert other model .pb file how to input this parameter?\nThanks", "body": "I use the guide below to download a v3 model and convert it to 8 bit precision without no issue.\n\nhttps://www.tensorflow.org/versions/master/how_tos/quantization/index.html\n\ncurl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz\ntar xzf /tmp/inceptionv3.tgz -C /tmp/\nbazel build tensorflow/contrib/quantization/tools:quantize_graph\n**bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \\\n--input=/tmp/classify_image_graph_def.pb \\\n--output_node_names=\"softmax\" --output=/tmp/quantized_graph.pb \\\n--mode=eightbit**\n\nHowever, i download a v1 model from below and try to convert to 8 bit precision fail.\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/\n\nhttps://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\n\nthe model file is: \"tensorflow_inception_graph.pb\" \n## I get error:\n\nFile \"/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py\", line 319, in rewrite\n    for output_node_name in output_node_names]\n## KeyError: 'softmax'\n\nit says key error: softmax. is this caused by v1 model use something other than softmax or it has different name? What should i input for --output_node_names parameter when I convert v1? Or how can i get the output node name? If i try to convert other model .pb file how to input this parameter?\n\nThanks\n"}