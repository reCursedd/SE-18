{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19741", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19741/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19741/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19741/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19741", "id": 328952247, "node_id": "MDU6SXNzdWUzMjg5NTIyNDc=", "number": 19741, "title": "Issue on using custom Op with TensorRT", "user": {"login": "lmatt-bit", "id": 373383, "node_id": "MDQ6VXNlcjM3MzM4Mw==", "avatar_url": "https://avatars2.githubusercontent.com/u/373383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmatt-bit", "html_url": "https://github.com/lmatt-bit", "followers_url": "https://api.github.com/users/lmatt-bit/followers", "following_url": "https://api.github.com/users/lmatt-bit/following{/other_user}", "gists_url": "https://api.github.com/users/lmatt-bit/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmatt-bit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmatt-bit/subscriptions", "organizations_url": "https://api.github.com/users/lmatt-bit/orgs", "repos_url": "https://api.github.com/users/lmatt-bit/repos", "events_url": "https://api.github.com/users/lmatt-bit/events{/privacy}", "received_events_url": "https://api.github.com/users/lmatt-bit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-04T08:16:25Z", "updated_at": "2018-06-04T08:24:52Z", "closed_at": "2018-06-04T08:24:52Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nI am using tensor2tensor library.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nTensorflow docker image with \"1.7.1-devel-gpu\" tag</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nSource(From the docker image, nothing changed), with TensorRT enabled</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.7.1</li>\n<li><strong>Python version</strong>:<br>\n2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.11.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\n5.4.0 20160609</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nCUDA 9.0</li>\n<li><strong>GPU model and memory</strong>:<br>\nGTX 1080 Ti 11 G</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nSee problem description.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am trying to use TensorRT feature to optimize the inference performance of Tensor2Tensor.  But got Op type not registered issue.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre>graph <span class=\"pl-k\">=</span> tf.get_default_graph().as_graph_def()\nfrozen_graph <span class=\"pl-k\">=</span> tf.graph_util.remove_training_nodes(graph)\ntrt_graph <span class=\"pl-k\">=</span> trt.create_inference_graph(\n    <span class=\"pl-v\">input_graph_def</span> <span class=\"pl-k\">=</span> frozen_graph, \n    <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>[model_output],\n    <span class=\"pl-v\">max_batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)</pre></div>\n<p>below error shows when create_inference_graph is called.</p>\n<blockquote>\n<p>tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'convert_gradient_to_tensor_cc661786' in binary running on cffae59e0618. Makre sure the Op and Kernal are registered in the binary running in this process.</p>\n</blockquote>\n<p>convert_gradient_to_tensor is defined in tensor2tensor in below way.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@function.Defun</span>(\n    <span class=\"pl-v\">python_grad_func</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">dy</span>: tf.convert_to_tensor(dy),\n    <span class=\"pl-v\">shape_func</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">op</span>: [op.inputs[<span class=\"pl-c1\">0</span>].get_shape()])\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">convert_gradient_to_tensor</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-c1\">...</span></pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nI am using tensor2tensor library.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nTensorflow docker image with \"1.7.1-devel-gpu\" tag\nTensorFlow installed from (source or binary):\nSource(From the docker image, nothing changed), with TensorRT enabled\nTensorFlow version (use command below):\n1.7.1\nPython version:\n2.7\nBazel version (if compiling from source):\n0.11.0\nGCC/Compiler version (if compiling from source):\n5.4.0 20160609\nCUDA/cuDNN version:\nCUDA 9.0\nGPU model and memory:\nGTX 1080 Ti 11 G\nExact command to reproduce:\nSee problem description.\n\nDescribe the problem\nI am trying to use TensorRT feature to optimize the inference performance of Tensor2Tensor.  But got Op type not registered issue.\nSource code / logs\ngraph = tf.get_default_graph().as_graph_def()\nfrozen_graph = tf.graph_util.remove_training_nodes(graph)\ntrt_graph = trt.create_inference_graph(\n    input_graph_def = frozen_graph, \n    outputs=[model_output],\n    max_batch_size=10)\nbelow error shows when create_inference_graph is called.\n\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'convert_gradient_to_tensor_cc661786' in binary running on cffae59e0618. Makre sure the Op and Kernal are registered in the binary running in this process.\n\nconvert_gradient_to_tensor is defined in tensor2tensor in below way.\n@function.Defun(\n    python_grad_func=lambda x, dy: tf.convert_to_tensor(dy),\n    shape_func=lambda op: [op.inputs[0].get_shape()])\ndef convert_gradient_to_tensor(x):\n    ...", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI am using tensor2tensor library.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nTensorflow docker image with \"1.7.1-devel-gpu\" tag\r\n- **TensorFlow installed from (source or binary)**:\r\nSource(From the docker image, nothing changed), with TensorRT enabled\r\n- **TensorFlow version (use command below)**:\r\n1.7.1\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0\r\n- **GPU model and memory**:\r\nGTX 1080 Ti 11 G\r\n- **Exact command to reproduce**:\r\nSee problem description.\r\n\r\n### Describe the problem\r\nI am trying to use TensorRT feature to optimize the inference performance of Tensor2Tensor.  But got Op type not registered issue. \r\n\r\n### Source code / logs\r\n```python\r\ngraph = tf.get_default_graph().as_graph_def()\r\nfrozen_graph = tf.graph_util.remove_training_nodes(graph)\r\ntrt_graph = trt.create_inference_graph(\r\n    input_graph_def = frozen_graph, \r\n    outputs=[model_output],\r\n    max_batch_size=10)\r\n```\r\nbelow error shows when create_inference_graph is called. \r\n\r\n>tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'convert_gradient_to_tensor_cc661786' in binary running on cffae59e0618. Makre sure the Op and Kernal are registered in the binary running in this process. \r\n\r\nconvert_gradient_to_tensor is defined in tensor2tensor in below way. \r\n\r\n```python\r\n@function.Defun(\r\n    python_grad_func=lambda x, dy: tf.convert_to_tensor(dy),\r\n    shape_func=lambda op: [op.inputs[0].get_shape()])\r\ndef convert_gradient_to_tensor(x):\r\n    ...\r\n```\r\n"}