{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/76345961", "pull_request_review_id": null, "id": 76345961, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc2MzQ1OTYx", "diff_hunk": "@@ -0,0 +1,478 @@\n+/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// Implements convolution operations with other kernels baked into the\n+// processing, to optimize latency and memory usage.\n+\n+#include <string.h>\n+#include <map>\n+#include <vector>\n+#include \"tensorflow/core/framework/common_shape_fns.h\"\n+#include \"tensorflow/core/framework/numeric_op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/resource_mgr.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_slice.h\"\n+#include \"tensorflow/core/kernels/bounds_check.h\"\n+#include \"tensorflow/core/kernels/conv_ops.h\"\n+#include \"tensorflow/core/kernels/gemm_functors.h\"\n+#include \"tensorflow/core/kernels/image_resizer_state.h\"\n+#include \"tensorflow/core/util/mirror_pad_mode.h\"\n+#include \"tensorflow/core/util/padding.h\"\n+#include \"tensorflow/core/util/tensor_format.h\"\n+\n+namespace tensorflow {\n+\n+namespace {\n+\n+// Combines bilinear resizing and mirror padding into the im2col transformation\n+// stage of convolution,\n+template <class T1, class T2, class T3, class TGemmFunctor>\n+class FusedResizeAndPadConvFunctor {\n+ public:\n+  void operator()(OpKernelContext* context, const Tensor& input,\n+                  int input_batches, int resized_height, int resized_width,\n+                  int padded_height, int padded_width, int input_depth,\n+                  const T2* filter_data, int filter_height, int filter_width,\n+                  int filter_count, int stride_rows, int stride_cols,\n+                  Padding padding, T3* output_data, int output_height,\n+                  int output_width, const ImageResizerState& st,\n+                  int top_padding, int bottom_padding, int left_padding,\n+                  int right_padding, int pad_offset) {\n+    if ((input_batches <= 0) || (padded_width <= 0) || (padded_height <= 0) ||\n+        (input_depth <= 0)) {\n+      LOG(WARNING) << \"Conv2D was called with bad input dimensions: \"\n+                   << input_batches << \", \" << padded_height << \", \"\n+                   << padded_width << \", \" << input_depth;\n+      return;\n+    }\n+    if ((filter_width <= 0) || (filter_height <= 0) || (filter_count <= 0)) {\n+      LOG(WARNING) << \"Conv2D was called with bad filter dimensions: \"\n+                   << filter_width << \", \" << filter_height << \", \"\n+                   << filter_count;\n+      return;\n+    }\n+    if ((output_width <= 0) || (output_height <= 0)) {\n+      LOG(WARNING) << \"Conv2D was called with bad output width or height: \"\n+                   << output_width << \", \" << output_height;\n+      return;\n+    }\n+\n+    // These calculations define how the patches will be positioned within the\n+    // input image. The actual definitions are quite complex, and rely on the\n+    // previously-calculated output size.\n+    int filter_left_offset;\n+    int filter_top_offset;\n+    if (padding == VALID) {\n+      filter_left_offset =\n+          ((output_width - 1) * stride_cols + filter_width - padded_width + 1) /\n+          2;\n+      filter_top_offset = ((output_height - 1) * stride_rows + filter_height -\n+                           padded_height + 1) /\n+                          2;\n+    } else {\n+      filter_left_offset =\n+          ((output_width - 1) * stride_cols + filter_width - padded_width) / 2;\n+      filter_top_offset =\n+          ((output_height - 1) * stride_rows + filter_height - padded_height) /\n+          2;\n+    }\n+\n+    // The im2col buffer has # of patches rows, and # of filters cols.\n+    // It's laid out like this, in row major order in memory:\n+    //        < filter value count >\n+    //   ^   +---------------------+\n+    // patch |                     |\n+    // count |                     |\n+    //   v   +---------------------+\n+    // Each patch row contains a filter_width x filter_height patch of the\n+    // input, with the depth channel as the most contiguous in memory, followed\n+    // by the width, then the height. This is the standard memory order in the\n+    // image world if it helps to visualize it.\n+    const int filter_value_count = filter_width * filter_height * input_depth;\n+\n+    // We don't want to allocate a buffer to hold all the patches if the size is\n+    // going to be extremely large, so break it into chunks if it's bigger than\n+    // a limit. Each chunk will be processed serially, so we can refill the\n+    // buffer for the next chunk and reuse it, keeping maximum memory size down.\n+    // In this case, we've picked 16 megabytes as a reasonable limit.\n+    const size_t max_chunk_size = (16 * 1024 * 1024);\n+    OP_REQUIRES(context, (filter_value_count * sizeof(T1)) <= max_chunk_size,\n+                errors::InvalidArgument(\"Im2Col patch too large for buffer\"));\n+    const size_t patches_per_chunk =\n+        max_chunk_size / (filter_value_count * sizeof(T1));\n+    // Because memory allocation is very expensive on mobile platforms, try to\n+    // allocate a persistent buffer that will be kept around between calls. We\n+    // use TensorFlow's resource management to ensure that the memory will be\n+    // released when the session is over.\n+    Im2ColBufferResource<T1, max_chunk_size>* im2col_buffer_resource;\n+    std::function<Status(Im2ColBufferResource<T1, max_chunk_size>**)> creator =\n+        [](Im2ColBufferResource<T1, max_chunk_size>** resource) {\n+          *resource = new Im2ColBufferResource<T1, max_chunk_size>();\n+          return Status::OK();\n+        };\n+    OP_REQUIRES_OK(context, context->resource_manager()->LookupOrCreate(\n+                                \"Conv2d\", \"im2col_buffer\",\n+                                &im2col_buffer_resource, creator));\n+    // This means that multiple ops can't be run simultaneously on different\n+    // threads, because we have a single shared resource. The platforms this is\n+    // aimed at have intra-op parallelism as their focus though, so it shouldn't\n+    // be an issue.\n+    mutex_lock lock_buffer(im2col_buffer_resource->mu);\n+    core::ScopedUnref unref_buffer(im2col_buffer_resource);\n+    T1* im2col_buffer = im2col_buffer_resource->data;\n+\n+    typename TTypes<T1, 4>::ConstTensor input_data = input.tensor<T1, 4>();\n+\n+    for (int batch = 0; batch < input_batches; ++batch) {\n+      for (int out_y = 0; out_y < output_height; ++out_y) {\n+        const int in_y_origin = (out_y * stride_rows) - filter_top_offset;\n+        for (int out_x = 0; out_x < output_width; ++out_x) {\n+          const int in_x_origin = (out_x * stride_cols) - filter_left_offset;\n+          const int patch_index = (batch * output_width * output_height) +\n+                                  (out_y * output_width) + out_x;\n+          const int patch_index_within_chunk = patch_index % patches_per_chunk;\n+          T1* im2col_patch_start =\n+              im2col_buffer + (patch_index_within_chunk * filter_value_count);\n+          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n+            const int conv_in_y = in_y_origin + filter_y;\n+            float in_y = (conv_in_y - top_padding);\n+            if (in_y < 0) {\n+              in_y = -(in_y + 1.0f - pad_offset);\n+            } else if (in_y >= resized_height) {\n+              in_y = (resized_height * 2.0f) - (in_y + 1.0f + pad_offset);\n+            }\n+            in_y *= st.height_scale;\n+            const int64 top_y_index = static_cast<int64>(floorf(in_y));\n+            const int64 bottom_y_index =\n+                std::min(static_cast<int64>(ceilf(in_y)), (st.in_height - 1));\n+            const T1 y_lerp = in_y - top_y_index;\n+            T1* im2col_row_start =\n+                im2col_patch_start + (filter_y * filter_width * input_depth);\n+            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n+              const int conv_in_x = in_x_origin + filter_x;\n+              float in_x = (conv_in_x - left_padding);\n+              if (in_x < 0) {\n+                in_x = -(in_x + 1.0f - pad_offset);\n+              } else if (in_x >= resized_width) {\n+                in_x = (resized_width * 2.0f) - (in_x + 1.0f + pad_offset);\n+              }\n+              in_x *= st.width_scale;\n+              const int64 left_x_index = static_cast<int64>(floorf(in_x));\n+              const int64 right_x_index =\n+                  std::min(static_cast<int64>(ceilf(in_x)), (st.in_width - 1));\n+              const T1 x_lerp = in_x - left_x_index;\n+              T1* im2col_row_pixel =\n+                  im2col_row_start + (filter_x * input_depth);\n+              for (int in_channel = 0; in_channel < input_depth; ++in_channel) {\n+                T1 in_value;\n+                if ((conv_in_x >= 0) && (conv_in_x < padded_width) &&\n+                    (conv_in_y >= 0) && (conv_in_y < padded_height)) {\n+                  const T1 top_left(\n+                      input_data(batch, top_y_index, left_x_index, in_channel));\n+                  const T1 top_right(input_data(batch, top_y_index,\n+                                                right_x_index, in_channel));\n+                  const T1 bottom_left(input_data(batch, bottom_y_index,\n+                                                  left_x_index, in_channel));\n+                  const T1 bottom_right(input_data(batch, bottom_y_index,\n+                                                   right_x_index, in_channel));\n+                  const T1 top = top_left + (top_right - top_left) * x_lerp;\n+                  const T1 bottom =\n+                      bottom_left + (bottom_right - bottom_left) * x_lerp;\n+                  in_value = top + (bottom - top) * y_lerp;\n+                } else {\n+                  in_value = T1(0);\n+                }\n+                im2col_row_pixel[in_channel] = in_value;\n+              }\n+            }\n+          }\n+          const bool is_last_in_chunk =\n+              (patch_index_within_chunk == (patches_per_chunk - 1));\n+          const bool is_last_overall =\n+              ((batch == (input_batches - 1)) &&\n+               (out_y == (output_height - 1)) && (out_x == (output_width - 1)));\n+          if (is_last_in_chunk || is_last_overall) {\n+            // Now we've assembled a set of image patches into a matrix, apply a\n+            // GEMM matrix multiply of the patches as rows, times the filter\n+            // weights in columns, to get partial results in the output matrix.\n+            const int how_many_patches = patch_index_within_chunk + 1;\n+            const int m = how_many_patches;\n+            const int n = filter_count;\n+            const int k = filter_value_count;\n+            const int lda = filter_value_count;\n+            const int ldb = filter_count;\n+            const int ldc = filter_count;\n+            const size_t start_patch_index =\n+                patch_index - (how_many_patches - 1);\n+            T3* chunk_output_data =\n+                output_data + (start_patch_index * filter_count);\n+            TGemmFunctor gemm_functor;\n+            gemm_functor(m, n, k, im2col_buffer, lda, filter_data, ldb,\n+                         chunk_output_data, ldc);\n+          }\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+}  // namespace\n+\n+// Implements a version of convolution with bilinear resizing and mirror padding\n+// included.\n+template <class T, class TConvFunctor>\n+class FusedResizeConv2DUsingGemmOp : public OpKernel {\n+ public:\n+  explicit FusedResizeConv2DUsingGemmOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    OP_REQUIRES_OK(context,\n+                   context->GetAttr(\"resize_align_corners\", &align_corners_));\n+    MirrorPadMode mode;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"mode\", &mode));\n+\n+    switch (mode) {\n+      case MirrorPadMode::SYMMETRIC: {\n+        offset_ = 0;\n+        break;\n+      }\n+      case MirrorPadMode::REFLECT: {\n+        offset_ = 1;\n+        break;\n+      }\n+      default:\n+        OP_REQUIRES(context, false,\n+                    errors::InvalidArgument(\n+                        \"mode must be either REFLECT or SYMMETRIC.\"));\n+    }\n+    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));\n+    OP_REQUIRES(context, strides_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window strides field must \"\n+                                        \"specify 4 dimensions\"));\n+    const int64 stride_n = GetTensorDim(strides_, FORMAT_NHWC, 'N');\n+    const int64 stride_c = GetTensorDim(strides_, FORMAT_NHWC, 'C');\n+    OP_REQUIRES(\n+        context, stride_n == 1 && stride_c == 1,\n+        errors::InvalidArgument(\"Current implementation does not yet support \"\n+                                \"strides in the batch and depth dimensions.\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    // Input tensor is of the following dimensions:\n+    // [ batch, in_rows, in_cols, in_depth ]\n+    const Tensor& input = context->input(0);\n+\n+    ImageResizerState st(align_corners_);\n+    st.ValidateAndCalculateOutputSize(context, input);\n+    const TensorShape resized_shape(\n+        {input.dim_size(0), st.out_height, st.out_width, input.dim_size(3)});\n+\n+    const Tensor& paddings = context->input(2);\n+\n+    const int dims = resized_shape.dims();\n+    static const int kMinDims = 0;\n+    static const int kMaxDims = 5;\n+    OP_REQUIRES(context, kMinDims <= dims && dims <= kMaxDims,\n+                errors::Unimplemented(\"inputs rank not in [\", kMinDims, \",\",\n+                                      kMaxDims, \"]: \", dims));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsMatrix(paddings.shape()) &&\n+                     paddings.dim_size(1) == 2,\n+        errors::InvalidArgument(\"paddings must be a matrix with 2 columns: \",\n+                                paddings.shape().DebugString()));\n+    const int fixed_dims =\n+        (allow_legacy_scalars() && dims == 0 && paddings.dim_size(0) == 1)\n+            ? 1\n+            : dims;\n+    OP_REQUIRES(\n+        context, fixed_dims == paddings.dim_size(0),\n+        errors::InvalidArgument(\n+            \"The first dimension of paddings must be the rank of inputs: \",\n+            fixed_dims, \" \", paddings.shape().DebugString(), \" \",\n+            resized_shape.DebugString()));\n+\n+    OP_REQUIRES(\n+        context, dims == 4,\n+        errors::InvalidArgument(\n+            \"Fused mirror padding only supports four-dimensional inputs, but \",\n+            dims, \" requested\"));\n+\n+    // Compute the shape of the output tensor, and allocate it.\n+    TensorShape padded_shape;\n+    TTypes<int32>::ConstMatrix paddings_matrix = paddings.matrix<int32>();\n+    for (int d = 0; d < dims; ++d) {\n+      const int32 before =\n+          paddings_matrix(d, 0);  // Pad before existing elements.\n+      const int32 after =\n+          paddings_matrix(d, 1);  // Pad after exisitng elements.\n+      OP_REQUIRES(context, before >= 0 && after >= 0,\n+                  errors::InvalidArgument(\"paddings must be non-negative: \",\n+                                          before, \" \", after));\n+      if (offset_ == 0) {  // SYMMETRIC mode.\n+        OP_REQUIRES(\n+            context, before <= resized_shape.dim_size(d) &&\n+                         after <= resized_shape.dim_size(d),\n+            errors::InvalidArgument(\"paddings must be no greater \"\n+                                    \"than the dimension size: \",\n+                                    before, \", \", after, \" greater than \",\n+                                    resized_shape.dim_size(d)));\n+      } else if (offset_ == 1) {  // REFLECT mode.\n+        OP_REQUIRES(\n+            context, before < resized_shape.dim_size(d) &&\n+                         after < resized_shape.dim_size(d),\n+            errors::InvalidArgument(\"paddings must be less than\"\n+                                    \" the dimension size: \",\n+                                    before, \", \", after, \" not less than \",\n+                                    resized_shape.dim_size(d)));\n+      }\n+      padded_shape.AddDim(before + resized_shape.dim_size(d) + after);\n+    }\n+\n+    OP_REQUIRES(\n+        context, ((paddings_matrix(0, 0) == 0) && (paddings_matrix(0, 1) == 0)),\n+        errors::InvalidArgument(\n+            \"Fused mirror padding only support spatial padding, not batches: \",\n+            paddings.DebugString()));\n+    OP_REQUIRES(\n+        context, ((paddings_matrix(3, 0) == 0) && (paddings_matrix(3, 1) == 0)),\n+        errors::InvalidArgument(\n+            \"Fused mirror padding only support spatial padding, not channels: \",\n+            paddings.DebugString()));\n+    const int32 top_padding = paddings_matrix(1, 0);\n+    const int32 bottom_padding = paddings_matrix(1, 1);\n+    const int32 left_padding = paddings_matrix(2, 0);\n+    const int32 right_padding = paddings_matrix(2, 1);\n+\n+    // Input filter is of the following dimensions:\n+    // [ filter_rows, filter_cols, in_depth, out_depth]\n+    const Tensor& filter = context->input(3);\n+\n+    // For 2D convolution, there should be 4 dimensions.\n+    OP_REQUIRES(context, padded_shape.dims() == 4,\n+                errors::InvalidArgument(\"input must be 4-dimensional\",\n+                                        padded_shape.DebugString()));\n+    OP_REQUIRES(context, filter.dims() == 4,\n+                errors::InvalidArgument(\"filter must be 4-dimensional: \",\n+                                        filter.shape().DebugString()));\n+\n+    for (int i = 0; i < 3; i++) {", "path": "tensorflow/core/kernels/conv_ops_fused.cc", "position": 373, "original_position": 373, "commit_id": "8a7479a046882d729bd2ddea75c83d17cdfe059f", "original_commit_id": "8a7479a046882d729bd2ddea75c83d17cdfe059f", "user": {"login": "cwhipkey", "id": 17578177, "node_id": "MDQ6VXNlcjE3NTc4MTc3", "avatar_url": "https://avatars0.githubusercontent.com/u/17578177?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cwhipkey", "html_url": "https://github.com/cwhipkey", "followers_url": "https://api.github.com/users/cwhipkey/followers", "following_url": "https://api.github.com/users/cwhipkey/following{/other_user}", "gists_url": "https://api.github.com/users/cwhipkey/gists{/gist_id}", "starred_url": "https://api.github.com/users/cwhipkey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cwhipkey/subscriptions", "organizations_url": "https://api.github.com/users/cwhipkey/orgs", "repos_url": "https://api.github.com/users/cwhipkey/repos", "events_url": "https://api.github.com/users/cwhipkey/events{/privacy}", "received_events_url": "https://api.github.com/users/cwhipkey/received_events", "type": "User", "site_admin": false}, "body": "< 4 ?\n", "created_at": "2016-08-25T23:55:41Z", "updated_at": "2016-08-25T23:55:41Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4046#discussion_r76345961", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4046", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/76345961"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4046#discussion_r76345961"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4046"}}, "body_html": "<p>&lt; 4 ?</p>", "body_text": "< 4 ?"}