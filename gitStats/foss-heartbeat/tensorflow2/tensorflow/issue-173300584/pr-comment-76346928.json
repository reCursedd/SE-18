{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/76346928", "pull_request_review_id": null, "id": 76346928, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc2MzQ2OTI4", "diff_hunk": "@@ -0,0 +1,478 @@\n+/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// Implements convolution operations with other kernels baked into the\n+// processing, to optimize latency and memory usage.\n+\n+#include <string.h>\n+#include <map>\n+#include <vector>\n+#include \"tensorflow/core/framework/common_shape_fns.h\"\n+#include \"tensorflow/core/framework/numeric_op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/resource_mgr.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_slice.h\"\n+#include \"tensorflow/core/kernels/bounds_check.h\"\n+#include \"tensorflow/core/kernels/conv_ops.h\"\n+#include \"tensorflow/core/kernels/gemm_functors.h\"\n+#include \"tensorflow/core/kernels/image_resizer_state.h\"\n+#include \"tensorflow/core/util/mirror_pad_mode.h\"\n+#include \"tensorflow/core/util/padding.h\"\n+#include \"tensorflow/core/util/tensor_format.h\"\n+\n+namespace tensorflow {\n+\n+namespace {\n+\n+// Combines bilinear resizing and mirror padding into the im2col transformation\n+// stage of convolution,\n+template <class T1, class T2, class T3, class TGemmFunctor>\n+class FusedResizeAndPadConvFunctor {\n+ public:\n+  void operator()(OpKernelContext* context, const Tensor& input,\n+                  int input_batches, int resized_height, int resized_width,\n+                  int padded_height, int padded_width, int input_depth,\n+                  const T2* filter_data, int filter_height, int filter_width,\n+                  int filter_count, int stride_rows, int stride_cols,\n+                  Padding padding, T3* output_data, int output_height,\n+                  int output_width, const ImageResizerState& st,\n+                  int top_padding, int bottom_padding, int left_padding,\n+                  int right_padding, int pad_offset) {\n+    if ((input_batches <= 0) || (padded_width <= 0) || (padded_height <= 0) ||\n+        (input_depth <= 0)) {\n+      LOG(WARNING) << \"Conv2D was called with bad input dimensions: \"\n+                   << input_batches << \", \" << padded_height << \", \"\n+                   << padded_width << \", \" << input_depth;\n+      return;\n+    }\n+    if ((filter_width <= 0) || (filter_height <= 0) || (filter_count <= 0)) {\n+      LOG(WARNING) << \"Conv2D was called with bad filter dimensions: \"\n+                   << filter_width << \", \" << filter_height << \", \"\n+                   << filter_count;\n+      return;\n+    }\n+    if ((output_width <= 0) || (output_height <= 0)) {\n+      LOG(WARNING) << \"Conv2D was called with bad output width or height: \"\n+                   << output_width << \", \" << output_height;\n+      return;\n+    }\n+\n+    // These calculations define how the patches will be positioned within the\n+    // input image. The actual definitions are quite complex, and rely on the\n+    // previously-calculated output size.\n+    int filter_left_offset;\n+    int filter_top_offset;\n+    if (padding == VALID) {\n+      filter_left_offset =\n+          ((output_width - 1) * stride_cols + filter_width - padded_width + 1) /\n+          2;\n+      filter_top_offset = ((output_height - 1) * stride_rows + filter_height -\n+                           padded_height + 1) /\n+                          2;\n+    } else {\n+      filter_left_offset =\n+          ((output_width - 1) * stride_cols + filter_width - padded_width) / 2;\n+      filter_top_offset =\n+          ((output_height - 1) * stride_rows + filter_height - padded_height) /\n+          2;\n+    }\n+\n+    // The im2col buffer has # of patches rows, and # of filters cols.\n+    // It's laid out like this, in row major order in memory:\n+    //        < filter value count >\n+    //   ^   +---------------------+\n+    // patch |                     |\n+    // count |                     |\n+    //   v   +---------------------+\n+    // Each patch row contains a filter_width x filter_height patch of the\n+    // input, with the depth channel as the most contiguous in memory, followed\n+    // by the width, then the height. This is the standard memory order in the\n+    // image world if it helps to visualize it.\n+    const int filter_value_count = filter_width * filter_height * input_depth;\n+\n+    // We don't want to allocate a buffer to hold all the patches if the size is\n+    // going to be extremely large, so break it into chunks if it's bigger than\n+    // a limit. Each chunk will be processed serially, so we can refill the\n+    // buffer for the next chunk and reuse it, keeping maximum memory size down.\n+    // In this case, we've picked 16 megabytes as a reasonable limit.\n+    const size_t max_chunk_size = (16 * 1024 * 1024);\n+    OP_REQUIRES(context, (filter_value_count * sizeof(T1)) <= max_chunk_size,\n+                errors::InvalidArgument(\"Im2Col patch too large for buffer\"));\n+    const size_t patches_per_chunk =\n+        max_chunk_size / (filter_value_count * sizeof(T1));\n+    // Because memory allocation is very expensive on mobile platforms, try to\n+    // allocate a persistent buffer that will be kept around between calls. We\n+    // use TensorFlow's resource management to ensure that the memory will be\n+    // released when the session is over.\n+    Im2ColBufferResource<T1, max_chunk_size>* im2col_buffer_resource;\n+    std::function<Status(Im2ColBufferResource<T1, max_chunk_size>**)> creator =\n+        [](Im2ColBufferResource<T1, max_chunk_size>** resource) {\n+          *resource = new Im2ColBufferResource<T1, max_chunk_size>();\n+          return Status::OK();\n+        };\n+    OP_REQUIRES_OK(context, context->resource_manager()->LookupOrCreate(\n+                                \"Conv2d\", \"im2col_buffer\",\n+                                &im2col_buffer_resource, creator));\n+    // This means that multiple ops can't be run simultaneously on different\n+    // threads, because we have a single shared resource. The platforms this is\n+    // aimed at have intra-op parallelism as their focus though, so it shouldn't\n+    // be an issue.\n+    mutex_lock lock_buffer(im2col_buffer_resource->mu);\n+    core::ScopedUnref unref_buffer(im2col_buffer_resource);\n+    T1* im2col_buffer = im2col_buffer_resource->data;\n+\n+    typename TTypes<T1, 4>::ConstTensor input_data = input.tensor<T1, 4>();\n+\n+    for (int batch = 0; batch < input_batches; ++batch) {", "path": "tensorflow/core/kernels/conv_ops_fused.cc", "position": 141, "original_position": 141, "commit_id": "8a7479a046882d729bd2ddea75c83d17cdfe059f", "original_commit_id": "8a7479a046882d729bd2ddea75c83d17cdfe059f", "user": {"login": "cwhipkey", "id": 17578177, "node_id": "MDQ6VXNlcjE3NTc4MTc3", "avatar_url": "https://avatars0.githubusercontent.com/u/17578177?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cwhipkey", "html_url": "https://github.com/cwhipkey", "followers_url": "https://api.github.com/users/cwhipkey/followers", "following_url": "https://api.github.com/users/cwhipkey/following{/other_user}", "gists_url": "https://api.github.com/users/cwhipkey/gists{/gist_id}", "starred_url": "https://api.github.com/users/cwhipkey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cwhipkey/subscriptions", "organizations_url": "https://api.github.com/users/cwhipkey/orgs", "repos_url": "https://api.github.com/users/cwhipkey/repos", "events_url": "https://api.github.com/users/cwhipkey/events{/privacy}", "received_events_url": "https://api.github.com/users/cwhipkey/received_events", "type": "User", "site_admin": false}, "body": "I should have asked on the other change too - is it possible to do these directly in eigen?  (would it be faster to run that way?)\n", "created_at": "2016-08-26T00:07:22Z", "updated_at": "2016-08-26T00:07:22Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4046#discussion_r76346928", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4046", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/76346928"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4046#discussion_r76346928"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4046"}}, "body_html": "<p>I should have asked on the other change too - is it possible to do these directly in eigen?  (would it be faster to run that way?)</p>", "body_text": "I should have asked on the other change too - is it possible to do these directly in eigen?  (would it be faster to run that way?)"}