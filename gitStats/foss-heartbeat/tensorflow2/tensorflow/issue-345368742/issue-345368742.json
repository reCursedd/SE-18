{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21192", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21192/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21192/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21192/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21192", "id": 345368742, "node_id": "MDU6SXNzdWUzNDUzNjg3NDI=", "number": 21192, "title": "Unimplemented cast int64 to string is not supported", "user": {"login": "aroraakshit", "id": 30349184, "node_id": "MDQ6VXNlcjMwMzQ5MTg0", "avatar_url": "https://avatars1.githubusercontent.com/u/30349184?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aroraakshit", "html_url": "https://github.com/aroraakshit", "followers_url": "https://api.github.com/users/aroraakshit/followers", "following_url": "https://api.github.com/users/aroraakshit/following{/other_user}", "gists_url": "https://api.github.com/users/aroraakshit/gists{/gist_id}", "starred_url": "https://api.github.com/users/aroraakshit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aroraakshit/subscriptions", "organizations_url": "https://api.github.com/users/aroraakshit/orgs", "repos_url": "https://api.github.com/users/aroraakshit/repos", "events_url": "https://api.github.com/users/aroraakshit/events{/privacy}", "received_events_url": "https://api.github.com/users/aroraakshit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-07-27T21:16:10Z", "updated_at": "2018-11-23T18:38:46Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04.4</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li>Bazel version: N/A</li>\n<li>CUDA/cuDNN version: N/A</li>\n<li>GPU model and memory: N/A</li>\n<li>Exact command to reproduce: described below</li>\n<li>Mobile device: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>After following the steps in Using SavedModel with Estimators: When sending a classification request from client, the server is not able to cast it into serialized tf example that model is expecting.</p>\n<p>The same error comes up even when I used <code> tf.contrib.predictor.from_saved_model</code></p>\n<p>So far, I have checked StackOverflow, Tf documentation, issues on Tf and Tf/serving. No one has ever encountered this error. Hence reporting here. This <a href=\"https://github.com/tensorflow/serving/issues/1017\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/serving/issues/1017/hovercard\">issue {#1017}</a> was rejected by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17073551\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/chrisolston\">@chrisolston</a> in Tf/serving because the error trace indicates it is an issue in one of the core modules on main Tf repo and it had looked like there was no serialization of tf example proto in the client side code: But that task is already accomplished by <a href=\"https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service_pb2.py\">prediction_service_pb2.py</a> file.</p>\n<p>There are no other pointers online to what is going wrong here, please help!</p>\n<h3>Source code / error logs</h3>\n<p>TensorFlow model server <strong>error</strong> trace:</p>\n<pre><code>2018-07-27 14:54:49.685755: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at cast_op.cc:77 : Unimplemented: Cast int64 to string is not supported\n2018-07-27 14:54:49.685822: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Unimplemented: Cast int64 to string is not supported\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\n</code></pre>\n<p><strong>Error</strong> on running client side code:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 193, in _blocking_unary_unary\n    credentials=_credentials(protocol_options))\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 500, in __call__\n    return _end_unary_response_blocking(state, call, False, None)\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 434, in _end_unary_response_blocking\n    raise _Rendezvous(state, None, None, deadline)\ngrpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with (StatusCode.UNIMPLEMENTED, Cast int64 to string is notsupported\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]])&gt;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/mldev/intelligent_sp/train_scripts/client.py\", line 63, in &lt;module&gt;\n    run(args.host, args.port, args.input, args.model, args.signature_name)\n  File \"/home/mldev/intelligent_sp/train_scripts/client.py\", line 42, in run\n    result = stub.Classify(request, 10.0)\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 309, in __call__\n    self._request_serializer, self._response_deserializer)\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 195, in _blocking_unary_unary\n    raise _abortion_error(rpc_error_call)\ngrpc.framework.interfaces.face.face.LocalError: LocalError(code=StatusCode.UNIMPLEMENTED, details=\"Cast int64 to string isnot supported\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\")\n</code></pre>\n<p><strong>Relevant Client Code</strong>:</p>\n<pre><code>    channel = implementations.insecure_channel(host, port)\n    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n    # Pre-processing\n    prediction_input = [json.dumps(eval(input_str))]\n    \n    ink, classname = creat.parse_line(prediction_input[0])\n\n    classnames = ['doodle', 'expression', 'symbols']\n    features = {}\n    features[\"class_index\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[classnames.index(\"doodle\")]))\n    features[\"ink\"] = tf.train.Feature(float_list=tf.train.FloatList(value=ink.flatten()))\n    features[\"shape\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=ink.shape))\n    f = tf.train.Features(feature=features)\n    example = tf.train.Example(features=f)\n    final_req = [example]\n    start = time.time()\n\n    # Call classification model to make prediction\n    request = classification_pb2.ClassificationRequest()\n    request.model_spec.name = model\n    request.model_spec.signature_name = signature_name\n    request.input.example_list.examples.extend(final_req)\n    \n    result = stub.Classify(request, 10.0)\n</code></pre>\n<p>Imports used in Client Code:</p>\n<pre><code>from tensorflow_serving.apis import classification_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\nfrom tensorflow_serving.apis import input_pb2 as final_inp\n</code></pre>\n<p>Code used to <strong>export saved model</strong> (runs successfully):</p>\n<pre><code>      feature_spec = {\n      \"ink\": tf.VarLenFeature(dtype=tf.float32),\n      \"shape\": tf.FixedLenFeature([2], dtype=tf.int64),\n      \"class_index\": tf.FixedLenFeature([1], dtype=tf.int64)\n      }\n      \n      # defining serving input receiver function\n      def serving_input_receiver_fn():\n        \"\"\"An input receiver that expects a serialized tf.Example.\"\"\"\n        serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[None], name='input_example_tensor')\n        receiver_tensors = {'examples': serialized_tf_example}\n        parsed_features = tf.parse_example(serialized_tf_example, feature_spec)\n        parsed_features[\"ink\"] = tf.sparse_tensor_to_dense(parsed_features[\"ink\"])\n        return tf.estimator.export.ServingInputReceiver(parsed_features, receiver_tensors)\n\n      # export saved model\n      estimator.export_savedmodel(FLAGS.model_dir+\"/serve/\", serving_input_receiver_fn, strip_default_attrs=True)\n      print(\"done exporting\")\n</code></pre>\n<p>The output from <strong>saved_model_cli</strong> (used to inspect saved model) looks like this:</p>\n<pre><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['output']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: input_example_tensor:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['classes'] tensor_info:\n        dtype: DT_STRING\n        shape: (2)\n        name: Cast:0\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 348)\n        name: dense/BiasAdd:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: input_example_tensor:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['classes'] tensor_info:\n        dtype: DT_STRING\n        shape: (2)\n        name: Cast:0\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 348)\n        name: dense/BiasAdd:0\n  Method name is: tensorflow/serving/classify\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution: Linux Ubuntu 16.04.4\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.8\nPython version: 3.5.2\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: described below\nMobile device: N/A\n\nDescribe the problem\nAfter following the steps in Using SavedModel with Estimators: When sending a classification request from client, the server is not able to cast it into serialized tf example that model is expecting.\nThe same error comes up even when I used  tf.contrib.predictor.from_saved_model\nSo far, I have checked StackOverflow, Tf documentation, issues on Tf and Tf/serving. No one has ever encountered this error. Hence reporting here. This issue {#1017} was rejected by @chrisolston in Tf/serving because the error trace indicates it is an issue in one of the core modules on main Tf repo and it had looked like there was no serialization of tf example proto in the client side code: But that task is already accomplished by prediction_service_pb2.py file.\nThere are no other pointers online to what is going wrong here, please help!\nSource code / error logs\nTensorFlow model server error trace:\n2018-07-27 14:54:49.685755: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at cast_op.cc:77 : Unimplemented: Cast int64 to string is not supported\n2018-07-27 14:54:49.685822: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Unimplemented: Cast int64 to string is not supported\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\n\nError on running client side code:\nTraceback (most recent call last):\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 193, in _blocking_unary_unary\n    credentials=_credentials(protocol_options))\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 500, in __call__\n    return _end_unary_response_blocking(state, call, False, None)\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 434, in _end_unary_response_blocking\n    raise _Rendezvous(state, None, None, deadline)\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNIMPLEMENTED, Cast int64 to string is notsupported\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]])>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/mldev/intelligent_sp/train_scripts/client.py\", line 63, in <module>\n    run(args.host, args.port, args.input, args.model, args.signature_name)\n  File \"/home/mldev/intelligent_sp/train_scripts/client.py\", line 42, in run\n    result = stub.Classify(request, 10.0)\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 309, in __call__\n    self._request_serializer, self._response_deserializer)\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 195, in _blocking_unary_unary\n    raise _abortion_error(rpc_error_call)\ngrpc.framework.interfaces.face.face.LocalError: LocalError(code=StatusCode.UNIMPLEMENTED, details=\"Cast int64 to string isnot supported\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\")\n\nRelevant Client Code:\n    channel = implementations.insecure_channel(host, port)\n    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n    # Pre-processing\n    prediction_input = [json.dumps(eval(input_str))]\n    \n    ink, classname = creat.parse_line(prediction_input[0])\n\n    classnames = ['doodle', 'expression', 'symbols']\n    features = {}\n    features[\"class_index\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[classnames.index(\"doodle\")]))\n    features[\"ink\"] = tf.train.Feature(float_list=tf.train.FloatList(value=ink.flatten()))\n    features[\"shape\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=ink.shape))\n    f = tf.train.Features(feature=features)\n    example = tf.train.Example(features=f)\n    final_req = [example]\n    start = time.time()\n\n    # Call classification model to make prediction\n    request = classification_pb2.ClassificationRequest()\n    request.model_spec.name = model\n    request.model_spec.signature_name = signature_name\n    request.input.example_list.examples.extend(final_req)\n    \n    result = stub.Classify(request, 10.0)\n\nImports used in Client Code:\nfrom tensorflow_serving.apis import classification_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\nfrom tensorflow_serving.apis import input_pb2 as final_inp\n\nCode used to export saved model (runs successfully):\n      feature_spec = {\n      \"ink\": tf.VarLenFeature(dtype=tf.float32),\n      \"shape\": tf.FixedLenFeature([2], dtype=tf.int64),\n      \"class_index\": tf.FixedLenFeature([1], dtype=tf.int64)\n      }\n      \n      # defining serving input receiver function\n      def serving_input_receiver_fn():\n        \"\"\"An input receiver that expects a serialized tf.Example.\"\"\"\n        serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[None], name='input_example_tensor')\n        receiver_tensors = {'examples': serialized_tf_example}\n        parsed_features = tf.parse_example(serialized_tf_example, feature_spec)\n        parsed_features[\"ink\"] = tf.sparse_tensor_to_dense(parsed_features[\"ink\"])\n        return tf.estimator.export.ServingInputReceiver(parsed_features, receiver_tensors)\n\n      # export saved model\n      estimator.export_savedmodel(FLAGS.model_dir+\"/serve/\", serving_input_receiver_fn, strip_default_attrs=True)\n      print(\"done exporting\")\n\nThe output from saved_model_cli (used to inspect saved model) looks like this:\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['output']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: input_example_tensor:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['classes'] tensor_info:\n        dtype: DT_STRING\n        shape: (2)\n        name: Cast:0\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 348)\n        name: dense/BiasAdd:0\n  Method name is: tensorflow/serving/classify\n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_STRING\n        shape: (-1)\n        name: input_example_tensor:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['classes'] tensor_info:\n        dtype: DT_STRING\n        shape: (2)\n        name: Cast:0\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (2, 348)\n        name: dense/BiasAdd:0\n  Method name is: tensorflow/serving/classify", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04.4\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5.2\r\n- Bazel version: N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Exact command to reproduce: described below\r\n- Mobile device: N/A\r\n\r\n### Describe the problem\r\nAfter following the steps in Using SavedModel with Estimators: When sending a classification request from client, the server is not able to cast it into serialized tf example that model is expecting.\r\n\r\nThe same error comes up even when I used ```\r\ntf.contrib.predictor.from_saved_model```\r\n\r\nSo far, I have checked StackOverflow, Tf documentation, issues on Tf and Tf/serving. No one has ever encountered this error. Hence reporting here. This [issue {#1017}](https://github.com/tensorflow/serving/issues/1017) was rejected by @chrisolston in Tf/serving because the error trace indicates it is an issue in one of the core modules on main Tf repo and it had looked like there was no serialization of tf example proto in the client side code: But that task is already accomplished by [prediction_service_pb2.py](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service_pb2.py) file. \r\n\r\nThere are no other pointers online to what is going wrong here, please help!\r\n\r\n### Source code / error logs\r\nTensorFlow model server **error** trace:\r\n```\r\n2018-07-27 14:54:49.685755: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at cast_op.cc:77 : Unimplemented: Cast int64 to string is not supported\r\n2018-07-27 14:54:49.685822: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Unimplemented: Cast int64 to string is not supported\r\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\r\n```\r\n\r\n**Error** on running client side code:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 193, in _blocking_unary_unary\r\n    credentials=_credentials(protocol_options))\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 500, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 434, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNIMPLEMENTED, Cast int64 to string is notsupported\r\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]])>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mldev/intelligent_sp/train_scripts/client.py\", line 63, in <module>\r\n    run(args.host, args.port, args.input, args.model, args.signature_name)\r\n  File \"/home/mldev/intelligent_sp/train_scripts/client.py\", line 42, in run\r\n    result = stub.Classify(request, 10.0)\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 309, in __call__\r\n    self._request_serializer, self._response_deserializer)\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 195, in _blocking_unary_unary\r\n    raise _abortion_error(rpc_error_call)\r\ngrpc.framework.interfaces.face.face.LocalError: LocalError(code=StatusCode.UNIMPLEMENTED, details=\"Cast int64 to string isnot supported\r\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\")\r\n```\r\n\r\n**Relevant Client Code**:\r\n```\r\n    channel = implementations.insecure_channel(host, port)\r\n    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n\r\n    # Pre-processing\r\n    prediction_input = [json.dumps(eval(input_str))]\r\n    \r\n    ink, classname = creat.parse_line(prediction_input[0])\r\n\r\n    classnames = ['doodle', 'expression', 'symbols']\r\n    features = {}\r\n    features[\"class_index\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[classnames.index(\"doodle\")]))\r\n    features[\"ink\"] = tf.train.Feature(float_list=tf.train.FloatList(value=ink.flatten()))\r\n    features[\"shape\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=ink.shape))\r\n    f = tf.train.Features(feature=features)\r\n    example = tf.train.Example(features=f)\r\n    final_req = [example]\r\n    start = time.time()\r\n\r\n    # Call classification model to make prediction\r\n    request = classification_pb2.ClassificationRequest()\r\n    request.model_spec.name = model\r\n    request.model_spec.signature_name = signature_name\r\n    request.input.example_list.examples.extend(final_req)\r\n    \r\n    result = stub.Classify(request, 10.0)\r\n```\r\n\r\nImports used in Client Code:\r\n```\r\nfrom tensorflow_serving.apis import classification_pb2\r\nfrom tensorflow_serving.apis import prediction_service_pb2\r\nfrom tensorflow_serving.apis import input_pb2 as final_inp\r\n```\r\n\r\nCode used to **export saved model** (runs successfully):\r\n```\r\n      feature_spec = {\r\n      \"ink\": tf.VarLenFeature(dtype=tf.float32),\r\n      \"shape\": tf.FixedLenFeature([2], dtype=tf.int64),\r\n      \"class_index\": tf.FixedLenFeature([1], dtype=tf.int64)\r\n      }\r\n      \r\n      # defining serving input receiver function\r\n      def serving_input_receiver_fn():\r\n        \"\"\"An input receiver that expects a serialized tf.Example.\"\"\"\r\n        serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[None], name='input_example_tensor')\r\n        receiver_tensors = {'examples': serialized_tf_example}\r\n        parsed_features = tf.parse_example(serialized_tf_example, feature_spec)\r\n        parsed_features[\"ink\"] = tf.sparse_tensor_to_dense(parsed_features[\"ink\"])\r\n        return tf.estimator.export.ServingInputReceiver(parsed_features, receiver_tensors)\r\n\r\n      # export saved model\r\n      estimator.export_savedmodel(FLAGS.model_dir+\"/serve/\", serving_input_receiver_fn, strip_default_attrs=True)\r\n      print(\"done exporting\")\r\n```\r\n\r\nThe output from **saved_model_cli** (used to inspect saved model) looks like this:\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['output']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['inputs'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1)\r\n        name: input_example_tensor:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['classes'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (2)\r\n        name: Cast:0\r\n    outputs['scores'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (2, 348)\r\n        name: dense/BiasAdd:0\r\n  Method name is: tensorflow/serving/classify\r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['inputs'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1)\r\n        name: input_example_tensor:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['classes'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (2)\r\n        name: Cast:0\r\n    outputs['scores'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (2, 348)\r\n        name: dense/BiasAdd:0\r\n  Method name is: tensorflow/serving/classify\r\n```\r\n\r\n"}