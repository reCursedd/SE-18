{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243592117", "html_url": "https://github.com/tensorflow/tensorflow/issues/4072#issuecomment-243592117", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4072", "id": 243592117, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzU5MjExNw==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-30T21:47:57Z", "updated_at": "2016-08-30T21:47:57Z", "author_association": "MEMBER", "body_html": "<p>We are very interested in improving TF performance on the kinds of models users care about.</p>\n<p>For the models in this paper it seems unlikely that python overhead for initiating each minibatch is a major factor.  It's more likely that the performance differences are primarily due to the efficiency of the kernels in use for the expensive operations (matmul, for FCN), and the degree to which operator fusion is applied to compile away the overhead of launching individual ops, and save on memory bandwidth.   I notice that in the chart you posted, TF's relative performance is best for CPUs with lots of cores, which corresponds to the kind of CPU platform used at Google.  I'm a little surprised at the cited relatively poor performance for FCNs on GPUs, given that the paper claims all systems are using cuDNN.</p>\n<p>In the future we intend to apply operator fusion more widely and continue to improve the kernel libraries used.</p>", "body_text": "We are very interested in improving TF performance on the kinds of models users care about.\nFor the models in this paper it seems unlikely that python overhead for initiating each minibatch is a major factor.  It's more likely that the performance differences are primarily due to the efficiency of the kernels in use for the expensive operations (matmul, for FCN), and the degree to which operator fusion is applied to compile away the overhead of launching individual ops, and save on memory bandwidth.   I notice that in the chart you posted, TF's relative performance is best for CPUs with lots of cores, which corresponds to the kind of CPU platform used at Google.  I'm a little surprised at the cited relatively poor performance for FCNs on GPUs, given that the paper claims all systems are using cuDNN.\nIn the future we intend to apply operator fusion more widely and continue to improve the kernel libraries used.", "body": "We are very interested in improving TF performance on the kinds of models users care about. \n\nFor the models in this paper it seems unlikely that python overhead for initiating each minibatch is a major factor.  It's more likely that the performance differences are primarily due to the efficiency of the kernels in use for the expensive operations (matmul, for FCN), and the degree to which operator fusion is applied to compile away the overhead of launching individual ops, and save on memory bandwidth.   I notice that in the chart you posted, TF's relative performance is best for CPUs with lots of cores, which corresponds to the kind of CPU platform used at Google.  I'm a little surprised at the cited relatively poor performance for FCNs on GPUs, given that the paper claims all systems are using cuDNN.  \n\nIn the future we intend to apply operator fusion more widely and continue to improve the kernel libraries used.\n"}