{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4539", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4539/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4539/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4539/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4539", "id": 178765408, "node_id": "MDU6SXNzdWUxNzg3NjU0MDg=", "number": 4539, "title": "Clarification about embedding_attention_seq2seq", "user": {"login": "sahiliitm", "id": 5723372, "node_id": "MDQ6VXNlcjU3MjMzNzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/5723372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sahiliitm", "html_url": "https://github.com/sahiliitm", "followers_url": "https://api.github.com/users/sahiliitm/followers", "following_url": "https://api.github.com/users/sahiliitm/following{/other_user}", "gists_url": "https://api.github.com/users/sahiliitm/gists{/gist_id}", "starred_url": "https://api.github.com/users/sahiliitm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sahiliitm/subscriptions", "organizations_url": "https://api.github.com/users/sahiliitm/orgs", "repos_url": "https://api.github.com/users/sahiliitm/repos", "events_url": "https://api.github.com/users/sahiliitm/events{/privacy}", "received_events_url": "https://api.github.com/users/sahiliitm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2016-09-23T01:56:23Z", "updated_at": "2016-09-23T02:24:40Z", "closed_at": "2016-09-23T02:24:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This would qualify as a bug depending on what was the intention behind writing the code, the way it was written and whether I understand the workings of Tensorflow well enough.</p>\n<p>The <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L755\"><code>cell</code> input</a> given to<br>\n<code>embedding_attention_seq2seq</code> function in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py\">seq2seq.py</a>  seems to be used both by <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L811\">the encoder</a> and by <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L832\">the decoder</a>. Internally, the EmbeddingWrapper class just does <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L717\"><code>self._cell = cell</code></a>  which means that it does not create a copy of the cell. Doesn't this mean that the decoder and encoder end up sharing the parameters of their respective LSTMs? is this intentional?</p>\n<p>EDIT: realized why they're different cells: because of the scopes.</p>", "body_text": "This would qualify as a bug depending on what was the intention behind writing the code, the way it was written and whether I understand the workings of Tensorflow well enough.\nThe cell input given to\nembedding_attention_seq2seq function in seq2seq.py  seems to be used both by the encoder and by the decoder. Internally, the EmbeddingWrapper class just does self._cell = cell  which means that it does not create a copy of the cell. Doesn't this mean that the decoder and encoder end up sharing the parameters of their respective LSTMs? is this intentional?\nEDIT: realized why they're different cells: because of the scopes.", "body": "This would qualify as a bug depending on what was the intention behind writing the code, the way it was written and whether I understand the workings of Tensorflow well enough.\n\nThe [`cell` input](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L755) given to\n`embedding_attention_seq2seq` function in [seq2seq.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py)  seems to be used both by [the encoder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L811) and by [the decoder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L832). Internally, the EmbeddingWrapper class just does [`self._cell = cell`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L717)  which means that it does not create a copy of the cell. Doesn't this mean that the decoder and encoder end up sharing the parameters of their respective LSTMs? is this intentional?\n\nEDIT: realized why they're different cells: because of the scopes.\n"}