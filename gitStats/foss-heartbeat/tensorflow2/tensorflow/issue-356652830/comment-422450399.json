{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422450399", "html_url": "https://github.com/tensorflow/tensorflow/issues/22042#issuecomment-422450399", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22042", "id": 422450399, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjQ1MDM5OQ==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-18T15:57:37Z", "updated_at": "2018-09-18T15:57:37Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> This error is happening because once the variable has been placed on the GPU we need to run any op which updates it (including <code>ResourceSparseApplyRMSProp</code> here) on the same device the variable is, which means there's nothing the eager placer can do but fail.</p>\n<p>The correct long-term fix here is to register the adagrad optimizers for CPU and GPU.</p>\n<p>The correct short-term fix is to force-place the variables on the CPU by building the model inside a <code>with tf.device('cpu:0'):</code> block.</p>", "body_text": "@asimshankar This error is happening because once the variable has been placed on the GPU we need to run any op which updates it (including ResourceSparseApplyRMSProp here) on the same device the variable is, which means there's nothing the eager placer can do but fail.\nThe correct long-term fix here is to register the adagrad optimizers for CPU and GPU.\nThe correct short-term fix is to force-place the variables on the CPU by building the model inside a with tf.device('cpu:0'): block.", "body": "@asimshankar This error is happening because once the variable has been placed on the GPU we need to run any op which updates it (including `ResourceSparseApplyRMSProp` here) on the same device the variable is, which means there's nothing the eager placer can do but fail.\r\n\r\nThe correct long-term fix here is to register the adagrad optimizers for CPU and GPU.\r\n\r\nThe correct short-term fix is to force-place the variables on the CPU by building the model inside a `with tf.device('cpu:0'):` block."}