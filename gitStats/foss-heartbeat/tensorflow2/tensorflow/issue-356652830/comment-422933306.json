{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422933306", "html_url": "https://github.com/tensorflow/tensorflow/issues/22042#issuecomment-422933306", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22042", "id": 422933306, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjkzMzMwNg==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-19T19:42:46Z", "updated_at": "2018-09-19T19:42:46Z", "author_association": "MEMBER", "body_html": "<div class=\"email-quoted-reply\">On Wed, Sep 19, 2018 at 12:33 PM trias702 ***@***.***&gt; wrote:\n There's definitely something odd going on with TF Eager and memory\n allocation. Doing the set_random_seed(1) trick fixes the strange QR issue,\n but the model is for some reason then trying to fit the entire training set\n into memory, even though I have batch_size = 16 set, so it should not be\n doing that.\n</div>\n<div class=\"email-fragment\">Are you using tf.data to do your batching? It is possible if you're loading\nnumpy data without tf.data that you'll accidentally copy the entire dataset\nto the GPU only to perform slicing of a minibatch on the GPU. If you open\nanother bug for this issue with some example code I can help you debug.</div>\n<div class=\"email-quoted-reply\"> Furthermore, if I just restart the entire kernel, everything works, but\n only for the first call to model.fit(). Any subsequent calls to model.fit()\n will hit all of the issues I noted above, first the cuSolverDN issue, then\n the OOM issue.\n</div>\n<div class=\"email-fragment\">Can you file an issue with instructions to reproduce for the cuSolverDN\nissue? Hopefully after testing that it still appears on nightly?</div>", "body_text": "On Wed, Sep 19, 2018 at 12:33 PM trias702 ***@***.***> wrote:\n There's definitely something odd going on with TF Eager and memory\n allocation. Doing the set_random_seed(1) trick fixes the strange QR issue,\n but the model is for some reason then trying to fit the entire training set\n into memory, even though I have batch_size = 16 set, so it should not be\n doing that.\n\nAre you using tf.data to do your batching? It is possible if you're loading\nnumpy data without tf.data that you'll accidentally copy the entire dataset\nto the GPU only to perform slicing of a minibatch on the GPU. If you open\nanother bug for this issue with some example code I can help you debug.\n Furthermore, if I just restart the entire kernel, everything works, but\n only for the first call to model.fit(). Any subsequent calls to model.fit()\n will hit all of the issues I noted above, first the cuSolverDN issue, then\n the OOM issue.\n\nCan you file an issue with instructions to reproduce for the cuSolverDN\nissue? Hopefully after testing that it still appears on nightly?", "body": "On Wed, Sep 19, 2018 at 12:33 PM trias702 <notifications@github.com> wrote:\n\n> There's definitely something odd going on with TF Eager and memory\n> allocation. Doing the set_random_seed(1) trick fixes the strange QR issue,\n> but the model is for some reason then trying to fit the entire training set\n> into memory, even though I have batch_size = 16 set, so it should not be\n> doing that.\n>\nAre you using tf.data to do your batching? It is possible if you're loading\nnumpy data without tf.data that you'll accidentally copy the entire dataset\nto the GPU only to perform slicing of a minibatch on the GPU. If you open\nanother bug for this issue with some example code I can help you debug.\n\n\n> Furthermore, if I just restart the entire kernel, everything works, but\n> only for the first call to model.fit(). Any subsequent calls to model.fit()\n> will hit all of the issues I noted above, first the cuSolverDN issue, then\n> the OOM issue.\n>\nCan you file an issue with instructions to reproduce for the cuSolverDN\nissue? Hopefully after testing that it still appears on nightly?\n"}