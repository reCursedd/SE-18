{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/418392971", "html_url": "https://github.com/tensorflow/tensorflow/issues/22042#issuecomment-418392971", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22042", "id": 418392971, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODM5Mjk3MQ==", "user": {"login": "trias702", "id": 25867060, "node_id": "MDQ6VXNlcjI1ODY3MDYw", "avatar_url": "https://avatars3.githubusercontent.com/u/25867060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trias702", "html_url": "https://github.com/trias702", "followers_url": "https://api.github.com/users/trias702/followers", "following_url": "https://api.github.com/users/trias702/following{/other_user}", "gists_url": "https://api.github.com/users/trias702/gists{/gist_id}", "starred_url": "https://api.github.com/users/trias702/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trias702/subscriptions", "organizations_url": "https://api.github.com/users/trias702/orgs", "repos_url": "https://api.github.com/users/trias702/repos", "events_url": "https://api.github.com/users/trias702/events{/privacy}", "received_events_url": "https://api.github.com/users/trias702/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-04T14:41:30Z", "updated_at": "2018-09-04T14:41:30Z", "author_association": "NONE", "body_html": "<p>Came to post the same thing, only for me it's using the RMS Optimizer with an RNN and Eager. I note that using the same RMS Optimizer with a vanilla ANN works fine. Here is my error for RMS + RNN:</p>\n<pre><code>NotFoundError: No registered 'ResourceSparseApplyRMSProp' OpKernel for GPU devices compatible with node ResourceSparseApplyRMSProp = ResourceSparseApplyRMSProp[T=DT_FLOAT, Tindices=DT_INT32, use_locking=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)\n\t.  Registered:  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\n [Op:ResourceSparseApplyRMSProp]\n</code></pre>\n<p>I also note that this example was done using tf.keras and Eager. If I use regular Keras and static graphs, I am able to use RMSProp with any RNN I build.</p>\n<p>Have I written custom code: No<br>\nOS Platform and Distribution: Windows 10 x64 1603 LTSB<br>\nTensorFlow installed from: Self compiled<br>\nTensorFlow version: 1.10<br>\nBazel version: 0.16.1<br>\nCUDA/cuDNN version: 9.2 / 7.1.4<br>\nGPU model and memory: GTX 1060m<br>\nExact command to reproduce:</p>\n<pre><code>    model = tf.keras.models.Sequential()\n    model.add(layers.Embedding(NUM_WORDS, 128, input_length=MAXLEN))\n    model.add(layers.SeparableConv1D(64,\n                     5,\n                     padding='valid',\n                     activation='relu',\n                     strides=1))\n    model.add(layers.MaxPooling1D(pool_size=4))\n        \n    model.add(layers.LSTM(256, dropout=0.2, recurrent_dropout=0.5, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None), return_sequences=False))\n\n    model.add(layers.Dense(128, activation='elu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None)))\n    model.add(layers.BatchNormalization(momentum=0.9))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(128, activation='elu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None)))\n    model.add(layers.BatchNormalization(momentum=0.9))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(y_num_classes, activation='sigmoid'))\n\n    # Eager doesn't like RMSProp here for some reason, causes errors\n    model.compile(optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01), loss='binary_crossentropy', metrics=['acc'])\n    \n    model.fit(...)\n</code></pre>\n<p>After I get this error, if I switch to Adam as the optimizer, the code .fit() code runs, but the gradient descent never works, that is, training loss never changes and the weights never change (so training is basically not happening). The only way to fix that is to reset the python kernel entirely, but even then, if you use Adam from the start, you can only call model.fit() once, and then any other calls never train, each epoch runs, but nothing happens.</p>\n<p>All of these Eager bugs seem to be happening only with RNNs though, I do not get any of these issues when just using Dense layers and no Conv or RNN layers.</p>\n<p>So it seems that something may be very seriously wrong with Eager + tf.keras + Conv/RNN layers on GPU.</p>", "body_text": "Came to post the same thing, only for me it's using the RMS Optimizer with an RNN and Eager. I note that using the same RMS Optimizer with a vanilla ANN works fine. Here is my error for RMS + RNN:\nNotFoundError: No registered 'ResourceSparseApplyRMSProp' OpKernel for GPU devices compatible with node ResourceSparseApplyRMSProp = ResourceSparseApplyRMSProp[T=DT_FLOAT, Tindices=DT_INT32, use_locking=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)\n\t.  Registered:  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\n [Op:ResourceSparseApplyRMSProp]\n\nI also note that this example was done using tf.keras and Eager. If I use regular Keras and static graphs, I am able to use RMSProp with any RNN I build.\nHave I written custom code: No\nOS Platform and Distribution: Windows 10 x64 1603 LTSB\nTensorFlow installed from: Self compiled\nTensorFlow version: 1.10\nBazel version: 0.16.1\nCUDA/cuDNN version: 9.2 / 7.1.4\nGPU model and memory: GTX 1060m\nExact command to reproduce:\n    model = tf.keras.models.Sequential()\n    model.add(layers.Embedding(NUM_WORDS, 128, input_length=MAXLEN))\n    model.add(layers.SeparableConv1D(64,\n                     5,\n                     padding='valid',\n                     activation='relu',\n                     strides=1))\n    model.add(layers.MaxPooling1D(pool_size=4))\n        \n    model.add(layers.LSTM(256, dropout=0.2, recurrent_dropout=0.5, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None), return_sequences=False))\n\n    model.add(layers.Dense(128, activation='elu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None)))\n    model.add(layers.BatchNormalization(momentum=0.9))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(128, activation='elu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None)))\n    model.add(layers.BatchNormalization(momentum=0.9))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(y_num_classes, activation='sigmoid'))\n\n    # Eager doesn't like RMSProp here for some reason, causes errors\n    model.compile(optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01), loss='binary_crossentropy', metrics=['acc'])\n    \n    model.fit(...)\n\nAfter I get this error, if I switch to Adam as the optimizer, the code .fit() code runs, but the gradient descent never works, that is, training loss never changes and the weights never change (so training is basically not happening). The only way to fix that is to reset the python kernel entirely, but even then, if you use Adam from the start, you can only call model.fit() once, and then any other calls never train, each epoch runs, but nothing happens.\nAll of these Eager bugs seem to be happening only with RNNs though, I do not get any of these issues when just using Dense layers and no Conv or RNN layers.\nSo it seems that something may be very seriously wrong with Eager + tf.keras + Conv/RNN layers on GPU.", "body": "Came to post the same thing, only for me it's using the RMS Optimizer with an RNN and Eager. I note that using the same RMS Optimizer with a vanilla ANN works fine. Here is my error for RMS + RNN:\r\n\r\n```\r\nNotFoundError: No registered 'ResourceSparseApplyRMSProp' OpKernel for GPU devices compatible with node ResourceSparseApplyRMSProp = ResourceSparseApplyRMSProp[T=DT_FLOAT, Tindices=DT_INT32, use_locking=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)\r\n\t.  Registered:  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n [Op:ResourceSparseApplyRMSProp]\r\n```\r\n\r\nI also note that this example was done using tf.keras and Eager. If I use regular Keras and static graphs, I am able to use RMSProp with any RNN I build.\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows 10 x64 1603 LTSB\r\nTensorFlow installed from: Self compiled\r\nTensorFlow version: 1.10\r\nBazel version: 0.16.1\r\nCUDA/cuDNN version: 9.2 / 7.1.4\r\nGPU model and memory: GTX 1060m\r\nExact command to reproduce:\r\n\r\n```\r\n    model = tf.keras.models.Sequential()\r\n    model.add(layers.Embedding(NUM_WORDS, 128, input_length=MAXLEN))\r\n    model.add(layers.SeparableConv1D(64,\r\n                     5,\r\n                     padding='valid',\r\n                     activation='relu',\r\n                     strides=1))\r\n    model.add(layers.MaxPooling1D(pool_size=4))\r\n        \r\n    model.add(layers.LSTM(256, dropout=0.2, recurrent_dropout=0.5, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None), return_sequences=False))\r\n\r\n    model.add(layers.Dense(128, activation='elu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None)))\r\n    model.add(layers.BatchNormalization(momentum=0.9))\r\n    model.add(layers.Dropout(0.5))\r\n    model.add(layers.Dense(128, activation='elu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=1.0, mode='fan_avg', seed=None)))\r\n    model.add(layers.BatchNormalization(momentum=0.9))\r\n    model.add(layers.Dropout(0.5))\r\n    model.add(layers.Dense(y_num_classes, activation='sigmoid'))\r\n\r\n    # Eager doesn't like RMSProp here for some reason, causes errors\r\n    model.compile(optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01), loss='binary_crossentropy', metrics=['acc'])\r\n    \r\n    model.fit(...)\r\n```\r\n\r\nAfter I get this error, if I switch to Adam as the optimizer, the code .fit() code runs, but the gradient descent never works, that is, training loss never changes and the weights never change (so training is basically not happening). The only way to fix that is to reset the python kernel entirely, but even then, if you use Adam from the start, you can only call model.fit() once, and then any other calls never train, each epoch runs, but nothing happens.\r\n\r\nAll of these Eager bugs seem to be happening only with RNNs though, I do not get any of these issues when just using Dense layers and no Conv or RNN layers.\r\n\r\nSo it seems that something may be very seriously wrong with Eager + tf.keras + Conv/RNN layers on GPU."}