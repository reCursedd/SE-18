{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422927957", "html_url": "https://github.com/tensorflow/tensorflow/issues/22042#issuecomment-422927957", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22042", "id": 422927957, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjkyNzk1Nw==", "user": {"login": "trias702", "id": 25867060, "node_id": "MDQ6VXNlcjI1ODY3MDYw", "avatar_url": "https://avatars3.githubusercontent.com/u/25867060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trias702", "html_url": "https://github.com/trias702", "followers_url": "https://api.github.com/users/trias702/followers", "following_url": "https://api.github.com/users/trias702/following{/other_user}", "gists_url": "https://api.github.com/users/trias702/gists{/gist_id}", "starred_url": "https://api.github.com/users/trias702/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trias702/subscriptions", "organizations_url": "https://api.github.com/users/trias702/orgs", "repos_url": "https://api.github.com/users/trias702/repos", "events_url": "https://api.github.com/users/trias702/events{/privacy}", "received_events_url": "https://api.github.com/users/trias702/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-19T19:24:19Z", "updated_at": "2018-09-19T19:24:19Z", "author_association": "NONE", "body_html": "<p>Ok, just wanted to check since I've only ever seen it done the other way.</p>\n<p>Just a quick thing though, I think there is another strange bug here happening somewhere. I've slipstreamed the changes from your commit into my local Tensorflow 1.10 installation on Windows. I was then able to successfully run, my code as written above, so from that perspective, job done, your fix seems to work.</p>\n<p>However, when I immediately re-ran the model.fit() again, I hit this error:</p>\n<pre><code>File \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1903, in &lt;lambda&gt;\n    shape, dtype=dtype, partition_info=partition_info)\n\n  File \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py\", line 549, in __call__\n    q, r = gen_linalg_ops.qr(a, full_matrices=False)\n\n  File \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py\", line 1518, in qr\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\n\n  File \"&lt;string&gt;\", line 3, in raise_from\n\nInternalError: E:\\Github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:468: cuSolverDN call failed with status =7 [Op:Qr]\n</code></pre>\n<p>So I tried again, and got:</p>\n<pre><code>...\nInternalError: E:\\Github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:468: cuSolverDN call failed with status =6 [Op:Qr]\n</code></pre>\n<p>I looked into both files, and saw that this might be happening from failed memory allocation calls, so I did a quick Google search which brought me to: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"328179203\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/19671\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/19671/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/19671\">#19671</a></p>\n<p>I tried the workaround from that, by calling</p>\n<p>tf.set_random_seed(1)</p>\n<p>And this fixed the cuSolverDN issue, however, trying to run model.fit() now gives the following error every time:</p>\n<p><code>ResourceExhaustedError: OOM when allocating tensor with shape[10000,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc [Op:AssignVariableOp]</code></p>\n<p>There's definitely something odd going on with TF Eager and memory allocation. Doing the set_random_seed(1) trick fixes the strange QR issue, but the model is for some reason then trying to fit the entire training set into memory, even though I have batch_size = 16 set, so it should not be doing that.</p>\n<p>Furthermore, if I just restart the entire kernel, everything works, but only for the first call to model.fit(). Any subsequent calls to model.fit() will hit all of the issues I noted above, first the cuSolverDN issue, then the OOM issue.</p>", "body_text": "Ok, just wanted to check since I've only ever seen it done the other way.\nJust a quick thing though, I think there is another strange bug here happening somewhere. I've slipstreamed the changes from your commit into my local Tensorflow 1.10 installation on Windows. I was then able to successfully run, my code as written above, so from that perspective, job done, your fix seems to work.\nHowever, when I immediately re-ran the model.fit() again, I hit this error:\nFile \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1903, in <lambda>\n    shape, dtype=dtype, partition_info=partition_info)\n\n  File \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py\", line 549, in __call__\n    q, r = gen_linalg_ops.qr(a, full_matrices=False)\n\n  File \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py\", line 1518, in qr\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\n\n  File \"<string>\", line 3, in raise_from\n\nInternalError: E:\\Github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:468: cuSolverDN call failed with status =7 [Op:Qr]\n\nSo I tried again, and got:\n...\nInternalError: E:\\Github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:468: cuSolverDN call failed with status =6 [Op:Qr]\n\nI looked into both files, and saw that this might be happening from failed memory allocation calls, so I did a quick Google search which brought me to: #19671\nI tried the workaround from that, by calling\ntf.set_random_seed(1)\nAnd this fixed the cuSolverDN issue, however, trying to run model.fit() now gives the following error every time:\nResourceExhaustedError: OOM when allocating tensor with shape[10000,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc [Op:AssignVariableOp]\nThere's definitely something odd going on with TF Eager and memory allocation. Doing the set_random_seed(1) trick fixes the strange QR issue, but the model is for some reason then trying to fit the entire training set into memory, even though I have batch_size = 16 set, so it should not be doing that.\nFurthermore, if I just restart the entire kernel, everything works, but only for the first call to model.fit(). Any subsequent calls to model.fit() will hit all of the issues I noted above, first the cuSolverDN issue, then the OOM issue.", "body": "Ok, just wanted to check since I've only ever seen it done the other way.\r\n\r\nJust a quick thing though, I think there is another strange bug here happening somewhere. I've slipstreamed the changes from your commit into my local Tensorflow 1.10 installation on Windows. I was then able to successfully run, my code as written above, so from that perspective, job done, your fix seems to work.\r\n\r\nHowever, when I immediately re-ran the model.fit() again, I hit this error:\r\n\r\n```\r\nFile \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1903, in <lambda>\r\n    shape, dtype=dtype, partition_info=partition_info)\r\n\r\n  File \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py\", line 549, in __call__\r\n    q, r = gen_linalg_ops.qr(a, full_matrices=False)\r\n\r\n  File \"C:\\Unix\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py\", line 1518, in qr\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nInternalError: E:\\Github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:468: cuSolverDN call failed with status =7 [Op:Qr]\r\n```\r\n\r\nSo I tried again, and got:\r\n\r\n```\r\n...\r\nInternalError: E:\\Github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:468: cuSolverDN call failed with status =6 [Op:Qr]\r\n```\r\n\r\nI looked into both files, and saw that this might be happening from failed memory allocation calls, so I did a quick Google search which brought me to: https://github.com/tensorflow/tensorflow/issues/19671\r\n\r\nI tried the workaround from that, by calling \r\n\r\ntf.set_random_seed(1)\r\n\r\nAnd this fixed the cuSolverDN issue, however, trying to run model.fit() now gives the following error every time:\r\n\r\n`ResourceExhaustedError: OOM when allocating tensor with shape[10000,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc [Op:AssignVariableOp]`\r\n\r\nThere's definitely something odd going on with TF Eager and memory allocation. Doing the set_random_seed(1) trick fixes the strange QR issue, but the model is for some reason then trying to fit the entire training set into memory, even though I have batch_size = 16 set, so it should not be doing that.\r\n\r\nFurthermore, if I just restart the entire kernel, everything works, but only for the first call to model.fit(). Any subsequent calls to model.fit() will hit all of the issues I noted above, first the cuSolverDN issue, then the OOM issue."}