{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6499", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6499/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6499/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6499/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6499", "id": 197570823, "node_id": "MDU6SXNzdWUxOTc1NzA4MjM=", "number": 6499, "title": "how test a model made tensorflow and python following the code", "user": {"login": "abhishekmm", "id": 8172732, "node_id": "MDQ6VXNlcjgxNzI3MzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/8172732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhishekmm", "html_url": "https://github.com/abhishekmm", "followers_url": "https://api.github.com/users/abhishekmm/followers", "following_url": "https://api.github.com/users/abhishekmm/following{/other_user}", "gists_url": "https://api.github.com/users/abhishekmm/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhishekmm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhishekmm/subscriptions", "organizations_url": "https://api.github.com/users/abhishekmm/orgs", "repos_url": "https://api.github.com/users/abhishekmm/repos", "events_url": "https://api.github.com/users/abhishekmm/events{/privacy}", "received_events_url": "https://api.github.com/users/abhishekmm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-12-26T10:13:19Z", "updated_at": "2017-01-05T22:04:11Z", "closed_at": "2017-01-05T22:04:11Z", "author_association": "NONE", "body_html": "<pre><code>from __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nimport math as math\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('dataset')\nargs = parser.parse_args()\n\ndef file_len(fname):\n    with open(fname) as f:\n        for i, l in enumerate(f):\n            pass\n            #print(l)\n            #print(\"pass\")\n            #print(i)\n    return i + 1\n\ndef read_from_csv(filename_queue):\n  reader = tf.TextLineReader(skip_header_lines=1)\n  _, csv_row = reader.read(filename_queue)\n  record_defaults = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n  col0,col1, col2, col3, col4, col5,col6, col7, col8, col9, col10,col11, col12, col13, col14, col15,col16, col17, col18, col19, col20,col21, col22, col23, col24, col25,col26, col27, col28, col29, col30,col31, col32, col33, col34, col35,col36, col37, col38, col39, col40,col41, col42, col43, col44, col45,col46, col47, col48, col49, col50,col51, col52, col53, col54, col55,col56, colLabel = tf.decode_csv(csv_row, record_defaults=record_defaults)\n  features = tf.pack([col0,col1, col2, col3, col4, col5,col6, col7, col8, col9, col10,col11, col12, col13, col14, col15,col16, col17, col18, col19, col20,col21, col22, col23, col24, col25,col26, col27, col28, col29, col30,col31, col32, col33, col34, col35,col36, col37, col38, col39, col40,col41, col42, col43, col44, col45,col46, col47, col48, col49, col50,col51, col52, col53, col54, col55,col56])  \n  label = tf.pack([colLabel])  \n  return features, label\n\ndef input_pipeline(batch_size, num_epochs=1):\n  #filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)  \n  \n  [args.dataset]\n  filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=1, shuffle=True)  \n\n  example, label = read_from_csv(filename_queue)\n  min_after_dequeue = 4598\n  #capacity = min_after_dequeue + 3 * batch_size\n  capacity = 4599\n  example_batch, label_batch = tf.train.shuffle_batch(\n      [example, label], batch_size=batch_size, capacity=capacity,\n      min_after_dequeue=min_after_dequeue)\n  return example_batch, label_batch\n  \ndef input_pipeline_test(batch_size, num_epochs=1):\n  #filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)  \n  \n  \n  print(\"filename_queue_test\")\n  filename_queue_test = tf.train.string_input_producer([args.dataset], num_epochs=1, shuffle=True)  \n\n  example, label = read_from_csv(filename_queue_test)\n  print(\"filename_queue_test end\")\n  min_after_dequeue = 1379\n  #capacity = min_after_dequeue + 3 * batch_size\n  capacity = 1380\n  example_batch, label_batch = tf.train.shuffle_batch(\n      [example, label], batch_size=batch_size, capacity=capacity,\n      min_after_dequeue=min_after_dequeue)\n  print(example_batch)\n  return example_batch, label_batch\n#####################################################################################################################################\n#####################################################################################################################################\n#####################################################################################################################################\n# Parameters\nlearning_rate = 0.6\ntraining_epochs = 1\nbatch_size = 1000\ndisplay_step = 1\n\n# Network Parameters\nn_hidden_1 = 100# 1st layer number of features\nn_hidden_2 = 30# 2nd layer number of features\nn_input = 57 # MNIST data input (img shape: 28*28)\nn_classes = 1 # MNIST total classes (0-9 digits)\n\n\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n    layer_1 = tf.nn.relu(layer_1)\n    # Hidden layer with RELU activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n    layer_2 = tf.nn.relu(layer_2)\n    # Output layer with linear activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n    return out_layer\n\n# Store layers weight &amp; bias\nweights = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n\n\n\n\n\n#########################################################################################################################################\n#########################################################################################################################################\n#print(\"file_len(args.dataset) - 1\")\n#print(file_len(args.dataset) - 1)\nfile_length = file_len(args.dataset) - 1\n#file_length = 4599\nexamples, labels = input_pipeline(file_length, 1)\nexamples_batch=tf.global_variables()\nlabels_batch=tf.global_variables()\nexamples_batch_test=tf.global_variables()\nlabels_batch_test=tf.global_variables()\nx = tf.placeholder(\"float\", [None, 57])\ny = tf.placeholder(\"float\", [None, 1])\n\npred = multilayer_perceptron(x, weights, biases)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\nconfig=tf.ConfigProto(inter_op_parallelism_threads=2)\nwith tf.Session(config=config) as sess:\n  init_op = tf.group(tf.initialize_all_variables(),tf.initialize_local_variables())\n  sess.run(init_op)\n  #tf.initialize_all_variables().run()\n\n  # start populating filename queue\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord)\n\n  try:\n    while not coord.should_stop():\n      examples_batch, labels_batch = sess.run([examples, labels])\n    #x, y = sess.run([examples, labels])\n     \n  except tf.errors.OutOfRangeError:\n    print('Done training, epoch reached')\n    #print(examples_batch)\n    #print(labels_batch)\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.0\n        total_batch = 1\n        # Loop over all batches\n        for i in range(total_batch):\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={x: examples_batch,\n                                                          y: labels_batch})\n            print(\"c\")\n            print(c)\n            #c = {x: batch_x,y: batch_y}\n            # Compute average loss\n            #print(c / total_batch)\n            avg_cost = c / total_batch\n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n                \"{:.9f}\".format(avg_cost))\n    print(\"Optimization Finished!\")\n    print(\"TEST BLOCK!\")\n    try:\n        file_length_test=1379\n        examples_test, labels_test = input_pipeline_test(file_length_test, 1)\n        print(\"file print\")\n        print(sess.run(labels_test))\n        print(\"file print end\")\n    except:\n        print(\"tftftftftfttftftftft\")\n        \n    coord.request_stop()\n\n\n# with tf.Session() as sess_test:\n  # init_op = tf.group(tf.initialize_all_variables(),tf.initialize_local_variables())\n  # sess_test.run(init_op)\n  # #tf.initialize_all_variables().run()\n\n  # # start populating filename queue\n  # coord = tf.train.Coordinator()\n  # threads = tf.train.start_queue_runners(coord=coord)\n\n  # print(\"TEST BLOCK!\")\n  # file_length_test=1379\n  # examples_test, labels_test = input_pipeline_test(file_length_test, 1)\n  # print(examples_test)\n # # start populating filename queue\n  # coord = tf.train.Coordinator()\n  # threads = tf.train.start_queue_runners(coord=coord)\n  # #with tf.Session() as sess_test:\n  # try:\n       # while not coord.should_stop():\n           # examples_batch_test, labels_batch_test = sess_test.run([examples_test, labels_test])\n           # #x, y = sess_test.run([examples, labels])\n        \n  # except tf.errors.OutOfRangeError:\n       # # Test model\n       # correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n       # # Calculate accuracy\n       # accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n       # print(\"helooOOOOO\")\n       # print(\"Accuracy:\", accuracy.eval({x: examples_batch_test, y: labels_test}))\n    \n    \n    \n  # coord.request_stop()\n\ncoord.join(threads) \n\n#######################################################################################################################################\n\n</code></pre>\n<p>C:\\Users\\ABC\\Desktop\\DemoTensarflow&gt;cmd<br>\nMicrosoft Windows [Version 6.1.7601]<br>\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.</p>\n<p>C:\\Users\\ABC\\Desktop\\DemoTensarflow&gt;python ReadTF_mod.py train.csv<br>\nWARNING:tensorflow:From ReadTF_mod.py:124 in .: initialize_all_variables<br>\n(from tensorflow.python.ops.variables) is deprecated and will be removed after<br>\n2017-03-02.<br>\nInstructions for updating:<br>\nUse <code>tf.global_variables_initializer</code> instead.<br>\nWARNING:tensorflow:From ReadTF_mod.py:124 in .: initialize_local_variabl<br>\nes (from tensorflow.python.ops.variables) is deprecated and will be removed afte<br>\nr 2017-03-02.<br>\nInstructions for updating:<br>\nUse <code>tf.local_variables_initializer</code> instead.<br>\nDone training, epoch reached<br>\nc<br>\n0.0<br>\nEpoch: 0001 cost= 0.000000000<br>\nOptimization Finished!<br>\nTEST BLOCK!<br>\nfilename_queue_test<br>\nfilename_queue_test end<br>\nTensor(\"shuffle_batch_1:0\", shape=(1379, 57), dtype=float32)<br>\nfile print<br>\n^C</p>\n<p>the program gets stuck</p>", "body_text": "from __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nimport math as math\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('dataset')\nargs = parser.parse_args()\n\ndef file_len(fname):\n    with open(fname) as f:\n        for i, l in enumerate(f):\n            pass\n            #print(l)\n            #print(\"pass\")\n            #print(i)\n    return i + 1\n\ndef read_from_csv(filename_queue):\n  reader = tf.TextLineReader(skip_header_lines=1)\n  _, csv_row = reader.read(filename_queue)\n  record_defaults = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n  col0,col1, col2, col3, col4, col5,col6, col7, col8, col9, col10,col11, col12, col13, col14, col15,col16, col17, col18, col19, col20,col21, col22, col23, col24, col25,col26, col27, col28, col29, col30,col31, col32, col33, col34, col35,col36, col37, col38, col39, col40,col41, col42, col43, col44, col45,col46, col47, col48, col49, col50,col51, col52, col53, col54, col55,col56, colLabel = tf.decode_csv(csv_row, record_defaults=record_defaults)\n  features = tf.pack([col0,col1, col2, col3, col4, col5,col6, col7, col8, col9, col10,col11, col12, col13, col14, col15,col16, col17, col18, col19, col20,col21, col22, col23, col24, col25,col26, col27, col28, col29, col30,col31, col32, col33, col34, col35,col36, col37, col38, col39, col40,col41, col42, col43, col44, col45,col46, col47, col48, col49, col50,col51, col52, col53, col54, col55,col56])  \n  label = tf.pack([colLabel])  \n  return features, label\n\ndef input_pipeline(batch_size, num_epochs=1):\n  #filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)  \n  \n  [args.dataset]\n  filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=1, shuffle=True)  \n\n  example, label = read_from_csv(filename_queue)\n  min_after_dequeue = 4598\n  #capacity = min_after_dequeue + 3 * batch_size\n  capacity = 4599\n  example_batch, label_batch = tf.train.shuffle_batch(\n      [example, label], batch_size=batch_size, capacity=capacity,\n      min_after_dequeue=min_after_dequeue)\n  return example_batch, label_batch\n  \ndef input_pipeline_test(batch_size, num_epochs=1):\n  #filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)  \n  \n  \n  print(\"filename_queue_test\")\n  filename_queue_test = tf.train.string_input_producer([args.dataset], num_epochs=1, shuffle=True)  \n\n  example, label = read_from_csv(filename_queue_test)\n  print(\"filename_queue_test end\")\n  min_after_dequeue = 1379\n  #capacity = min_after_dequeue + 3 * batch_size\n  capacity = 1380\n  example_batch, label_batch = tf.train.shuffle_batch(\n      [example, label], batch_size=batch_size, capacity=capacity,\n      min_after_dequeue=min_after_dequeue)\n  print(example_batch)\n  return example_batch, label_batch\n#####################################################################################################################################\n#####################################################################################################################################\n#####################################################################################################################################\n# Parameters\nlearning_rate = 0.6\ntraining_epochs = 1\nbatch_size = 1000\ndisplay_step = 1\n\n# Network Parameters\nn_hidden_1 = 100# 1st layer number of features\nn_hidden_2 = 30# 2nd layer number of features\nn_input = 57 # MNIST data input (img shape: 28*28)\nn_classes = 1 # MNIST total classes (0-9 digits)\n\n\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Hidden layer with RELU activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n    layer_1 = tf.nn.relu(layer_1)\n    # Hidden layer with RELU activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n    layer_2 = tf.nn.relu(layer_2)\n    # Output layer with linear activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n    return out_layer\n\n# Store layers weight & bias\nweights = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n}\nbiases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n\n\n\n\n\n#########################################################################################################################################\n#########################################################################################################################################\n#print(\"file_len(args.dataset) - 1\")\n#print(file_len(args.dataset) - 1)\nfile_length = file_len(args.dataset) - 1\n#file_length = 4599\nexamples, labels = input_pipeline(file_length, 1)\nexamples_batch=tf.global_variables()\nlabels_batch=tf.global_variables()\nexamples_batch_test=tf.global_variables()\nlabels_batch_test=tf.global_variables()\nx = tf.placeholder(\"float\", [None, 57])\ny = tf.placeholder(\"float\", [None, 1])\n\npred = multilayer_perceptron(x, weights, biases)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\nconfig=tf.ConfigProto(inter_op_parallelism_threads=2)\nwith tf.Session(config=config) as sess:\n  init_op = tf.group(tf.initialize_all_variables(),tf.initialize_local_variables())\n  sess.run(init_op)\n  #tf.initialize_all_variables().run()\n\n  # start populating filename queue\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord)\n\n  try:\n    while not coord.should_stop():\n      examples_batch, labels_batch = sess.run([examples, labels])\n    #x, y = sess.run([examples, labels])\n     \n  except tf.errors.OutOfRangeError:\n    print('Done training, epoch reached')\n    #print(examples_batch)\n    #print(labels_batch)\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.0\n        total_batch = 1\n        # Loop over all batches\n        for i in range(total_batch):\n            # Run optimization op (backprop) and cost op (to get loss value)\n            _, c = sess.run([optimizer, cost], feed_dict={x: examples_batch,\n                                                          y: labels_batch})\n            print(\"c\")\n            print(c)\n            #c = {x: batch_x,y: batch_y}\n            # Compute average loss\n            #print(c / total_batch)\n            avg_cost = c / total_batch\n        # Display logs per epoch step\n        if epoch % display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n                \"{:.9f}\".format(avg_cost))\n    print(\"Optimization Finished!\")\n    print(\"TEST BLOCK!\")\n    try:\n        file_length_test=1379\n        examples_test, labels_test = input_pipeline_test(file_length_test, 1)\n        print(\"file print\")\n        print(sess.run(labels_test))\n        print(\"file print end\")\n    except:\n        print(\"tftftftftfttftftftft\")\n        \n    coord.request_stop()\n\n\n# with tf.Session() as sess_test:\n  # init_op = tf.group(tf.initialize_all_variables(),tf.initialize_local_variables())\n  # sess_test.run(init_op)\n  # #tf.initialize_all_variables().run()\n\n  # # start populating filename queue\n  # coord = tf.train.Coordinator()\n  # threads = tf.train.start_queue_runners(coord=coord)\n\n  # print(\"TEST BLOCK!\")\n  # file_length_test=1379\n  # examples_test, labels_test = input_pipeline_test(file_length_test, 1)\n  # print(examples_test)\n # # start populating filename queue\n  # coord = tf.train.Coordinator()\n  # threads = tf.train.start_queue_runners(coord=coord)\n  # #with tf.Session() as sess_test:\n  # try:\n       # while not coord.should_stop():\n           # examples_batch_test, labels_batch_test = sess_test.run([examples_test, labels_test])\n           # #x, y = sess_test.run([examples, labels])\n        \n  # except tf.errors.OutOfRangeError:\n       # # Test model\n       # correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n       # # Calculate accuracy\n       # accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n       # print(\"helooOOOOO\")\n       # print(\"Accuracy:\", accuracy.eval({x: examples_batch_test, y: labels_test}))\n    \n    \n    \n  # coord.request_stop()\n\ncoord.join(threads) \n\n#######################################################################################################################################\n\n\nC:\\Users\\ABC\\Desktop\\DemoTensarflow>cmd\nMicrosoft Windows [Version 6.1.7601]\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\nC:\\Users\\ABC\\Desktop\\DemoTensarflow>python ReadTF_mod.py train.csv\nWARNING:tensorflow:From ReadTF_mod.py:124 in .: initialize_all_variables\n(from tensorflow.python.ops.variables) is deprecated and will be removed after\n2017-03-02.\nInstructions for updating:\nUse tf.global_variables_initializer instead.\nWARNING:tensorflow:From ReadTF_mod.py:124 in .: initialize_local_variabl\nes (from tensorflow.python.ops.variables) is deprecated and will be removed afte\nr 2017-03-02.\nInstructions for updating:\nUse tf.local_variables_initializer instead.\nDone training, epoch reached\nc\n0.0\nEpoch: 0001 cost= 0.000000000\nOptimization Finished!\nTEST BLOCK!\nfilename_queue_test\nfilename_queue_test end\nTensor(\"shuffle_batch_1:0\", shape=(1379, 57), dtype=float32)\nfile print\n^C\nthe program gets stuck", "body": "```\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport math as math\r\nimport argparse\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('dataset')\r\nargs = parser.parse_args()\r\n\r\ndef file_len(fname):\r\n    with open(fname) as f:\r\n        for i, l in enumerate(f):\r\n            pass\r\n            #print(l)\r\n            #print(\"pass\")\r\n            #print(i)\r\n    return i + 1\r\n\r\ndef read_from_csv(filename_queue):\r\n  reader = tf.TextLineReader(skip_header_lines=1)\r\n  _, csv_row = reader.read(filename_queue)\r\n  record_defaults = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\r\n  col0,col1, col2, col3, col4, col5,col6, col7, col8, col9, col10,col11, col12, col13, col14, col15,col16, col17, col18, col19, col20,col21, col22, col23, col24, col25,col26, col27, col28, col29, col30,col31, col32, col33, col34, col35,col36, col37, col38, col39, col40,col41, col42, col43, col44, col45,col46, col47, col48, col49, col50,col51, col52, col53, col54, col55,col56, colLabel = tf.decode_csv(csv_row, record_defaults=record_defaults)\r\n  features = tf.pack([col0,col1, col2, col3, col4, col5,col6, col7, col8, col9, col10,col11, col12, col13, col14, col15,col16, col17, col18, col19, col20,col21, col22, col23, col24, col25,col26, col27, col28, col29, col30,col31, col32, col33, col34, col35,col36, col37, col38, col39, col40,col41, col42, col43, col44, col45,col46, col47, col48, col49, col50,col51, col52, col53, col54, col55,col56])  \r\n  label = tf.pack([colLabel])  \r\n  return features, label\r\n\r\ndef input_pipeline(batch_size, num_epochs=1):\r\n  #filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)  \r\n  \r\n  [args.dataset]\r\n  filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=1, shuffle=True)  \r\n\r\n  example, label = read_from_csv(filename_queue)\r\n  min_after_dequeue = 4598\r\n  #capacity = min_after_dequeue + 3 * batch_size\r\n  capacity = 4599\r\n  example_batch, label_batch = tf.train.shuffle_batch(\r\n      [example, label], batch_size=batch_size, capacity=capacity,\r\n      min_after_dequeue=min_after_dequeue)\r\n  return example_batch, label_batch\r\n  \r\ndef input_pipeline_test(batch_size, num_epochs=1):\r\n  #filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)  \r\n  \r\n  \r\n  print(\"filename_queue_test\")\r\n  filename_queue_test = tf.train.string_input_producer([args.dataset], num_epochs=1, shuffle=True)  \r\n\r\n  example, label = read_from_csv(filename_queue_test)\r\n  print(\"filename_queue_test end\")\r\n  min_after_dequeue = 1379\r\n  #capacity = min_after_dequeue + 3 * batch_size\r\n  capacity = 1380\r\n  example_batch, label_batch = tf.train.shuffle_batch(\r\n      [example, label], batch_size=batch_size, capacity=capacity,\r\n      min_after_dequeue=min_after_dequeue)\r\n  print(example_batch)\r\n  return example_batch, label_batch\r\n#####################################################################################################################################\r\n#####################################################################################################################################\r\n#####################################################################################################################################\r\n# Parameters\r\nlearning_rate = 0.6\r\ntraining_epochs = 1\r\nbatch_size = 1000\r\ndisplay_step = 1\r\n\r\n# Network Parameters\r\nn_hidden_1 = 100# 1st layer number of features\r\nn_hidden_2 = 30# 2nd layer number of features\r\nn_input = 57 # MNIST data input (img shape: 28*28)\r\nn_classes = 1 # MNIST total classes (0-9 digits)\r\n\r\n\r\n# Create model\r\ndef multilayer_perceptron(x, weights, biases):\r\n    # Hidden layer with RELU activation\r\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\r\n    layer_1 = tf.nn.relu(layer_1)\r\n    # Hidden layer with RELU activation\r\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\r\n    layer_2 = tf.nn.relu(layer_2)\r\n    # Output layer with linear activation\r\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\r\n    return out_layer\r\n\r\n# Store layers weight & bias\r\nweights = {\r\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\r\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\r\n    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\r\n}\r\nbiases = {\r\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\r\n    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\r\n    'out': tf.Variable(tf.random_normal([n_classes]))\r\n}\r\n\r\n\r\n\r\n\r\n\r\n#########################################################################################################################################\r\n#########################################################################################################################################\r\n#print(\"file_len(args.dataset) - 1\")\r\n#print(file_len(args.dataset) - 1)\r\nfile_length = file_len(args.dataset) - 1\r\n#file_length = 4599\r\nexamples, labels = input_pipeline(file_length, 1)\r\nexamples_batch=tf.global_variables()\r\nlabels_batch=tf.global_variables()\r\nexamples_batch_test=tf.global_variables()\r\nlabels_batch_test=tf.global_variables()\r\nx = tf.placeholder(\"float\", [None, 57])\r\ny = tf.placeholder(\"float\", [None, 1])\r\n\r\npred = multilayer_perceptron(x, weights, biases)\r\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\nconfig=tf.ConfigProto(inter_op_parallelism_threads=2)\r\nwith tf.Session(config=config) as sess:\r\n  init_op = tf.group(tf.initialize_all_variables(),tf.initialize_local_variables())\r\n  sess.run(init_op)\r\n  #tf.initialize_all_variables().run()\r\n\r\n  # start populating filename queue\r\n  coord = tf.train.Coordinator()\r\n  threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n  try:\r\n    while not coord.should_stop():\r\n      examples_batch, labels_batch = sess.run([examples, labels])\r\n    #x, y = sess.run([examples, labels])\r\n     \r\n  except tf.errors.OutOfRangeError:\r\n    print('Done training, epoch reached')\r\n    #print(examples_batch)\r\n    #print(labels_batch)\r\n\r\n    # Training cycle\r\n    for epoch in range(training_epochs):\r\n        avg_cost = 0.0\r\n        total_batch = 1\r\n        # Loop over all batches\r\n        for i in range(total_batch):\r\n            # Run optimization op (backprop) and cost op (to get loss value)\r\n            _, c = sess.run([optimizer, cost], feed_dict={x: examples_batch,\r\n                                                          y: labels_batch})\r\n            print(\"c\")\r\n            print(c)\r\n            #c = {x: batch_x,y: batch_y}\r\n            # Compute average loss\r\n            #print(c / total_batch)\r\n            avg_cost = c / total_batch\r\n        # Display logs per epoch step\r\n        if epoch % display_step == 0:\r\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\r\n                \"{:.9f}\".format(avg_cost))\r\n    print(\"Optimization Finished!\")\r\n    print(\"TEST BLOCK!\")\r\n    try:\r\n        file_length_test=1379\r\n        examples_test, labels_test = input_pipeline_test(file_length_test, 1)\r\n        print(\"file print\")\r\n        print(sess.run(labels_test))\r\n        print(\"file print end\")\r\n    except:\r\n        print(\"tftftftftfttftftftft\")\r\n        \r\n    coord.request_stop()\r\n\r\n\r\n# with tf.Session() as sess_test:\r\n  # init_op = tf.group(tf.initialize_all_variables(),tf.initialize_local_variables())\r\n  # sess_test.run(init_op)\r\n  # #tf.initialize_all_variables().run()\r\n\r\n  # # start populating filename queue\r\n  # coord = tf.train.Coordinator()\r\n  # threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n  # print(\"TEST BLOCK!\")\r\n  # file_length_test=1379\r\n  # examples_test, labels_test = input_pipeline_test(file_length_test, 1)\r\n  # print(examples_test)\r\n # # start populating filename queue\r\n  # coord = tf.train.Coordinator()\r\n  # threads = tf.train.start_queue_runners(coord=coord)\r\n  # #with tf.Session() as sess_test:\r\n  # try:\r\n       # while not coord.should_stop():\r\n           # examples_batch_test, labels_batch_test = sess_test.run([examples_test, labels_test])\r\n           # #x, y = sess_test.run([examples, labels])\r\n        \r\n  # except tf.errors.OutOfRangeError:\r\n       # # Test model\r\n       # correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\r\n       # # Calculate accuracy\r\n       # accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n       # print(\"helooOOOOO\")\r\n       # print(\"Accuracy:\", accuracy.eval({x: examples_batch_test, y: labels_test}))\r\n    \r\n    \r\n    \r\n  # coord.request_stop()\r\n\r\ncoord.join(threads) \r\n\r\n#######################################################################################################################################\r\n\r\n```\r\n\r\nC:\\Users\\ABC\\Desktop\\DemoTensarflow>cmd\r\nMicrosoft Windows [Version 6.1.7601]\r\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\r\n\r\nC:\\Users\\ABC\\Desktop\\DemoTensarflow>python ReadTF_mod.py train.csv\r\nWARNING:tensorflow:From ReadTF_mod.py:124 in <module>.: initialize_all_variables\r\n (from tensorflow.python.ops.variables) is deprecated and will be removed after\r\n2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\nWARNING:tensorflow:From ReadTF_mod.py:124 in <module>.: initialize_local_variabl\r\nes (from tensorflow.python.ops.variables) is deprecated and will be removed afte\r\nr 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.local_variables_initializer` instead.\r\nDone training, epoch reached\r\nc\r\n0.0\r\nEpoch: 0001 cost= 0.000000000\r\nOptimization Finished!\r\nTEST BLOCK!\r\nfilename_queue_test\r\nfilename_queue_test end\r\nTensor(\"shuffle_batch_1:0\", shape=(1379, 57), dtype=float32)\r\nfile print\r\n^C\r\n\r\nthe program gets stuck\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}