{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/143855753", "pull_request_review_id": 68430082, "id": 143855753, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0Mzg1NTc1Mw==", "diff_hunk": "@@ -2305,3 +2354,268 @@ def call(self, inputs, state):\n \n     new_state = rnn_cell_impl.LSTMStateTuple(c, m)\n     return m, new_state\n+\n+\n+class LayerNormLSTMCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Long short-term memory unit (LSTM) recurrent network cell.\n+\n+  The default non-peephole implementation is based on:\n+\n+    http://www.bioinf.jku.at/publications/older/2604.pdf\n+\n+  S. Hochreiter and J. Schmidhuber.\n+  \"Long Short-Term Memory\". Neural Computation, 9(8):1735-1780, 1997.\n+\n+  The peephole implementation is based on:\n+\n+    https://research.google.com/pubs/archive/43905.pdf\n+\n+  Hasim Sak, Andrew Senior, and Francoise Beaufays.\n+  \"Long short-term memory recurrent neural network architectures for\n+   large scale acoustic modeling.\" INTERSPEECH, 2014.\n+\n+  The class uses optional peep-hole connections, optional cell clipping, and\n+  an optional projection layer.\n+\n+  Layer normalization implementation is based on:\n+\n+    https://arxiv.org/abs/1607.06450.\n+\n+  \"Layer Normalization\"\n+  Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n+\n+  and is applied before the internal nonlinearities.\n+\n+  \"\"\"\n+\n+  def __init__(self, num_units,\n+               use_peepholes=False, cell_clip=None,\n+               initializer=None, num_proj=None, proj_clip=None,\n+               forget_bias=1.0,\n+               activation=None, layer_norm=False,\n+               norm_gain=1.0, norm_shift=0.0, reuse=None):\n+    \"\"\"Initialize the parameters for an LSTM cell.\n+\n+    Args:\n+      num_units: int, The number of units in the LSTM cell\n+      use_peepholes: bool, set True to enable diagonal/peephole connections.\n+      cell_clip: (optional) A float value, if provided the cell state is clipped\n+        by this value prior to the cell output activation.\n+      initializer: (optional) The initializer to use for the weight and\n+        projection matrices.\n+      num_proj: (optional) int, The output dimensionality for the projection\n+        matrices.  If None, no projection is performed.\n+      proj_clip: (optional) A float value.  If `num_proj > 0` and `proj_clip` is\n+        provided, then the projected values are clipped elementwise to within\n+        `[-proj_clip, proj_clip]`.\n+      forget_bias: Biases of the forget gate are initialized by default to 1\n+        in order to reduce the scale of forgetting at the beginning of\n+        the training. Must set it manually to `0.0` when restoring from\n+        CudnnLSTM trained checkpoints.\n+      activation: Activation function of the inner states.  Default: `tanh`.\n+      layer_norm: If `True`, layer normalization will be applied.\n+      norm_gain: float, The layer normalization gain initial value. If\n+        `layer_norm` has been set to `False`, this argument will be ignored.\n+      norm_shift: float, The layer normalization shift initial value. If\n+        `layer_norm` has been set to `False`, this argument will be ignored.\n+      reuse: (optional) Python boolean describing whether to reuse variables\n+        in an existing scope.  If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n+\n+      When restoring from CudnnLSTM-trained checkpoints, must use\n+      CudnnCompatibleLSTMCell instead.\n+    \"\"\"\n+    super(LayerNormLSTMCell, self).__init__(_reuse=reuse)\n+\n+    self._num_units = num_units\n+    self._use_peepholes = use_peepholes\n+    self._cell_clip = cell_clip\n+    self._initializer = initializer\n+    self._num_proj = num_proj\n+    self._proj_clip = proj_clip\n+    self._forget_bias = forget_bias\n+    self._activation = activation or math_ops.tanh\n+    self._layer_norm = layer_norm\n+    self._norm_gain = norm_gain\n+    self._norm_shift = norm_shift\n+\n+    if num_proj:\n+      self._state_size = (\n+        rnn_cell_impl.LSTMStateTuple(num_units, num_proj)\n+        )", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 224, "commit_id": "b435d2ceac8dc9077f3f2f35842d1929c889acd3", "original_commit_id": "c8689dcec7efc31662207c805ed30a625d261729", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "nit: this can all be one line, no?  same below?  within 80 chars?", "created_at": "2017-10-10T21:20:41Z", "updated_at": "2017-10-29T10:04:52Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9839#discussion_r143855753", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9839", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/143855753"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9839#discussion_r143855753"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9839"}}, "body_html": "<p>nit: this can all be one line, no?  same below?  within 80 chars?</p>", "body_text": "nit: this can all be one line, no?  same below?  within 80 chars?"}