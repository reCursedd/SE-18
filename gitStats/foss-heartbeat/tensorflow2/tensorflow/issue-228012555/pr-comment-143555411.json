{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/143555411", "pull_request_review_id": 68083244, "id": 143555411, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MzU1NTQxMQ==", "diff_hunk": "@@ -2271,3 +2320,302 @@ def call(self, inputs, state):\n \n     new_state = rnn_cell_impl.LSTMStateTuple(c, m)\n     return m, new_state\n+\n+\n+class LayerNormLSTMCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Long short-term memory unit (LSTM) recurrent network cell.\n+\n+  The default non-peephole implementation is based on:\n+\n+    http://www.bioinf.jku.at/publications/older/2604.pdf\n+\n+  S. Hochreiter and J. Schmidhuber.\n+  \"Long Short-Term Memory\". Neural Computation, 9(8):1735-1780, 1997.\n+\n+  The peephole implementation is based on:\n+\n+    https://research.google.com/pubs/archive/43905.pdf\n+\n+  Hasim Sak, Andrew Senior, and Francoise Beaufays.\n+  \"Long short-term memory recurrent neural network architectures for\n+   large scale acoustic modeling.\" INTERSPEECH, 2014.\n+\n+  The class uses optional peep-hole connections, optional cell clipping, and\n+  an optional projection layer.\n+\n+  Layer normalization implementation is based on:\n+\n+    https://arxiv.org/abs/1607.06450.\n+\n+  \"Layer Normalization\"\n+  Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n+\n+  and is applied before the internal nonlinearities.\n+\n+  \"\"\"\n+\n+  def __init__(self, num_units,\n+               use_peepholes=False, cell_clip=None,\n+               initializer=None, num_proj=None, proj_clip=None,\n+               num_unit_shards=None, num_proj_shards=None,\n+               forget_bias=1.0, state_is_tuple=True,\n+               activation=None, layer_norm=False,\n+               norm_gain=1.0, norm_shift=0.0, reuse=None):\n+    \"\"\"Initialize the parameters for an LSTM cell.\n+\n+    Args:\n+      num_units: int, The number of units in the LSTM cell\n+      use_peepholes: bool, set True to enable diagonal/peephole connections.\n+      cell_clip: (optional) A float value, if provided the cell state is clipped\n+        by this value prior to the cell output activation.\n+      initializer: (optional) The initializer to use for the weight and\n+        projection matrices.\n+      num_proj: (optional) int, The output dimensionality for the projection\n+        matrices.  If None, no projection is performed.\n+      proj_clip: (optional) A float value.  If `num_proj > 0` and `proj_clip` is\n+        provided, then the projected values are clipped elementwise to within\n+        `[-proj_clip, proj_clip]`.\n+      num_unit_shards: Deprecated, will be removed by Jan. 2017.\n+        Use a variable_scope partitioner instead.\n+      num_proj_shards: Deprecated, will be removed by Jan. 2017.\n+        Use a variable_scope partitioner instead.\n+      forget_bias: Biases of the forget gate are initialized by default to 1\n+        in order to reduce the scale of forgetting at the beginning of\n+        the training. Must set it manually to `0.0` when restoring from\n+        CudnnLSTM trained checkpoints.\n+      state_is_tuple: If True, accepted and returned states are 2-tuples of\n+        the `c_state` and `m_state`.  If False, they are concatenated\n+        along the column axis.  This latter behavior will soon be deprecated.\n+      activation: Activation function of the inner states.  Default: `tanh`.\n+      layer_norm: If `True`, layer normalization will be applied.\n+      norm_gain: float, The layer normalization gain initial value. If\n+        `layer_norm` has been set to `False`, this argument will be ignored.\n+      norm_shift: float, The layer normalization shift initial value. If\n+        `layer_norm` has been set to `False`, this argument will be ignored.\n+      reuse: (optional) Python boolean describing whether to reuse variables\n+        in an existing scope.  If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n+\n+      When restoring from CudnnLSTM-trained checkpoints, must use\n+      CudnnCompatibleLSTMCell instead.\n+    \"\"\"\n+    super(LayerNormLSTMCell, self).__init__(_reuse=reuse)\n+    if not state_is_tuple:\n+      logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n+                   \"deprecated.  Use state_is_tuple=True.\", self)\n+    if num_unit_shards is not None or num_proj_shards is not None:\n+      logging.warn(\n+        \"%s: The num_unit_shards and proj_unit_shards parameters are \"\n+        \"deprecated and will be removed in Jan 2017.  \"\n+        \"Use a variable scope with a partitioner instead.\", self)\n+\n+    self._num_units = num_units\n+    self._use_peepholes = use_peepholes\n+    self._cell_clip = cell_clip\n+    self._initializer = initializer\n+    self._num_proj = num_proj\n+    self._proj_clip = proj_clip\n+    self._num_unit_shards = num_unit_shards\n+    self._num_proj_shards = num_proj_shards\n+    self._forget_bias = forget_bias\n+    self._state_is_tuple = state_is_tuple\n+    self._activation = activation or math_ops.tanh\n+    self._layer_norm = layer_norm\n+    self._norm_gain = norm_gain\n+    self._norm_shift = norm_shift\n+\n+    if num_proj:\n+      self._state_size = (\n+        rnn_cell_impl.LSTMStateTuple(num_units, num_proj)\n+        if state_is_tuple else num_units + num_proj)\n+      self._output_size = num_proj\n+    else:\n+      self._state_size = (\n+        rnn_cell_impl.LSTMStateTuple(num_units, num_units)\n+        if state_is_tuple else 2 * num_units)\n+      self._output_size = num_units\n+\n+  @property\n+  def state_size(self):\n+    return self._state_size\n+\n+  @property\n+  def output_size(self):\n+    return self._output_size\n+\n+\n+  def _linear(self,\n+              args,\n+              output_size,\n+              bias,\n+              bias_initializer=None,\n+              kernel_initializer=None,\n+              layer_norm=False):\n+    \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n+\n+    Args:\n+      args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n+      output_size: int, second dimension of W[i].\n+      bias: boolean, whether to add a bias term or not.\n+      bias_initializer: starting value to initialize the bias\n+        (default is all zeros).\n+      kernel_initializer: starting value to initialize the weight.\n+      layer_norm: boolean, whether to apply layer normalization.\n+\n+\n+    Returns:\n+      A 2D Tensor with shape [batch x output_size] equal to\n+      sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n+\n+    Raises:\n+      ValueError: if some of the arguments has unspecified or wrong shape.\n+    \"\"\"\n+    if args is None or (nest.is_sequence(args) and not args):\n+      raise ValueError(\"`args` must be specified\")\n+    if not nest.is_sequence(args):\n+      args = [args]\n+\n+    # Calculate the total size of arguments on dimension 1.\n+    total_arg_size = 0\n+    shapes = [a.get_shape() for a in args]\n+    for shape in shapes:\n+      if shape.ndims != 2:\n+        raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n+      if shape[1].value is None:\n+        raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n+                         \"but saw %s\" % (shape, shape[1]))\n+      else:\n+        total_arg_size += shape[1].value\n+\n+    dtype = [a.dtype for a in args][0]\n+\n+    # Now the computation.\n+    scope = vs.get_variable_scope()\n+    with vs.variable_scope(scope) as outer_scope:\n+      weights = vs.get_variable(\n+        \"kernel\", [total_arg_size, output_size],\n+        dtype=dtype,\n+        initializer=kernel_initializer)\n+      if len(args) == 1:\n+        res = math_ops.matmul(args[0], weights)\n+      else:\n+        res = math_ops.matmul(array_ops.concat(args, 1), weights)\n+      if not bias:\n+        return res\n+      with vs.variable_scope(outer_scope) as inner_scope:\n+        inner_scope.set_partitioner(None)\n+        if bias_initializer is None:\n+          bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n+        biases = vs.get_variable(\n+          \"bias\", [output_size],\n+          dtype=dtype,\n+          initializer=bias_initializer)\n+\n+    if not layer_norm:\n+      res = nn_ops.bias_add(res, biases)\n+\n+    return res\n+\n+  def call(self, inputs, state):\n+    \"\"\"Run one step of LSTM.\n+\n+    Args:\n+      inputs: input Tensor, 2D, batch x num_units.\n+      state: if `state_is_tuple` is False, this must be a state Tensor,\n+        `2-D, batch x state_size`.  If `state_is_tuple` is True, this must be a\n+        tuple of state Tensors, both `2-D`, with column sizes `c_state` and\n+        `m_state`.\n+\n+    Returns:\n+      A tuple containing:\n+\n+      - A `2-D, [batch x output_dim]`, Tensor representing the output of the\n+        LSTM after reading `inputs` when previous state was `state`.\n+        Here output_dim is:\n+           num_proj if num_proj was set,\n+           num_units otherwise.\n+      - Tensor(s) representing the new state of LSTM after reading `inputs` when\n+        the previous state was `state`.  Same type and shape(s) as `state`.\n+\n+    Raises:\n+      ValueError: If input size cannot be inferred from inputs via\n+        static shape inference.\n+    \"\"\"\n+    num_proj = self._num_units if self._num_proj is None else self._num_proj\n+    sigmoid = math_ops.sigmoid\n+\n+    if self._state_is_tuple:\n+      (c_prev, m_prev) = state\n+    else:\n+      c_prev = array_ops.slice(state, [0, 0], [-1, self._num_units])\n+      m_prev = array_ops.slice(state, [0, self._num_units], [-1, num_proj])\n+\n+    dtype = inputs.dtype\n+    input_size = inputs.get_shape().with_rank(2)[1]\n+    if input_size.value is None:\n+      raise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\")\n+    scope = vs.get_variable_scope()\n+    with vs.variable_scope(scope, initializer=self._initializer) as unit_scope:\n+      if self._num_unit_shards is not None:\n+        unit_scope.set_partitioner(\n+          partitioned_variables.fixed_size_partitioner(\n+            self._num_unit_shards))\n+      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n+      lstm_matrix = self._linear([inputs, m_prev], 4 * self._num_units, bias=True,\n+                            bias_initializer=None, layer_norm=self._layer_norm)\n+      i, j, f, o = array_ops.split(\n+        value=lstm_matrix, num_or_size_splits=4, axis=1)\n+\n+      if self._layer_norm:\n+        i = _norm(self._norm_gain, self._norm_shift, i, \"input\")\n+        j = _norm(self._norm_gain, self._norm_shift, j, \"transform\")\n+        f = _norm(self._norm_gain, self._norm_shift, f, \"forget\")\n+        o = _norm(self._norm_gain, self._norm_shift, o, \"output\")\n+\n+      # Diagonal connections\n+      if self._use_peepholes:\n+        with vs.variable_scope(unit_scope) as projection_scope:\n+          if self._num_unit_shards is not None:\n+            projection_scope.set_partitioner(None)\n+          w_f_diag = vs.get_variable(\n+            \"w_f_diag\", shape=[self._num_units], dtype=dtype)\n+          w_i_diag = vs.get_variable(\n+            \"w_i_diag\", shape=[self._num_units], dtype=dtype)\n+          w_o_diag = vs.get_variable(\n+            \"w_o_diag\", shape=[self._num_units], dtype=dtype)\n+\n+      if self._use_peepholes:\n+        c = (sigmoid(f + self._forget_bias + w_f_diag * c_prev) * c_prev +\n+             sigmoid(i + w_i_diag * c_prev) * self._activation(j))\n+      else:\n+        c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *\n+             self._activation(j))\n+\n+      if self._layer_norm:\n+        c = _norm(self._norm_gain, self._norm_shift, c, \"state\")\n+\n+      if self._cell_clip is not None:\n+        # pylint: disable=invalid-unary-operand-type\n+        c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip)\n+        # pylint: enable=invalid-unary-operand-type\n+      if self._use_peepholes:\n+        m = sigmoid(o + w_o_diag * c) * self._activation(c)\n+      else:\n+        m = sigmoid(o) * self._activation(c)\n+\n+      if self._num_proj is not None:\n+        with vs.variable_scope(\"projection\") as proj_scope:\n+          if self._num_proj_shards is not None:\n+            proj_scope.set_partitioner(\n+              partitioned_variables.fixed_size_partitioner(\n+                self._num_proj_shards))\n+          m = self._linear(m, self._num_proj, bias=False)\n+\n+        if self._proj_clip is not None:\n+          # pylint: disable=invalid-unary-operand-type\n+          m = clip_ops.clip_by_value(m, -self._proj_clip, self._proj_clip)\n+          # pylint: enable=invalid-unary-operand-type\n+\n+    new_state = (rnn_cell_impl.LSTMStateTuple(c, m) if self._state_is_tuple else\n+                 array_ops.concat([c, m], 1))\n+    return m, new_state", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 434, "commit_id": "b435d2ceac8dc9077f3f2f35842d1929c889acd3", "original_commit_id": "a177c52f142bad9dfc25bdfd5e439e14bb5fa041", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "nit: newline after this.", "created_at": "2017-10-09T19:21:19Z", "updated_at": "2017-10-29T10:04:52Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9839#discussion_r143555411", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9839", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/143555411"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9839#discussion_r143555411"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9839"}}, "body_html": "<p>nit: newline after this.</p>", "body_text": "nit: newline after this."}