{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/441174532", "html_url": "https://github.com/tensorflow/tensorflow/issues/14357#issuecomment-441174532", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14357", "id": 441174532, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MTE3NDUzMg==", "user": {"login": "jchia", "id": 446676, "node_id": "MDQ6VXNlcjQ0NjY3Ng==", "avatar_url": "https://avatars0.githubusercontent.com/u/446676?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jchia", "html_url": "https://github.com/jchia", "followers_url": "https://api.github.com/users/jchia/followers", "following_url": "https://api.github.com/users/jchia/following{/other_user}", "gists_url": "https://api.github.com/users/jchia/gists{/gist_id}", "starred_url": "https://api.github.com/users/jchia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jchia/subscriptions", "organizations_url": "https://api.github.com/users/jchia/orgs", "repos_url": "https://api.github.com/users/jchia/repos", "events_url": "https://api.github.com/users/jchia/events{/privacy}", "received_events_url": "https://api.github.com/users/jchia/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-23T08:02:05Z", "updated_at": "2018-11-23T08:02:31Z", "author_association": "NONE", "body_html": "<ol>\n<li>\n<p>How is the workaround with <code>tf.contrib.layers.batch_norm</code> using <code>updates_collection=None</code> supposed to work exactly? Does it update the batch norm stats with the input iff <code>is_training</code> is True? If it updates the batch norm stats on every input, even when <code>is_training</code> is False, then it's a poor workaround. I can't tell the exact behavior from reading the docs, which just says \"one can set updates_collections=None to force the updates in place\".</p>\n</li>\n<li>\n<p>I have a similar problem using a <a href=\"https://gist.github.com/jchia/4c30e4f6a901d117193c4833680f8be4\">modified version</a> of the <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=657600\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/naktinis\">@naktinis</a>'s <a href=\"https://gist.github.com/naktinis/4200f955087bb07223f1fc3bb6255528\">gist</a> using Keras layers. I was able to make the problem go away by applying the <code>tf.contrib.layers.batch_norm</code> workaround. So, is the official prescription to never use a recurrent layer followed by a batch norm layers other than a <code>tf.contrib.layers.batch_norm</code> with <code>updates_collections=None</code>? Is it OK to mix Keras layers with non-Keras layers like those from tf.contrib.layers? However, isn't tf.contrib going away with TF 2.0? How should the current problem be addressed then?</p>\n</li>\n</ol>", "body_text": "How is the workaround with tf.contrib.layers.batch_norm using updates_collection=None supposed to work exactly? Does it update the batch norm stats with the input iff is_training is True? If it updates the batch norm stats on every input, even when is_training is False, then it's a poor workaround. I can't tell the exact behavior from reading the docs, which just says \"one can set updates_collections=None to force the updates in place\".\n\n\nI have a similar problem using a modified version of the @naktinis's gist using Keras layers. I was able to make the problem go away by applying the tf.contrib.layers.batch_norm workaround. So, is the official prescription to never use a recurrent layer followed by a batch norm layers other than a tf.contrib.layers.batch_norm with updates_collections=None? Is it OK to mix Keras layers with non-Keras layers like those from tf.contrib.layers? However, isn't tf.contrib going away with TF 2.0? How should the current problem be addressed then?", "body": "1. How is the workaround with `tf.contrib.layers.batch_norm` using `updates_collection=None` supposed to work exactly? Does it update the batch norm stats with the input iff `is_training` is True? If it updates the batch norm stats on every input, even when `is_training` is False, then it's a poor workaround. I can't tell the exact behavior from reading the docs, which just says \"one can set updates_collections=None to force the updates in place\".\r\n\r\n2. I have a similar problem using a [modified version](https://gist.github.com/jchia/4c30e4f6a901d117193c4833680f8be4) of the @naktinis's [gist](https://gist.github.com/naktinis/4200f955087bb07223f1fc3bb6255528) using Keras layers. I was able to make the problem go away by applying the `tf.contrib.layers.batch_norm` workaround. So, is the official prescription to never use a recurrent layer followed by a batch norm layers other than a `tf.contrib.layers.batch_norm` with `updates_collections=None`? Is it OK to mix Keras layers with non-Keras layers like those from tf.contrib.layers? However, isn't tf.contrib going away with TF 2.0? How should the current problem be addressed then?"}