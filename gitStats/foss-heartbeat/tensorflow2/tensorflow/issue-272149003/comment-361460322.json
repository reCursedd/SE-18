{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361460322", "html_url": "https://github.com/tensorflow/tensorflow/issues/14357#issuecomment-361460322", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14357", "id": 361460322, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTQ2MDMyMg==", "user": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T02:52:26Z", "updated_at": "2018-01-30T02:52:43Z", "author_association": "MEMBER", "body_html": "<p>A simply modification of the reproduction script no longer produces the error (removing the GRU):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nclass_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\nwidth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Placeholders</span>\nph <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, width], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nlabels_ph <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n\nnorm_output_gru <span class=\"pl-k\">=</span> tf.layers.batch_normalization(ph, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The prediction layer</span>\nlogits <span class=\"pl-k\">=</span> tf.layers.dense(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>norm_output_gru, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>class_count)\nonehot_labels <span class=\"pl-k\">=</span> tf.one_hot(labels_ph, <span class=\"pl-v\">depth</span><span class=\"pl-k\">=</span>class_count)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Optimizer</span>\nloss <span class=\"pl-k\">=</span> tf.losses.softmax_cross_entropy(onehot_labels, logits)\noptimizer <span class=\"pl-k\">=</span> tf.train.MomentumOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)\n\nupdate_ops <span class=\"pl-k\">=</span> tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)\n<span class=\"pl-k\">with</span> tf.control_dependencies(update_ops):\n    train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run([train_op, loss],\n             <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{ph: np.zeros([<span class=\"pl-c1\">1</span>, width]),\n                        labels_ph: np.zeros([<span class=\"pl-c1\">1</span>])})</pre></div>\n<p>This indicates that the problem lies in the combination of <code>GRU</code> and batchnorm updates.</p>\n<p>Looking at the stack trace, the error appears in many contexts, but seems most commonly associated with TF RNNs and while loops.</p>", "body_text": "A simply modification of the reproduction script no longer produces the error (removing the GRU):\nimport numpy as np\nimport tensorflow as tf\n\nclass_count = 5\nwidth = 100\n\n# Placeholders\nph = tf.placeholder(shape=[None, width], dtype=tf.float32)\nlabels_ph = tf.placeholder(shape=[None], dtype=tf.int32)\n\nnorm_output_gru = tf.layers.batch_normalization(ph, training=True, axis=1)\n\n# The prediction layer\nlogits = tf.layers.dense(inputs=norm_output_gru, units=class_count)\nonehot_labels = tf.one_hot(labels_ph, depth=class_count)\n\n# Optimizer\nloss = tf.losses.softmax_cross_entropy(onehot_labels, logits)\noptimizer = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.9)\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    train_op = optimizer.minimize(loss)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run([train_op, loss],\n             feed_dict={ph: np.zeros([1, width]),\n                        labels_ph: np.zeros([1])})\nThis indicates that the problem lies in the combination of GRU and batchnorm updates.\nLooking at the stack trace, the error appears in many contexts, but seems most commonly associated with TF RNNs and while loops.", "body": "A simply modification of the reproduction script no longer produces the error (removing the GRU):\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass_count = 5\r\nwidth = 100\r\n\r\n# Placeholders\r\nph = tf.placeholder(shape=[None, width], dtype=tf.float32)\r\nlabels_ph = tf.placeholder(shape=[None], dtype=tf.int32)\r\n\r\nnorm_output_gru = tf.layers.batch_normalization(ph, training=True, axis=1)\r\n\r\n# The prediction layer\r\nlogits = tf.layers.dense(inputs=norm_output_gru, units=class_count)\r\nonehot_labels = tf.one_hot(labels_ph, depth=class_count)\r\n\r\n# Optimizer\r\nloss = tf.losses.softmax_cross_entropy(onehot_labels, logits)\r\noptimizer = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.9)\r\n\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    train_op = optimizer.minimize(loss)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run([train_op, loss],\r\n             feed_dict={ph: np.zeros([1, width]),\r\n                        labels_ph: np.zeros([1])})\r\n```\r\n\r\nThis indicates that the problem lies in the combination of `GRU` and batchnorm updates.\r\n\r\nLooking at the stack trace, the error appears in many contexts, but seems most commonly associated with TF RNNs and while loops."}