{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/115342519", "pull_request_review_id": 36873747, "id": 115342519, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNTM0MjUxOQ==", "diff_hunk": "@@ -51,11 +51,25 @@ The core of the model consists of an LSTM cell that processes one word at a\n time and computes probabilities of the possible values for the next word in the\n sentence. The memory state of the network is initialized with a vector of zeros\n and gets updated after reading each word. For computational reasons, we will\n-process data in mini-batches of size `batch_size`.\n+process data in mini-batches of size `batch_size`.  In this example, it is important to note that `current_batch_of_words` does not correspond to a \"sentence\" of words.  Every word in a batch should correspond to time t.  Tensorflow will automatically sum the gradients of each batch for you.\n+\n+For example:\n+```\n+ t=0  t=1  t=2     t=3\n+[The, fox, is,     quick]\n+[The, fox, jumped, high]\n+\n+words_in_dataset[0] = [The, The]\n+words_in_dataset[1] = [fox, fox]\n+words_in_dataset[2] = [is, jumped]\n+words_in_dataset[3] = [quick, high]\n+num_batches = 4, batch_size = 2", "path": "tensorflow/docs_src/tutorials/recurrent.md", "position": null, "original_position": 17, "commit_id": "691e6ec81eda9bec6b635d4261fbc1186deca988", "original_commit_id": "0d4fe55bd2a6d2adcf874e319762a6de7a70b478", "user": {"login": "CarbonComputed", "id": 1115442, "node_id": "MDQ6VXNlcjExMTU0NDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/1115442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CarbonComputed", "html_url": "https://github.com/CarbonComputed", "followers_url": "https://api.github.com/users/CarbonComputed/followers", "following_url": "https://api.github.com/users/CarbonComputed/following{/other_user}", "gists_url": "https://api.github.com/users/CarbonComputed/gists{/gist_id}", "starred_url": "https://api.github.com/users/CarbonComputed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CarbonComputed/subscriptions", "organizations_url": "https://api.github.com/users/CarbonComputed/orgs", "repos_url": "https://api.github.com/users/CarbonComputed/repos", "events_url": "https://api.github.com/users/CarbonComputed/events{/privacy}", "received_events_url": "https://api.github.com/users/CarbonComputed/received_events", "type": "User", "site_admin": false}, "body": "I can add another variable describing the number of timesteps (time_steps = 4). So that was initially my source of confusion when looking at the tutorial, the time steps don't necessarily apply in the first example (non-truncated BP). It only applies when we split it up into batches. Otherwise, we would just process one word at at time over the entire dataset. Does that make sense?", "created_at": "2017-05-08T20:17:21Z", "updated_at": "2017-05-18T06:34:09Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9746#discussion_r115342519", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9746", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/115342519"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9746#discussion_r115342519"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9746"}}, "body_html": "<p>I can add another variable describing the number of timesteps (time_steps = 4). So that was initially my source of confusion when looking at the tutorial, the time steps don't necessarily apply in the first example (non-truncated BP). It only applies when we split it up into batches. Otherwise, we would just process one word at at time over the entire dataset. Does that make sense?</p>", "body_text": "I can add another variable describing the number of timesteps (time_steps = 4). So that was initially my source of confusion when looking at the tutorial, the time steps don't necessarily apply in the first example (non-truncated BP). It only applies when we split it up into batches. Otherwise, we would just process one word at at time over the entire dataset. Does that make sense?", "in_reply_to_id": 115293254}