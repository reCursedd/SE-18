{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6690", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6690/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6690/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6690/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6690", "id": 199177476, "node_id": "MDU6SXNzdWUxOTkxNzc0NzY=", "number": 6690, "title": "Calculation Delta, Memoization, Update Propagation and Laziness", "user": {"login": "sirinath", "id": 637415, "node_id": "MDQ6VXNlcjYzNzQxNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/637415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sirinath", "html_url": "https://github.com/sirinath", "followers_url": "https://api.github.com/users/sirinath/followers", "following_url": "https://api.github.com/users/sirinath/following{/other_user}", "gists_url": "https://api.github.com/users/sirinath/gists{/gist_id}", "starred_url": "https://api.github.com/users/sirinath/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sirinath/subscriptions", "organizations_url": "https://api.github.com/users/sirinath/orgs", "repos_url": "https://api.github.com/users/sirinath/repos", "events_url": "https://api.github.com/users/sirinath/events{/privacy}", "received_events_url": "https://api.github.com/users/sirinath/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-01-06T11:26:28Z", "updated_at": "2017-01-06T17:27:21Z", "closed_at": "2017-01-06T17:27:21Z", "author_association": "NONE", "body_html": "<p>If you just update a few element in the data structure then having to do calculations which are not affected by the change is a waste.  Also if you read only or consume un affected values then the update and calculations themselves are a waste. Also eagerly calculating this would be a waste since I might only read the result after few updates. In addition you can avoid calculating invariant items. Perhaps XLA can get Lightweight Modular Staging (LMS) functionality to do this type of analysis and optimisation.</p>\n<p>Also see: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"199177476\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6690\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6690/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6690\">#6690</a></p>\n<blockquote>\n<p>For example, if you took the sum of your price data, then adjusted the window, then recomputed the sum, TensorFlow, would recompute the entire sum, whereas you would probably prefer it to simply subtract the entries that were removed from the window, and add the entries that entered the window.</p>\n</blockquote>\n<p>Also other potential optimisations (will require some research):</p>\n<ul>\n<li>If I have a matrix inverse and I update the original matrix, what would be the optimal calculation to arrive at an updated inverse matrix</li>\n<li>Similarly for matrix multiplication and other operations.</li>\n</ul>", "body_text": "If you just update a few element in the data structure then having to do calculations which are not affected by the change is a waste.  Also if you read only or consume un affected values then the update and calculations themselves are a waste. Also eagerly calculating this would be a waste since I might only read the result after few updates. In addition you can avoid calculating invariant items. Perhaps XLA can get Lightweight Modular Staging (LMS) functionality to do this type of analysis and optimisation.\nAlso see: #6690\n\nFor example, if you took the sum of your price data, then adjusted the window, then recomputed the sum, TensorFlow, would recompute the entire sum, whereas you would probably prefer it to simply subtract the entries that were removed from the window, and add the entries that entered the window.\n\nAlso other potential optimisations (will require some research):\n\nIf I have a matrix inverse and I update the original matrix, what would be the optimal calculation to arrive at an updated inverse matrix\nSimilarly for matrix multiplication and other operations.", "body": "If you just update a few element in the data structure then having to do calculations which are not affected by the change is a waste.  Also if you read only or consume un affected values then the update and calculations themselves are a waste. Also eagerly calculating this would be a waste since I might only read the result after few updates. In addition you can avoid calculating invariant items. Perhaps XLA can get Lightweight Modular Staging (LMS) functionality to do this type of analysis and optimisation.\r\n\r\nAlso see: https://github.com/tensorflow/tensorflow/issues/6690\r\n\r\n> For example, if you took the sum of your price data, then adjusted the window, then recomputed the sum, TensorFlow, would recompute the entire sum, whereas you would probably prefer it to simply subtract the entries that were removed from the window, and add the entries that entered the window.\r\n\r\nAlso other potential optimisations (will require some research):\r\n\r\n - If I have a matrix inverse and I update the original matrix, what would be the optimal calculation to arrive at an updated inverse matrix\r\n - Similarly for matrix multiplication and other operations."}