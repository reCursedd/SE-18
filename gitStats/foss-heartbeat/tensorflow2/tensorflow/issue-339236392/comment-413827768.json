{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/413827768", "html_url": "https://github.com/tensorflow/tensorflow/issues/20628#issuecomment-413827768", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20628", "id": 413827768, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzgyNzc2OA==", "user": {"login": "sh1ng", "id": 2821871, "node_id": "MDQ6VXNlcjI4MjE4NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2821871?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sh1ng", "html_url": "https://github.com/sh1ng", "followers_url": "https://api.github.com/users/sh1ng/followers", "following_url": "https://api.github.com/users/sh1ng/following{/other_user}", "gists_url": "https://api.github.com/users/sh1ng/gists{/gist_id}", "starred_url": "https://api.github.com/users/sh1ng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sh1ng/subscriptions", "organizations_url": "https://api.github.com/users/sh1ng/orgs", "repos_url": "https://api.github.com/users/sh1ng/repos", "events_url": "https://api.github.com/users/sh1ng/events{/privacy}", "received_events_url": "https://api.github.com/users/sh1ng/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-17T10:42:54Z", "updated_at": "2018-08-17T10:42:54Z", "author_association": "NONE", "body_html": "<p>Ok, I've changed the script a bit. BTW now it's TF 1.10<br>\nBriefly I set <code>prefetch_to_device</code> buffer to 1(it could be described in the documentation that it's multiplier for batch size) and input size from 128 to 1024 to make computation more time consuming.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nx = tf.placeholder(shape=[None, 1024], dtype=np.float32)\ny = tf.placeholder(shape=[None], dtype=np.float32)\n\nds = tf.data.Dataset.from_tensor_slices((x, y))\nds = ds.repeat()\nds = ds.batch(1024)\nds = ds.apply(tf.contrib.data.prefetch_to_device(\"/gpu:0\", 1))\nit = ds.make_initializable_iterator()\n\nw = tf.get_variable(\n                    name='w',\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    shape=[1024, 1],\n                    dtype=tf.float32)\nb = tf.get_variable(\n                    name='b',\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    shape=[1],\n                    dtype=tf.float32)\n\noptimizer = tf.train.AdamOptimizer(1e-3)\n\nnext_x, next_y = it.get_next()\nprediction = tf.matmul(next_x, w) + b\ntrain = optimizer.minimize((prediction - next_y)**2)\n\ngpu_options = tf.GPUOptions(force_gpu_compatible=True)\nconfig = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True)\nwith tf.Session() as session:\n    x_val = np.random.normal(size=(1024*1024, 1024)).astype(np.float32)\n    y_val = np.random.normal(size=(1024 * 1024)).astype(np.float32)\n    session.run(tf.global_variables_initializer())\n    session.run(it.initializer, feed_dict={x: x_val, y: y_val})\n    for _ in range(1024):\n        session.run(train)\n\n\n</code></pre>\n<p>I don't see any improvements at all.<br>\nAlso there're a big gap after each iteration(it's longer than H2D copy plus computation itself). Is it a TF or/and python overhead?</p>\n<p>I've implemented before overlapping copying into a device with computation in c++. And result was predictable and much faster. Does make sense to use TF C++ API?</p>\n<p>A new screenshot and profiling report are attached.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2821871/44262247-a66fe300-a21a-11e8-9f3d-18ed16ed957f.png\"><img src=\"https://user-images.githubusercontent.com/2821871/44262247-a66fe300-a21a-11e8-9f3d-18ed16ed957f.png\" alt=\"screen shot 2018-08-17 at 12 29 14\" style=\"max-width:100%;\"></a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/2296907/prefetch_on_device.nvvp.2.zip\">prefetch_on_device.nvvp 2.zip</a></p>", "body_text": "Ok, I've changed the script a bit. BTW now it's TF 1.10\nBriefly I set prefetch_to_device buffer to 1(it could be described in the documentation that it's multiplier for batch size) and input size from 128 to 1024 to make computation more time consuming.\nimport tensorflow as tf\nimport numpy as np\n\nx = tf.placeholder(shape=[None, 1024], dtype=np.float32)\ny = tf.placeholder(shape=[None], dtype=np.float32)\n\nds = tf.data.Dataset.from_tensor_slices((x, y))\nds = ds.repeat()\nds = ds.batch(1024)\nds = ds.apply(tf.contrib.data.prefetch_to_device(\"/gpu:0\", 1))\nit = ds.make_initializable_iterator()\n\nw = tf.get_variable(\n                    name='w',\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    shape=[1024, 1],\n                    dtype=tf.float32)\nb = tf.get_variable(\n                    name='b',\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    shape=[1],\n                    dtype=tf.float32)\n\noptimizer = tf.train.AdamOptimizer(1e-3)\n\nnext_x, next_y = it.get_next()\nprediction = tf.matmul(next_x, w) + b\ntrain = optimizer.minimize((prediction - next_y)**2)\n\ngpu_options = tf.GPUOptions(force_gpu_compatible=True)\nconfig = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True)\nwith tf.Session() as session:\n    x_val = np.random.normal(size=(1024*1024, 1024)).astype(np.float32)\n    y_val = np.random.normal(size=(1024 * 1024)).astype(np.float32)\n    session.run(tf.global_variables_initializer())\n    session.run(it.initializer, feed_dict={x: x_val, y: y_val})\n    for _ in range(1024):\n        session.run(train)\n\n\n\nI don't see any improvements at all.\nAlso there're a big gap after each iteration(it's longer than H2D copy plus computation itself). Is it a TF or/and python overhead?\nI've implemented before overlapping copying into a device with computation in c++. And result was predictable and much faster. Does make sense to use TF C++ API?\nA new screenshot and profiling report are attached.\n\nprefetch_on_device.nvvp 2.zip", "body": "Ok, I've changed the script a bit. BTW now it's TF 1.10\r\nBriefly I set `prefetch_to_device` buffer to 1(it could be described in the documentation that it's multiplier for batch size) and input size from 128 to 1024 to make computation more time consuming. \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.placeholder(shape=[None, 1024], dtype=np.float32)\r\ny = tf.placeholder(shape=[None], dtype=np.float32)\r\n\r\nds = tf.data.Dataset.from_tensor_slices((x, y))\r\nds = ds.repeat()\r\nds = ds.batch(1024)\r\nds = ds.apply(tf.contrib.data.prefetch_to_device(\"/gpu:0\", 1))\r\nit = ds.make_initializable_iterator()\r\n\r\nw = tf.get_variable(\r\n                    name='w',\r\n                    initializer=tf.contrib.layers.xavier_initializer(),\r\n                    shape=[1024, 1],\r\n                    dtype=tf.float32)\r\nb = tf.get_variable(\r\n                    name='b',\r\n                    initializer=tf.contrib.layers.xavier_initializer(),\r\n                    shape=[1],\r\n                    dtype=tf.float32)\r\n\r\noptimizer = tf.train.AdamOptimizer(1e-3)\r\n\r\nnext_x, next_y = it.get_next()\r\nprediction = tf.matmul(next_x, w) + b\r\ntrain = optimizer.minimize((prediction - next_y)**2)\r\n\r\ngpu_options = tf.GPUOptions(force_gpu_compatible=True)\r\nconfig = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True)\r\nwith tf.Session() as session:\r\n    x_val = np.random.normal(size=(1024*1024, 1024)).astype(np.float32)\r\n    y_val = np.random.normal(size=(1024 * 1024)).astype(np.float32)\r\n    session.run(tf.global_variables_initializer())\r\n    session.run(it.initializer, feed_dict={x: x_val, y: y_val})\r\n    for _ in range(1024):\r\n        session.run(train)\r\n\r\n\r\n```\r\n\r\nI don't see any improvements at all. \r\nAlso there're a big gap after each iteration(it's longer than H2D copy plus computation itself). Is it a TF or/and python overhead?\r\n\r\nI've implemented before overlapping copying into a device with computation in c++. And result was predictable and much faster. Does make sense to use TF C++ API? \r\n\r\nA new screenshot and profiling report are attached. \r\n![screen shot 2018-08-17 at 12 29 14](https://user-images.githubusercontent.com/2821871/44262247-a66fe300-a21a-11e8-9f3d-18ed16ed957f.png)\r\n[prefetch_on_device.nvvp 2.zip](https://github.com/tensorflow/tensorflow/files/2296907/prefetch_on_device.nvvp.2.zip)\r\n\r\n\r\n \r\n\r\n"}