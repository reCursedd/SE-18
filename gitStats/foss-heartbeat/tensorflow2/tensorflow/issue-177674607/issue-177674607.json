{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4443", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4443/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4443/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4443/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4443", "id": 177674607, "node_id": "MDU6SXNzdWUxNzc2NzQ2MDc=", "number": 4443, "title": "Breaking change of `sparse_softmax_cross_entropy_with_logits` in v0.10: bug or feature", "user": {"login": "ivan-aksamentov", "id": 9403403, "node_id": "MDQ6VXNlcjk0MDM0MDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/9403403?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ivan-aksamentov", "html_url": "https://github.com/ivan-aksamentov", "followers_url": "https://api.github.com/users/ivan-aksamentov/followers", "following_url": "https://api.github.com/users/ivan-aksamentov/following{/other_user}", "gists_url": "https://api.github.com/users/ivan-aksamentov/gists{/gist_id}", "starred_url": "https://api.github.com/users/ivan-aksamentov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ivan-aksamentov/subscriptions", "organizations_url": "https://api.github.com/users/ivan-aksamentov/orgs", "repos_url": "https://api.github.com/users/ivan-aksamentov/repos", "events_url": "https://api.github.com/users/ivan-aksamentov/events{/privacy}", "received_events_url": "https://api.github.com/users/ivan-aksamentov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2016-09-18T20:25:20Z", "updated_at": "2016-09-22T00:45:44Z", "closed_at": "2016-09-22T00:45:32Z", "author_association": "NONE", "body_html": "<p>When dealing with sequences of different lengths and RNNs, it is very common to pad \"out-of-range\" part of the input sequence. More than that, labels should also be padded. It is convenient to pad labels with bogus value <code>-1</code> and not introducing any new valid label values.</p>\n<p>Prior to version 0.10, <code>sparse_softmax_cross_entropy_with_logits</code> returned <code>0.0</code> for logits that correspond to <code>-1</code> labels. These cross-entropy values thus weren't influencing calculation of loss, effectively being ignored.</p>\n<p>Somewhere along the way between v0.9 and v0.10 this behavior has changed, and:</p>\n<ul>\n<li><code>nan</code> is now being returned for logits that correspond to <code>-1</code> labels</li>\n<li>gradient calculation results in <code>nan</code></li>\n</ul>\n<p>New behavior silently breaks the code that rely on the assumption that zero cross-entropy will be returned and valid gradients will be calculated in case of labels <code>-1</code> .</p>\n<p>The goal of this issue is to discuss the possibility and practicability of reverting to the old behavior, i.e.:</p>\n<ul>\n<li>returning <code>0.0</code> for logits that correspond to <code>-1</code> labels</li>\n<li>fixing gradient computation</li>\n</ul>\n<p>I would also like to point out that it would be very good to document changes, that could potentially break user code, in the release notes (in the section \"breaking changes\").</p>\n<p><sub>Sad story: in my case, it took some time to understand what's happening. Current version of documentation says that passing <code>-1</code> labels is illegal and leads to incorrect gradient calculations. It was not the case when I've written my code. I couldn't know about this change, as I cannot possibly re-read all the documentation every time I update tensorflow. I spent about 2 weeks exploring NaN loss and thought it's a gradient explosion. I almost threw away a perfectly good model. Advice for future me: if stuck, read the docs AGAIN ;) </sub></p>\n<p>Related:  <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"135373092\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1234\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1234/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1234\">#1234</a></p>\n<h3>Code snippet to reproduce the issue</h3>\n<p>I tried hard to produce a minimum amount of code, but, well, ... it's still huge.<br>\n(gist:  <a href=\"https://gist.github.com/ivan-aksamentov/b98a33b2513aac8dca8f70a0a16bc592\">b98a33b2513aac8dca8f70a0a16bc592</a>)</p>\n<p>Imagine <code>x</code> to be an output of RNN. Notice that  <code>X_i = 0.0</code>, <code>Y_i = -1</code> and <code>MASK_i</code> = 0.0 for pading (indices greater then <code>LENGTH</code>)</p>\n<pre><code>from __future__ import print_function, division\nimport tensorflow as tf\n\nI = 1  # input size\nT = 6  # sequence length (num timesteps)\nB = 3  # batch size\nC = 7  # number of classes\n\nX = [\n    [[0.0], [1.0], [2.0], [3.0], [0.0], [0.0]],\n    [[0.0], [1.0], [2.0], [0.0], [0.0], [0.0]],\n    [[0.0], [1.0], [2.0], [3.0], [4.0], [5.0]],\n]\n\nY = [\n    [0, 1, 2, 3, -1, -1],\n    [0, 1, 2, -1, -1, -1],\n    [0, 1, 2, 3, 4, 5],\n]\n\nMASK = [\n    [1, 1, 1, 1, 0, 0],\n    [1, 1, 1, 0, 0, 0],\n    [1, 1, 1, 1, 1, 1],\n]\n\nLENGTHS = [4, 3, 6]\n\nx = tf.placeholder(shape=[B, T, I], dtype=tf.float32, name=\"x\")\ny = tf.placeholder(shape=[B, T], dtype=tf.int32, name=\"y\")\nmask = tf.placeholder(shape=[B, T], dtype=tf.bool, name=\"mask\")\nlengths = tf.placeholder(shape=[B], dtype=tf.int32, name=\"lengths\")\n\nx_flat = tf.reshape(x, shape=[B * T, I])\ny_flat = tf.reshape(y, shape=[B * T])\nmask_flat = tf.reshape(mask, shape=[B * T])\n\nw = tf.Variable(tf.random_normal(mean=0.0, stddev=1e-2, shape=[I, C]), name=\"w\")\nb = tf.Variable(tf.random_normal(mean=0.0, stddev=1e-2, shape=[C]), name=\"b\")\n\nz = tf.nn.xw_plus_b(x_flat, w, b, name=\"z\")\n\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(z, y_flat)\n\nxentropy_masked = tf.select(\n    mask_flat,\n    xentropy,\n    tf.zeros_like(xentropy)\n)\n\nxentropy_masked = tf.reshape(xentropy_masked, [B, T])\n\nxentropy_sum = tf.reduce_sum(xentropy_masked, reduction_indices=1)\n\nxentropy_sum_norm = tf.div(\n    xentropy_sum, tf.cast(lengths, dtype=tf.float32)\n)\n\nloss = tf.reduce_mean(xentropy_sum_norm)\n\nopt = tf.train.GradientDescentOptimizer(learning_rate=1e-3)\ngrads_and_vars_op = opt.compute_gradients(loss)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    grads_ops = [g for (g, v) in grads_and_vars_op]\n\n    fetches = [\n        xentropy, xentropy_masked\n    ]\n    fetches.extend(grads_ops)\n\n    xe, xe_masked, grads, _ = sess.run(fetches, feed_dict={\n        x: X,\n        y: Y,\n        mask: MASK,\n        lengths: LENGTHS\n    })\n\n    print(\"xe:\\n\", xe)\n    print(\"xe_masked:\\n\", xe_masked)\n    print(\"grads:\\n\", grads)\n\nprint(\"tf.__version__: \", tf.__version__)\n</code></pre>\n<h3>Desired, old output</h3>\n<p>with <a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">tensorflow-0.9.0-cp27-none-linux_x86_64.whl</a></p>\n<pre><code>xe:\n [ 1.94680917  1.95220029  1.97665489  1.90392613  0.          0.\n  1.94680917  1.95220029  1.97665489  0.          0.          0.\n  1.94680917  1.95220029  1.97665489  1.90392613  1.99289608  1.89933121]\nxe_masked:\n [[ 1.94680917  1.95220029  1.97665489  1.90392613  0.          0.        ]\n [ 1.94680917  1.95220029  1.97665489  0.          0.          0.        ]\n [ 1.94680917  1.95220029  1.97665489  1.90392613  1.99289608  1.89933121]]\ngrads:\n [[ 0.23479398 -0.01371239 -0.27168125 -0.16881087  0.00719513 -0.03152646\n   0.24374181]]\ntf.__version__:  0.9.0\n</code></pre>\n<h3>New output</h3>\n<p>with <a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">tensorflow-0.10.0-cp27-none-linux_x86_64.whl</a></p>\n<pre><code>xe:\n [ 1.96044326  1.94155979  1.98353648  1.90060949         nan         nan\n  1.96044326  1.94155979  1.98353648         nan         nan         nan\n  1.96044326  1.94155979  1.98353648  1.90060949  1.93305326  1.96117282]\nxe_masked:\n [[ 1.96044326  1.94155979  1.98353648  1.90060949  0.          0.        ]\n [ 1.96044326  1.94155979  1.98353648  0.          0.          0.        ]\n [ 1.96044326  1.94155979  1.98353648  1.90060949  1.93305326  1.96117282]]\ngrads:\n [[ nan  nan  nan  nan  nan  nan  nan]]\ntf.__version__:  0.10.0\n</code></pre>", "body_text": "When dealing with sequences of different lengths and RNNs, it is very common to pad \"out-of-range\" part of the input sequence. More than that, labels should also be padded. It is convenient to pad labels with bogus value -1 and not introducing any new valid label values.\nPrior to version 0.10, sparse_softmax_cross_entropy_with_logits returned 0.0 for logits that correspond to -1 labels. These cross-entropy values thus weren't influencing calculation of loss, effectively being ignored.\nSomewhere along the way between v0.9 and v0.10 this behavior has changed, and:\n\nnan is now being returned for logits that correspond to -1 labels\ngradient calculation results in nan\n\nNew behavior silently breaks the code that rely on the assumption that zero cross-entropy will be returned and valid gradients will be calculated in case of labels -1 .\nThe goal of this issue is to discuss the possibility and practicability of reverting to the old behavior, i.e.:\n\nreturning 0.0 for logits that correspond to -1 labels\nfixing gradient computation\n\nI would also like to point out that it would be very good to document changes, that could potentially break user code, in the release notes (in the section \"breaking changes\").\nSad story: in my case, it took some time to understand what's happening. Current version of documentation says that passing -1 labels is illegal and leads to incorrect gradient calculations. It was not the case when I've written my code. I couldn't know about this change, as I cannot possibly re-read all the documentation every time I update tensorflow. I spent about 2 weeks exploring NaN loss and thought it's a gradient explosion. I almost threw away a perfectly good model. Advice for future me: if stuck, read the docs AGAIN ;) \nRelated:  #1234\nCode snippet to reproduce the issue\nI tried hard to produce a minimum amount of code, but, well, ... it's still huge.\n(gist:  b98a33b2513aac8dca8f70a0a16bc592)\nImagine x to be an output of RNN. Notice that  X_i = 0.0, Y_i = -1 and MASK_i = 0.0 for pading (indices greater then LENGTH)\nfrom __future__ import print_function, division\nimport tensorflow as tf\n\nI = 1  # input size\nT = 6  # sequence length (num timesteps)\nB = 3  # batch size\nC = 7  # number of classes\n\nX = [\n    [[0.0], [1.0], [2.0], [3.0], [0.0], [0.0]],\n    [[0.0], [1.0], [2.0], [0.0], [0.0], [0.0]],\n    [[0.0], [1.0], [2.0], [3.0], [4.0], [5.0]],\n]\n\nY = [\n    [0, 1, 2, 3, -1, -1],\n    [0, 1, 2, -1, -1, -1],\n    [0, 1, 2, 3, 4, 5],\n]\n\nMASK = [\n    [1, 1, 1, 1, 0, 0],\n    [1, 1, 1, 0, 0, 0],\n    [1, 1, 1, 1, 1, 1],\n]\n\nLENGTHS = [4, 3, 6]\n\nx = tf.placeholder(shape=[B, T, I], dtype=tf.float32, name=\"x\")\ny = tf.placeholder(shape=[B, T], dtype=tf.int32, name=\"y\")\nmask = tf.placeholder(shape=[B, T], dtype=tf.bool, name=\"mask\")\nlengths = tf.placeholder(shape=[B], dtype=tf.int32, name=\"lengths\")\n\nx_flat = tf.reshape(x, shape=[B * T, I])\ny_flat = tf.reshape(y, shape=[B * T])\nmask_flat = tf.reshape(mask, shape=[B * T])\n\nw = tf.Variable(tf.random_normal(mean=0.0, stddev=1e-2, shape=[I, C]), name=\"w\")\nb = tf.Variable(tf.random_normal(mean=0.0, stddev=1e-2, shape=[C]), name=\"b\")\n\nz = tf.nn.xw_plus_b(x_flat, w, b, name=\"z\")\n\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(z, y_flat)\n\nxentropy_masked = tf.select(\n    mask_flat,\n    xentropy,\n    tf.zeros_like(xentropy)\n)\n\nxentropy_masked = tf.reshape(xentropy_masked, [B, T])\n\nxentropy_sum = tf.reduce_sum(xentropy_masked, reduction_indices=1)\n\nxentropy_sum_norm = tf.div(\n    xentropy_sum, tf.cast(lengths, dtype=tf.float32)\n)\n\nloss = tf.reduce_mean(xentropy_sum_norm)\n\nopt = tf.train.GradientDescentOptimizer(learning_rate=1e-3)\ngrads_and_vars_op = opt.compute_gradients(loss)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    grads_ops = [g for (g, v) in grads_and_vars_op]\n\n    fetches = [\n        xentropy, xentropy_masked\n    ]\n    fetches.extend(grads_ops)\n\n    xe, xe_masked, grads, _ = sess.run(fetches, feed_dict={\n        x: X,\n        y: Y,\n        mask: MASK,\n        lengths: LENGTHS\n    })\n\n    print(\"xe:\\n\", xe)\n    print(\"xe_masked:\\n\", xe_masked)\n    print(\"grads:\\n\", grads)\n\nprint(\"tf.__version__: \", tf.__version__)\n\nDesired, old output\nwith tensorflow-0.9.0-cp27-none-linux_x86_64.whl\nxe:\n [ 1.94680917  1.95220029  1.97665489  1.90392613  0.          0.\n  1.94680917  1.95220029  1.97665489  0.          0.          0.\n  1.94680917  1.95220029  1.97665489  1.90392613  1.99289608  1.89933121]\nxe_masked:\n [[ 1.94680917  1.95220029  1.97665489  1.90392613  0.          0.        ]\n [ 1.94680917  1.95220029  1.97665489  0.          0.          0.        ]\n [ 1.94680917  1.95220029  1.97665489  1.90392613  1.99289608  1.89933121]]\ngrads:\n [[ 0.23479398 -0.01371239 -0.27168125 -0.16881087  0.00719513 -0.03152646\n   0.24374181]]\ntf.__version__:  0.9.0\n\nNew output\nwith tensorflow-0.10.0-cp27-none-linux_x86_64.whl\nxe:\n [ 1.96044326  1.94155979  1.98353648  1.90060949         nan         nan\n  1.96044326  1.94155979  1.98353648         nan         nan         nan\n  1.96044326  1.94155979  1.98353648  1.90060949  1.93305326  1.96117282]\nxe_masked:\n [[ 1.96044326  1.94155979  1.98353648  1.90060949  0.          0.        ]\n [ 1.96044326  1.94155979  1.98353648  0.          0.          0.        ]\n [ 1.96044326  1.94155979  1.98353648  1.90060949  1.93305326  1.96117282]]\ngrads:\n [[ nan  nan  nan  nan  nan  nan  nan]]\ntf.__version__:  0.10.0", "body": "When dealing with sequences of different lengths and RNNs, it is very common to pad \"out-of-range\" part of the input sequence. More than that, labels should also be padded. It is convenient to pad labels with bogus value `-1` and not introducing any new valid label values.\n\nPrior to version 0.10, `sparse_softmax_cross_entropy_with_logits` returned `0.0` for logits that correspond to `-1` labels. These cross-entropy values thus weren't influencing calculation of loss, effectively being ignored. \n\nSomewhere along the way between v0.9 and v0.10 this behavior has changed, and:\n- `nan` is now being returned for logits that correspond to `-1` labels\n- gradient calculation results in `nan`\n\nNew behavior silently breaks the code that rely on the assumption that zero cross-entropy will be returned and valid gradients will be calculated in case of labels `-1` .\n\nThe goal of this issue is to discuss the possibility and practicability of reverting to the old behavior, i.e.:\n- returning `0.0` for logits that correspond to `-1` labels\n- fixing gradient computation\n\nI would also like to point out that it would be very good to document changes, that could potentially break user code, in the release notes (in the section \"breaking changes\"). \n\n<sub>Sad story: in my case, it took some time to understand what's happening. Current version of documentation says that passing `-1` labels is illegal and leads to incorrect gradient calculations. It was not the case when I've written my code. I couldn't know about this change, as I cannot possibly re-read all the documentation every time I update tensorflow. I spent about 2 weeks exploring NaN loss and thought it's a gradient explosion. I almost threw away a perfectly good model. Advice for future me: if stuck, read the docs AGAIN ;) </sub>\n\nRelated:  #1234 \n### Code snippet to reproduce the issue\n\nI tried hard to produce a minimum amount of code, but, well, ... it's still huge.\n(gist:  [b98a33b2513aac8dca8f70a0a16bc592](https://gist.github.com/ivan-aksamentov/b98a33b2513aac8dca8f70a0a16bc592))\n\nImagine `x` to be an output of RNN. Notice that  `X_i = 0.0`, `Y_i = -1` and `MASK_i` = 0.0 for pading (indices greater then `LENGTH`)\n\n```\nfrom __future__ import print_function, division\nimport tensorflow as tf\n\nI = 1  # input size\nT = 6  # sequence length (num timesteps)\nB = 3  # batch size\nC = 7  # number of classes\n\nX = [\n    [[0.0], [1.0], [2.0], [3.0], [0.0], [0.0]],\n    [[0.0], [1.0], [2.0], [0.0], [0.0], [0.0]],\n    [[0.0], [1.0], [2.0], [3.0], [4.0], [5.0]],\n]\n\nY = [\n    [0, 1, 2, 3, -1, -1],\n    [0, 1, 2, -1, -1, -1],\n    [0, 1, 2, 3, 4, 5],\n]\n\nMASK = [\n    [1, 1, 1, 1, 0, 0],\n    [1, 1, 1, 0, 0, 0],\n    [1, 1, 1, 1, 1, 1],\n]\n\nLENGTHS = [4, 3, 6]\n\nx = tf.placeholder(shape=[B, T, I], dtype=tf.float32, name=\"x\")\ny = tf.placeholder(shape=[B, T], dtype=tf.int32, name=\"y\")\nmask = tf.placeholder(shape=[B, T], dtype=tf.bool, name=\"mask\")\nlengths = tf.placeholder(shape=[B], dtype=tf.int32, name=\"lengths\")\n\nx_flat = tf.reshape(x, shape=[B * T, I])\ny_flat = tf.reshape(y, shape=[B * T])\nmask_flat = tf.reshape(mask, shape=[B * T])\n\nw = tf.Variable(tf.random_normal(mean=0.0, stddev=1e-2, shape=[I, C]), name=\"w\")\nb = tf.Variable(tf.random_normal(mean=0.0, stddev=1e-2, shape=[C]), name=\"b\")\n\nz = tf.nn.xw_plus_b(x_flat, w, b, name=\"z\")\n\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(z, y_flat)\n\nxentropy_masked = tf.select(\n    mask_flat,\n    xentropy,\n    tf.zeros_like(xentropy)\n)\n\nxentropy_masked = tf.reshape(xentropy_masked, [B, T])\n\nxentropy_sum = tf.reduce_sum(xentropy_masked, reduction_indices=1)\n\nxentropy_sum_norm = tf.div(\n    xentropy_sum, tf.cast(lengths, dtype=tf.float32)\n)\n\nloss = tf.reduce_mean(xentropy_sum_norm)\n\nopt = tf.train.GradientDescentOptimizer(learning_rate=1e-3)\ngrads_and_vars_op = opt.compute_gradients(loss)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    grads_ops = [g for (g, v) in grads_and_vars_op]\n\n    fetches = [\n        xentropy, xentropy_masked\n    ]\n    fetches.extend(grads_ops)\n\n    xe, xe_masked, grads, _ = sess.run(fetches, feed_dict={\n        x: X,\n        y: Y,\n        mask: MASK,\n        lengths: LENGTHS\n    })\n\n    print(\"xe:\\n\", xe)\n    print(\"xe_masked:\\n\", xe_masked)\n    print(\"grads:\\n\", grads)\n\nprint(\"tf.__version__: \", tf.__version__)\n```\n### Desired, old output\n\nwith [tensorflow-0.9.0-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl)\n\n```\nxe:\n [ 1.94680917  1.95220029  1.97665489  1.90392613  0.          0.\n  1.94680917  1.95220029  1.97665489  0.          0.          0.\n  1.94680917  1.95220029  1.97665489  1.90392613  1.99289608  1.89933121]\nxe_masked:\n [[ 1.94680917  1.95220029  1.97665489  1.90392613  0.          0.        ]\n [ 1.94680917  1.95220029  1.97665489  0.          0.          0.        ]\n [ 1.94680917  1.95220029  1.97665489  1.90392613  1.99289608  1.89933121]]\ngrads:\n [[ 0.23479398 -0.01371239 -0.27168125 -0.16881087  0.00719513 -0.03152646\n   0.24374181]]\ntf.__version__:  0.9.0\n```\n### New output\n\nwith [tensorflow-0.10.0-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl)\n\n```\nxe:\n [ 1.96044326  1.94155979  1.98353648  1.90060949         nan         nan\n  1.96044326  1.94155979  1.98353648         nan         nan         nan\n  1.96044326  1.94155979  1.98353648  1.90060949  1.93305326  1.96117282]\nxe_masked:\n [[ 1.96044326  1.94155979  1.98353648  1.90060949  0.          0.        ]\n [ 1.96044326  1.94155979  1.98353648  0.          0.          0.        ]\n [ 1.96044326  1.94155979  1.98353648  1.90060949  1.93305326  1.96117282]]\ngrads:\n [[ nan  nan  nan  nan  nan  nan  nan]]\ntf.__version__:  0.10.0\n```\n"}