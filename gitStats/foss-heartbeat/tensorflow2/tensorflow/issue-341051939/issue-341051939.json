{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20781", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20781/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20781/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20781/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20781", "id": 341051939, "node_id": "MDU6SXNzdWUzNDEwNTE5Mzk=", "number": 20781, "title": "Feature request: preserve cycle order of open iterators in tf.data.Dataset.interleave", "user": {"login": "carlthome", "id": 1595907, "node_id": "MDQ6VXNlcjE1OTU5MDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1595907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carlthome", "html_url": "https://github.com/carlthome", "followers_url": "https://api.github.com/users/carlthome/followers", "following_url": "https://api.github.com/users/carlthome/following{/other_user}", "gists_url": "https://api.github.com/users/carlthome/gists{/gist_id}", "starred_url": "https://api.github.com/users/carlthome/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carlthome/subscriptions", "organizations_url": "https://api.github.com/users/carlthome/orgs", "repos_url": "https://api.github.com/users/carlthome/repos", "events_url": "https://api.github.com/users/carlthome/events{/privacy}", "received_events_url": "https://api.github.com/users/carlthome/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-07-13T14:59:26Z", "updated_at": "2018-11-20T07:54:14Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>I'm trying to train RNNs with truncated BPTT with <code>tf.data</code> (a great API by the way!) but got tripped up by <a href=\"https://github.com/tensorflow/tensorflow/blob/744cf3d3e06fb63ffa40086766137daedc01a5ba/tensorflow/core/kernels/data/interleave_dataset_op.cc#L190-L195\">these lines</a> as I've assumed an exhausted iterator would result in a new element being opened directly at the same position in the cycle (in order to pass around RNN states reliably).</p>\n<p>Instead what seems to be happening is that my sequences are accidentally shifted in in the subsequent <code>.batch()</code> call whenever a sequence is done. Could the default be changed so that a new element is consumed directly as long as there are any left, such that consecutive dataset elements can be batched in a more straightforward way for RNN training.</p>\n<p>Or could we have a <code>tf.contrib.data.batched_interleave</code> or similar?</p>", "body_text": "I'm trying to train RNNs with truncated BPTT with tf.data (a great API by the way!) but got tripped up by these lines as I've assumed an exhausted iterator would result in a new element being opened directly at the same position in the cycle (in order to pass around RNN states reliably).\nInstead what seems to be happening is that my sequences are accidentally shifted in in the subsequent .batch() call whenever a sequence is done. Could the default be changed so that a new element is consumed directly as long as there are any left, such that consecutive dataset elements can be batched in a more straightforward way for RNN training.\nOr could we have a tf.contrib.data.batched_interleave or similar?", "body": "I'm trying to train RNNs with truncated BPTT with `tf.data` (a great API by the way!) but got tripped up by [these lines](https://github.com/tensorflow/tensorflow/blob/744cf3d3e06fb63ffa40086766137daedc01a5ba/tensorflow/core/kernels/data/interleave_dataset_op.cc#L190-L195) as I've assumed an exhausted iterator would result in a new element being opened directly at the same position in the cycle (in order to pass around RNN states reliably).\r\n\r\nInstead what seems to be happening is that my sequences are accidentally shifted in in the subsequent `.batch()` call whenever a sequence is done. Could the default be changed so that a new element is consumed directly as long as there are any left, such that consecutive dataset elements can be batched in a more straightforward way for RNN training.\r\n\r\nOr could we have a `tf.contrib.data.batched_interleave` or similar?"}