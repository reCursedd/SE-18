{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436807398", "html_url": "https://github.com/tensorflow/tensorflow/issues/19270#issuecomment-436807398", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19270", "id": 436807398, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjgwNzM5OA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-07T22:52:12Z", "updated_at": "2018-11-07T22:52:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When performing floating point operations, you only get guarantees up to machine precision.  You cannot check for equality using \"==\"; in fact, the machine precision for float32 calculations is:</p>\n<pre><code>np.finfo(np.float32).eps\n1.1920929e-07\n</code></pre>\n<p>which matches your errors pretty well.</p>\n<p>try this:</p>\n<pre><code>a = tf.Variable(initial_value=np.random.randn(512, 1024).astype(np.float32))\nb = tf.Variable(initial_value=np.random.randn(10, 512).astype(np.float32))\nba = tf.matmul(b, a)\nb0 = b[0][tf.newaxis, ...]\n\nversion1 = tf.matmul(b0, a)\nversion2 = tf.matmul(b, a)[0]\n\nsession = tf.Session()\nsession.run(tf.global_variables_initializer())\nvalues = session.run((version1, version2))\n</code></pre>\n<p>in perfect precision, you will get the same values.  in practice values[0] and values[1] are off by about <code>10 * eps</code> or <code>1e-6</code>:</p>\n<pre><code>In [14]: values[0] - values[1]\nOut[14]: \narray([[ 9.5367432e-07, -6.6757202e-06,  1.9073486e-05, ...,\n        -9.5367432e-06, -9.5367432e-06,  4.7683716e-07]], dtype=float32)\n</code></pre>\n<p>The underlying reasoning behind this is that cuda's matmul kernels only guarantee accuracy up to machine epsilon.  They use different algorithms (accumulation order) depending on the shape of the inputs, and sometimes even for the same input shapes they'll perform accumulation differently.</p>\n<p>See <a href=\"https://en.wikipedia.org/wiki/Machine_epsilon\" rel=\"nofollow\">machine epsilon</a> for more details.</p>", "body_text": "When performing floating point operations, you only get guarantees up to machine precision.  You cannot check for equality using \"==\"; in fact, the machine precision for float32 calculations is:\nnp.finfo(np.float32).eps\n1.1920929e-07\n\nwhich matches your errors pretty well.\ntry this:\na = tf.Variable(initial_value=np.random.randn(512, 1024).astype(np.float32))\nb = tf.Variable(initial_value=np.random.randn(10, 512).astype(np.float32))\nba = tf.matmul(b, a)\nb0 = b[0][tf.newaxis, ...]\n\nversion1 = tf.matmul(b0, a)\nversion2 = tf.matmul(b, a)[0]\n\nsession = tf.Session()\nsession.run(tf.global_variables_initializer())\nvalues = session.run((version1, version2))\n\nin perfect precision, you will get the same values.  in practice values[0] and values[1] are off by about 10 * eps or 1e-6:\nIn [14]: values[0] - values[1]\nOut[14]: \narray([[ 9.5367432e-07, -6.6757202e-06,  1.9073486e-05, ...,\n        -9.5367432e-06, -9.5367432e-06,  4.7683716e-07]], dtype=float32)\n\nThe underlying reasoning behind this is that cuda's matmul kernels only guarantee accuracy up to machine epsilon.  They use different algorithms (accumulation order) depending on the shape of the inputs, and sometimes even for the same input shapes they'll perform accumulation differently.\nSee machine epsilon for more details.", "body": "When performing floating point operations, you only get guarantees up to machine precision.  You cannot check for equality using \"==\"; in fact, the machine precision for float32 calculations is:\r\n\r\n```\r\nnp.finfo(np.float32).eps\r\n1.1920929e-07\r\n```\r\n\r\nwhich matches your errors pretty well.\r\n\r\ntry this:\r\n\r\n```\r\na = tf.Variable(initial_value=np.random.randn(512, 1024).astype(np.float32))\r\nb = tf.Variable(initial_value=np.random.randn(10, 512).astype(np.float32))\r\nba = tf.matmul(b, a)\r\nb0 = b[0][tf.newaxis, ...]\r\n\r\nversion1 = tf.matmul(b0, a)\r\nversion2 = tf.matmul(b, a)[0]\r\n\r\nsession = tf.Session()\r\nsession.run(tf.global_variables_initializer())\r\nvalues = session.run((version1, version2))\r\n```\r\n\r\nin perfect precision, you will get the same values.  in practice values[0] and values[1] are off by about `10 * eps` or `1e-6`:\r\n\r\n```\r\nIn [14]: values[0] - values[1]\r\nOut[14]: \r\narray([[ 9.5367432e-07, -6.6757202e-06,  1.9073486e-05, ...,\r\n        -9.5367432e-06, -9.5367432e-06,  4.7683716e-07]], dtype=float32)\r\n```\r\n\r\nThe underlying reasoning behind this is that cuda's matmul kernels only guarantee accuracy up to machine epsilon.  They use different algorithms (accumulation order) depending on the shape of the inputs, and sometimes even for the same input shapes they'll perform accumulation differently.\r\n\r\nSee [machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon) for more details."}