{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19270", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19270/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19270/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19270/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19270", "id": 322836935, "node_id": "MDU6SXNzdWUzMjI4MzY5MzU=", "number": 19270, "title": "Different dynamic rnn output depending on batch_size", "user": {"login": "gilmoright", "id": 9256274, "node_id": "MDQ6VXNlcjkyNTYyNzQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9256274?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gilmoright", "html_url": "https://github.com/gilmoright", "followers_url": "https://api.github.com/users/gilmoright/followers", "following_url": "https://api.github.com/users/gilmoright/following{/other_user}", "gists_url": "https://api.github.com/users/gilmoright/gists{/gist_id}", "starred_url": "https://api.github.com/users/gilmoright/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gilmoright/subscriptions", "organizations_url": "https://api.github.com/users/gilmoright/orgs", "repos_url": "https://api.github.com/users/gilmoright/repos", "events_url": "https://api.github.com/users/gilmoright/events{/privacy}", "received_events_url": "https://api.github.com/users/gilmoright/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-05-14T14:17:44Z", "updated_at": "2018-11-07T22:52:12Z", "closed_at": "2018-11-07T22:52:12Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes, own code</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: CentOS 6.7 and Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:  by pip install</li>\n<li><strong>Bazel version</strong>:  N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:  by pip install</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.4.0-rc0-21-g1e25994', '1.4.0-rc1')<br>\non CentOS ; 1.8.0 on Windows</li>\n<li><strong>Python version</strong>: 2.7 on CentOS ; 3 on Windows</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 8.0/cuDNN 6 on CentOS; CUDA 9.0/cuDNN 7.1 on Windows</li>\n<li><strong>GPU model and memory</strong>: NVidia Tesla K80 on CentOS ; GeForce GTX 1050 Ti 4GB on Windows</li>\n<li><strong>Exact command to reproduce</strong>: Example below, tested in jupyter-notebook from anaconda 5.1 (i think so)<br>\nipykernel==4.8.2<br>\nipython==5.6.0<br>\njupyter==1.0.0<br>\njupyter-client==5.2.3<br>\njupyter-console==5.2.0<br>\njupyter-core==4.4.0<br>\njupyter-tensorboard==0.1.6</li>\n</ul>\n<h3>Describe the problem</h3>\n<p><strong>Context</strong>:  I'm trying to create a syntactic parser with NN classifier which defines state of next parsing step. Batch size is dynamic and equals to parsing steps count during training, but on prediction i feed into NN only 1 state per parsing step, so batch size=1. When i found that i lost some prediction accuracy i started to dig and that's what i found.<br>\n<strong>Problem</strong>: hidden states of dynamic LSTM  are a bit different when batch size is 1 and &gt;1. The difference between them is small, about 0.0000001, but since i have several LSTM in NN, it affects the output of network. And interesting that if batch size is 2, 3 or more, the outputs are equal, but they are not if batch size is 1. And last one, it's ok with input with small dimensions, like [batch_size, 4, 4], but not when i have [batch size, 4, &gt;10]<br>\nI wrote some test example to represent this. It worked for me on 2 systems.</p>\n<h3>Source code / logs</h3>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nconfig = tf.ConfigProto(allow_soft_placement=True)\nconfig.gpu_options.allow_growth = True\nsess = tf.InteractiveSession(config=config)\n\nmodel_config = {}\nmodel_config[\"n_hidden\"]=10\nmodel_config[\"lstmStacks\"]=2\nwordsCount = 4\n</code></pre>\n<pre><code>gatherOut = tf.placeholder(shape=(None,None,model_config[\"n_hidden\"]), dtype=tf.float32)\ninp_l1_length = tf.placeholder(shape=(None, ), dtype=tf.int32,name=\"inp_l1_length\")\n\ncell = tf.contrib.rnn.LSTMCell(model_config[\"n_hidden\"])\nlstm_layer, lstm_states = tf.nn.dynamic_rnn(cell, gatherOut, sequence_length=inp_l1_length, dtype=tf.float32)    \nlistOut = lstm_states[1]\n\nsess.run(tf.global_variables_initializer())\n</code></pre>\n<pre><code>inpArr = np.random.uniform(high=1,low=0,size=(1,wordsCount, model_config[\"n_hidden\"]))\n\nres1 = sess.run(listOut, feed_dict= {\n    gatherOut: inpArr,\n    inp_l1_length: [1]\n})\n\nres2 = sess.run(listOut, feed_dict= {\n    gatherOut: np.tile(inpArr,(2,1,1)),\n    inp_l1_length: [1,1]\n})\n\nres3 = sess.run(listOut, feed_dict= {\n    gatherOut: np.tile(inpArr,(3,1,1)),\n    inp_l1_length: [1,1,1]\n})\n</code></pre>\n<pre><code>(res1[0]==res2[0]).all()  # False\n(res2[:2]==res3[:2]).all()  # True\n(res2[-1]==res3[-1]).all()  # True\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, own code\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 6.7 and Windows 10\nTensorFlow installed from (source or binary):  by pip install\nBazel version:  N/A\nTensorFlow installed from (source or binary):  by pip install\nTensorFlow version (use command below): ('v1.4.0-rc0-21-g1e25994', '1.4.0-rc1')\non CentOS ; 1.8.0 on Windows\nPython version: 2.7 on CentOS ; 3 on Windows\nCUDA/cuDNN version: CUDA 8.0/cuDNN 6 on CentOS; CUDA 9.0/cuDNN 7.1 on Windows\nGPU model and memory: NVidia Tesla K80 on CentOS ; GeForce GTX 1050 Ti 4GB on Windows\nExact command to reproduce: Example below, tested in jupyter-notebook from anaconda 5.1 (i think so)\nipykernel==4.8.2\nipython==5.6.0\njupyter==1.0.0\njupyter-client==5.2.3\njupyter-console==5.2.0\njupyter-core==4.4.0\njupyter-tensorboard==0.1.6\n\nDescribe the problem\nContext:  I'm trying to create a syntactic parser with NN classifier which defines state of next parsing step. Batch size is dynamic and equals to parsing steps count during training, but on prediction i feed into NN only 1 state per parsing step, so batch size=1. When i found that i lost some prediction accuracy i started to dig and that's what i found.\nProblem: hidden states of dynamic LSTM  are a bit different when batch size is 1 and >1. The difference between them is small, about 0.0000001, but since i have several LSTM in NN, it affects the output of network. And interesting that if batch size is 2, 3 or more, the outputs are equal, but they are not if batch size is 1. And last one, it's ok with input with small dimensions, like [batch_size, 4, 4], but not when i have [batch size, 4, >10]\nI wrote some test example to represent this. It worked for me on 2 systems.\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\n\nconfig = tf.ConfigProto(allow_soft_placement=True)\nconfig.gpu_options.allow_growth = True\nsess = tf.InteractiveSession(config=config)\n\nmodel_config = {}\nmodel_config[\"n_hidden\"]=10\nmodel_config[\"lstmStacks\"]=2\nwordsCount = 4\n\ngatherOut = tf.placeholder(shape=(None,None,model_config[\"n_hidden\"]), dtype=tf.float32)\ninp_l1_length = tf.placeholder(shape=(None, ), dtype=tf.int32,name=\"inp_l1_length\")\n\ncell = tf.contrib.rnn.LSTMCell(model_config[\"n_hidden\"])\nlstm_layer, lstm_states = tf.nn.dynamic_rnn(cell, gatherOut, sequence_length=inp_l1_length, dtype=tf.float32)    \nlistOut = lstm_states[1]\n\nsess.run(tf.global_variables_initializer())\n\ninpArr = np.random.uniform(high=1,low=0,size=(1,wordsCount, model_config[\"n_hidden\"]))\n\nres1 = sess.run(listOut, feed_dict= {\n    gatherOut: inpArr,\n    inp_l1_length: [1]\n})\n\nres2 = sess.run(listOut, feed_dict= {\n    gatherOut: np.tile(inpArr,(2,1,1)),\n    inp_l1_length: [1,1]\n})\n\nres3 = sess.run(listOut, feed_dict= {\n    gatherOut: np.tile(inpArr,(3,1,1)),\n    inp_l1_length: [1,1,1]\n})\n\n(res1[0]==res2[0]).all()  # False\n(res2[:2]==res3[:2]).all()  # True\n(res2[-1]==res3[-1]).all()  # True", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, own code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 6.7 and Windows 10\r\n- **TensorFlow installed from (source or binary)**:  by pip install\r\n- **Bazel version**:  N/A\r\n- **TensorFlow installed from (source or binary)**:  by pip install\r\n- **TensorFlow version (use command below)**: ('v1.4.0-rc0-21-g1e25994', '1.4.0-rc1')\r\n on CentOS ; 1.8.0 on Windows\r\n- **Python version**: 2.7 on CentOS ; 3 on Windows\r\n- **CUDA/cuDNN version**: CUDA 8.0/cuDNN 6 on CentOS; CUDA 9.0/cuDNN 7.1 on Windows\r\n- **GPU model and memory**: NVidia Tesla K80 on CentOS ; GeForce GTX 1050 Ti 4GB on Windows\r\n- **Exact command to reproduce**: Example below, tested in jupyter-notebook from anaconda 5.1 (i think so)\r\nipykernel==4.8.2\r\nipython==5.6.0\r\njupyter==1.0.0\r\njupyter-client==5.2.3\r\njupyter-console==5.2.0\r\njupyter-core==4.4.0\r\njupyter-tensorboard==0.1.6\r\n\r\n\r\n### Describe the problem\r\n**Context**:  I'm trying to create a syntactic parser with NN classifier which defines state of next parsing step. Batch size is dynamic and equals to parsing steps count during training, but on prediction i feed into NN only 1 state per parsing step, so batch size=1. When i found that i lost some prediction accuracy i started to dig and that's what i found.\r\n**Problem**: hidden states of dynamic LSTM  are a bit different when batch size is 1 and >1. The difference between them is small, about 0.0000001, but since i have several LSTM in NN, it affects the output of network. And interesting that if batch size is 2, 3 or more, the outputs are equal, but they are not if batch size is 1. And last one, it's ok with input with small dimensions, like [batch_size, 4, 4], but not when i have [batch size, 4, >10]\r\nI wrote some test example to represent this. It worked for me on 2 systems.\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nconfig = tf.ConfigProto(allow_soft_placement=True)\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.InteractiveSession(config=config)\r\n\r\nmodel_config = {}\r\nmodel_config[\"n_hidden\"]=10\r\nmodel_config[\"lstmStacks\"]=2\r\nwordsCount = 4\r\n```\r\n```\r\ngatherOut = tf.placeholder(shape=(None,None,model_config[\"n_hidden\"]), dtype=tf.float32)\r\ninp_l1_length = tf.placeholder(shape=(None, ), dtype=tf.int32,name=\"inp_l1_length\")\r\n\r\ncell = tf.contrib.rnn.LSTMCell(model_config[\"n_hidden\"])\r\nlstm_layer, lstm_states = tf.nn.dynamic_rnn(cell, gatherOut, sequence_length=inp_l1_length, dtype=tf.float32)    \r\nlistOut = lstm_states[1]\r\n\r\nsess.run(tf.global_variables_initializer())\r\n```\r\n```\r\ninpArr = np.random.uniform(high=1,low=0,size=(1,wordsCount, model_config[\"n_hidden\"]))\r\n\r\nres1 = sess.run(listOut, feed_dict= {\r\n    gatherOut: inpArr,\r\n    inp_l1_length: [1]\r\n})\r\n\r\nres2 = sess.run(listOut, feed_dict= {\r\n    gatherOut: np.tile(inpArr,(2,1,1)),\r\n    inp_l1_length: [1,1]\r\n})\r\n\r\nres3 = sess.run(listOut, feed_dict= {\r\n    gatherOut: np.tile(inpArr,(3,1,1)),\r\n    inp_l1_length: [1,1,1]\r\n})\r\n```\r\n```\r\n(res1[0]==res2[0]).all()  # False\r\n(res2[:2]==res3[:2]).all()  # True\r\n(res2[-1]==res3[-1]).all()  # True\r\n```"}