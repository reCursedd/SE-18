{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243645126", "html_url": "https://github.com/tensorflow/tensorflow/issues/2283#issuecomment-243645126", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2283", "id": 243645126, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzY0NTEyNg==", "user": {"login": "lijiajia", "id": 8953690, "node_id": "MDQ6VXNlcjg5NTM2OTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/8953690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lijiajia", "html_url": "https://github.com/lijiajia", "followers_url": "https://api.github.com/users/lijiajia/followers", "following_url": "https://api.github.com/users/lijiajia/following{/other_user}", "gists_url": "https://api.github.com/users/lijiajia/gists{/gist_id}", "starred_url": "https://api.github.com/users/lijiajia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lijiajia/subscriptions", "organizations_url": "https://api.github.com/users/lijiajia/orgs", "repos_url": "https://api.github.com/users/lijiajia/repos", "events_url": "https://api.github.com/users/lijiajia/events{/privacy}", "received_events_url": "https://api.github.com/users/lijiajia/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-31T03:02:31Z", "updated_at": "2016-08-31T03:04:33Z", "author_association": "NONE", "body_html": "<p>I have the same problem, my code is as following. Please help me.</p>\n<pre><code>tower_outputs = []\ntower_loss = []\ntower_grad_norms = []\ntower_clipped_grads = []\n\nfor i in xrange(1, 2):\n  with tf.device('/gpu:%d' % i):\n    # Training outputs and losses.\n    if forward_only:\n      outputs, losses = tf.nn.seq2seq.model_with_buckets(\n          encoder_inputs_split[i], decoder_inputs_split[i], targets_split[i],\n          target_weights_split[i], buckets, lambda x, y: seq2seq_f(x, y, True),\n          softmax_loss_function=softmax_loss_function)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          outputs[b] = [\n              tf.matmul(output, output_projection[0]) + output_projection[1]\n              for output in outputs[b]\n          ]\n    else:\n      outputs, losses = tf.nn.seq2seq.model_with_buckets(\n          encoder_inputs_split[i], decoder_inputs_split[i], targets_split[i],\n          target_weights_split[i], buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    tower_outputs.append(outputs)\n    tower_loss.append(losses)\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    print(params)\n    tmp_norms = []\n    tmp_clip_grad = []\n    if not forward_only:\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n        tmp_norms.append(norm)\n        tmp_clip_grad.append(clipped_gradients)\n        tower_grad_norms.append(tmp_norms)\n        tower_clipped_grads.append(tmp_clip_grad)\n     print('gpu:%d' % i)\n     tf.get_variable_scope().reuse_variables()\n\nself.outputs = tf.concat(1, tower_outputs)\nself.losses = tf.reduce_mean(tower_loss, 0)\n\nif not forward_only:\n  self.gradient_norms = tf.reduce_mean(tower_grad_norms, 0)\n  gradients = self.average_gradients(tower_clipped_grads)\n  self.updates = []\n  for b in xrange(len(buckets)):\n    clipped_gradients = gradients[b]\n    self.updates.append(opt.apply_gradients(\n        zip(clipped_gradients, params), global_step=self.global_step))\n\nself.saver = tf.train.Saver(tf.all_variables())\n\ndef average_gradients(self, tower_grads):\n  average_grads = []\n  for tower_grad in zip(*tower_grads):\n    # Average over the 'tower' dimension.\n    grad = tf.reduce_mean(tower_grad, 1)\n\n    # Keep in mind that the Variables are redundant because they are shared across towers. So .. we will just return the first tower's pointer to the Variable.\n    average_grads.append(grad)\n  return average_grads\n</code></pre>", "body_text": "I have the same problem, my code is as following. Please help me.\ntower_outputs = []\ntower_loss = []\ntower_grad_norms = []\ntower_clipped_grads = []\n\nfor i in xrange(1, 2):\n  with tf.device('/gpu:%d' % i):\n    # Training outputs and losses.\n    if forward_only:\n      outputs, losses = tf.nn.seq2seq.model_with_buckets(\n          encoder_inputs_split[i], decoder_inputs_split[i], targets_split[i],\n          target_weights_split[i], buckets, lambda x, y: seq2seq_f(x, y, True),\n          softmax_loss_function=softmax_loss_function)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          outputs[b] = [\n              tf.matmul(output, output_projection[0]) + output_projection[1]\n              for output in outputs[b]\n          ]\n    else:\n      outputs, losses = tf.nn.seq2seq.model_with_buckets(\n          encoder_inputs_split[i], decoder_inputs_split[i], targets_split[i],\n          target_weights_split[i], buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    tower_outputs.append(outputs)\n    tower_loss.append(losses)\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    print(params)\n    tmp_norms = []\n    tmp_clip_grad = []\n    if not forward_only:\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n        tmp_norms.append(norm)\n        tmp_clip_grad.append(clipped_gradients)\n        tower_grad_norms.append(tmp_norms)\n        tower_clipped_grads.append(tmp_clip_grad)\n     print('gpu:%d' % i)\n     tf.get_variable_scope().reuse_variables()\n\nself.outputs = tf.concat(1, tower_outputs)\nself.losses = tf.reduce_mean(tower_loss, 0)\n\nif not forward_only:\n  self.gradient_norms = tf.reduce_mean(tower_grad_norms, 0)\n  gradients = self.average_gradients(tower_clipped_grads)\n  self.updates = []\n  for b in xrange(len(buckets)):\n    clipped_gradients = gradients[b]\n    self.updates.append(opt.apply_gradients(\n        zip(clipped_gradients, params), global_step=self.global_step))\n\nself.saver = tf.train.Saver(tf.all_variables())\n\ndef average_gradients(self, tower_grads):\n  average_grads = []\n  for tower_grad in zip(*tower_grads):\n    # Average over the 'tower' dimension.\n    grad = tf.reduce_mean(tower_grad, 1)\n\n    # Keep in mind that the Variables are redundant because they are shared across towers. So .. we will just return the first tower's pointer to the Variable.\n    average_grads.append(grad)\n  return average_grads", "body": "I have the same problem, my code is as following. Please help me.\n\n```\ntower_outputs = []\ntower_loss = []\ntower_grad_norms = []\ntower_clipped_grads = []\n\nfor i in xrange(1, 2):\n  with tf.device('/gpu:%d' % i):\n    # Training outputs and losses.\n    if forward_only:\n      outputs, losses = tf.nn.seq2seq.model_with_buckets(\n          encoder_inputs_split[i], decoder_inputs_split[i], targets_split[i],\n          target_weights_split[i], buckets, lambda x, y: seq2seq_f(x, y, True),\n          softmax_loss_function=softmax_loss_function)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          outputs[b] = [\n              tf.matmul(output, output_projection[0]) + output_projection[1]\n              for output in outputs[b]\n          ]\n    else:\n      outputs, losses = tf.nn.seq2seq.model_with_buckets(\n          encoder_inputs_split[i], decoder_inputs_split[i], targets_split[i],\n          target_weights_split[i], buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    tower_outputs.append(outputs)\n    tower_loss.append(losses)\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    print(params)\n    tmp_norms = []\n    tmp_clip_grad = []\n    if not forward_only:\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n        tmp_norms.append(norm)\n        tmp_clip_grad.append(clipped_gradients)\n        tower_grad_norms.append(tmp_norms)\n        tower_clipped_grads.append(tmp_clip_grad)\n     print('gpu:%d' % i)\n     tf.get_variable_scope().reuse_variables()\n\nself.outputs = tf.concat(1, tower_outputs)\nself.losses = tf.reduce_mean(tower_loss, 0)\n\nif not forward_only:\n  self.gradient_norms = tf.reduce_mean(tower_grad_norms, 0)\n  gradients = self.average_gradients(tower_clipped_grads)\n  self.updates = []\n  for b in xrange(len(buckets)):\n    clipped_gradients = gradients[b]\n    self.updates.append(opt.apply_gradients(\n        zip(clipped_gradients, params), global_step=self.global_step))\n\nself.saver = tf.train.Saver(tf.all_variables())\n\ndef average_gradients(self, tower_grads):\n  average_grads = []\n  for tower_grad in zip(*tower_grads):\n    # Average over the 'tower' dimension.\n    grad = tf.reduce_mean(tower_grad, 1)\n\n    # Keep in mind that the Variables are redundant because they are shared across towers. So .. we will just return the first tower's pointer to the Variable.\n    average_grads.append(grad)\n  return average_grads\n```\n"}