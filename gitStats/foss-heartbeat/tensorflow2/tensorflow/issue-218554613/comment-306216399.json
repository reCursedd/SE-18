{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/306216399", "html_url": "https://github.com/tensorflow/tensorflow/issues/8873#issuecomment-306216399", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8873", "id": 306216399, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNjIxNjM5OQ==", "user": {"login": "yanchen036", "id": 3898819, "node_id": "MDQ6VXNlcjM4OTg4MTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3898819?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yanchen036", "html_url": "https://github.com/yanchen036", "followers_url": "https://api.github.com/users/yanchen036/followers", "following_url": "https://api.github.com/users/yanchen036/following{/other_user}", "gists_url": "https://api.github.com/users/yanchen036/gists{/gist_id}", "starred_url": "https://api.github.com/users/yanchen036/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yanchen036/subscriptions", "organizations_url": "https://api.github.com/users/yanchen036/orgs", "repos_url": "https://api.github.com/users/yanchen036/repos", "events_url": "https://api.github.com/users/yanchen036/events{/privacy}", "received_events_url": "https://api.github.com/users/yanchen036/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-05T15:21:38Z", "updated_at": "2017-06-05T16:09:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> I did 2 things.<br>\nFirst, I wrote a TileSimple stub(now I know that's not you need) and a TileUsingEigen function</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">void</span> <span class=\"pl-en\">TileUsingEigen</span>(<span class=\"pl-k\">const</span> Device&amp; d, <span class=\"pl-k\">const</span> Tensor&amp; in, Tensor* out,\n                    <span class=\"pl-k\">const</span> gtl::ArraySlice&lt;int32&gt; broadcast_array)</pre></div>\n<p>which implements the old Tile::operator(). See <a href=\"https://github.com/yanchen036/tensorflow/blob/arbitrary_dim_for_tile/tensorflow/core/kernels/tile_ops_impl.h#L32\">https://github.com/yanchen036/tensorflow/blob/arbitrary_dim_for_tile/tensorflow/core/kernels/tile_ops_impl.h#L32</a></p>\n<p>Second, I changed the Tile::operator() function from</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">typename</span> Device, <span class=\"pl-k\">typename</span> T, <span class=\"pl-k\">int</span> NDIM&gt;\n<span class=\"pl-k\">void</span> <span class=\"pl-en\">operator</span>()(<span class=\"pl-k\">const</span> Device&amp; d, <span class=\"pl-k\">typename</span> TTypes&lt;T, NDIM&gt;::Tensor out,\n                <span class=\"pl-k\">typename</span> TTypes&lt;T, NDIM&gt;::ConstTensor in,\n                <span class=\"pl-k\">const</span> Eigen::array&lt;int32, NDIM&gt;&amp; broadcast_array) <span class=\"pl-k\">const</span></pre></div>\n<p>To</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">typename</span> Device, <span class=\"pl-k\">typename</span> T&gt;\n<span class=\"pl-k\">void</span> <span class=\"pl-en\">operator</span>()(<span class=\"pl-k\">const</span> Device&amp; d, <span class=\"pl-k\">const</span> Tensor&amp; in, Tensor* out,\n                <span class=\"pl-k\">const</span> gtl::ArraySlice&lt;int32&gt; broadcast_array) <span class=\"pl-k\">const</span> {\n    <span class=\"pl-k\">switch</span> (in.<span class=\"pl-c1\">dims</span>()) {\n        <span class=\"pl-k\">case</span> <span class=\"pl-c1\">0</span>:\n            internal::TileUsingEigen&lt;Device, T, <span class=\"pl-c1\">0</span>&gt;(d, in, out, broadcast_array);\n            <span class=\"pl-k\">break</span>;\n        <span class=\"pl-k\">case</span> <span class=\"pl-c1\">1</span>:\n            internal::TileUsingEigen&lt;Device, T, <span class=\"pl-c1\">1</span>&gt;(d, in, out, broadcast_array);\n            <span class=\"pl-k\">break</span>;\n...\n    }\n}</pre></div>\n<p>This uses template specialization as you mentioned.</p>", "body_text": "@girving I did 2 things.\nFirst, I wrote a TileSimple stub(now I know that's not you need) and a TileUsingEigen function\nvoid TileUsingEigen(const Device& d, const Tensor& in, Tensor* out,\n                    const gtl::ArraySlice<int32> broadcast_array)\nwhich implements the old Tile::operator(). See https://github.com/yanchen036/tensorflow/blob/arbitrary_dim_for_tile/tensorflow/core/kernels/tile_ops_impl.h#L32\nSecond, I changed the Tile::operator() function from\ntemplate <typename Device, typename T, int NDIM>\nvoid operator()(const Device& d, typename TTypes<T, NDIM>::Tensor out,\n                typename TTypes<T, NDIM>::ConstTensor in,\n                const Eigen::array<int32, NDIM>& broadcast_array) const\nTo\ntemplate <typename Device, typename T>\nvoid operator()(const Device& d, const Tensor& in, Tensor* out,\n                const gtl::ArraySlice<int32> broadcast_array) const {\n    switch (in.dims()) {\n        case 0:\n            internal::TileUsingEigen<Device, T, 0>(d, in, out, broadcast_array);\n            break;\n        case 1:\n            internal::TileUsingEigen<Device, T, 1>(d, in, out, broadcast_array);\n            break;\n...\n    }\n}\nThis uses template specialization as you mentioned.", "body": "@girving I did 2 things. \r\nFirst, I wrote a TileSimple stub(now I know that's not you need) and a TileUsingEigen function\r\n```c++\r\nvoid TileUsingEigen(const Device& d, const Tensor& in, Tensor* out,\r\n                    const gtl::ArraySlice<int32> broadcast_array)\r\n```\r\nwhich implements the old Tile::operator(). See https://github.com/yanchen036/tensorflow/blob/arbitrary_dim_for_tile/tensorflow/core/kernels/tile_ops_impl.h#L32\r\n\r\nSecond, I changed the Tile::operator() function from\r\n```c++\r\ntemplate <typename Device, typename T, int NDIM>\r\nvoid operator()(const Device& d, typename TTypes<T, NDIM>::Tensor out,\r\n                typename TTypes<T, NDIM>::ConstTensor in,\r\n                const Eigen::array<int32, NDIM>& broadcast_array) const\r\n```\r\nTo\r\n```c++\r\ntemplate <typename Device, typename T>\r\nvoid operator()(const Device& d, const Tensor& in, Tensor* out,\r\n                const gtl::ArraySlice<int32> broadcast_array) const {\r\n    switch (in.dims()) {\r\n        case 0:\r\n            internal::TileUsingEigen<Device, T, 0>(d, in, out, broadcast_array);\r\n            break;\r\n        case 1:\r\n            internal::TileUsingEigen<Device, T, 1>(d, in, out, broadcast_array);\r\n            break;\r\n...\r\n    }\r\n}\r\n```\r\nThis uses template specialization as you mentioned."}