{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/290722014", "html_url": "https://github.com/tensorflow/tensorflow/pull/8673#issuecomment-290722014", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8673", "id": 290722014, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MDcyMjAxNA==", "user": {"login": "cancan101", "id": 51059, "node_id": "MDQ6VXNlcjUxMDU5", "avatar_url": "https://avatars1.githubusercontent.com/u/51059?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cancan101", "html_url": "https://github.com/cancan101", "followers_url": "https://api.github.com/users/cancan101/followers", "following_url": "https://api.github.com/users/cancan101/following{/other_user}", "gists_url": "https://api.github.com/users/cancan101/gists{/gist_id}", "starred_url": "https://api.github.com/users/cancan101/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cancan101/subscriptions", "organizations_url": "https://api.github.com/users/cancan101/orgs", "repos_url": "https://api.github.com/users/cancan101/repos", "events_url": "https://api.github.com/users/cancan101/events{/privacy}", "received_events_url": "https://api.github.com/users/cancan101/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-31T14:08:37Z", "updated_at": "2017-03-31T14:14:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for the review and comments. A few thoughts:</p>\n<p>As far as the (new) fused <code>Conv2D</code> Op that I am using, I used the <a href=\"https://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/core/ops/nn_ops.cc#L2632-L2645\">MKL Conv2D Op</a> as a predicate. In that case Intel has defined a new Op that includes both the convolution Op and the BiasAdd Op. My Conv Op actually isn't pulling in just the activation function, rather, my Op and the MKL both have the bias add has been fused as well.</p>\n<p>I understand that the primitive <code>Conv2D</code> op in TF does not currently handle activations or bias addition, but this op may be \"too primitive\". In fact cuDNN is now moving in the direction of fusing these three into one kernel (see: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"218065786\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8828\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8828/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/8828\">#8828</a>). Perhaps, a different FusedConv2D Op should be added that can be used by not just BNNS but also cuDNN v6 and any other underlying platform implementations that can take advantage of doing all three ops together. This would be  along the lines of batch norm, which now offers a <code>fused</code> version of the Op to reduce the number of primitive Ops used.</p>\n<p>The secondary benefit of this Op is it allows for BNNS specific optimization flags, for example the level of precision (float16 vs float32) and control over the <code>filter_layout</code> (I already have code that can avoid having to transpose the weights on each call).</p>\n<p>As far as testing, I have tested locally with some models, the one thing to keep in mind is that these two new Ops work with NCHW so the models do have to work with that channel layout.</p>\n<p>Addendum<br>\nThere are even more implementations that can benefit from fusing Conv-Bias-Activation. Others include:</p>\n<ul>\n<li>Metal Performance Shaders (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"210971437\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7958\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7958/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/7958\">#7958</a>). Here it might be extra extra important because of 1. avoiding extra copies back and forth from GPU memory (ie using the tuned <code>MPSTemporary\u200bImage</code> and 2. because MPS uses a very weird memory layout and ideally transposing to this only needs to be done once.</li>\n<li>MKL (as linked above, perhaps the MKL specific Op (<code>MklConv2DWithBias</code>) can be unified with this new Op.</li>\n</ul>", "body_text": "Thanks for the review and comments. A few thoughts:\nAs far as the (new) fused Conv2D Op that I am using, I used the MKL Conv2D Op as a predicate. In that case Intel has defined a new Op that includes both the convolution Op and the BiasAdd Op. My Conv Op actually isn't pulling in just the activation function, rather, my Op and the MKL both have the bias add has been fused as well.\nI understand that the primitive Conv2D op in TF does not currently handle activations or bias addition, but this op may be \"too primitive\". In fact cuDNN is now moving in the direction of fusing these three into one kernel (see: #8828). Perhaps, a different FusedConv2D Op should be added that can be used by not just BNNS but also cuDNN v6 and any other underlying platform implementations that can take advantage of doing all three ops together. This would be  along the lines of batch norm, which now offers a fused version of the Op to reduce the number of primitive Ops used.\nThe secondary benefit of this Op is it allows for BNNS specific optimization flags, for example the level of precision (float16 vs float32) and control over the filter_layout (I already have code that can avoid having to transpose the weights on each call).\nAs far as testing, I have tested locally with some models, the one thing to keep in mind is that these two new Ops work with NCHW so the models do have to work with that channel layout.\nAddendum\nThere are even more implementations that can benefit from fusing Conv-Bias-Activation. Others include:\n\nMetal Performance Shaders (#7958). Here it might be extra extra important because of 1. avoiding extra copies back and forth from GPU memory (ie using the tuned MPSTemporary\u200bImage and 2. because MPS uses a very weird memory layout and ideally transposing to this only needs to be done once.\nMKL (as linked above, perhaps the MKL specific Op (MklConv2DWithBias) can be unified with this new Op.", "body": "Thanks for the review and comments. A few thoughts:\r\n\r\nAs far as the (new) fused `Conv2D` Op that I am using, I used the [MKL Conv2D Op](https://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/core/ops/nn_ops.cc#L2632-L2645) as a predicate. In that case Intel has defined a new Op that includes both the convolution Op and the BiasAdd Op. My Conv Op actually isn't pulling in just the activation function, rather, my Op and the MKL both have the bias add has been fused as well.\r\n\r\nI understand that the primitive `Conv2D` op in TF does not currently handle activations or bias addition, but this op may be \"too primitive\". In fact cuDNN is now moving in the direction of fusing these three into one kernel (see: https://github.com/tensorflow/tensorflow/issues/8828). Perhaps, a different FusedConv2D Op should be added that can be used by not just BNNS but also cuDNN v6 and any other underlying platform implementations that can take advantage of doing all three ops together. This would be  along the lines of batch norm, which now offers a `fused` version of the Op to reduce the number of primitive Ops used.\r\n\r\nThe secondary benefit of this Op is it allows for BNNS specific optimization flags, for example the level of precision (float16 vs float32) and control over the `filter_layout` (I already have code that can avoid having to transpose the weights on each call).\r\n\r\nAs far as testing, I have tested locally with some models, the one thing to keep in mind is that these two new Ops work with NCHW so the models do have to work with that channel layout.\r\n\r\nAddendum\r\nThere are even more implementations that can benefit from fusing Conv-Bias-Activation. Others include:\r\n* Metal Performance Shaders (https://github.com/tensorflow/tensorflow/issues/7958). Here it might be extra extra important because of 1. avoiding extra copies back and forth from GPU memory (ie using the tuned `MPSTemporary\u200bImage` and 2. because MPS uses a very weird memory layout and ideally transposing to this only needs to be done once.\r\n* MKL (as linked above, perhaps the MKL specific Op (`MklConv2DWithBias`) can be unified with this new Op."}