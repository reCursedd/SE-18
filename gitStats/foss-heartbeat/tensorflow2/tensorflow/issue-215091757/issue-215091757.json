{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8506", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8506/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8506/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8506/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8506", "id": 215091757, "node_id": "MDU6SXNzdWUyMTUwOTE3NTc=", "number": 8506, "title": "A Strange Behavior in TensorFlow about Conv2d Creation and Weight Optimization", "user": {"login": "mabbs90", "id": 8404745, "node_id": "MDQ6VXNlcjg0MDQ3NDU=", "avatar_url": "https://avatars2.githubusercontent.com/u/8404745?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mabbs90", "html_url": "https://github.com/mabbs90", "followers_url": "https://api.github.com/users/mabbs90/followers", "following_url": "https://api.github.com/users/mabbs90/following{/other_user}", "gists_url": "https://api.github.com/users/mabbs90/gists{/gist_id}", "starred_url": "https://api.github.com/users/mabbs90/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mabbs90/subscriptions", "organizations_url": "https://api.github.com/users/mabbs90/orgs", "repos_url": "https://api.github.com/users/mabbs90/repos", "events_url": "https://api.github.com/users/mabbs90/events{/privacy}", "received_events_url": "https://api.github.com/users/mabbs90/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-17T18:50:39Z", "updated_at": "2017-03-17T21:47:35Z", "closed_at": "2017-03-17T21:26:48Z", "author_association": "NONE", "body_html": "<p>To simplify the tensorflow syntax, I created utility functions that would generate a conv2d layer more easily for me. However, when I use it in an architecture, the results are bizzare and different from the vanilla coding version. Even though both methods are returning the same tensor shape, tensorflow fails to optimize the weights in the second approach and I always get extremely low accuracies, vs the first version returns a perfect accuracy.</p>\n<p>It seems like every time Tensorflow initializes the weights randomly instead of optimizing it. Is this a known bug?</p>\n<h2>Version 1</h2>\n<p><code>sigma = 0.1</code><br>\n<code>x = tf.placeholder(tf.float32, (None, 32, 32, 3))</code></p>\n<p><code>conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))</code><br>\n<code>conv1_b = tf.Variable(tf.zeros(6))</code><br>\n<code>conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b</code></p>\n<h2>Version 2</h2>\n<p><code>def weights(dims, mu = 0, sigma = 0.1):</code><br>\n\u00a0\u00a0\u00a0\u00a0<code>w = tf.Variable(tf.truncated_normal(shape = dims, mean = mu, stddev = sigma))</code><br>\n\u00a0\u00a0\u00a0\u00a0<code>return w</code></p>\n<p><code>def bias(dims):</code><br>\n\u00a0\u00a0\u00a0\u00a0<code>b = tf.Variable(tf.zeros(dims))</code><br>\n\u00a0\u00a0\u00a0\u00a0<code>return b</code></p>\n<p><code>def conv(layer, dims, stride = [1, 1, 1, 1], pad = 'VALID'):</code><br>\n\u00a0\u00a0\u00a0\u00a0<code>w = weights(dims)</code><br>\n\u00a0\u00a0\u00a0\u00a0<code>b = bias(dims[-1])</code><br>\n\u00a0\u00a0\u00a0\u00a0<code>conv2 = tf.nn.conv2d(layer, w, strides = stride, padding = pad) + b</code><br>\n\u00a0\u00a0\u00a0\u00a0<code>return conv2</code><br>\nthis line is the only line that I put in my network architecture: so I created my conv2d layers like this:</p>\n<p><code>conv2 = conv(x, [5, 5, 3, 6]) </code></p>", "body_text": "To simplify the tensorflow syntax, I created utility functions that would generate a conv2d layer more easily for me. However, when I use it in an architecture, the results are bizzare and different from the vanilla coding version. Even though both methods are returning the same tensor shape, tensorflow fails to optimize the weights in the second approach and I always get extremely low accuracies, vs the first version returns a perfect accuracy.\nIt seems like every time Tensorflow initializes the weights randomly instead of optimizing it. Is this a known bug?\nVersion 1\nsigma = 0.1\nx = tf.placeholder(tf.float32, (None, 32, 32, 3))\nconv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\nconv1_b = tf.Variable(tf.zeros(6))\nconv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\nVersion 2\ndef weights(dims, mu = 0, sigma = 0.1):\n\u00a0\u00a0\u00a0\u00a0w = tf.Variable(tf.truncated_normal(shape = dims, mean = mu, stddev = sigma))\n\u00a0\u00a0\u00a0\u00a0return w\ndef bias(dims):\n\u00a0\u00a0\u00a0\u00a0b = tf.Variable(tf.zeros(dims))\n\u00a0\u00a0\u00a0\u00a0return b\ndef conv(layer, dims, stride = [1, 1, 1, 1], pad = 'VALID'):\n\u00a0\u00a0\u00a0\u00a0w = weights(dims)\n\u00a0\u00a0\u00a0\u00a0b = bias(dims[-1])\n\u00a0\u00a0\u00a0\u00a0conv2 = tf.nn.conv2d(layer, w, strides = stride, padding = pad) + b\n\u00a0\u00a0\u00a0\u00a0return conv2\nthis line is the only line that I put in my network architecture: so I created my conv2d layers like this:\nconv2 = conv(x, [5, 5, 3, 6])", "body": "To simplify the tensorflow syntax, I created utility functions that would generate a conv2d layer more easily for me. However, when I use it in an architecture, the results are bizzare and different from the vanilla coding version. Even though both methods are returning the same tensor shape, tensorflow fails to optimize the weights in the second approach and I always get extremely low accuracies, vs the first version returns a perfect accuracy.\r\n\r\nIt seems like every time Tensorflow initializes the weights randomly instead of optimizing it. Is this a known bug?\r\n\r\n\r\nVersion 1\r\n---\r\n`sigma = 0.1`\r\n`x = tf.placeholder(tf.float32, (None, 32, 32, 3))`\r\n\r\n`conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))`\r\n`conv1_b = tf.Variable(tf.zeros(6))`\r\n`conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b`\r\n\r\n\r\nVersion 2\r\n---\r\n\r\n`def weights(dims, mu = 0, sigma = 0.1):`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`w = tf.Variable(tf.truncated_normal(shape = dims, mean = mu, stddev = sigma))`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`return w`\r\n\r\n`def bias(dims):`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`b = tf.Variable(tf.zeros(dims))`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`return b`\r\n\r\n`def conv(layer, dims, stride = [1, 1, 1, 1], pad = 'VALID'):`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`w = weights(dims)`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`b = bias(dims[-1])`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`conv2 = tf.nn.conv2d(layer, w, strides = stride, padding = pad) + b`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`return conv2`\r\nthis line is the only line that I put in my network architecture: so I created my conv2d layers like this:\r\n\r\n`conv2 = conv(x, [5, 5, 3, 6]) `"}