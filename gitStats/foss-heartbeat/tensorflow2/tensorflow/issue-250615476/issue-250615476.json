{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12327", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12327/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12327/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12327/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12327", "id": 250615476, "node_id": "MDU6SXNzdWUyNTA2MTU0NzY=", "number": 12327, "title": "No half-precision support for DepthwiseConv2Native", "user": {"login": "guyrose3", "id": 12236146, "node_id": "MDQ6VXNlcjEyMjM2MTQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/12236146?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guyrose3", "html_url": "https://github.com/guyrose3", "followers_url": "https://api.github.com/users/guyrose3/followers", "following_url": "https://api.github.com/users/guyrose3/following{/other_user}", "gists_url": "https://api.github.com/users/guyrose3/gists{/gist_id}", "starred_url": "https://api.github.com/users/guyrose3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guyrose3/subscriptions", "organizations_url": "https://api.github.com/users/guyrose3/orgs", "repos_url": "https://api.github.com/users/guyrose3/repos", "events_url": "https://api.github.com/users/guyrose3/events{/privacy}", "received_events_url": "https://api.github.com/users/guyrose3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-08-16T12:50:42Z", "updated_at": "2017-11-08T20:41:04Z", "closed_at": "2017-11-08T20:41:04Z", "author_association": "NONE", "body_html": "<p>Hi, I'm trying to convert a network from float32 to float16.<br>\nConversion is done postmortem, for inference only.<br>\nI converted both weights and activation tensors to float16.<br>\nWhen trying to run inference, I get the following error:<br>\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'DepthwiseConv2dNative' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:<br>\ndevice='CPU'; label='neon'; T in [DT_FLOAT]<br>\ndevice='CPU'; T in [DT_DOUBLE]<br>\ndevice='CPU'; T in [DT_FLOAT]<br>\ndevice='GPU'; T in [DT_DOUBLE]<br>\ndevice='GPU'; T in [DT_FLOAT]</p>\n<p>Assuming that this op is not implemented for float16, what is the easiest way to bypass this issue?</p>\n<ul>\n<li>implement a python op (run-time is not very crucial)?</li>\n<li>implement a CPU version of this op?<br>\n-cast to f32 and cast back to f16?</li>\n</ul>\n<p>System details</p>\n<ul>\n<li>**OS Platform: Ubuntu 14.04</li>\n<li>**TensorFlow installed from binary</li>\n<li>**TensorFlow version 1.3.0-rc2</li>\n<li><strong>CUDA/cuDNN version</strong>: cuda 8.0, cudnn 6.0</li>\n</ul>", "body_text": "Hi, I'm trying to convert a network from float32 to float16.\nConversion is done postmortem, for inference only.\nI converted both weights and activation tensors to float16.\nWhen trying to run inference, I get the following error:\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'DepthwiseConv2dNative' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\ndevice='CPU'; label='neon'; T in [DT_FLOAT]\ndevice='CPU'; T in [DT_DOUBLE]\ndevice='CPU'; T in [DT_FLOAT]\ndevice='GPU'; T in [DT_DOUBLE]\ndevice='GPU'; T in [DT_FLOAT]\nAssuming that this op is not implemented for float16, what is the easiest way to bypass this issue?\n\nimplement a python op (run-time is not very crucial)?\nimplement a CPU version of this op?\n-cast to f32 and cast back to f16?\n\nSystem details\n\n**OS Platform: Ubuntu 14.04\n**TensorFlow installed from binary\n**TensorFlow version 1.3.0-rc2\nCUDA/cuDNN version: cuda 8.0, cudnn 6.0", "body": "Hi, I'm trying to convert a network from float32 to float16.\r\nConversion is done postmortem, for inference only.\r\nI converted both weights and activation tensors to float16.\r\nWhen trying to run inference, I get the following error:\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'DepthwiseConv2dNative' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  device='CPU'; label='neon'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n\r\nAssuming that this op is not implemented for float16, what is the easiest way to bypass this issue?\r\n- implement a python op (run-time is not very crucial)?\r\n- implement a CPU version of this op?\r\n-cast to f32 and cast back to f16?\r\n\r\nSystem details\r\n- **OS Platform: Ubuntu 14.04\r\n- **TensorFlow installed from binary\r\n- **TensorFlow version 1.3.0-rc2\r\n- **CUDA/cuDNN version**: cuda 8.0, cudnn 6.0\r\n\r\n\r\n"}