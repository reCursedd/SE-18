{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12510", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12510/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12510/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12510/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12510", "id": 252140095, "node_id": "MDU6SXNzdWUyNTIxNDAwOTU=", "number": 12510, "title": "Support layer wise batch normalization parameter in tensorflow/contrib estimators", "user": {"login": "king821221", "id": 1598436, "node_id": "MDQ6VXNlcjE1OTg0MzY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1598436?v=4", "gravatar_id": "", "url": "https://api.github.com/users/king821221", "html_url": "https://github.com/king821221", "followers_url": "https://api.github.com/users/king821221/followers", "following_url": "https://api.github.com/users/king821221/following{/other_user}", "gists_url": "https://api.github.com/users/king821221/gists{/gist_id}", "starred_url": "https://api.github.com/users/king821221/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/king821221/subscriptions", "organizations_url": "https://api.github.com/users/king821221/orgs", "repos_url": "https://api.github.com/users/king821221/repos", "events_url": "https://api.github.com/users/king821221/events{/privacy}", "received_events_url": "https://api.github.com/users/king821221/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-08-23T02:17:51Z", "updated_at": "2018-01-03T22:29:10Z", "closed_at": "2018-01-03T22:29:09Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:   yes</li>\n<li>**OS Platform and Distribution:    Linux Ubuntu 14.04)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: installed from source</li>\n<li><strong>TensorFlow version (use command below)</strong>:   1.2.0</li>\n<li><strong>Python version</strong>: Python 2.7.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Tensorflow's current DNN classifier, regressor do not provide support to plugin in  layer-wise normalization function. This issue is fired to provide support to add a layer-wise norm func parameter in their constructors.</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<p>Following is a snippet of my proposed update in DNNClassifier. In case this change passes your review, I will submit the full code change including updates to all other related estimators:</p>\n<p>diff<br>\n--git a/tensorflow/contrib/learn/python/learn/estimators/dnn.py b/tensorflow/contrib/learn/python/learn/estimators/dnn.py<br>\nindex cb15ef2..a3d9c01 100644<br>\n--- a/tensorflow/contrib/learn/python/learn/estimators/dnn.py<br>\n+++ b/tensorflow/contrib/learn/python/learn/estimators/dnn.py<br>\n@@ -127,6 +127,10 @@ def _dnn_model_fn(features, labels, mode, params, config=None):<br>\nparams.get(\"input_layer_min_slice_size\") or 64 &lt;&lt; 20)<br>\nnum_ps_replicas = config.num_ps_replicas if config else 0<br>\nembedding_lr_multipliers = params.get(\"embedding_lr_multipliers\", {})</p>\n<ul>\n<li>\n<p>layer_norm_func = params.get(\"layer_norm_func\")</p>\n</li>\n<li>\n<p>layer_norm_params = params.get(\"layer_norm_params\", {})</p>\n</li>\n<li></li>\n<li>\n<p>layer_norm_params[\"mode\"] = mode</p>\n<p>features = _get_feature_dict(features)<br>\nparent_scope = \"dnn\"<br>\n@@ -168,6 +172,8 @@ def _dnn_model_fn(features, labels, mode, params, config=None):<br>\nnet,<br>\nnum_hidden_units,<br>\nactivation_fn=activation_fn,</p>\n</li>\n<li>\n<pre><code>      normalizer_fn=layer_norm_func,\n</code></pre>\n</li>\n<li>\n<pre><code>      normalizer_params=layer_norm_params,\n       variables_collections=[parent_scope],\n       scope=hidden_layer_scope)\n   if dropout is not None and mode == model_fn.ModeKeys.TRAIN:\n</code></pre>\n</li>\n</ul>\n<p>@@ -297,6 +303,8 @@ class DNNClassifier(estimator.Estimator):<br>\nweight_column_name=None,<br>\noptimizer=None,<br>\nactivation_fn=nn.relu,</p>\n<ul>\n<li>\n<pre><code>         layer_norm_func=None,\n</code></pre>\n</li>\n<li>\n<pre><code>         layer_norm_params=None,\n          dropout=None,\n          gradient_clip_norm=None,\n          enable_centered_bias=False,\n</code></pre>\n</li>\n</ul>\n<p>@@ -372,6 +380,8 @@ class DNNClassifier(estimator.Estimator):<br>\n\"optimizer\": optimizer,<br>\n\"activation_fn\": activation_fn,<br>\n\"dropout\": dropout,</p>\n<ul>\n<li>\n<pre><code>      \"layer_norm_func\": layer_norm_func,\n</code></pre>\n</li>\n<li>\n<pre><code>      \"layer_norm_params\": layer_norm_params,\n       \"gradient_clip_norm\": gradient_clip_norm,\n       \"embedding_lr_multipliers\": embedding_lr_multipliers,\n       \"input_layer_min_slice_size\": input_layer_min_slice_size,\n</code></pre>\n</li>\n</ul>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):   yes\n**OS Platform and Distribution:    Linux Ubuntu 14.04)\nTensorFlow installed from (source or binary): installed from source\nTensorFlow version (use command below):   1.2.0\nPython version: Python 2.7.6\nBazel version (if compiling from source): 0.5.2\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nTensorflow's current DNN classifier, regressor do not provide support to plugin in  layer-wise normalization function. This issue is fired to provide support to add a layer-wise norm func parameter in their constructors.\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\nFollowing is a snippet of my proposed update in DNNClassifier. In case this change passes your review, I will submit the full code change including updates to all other related estimators:\ndiff\n--git a/tensorflow/contrib/learn/python/learn/estimators/dnn.py b/tensorflow/contrib/learn/python/learn/estimators/dnn.py\nindex cb15ef2..a3d9c01 100644\n--- a/tensorflow/contrib/learn/python/learn/estimators/dnn.py\n+++ b/tensorflow/contrib/learn/python/learn/estimators/dnn.py\n@@ -127,6 +127,10 @@ def _dnn_model_fn(features, labels, mode, params, config=None):\nparams.get(\"input_layer_min_slice_size\") or 64 << 20)\nnum_ps_replicas = config.num_ps_replicas if config else 0\nembedding_lr_multipliers = params.get(\"embedding_lr_multipliers\", {})\n\n\nlayer_norm_func = params.get(\"layer_norm_func\")\n\n\nlayer_norm_params = params.get(\"layer_norm_params\", {})\n\n\n\nlayer_norm_params[\"mode\"] = mode\nfeatures = _get_feature_dict(features)\nparent_scope = \"dnn\"\n@@ -168,6 +172,8 @@ def _dnn_model_fn(features, labels, mode, params, config=None):\nnet,\nnum_hidden_units,\nactivation_fn=activation_fn,\n\n\n      normalizer_fn=layer_norm_func,\n\n\n\n      normalizer_params=layer_norm_params,\n       variables_collections=[parent_scope],\n       scope=hidden_layer_scope)\n   if dropout is not None and mode == model_fn.ModeKeys.TRAIN:\n\n\n\n@@ -297,6 +303,8 @@ class DNNClassifier(estimator.Estimator):\nweight_column_name=None,\noptimizer=None,\nactivation_fn=nn.relu,\n\n\n         layer_norm_func=None,\n\n\n\n         layer_norm_params=None,\n          dropout=None,\n          gradient_clip_norm=None,\n          enable_centered_bias=False,\n\n\n\n@@ -372,6 +380,8 @@ class DNNClassifier(estimator.Estimator):\n\"optimizer\": optimizer,\n\"activation_fn\": activation_fn,\n\"dropout\": dropout,\n\n\n      \"layer_norm_func\": layer_norm_func,\n\n\n\n      \"layer_norm_params\": layer_norm_params,\n       \"gradient_clip_norm\": gradient_clip_norm,\n       \"embedding_lr_multipliers\": embedding_lr_multipliers,\n       \"input_layer_min_slice_size\": input_layer_min_slice_size,", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:   yes\r\n- **OS Platform and Distribution:    Linux Ubuntu 14.04)\r\n- **TensorFlow installed from (source or binary)**: installed from source\r\n- **TensorFlow version (use command below)**:   1.2.0\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nTensorflow's current DNN classifier, regressor do not provide support to plugin in  layer-wise normalization function. This issue is fired to provide support to add a layer-wise norm func parameter in their constructors.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nFollowing is a snippet of my proposed update in DNNClassifier. In case this change passes your review, I will submit the full code change including updates to all other related estimators:\r\n\r\ndiff\r\n--git a/tensorflow/contrib/learn/python/learn/estimators/dnn.py b/tensorflow/contrib/learn/python/learn/estimators/dnn.py\r\nindex cb15ef2..a3d9c01 100644\r\n--- a/tensorflow/contrib/learn/python/learn/estimators/dnn.py\r\n+++ b/tensorflow/contrib/learn/python/learn/estimators/dnn.py\r\n@@ -127,6 +127,10 @@ def _dnn_model_fn(features, labels, mode, params, config=None):\r\n       params.get(\"input_layer_min_slice_size\") or 64 << 20)\r\n   num_ps_replicas = config.num_ps_replicas if config else 0\r\n   embedding_lr_multipliers = params.get(\"embedding_lr_multipliers\", {})\r\n+  layer_norm_func = params.get(\"layer_norm_func\")\r\n+  layer_norm_params = params.get(\"layer_norm_params\", {})\r\n+\r\n+  layer_norm_params[\"mode\"] = mode\r\n \r\n   features = _get_feature_dict(features)\r\n   parent_scope = \"dnn\"\r\n@@ -168,6 +172,8 @@ def _dnn_model_fn(features, labels, mode, params, config=None):\r\n             net,\r\n             num_hidden_units,\r\n             activation_fn=activation_fn,\r\n+           normalizer_fn=layer_norm_func,\r\n+           normalizer_params=layer_norm_params,\r\n             variables_collections=[parent_scope],\r\n             scope=hidden_layer_scope)\r\n         if dropout is not None and mode == model_fn.ModeKeys.TRAIN:\r\n@@ -297,6 +303,8 @@ class DNNClassifier(estimator.Estimator):\r\n                weight_column_name=None,\r\n                optimizer=None,\r\n                activation_fn=nn.relu,\r\n+              layer_norm_func=None,\r\n+              layer_norm_params=None,\r\n                dropout=None,\r\n                gradient_clip_norm=None,\r\n                enable_centered_bias=False,\r\n@@ -372,6 +380,8 @@ class DNNClassifier(estimator.Estimator):\r\n             \"optimizer\": optimizer,\r\n             \"activation_fn\": activation_fn,\r\n             \"dropout\": dropout,\r\n+           \"layer_norm_func\": layer_norm_func,\r\n+           \"layer_norm_params\": layer_norm_params,\r\n             \"gradient_clip_norm\": gradient_clip_norm,\r\n             \"embedding_lr_multipliers\": embedding_lr_multipliers,\r\n             \"input_layer_min_slice_size\": input_layer_min_slice_size,\r\n\r\n\r\n"}