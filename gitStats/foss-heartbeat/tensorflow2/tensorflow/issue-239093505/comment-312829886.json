{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312829886", "html_url": "https://github.com/tensorflow/tensorflow/issues/11103#issuecomment-312829886", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11103", "id": 312829886, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjgyOTg4Ng==", "user": {"login": "zaccharieramzi", "id": 6387497, "node_id": "MDQ6VXNlcjYzODc0OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/6387497?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zaccharieramzi", "html_url": "https://github.com/zaccharieramzi", "followers_url": "https://api.github.com/users/zaccharieramzi/followers", "following_url": "https://api.github.com/users/zaccharieramzi/following{/other_user}", "gists_url": "https://api.github.com/users/zaccharieramzi/gists{/gist_id}", "starred_url": "https://api.github.com/users/zaccharieramzi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zaccharieramzi/subscriptions", "organizations_url": "https://api.github.com/users/zaccharieramzi/orgs", "repos_url": "https://api.github.com/users/zaccharieramzi/repos", "events_url": "https://api.github.com/users/zaccharieramzi/events{/privacy}", "received_events_url": "https://api.github.com/users/zaccharieramzi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-04T09:26:06Z", "updated_at": "2017-07-04T09:26:56Z", "author_association": "NONE", "body_html": "<p>Sorry to answer only now. I understand the fix that you suggested and had applied it.<br>\nHowever, I don't understand with your explanation how the following works:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">28</span>])\nx_test <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">28</span>])\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>forward<span class=\"pl-pds\">\"</span></span>):\n        normalized_x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(\n            x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>forward<span class=\"pl-pds\">\"</span></span>):\n        normalized_x_test <span class=\"pl-k\">=</span> tf.layers.batch_normalization(\n            x_test, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p>and this doesn't:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">28</span>])\nx_test <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">28</span>])\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>forward<span class=\"pl-pds\">\"</span></span>):\n        normalized_x <span class=\"pl-k\">=</span> tf.layers.batch_normalization(\n            x, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n        normalized_x_test <span class=\"pl-k\">=</span> tf.layers.batch_normalization(\n            x_test, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p>If I follow your explanation correctly, both shouldn't work, right?</p>", "body_text": "Sorry to answer only now. I understand the fix that you suggested and had applied it.\nHowever, I don't understand with your explanation how the following works:\nimport tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 28])\nx_test = tf.placeholder(tf.float32, [None, 28])\nwith tf.variable_scope(\"forward\"):\n        normalized_x = tf.layers.batch_normalization(\n            x, training=True, reuse=None)\nwith tf.variable_scope(\"forward\"):\n        normalized_x_test = tf.layers.batch_normalization(\n            x_test, training=False, reuse=True)\nand this doesn't:\nimport tensorflow as tf\nx = tf.placeholder(tf.float32, [None, 28])\nx_test = tf.placeholder(tf.float32, [None, 28])\nwith tf.variable_scope(\"forward\"):\n        normalized_x = tf.layers.batch_normalization(\n            x, training=True, reuse=None)\n        normalized_x_test = tf.layers.batch_normalization(\n            x_test, training=False, reuse=True)\nIf I follow your explanation correctly, both shouldn't work, right?", "body": "Sorry to answer only now. I understand the fix that you suggested and had applied it.\r\nHowever, I don't understand with your explanation how the following works:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.placeholder(tf.float32, [None, 28])\r\nx_test = tf.placeholder(tf.float32, [None, 28])\r\nwith tf.variable_scope(\"forward\"):\r\n        normalized_x = tf.layers.batch_normalization(\r\n            x, training=True, reuse=None)\r\nwith tf.variable_scope(\"forward\"):\r\n        normalized_x_test = tf.layers.batch_normalization(\r\n            x_test, training=False, reuse=True)\r\n```\r\n and this doesn't:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.placeholder(tf.float32, [None, 28])\r\nx_test = tf.placeholder(tf.float32, [None, 28])\r\nwith tf.variable_scope(\"forward\"):\r\n        normalized_x = tf.layers.batch_normalization(\r\n            x, training=True, reuse=None)\r\n        normalized_x_test = tf.layers.batch_normalization(\r\n            x_test, training=False, reuse=True)\r\n```\r\nIf I follow your explanation correctly, both shouldn't work, right?\r\n"}