{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19547", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19547/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19547/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19547/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19547", "id": 326336450, "node_id": "MDU6SXNzdWUzMjYzMzY0NTA=", "number": 19547, "title": "TFServing returns inconsistent predict result under stress load", "user": {"login": "stoneyan", "id": 10360693, "node_id": "MDQ6VXNlcjEwMzYwNjkz", "avatar_url": "https://avatars0.githubusercontent.com/u/10360693?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stoneyan", "html_url": "https://github.com/stoneyan", "followers_url": "https://api.github.com/users/stoneyan/followers", "following_url": "https://api.github.com/users/stoneyan/following{/other_user}", "gists_url": "https://api.github.com/users/stoneyan/gists{/gist_id}", "starred_url": "https://api.github.com/users/stoneyan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stoneyan/subscriptions", "organizations_url": "https://api.github.com/users/stoneyan/orgs", "repos_url": "https://api.github.com/users/stoneyan/repos", "events_url": "https://api.github.com/users/stoneyan/events{/privacy}", "received_events_url": "https://api.github.com/users/stoneyan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-24T23:55:31Z", "updated_at": "2018-05-25T17:33:07Z", "closed_at": "2018-05-25T17:33:06Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNo</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</li>\n<li><strong>TensorFlow version (use command below)</strong>: latest</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.4</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:CPU only</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>We trained a model and use TFServing to Predict API to return the matched result. When we run our test scripts in single threaded, we can get the same result every time. But when we run the scripts in 2 threads, or run them in multiple machines, sometimes we the returned result is slightly (10%) different from the original one. The more threads we use, the more frequent the result become inconsistent. After we added a lock/unlock when calling \"TF_RETURN_IF_ERROR(bundle-&gt;session-&gt;Run(run_options, input_tensors, output_tensor_names, {}, &amp;outputs, &amp;run_metadata));\" in SaveModelPredict() in tensorflow_serving/servables/tensorflow/predict_impl.cc, the issue got suppressed.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 14.04\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below): latest\nPython version: 2.7\nBazel version (if compiling from source): 0.5.4\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:CPU only\nExact command to reproduce:\n\nDescribe the problem\nWe trained a model and use TFServing to Predict API to return the matched result. When we run our test scripts in single threaded, we can get the same result every time. But when we run the scripts in 2 threads, or run them in multiple machines, sometimes we the returned result is slightly (10%) different from the original one. The more threads we use, the more frequent the result become inconsistent. After we added a lock/unlock when calling \"TF_RETURN_IF_ERROR(bundle->session->Run(run_options, input_tensors, output_tensor_names, {}, &outputs, &run_metadata));\" in SaveModelPredict() in tensorflow_serving/servables/tensorflow/predict_impl.cc, the issue got suppressed.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**: latest\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:CPU only\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWe trained a model and use TFServing to Predict API to return the matched result. When we run our test scripts in single threaded, we can get the same result every time. But when we run the scripts in 2 threads, or run them in multiple machines, sometimes we the returned result is slightly (10%) different from the original one. The more threads we use, the more frequent the result become inconsistent. After we added a lock/unlock when calling \"TF_RETURN_IF_ERROR(bundle->session->Run(run_options, input_tensors, output_tensor_names, {}, &outputs, &run_metadata));\" in SaveModelPredict() in tensorflow_serving/servables/tensorflow/predict_impl.cc, the issue got suppressed.\r\n\r\n"}