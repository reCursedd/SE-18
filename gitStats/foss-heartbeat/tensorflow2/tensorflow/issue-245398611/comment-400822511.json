{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/400822511", "html_url": "https://github.com/tensorflow/tensorflow/pull/11749#issuecomment-400822511", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11749", "id": 400822511, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDgyMjUxMQ==", "user": {"login": "loshchil", "id": 3809019, "node_id": "MDQ6VXNlcjM4MDkwMTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/3809019?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loshchil", "html_url": "https://github.com/loshchil", "followers_url": "https://api.github.com/users/loshchil/followers", "following_url": "https://api.github.com/users/loshchil/following{/other_user}", "gists_url": "https://api.github.com/users/loshchil/gists{/gist_id}", "starred_url": "https://api.github.com/users/loshchil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loshchil/subscriptions", "organizations_url": "https://api.github.com/users/loshchil/orgs", "repos_url": "https://api.github.com/users/loshchil/repos", "events_url": "https://api.github.com/users/loshchil/events{/privacy}", "received_events_url": "https://api.github.com/users/loshchil/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-27T20:43:19Z", "updated_at": "2018-06-27T20:43:19Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=959847\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martin-gorner\">@martin-gorner</a> To my best understanding, the original function mentioned here was removed in <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/65a11ef710083c106fb5479145b3b8e133f79b1d/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/65a11ef710083c106fb5479145b3b8e133f79b1d\"><tt>65a11ef</tt></a> after <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=21221121\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PatrykChrabaszcz\">@PatrykChrabaszcz</a> introduced (please see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"273240660\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/14500\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/14500/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/14500\">#14500</a> ) cosine_decay function and cosine_decay_restarts which implements cosine annealing with restarts.</p>\n<p>The paper is referenced correctly because this is where cosine decay was proposed. Cosine decay  does not automatically assume restarts, it defines how learning rate decays / the shape of its decrease. SGDR represents SGD with restarts employing cosine annealing for each of its cycles by default (SGDR will remain SGDR if you replace cosine annealing for each cycle with an alternative decay scheme such as  piecewise_constant).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=21221121\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PatrykChrabaszcz\">@PatrykChrabaszcz</a> could you please double-check my assessment and (if you agree with it) add cosine_decay_restarts to <a href=\"https://www.tensorflow.org/api_guides/python/train#Decaying_the_learning_rate\" rel=\"nofollow\">https://www.tensorflow.org/api_guides/python/train#Decaying_the_learning_rate</a><br>\nonce you have some spare time?</p>", "body_text": "@martin-gorner To my best understanding, the original function mentioned here was removed in 65a11ef after @PatrykChrabaszcz introduced (please see #14500 ) cosine_decay function and cosine_decay_restarts which implements cosine annealing with restarts.\nThe paper is referenced correctly because this is where cosine decay was proposed. Cosine decay  does not automatically assume restarts, it defines how learning rate decays / the shape of its decrease. SGDR represents SGD with restarts employing cosine annealing for each of its cycles by default (SGDR will remain SGDR if you replace cosine annealing for each cycle with an alternative decay scheme such as  piecewise_constant).\n@PatrykChrabaszcz could you please double-check my assessment and (if you agree with it) add cosine_decay_restarts to https://www.tensorflow.org/api_guides/python/train#Decaying_the_learning_rate\nonce you have some spare time?", "body": "@martin-gorner To my best understanding, the original function mentioned here was removed in https://github.com/tensorflow/tensorflow/pull/14500/commits/65a11ef710083c106fb5479145b3b8e133f79b1d after @PatrykChrabaszcz introduced (please see https://github.com/tensorflow/tensorflow/pull/14500 ) cosine_decay function and cosine_decay_restarts which implements cosine annealing with restarts.\r\n\r\nThe paper is referenced correctly because this is where cosine decay was proposed. Cosine decay  does not automatically assume restarts, it defines how learning rate decays / the shape of its decrease. SGDR represents SGD with restarts employing cosine annealing for each of its cycles by default (SGDR will remain SGDR if you replace cosine annealing for each cycle with an alternative decay scheme such as  piecewise_constant).  \r\n\r\n@PatrykChrabaszcz could you please double-check my assessment and (if you agree with it) add cosine_decay_restarts to https://www.tensorflow.org/api_guides/python/train#Decaying_the_learning_rate\r\nonce you have some spare time? "}