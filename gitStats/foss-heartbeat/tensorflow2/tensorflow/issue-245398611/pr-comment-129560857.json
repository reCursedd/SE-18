{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/129560857", "pull_request_review_id": 52346021, "id": 129560857, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyOTU2MDg1Nw==", "diff_hunk": "@@ -0,0 +1,146 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"SGDR learning rate decay function.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import math\n+\n+from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import math_ops, control_flow_ops\n+\n+\n+def sgdr_decay(learning_rate, global_step, t_0=1000, t_mul=1, m_mul=1,\n+               name=None):\n+  \"\"\" This procedure implements Stochastic Gradient Descent with Warm\n+  Restarts (SGDR) as described in \"SGDR: Stochastic Gradient Descent\n+  with Warm Restarts\" by Ilya Loshchilov & Frank Hutter, Proceedings of\n+  ICLR'2017, available at https://arxiv.org/pdf/1608.03983.pdf\n+  The basic idea of the algorithm is the following.\n+  The learning rate decreases according to cosine annealing:\n+\n+  ```python\n+  learning_rate * 0.5 * (1 + cos(x_val * pi)) # for x_val defined in [0, 1]\n+  ```\n+\n+  Thus, in the initial run (when the restart index i = 0),\n+  the learning rate decreases from the initial learning rate\n+  `learning_rate` (when `x_val=0`, we get `cos(0)=1`) to\n+  0 (when `x_val=1`, we get `cos(pi)=-1`).\n+  The decrease within i-th restart takes `t_i` steps,\n+  while `t_0` is user-defined.\n+  Then, we perform the first restart (i=1) by setting the learning rate to\n+  `learning_rate*(m_mul^i)`, where `m_mul in [0,1]` (set to 1 by default).\n+  Also, every restart runs for `t_i=t_0*(t_mul^i)` steps, i.e., every new\n+  restart runs for `t_mul` longer than the previous one.\n+\n+  Importantly, when one has no access to a validation set, SGDR suggests\n+  to report the best expected / recommended solution in the following way.\n+  When we are within our initial run (i=0), every new solution represents\n+  SGDR's recommended solution. Instead, when i>0, the recommended solution is\n+  the one obtained at the end of each restart,\n+  i.e., when the learning rate is 0.\n+\n+  Note that the minimum learning rate is set to 0 for simplicity,\n+  you can adjust the code to deal with any positive minimum learning rate\n+  as defined in the paper.\n+\n+  ```\n+  Args:\n+    learning_rate: A scalar `float32` or `float64` `Tensor` or a\n+      Python number.  The initial learning rate.\n+    global_step: A scalar `int32` or `int64` `Tensor` or a Python number.\n+      Global step to use for the decay computation.  Must not be negative.\n+    t_0: A scalar `int32` or `int64` `Tensor` or a Python number.\n+      Must be positive.  Number of iterations in the first restart,\n+      it can be set to a multiplicative of the number of batches per epoch.\n+      Defaults to 1\n+    t_mul: A scalar `int32` or `int64` `Tensor` or a Python number.\n+      Must be positive.\n+      Used to derive the number of iterations in the i-th restart:\n+      `t_0 * (t_mul^i)`. Defaults to 1", "path": "tensorflow/contrib/training/python/training/sgdr_learning_rate_decay.py", "position": null, "original_position": 76, "commit_id": "d330eb5eac6ba848842ee6966fa47cccefed33d9", "original_commit_id": "871a2c05b14decbff7b67b0ff0a2133a9ac10e72", "user": {"login": "PatrykChrabaszcz", "id": 21221121, "node_id": "MDQ6VXNlcjIxMjIxMTIx", "avatar_url": "https://avatars1.githubusercontent.com/u/21221121?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PatrykChrabaszcz", "html_url": "https://github.com/PatrykChrabaszcz", "followers_url": "https://api.github.com/users/PatrykChrabaszcz/followers", "following_url": "https://api.github.com/users/PatrykChrabaszcz/following{/other_user}", "gists_url": "https://api.github.com/users/PatrykChrabaszcz/gists{/gist_id}", "starred_url": "https://api.github.com/users/PatrykChrabaszcz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PatrykChrabaszcz/subscriptions", "organizations_url": "https://api.github.com/users/PatrykChrabaszcz/orgs", "repos_url": "https://api.github.com/users/PatrykChrabaszcz/repos", "events_url": "https://api.github.com/users/PatrykChrabaszcz/events{/privacy}", "received_events_url": "https://api.github.com/users/PatrykChrabaszcz/received_events", "type": "User", "site_admin": false}, "body": "Ok", "created_at": "2017-07-26T12:47:51Z", "updated_at": "2017-08-08T19:53:22Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11749#discussion_r129560857", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11749", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/129560857"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11749#discussion_r129560857"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11749"}}, "body_html": "<p>Ok</p>", "body_text": "Ok", "in_reply_to_id": 129476373}