{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/327562271", "html_url": "https://github.com/tensorflow/tensorflow/issues/12816#issuecomment-327562271", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12816", "id": 327562271, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzU2MjI3MQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-06T17:50:42Z", "updated_at": "2017-09-06T17:50:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=30011671\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/juventi\">@juventi</a> this is a general issue not specific to TensorFlow -- you get better utilization when dealing with large batches. For instance large matrix multiply can be 10x more efficient, in terms of ops/sec than small matrix multiplies. It has to do with underlying hardware organizing computation more efficiently, ie, by using SRAM cache, this is not something you can directly control from TensorFlow client side.</p>", "body_text": "@juventi this is a general issue not specific to TensorFlow -- you get better utilization when dealing with large batches. For instance large matrix multiply can be 10x more efficient, in terms of ops/sec than small matrix multiplies. It has to do with underlying hardware organizing computation more efficiently, ie, by using SRAM cache, this is not something you can directly control from TensorFlow client side.", "body": "@juventi this is a general issue not specific to TensorFlow -- you get better utilization when dealing with large batches. For instance large matrix multiply can be 10x more efficient, in terms of ops/sec than small matrix multiplies. It has to do with underlying hardware organizing computation more efficiently, ie, by using SRAM cache, this is not something you can directly control from TensorFlow client side. "}