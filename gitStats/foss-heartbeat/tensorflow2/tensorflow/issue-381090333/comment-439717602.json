{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439717602", "html_url": "https://github.com/tensorflow/tensorflow/issues/23766#issuecomment-439717602", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23766", "id": 439717602, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTcxNzYwMg==", "user": {"login": "jdduke", "id": 479117, "node_id": "MDQ6VXNlcjQ3OTExNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/479117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdduke", "html_url": "https://github.com/jdduke", "followers_url": "https://api.github.com/users/jdduke/followers", "following_url": "https://api.github.com/users/jdduke/following{/other_user}", "gists_url": "https://api.github.com/users/jdduke/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdduke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdduke/subscriptions", "organizations_url": "https://api.github.com/users/jdduke/orgs", "repos_url": "https://api.github.com/users/jdduke/repos", "events_url": "https://api.github.com/users/jdduke/events{/privacy}", "received_events_url": "https://api.github.com/users/jdduke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-18T19:14:13Z", "updated_at": "2018-11-18T19:14:13Z", "author_association": "MEMBER", "body_html": "<p>The initial inference run can take longer than subsequent runs, as it can trigger one-time execution of memory allocation and op preparation. This is expected.</p>\n<p>Let us know if using a ByteBuffer helps. Otherwise, we'd be happy to take a closer look if you're willing to privately attach and send the model you're using. A good subset of TensorFlow Lite ops are highly optimized, but not all of them, and it's possible there's a bottlenecking op in your graph.</p>", "body_text": "The initial inference run can take longer than subsequent runs, as it can trigger one-time execution of memory allocation and op preparation. This is expected.\nLet us know if using a ByteBuffer helps. Otherwise, we'd be happy to take a closer look if you're willing to privately attach and send the model you're using. A good subset of TensorFlow Lite ops are highly optimized, but not all of them, and it's possible there's a bottlenecking op in your graph.", "body": "The initial inference run can take longer than subsequent runs, as it can trigger one-time execution of memory allocation and op preparation. This is expected.\r\n\r\nLet us know if using a ByteBuffer helps. Otherwise, we'd be happy to take a closer look if you're willing to privately attach and send the model you're using. A good subset of TensorFlow Lite ops are highly optimized, but not all of them, and it's possible there's a bottlenecking op in your graph."}