{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/193651606", "html_url": "https://github.com/tensorflow/tensorflow/issues/1390#issuecomment-193651606", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1390", "id": 193651606, "node_id": "MDEyOklzc3VlQ29tbWVudDE5MzY1MTYwNg==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-08T07:43:30Z", "updated_at": "2016-03-08T07:43:30Z", "author_association": "MEMBER", "body_html": "<p>I tried to replicate your problem, but I can't -- I don't have a multi-gpu machine and, even though I tried, I cannot replicate the problem in a single-gpu setting in any way. In fact I even managed to run \"python cifar10_multi_gpu_train.py --num_gpus=2\" with your change and it was all ok, but I guess it just soft-placed everything on GPU0. One problem I can think of is that in your code your LSTM variables will not be placed on CPU, like the other ones. Can you try to put the whole LSTM on cpu first, just for a test (using tf.device around it)? If that works, then you can use pin_variables_to_cpu in your tf.device to just put the variables there, I hope. (It did not work once, I guess that's why cifar10 has _variable_on_cpu, but I think it's ok now.)</p>", "body_text": "I tried to replicate your problem, but I can't -- I don't have a multi-gpu machine and, even though I tried, I cannot replicate the problem in a single-gpu setting in any way. In fact I even managed to run \"python cifar10_multi_gpu_train.py --num_gpus=2\" with your change and it was all ok, but I guess it just soft-placed everything on GPU0. One problem I can think of is that in your code your LSTM variables will not be placed on CPU, like the other ones. Can you try to put the whole LSTM on cpu first, just for a test (using tf.device around it)? If that works, then you can use pin_variables_to_cpu in your tf.device to just put the variables there, I hope. (It did not work once, I guess that's why cifar10 has _variable_on_cpu, but I think it's ok now.)", "body": "I tried to replicate your problem, but I can't -- I don't have a multi-gpu machine and, even though I tried, I cannot replicate the problem in a single-gpu setting in any way. In fact I even managed to run \"python cifar10_multi_gpu_train.py --num_gpus=2\" with your change and it was all ok, but I guess it just soft-placed everything on GPU0. One problem I can think of is that in your code your LSTM variables will not be placed on CPU, like the other ones. Can you try to put the whole LSTM on cpu first, just for a test (using tf.device around it)? If that works, then you can use pin_variables_to_cpu in your tf.device to just put the variables there, I hope. (It did not work once, I guess that's why cifar10 has _variable_on_cpu, but I think it's ok now.)\n"}