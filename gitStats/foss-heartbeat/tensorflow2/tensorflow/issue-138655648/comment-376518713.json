{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/376518713", "html_url": "https://github.com/tensorflow/tensorflow/issues/1390#issuecomment-376518713", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1390", "id": 376518713, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjUxODcxMw==", "user": {"login": "menasria", "id": 17971185, "node_id": "MDQ6VXNlcjE3OTcxMTg1", "avatar_url": "https://avatars0.githubusercontent.com/u/17971185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/menasria", "html_url": "https://github.com/menasria", "followers_url": "https://api.github.com/users/menasria/followers", "following_url": "https://api.github.com/users/menasria/following{/other_user}", "gists_url": "https://api.github.com/users/menasria/gists{/gist_id}", "starred_url": "https://api.github.com/users/menasria/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/menasria/subscriptions", "organizations_url": "https://api.github.com/users/menasria/orgs", "repos_url": "https://api.github.com/users/menasria/repos", "events_url": "https://api.github.com/users/menasria/events{/privacy}", "received_events_url": "https://api.github.com/users/menasria/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-27T13:09:56Z", "updated_at": "2018-03-27T13:09:56Z", "author_association": "NONE", "body_html": "<p>I have the same problem, could you help me to solve it<br>\nmy code is :</p>\n<hr>\n<p>train_count = len(X_train)<br>\nN_HIDDEN_UNITS = 64</p>\n<p>lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(N_HIDDEN_UNITS)<br>\nlstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(N_HIDDEN_UNITS)</p>\n<p>def shared_layer(input_data):<br>\nprint('i m in shared layer')<br>\nprint(input_data)<br>\n(output_fw, output_bw), (output_state_fw, output_state_bw) = tf.nn.bidirectional_dynamic_rnn(<br>\nlstm_fw_cell,<br>\nlstm_bw_cell,<br>\ninput_data,<br>\nsequence_length=None,<br>\ninitial_state_fw=None,<br>\ninitial_state_bw=None,<br>\ndtype=tf.float32,<br>\nparallel_iterations=None,<br>\nswap_memory=False,<br>\ntime_major=False,<br>\nscope=None<br>\n)<br>\noutputs = tf.concat(axis=2, values=[output_fw, output_bw])<br>\nreturn outputs</p>\n<p>tf.get_default_graph()<br>\nX = tf.placeholder(tf.float32, [BATCH_SIZE, N_TIME_STEPS,N_FEATURES],name=\"input\")<br>\n#Initialize the variables<br>\ninit = tf.global_variables_initializer()</p>\n<h1>launch the graph</h1>\n<p>with tf.Session() as sess:<br>\nsess.run(init)<br>\nfor start, end in zip(range(0, train_count, BATCH_SIZE),<br>\nrange(BATCH_SIZE, train_count + 1,BATCH_SIZE)):<br>\nsess.run(shared_layer(X), feed_dict={X: X_train[start:end]})</p>\n<hr>\n<p>the shared layer function run for tensor(input:0) but it does not for the next tensor, it araise the folowing error :<br>\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value bidirectional_rnn/bw/basic_lstm_cell/kernel<br>\n[[Node: bidirectional_rnn/bw/basic_lstm_cell/kernel/read = Identity<a href=\"bidirectional_rnn/bw/basic_lstm_cell/kernel\">T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"</a>]]</p>", "body_text": "I have the same problem, could you help me to solve it\nmy code is :\n\ntrain_count = len(X_train)\nN_HIDDEN_UNITS = 64\nlstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(N_HIDDEN_UNITS)\nlstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(N_HIDDEN_UNITS)\ndef shared_layer(input_data):\nprint('i m in shared layer')\nprint(input_data)\n(output_fw, output_bw), (output_state_fw, output_state_bw) = tf.nn.bidirectional_dynamic_rnn(\nlstm_fw_cell,\nlstm_bw_cell,\ninput_data,\nsequence_length=None,\ninitial_state_fw=None,\ninitial_state_bw=None,\ndtype=tf.float32,\nparallel_iterations=None,\nswap_memory=False,\ntime_major=False,\nscope=None\n)\noutputs = tf.concat(axis=2, values=[output_fw, output_bw])\nreturn outputs\ntf.get_default_graph()\nX = tf.placeholder(tf.float32, [BATCH_SIZE, N_TIME_STEPS,N_FEATURES],name=\"input\")\n#Initialize the variables\ninit = tf.global_variables_initializer()\nlaunch the graph\nwith tf.Session() as sess:\nsess.run(init)\nfor start, end in zip(range(0, train_count, BATCH_SIZE),\nrange(BATCH_SIZE, train_count + 1,BATCH_SIZE)):\nsess.run(shared_layer(X), feed_dict={X: X_train[start:end]})\n\nthe shared layer function run for tensor(input:0) but it does not for the next tensor, it araise the folowing error :\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value bidirectional_rnn/bw/basic_lstm_cell/kernel\n[[Node: bidirectional_rnn/bw/basic_lstm_cell/kernel/read = IdentityT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]", "body": "I have the same problem, could you help me to solve it \r\nmy code is :\r\n****\r\ntrain_count = len(X_train)\r\nN_HIDDEN_UNITS = 64\r\n\r\nlstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(N_HIDDEN_UNITS)\r\nlstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(N_HIDDEN_UNITS)\r\n\r\ndef shared_layer(input_data):\r\n    print('i m in shared layer')\r\n    print(input_data)\r\n    (output_fw, output_bw), (output_state_fw, output_state_bw) = tf.nn.bidirectional_dynamic_rnn(\r\n                lstm_fw_cell,\r\n                lstm_bw_cell,\r\n                input_data,\r\n                sequence_length=None,\r\n                initial_state_fw=None,\r\n                initial_state_bw=None,\r\n                dtype=tf.float32,\r\n                parallel_iterations=None,\r\n                swap_memory=False,\r\n                time_major=False,\r\n                scope=None\r\n            )\r\n    outputs = tf.concat(axis=2, values=[output_fw, output_bw])\r\n    return outputs\r\n\r\ntf.get_default_graph()\r\nX = tf.placeholder(tf.float32, [BATCH_SIZE, N_TIME_STEPS,N_FEATURES],name=\"input\")\r\n#Initialize the variables\r\ninit = tf.global_variables_initializer()\r\n# launch the graph\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    for start, end in zip(range(0, train_count, BATCH_SIZE),\r\n                          range(BATCH_SIZE, train_count + 1,BATCH_SIZE)):\r\n        sess.run(shared_layer(X), feed_dict={X: X_train[start:end]})\r\n****\r\nthe shared layer function run for tensor(input:0) but it does not for the next tensor, it araise the folowing error : \r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value bidirectional_rnn/bw/basic_lstm_cell/kernel\r\n\t [[Node: bidirectional_rnn/bw/basic_lstm_cell/kernel/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](bidirectional_rnn/bw/basic_lstm_cell/kernel)]]\r\n\r\n"}