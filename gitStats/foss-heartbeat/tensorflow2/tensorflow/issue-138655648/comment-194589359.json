{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/194589359", "html_url": "https://github.com/tensorflow/tensorflow/issues/1390#issuecomment-194589359", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1390", "id": 194589359, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NDU4OTM1OQ==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-10T00:37:04Z", "updated_at": "2016-03-10T00:37:04Z", "author_association": "MEMBER", "body_html": "<p>I don't think there are any bugs in LSTM. Variable placement in a multi-device or multi-machine setting is hard, but it's solved by tf.device. We do not want to have parameters for this in LSTM or any other function, as we'd basically need to add the same code to every function we write. It is solved differently using tf.device -- you can use a tf.device context around anything you want, and it will put variables (or any other ops you wish) on any device you specify. Here is an example.<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/graph_util_test.py#L57\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/graph_util_test.py#L57</a></p>", "body_text": "I don't think there are any bugs in LSTM. Variable placement in a multi-device or multi-machine setting is hard, but it's solved by tf.device. We do not want to have parameters for this in LSTM or any other function, as we'd basically need to add the same code to every function we write. It is solved differently using tf.device -- you can use a tf.device context around anything you want, and it will put variables (or any other ops you wish) on any device you specify. Here is an example.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/graph_util_test.py#L57", "body": "I don't think there are any bugs in LSTM. Variable placement in a multi-device or multi-machine setting is hard, but it's solved by tf.device. We do not want to have parameters for this in LSTM or any other function, as we'd basically need to add the same code to every function we write. It is solved differently using tf.device -- you can use a tf.device context around anything you want, and it will put variables (or any other ops you wish) on any device you specify. Here is an example.\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/graph_util_test.py#L57\n"}