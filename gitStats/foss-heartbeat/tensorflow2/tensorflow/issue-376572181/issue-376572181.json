{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23434", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23434/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23434/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23434/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23434", "id": 376572181, "node_id": "MDU6SXNzdWUzNzY1NzIxODE=", "number": 23434, "title": "Windows cluster: could not start gRPC server", "user": {"login": "ivan-marroquin", "id": 28459534, "node_id": "MDQ6VXNlcjI4NDU5NTM0", "avatar_url": "https://avatars1.githubusercontent.com/u/28459534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ivan-marroquin", "html_url": "https://github.com/ivan-marroquin", "followers_url": "https://api.github.com/users/ivan-marroquin/followers", "following_url": "https://api.github.com/users/ivan-marroquin/following{/other_user}", "gists_url": "https://api.github.com/users/ivan-marroquin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ivan-marroquin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ivan-marroquin/subscriptions", "organizations_url": "https://api.github.com/users/ivan-marroquin/orgs", "repos_url": "https://api.github.com/users/ivan-marroquin/repos", "events_url": "https://api.github.com/users/ivan-marroquin/events{/privacy}", "received_events_url": "https://api.github.com/users/ivan-marroquin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": {"login": "gunan", "id": 7946809, "node_id": "MDQ6VXNlcjc5NDY4MDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/7946809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gunan", "html_url": "https://github.com/gunan", "followers_url": "https://api.github.com/users/gunan/followers", "following_url": "https://api.github.com/users/gunan/following{/other_user}", "gists_url": "https://api.github.com/users/gunan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gunan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gunan/subscriptions", "organizations_url": "https://api.github.com/users/gunan/orgs", "repos_url": "https://api.github.com/users/gunan/repos", "events_url": "https://api.github.com/users/gunan/events{/privacy}", "received_events_url": "https://api.github.com/users/gunan/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gunan", "id": 7946809, "node_id": "MDQ6VXNlcjc5NDY4MDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/7946809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gunan", "html_url": "https://github.com/gunan", "followers_url": "https://api.github.com/users/gunan/followers", "following_url": "https://api.github.com/users/gunan/following{/other_user}", "gists_url": "https://api.github.com/users/gunan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gunan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gunan/subscriptions", "organizations_url": "https://api.github.com/users/gunan/orgs", "repos_url": "https://api.github.com/users/gunan/repos", "events_url": "https://api.github.com/users/gunan/events{/privacy}", "received_events_url": "https://api.github.com/users/gunan/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-11-01T21:10:19Z", "updated_at": "2018-11-13T21:44:20Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a bug. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:</li>\n<li>TensorFlow installed from (source or binary): binary</li>\n<li>TensorFlow version (use command below): 1.8.0</li>\n<li>Python version: 3.6.5</li>\n<li>Bazel version (if compiling from source): n/a</li>\n<li>GCC/Compiler version (if compiling from source): n/a</li>\n<li>CUDA/cuDNN version: 9</li>\n<li>GPU model and memory:  Quadro K22000 with 3.34GB</li>\n</ul>\n<p>You can collect some of this information using our environment capture <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">script</a><br>\nYou can also obtain the TensorFlow version with<br>\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<p><strong>Describe the current behavior</strong><br>\nI have a cluster running on Windows 10 and wanted to test tensorflow on a distributed environment. As a test, I want to detect the video card installed on 3 machines. When I ran the code, I get the following error messages:</p>\n<p>E1101 16:04:32.821000000 17460 server_chttp2.cc:40] {\"created\":\"@1541106272.820000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541106272.820000000\",\"description\":\"Failed to add port to server\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"created\":\"@1541106272.820000000\",\"description\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage of each socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\":10048}]}]}<br>\nProcess Process-4:<br>\nTraceback (most recent call last):<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap<br>\nself.run()<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run<br>\nself._target(*self._args, **self._kwargs)<br>\nFile \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus<br>\nserver= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in <strong>init</strong><br>\nself._server_def.SerializeToString(), status)<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in <strong>exit</strong><br>\nc_api.TF_GetCode(self.status.status))<br>\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server<br>\nEE11101 16:04:321.0976000000 17868 server_chttp2.1 16:04:32c.c976000000 16224 server_chttp2.cc:40] {\"created\":\"@1541106272.976000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307:,40] {\"created\":\"@1541106272.976000000\",\"description\":\"No address added out of total 1 resolv\"referenced_errors\":[{\"created\":\"@1541106272.976000000\",\"description\":\"Failed to add port to server\",\"filed\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_beu\"ild\\grpc\\src\\grpc\\src\\core\\ext\\transpo:rt\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541106272.976000000\",\"descript\"Ti:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"created\":\"@1541106272.9760000on\":\"Failed to add p0ort to0 server\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_bui\"l,d\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"crea\"ted\":\"@1541106272.976000000\",\"descriptdion\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmaeke_builds\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_secrirver_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage option\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage of eachf  seoach socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\"c:\"bind\",\"wsa_error\":10048}]}]}<br>\nket addressProcess Process-2:<br>\n(protocol/network addreTraceback (most recent call last):<br>\nss  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap<br>\nself.run()<br>\n/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\"  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run<br>\nself._target(*self._args, **self._kwargs)<br>\n:10048}]}]}<br>\nFile \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus<br>\nserver= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in <strong>init</strong><br>\nself._server_def.SerializeToString(), status)<br>\nProcess Process-3:<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in <strong>exit</strong><br>\nc_api.TF_GetCode(self.status.status))<br>\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server<br>\nTraceback (most recent call last):<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap<br>\nself.run()<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run<br>\nself._target(*self._args, **self._kwargs)<br>\nFile \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus<br>\nserver= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in <strong>init</strong><br>\nself._server_def.SerializeToString(), status)<br>\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in <strong>exit</strong><br>\nc_api.TF_GetCode(self.status.status))<br>\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server</p>\n<p><strong>Describe the expected behavior</strong><br>\nI would like to be able to detect the video card and report it</p>\n<p><strong>Code to reproduce the issue</strong><br>\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.<br>\nMain.py file:</p>\n<p>from multiprocessing import Process, Pipe<br>\nimport tensorflow as tf<br>\nfrom tensorflow.contrib.training import HParams<br>\nfrom GPUs import gpus_configuration</p>\n<p>N_workers= 3<br>\nSPEC_Second= {\"ps\": [\"192.168.3.18:2222\"], \"worker\": [\"192.168.3.18:2222\", \"192.168.3.14:2222\", \"192.168.3.16:2222\"]}</p>\n<p>def find_amount_gpus(task, connection):<br>\nspec= tf.train.ClusterSpec(SPEC_Second)</p>\n<pre><code>params= HParams(cluster= spec, job_name= task[0], task_index= task[1])\nserver= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\n\nif (params.job_name == \"ps\"):\n    server.join()\nelif (params.job_name == \"worker\"):\n    with tf.device(tf.train.replica_device_setter(worker_device= \"/job:worker/task:%d\" % params.task_index, cluster= spec)):\n        gpus_configuration(connection)\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\ndevices= [['ps', 0], ['worker', 0], ['worker', 1], ['worker', 2]]</p>\n<p>jobs= []<br>\npipe_list= []</p>\n<pre><code>for i in devices: #for i in range(0, 1):#N_workers):\n    print(\"In loop to find GPUs {}\\n\".format(i))\n    parent_conn, child_conn= Pipe()\n    p= Process(target= find_amount_gpus, args=(i, child_conn))\n    jobs.append(p)\n    pipe_list.append(parent_conn)\n    p.start()\n\ntotal_results= [out.recv() for out in pipe_list]\n\nWorker_1_gpus= total_results[0]['GPUs_info']\nWorker_1_ip= total_results[0]['IPaddr']\nprint(\"Worker 1 {}\\n\".format(Worker_1_gpus))\nprint(\"Worker 1 IP {}\\n\".format(Worker_1_ip))\nWorker_2_gpus= total_results[1]['GPUs_info']\nWorker_2_ip= total_results[1]['IPaddr']\nprint(\"Worker 2 {}\\n\".format(Worker_2_gpus))\nprint(\"Worker 2 IP {}\\n\".format(Worker_2_ip))\nWorker_3_gpus= total_results[2]['GPUs_info']\nWorker_3_ip= total_results[2]['IPaddr']\nprint(\"Worker 3 {}\\n\".format(Worker_3_gpus))\nprint(\"Worker 3 IP {}\\n\".format(Worker_3_ip))\n</code></pre>\n<p>GPUs.py file</p>\n<p>import os<br>\nimport tensorflow as tf<br>\nfrom tensorflow.python.client import device_lib</p>\n<p>os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\" # Turn off Tensorflow messages<br>\nos.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"</p>\n<p>\"\"\" Determine presence of GPUs or CPUs<br>\n\"\"\"</p>\n<p>def gpus_configuration(child_conn):<br>\nimport socket</p>\n<pre><code>hostname= socket.gethostname()\nIPaddr= socket.gethostbyname(hostname)\n\nconfig= tf.ConfigProto()\nconfig.gpu_options.allow_growth= True            \n\nwith tf.Session(config= config) as sess:\n    local_device_protos= device_lib.list_local_devices()\n    sess.close()\n\ngpus= len([x.name for x in local_device_protos if x.device_type == 'GPU'])\n\ntotal_results= {\"GPUs_info\": gpus,\n                \"IPaddr\": IPaddr}\n\nchild_conn.send(total_results)\n</code></pre>\n<p><strong>Other info / logs</strong><br>\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.</p>", "body_text": "Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.8.0\nPython version: 3.6.5\nBazel version (if compiling from source): n/a\nGCC/Compiler version (if compiling from source): n/a\nCUDA/cuDNN version: 9\nGPU model and memory:  Quadro K22000 with 3.34GB\n\nYou can collect some of this information using our environment capture script\nYou can also obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the current behavior\nI have a cluster running on Windows 10 and wanted to test tensorflow on a distributed environment. As a test, I want to detect the video card installed on 3 machines. When I ran the code, I get the following error messages:\nE1101 16:04:32.821000000 17460 server_chttp2.cc:40] {\"created\":\"@1541106272.820000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541106272.820000000\",\"description\":\"Failed to add port to server\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"created\":\"@1541106272.820000000\",\"description\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage of each socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\":10048}]}]}\nProcess Process-4:\nTraceback (most recent call last):\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\nself.run()\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\nself._target(*self._args, **self._kwargs)\nFile \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus\nserver= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in init\nself._server_def.SerializeToString(), status)\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in exit\nc_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\nEE11101 16:04:321.0976000000 17868 server_chttp2.1 16:04:32c.c976000000 16224 server_chttp2.cc:40] {\"created\":\"@1541106272.976000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307:,40] {\"created\":\"@1541106272.976000000\",\"description\":\"No address added out of total 1 resolv\"referenced_errors\":[{\"created\":\"@1541106272.976000000\",\"description\":\"Failed to add port to server\",\"filed\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_beu\"ild\\grpc\\src\\grpc\\src\\core\\ext\\transpo:rt\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541106272.976000000\",\"descript\"Ti:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"created\":\"@1541106272.9760000on\":\"Failed to add p0ort to0 server\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_bui\"l,d\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"crea\"ted\":\"@1541106272.976000000\",\"descriptdion\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmaeke_builds\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_secrirver_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage option\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage of eachf  seoach socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\"c:\"bind\",\"wsa_error\":10048}]}]}\nket addressProcess Process-2:\n(protocol/network addreTraceback (most recent call last):\nss  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\nself.run()\n/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\"  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\nself._target(*self._args, **self._kwargs)\n:10048}]}]}\nFile \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus\nserver= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in init\nself._server_def.SerializeToString(), status)\nProcess Process-3:\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in exit\nc_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\nTraceback (most recent call last):\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\nself.run()\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\nself._target(*self._args, **self._kwargs)\nFile \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus\nserver= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in init\nself._server_def.SerializeToString(), status)\nFile \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in exit\nc_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\nDescribe the expected behavior\nI would like to be able to detect the video card and report it\nCode to reproduce the issue\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\nMain.py file:\nfrom multiprocessing import Process, Pipe\nimport tensorflow as tf\nfrom tensorflow.contrib.training import HParams\nfrom GPUs import gpus_configuration\nN_workers= 3\nSPEC_Second= {\"ps\": [\"192.168.3.18:2222\"], \"worker\": [\"192.168.3.18:2222\", \"192.168.3.14:2222\", \"192.168.3.16:2222\"]}\ndef find_amount_gpus(task, connection):\nspec= tf.train.ClusterSpec(SPEC_Second)\nparams= HParams(cluster= spec, job_name= task[0], task_index= task[1])\nserver= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\n\nif (params.job_name == \"ps\"):\n    server.join()\nelif (params.job_name == \"worker\"):\n    with tf.device(tf.train.replica_device_setter(worker_device= \"/job:worker/task:%d\" % params.task_index, cluster= spec)):\n        gpus_configuration(connection)\n\nif name == 'main':\ndevices= [['ps', 0], ['worker', 0], ['worker', 1], ['worker', 2]]\njobs= []\npipe_list= []\nfor i in devices: #for i in range(0, 1):#N_workers):\n    print(\"In loop to find GPUs {}\\n\".format(i))\n    parent_conn, child_conn= Pipe()\n    p= Process(target= find_amount_gpus, args=(i, child_conn))\n    jobs.append(p)\n    pipe_list.append(parent_conn)\n    p.start()\n\ntotal_results= [out.recv() for out in pipe_list]\n\nWorker_1_gpus= total_results[0]['GPUs_info']\nWorker_1_ip= total_results[0]['IPaddr']\nprint(\"Worker 1 {}\\n\".format(Worker_1_gpus))\nprint(\"Worker 1 IP {}\\n\".format(Worker_1_ip))\nWorker_2_gpus= total_results[1]['GPUs_info']\nWorker_2_ip= total_results[1]['IPaddr']\nprint(\"Worker 2 {}\\n\".format(Worker_2_gpus))\nprint(\"Worker 2 IP {}\\n\".format(Worker_2_ip))\nWorker_3_gpus= total_results[2]['GPUs_info']\nWorker_3_ip= total_results[2]['IPaddr']\nprint(\"Worker 3 {}\\n\".format(Worker_3_gpus))\nprint(\"Worker 3 IP {}\\n\".format(Worker_3_ip))\n\nGPUs.py file\nimport os\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\" # Turn off Tensorflow messages\nos.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\n\"\"\" Determine presence of GPUs or CPUs\n\"\"\"\ndef gpus_configuration(child_conn):\nimport socket\nhostname= socket.gethostname()\nIPaddr= socket.gethostbyname(hostname)\n\nconfig= tf.ConfigProto()\nconfig.gpu_options.allow_growth= True            \n\nwith tf.Session(config= config) as sess:\n    local_device_protos= device_lib.list_local_devices()\n    sess.close()\n\ngpus= len([x.name for x in local_device_protos if x.device_type == 'GPU'])\n\ntotal_results= {\"GPUs_info\": gpus,\n                \"IPaddr\": IPaddr}\n\nchild_conn.send(total_results)\n\nOther info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.8.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory:  Quadro K22000 with 3.34GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI have a cluster running on Windows 10 and wanted to test tensorflow on a distributed environment. As a test, I want to detect the video card installed on 3 machines. When I ran the code, I get the following error messages:\r\n\r\nE1101 16:04:32.821000000 17460 server_chttp2.cc:40] {\"created\":\"@1541106272.820000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541106272.820000000\",\"description\":\"Failed to add port to server\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"created\":\"@1541106272.820000000\",\"description\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage of each socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\":10048}]}]}\r\nProcess Process-4:\r\nTraceback (most recent call last):\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus\r\n    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in __init__\r\n    self._server_def.SerializeToString(), status)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\nEE11101 16:04:321.0976000000 17868 server_chttp2.1 16:04:32c.c976000000 16224 server_chttp2.cc:40] {\"created\":\"@1541106272.976000000\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307:,40] {\"created\":\"@1541106272.976000000\",\"description\":\"No address added out of total 1 resolv\"referenced_errors\":[{\"created\":\"@1541106272.976000000\",\"description\":\"Failed to add port to server\",\"filed\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_beu\"ild\\grpc\\src\\grpc\\src\\core\\ext\\transpo:rt\\chttp2\\server\\chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1541106272.976000000\",\"descript\"Ti:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"created\":\"@1541106272.9760000on\":\"Failed to add p0ort to0 server\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_bui\"l,d\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":508,\"referenced_errors\":[{\"crea\"ted\":\"@1541106272.976000000\",\"descriptdion\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmaeke_builds\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_secrirver_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage option\":\"OS Error\",\"file\":\"T:\\src\\github\\tensorflow\\cmake_build\\grpc\\src\\grpc\\src\\core\\lib\\iomgr\\tcp_server_windows.cc\",\"file_line\":200,\"os_error\":\"Only one usage of eachf  seoach socket address (protocol/network address/port) is normally permitted.\\r\\n\",\"syscall\"c:\"bind\",\"wsa_error\":10048}]}]}\r\nket addressProcess Process-2:\r\n (protocol/network addreTraceback (most recent call last):\r\nss  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\r\n    self.run()\r\n/port) is normally permitted.\\r\\n\",\"syscall\":\"bind\",\"wsa_error\"  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n:10048}]}]}\r\n  File \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus\r\n    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in __init__\r\n    self._server_def.SerializeToString(), status)\r\nProcess Process-3:\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\nTraceback (most recent call last):\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\multiprocessing\\process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:\\Users\\IMarroquin\\Documents\\My_Python_Scripts\\MLP\\Test_main_distributed.py\", line 34, in find_amount_gpus\r\n    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\", line 147, in __init__\r\n    self._server_def.SerializeToString(), status)\r\n  File \"C:\\Temp\\Python\\Python-3.6.5\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\n\r\n\r\n**Describe the expected behavior**\r\nI would like to be able to detect the video card and report it\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nMain.py file:\r\n\r\nfrom multiprocessing import Process, Pipe\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.training import HParams\r\nfrom GPUs import gpus_configuration\r\n\r\nN_workers= 3\r\nSPEC_Second= {\"ps\": [\"192.168.3.18:2222\"], \"worker\": [\"192.168.3.18:2222\", \"192.168.3.14:2222\", \"192.168.3.16:2222\"]}\r\n\r\ndef find_amount_gpus(task, connection):\r\n    spec= tf.train.ClusterSpec(SPEC_Second)\r\n    \r\n    params= HParams(cluster= spec, job_name= task[0], task_index= task[1])\r\n    server= tf.train.Server(spec, job_name= params.job_name, task_index= params.task_index)\r\n    \r\n    if (params.job_name == \"ps\"):\r\n        server.join()\r\n    elif (params.job_name == \"worker\"):\r\n        with tf.device(tf.train.replica_device_setter(worker_device= \"/job:worker/task:%d\" % params.task_index, cluster= spec)):\r\n            gpus_configuration(connection)\r\n\r\nif __name__ == '__main__':\r\n    devices= [['ps', 0], ['worker', 0], ['worker', 1], ['worker', 2]]\r\n\r\njobs= []\r\n    pipe_list= []\r\n    \r\n    for i in devices: #for i in range(0, 1):#N_workers):\r\n        print(\"In loop to find GPUs {}\\n\".format(i))\r\n        parent_conn, child_conn= Pipe()\r\n        p= Process(target= find_amount_gpus, args=(i, child_conn))\r\n        jobs.append(p)\r\n        pipe_list.append(parent_conn)\r\n        p.start()\r\n    \r\n    total_results= [out.recv() for out in pipe_list]\r\n    \r\n    Worker_1_gpus= total_results[0]['GPUs_info']\r\n    Worker_1_ip= total_results[0]['IPaddr']\r\n    print(\"Worker 1 {}\\n\".format(Worker_1_gpus))\r\n    print(\"Worker 1 IP {}\\n\".format(Worker_1_ip))\r\n    Worker_2_gpus= total_results[1]['GPUs_info']\r\n    Worker_2_ip= total_results[1]['IPaddr']\r\n    print(\"Worker 2 {}\\n\".format(Worker_2_gpus))\r\n    print(\"Worker 2 IP {}\\n\".format(Worker_2_ip))\r\n    Worker_3_gpus= total_results[2]['GPUs_info']\r\n    Worker_3_ip= total_results[2]['IPaddr']\r\n    print(\"Worker 3 {}\\n\".format(Worker_3_gpus))\r\n    print(\"Worker 3 IP {}\\n\".format(Worker_3_ip))\r\n\r\nGPUs.py file\r\n\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import device_lib\r\n    \r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\" # Turn off Tensorflow messages\r\nos.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\r\n\r\n\"\"\" Determine presence of GPUs or CPUs\r\n\"\"\"\r\n\r\ndef gpus_configuration(child_conn):\r\n    import socket\r\n    \r\n    hostname= socket.gethostname()\r\n    IPaddr= socket.gethostbyname(hostname)\r\n\r\n    config= tf.ConfigProto()\r\n    config.gpu_options.allow_growth= True            \r\n    \r\n    with tf.Session(config= config) as sess:\r\n        local_device_protos= device_lib.list_local_devices()\r\n        sess.close()\r\n\r\n    gpus= len([x.name for x in local_device_protos if x.device_type == 'GPU'])\r\n    \r\n    total_results= {\"GPUs_info\": gpus,\r\n                    \"IPaddr\": IPaddr}\r\n    \r\n    child_conn.send(total_results)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n"}