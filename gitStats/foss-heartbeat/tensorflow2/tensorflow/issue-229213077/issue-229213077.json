{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9949", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9949/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9949/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9949/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9949", "id": 229213077, "node_id": "MDU6SXNzdWUyMjkyMTMwNzc=", "number": 9949, "title": "Convolution of zero length input gives junk gradients", "user": {"login": "mscheifer", "id": 19338856, "node_id": "MDQ6VXNlcjE5MzM4ODU2", "avatar_url": "https://avatars1.githubusercontent.com/u/19338856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mscheifer", "html_url": "https://github.com/mscheifer", "followers_url": "https://api.github.com/users/mscheifer/followers", "following_url": "https://api.github.com/users/mscheifer/following{/other_user}", "gists_url": "https://api.github.com/users/mscheifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/mscheifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mscheifer/subscriptions", "organizations_url": "https://api.github.com/users/mscheifer/orgs", "repos_url": "https://api.github.com/users/mscheifer/repos", "events_url": "https://api.github.com/users/mscheifer/events{/privacy}", "received_events_url": "https://api.github.com/users/mscheifer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-17T01:29:15Z", "updated_at": "2017-05-18T00:29:21Z", "closed_at": "2017-05-18T00:29:21Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: python3 junk_gradients.py</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When you pass a zero length input to tf.conv1d and then calculate gradients, you will get junk values. I think it's reading from uninitialized memory because it's nondeterministic and can be very small or very large positive or negative values or NaNs.</p>\n<p>The expected behavior is that the gradients should all be 0 because the weights aren't making any contribution to the loss.</p>\n<p>In the application I am writing the input has a variable length (including 0) but for this minimal example I've set it to a constant (tf.ones([0,2])).</p>\n<p>If you force the convolution to always have an input with length &gt; 0 then the bug goes away. I've included that in the reproduction code under the variable 'remove_bug'.</p>\n<p>For me, with this reduced example, the gradients are always junk but vary widely. You might see 0s if it happens to read from zeroed out memory. Hopefully the bug will show up if you just run it a few times.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nremove_bug <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\nvals <span class=\"pl-k\">=</span> tf.ones([<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">2</span>])\n\n<span class=\"pl-k\">if</span> remove_bug: <span class=\"pl-c\"><span class=\"pl-c\">#</span> hack it to not actually have zero length</span>\n    vals <span class=\"pl-k\">=</span> tf.concat([tf.ones([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>]), vals], <span class=\"pl-c1\">0</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> At this point 'vals' will either have length 0 or 2 if the bug was removed</span>\n\n<span class=\"pl-c1\">filter</span> <span class=\"pl-k\">=</span> tf.Variable(tf.ones([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>]))\n\nconv <span class=\"pl-k\">=</span> tf.nn.conv1d(tf.expand_dims(vals, <span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">filter</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> At this point 'conv' will either have length 0 or 1 if the bug was removed</span>\n\n<span class=\"pl-k\">if</span> remove_bug:\n    conv <span class=\"pl-k\">=</span> conv[<span class=\"pl-c1\">1</span>:] <span class=\"pl-c\"><span class=\"pl-c\">#</span> slice off hack, make 'conv' zero length again</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> At this point 'conv' will have length 0 whether or not the bug was removed.</span>\n\noptimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.01</span>)\n\ngrads <span class=\"pl-k\">=</span> [g <span class=\"pl-k\">for</span> g, _ <span class=\"pl-k\">in</span> optimizer.compute_gradients(conv)]\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n\n    gs <span class=\"pl-k\">=</span> sess.run(grads)\n\n<span class=\"pl-c1\">print</span>(gs)</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.1\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: python3 junk_gradients.py\n\nDescribe the problem\nWhen you pass a zero length input to tf.conv1d and then calculate gradients, you will get junk values. I think it's reading from uninitialized memory because it's nondeterministic and can be very small or very large positive or negative values or NaNs.\nThe expected behavior is that the gradients should all be 0 because the weights aren't making any contribution to the loss.\nIn the application I am writing the input has a variable length (including 0) but for this minimal example I've set it to a constant (tf.ones([0,2])).\nIf you force the convolution to always have an input with length > 0 then the bug goes away. I've included that in the reproduction code under the variable 'remove_bug'.\nFor me, with this reduced example, the gradients are always junk but vary widely. You might see 0s if it happens to read from zeroed out memory. Hopefully the bug will show up if you just run it a few times.\nSource code / logs\nimport tensorflow as tf\n\nremove_bug = False\n\nvals = tf.ones([0,2])\n\nif remove_bug: # hack it to not actually have zero length\n    vals = tf.concat([tf.ones([2, 2]), vals], 0)\n\n# At this point 'vals' will either have length 0 or 2 if the bug was removed\n\nfilter = tf.Variable(tf.ones([2, 2, 2]))\n\nconv = tf.nn.conv1d(tf.expand_dims(vals, 0), filter, 2, 'SAME')[0]\n\n# At this point 'conv' will either have length 0 or 1 if the bug was removed\n\nif remove_bug:\n    conv = conv[1:] # slice off hack, make 'conv' zero length again\n\n# At this point 'conv' will have length 0 whether or not the bug was removed.\n\noptimizer = tf.train.GradientDescentOptimizer(0.01)\n\ngrads = [g for g, _ in optimizer.compute_gradients(conv)]\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    gs = sess.run(grads)\n\nprint(gs)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: python3 junk_gradients.py\r\n\r\n### Describe the problem\r\nWhen you pass a zero length input to tf.conv1d and then calculate gradients, you will get junk values. I think it's reading from uninitialized memory because it's nondeterministic and can be very small or very large positive or negative values or NaNs.\r\n\r\nThe expected behavior is that the gradients should all be 0 because the weights aren't making any contribution to the loss.\r\n\r\nIn the application I am writing the input has a variable length (including 0) but for this minimal example I've set it to a constant (tf.ones([0,2])).\r\n\r\nIf you force the convolution to always have an input with length > 0 then the bug goes away. I've included that in the reproduction code under the variable 'remove_bug'.\r\n\r\nFor me, with this reduced example, the gradients are always junk but vary widely. You might see 0s if it happens to read from zeroed out memory. Hopefully the bug will show up if you just run it a few times.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\n\r\nremove_bug = False\r\n\r\nvals = tf.ones([0,2])\r\n\r\nif remove_bug: # hack it to not actually have zero length\r\n    vals = tf.concat([tf.ones([2, 2]), vals], 0)\r\n\r\n# At this point 'vals' will either have length 0 or 2 if the bug was removed\r\n\r\nfilter = tf.Variable(tf.ones([2, 2, 2]))\r\n\r\nconv = tf.nn.conv1d(tf.expand_dims(vals, 0), filter, 2, 'SAME')[0]\r\n\r\n# At this point 'conv' will either have length 0 or 1 if the bug was removed\r\n\r\nif remove_bug:\r\n    conv = conv[1:] # slice off hack, make 'conv' zero length again\r\n\r\n# At this point 'conv' will have length 0 whether or not the bug was removed.\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(0.01)\r\n\r\ngrads = [g for g, _ in optimizer.compute_gradients(conv)]\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    gs = sess.run(grads)\r\n\r\nprint(gs)\r\n```"}