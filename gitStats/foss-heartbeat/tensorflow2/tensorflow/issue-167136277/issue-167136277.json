{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3470", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3470/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3470/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3470/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3470", "id": 167136277, "node_id": "MDU6SXNzdWUxNjcxMzYyNzc=", "number": 3470, "title": "TensorFlow r0.9rc0 and r0.10rc0 cluster memory leak and core dump", "user": {"login": "hholst80", "id": 6200749, "node_id": "MDQ6VXNlcjYyMDA3NDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/6200749?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hholst80", "html_url": "https://github.com/hholst80", "followers_url": "https://api.github.com/users/hholst80/followers", "following_url": "https://api.github.com/users/hholst80/following{/other_user}", "gists_url": "https://api.github.com/users/hholst80/gists{/gist_id}", "starred_url": "https://api.github.com/users/hholst80/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hholst80/subscriptions", "organizations_url": "https://api.github.com/users/hholst80/orgs", "repos_url": "https://api.github.com/users/hholst80/repos", "events_url": "https://api.github.com/users/hholst80/events{/privacy}", "received_events_url": "https://api.github.com/users/hholst80/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2016-07-22T21:29:19Z", "updated_at": "2016-08-15T22:53:07Z", "closed_at": "2016-08-12T17:51:48Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Environment info</h3>\n<p>Operating System:</p>\n<pre><code>$ uname -svm\nLinux #50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016 x86_64\n$ lsb_release -a\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.1 LTS\nRelease:        16.04\nCodename:       xenial\n</code></pre>\n<p>Installed version of CUDA and cuDNN:</p>\n<pre><code>$ find /usr/lib -name libcud\\*\n/usr/lib/i386-linux-gnu/libcuda.so.1\n/usr/lib/i386-linux-gnu/libcuda.so.361.42\n/usr/lib/i386-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\n/usr/lib/x86_64-linux-gnu/libcuda.so.1\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n/usr/lib/x86_64-linux-gnu/libcudnn.so\n/usr/lib/x86_64-linux-gnu/libcuda.so.361.42\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.0.5\n/usr/lib/x86_64-linux-gnu/libcudart.so\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5\n/usr/lib/x86_64-linux-gnu/libcudadevrt.a\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5\n/usr/lib/x86_64-linux-gnu/stubs/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcudart_static.a\n</code></pre>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed.</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<p>Built from source:</p>\n<pre><code>* No Google Cloud Platform support\n* GPU support\n* CUDA Compute Capability: 3.0, 3.5 and 5.2\n\nprintf '\\n\\nY\\n\\n\\n/usr\\n\\n\\n3.0,3.5,5.2\\n' | ./configure\n</code></pre>\n<pre><code>tensorflow-0.9.0rc0-py3-none-any.whl\n</code></pre>\n<pre><code>0.9.0rc0\n</code></pre>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)</li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<pre><code>$ git rev-parse HEAD\n2f5ca43750dcd052843fd02a841bf041c3958670\n</code></pre>\n<pre><code>$ bazel version\n...............\nBuild label: 0.2.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Apr 21 13:01:41 2016 (1461243701)\nBuild timestamp: 1461243701\nBuild timestamp as int: 1461243701\n</code></pre>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>I'll provide a gist with an example and update.</li>\n</ol>\n<h3>What have you tried?</h3>\n<ol>\n<li>Tried disabling summary and checkpoint restart.</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment).</p>\n<p>On 16 worker processes the memory consumption on the (single) PS process grows with &gt;1MB/s. The worker processes also grows in memory size but not as much. My workers are only executing a sequence of \"sess.run(..)\" commands in the loop so I don't think its even possible to add stuff to the graph. We'll try to prove me wrong in the gist if I can get a usable repro.</p>\n<h3>Update (TensorFlow r0.10 and debugging)</h3>\n<p>I just checked with the latest version of TensorFlow (r0.10) and it seems to leaks memory in the same way. Disclaimer: I used the TensorFlow package from TensorFlow.org but I have cuDNN 5 installed right now.</p>\n<p>The TensorFlow package I used (in a Python 3.5 conda environment called <code>tf-r0.10</code>):</p>\n<pre><code>$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl\n$ pip install --upgrade $TF_BINARY_URL\n</code></pre>\n<ol>\n<li>I tried adding a <code>sess.graph.finalize()</code> and I got no exceptions, thus I make the assumption that the code does not create any new nodes in each iteration.</li>\n<li>If I use the tmalloc from Ubuntu 16.04 (<code>libtcmalloc-minimal4</code>) the memory leak just got worse, but I did not get any output that I could try and examine why?</li>\n</ol>\n<pre><code>pre_window: source activate tf-r0.10 &amp;&amp; export CUDA_VISIBLE_DEVICES='' LD_PRELOAD=/usr/lib/libtcmalloc_minimal_debug.so.4 HEAPPROFILE=/tmp/heapprofile HEAPCHECK=1\n</code></pre>\n<p>I can still reproduce the core dump with the <code>for _ in range(100)</code> change:</p>\n<pre><code>terminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\nAborted (core dumped)\n</code></pre>", "body_text": "Environment info\nOperating System:\n$ uname -svm\nLinux #50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016 x86_64\n$ lsb_release -a\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.1 LTS\nRelease:        16.04\nCodename:       xenial\n\nInstalled version of CUDA and cuDNN:\n$ find /usr/lib -name libcud\\*\n/usr/lib/i386-linux-gnu/libcuda.so.1\n/usr/lib/i386-linux-gnu/libcuda.so.361.42\n/usr/lib/i386-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\n/usr/lib/x86_64-linux-gnu/libcuda.so.1\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n/usr/lib/x86_64-linux-gnu/libcudnn.so\n/usr/lib/x86_64-linux-gnu/libcuda.so.361.42\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.0.5\n/usr/lib/x86_64-linux-gnu/libcudart.so\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5\n/usr/lib/x86_64-linux-gnu/libcudadevrt.a\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5\n/usr/lib/x86_64-linux-gnu/stubs/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcudart_static.a\n\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nBuilt from source:\n* No Google Cloud Platform support\n* GPU support\n* CUDA Compute Capability: 3.0, 3.5 and 5.2\n\nprintf '\\n\\nY\\n\\n\\n/usr\\n\\n\\n3.0,3.5,5.2\\n' | ./configure\n\ntensorflow-0.9.0rc0-py3-none-any.whl\n\n0.9.0rc0\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\nThe output of bazel version\n\n$ git rev-parse HEAD\n2f5ca43750dcd052843fd02a841bf041c3958670\n\n$ bazel version\n...............\nBuild label: 0.2.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Apr 21 13:01:41 2016 (1461243701)\nBuild timestamp: 1461243701\nBuild timestamp as int: 1461243701\n\nSteps to reproduce\n\nI'll provide a gist with an example and update.\n\nWhat have you tried?\n\nTried disabling summary and checkpoint restart.\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).\nOn 16 worker processes the memory consumption on the (single) PS process grows with >1MB/s. The worker processes also grows in memory size but not as much. My workers are only executing a sequence of \"sess.run(..)\" commands in the loop so I don't think its even possible to add stuff to the graph. We'll try to prove me wrong in the gist if I can get a usable repro.\nUpdate (TensorFlow r0.10 and debugging)\nI just checked with the latest version of TensorFlow (r0.10) and it seems to leaks memory in the same way. Disclaimer: I used the TensorFlow package from TensorFlow.org but I have cuDNN 5 installed right now.\nThe TensorFlow package I used (in a Python 3.5 conda environment called tf-r0.10):\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl\n$ pip install --upgrade $TF_BINARY_URL\n\n\nI tried adding a sess.graph.finalize() and I got no exceptions, thus I make the assumption that the code does not create any new nodes in each iteration.\nIf I use the tmalloc from Ubuntu 16.04 (libtcmalloc-minimal4) the memory leak just got worse, but I did not get any output that I could try and examine why?\n\npre_window: source activate tf-r0.10 && export CUDA_VISIBLE_DEVICES='' LD_PRELOAD=/usr/lib/libtcmalloc_minimal_debug.so.4 HEAPPROFILE=/tmp/heapprofile HEAPCHECK=1\n\nI can still reproduce the core dump with the for _ in range(100) change:\nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\nAborted (core dumped)", "body": "### Environment info\n\nOperating System: \n\n```\n$ uname -svm\nLinux #50-Ubuntu SMP Wed Jul 13 00:07:12 UTC 2016 x86_64\n$ lsb_release -a\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.1 LTS\nRelease:        16.04\nCodename:       xenial\n```\n\nInstalled version of CUDA and cuDNN: \n\n```\n$ find /usr/lib -name libcud\\*\n/usr/lib/i386-linux-gnu/libcuda.so.1\n/usr/lib/i386-linux-gnu/libcuda.so.361.42\n/usr/lib/i386-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\n/usr/lib/x86_64-linux-gnu/libcuda.so.1\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n/usr/lib/x86_64-linux-gnu/libcudnn.so\n/usr/lib/x86_64-linux-gnu/libcuda.so.361.42\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.0.5\n/usr/lib/x86_64-linux-gnu/libcudart.so\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5\n/usr/lib/x86_64-linux-gnu/libcudadevrt.a\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5\n/usr/lib/x86_64-linux-gnu/stubs/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nBuilt from source:\n\n```\n* No Google Cloud Platform support\n* GPU support\n* CUDA Compute Capability: 3.0, 3.5 and 5.2\n\nprintf '\\n\\nY\\n\\n\\n/usr\\n\\n\\n3.0,3.5,5.2\\n' | ./configure\n```\n\n```\ntensorflow-0.9.0rc0-py3-none-any.whl\n```\n\n```\n0.9.0rc0\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n\n```\n$ git rev-parse HEAD\n2f5ca43750dcd052843fd02a841bf041c3958670\n```\n\n```\n$ bazel version\n...............\nBuild label: 0.2.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Apr 21 13:01:41 2016 (1461243701)\nBuild timestamp: 1461243701\nBuild timestamp as int: 1461243701\n```\n### Steps to reproduce\n1. I'll provide a gist with an example and update.\n### What have you tried?\n1. Tried disabling summary and checkpoint restart.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nOn 16 worker processes the memory consumption on the (single) PS process grows with >1MB/s. The worker processes also grows in memory size but not as much. My workers are only executing a sequence of \"sess.run(..)\" commands in the loop so I don't think its even possible to add stuff to the graph. We'll try to prove me wrong in the gist if I can get a usable repro.\n### Update (TensorFlow r0.10 and debugging)\n\nI just checked with the latest version of TensorFlow (r0.10) and it seems to leaks memory in the same way. Disclaimer: I used the TensorFlow package from TensorFlow.org but I have cuDNN 5 installed right now.\n\nThe TensorFlow package I used (in a Python 3.5 conda environment called `tf-r0.10`):\n\n```\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl\n$ pip install --upgrade $TF_BINARY_URL\n```\n1. I tried adding a `sess.graph.finalize()` and I got no exceptions, thus I make the assumption that the code does not create any new nodes in each iteration.\n2. If I use the tmalloc from Ubuntu 16.04 (`libtcmalloc-minimal4`) the memory leak just got worse, but I did not get any output that I could try and examine why?\n\n```\npre_window: source activate tf-r0.10 && export CUDA_VISIBLE_DEVICES='' LD_PRELOAD=/usr/lib/libtcmalloc_minimal_debug.so.4 HEAPPROFILE=/tmp/heapprofile HEAPCHECK=1\n```\n\nI can still reproduce the core dump with the `for _ in range(100)` change:\n\n```\nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\nAborted (core dumped)\n```\n"}