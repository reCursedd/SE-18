{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/246886015", "html_url": "https://github.com/tensorflow/tensorflow/issues/4330#issuecomment-246886015", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4330", "id": 246886015, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Njg4NjAxNQ==", "user": {"login": "Jongchan", "id": 5811413, "node_id": "MDQ6VXNlcjU4MTE0MTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5811413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jongchan", "html_url": "https://github.com/Jongchan", "followers_url": "https://api.github.com/users/Jongchan/followers", "following_url": "https://api.github.com/users/Jongchan/following{/other_user}", "gists_url": "https://api.github.com/users/Jongchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jongchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jongchan/subscriptions", "organizations_url": "https://api.github.com/users/Jongchan/orgs", "repos_url": "https://api.github.com/users/Jongchan/repos", "events_url": "https://api.github.com/users/Jongchan/events{/privacy}", "received_events_url": "https://api.github.com/users/Jongchan/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-14T02:19:00Z", "updated_at": "2016-09-14T02:31:35Z", "author_association": "NONE", "body_html": "<p>Yes, I understand that it can reduce flop count significantly, and that when channel_multiplier is large, the operation becomes very complex.<br>\nHowever, I think it should not be raised as an error for 2 reasons:</p>\n<ol>\n<li>separable_conv2d is not equivalent to conventional convolution.</li>\n<li>the amount of flops may be less even when in_channel * channel_multiplier &gt; out_channel. Will elaborate more below.</li>\n</ol>\n<p>I will assume in_channel = out_channel since many modules (layers) in deep architecture has such config.</p>\n<p>For a single patch, the number of multiplications in separable_conv2d is <strong>in_channel * filter_w * filter_h * channel_multiplier + 1 * 1 * (in_channel * channel_multiplier) * out_channel</strong>.<br>\nThe number of multiplications in conventional conv is <strong>in_channel * filter_w * filter_h * out_channel</strong>.<br>\nWhen we compare two computations, where out_channel &gt;&gt; channel_multiplier in most cases, the amount of computations may be less for separable_conv2d.</p>\n<p>Is it really necessary to add such a restriction that in_channel * channel_multiplier &lt;= out_channel ?<br>\n(in my case, for a single patch, channel_multiplier=3, the number of multiplications is 64 * 3 * 3 * 3+1 * 1 * 192 * 64 = 18,624, where the conventional conv yields 64 * 3 * 3 * 64=36,864)</p>", "body_text": "Yes, I understand that it can reduce flop count significantly, and that when channel_multiplier is large, the operation becomes very complex.\nHowever, I think it should not be raised as an error for 2 reasons:\n\nseparable_conv2d is not equivalent to conventional convolution.\nthe amount of flops may be less even when in_channel * channel_multiplier > out_channel. Will elaborate more below.\n\nI will assume in_channel = out_channel since many modules (layers) in deep architecture has such config.\nFor a single patch, the number of multiplications in separable_conv2d is in_channel * filter_w * filter_h * channel_multiplier + 1 * 1 * (in_channel * channel_multiplier) * out_channel.\nThe number of multiplications in conventional conv is in_channel * filter_w * filter_h * out_channel.\nWhen we compare two computations, where out_channel >> channel_multiplier in most cases, the amount of computations may be less for separable_conv2d.\nIs it really necessary to add such a restriction that in_channel * channel_multiplier <= out_channel ?\n(in my case, for a single patch, channel_multiplier=3, the number of multiplications is 64 * 3 * 3 * 3+1 * 1 * 192 * 64 = 18,624, where the conventional conv yields 64 * 3 * 3 * 64=36,864)", "body": "Yes, I understand that it can reduce flop count significantly, and that when channel_multiplier is large, the operation becomes very complex.\nHowever, I think it should not be raised as an error for 2 reasons:\n1. separable_conv2d is not equivalent to conventional convolution.\n2. the amount of flops may be less even when in_channel \\* channel_multiplier > out_channel. Will elaborate more below.\n\nI will assume in_channel = out_channel since many modules (layers) in deep architecture has such config.\n\nFor a single patch, the number of multiplications in separable_conv2d is **in_channel \\* filter_w \\* filter_h \\* channel_multiplier + 1 \\* 1 \\* (in_channel \\* channel_multiplier) \\* out_channel**.\nThe number of multiplications in conventional conv is **in_channel \\* filter_w \\* filter_h \\* out_channel**.\nWhen we compare two computations, where out_channel >> channel_multiplier in most cases, the amount of computations may be less for separable_conv2d. \n\nIs it really necessary to add such a restriction that in_channel \\* channel_multiplier <= out_channel ?\n(in my case, for a single patch, channel_multiplier=3, the number of multiplications is 64 \\* 3 \\* 3 \\* 3+1 \\* 1 \\* 192 \\* 64 = 18,624, where the conventional conv yields 64 \\* 3 \\* 3 \\* 64=36,864)\n"}