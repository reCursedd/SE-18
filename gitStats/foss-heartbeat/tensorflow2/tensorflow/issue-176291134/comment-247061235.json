{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/247061235", "html_url": "https://github.com/tensorflow/tensorflow/issues/4330#issuecomment-247061235", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4330", "id": 247061235, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NzA2MTIzNQ==", "user": {"login": "vincentvanhoucke", "id": 15737127, "node_id": "MDQ6VXNlcjE1NzM3MTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/15737127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vincentvanhoucke", "html_url": "https://github.com/vincentvanhoucke", "followers_url": "https://api.github.com/users/vincentvanhoucke/followers", "following_url": "https://api.github.com/users/vincentvanhoucke/following{/other_user}", "gists_url": "https://api.github.com/users/vincentvanhoucke/gists{/gist_id}", "starred_url": "https://api.github.com/users/vincentvanhoucke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vincentvanhoucke/subscriptions", "organizations_url": "https://api.github.com/users/vincentvanhoucke/orgs", "repos_url": "https://api.github.com/users/vincentvanhoucke/repos", "events_url": "https://api.github.com/users/vincentvanhoucke/events{/privacy}", "received_events_url": "https://api.github.com/users/vincentvanhoucke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-14T15:55:55Z", "updated_at": "2016-09-14T15:55:55Z", "author_association": "MEMBER", "body_html": "<p>If you have valid uses for a separable convolution which blows up the number of activations and then shrinks them back, then I'm ok accepting a PR that removes this check. Separable convolutions predate this paper by a couple of years, so it's entirely possible that people have found productive uses of this regime. In general, introducing an activation bottleneck anywhere in the network tends to be a terrible idea from an optimization standpoint, and unless your dimensions are widely pathological, it also implies that you're introducing more free parameters than there was in the convolution in the first place. But one can argue there is nothing broken about doing so.</p>", "body_text": "If you have valid uses for a separable convolution which blows up the number of activations and then shrinks them back, then I'm ok accepting a PR that removes this check. Separable convolutions predate this paper by a couple of years, so it's entirely possible that people have found productive uses of this regime. In general, introducing an activation bottleneck anywhere in the network tends to be a terrible idea from an optimization standpoint, and unless your dimensions are widely pathological, it also implies that you're introducing more free parameters than there was in the convolution in the first place. But one can argue there is nothing broken about doing so.", "body": "If you have valid uses for a separable convolution which blows up the number of activations and then shrinks them back, then I'm ok accepting a PR that removes this check. Separable convolutions predate this paper by a couple of years, so it's entirely possible that people have found productive uses of this regime. In general, introducing an activation bottleneck anywhere in the network tends to be a terrible idea from an optimization standpoint, and unless your dimensions are widely pathological, it also implies that you're introducing more free parameters than there was in the convolution in the first place. But one can argue there is nothing broken about doing so.\n"}