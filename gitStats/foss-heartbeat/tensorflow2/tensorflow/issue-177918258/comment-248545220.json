{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/248545220", "html_url": "https://github.com/tensorflow/tensorflow/issues/4478#issuecomment-248545220", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4478", "id": 248545220, "node_id": "MDEyOklzc3VlQ29tbWVudDI0ODU0NTIyMA==", "user": {"login": "link-er", "id": 1128291, "node_id": "MDQ6VXNlcjExMjgyOTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1128291?v=4", "gravatar_id": "", "url": "https://api.github.com/users/link-er", "html_url": "https://github.com/link-er", "followers_url": "https://api.github.com/users/link-er/followers", "following_url": "https://api.github.com/users/link-er/following{/other_user}", "gists_url": "https://api.github.com/users/link-er/gists{/gist_id}", "starred_url": "https://api.github.com/users/link-er/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/link-er/subscriptions", "organizations_url": "https://api.github.com/users/link-er/orgs", "repos_url": "https://api.github.com/users/link-er/repos", "events_url": "https://api.github.com/users/link-er/events{/privacy}", "received_events_url": "https://api.github.com/users/link-er/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-21T08:27:25Z", "updated_at": "2016-09-21T11:09:38Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Please provide the critical part of the code.</p>\n</blockquote>\n<pre><code>def step(prev_step, cur_input):\n        layer_1 = tf.add(tf.matmul(cur_input, weights['h1']), biases['b1'])\n        layer_1 = tf.nn.sigmoid(layer_1)\n\n        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n        layer_2 = tf.nn.sigmoid(layer_2)\n\n        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n        layer_3 = tf.nn.sigmoid(layer_3)\n\n        key = tf.nn.tanh(tf.add(tf.matmul(layer_3, weights['out']), biases['out']))\n\n        # calculate alpha gateway depending on the key\n        alpha = tf.nn.softmax(tf.add(tf.matmul(key, weights['alpha']), biases['alpha'])) # (batch_size, 1)\n        # calculate write weights as combination of previously read rows and least used rows\n        w_weights = tf.sigmoid(alpha) * prev_step[1] # (batch_size, mem_height) \n\n    # n-th smallest element's index in previous usage_weights\n    nth_smallest = tf.nn.top_k(-1*prev_step[2], k=number_of_reads, sorted=True)[1] # (batch_size, 1)\n\n    linear_index = tf.reshape(nth_smallest, [batch_size]) + (mem_height * tf.range(0,batch_size))\n    linear_w_weights = tf.reshape(w_weights, [-1])\n    ref = tf.Variable(linear_w_weights, trainable=False)\n    least_used_update = tf.reshape((1 - tf.sigmoid(alpha)), [batch_size])\n    w_weights = tf.stop_gradient(tf.reshape(tf.scatter_add(ref, linear_index, least_used_update, use_locking=True), \n                           [batch_size, mem_height]))\n\n    # put to 0 least used row in memory\n    linear_memory = tf.reshape(prev_step[0], [batch_size*mem_height, mem_width])\n    ref1 = tf.Variable(linear_memory, trainable=False)\n    memory = tf.stop_gradient(tf.reshape(tf.scatter_update(ref1, linear_index, tf.zeros([batch_size, mem_width]), use_locking=True),\n                        (batch_size,mem_height,mem_width))) # (batch_size,mem_height,mem_width)\n    # update memory state with write weights\n    # (batch_size, mem_height, mem_width)\n    memory = memory + \\\n            tf.batch_matmul(tf.reshape(w_weights, (batch_size,mem_height,1)), tf.reshape(key, (batch_size,1,mem_width)))\n        normed_key = key / tf.sqrt(tf.reduce_sum(tf.square(key))) # (batch_size, mem_width)\n        # (batch_size, mem_height, mem_width)\n        normed_memory = tf.div(memory, tf.sqrt(tf.reduce_sum(tf.square(memory), 1, keep_dims=True)))\n        # calculate similarities to each memory row\n        # (batch_size, mem_height)\n        similarity = tf.reshape(tf.batch_matmul(normed_memory, tf.reshape(normed_key, (batch_size,mem_width,1))),\n                                (batch_size,mem_height))\n        # calculate read weights as softmax probability distribution\n        r_weights = tf.nn.softmax(similarity) # (batch_size, mem_height)\n        # retrieve memory\n        retrieved_memory = tf.reshape(tf.batch_matmul(tf.reshape(r_weights, (batch_size,1,mem_height)), memory),\n                                      (batch_size,mem_width)) # (batch_size, mem_width)\n        # put retrieved memory through output layer to get prediction\n        do_output = tf.add(tf.matmul(retrieved_memory, weights['do']), biases['do'])\n        prediction = tf.nn.softmax(do_output) # (batch_size, n_classes)\n        # calculate usage weights\n        u_weights = gamma * prev_step[2] + r_weights + w_weights # (batch_size, mem_height)\n\n        return (memory, r_weights, u_weights, prediction) \n\ndef model(elems):\n        _, _, _, predictions = tf.scan(step, elems, \n                                    initializer=(np.zeros((batch_size, mem_height, mem_width)).astype(np.float32), \n                                      np.random.rand(batch_size, mem_height).astype(np.float32),\n                                      np.random.rand(batch_size, mem_height).astype(np.float32),\n                                      np.zeros((batch_size, n_classes)).astype(np.float32)), \n            parallel_iterations=1, back_prop=True, swap_memory=False)\n        return predictions\n\nweights = {\n  'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=stddev)),\n  'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=stddev)),\n  'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=stddev)),\n  'out': tf.Variable(tf.random_normal([n_hidden_3, mem_width],stddev=stddev)),\n  'do': tf.Variable(tf.random_normal([mem_width, n_classes],stddev=stddev)),\n  'alpha': tf.Variable(tf.random_normal([mem_width, 1],stddev=stddev))\n}\nbiases = {\n  'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n  'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n  'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n  'out': tf.Variable(tf.random_normal([mem_width])),\n  'do': tf.Variable(tf.random_normal([n_classes])),\n  'alpha': tf.Variable(tf.random_normal([1]))\n}\n\ninput_data = tf.placeholder(\"float32\",[n_classes*n_examples, batch_size, n_input], name='input_data')\nlabels = tf.placeholder(\"float32\",[n_classes*n_examples, batch_size, n_classes],name='labels')\n\npredictions = model(input_data)\n</code></pre>\n<p>And after this calling initialize_all_variables() to run causes that error</p>\n<blockquote>\n<p>p.s. is this something for which you can use tf.get_variable?</p>\n</blockquote>\n<p>What exactly this call will do?</p>", "body_text": "Please provide the critical part of the code.\n\ndef step(prev_step, cur_input):\n        layer_1 = tf.add(tf.matmul(cur_input, weights['h1']), biases['b1'])\n        layer_1 = tf.nn.sigmoid(layer_1)\n\n        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n        layer_2 = tf.nn.sigmoid(layer_2)\n\n        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n        layer_3 = tf.nn.sigmoid(layer_3)\n\n        key = tf.nn.tanh(tf.add(tf.matmul(layer_3, weights['out']), biases['out']))\n\n        # calculate alpha gateway depending on the key\n        alpha = tf.nn.softmax(tf.add(tf.matmul(key, weights['alpha']), biases['alpha'])) # (batch_size, 1)\n        # calculate write weights as combination of previously read rows and least used rows\n        w_weights = tf.sigmoid(alpha) * prev_step[1] # (batch_size, mem_height) \n\n    # n-th smallest element's index in previous usage_weights\n    nth_smallest = tf.nn.top_k(-1*prev_step[2], k=number_of_reads, sorted=True)[1] # (batch_size, 1)\n\n    linear_index = tf.reshape(nth_smallest, [batch_size]) + (mem_height * tf.range(0,batch_size))\n    linear_w_weights = tf.reshape(w_weights, [-1])\n    ref = tf.Variable(linear_w_weights, trainable=False)\n    least_used_update = tf.reshape((1 - tf.sigmoid(alpha)), [batch_size])\n    w_weights = tf.stop_gradient(tf.reshape(tf.scatter_add(ref, linear_index, least_used_update, use_locking=True), \n                           [batch_size, mem_height]))\n\n    # put to 0 least used row in memory\n    linear_memory = tf.reshape(prev_step[0], [batch_size*mem_height, mem_width])\n    ref1 = tf.Variable(linear_memory, trainable=False)\n    memory = tf.stop_gradient(tf.reshape(tf.scatter_update(ref1, linear_index, tf.zeros([batch_size, mem_width]), use_locking=True),\n                        (batch_size,mem_height,mem_width))) # (batch_size,mem_height,mem_width)\n    # update memory state with write weights\n    # (batch_size, mem_height, mem_width)\n    memory = memory + \\\n            tf.batch_matmul(tf.reshape(w_weights, (batch_size,mem_height,1)), tf.reshape(key, (batch_size,1,mem_width)))\n        normed_key = key / tf.sqrt(tf.reduce_sum(tf.square(key))) # (batch_size, mem_width)\n        # (batch_size, mem_height, mem_width)\n        normed_memory = tf.div(memory, tf.sqrt(tf.reduce_sum(tf.square(memory), 1, keep_dims=True)))\n        # calculate similarities to each memory row\n        # (batch_size, mem_height)\n        similarity = tf.reshape(tf.batch_matmul(normed_memory, tf.reshape(normed_key, (batch_size,mem_width,1))),\n                                (batch_size,mem_height))\n        # calculate read weights as softmax probability distribution\n        r_weights = tf.nn.softmax(similarity) # (batch_size, mem_height)\n        # retrieve memory\n        retrieved_memory = tf.reshape(tf.batch_matmul(tf.reshape(r_weights, (batch_size,1,mem_height)), memory),\n                                      (batch_size,mem_width)) # (batch_size, mem_width)\n        # put retrieved memory through output layer to get prediction\n        do_output = tf.add(tf.matmul(retrieved_memory, weights['do']), biases['do'])\n        prediction = tf.nn.softmax(do_output) # (batch_size, n_classes)\n        # calculate usage weights\n        u_weights = gamma * prev_step[2] + r_weights + w_weights # (batch_size, mem_height)\n\n        return (memory, r_weights, u_weights, prediction) \n\ndef model(elems):\n        _, _, _, predictions = tf.scan(step, elems, \n                                    initializer=(np.zeros((batch_size, mem_height, mem_width)).astype(np.float32), \n                                      np.random.rand(batch_size, mem_height).astype(np.float32),\n                                      np.random.rand(batch_size, mem_height).astype(np.float32),\n                                      np.zeros((batch_size, n_classes)).astype(np.float32)), \n            parallel_iterations=1, back_prop=True, swap_memory=False)\n        return predictions\n\nweights = {\n  'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=stddev)),\n  'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=stddev)),\n  'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=stddev)),\n  'out': tf.Variable(tf.random_normal([n_hidden_3, mem_width],stddev=stddev)),\n  'do': tf.Variable(tf.random_normal([mem_width, n_classes],stddev=stddev)),\n  'alpha': tf.Variable(tf.random_normal([mem_width, 1],stddev=stddev))\n}\nbiases = {\n  'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n  'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n  'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n  'out': tf.Variable(tf.random_normal([mem_width])),\n  'do': tf.Variable(tf.random_normal([n_classes])),\n  'alpha': tf.Variable(tf.random_normal([1]))\n}\n\ninput_data = tf.placeholder(\"float32\",[n_classes*n_examples, batch_size, n_input], name='input_data')\nlabels = tf.placeholder(\"float32\",[n_classes*n_examples, batch_size, n_classes],name='labels')\n\npredictions = model(input_data)\n\nAnd after this calling initialize_all_variables() to run causes that error\n\np.s. is this something for which you can use tf.get_variable?\n\nWhat exactly this call will do?", "body": "> Please provide the critical part of the code.\n\n```\ndef step(prev_step, cur_input):\n        layer_1 = tf.add(tf.matmul(cur_input, weights['h1']), biases['b1'])\n        layer_1 = tf.nn.sigmoid(layer_1)\n\n        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n        layer_2 = tf.nn.sigmoid(layer_2)\n\n        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n        layer_3 = tf.nn.sigmoid(layer_3)\n\n        key = tf.nn.tanh(tf.add(tf.matmul(layer_3, weights['out']), biases['out']))\n\n        # calculate alpha gateway depending on the key\n        alpha = tf.nn.softmax(tf.add(tf.matmul(key, weights['alpha']), biases['alpha'])) # (batch_size, 1)\n        # calculate write weights as combination of previously read rows and least used rows\n        w_weights = tf.sigmoid(alpha) * prev_step[1] # (batch_size, mem_height) \n\n    # n-th smallest element's index in previous usage_weights\n    nth_smallest = tf.nn.top_k(-1*prev_step[2], k=number_of_reads, sorted=True)[1] # (batch_size, 1)\n\n    linear_index = tf.reshape(nth_smallest, [batch_size]) + (mem_height * tf.range(0,batch_size))\n    linear_w_weights = tf.reshape(w_weights, [-1])\n    ref = tf.Variable(linear_w_weights, trainable=False)\n    least_used_update = tf.reshape((1 - tf.sigmoid(alpha)), [batch_size])\n    w_weights = tf.stop_gradient(tf.reshape(tf.scatter_add(ref, linear_index, least_used_update, use_locking=True), \n                           [batch_size, mem_height]))\n\n    # put to 0 least used row in memory\n    linear_memory = tf.reshape(prev_step[0], [batch_size*mem_height, mem_width])\n    ref1 = tf.Variable(linear_memory, trainable=False)\n    memory = tf.stop_gradient(tf.reshape(tf.scatter_update(ref1, linear_index, tf.zeros([batch_size, mem_width]), use_locking=True),\n                        (batch_size,mem_height,mem_width))) # (batch_size,mem_height,mem_width)\n    # update memory state with write weights\n    # (batch_size, mem_height, mem_width)\n    memory = memory + \\\n            tf.batch_matmul(tf.reshape(w_weights, (batch_size,mem_height,1)), tf.reshape(key, (batch_size,1,mem_width)))\n        normed_key = key / tf.sqrt(tf.reduce_sum(tf.square(key))) # (batch_size, mem_width)\n        # (batch_size, mem_height, mem_width)\n        normed_memory = tf.div(memory, tf.sqrt(tf.reduce_sum(tf.square(memory), 1, keep_dims=True)))\n        # calculate similarities to each memory row\n        # (batch_size, mem_height)\n        similarity = tf.reshape(tf.batch_matmul(normed_memory, tf.reshape(normed_key, (batch_size,mem_width,1))),\n                                (batch_size,mem_height))\n        # calculate read weights as softmax probability distribution\n        r_weights = tf.nn.softmax(similarity) # (batch_size, mem_height)\n        # retrieve memory\n        retrieved_memory = tf.reshape(tf.batch_matmul(tf.reshape(r_weights, (batch_size,1,mem_height)), memory),\n                                      (batch_size,mem_width)) # (batch_size, mem_width)\n        # put retrieved memory through output layer to get prediction\n        do_output = tf.add(tf.matmul(retrieved_memory, weights['do']), biases['do'])\n        prediction = tf.nn.softmax(do_output) # (batch_size, n_classes)\n        # calculate usage weights\n        u_weights = gamma * prev_step[2] + r_weights + w_weights # (batch_size, mem_height)\n\n        return (memory, r_weights, u_weights, prediction) \n\ndef model(elems):\n        _, _, _, predictions = tf.scan(step, elems, \n                                    initializer=(np.zeros((batch_size, mem_height, mem_width)).astype(np.float32), \n                                      np.random.rand(batch_size, mem_height).astype(np.float32),\n                                      np.random.rand(batch_size, mem_height).astype(np.float32),\n                                      np.zeros((batch_size, n_classes)).astype(np.float32)), \n            parallel_iterations=1, back_prop=True, swap_memory=False)\n        return predictions\n\nweights = {\n  'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=stddev)),\n  'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=stddev)),\n  'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=stddev)),\n  'out': tf.Variable(tf.random_normal([n_hidden_3, mem_width],stddev=stddev)),\n  'do': tf.Variable(tf.random_normal([mem_width, n_classes],stddev=stddev)),\n  'alpha': tf.Variable(tf.random_normal([mem_width, 1],stddev=stddev))\n}\nbiases = {\n  'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n  'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n  'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n  'out': tf.Variable(tf.random_normal([mem_width])),\n  'do': tf.Variable(tf.random_normal([n_classes])),\n  'alpha': tf.Variable(tf.random_normal([1]))\n}\n\ninput_data = tf.placeholder(\"float32\",[n_classes*n_examples, batch_size, n_input], name='input_data')\nlabels = tf.placeholder(\"float32\",[n_classes*n_examples, batch_size, n_classes],name='labels')\n\npredictions = model(input_data)\n```\n\nAnd after this calling initialize_all_variables() to run causes that error\n\n> p.s. is this something for which you can use tf.get_variable?\n\nWhat exactly this call will do?\n"}