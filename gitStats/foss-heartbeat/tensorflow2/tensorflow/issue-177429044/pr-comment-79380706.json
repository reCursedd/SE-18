{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/79380706", "pull_request_review_id": 540019, "id": 79380706, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc5MzgwNzA2", "diff_hunk": "@@ -203,6 +203,48 @@ def _BiasAddGrad(op, received_grad):\n   return (received_grad, gen_nn_ops.bias_add_grad(out_backprop=received_grad,\n                                                   data_format=data_format))\n \n+@ops.RegisterGradient(\"BiasAddGrad\")\n+def _BiasAddGradGrad(op, received_grad):\n+  \"\"\"Gradient for the BiasAddGrad op.\n+\n+  Args:\n+    op: BiasAddGrad op for which we are calculating gradients.\n+    received_grad: The gradients passed to the BiasAddGrad op.\n+    \n+  Returns:\n+    A single gradient Tensor for the input to BiasAddGrad (which\n+    is the gradient of the bias term in BiasAdd)\n+  \"\"\"\n+  \n+  try:\n+    data_format = op.get_attr(\"data_format\")\n+  except ValueError:\n+    data_format = None\n+    \n+  #zeros = array_ops.zeros_like(op.inputs[0])\n+  #return gen_nn_ops._bias_add(zeros, received_grad, data_format=data_format)\n+  \n+  shape = array_ops.shape(op.inputs[0])\n+  rank = array_ops.rank(op.inputs[0])\n+  bias_shape = array_ops.shape(received_grad)", "path": "tensorflow/python/ops/nn_grad.py", "position": null, "original_position": 27, "commit_id": "3f7374316e116450ad6c2ba76cb5c55f24c2297f", "original_commit_id": "f73f856b5c09d2706a5e377896afc5165362b2a3", "user": {"login": "admcrae", "id": 11247551, "node_id": "MDQ6VXNlcjExMjQ3NTUx", "avatar_url": "https://avatars3.githubusercontent.com/u/11247551?v=4", "gravatar_id": "", "url": "https://api.github.com/users/admcrae", "html_url": "https://github.com/admcrae", "followers_url": "https://api.github.com/users/admcrae/followers", "following_url": "https://api.github.com/users/admcrae/following{/other_user}", "gists_url": "https://api.github.com/users/admcrae/gists{/gist_id}", "starred_url": "https://api.github.com/users/admcrae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/admcrae/subscriptions", "organizations_url": "https://api.github.com/users/admcrae/orgs", "repos_url": "https://api.github.com/users/admcrae/repos", "events_url": "https://api.github.com/users/admcrae/events{/privacy}", "received_events_url": "https://api.github.com/users/admcrae/received_events", "type": "User", "site_admin": false}, "body": "Some crude benchmarks:\n[benchmark.zip](https://github.com/tensorflow/tensorflow/files/479926/benchmark.zip)\n\nThe benchmarks were done on a workstation with an i7-965 (yes, we laughed too when we got it) and a GTX 1080.\n\nThe results are a bit odd. Using `bias_add` with `zeros_like` seems to give a negligible to moderate performance advantage in most cases, but it is 2-5x slower on the CPU with statically determined shapes (it looks like something is not being optimized properly here). I did notice that on the GPU, using `bias_add` seemed to result in somewhat higher GPU usage overall.\n\nI'm inclined to stay with `tf.tile` for now, since (1) it is a more direct solution (using `bias_add` seems extremely hacky), (2) its performance is more consistent, and (3) you mentioned that the performance of `tf.tile` might be improved in the future.\n\nOf course, the optimal solution would be to write another kernel for this operation (and then we could finally end the chain of gradient ops, since this operation's gradient is identical to `bias_add_grad`), but I'm not sure if that is worthwhile for something that most machine learning people aren't going to need.\n", "created_at": "2016-09-19T12:37:50Z", "updated_at": "2016-09-23T17:41:48Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4411#discussion_r79380706", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4411", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/79380706"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4411#discussion_r79380706"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4411"}}, "body_html": "<p>Some crude benchmarks:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/479926/benchmark.zip\">benchmark.zip</a></p>\n<p>The benchmarks were done on a workstation with an i7-965 (yes, we laughed too when we got it) and a GTX 1080.</p>\n<p>The results are a bit odd. Using <code>bias_add</code> with <code>zeros_like</code> seems to give a negligible to moderate performance advantage in most cases, but it is 2-5x slower on the CPU with statically determined shapes (it looks like something is not being optimized properly here). I did notice that on the GPU, using <code>bias_add</code> seemed to result in somewhat higher GPU usage overall.</p>\n<p>I'm inclined to stay with <code>tf.tile</code> for now, since (1) it is a more direct solution (using <code>bias_add</code> seems extremely hacky), (2) its performance is more consistent, and (3) you mentioned that the performance of <code>tf.tile</code> might be improved in the future.</p>\n<p>Of course, the optimal solution would be to write another kernel for this operation (and then we could finally end the chain of gradient ops, since this operation's gradient is identical to <code>bias_add_grad</code>), but I'm not sure if that is worthwhile for something that most machine learning people aren't going to need.</p>", "body_text": "Some crude benchmarks:\nbenchmark.zip\nThe benchmarks were done on a workstation with an i7-965 (yes, we laughed too when we got it) and a GTX 1080.\nThe results are a bit odd. Using bias_add with zeros_like seems to give a negligible to moderate performance advantage in most cases, but it is 2-5x slower on the CPU with statically determined shapes (it looks like something is not being optimized properly here). I did notice that on the GPU, using bias_add seemed to result in somewhat higher GPU usage overall.\nI'm inclined to stay with tf.tile for now, since (1) it is a more direct solution (using bias_add seems extremely hacky), (2) its performance is more consistent, and (3) you mentioned that the performance of tf.tile might be improved in the future.\nOf course, the optimal solution would be to write another kernel for this operation (and then we could finally end the chain of gradient ops, since this operation's gradient is identical to bias_add_grad), but I'm not sure if that is worthwhile for something that most machine learning people aren't going to need.", "in_reply_to_id": 79271411}