{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13814", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13814/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13814/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13814/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13814", "id": 266560235, "node_id": "MDU6SXNzdWUyNjY1NjAyMzU=", "number": 13814, "title": "Kernel/Bias created for only one LSTM when using BasicLSTMCell with MultiRNNCell", "user": {"login": "ggeor84", "id": 3958117, "node_id": "MDQ6VXNlcjM5NTgxMTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/3958117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ggeor84", "html_url": "https://github.com/ggeor84", "followers_url": "https://api.github.com/users/ggeor84/followers", "following_url": "https://api.github.com/users/ggeor84/following{/other_user}", "gists_url": "https://api.github.com/users/ggeor84/gists{/gist_id}", "starred_url": "https://api.github.com/users/ggeor84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ggeor84/subscriptions", "organizations_url": "https://api.github.com/users/ggeor84/orgs", "repos_url": "https://api.github.com/users/ggeor84/repos", "events_url": "https://api.github.com/users/ggeor84/events{/privacy}", "received_events_url": "https://api.github.com/users/ggeor84/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-10-18T16:36:02Z", "updated_at": "2017-12-21T07:14:24Z", "closed_at": "2017-12-21T07:14:24Z", "author_association": "NONE", "body_html": "<p>I am using the example code ptb_word_lm.py that creates 2 layers of lstm's. The code clearly creates 2 (c,h) tuples, each tuple corresponding to one LSTM. Each c and h should depend on matrices i,j,f,o (input_gate, new_input, foget_gate, outpu_gate) that are found in BasicLSTMCell  in rnn_cell_impl.py. With tf.global_variables() you should be able to get these matrices. In fact, it returns a kernel matrix and a bias matrix. The kernel matrix is of size 400x800 which corresponds to only one LSTM parameters. (the i,j,f,o matrices are concatenated horizontally. The matrices for the input and h_t-1 are concatenated vertically). However, whether I am using 1 layer LSTM or 2 layer LSTM, the number of parameters stays the same (400x800), when it should be 400x1600 or a tuple of some sort or an additional kernel and bias. It seems that tf.global_variables() does not expose all trainable parameters in this case. Is that true or am I missing something?</p>\n<p>EDIT:<br>\nI am now fairly certain that this is a problem, because when I use the \"block\" option in the script, multirnncell works correctly and generates 2 kernel matrices and 2 bias matrices. with BasicLSTMCell, it only generates one kernel matrix and 1 bias matrix.</p>", "body_text": "I am using the example code ptb_word_lm.py that creates 2 layers of lstm's. The code clearly creates 2 (c,h) tuples, each tuple corresponding to one LSTM. Each c and h should depend on matrices i,j,f,o (input_gate, new_input, foget_gate, outpu_gate) that are found in BasicLSTMCell  in rnn_cell_impl.py. With tf.global_variables() you should be able to get these matrices. In fact, it returns a kernel matrix and a bias matrix. The kernel matrix is of size 400x800 which corresponds to only one LSTM parameters. (the i,j,f,o matrices are concatenated horizontally. The matrices for the input and h_t-1 are concatenated vertically). However, whether I am using 1 layer LSTM or 2 layer LSTM, the number of parameters stays the same (400x800), when it should be 400x1600 or a tuple of some sort or an additional kernel and bias. It seems that tf.global_variables() does not expose all trainable parameters in this case. Is that true or am I missing something?\nEDIT:\nI am now fairly certain that this is a problem, because when I use the \"block\" option in the script, multirnncell works correctly and generates 2 kernel matrices and 2 bias matrices. with BasicLSTMCell, it only generates one kernel matrix and 1 bias matrix.", "body": "I am using the example code ptb_word_lm.py that creates 2 layers of lstm's. The code clearly creates 2 (c,h) tuples, each tuple corresponding to one LSTM. Each c and h should depend on matrices i,j,f,o (input_gate, new_input, foget_gate, outpu_gate) that are found in BasicLSTMCell  in rnn_cell_impl.py. With tf.global_variables() you should be able to get these matrices. In fact, it returns a kernel matrix and a bias matrix. The kernel matrix is of size 400x800 which corresponds to only one LSTM parameters. (the i,j,f,o matrices are concatenated horizontally. The matrices for the input and h_t-1 are concatenated vertically). However, whether I am using 1 layer LSTM or 2 layer LSTM, the number of parameters stays the same (400x800), when it should be 400x1600 or a tuple of some sort or an additional kernel and bias. It seems that tf.global_variables() does not expose all trainable parameters in this case. Is that true or am I missing something?\r\n\r\nEDIT:\r\nI am now fairly certain that this is a problem, because when I use the \"block\" option in the script, multirnncell works correctly and generates 2 kernel matrices and 2 bias matrices. with BasicLSTMCell, it only generates one kernel matrix and 1 bias matrix.\r\n"}