{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/262305928", "html_url": "https://github.com/tensorflow/tensorflow/issues/4790#issuecomment-262305928", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4790", "id": 262305928, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MjMwNTkyOA==", "user": {"login": "vade", "id": 65011, "node_id": "MDQ6VXNlcjY1MDEx", "avatar_url": "https://avatars1.githubusercontent.com/u/65011?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vade", "html_url": "https://github.com/vade", "followers_url": "https://api.github.com/users/vade/followers", "following_url": "https://api.github.com/users/vade/following{/other_user}", "gists_url": "https://api.github.com/users/vade/gists{/gist_id}", "starred_url": "https://api.github.com/users/vade/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vade/subscriptions", "organizations_url": "https://api.github.com/users/vade/orgs", "repos_url": "https://api.github.com/users/vade/repos", "events_url": "https://api.github.com/users/vade/events{/privacy}", "received_events_url": "https://api.github.com/users/vade/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-22T17:23:33Z", "updated_at": "2016-11-22T17:23:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm running into this issue on desktop - running optimize_for_inference drastically changes my model size, but it also appears to change the output tensors names, even if I specify the input and output tensors Im interested in the script.</p>\n<p>I understand that optimize_for_inference changes the graph, but should the i/o node names not stay the same since, well, you're asking the script to do so?</p>\n<p>Apologies for poking at a closed issue, trying to wrap my head around this.\u00a0</p>\n<p>Specifics are here: <a href=\"http://stackoverflow.com/questions/40747431/running-optimize-for-inference-on-inceptionv1-results-in-different-output-tensor\" rel=\"nofollow\">http://stackoverflow.com/questions/40747431/running-optimize-for-inference-on-inceptionv1-results-in-different-output-tensor</a></p>", "body_text": "I'm running into this issue on desktop - running optimize_for_inference drastically changes my model size, but it also appears to change the output tensors names, even if I specify the input and output tensors Im interested in the script.\nI understand that optimize_for_inference changes the graph, but should the i/o node names not stay the same since, well, you're asking the script to do so?\nApologies for poking at a closed issue, trying to wrap my head around this.\u00a0\nSpecifics are here: http://stackoverflow.com/questions/40747431/running-optimize-for-inference-on-inceptionv1-results-in-different-output-tensor", "body": "I'm running into this issue on desktop - running optimize_for_inference drastically changes my model size, but it also appears to change the output tensors names, even if I specify the input and output tensors Im interested in the script.\r\n\r\nI understand that optimize_for_inference changes the graph, but should the i/o node names not stay the same since, well, you're asking the script to do so?\r\n\r\nApologies for poking at a closed issue, trying to wrap my head around this.\u00a0\r\n\r\nSpecifics are here: http://stackoverflow.com/questions/40747431/running-optimize-for-inference-on-inceptionv1-results-in-different-output-tensor"}