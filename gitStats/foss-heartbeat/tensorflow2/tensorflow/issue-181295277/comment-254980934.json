{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/254980934", "html_url": "https://github.com/tensorflow/tensorflow/issues/4790#issuecomment-254980934", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4790", "id": 254980934, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NDk4MDkzNA==", "user": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-20T00:47:56Z", "updated_at": "2016-10-20T00:47:56Z", "author_association": "MEMBER", "body_html": "<p>There's a guide to preparing models for mobile devices here:<br>\n<a href=\"https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/\" rel=\"nofollow\">https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/</a><br>\nThe problem you're hitting (which is true across both Android and iOS versions) is that only certain ops are included in the library by default:</p>\n<blockquote>\n<p>Mobile devices have limited amounts of memory, and apps need to be downloaded, so by default the iOS version of TensorFlow only includes support for operations that are common in inference and don\u2019t have large external dependencies. You can see the list of supported ops in the tensorflow/contrib/makefile/tf_op_files.txt file.</p>\n</blockquote>\n<p>In this case, using a FifoQueue op for inference doesn't make much sense, so I believe you should be able to fix it by running the optimize_for_inference script as described in the tutorial. The tricky part will be figuring out the correct input and output node names for the model, but you'll need those when you run it, so hopefully you have them already.</p>", "body_text": "There's a guide to preparing models for mobile devices here:\nhttps://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/\nThe problem you're hitting (which is true across both Android and iOS versions) is that only certain ops are included in the library by default:\n\nMobile devices have limited amounts of memory, and apps need to be downloaded, so by default the iOS version of TensorFlow only includes support for operations that are common in inference and don\u2019t have large external dependencies. You can see the list of supported ops in the tensorflow/contrib/makefile/tf_op_files.txt file.\n\nIn this case, using a FifoQueue op for inference doesn't make much sense, so I believe you should be able to fix it by running the optimize_for_inference script as described in the tutorial. The tricky part will be figuring out the correct input and output node names for the model, but you'll need those when you run it, so hopefully you have them already.", "body": "There's a guide to preparing models for mobile devices here:\nhttps://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/\nThe problem you're hitting (which is true across both Android and iOS versions) is that only certain ops are included in the library by default:\n\n> Mobile devices have limited amounts of memory, and apps need to be downloaded, so by default the iOS version of TensorFlow only includes support for operations that are common in inference and don\u2019t have large external dependencies. You can see the list of supported ops in the tensorflow/contrib/makefile/tf_op_files.txt file.\n\nIn this case, using a FifoQueue op for inference doesn't make much sense, so I believe you should be able to fix it by running the optimize_for_inference script as described in the tutorial. The tricky part will be figuring out the correct input and output node names for the model, but you'll need those when you run it, so hopefully you have them already.\n"}