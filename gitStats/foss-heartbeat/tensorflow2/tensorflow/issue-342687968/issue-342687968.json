{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20964", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20964/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20964/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20964/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20964", "id": 342687968, "node_id": "MDU6SXNzdWUzNDI2ODc5Njg=", "number": 20964, "title": "Using tf.contrib.training.batch_sequences_with_states with tf.data.Dataset", "user": {"login": "npuichigo", "id": 11533479, "node_id": "MDQ6VXNlcjExNTMzNDc5", "avatar_url": "https://avatars3.githubusercontent.com/u/11533479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/npuichigo", "html_url": "https://github.com/npuichigo", "followers_url": "https://api.github.com/users/npuichigo/followers", "following_url": "https://api.github.com/users/npuichigo/following{/other_user}", "gists_url": "https://api.github.com/users/npuichigo/gists{/gist_id}", "starred_url": "https://api.github.com/users/npuichigo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/npuichigo/subscriptions", "organizations_url": "https://api.github.com/users/npuichigo/orgs", "repos_url": "https://api.github.com/users/npuichigo/repos", "events_url": "https://api.github.com/users/npuichigo/events{/privacy}", "received_events_url": "https://api.github.com/users/npuichigo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-07-19T11:24:16Z", "updated_at": "2018-11-14T19:23:02Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>As far as I know, tf.contrib.training.SequenceQueuingStateSaver relies on queues and queue-runners, so we'll still need to call tf.train.start_queue_runners() when used with the new tf.data.Dataset API.</p>\n<p>However, when I use tf.data.Iterator.from_structure to construct the input that is reusable with many different datasets, error occurs unless I initialize the Iterator before calling tf.train.start_queue_runners(). So I need to switch to a certain dataset first, and it's a quite strange usage.</p>\n<div class=\"highlight highlight-source-python\"><pre>iterator <span class=\"pl-k\">=</span> Iterator.from_structure(tf.int64, tf.TensorShape([]))\n\ndataset_range <span class=\"pl-k\">=</span> Dataset.range(<span class=\"pl-c1\">10</span>)\nrange_initializer <span class=\"pl-k\">=</span> iterator.make_initializer(dataset_range)\n\ndataset_evens <span class=\"pl-k\">=</span> dataset_range.filter(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x <span class=\"pl-k\">%</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>)\nevens_initializer <span class=\"pl-k\">=</span> iterator.make_initializer(dataset_evens)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Define a model based on the iterator; in this example, the model_fn</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> is expected to take scalar tf.int64 Tensors as input (see</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the definition of 'iterator' above).</span>\nprediction, loss <span class=\"pl-k\">=</span> model_fn(iterator.get_next())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> I need to pick a certain dataset, and run the initializer first.</span>\nsess.run(range_initializer)\ntf.train.start_queue_runners(<span class=\"pl-v\">sess</span><span class=\"pl-k\">=</span>sess, <span class=\"pl-v\">coord</span><span class=\"pl-k\">=</span>coord)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Train for `num_epochs`, where for each epoch, we first iterate over</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> dataset_range, and then iterate over dataset_evens.</span>\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_epochs):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialize the iterator to `dataset_range`</span>\n  sess.run(range_initializer)\n  <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n    <span class=\"pl-k\">try</span>:\n      pred, loss_val <span class=\"pl-k\">=</span> sess.run([prediction, loss])\n    <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n      <span class=\"pl-k\">break</span>\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialize the iterator to `dataset_evens`</span>\n  sess.run(evens_initializer)\n  <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n    <span class=\"pl-k\">try</span>:\n      pred, loss_val <span class=\"pl-k\">=</span> sess.run([prediction, loss])\n    <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n      <span class=\"pl-k\">break</span></pre></div>\n<p>By doing so, it seems that the queue will be closed when I switch to another dataset next time.</p>\n<p>So will tensorflow add batch_sequences_with_states like support to tf.data.Dataset without relying on the old queue mechanisms and calling tf.train.start_queue_runners explicitly.</p>", "body_text": "As far as I know, tf.contrib.training.SequenceQueuingStateSaver relies on queues and queue-runners, so we'll still need to call tf.train.start_queue_runners() when used with the new tf.data.Dataset API.\nHowever, when I use tf.data.Iterator.from_structure to construct the input that is reusable with many different datasets, error occurs unless I initialize the Iterator before calling tf.train.start_queue_runners(). So I need to switch to a certain dataset first, and it's a quite strange usage.\niterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))\n\ndataset_range = Dataset.range(10)\nrange_initializer = iterator.make_initializer(dataset_range)\n\ndataset_evens = dataset_range.filter(lambda x: x % 2 == 0)\nevens_initializer = iterator.make_initializer(dataset_evens)\n\n# Define a model based on the iterator; in this example, the model_fn\n# is expected to take scalar tf.int64 Tensors as input (see\n# the definition of 'iterator' above).\nprediction, loss = model_fn(iterator.get_next())\n\n# I need to pick a certain dataset, and run the initializer first.\nsess.run(range_initializer)\ntf.train.start_queue_runners(sess=sess, coord=coord)\n\n# Train for `num_epochs`, where for each epoch, we first iterate over\n# dataset_range, and then iterate over dataset_evens.\nfor _ in range(num_epochs):\n  # Initialize the iterator to `dataset_range`\n  sess.run(range_initializer)\n  while True:\n    try:\n      pred, loss_val = sess.run([prediction, loss])\n    except tf.errors.OutOfRangeError:\n      break\n\n  # Initialize the iterator to `dataset_evens`\n  sess.run(evens_initializer)\n  while True:\n    try:\n      pred, loss_val = sess.run([prediction, loss])\n    except tf.errors.OutOfRangeError:\n      break\nBy doing so, it seems that the queue will be closed when I switch to another dataset next time.\nSo will tensorflow add batch_sequences_with_states like support to tf.data.Dataset without relying on the old queue mechanisms and calling tf.train.start_queue_runners explicitly.", "body": "As far as I know, tf.contrib.training.SequenceQueuingStateSaver relies on queues and queue-runners, so we'll still need to call tf.train.start_queue_runners() when used with the new tf.data.Dataset API.\r\n\r\nHowever, when I use tf.data.Iterator.from_structure to construct the input that is reusable with many different datasets, error occurs unless I initialize the Iterator before calling tf.train.start_queue_runners(). So I need to switch to a certain dataset first, and it's a quite strange usage.\r\n\r\n```python\r\niterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))\r\n\r\ndataset_range = Dataset.range(10)\r\nrange_initializer = iterator.make_initializer(dataset_range)\r\n\r\ndataset_evens = dataset_range.filter(lambda x: x % 2 == 0)\r\nevens_initializer = iterator.make_initializer(dataset_evens)\r\n\r\n# Define a model based on the iterator; in this example, the model_fn\r\n# is expected to take scalar tf.int64 Tensors as input (see\r\n# the definition of 'iterator' above).\r\nprediction, loss = model_fn(iterator.get_next())\r\n\r\n# I need to pick a certain dataset, and run the initializer first.\r\nsess.run(range_initializer)\r\ntf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n# Train for `num_epochs`, where for each epoch, we first iterate over\r\n# dataset_range, and then iterate over dataset_evens.\r\nfor _ in range(num_epochs):\r\n  # Initialize the iterator to `dataset_range`\r\n  sess.run(range_initializer)\r\n  while True:\r\n    try:\r\n      pred, loss_val = sess.run([prediction, loss])\r\n    except tf.errors.OutOfRangeError:\r\n      break\r\n\r\n  # Initialize the iterator to `dataset_evens`\r\n  sess.run(evens_initializer)\r\n  while True:\r\n    try:\r\n      pred, loss_val = sess.run([prediction, loss])\r\n    except tf.errors.OutOfRangeError:\r\n      break\r\n```\r\n\r\nBy doing so, it seems that the queue will be closed when I switch to another dataset next time.\r\n\r\nSo will tensorflow add batch_sequences_with_states like support to tf.data.Dataset without relying on the old queue mechanisms and calling tf.train.start_queue_runners explicitly."}