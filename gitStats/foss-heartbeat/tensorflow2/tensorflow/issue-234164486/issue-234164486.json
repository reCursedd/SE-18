{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10488", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10488/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10488/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10488/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10488", "id": 234164486, "node_id": "MDU6SXNzdWUyMzQxNjQ0ODY=", "number": 10488, "title": "Changing the cache_size in Gemmlowp/meta/single_thread_gemm.h cause random error in Requantize nodes", "user": {"login": "qjivy", "id": 24410810, "node_id": "MDQ6VXNlcjI0NDEwODEw", "avatar_url": "https://avatars2.githubusercontent.com/u/24410810?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qjivy", "html_url": "https://github.com/qjivy", "followers_url": "https://api.github.com/users/qjivy/followers", "following_url": "https://api.github.com/users/qjivy/following{/other_user}", "gists_url": "https://api.github.com/users/qjivy/gists{/gist_id}", "starred_url": "https://api.github.com/users/qjivy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qjivy/subscriptions", "organizations_url": "https://api.github.com/users/qjivy/orgs", "repos_url": "https://api.github.com/users/qjivy/repos", "events_url": "https://api.github.com/users/qjivy/events{/privacy}", "received_events_url": "https://api.github.com/users/qjivy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2017-06-07T10:32:03Z", "updated_at": "2018-07-17T20:29:23Z", "closed_at": "2018-07-17T20:29:22Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNo</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux ubuntu 16.04</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:</p>\n</li>\n</ul>\n<p>source, NDK built Android ARM64 binary</p>\n<ul>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n</ul>\n<p>commit id: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/f48673b5054b474fa1e51823edd075088cd16d5f/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/f48673b5054b474fa1e51823edd075088cd16d5f\"><tt>f48673b</tt></a><br>\nAuthor: Luke Iwanski <a href=\"mailto:luke@codeplay.com\">luke@codeplay.com</a></p>\n<ul>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:<br>\n0.4.5</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nno</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nno</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:</p>\n</li>\n</ul>\n<ol>\n<li>\n<p>bazel --output_base=../out/armv8_benchmark_model/ build -s -c opt --jobs=1 --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  tensorflow/tools/benchmark:benchmark_model 2&gt;&amp;1 |tee log.txt<br>\n2.adb push benchmark_model /data/local/tmp/</p>\n</li>\n<li>\n<p>run_vgg1 on Nexus5X6P phone:<br>\nscript:<br>\ntime ./$1 --graph=$2   --input_layer=\"images:0\"   --input_layer_shape=\"1,224,224,3\"   --input_layer_type=\"float\"   --output_layer=\"prob:0\" --num_runs=1</p>\n</li>\n</ol>\n<p>The final command line that I use:<br>\n./run_vgg1.sh benchmark_model vgg16.8bit.weightsnodes.model</p>\n<h3>My problem</h3>\n<p>When I change the cache_size from 256<em>1024 to 128</em>1024 in the Gemmlowp/meta/single_thread_gemm.h, the benchmark_model randomly failed on the 8 bit quantized both node and weights vgg16 model.</p>\n<h3>Source code / logs</h3>\n<p>My Tensorflow commit-id:<br>\ncommit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/f48673b5054b474fa1e51823edd075088cd16d5f/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/f48673b5054b474fa1e51823edd075088cd16d5f\"><tt>f48673b</tt></a></p>\n<p>My modify for the Gemmlowp:<br>\nin file  gemmlowp/meta/single_thread_gemm.h:<br>\nchange all the \"int cache_size = 256 * 1024\" to \"int cache_size = 128 * 1024\".</p>\n<p>The error log is :<br>\nnative : benchmark_model.cc:381 Graph: [vgg16.8bit.weightsnodes.model]<br>\nnative : benchmark_model.cc:382 Input layers: [images:0]<br>\nnative : benchmark_model.cc:383 Input shapes: [1,224,224,3]<br>\nnative : benchmark_model.cc:384 Input types: [float]<br>\nnative : benchmark_model.cc:385 Output layers: [prob:0]<br>\nnative : benchmark_model.cc:386 Num runs: [1]<br>\nnative : benchmark_model.cc:387 Inter-run delay (seconds): [-1.0]<br>\nnative : benchmark_model.cc:388 Num threads: [-1]<br>\nnative : benchmark_model.cc:389 Benchmark name: []<br>\nnative : benchmark_model.cc:390 Output prefix: []<br>\nnative : benchmark_model.cc:391 Show sizes: [0]<br>\nnative : benchmark_model.cc:392 Warmup runs: [2]<br>\nnative : benchmark_model.cc:52 Loading TensorFlow.<br>\nnative : benchmark_model.cc:59 Got config, 0 devices<br>\ncan't determine number of CPU cores: assuming 4<br>\ncan't determine number of CPU cores: assuming 4<br>\nnative : benchmark_model.cc:257 Running benchmark for 2 iterations without detailed stat logging:<br>\nnative : benchmark_model.cc:233 Error during inference: Invalid argument: requested_output_max must be &gt;= requested_output_min, but got nan and 0<br>\n[[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]<br>\nnative : benchmark_model.cc:268 Failed on run 0<br>\nnative : benchmark_model.cc:451 Timing failed with Invalid argument: requested_output_max must be &gt;= requested_output_min, but got nan and 0<br>\n[[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]<br>\n0m11.12s real     0m27.55s user     0m01.87s system</p>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux ubuntu 16.04\n\n\nTensorFlow installed from (source or binary):\n\n\nsource, NDK built Android ARM64 binary\n\nTensorFlow version (use command below):\n\ncommit id: f48673b\nAuthor: Luke Iwanski luke@codeplay.com\n\n\nBazel version (if compiling from source):\n0.4.5\n\n\nCUDA/cuDNN version:\nno\n\n\nGPU model and memory:\nno\n\n\nExact command to reproduce:\n\n\n\n\nbazel --output_base=../out/armv8_benchmark_model/ build -s -c opt --jobs=1 --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  tensorflow/tools/benchmark:benchmark_model 2>&1 |tee log.txt\n2.adb push benchmark_model /data/local/tmp/\n\n\nrun_vgg1 on Nexus5X6P phone:\nscript:\ntime ./$1 --graph=$2   --input_layer=\"images:0\"   --input_layer_shape=\"1,224,224,3\"   --input_layer_type=\"float\"   --output_layer=\"prob:0\" --num_runs=1\n\n\nThe final command line that I use:\n./run_vgg1.sh benchmark_model vgg16.8bit.weightsnodes.model\nMy problem\nWhen I change the cache_size from 2561024 to 1281024 in the Gemmlowp/meta/single_thread_gemm.h, the benchmark_model randomly failed on the 8 bit quantized both node and weights vgg16 model.\nSource code / logs\nMy Tensorflow commit-id:\ncommit f48673b\nMy modify for the Gemmlowp:\nin file  gemmlowp/meta/single_thread_gemm.h:\nchange all the \"int cache_size = 256 * 1024\" to \"int cache_size = 128 * 1024\".\nThe error log is :\nnative : benchmark_model.cc:381 Graph: [vgg16.8bit.weightsnodes.model]\nnative : benchmark_model.cc:382 Input layers: [images:0]\nnative : benchmark_model.cc:383 Input shapes: [1,224,224,3]\nnative : benchmark_model.cc:384 Input types: [float]\nnative : benchmark_model.cc:385 Output layers: [prob:0]\nnative : benchmark_model.cc:386 Num runs: [1]\nnative : benchmark_model.cc:387 Inter-run delay (seconds): [-1.0]\nnative : benchmark_model.cc:388 Num threads: [-1]\nnative : benchmark_model.cc:389 Benchmark name: []\nnative : benchmark_model.cc:390 Output prefix: []\nnative : benchmark_model.cc:391 Show sizes: [0]\nnative : benchmark_model.cc:392 Warmup runs: [2]\nnative : benchmark_model.cc:52 Loading TensorFlow.\nnative : benchmark_model.cc:59 Got config, 0 devices\ncan't determine number of CPU cores: assuming 4\ncan't determine number of CPU cores: assuming 4\nnative : benchmark_model.cc:257 Running benchmark for 2 iterations without detailed stat logging:\nnative : benchmark_model.cc:233 Error during inference: Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\n[[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]\nnative : benchmark_model.cc:268 Failed on run 0\nnative : benchmark_model.cc:451 Timing failed with Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\n[[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]\n0m11.12s real     0m27.55s user     0m01.87s system", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nsource, NDK built Android ARM64 binary\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\ncommit id: f48673b5054b474fa1e51823edd075088cd16d5f\r\nAuthor: Luke Iwanski <luke@codeplay.com>\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n\r\n- **CUDA/cuDNN version**:\r\nno\r\n\r\n- **GPU model and memory**:\r\nno\r\n\r\n- **Exact command to reproduce**:\r\n1. bazel --output_base=../out/armv8_benchmark_model/ build -s -c opt --jobs=1 --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  tensorflow/tools/benchmark:benchmark_model 2>&1 |tee log.txt\r\n2.adb push benchmark_model /data/local/tmp/\r\n \r\n3. run_vgg1 on Nexus5X6P phone:\r\nscript:\r\ntime ./$1 --graph=$2   --input_layer=\"images:0\"   --input_layer_shape=\"1,224,224,3\"   --input_layer_type=\"float\"   --output_layer=\"prob:0\" --num_runs=1\r\n\r\nThe final command line that I use:\r\n./run_vgg1.sh benchmark_model vgg16.8bit.weightsnodes.model\r\n\r\n### My problem\r\nWhen I change the cache_size from 256*1024 to 128*1024 in the Gemmlowp/meta/single_thread_gemm.h, the benchmark_model randomly failed on the 8 bit quantized both node and weights vgg16 model.\r\n\r\n### Source code / logs\r\nMy Tensorflow commit-id:\r\ncommit f48673b5054b474fa1e51823edd075088cd16d5f\r\n\r\nMy modify for the Gemmlowp:\r\nin file  gemmlowp/meta/single_thread_gemm.h:\r\nchange all the \"int cache_size = 256 * 1024\" to \"int cache_size = 128 * 1024\".\r\n\r\nThe error log is :\r\nnative : benchmark_model.cc:381 Graph: [vgg16.8bit.weightsnodes.model]\r\nnative : benchmark_model.cc:382 Input layers: [images:0]\r\nnative : benchmark_model.cc:383 Input shapes: [1,224,224,3]\r\nnative : benchmark_model.cc:384 Input types: [float]\r\nnative : benchmark_model.cc:385 Output layers: [prob:0]\r\nnative : benchmark_model.cc:386 Num runs: [1]\r\nnative : benchmark_model.cc:387 Inter-run delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:388 Num threads: [-1]\r\nnative : benchmark_model.cc:389 Benchmark name: []\r\nnative : benchmark_model.cc:390 Output prefix: []\r\nnative : benchmark_model.cc:391 Show sizes: [0]\r\nnative : benchmark_model.cc:392 Warmup runs: [2]\r\nnative : benchmark_model.cc:52 Loading TensorFlow.\r\nnative : benchmark_model.cc:59 Got config, 0 devices\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : benchmark_model.cc:257 Running benchmark for 2 iterations without detailed stat logging:\r\nnative : benchmark_model.cc:233 Error during inference: Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\r\n\t [[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]\r\nnative : benchmark_model.cc:268 Failed on run 0\r\nnative : benchmark_model.cc:451 Timing failed with Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\r\n\t [[Node: fc8/BiasAdd/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fc8/BiasAdd/eightbit, fc8/BiasAdd/eightbit:1, fc8/BiasAdd/eightbit:2, fc8/BiasAdd/eightbit/requant_range, fc8/BiasAdd/eightbit/requant_range:1)]]\r\n    0m11.12s real     0m27.55s user     0m01.87s system\r\n\r\n\r\n\r\n"}