{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/311870194", "html_url": "https://github.com/tensorflow/tensorflow/issues/10488#issuecomment-311870194", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10488", "id": 311870194, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTg3MDE5NA==", "user": {"login": "qjivy", "id": 24410810, "node_id": "MDQ6VXNlcjI0NDEwODEw", "avatar_url": "https://avatars2.githubusercontent.com/u/24410810?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qjivy", "html_url": "https://github.com/qjivy", "followers_url": "https://api.github.com/users/qjivy/followers", "following_url": "https://api.github.com/users/qjivy/following{/other_user}", "gists_url": "https://api.github.com/users/qjivy/gists{/gist_id}", "starred_url": "https://api.github.com/users/qjivy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qjivy/subscriptions", "organizations_url": "https://api.github.com/users/qjivy/orgs", "repos_url": "https://api.github.com/users/qjivy/repos", "events_url": "https://api.github.com/users/qjivy/events{/privacy}", "received_events_url": "https://api.github.com/users/qjivy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-29T05:59:32Z", "updated_at": "2017-06-29T06:04:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The same error seems happens on the quantized MobileNet_v1_1.0_224 too.<br>\nBy reference this page:<br>\n<a href=\"https://github.com/tensorflow/models/tree/master/slim\">https://github.com/tensorflow/models/tree/master/slim</a></p>\n<p><strong>Using the following command line, I get the mobilenet_v1_224.pb.</strong></p>\n<p>$ python export_inference_graph.py <br>\n--alsologtostderr <br>\n--model_name=mobilenet_v1 <br>\n--image_size=224 <br>\n--output_file=/tmp/mobilenet_v1_224.pb</p>\n<p><strong>Then I use the following command line to freeze it. The checkpoint file is downloaded from  <a href=\"http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz\" rel=\"nofollow\">here</a></strong></p>\n<p>python bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=../model/models/slim/download_models/mobilenet/mobilenet_v1_224.pb  --input_checkpoint=../model/models/slim/download_models/mobilenet/mobilenet_v1_1.0_224.ckpt --input_binary=true --output_graph=../model/models/slim/download_models/mobilenet/freeze_mobilenet_v1_224.pb --output_node_names=MobilenetV1/Predictions/Reshape_1</p>\n<p><strong>Then quantized it using the \"trasnform_graph\" to quantize both the wights and node.</strong><br>\n./transform_graph --in_graph=./freeze_mobilenet_v1_224.pb --out_graph=./quantized_freeze_mobilenet_v1_224.pb --inputs='input' --outputs=\"MobilenetV1/Predictions/Reshape_1\" --transforms='quantize_weights quantize_nodes'</p>\n<p><strong>After  I got the quantized model, I run it with benchmark_model then get the fault.</strong><br>\ncommand line of benchmark_model:<br>\n./benchmark_model_armv8a_0628 --graph=quantized_freeze_mobilenet_v1_224.pb --input_layer=\"input\"   --input_layer_shape=\"1,224,224,3\"   --input_layer_type=\"float\"   --output_layer=\"MobilenetV1/Predictions/Reshape_1\" --num_runs=20</p>\n<h2><strong>The</strong> error log<br>\nnative : benchmark_model.cc:382 Graph: [quantized_freeze_mobilenet_v1_224.pb]<br>\nnative : benchmark_model.cc:383 Input layers: [input]<br>\nnative : benchmark_model.cc:384 Input shapes: [1,224,224,3]<br>\nnative : benchmark_model.cc:385 Input types: [float]<br>\nnative : benchmark_model.cc:386 Output layers: [MobilenetV1/Predictions/Reshape_1]<br>\nnative : benchmark_model.cc:387 Num runs: [20]<br>\nnative : benchmark_model.cc:388 Inter-run delay (seconds): [-1.0]<br>\nnative : benchmark_model.cc:389 Num threads: [-1]<br>\nnative : benchmark_model.cc:390 Benchmark name: []<br>\nnative : benchmark_model.cc:391 Output prefix: []<br>\nnative : benchmark_model.cc:392 Show sizes: [0]<br>\nnative : benchmark_model.cc:393 Warmup runs: [2]<br>\nnative : benchmark_model.cc:53 Loading TensorFlow.<br>\nnative : benchmark_model.cc:60 Got config, 0 devices<br>\ncan't determine number of CPU cores: assuming 4<br>\ncan't determine number of CPU cores: assuming 4<br>\nnative : benchmark_model.cc:258 Running benchmark for 2 iterations without detailed stat logging:<br>\nnative : benchmark_model.cc:234 Error during inference: Invalid argument: requested_output_max must be &gt;= requested_output_min, but got nan and 0<br>\n[[Node: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:1, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:2, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range:1)]]<br>\nnative : benchmark_model.cc:269 Failed on run 0<br>\nnative : benchmark_model.cc:452 Timing failed with Invalid argument: requested_output_max must be &gt;= requested_output_min, but got nan and 0<br>\n[[Node: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:1, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:2, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range:1)]]</h2>", "body_text": "The same error seems happens on the quantized MobileNet_v1_1.0_224 too.\nBy reference this page:\nhttps://github.com/tensorflow/models/tree/master/slim\nUsing the following command line, I get the mobilenet_v1_224.pb.\n$ python export_inference_graph.py \n--alsologtostderr \n--model_name=mobilenet_v1 \n--image_size=224 \n--output_file=/tmp/mobilenet_v1_224.pb\nThen I use the following command line to freeze it. The checkpoint file is downloaded from  here\npython bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=../model/models/slim/download_models/mobilenet/mobilenet_v1_224.pb  --input_checkpoint=../model/models/slim/download_models/mobilenet/mobilenet_v1_1.0_224.ckpt --input_binary=true --output_graph=../model/models/slim/download_models/mobilenet/freeze_mobilenet_v1_224.pb --output_node_names=MobilenetV1/Predictions/Reshape_1\nThen quantized it using the \"trasnform_graph\" to quantize both the wights and node.\n./transform_graph --in_graph=./freeze_mobilenet_v1_224.pb --out_graph=./quantized_freeze_mobilenet_v1_224.pb --inputs='input' --outputs=\"MobilenetV1/Predictions/Reshape_1\" --transforms='quantize_weights quantize_nodes'\nAfter  I got the quantized model, I run it with benchmark_model then get the fault.\ncommand line of benchmark_model:\n./benchmark_model_armv8a_0628 --graph=quantized_freeze_mobilenet_v1_224.pb --input_layer=\"input\"   --input_layer_shape=\"1,224,224,3\"   --input_layer_type=\"float\"   --output_layer=\"MobilenetV1/Predictions/Reshape_1\" --num_runs=20\nThe error log\nnative : benchmark_model.cc:382 Graph: [quantized_freeze_mobilenet_v1_224.pb]\nnative : benchmark_model.cc:383 Input layers: [input]\nnative : benchmark_model.cc:384 Input shapes: [1,224,224,3]\nnative : benchmark_model.cc:385 Input types: [float]\nnative : benchmark_model.cc:386 Output layers: [MobilenetV1/Predictions/Reshape_1]\nnative : benchmark_model.cc:387 Num runs: [20]\nnative : benchmark_model.cc:388 Inter-run delay (seconds): [-1.0]\nnative : benchmark_model.cc:389 Num threads: [-1]\nnative : benchmark_model.cc:390 Benchmark name: []\nnative : benchmark_model.cc:391 Output prefix: []\nnative : benchmark_model.cc:392 Show sizes: [0]\nnative : benchmark_model.cc:393 Warmup runs: [2]\nnative : benchmark_model.cc:53 Loading TensorFlow.\nnative : benchmark_model.cc:60 Got config, 0 devices\ncan't determine number of CPU cores: assuming 4\ncan't determine number of CPU cores: assuming 4\nnative : benchmark_model.cc:258 Running benchmark for 2 iterations without detailed stat logging:\nnative : benchmark_model.cc:234 Error during inference: Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\n[[Node: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:1, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:2, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range:1)]]\nnative : benchmark_model.cc:269 Failed on run 0\nnative : benchmark_model.cc:452 Timing failed with Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\n[[Node: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:1, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:2, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range:1)]]", "body": "The same error seems happens on the quantized MobileNet_v1_1.0_224 too.\r\nBy reference this page:\r\nhttps://github.com/tensorflow/models/tree/master/slim\r\n\r\n**Using the following command line, I get the mobilenet_v1_224.pb.**\r\n\r\n$ python export_inference_graph.py \\\r\n  --alsologtostderr \\\r\n  --model_name=mobilenet_v1 \\\r\n  --image_size=224 \\\r\n  --output_file=/tmp/mobilenet_v1_224.pb\r\n\r\n**Then I use the following command line to freeze it. The checkpoint file is downloaded from  [here](http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz)**\r\n\r\npython bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=../model/models/slim/download_models/mobilenet/mobilenet_v1_224.pb  --input_checkpoint=../model/models/slim/download_models/mobilenet/mobilenet_v1_1.0_224.ckpt --input_binary=true --output_graph=../model/models/slim/download_models/mobilenet/freeze_mobilenet_v1_224.pb --output_node_names=MobilenetV1/Predictions/Reshape_1\r\n\r\n**Then quantized it using the \"trasnform_graph\" to quantize both the wights and node.**\r\n./transform_graph --in_graph=./freeze_mobilenet_v1_224.pb --out_graph=./quantized_freeze_mobilenet_v1_224.pb --inputs='input' --outputs=\"MobilenetV1/Predictions/Reshape_1\" --transforms='quantize_weights quantize_nodes' \r\n\r\n**After  I got the quantized model, I run it with benchmark_model then get the fault.**\r\ncommand line of benchmark_model:\r\n ./benchmark_model_armv8a_0628 --graph=quantized_freeze_mobilenet_v1_224.pb --input_layer=\"input\"   --input_layer_shape=\"1,224,224,3\"   --input_layer_type=\"float\"   --output_layer=\"MobilenetV1/Predictions/Reshape_1\" --num_runs=20\r\n\r\n**The** error log\r\nnative : benchmark_model.cc:382 Graph: [quantized_freeze_mobilenet_v1_224.pb]\r\nnative : benchmark_model.cc:383 Input layers: [input]\r\nnative : benchmark_model.cc:384 Input shapes: [1,224,224,3]\r\nnative : benchmark_model.cc:385 Input types: [float]\r\nnative : benchmark_model.cc:386 Output layers: [MobilenetV1/Predictions/Reshape_1]\r\nnative : benchmark_model.cc:387 Num runs: [20]\r\nnative : benchmark_model.cc:388 Inter-run delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:389 Num threads: [-1]\r\nnative : benchmark_model.cc:390 Benchmark name: []\r\nnative : benchmark_model.cc:391 Output prefix: []\r\nnative : benchmark_model.cc:392 Show sizes: [0]\r\nnative : benchmark_model.cc:393 Warmup runs: [2]\r\nnative : benchmark_model.cc:53 Loading TensorFlow.\r\nnative : benchmark_model.cc:60 Got config, 0 devices\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : benchmark_model.cc:258 Running benchmark for 2 iterations without detailed stat logging:\r\nnative : benchmark_model.cc:234 Error during inference: Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:1, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:2, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range:1)]]\r\nnative : benchmark_model.cc:269 Failed on run 0\r\nnative : benchmark_model.cc:452 Timing failed with Invalid argument: requested_output_max must be >= requested_output_min, but got nan and 0\r\n\t [[Node: MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:1, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit:2, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range, MobilenetV1/MobilenetV1/Conv2d_2_pointwise/BatchNorm/batchnorm/mul/eightbit/requant_range:1)]]\r\n----\r\n"}