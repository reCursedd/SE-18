{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312124687", "html_url": "https://github.com/tensorflow/tensorflow/issues/8820#issuecomment-312124687", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8820", "id": 312124687, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjEyNDY4Nw==", "user": {"login": "raulpuric", "id": 9101033, "node_id": "MDQ6VXNlcjkxMDEwMzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/9101033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raulpuric", "html_url": "https://github.com/raulpuric", "followers_url": "https://api.github.com/users/raulpuric/followers", "following_url": "https://api.github.com/users/raulpuric/following{/other_user}", "gists_url": "https://api.github.com/users/raulpuric/gists{/gist_id}", "starred_url": "https://api.github.com/users/raulpuric/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raulpuric/subscriptions", "organizations_url": "https://api.github.com/users/raulpuric/orgs", "repos_url": "https://api.github.com/users/raulpuric/repos", "events_url": "https://api.github.com/users/raulpuric/events{/privacy}", "received_events_url": "https://api.github.com/users/raulpuric/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-29T22:22:01Z", "updated_at": "2017-06-29T22:42:10Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> Tensorflow is always lauded for it's portability. However, even if one was to use <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=28743821\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/viper7882\">@viper7882</a>'s solution they'd still be constrained to using gpus, traditional cpus, and commonly used FPGAs with OpenCL vendor libraries. This is not true general portability.</p>\n<p>Would it be possible to add a custom device feature/interface that allows us to add unique devices, treating the tensor forward and backward computations within the context as a giant black box, that can be fed values via grpc serialization. Or the ability to pass a custom compiled executable/binary and link it in to the rest of the graph (not just a tensorflow lambda func). This would greatly help the applied deep learning community so we can stop pulling values out of the runtime graph, processing them, and reinserting it into the computation graph.</p>\n<p>Here are just several use cases based on projects I'm working on now:</p>\n<ul>\n<li>Integration of tensorflow into high throughput/low latency real time systems where we want to define custom caching logic for only a few specified ops to meet unique run time criteria.</li>\n<li>Usage of tensorflow to interface with custom ASICs without OpenCL libraries\n<ul>\n<li>usage of ASICs specific to performing fast neural memory access in NTMs/NDCs/etc</li>\n<li>usage of analog devices with a digital interface such as an Analog neural network (sub)graph or analog transmitter/receiver</li>\n</ul>\n</li>\n<li>Easy ability to use tensorflow autodifferentiation with more abstract concepts in NN theory (such as deep function machines)</li>\n<li>Easy ability to add distributed (something like MPI) computation for a specific tensor. FireCaffe/DeepScale have the functionality.</li>\n</ul>\n<p>My basic understanding is that the device contexts solely serve to define compile logic of the tensors in the context and properly link the compiled nodes with the rest of the graph. I'd implement a PR myself, but I only have a good grasp on the grpc serialization logic.</p>\n<p>Wish I understood the graph compilation portion better :(</p>", "body_text": "@aselle Tensorflow is always lauded for it's portability. However, even if one was to use @viper7882's solution they'd still be constrained to using gpus, traditional cpus, and commonly used FPGAs with OpenCL vendor libraries. This is not true general portability.\nWould it be possible to add a custom device feature/interface that allows us to add unique devices, treating the tensor forward and backward computations within the context as a giant black box, that can be fed values via grpc serialization. Or the ability to pass a custom compiled executable/binary and link it in to the rest of the graph (not just a tensorflow lambda func). This would greatly help the applied deep learning community so we can stop pulling values out of the runtime graph, processing them, and reinserting it into the computation graph.\nHere are just several use cases based on projects I'm working on now:\n\nIntegration of tensorflow into high throughput/low latency real time systems where we want to define custom caching logic for only a few specified ops to meet unique run time criteria.\nUsage of tensorflow to interface with custom ASICs without OpenCL libraries\n\nusage of ASICs specific to performing fast neural memory access in NTMs/NDCs/etc\nusage of analog devices with a digital interface such as an Analog neural network (sub)graph or analog transmitter/receiver\n\n\nEasy ability to use tensorflow autodifferentiation with more abstract concepts in NN theory (such as deep function machines)\nEasy ability to add distributed (something like MPI) computation for a specific tensor. FireCaffe/DeepScale have the functionality.\n\nMy basic understanding is that the device contexts solely serve to define compile logic of the tensors in the context and properly link the compiled nodes with the rest of the graph. I'd implement a PR myself, but I only have a good grasp on the grpc serialization logic.\nWish I understood the graph compilation portion better :(", "body": "@aselle Tensorflow is always lauded for it's portability. However, even if one was to use @viper7882's solution they'd still be constrained to using gpus, traditional cpus, and commonly used FPGAs with OpenCL vendor libraries. This is not true general portability.\r\n\r\nWould it be possible to add a custom device feature/interface that allows us to add unique devices, treating the tensor forward and backward computations within the context as a giant black box, that can be fed values via grpc serialization. Or the ability to pass a custom compiled executable/binary and link it in to the rest of the graph (not just a tensorflow lambda func). This would greatly help the applied deep learning community so we can stop pulling values out of the runtime graph, processing them, and reinserting it into the computation graph.\r\n\r\nHere are just several use cases based on projects I'm working on now:\r\n* Integration of tensorflow into high throughput/low latency real time systems where we want to define custom caching logic for only a few specified ops to meet unique run time criteria.\r\n* Usage of tensorflow to interface with custom ASICs without OpenCL libraries\r\n  * usage of ASICs specific to performing fast neural memory access in NTMs/NDCs/etc\r\n  * usage of analog devices with a digital interface such as an Analog neural network (sub)graph or analog transmitter/receiver \r\n* Easy ability to use tensorflow autodifferentiation with more abstract concepts in NN theory (such as deep function machines)\r\n* Easy ability to add distributed (something like MPI) computation for a specific tensor. FireCaffe/DeepScale have the functionality.\r\n\r\nMy basic understanding is that the device contexts solely serve to define compile logic of the tensors in the context and properly link the compiled nodes with the rest of the graph. I'd implement a PR myself, but I only have a good grasp on the grpc serialization logic. \r\n\r\nWish I understood the graph compilation portion better :("}