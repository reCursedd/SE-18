{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273114865", "html_url": "https://github.com/tensorflow/tensorflow/issues/3056#issuecomment-273114865", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3056", "id": 273114865, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzExNDg2NQ==", "user": {"login": "chiragjn", "id": 10295418, "node_id": "MDQ6VXNlcjEwMjk1NDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/10295418?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chiragjn", "html_url": "https://github.com/chiragjn", "followers_url": "https://api.github.com/users/chiragjn/followers", "following_url": "https://api.github.com/users/chiragjn/following{/other_user}", "gists_url": "https://api.github.com/users/chiragjn/gists{/gist_id}", "starred_url": "https://api.github.com/users/chiragjn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chiragjn/subscriptions", "organizations_url": "https://api.github.com/users/chiragjn/orgs", "repos_url": "https://api.github.com/users/chiragjn/repos", "events_url": "https://api.github.com/users/chiragjn/events{/privacy}", "received_events_url": "https://api.github.com/users/chiragjn/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-17T11:29:05Z", "updated_at": "2017-01-17T11:35:00Z", "author_association": "NONE", "body_html": "<pre lang=\"#\" data-meta=\"Running with tensorflow (0.12.1)\"><code>import tensorflow as tf\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell\n\nbatch_size = 100\nsize = 20\nnum_encoder_symbols = 30\nembedding_size = 64\nstate_is_tuple = True\nnum_layers = 3\nencoder_inputs = [tf.zeros([32],dtype=tf.int32) for _ in range(batch_size)]\nsingle_cell = tf.nn.rnn_cell.BasicLSTMCell(size, state_is_tuple=state_is_tuple) # Or GRU\ncell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\nencoder_cell = rnn_cell.EmbeddingWrapper(cell, embedding_classes=num_encoder_symbols, embedding_size=embedding_size)\n_, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=tf.float32)\nencoder_state[0].get_shape()\n</code></pre>\n<p><code>encoder_state</code> is a tuple of <code>LSTMStateTuple</code> of size <code>num_layers</code></p>\n<p>Calling <code>encoder_state[0].get_shape()</code> fails with <code> 'LSTMStateTuple' object has no attribute 'get_shape'</code></p>\n<p>For now <code>encoder_state[0].c.get_shape()</code> , <code>encoder_state[0].h.get_shape()</code> is the only way I know to work around this<br>\nor concatenating the states which is slower and goes against using state_is_tuple=True</p>", "body_text": "import tensorflow as tf\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.ops import rnn_cell\n\nbatch_size = 100\nsize = 20\nnum_encoder_symbols = 30\nembedding_size = 64\nstate_is_tuple = True\nnum_layers = 3\nencoder_inputs = [tf.zeros([32],dtype=tf.int32) for _ in range(batch_size)]\nsingle_cell = tf.nn.rnn_cell.BasicLSTMCell(size, state_is_tuple=state_is_tuple) # Or GRU\ncell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\nencoder_cell = rnn_cell.EmbeddingWrapper(cell, embedding_classes=num_encoder_symbols, embedding_size=embedding_size)\n_, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=tf.float32)\nencoder_state[0].get_shape()\n\nencoder_state is a tuple of LSTMStateTuple of size num_layers\nCalling encoder_state[0].get_shape() fails with  'LSTMStateTuple' object has no attribute 'get_shape'\nFor now encoder_state[0].c.get_shape() , encoder_state[0].h.get_shape() is the only way I know to work around this\nor concatenating the states which is slower and goes against using state_is_tuple=True", "body": "```# Running with tensorflow (0.12.1)\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import rnn\r\nfrom tensorflow.python.ops import rnn_cell\r\n\r\nbatch_size = 100\r\nsize = 20\r\nnum_encoder_symbols = 30\r\nembedding_size = 64\r\nstate_is_tuple = True\r\nnum_layers = 3\r\nencoder_inputs = [tf.zeros([32],dtype=tf.int32) for _ in range(batch_size)]\r\nsingle_cell = tf.nn.rnn_cell.BasicLSTMCell(size, state_is_tuple=state_is_tuple) # Or GRU\r\ncell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\r\nencoder_cell = rnn_cell.EmbeddingWrapper(cell, embedding_classes=num_encoder_symbols, embedding_size=embedding_size)\r\n_, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=tf.float32)\r\nencoder_state[0].get_shape()\r\n```\r\n\r\n`encoder_state` is a tuple of `LSTMStateTuple` of size `num_layers`\r\n\r\nCalling `encoder_state[0].get_shape()` fails with ` 'LSTMStateTuple' object has no attribute 'get_shape'`\r\n\r\nFor now `encoder_state[0].c.get_shape()` , `encoder_state[0].h.get_shape()` is the only way I know to work around this\r\nor concatenating the states which is slower and goes against using state_is_tuple=True"}