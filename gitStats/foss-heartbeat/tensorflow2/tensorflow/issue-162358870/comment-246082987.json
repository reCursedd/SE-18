{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/246082987", "html_url": "https://github.com/tensorflow/tensorflow/issues/3056#issuecomment-246082987", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3056", "id": 246082987, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NjA4Mjk4Nw==", "user": {"login": "zzjin13", "id": 7436697, "node_id": "MDQ6VXNlcjc0MzY2OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7436697?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zzjin13", "html_url": "https://github.com/zzjin13", "followers_url": "https://api.github.com/users/zzjin13/followers", "following_url": "https://api.github.com/users/zzjin13/following{/other_user}", "gists_url": "https://api.github.com/users/zzjin13/gists{/gist_id}", "starred_url": "https://api.github.com/users/zzjin13/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zzjin13/subscriptions", "organizations_url": "https://api.github.com/users/zzjin13/orgs", "repos_url": "https://api.github.com/users/zzjin13/repos", "events_url": "https://api.github.com/users/zzjin13/events{/privacy}", "received_events_url": "https://api.github.com/users/zzjin13/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-10T02:31:05Z", "updated_at": "2016-09-10T02:38:51Z", "author_association": "NONE", "body_html": "<p>In fact, I run the embedding_rnn_seq2seq model before. It works well with my GPU. However, when I switch to embedding_attention_seq2seq model with the same hyper-parameters, it cannot work even if I set the batch size to 1 (128 before) and RNN hidden state dimension to 1 (256 before). So I think this is not a issue related with memory. Any other factors may cause this problem?</p>", "body_text": "In fact, I run the embedding_rnn_seq2seq model before. It works well with my GPU. However, when I switch to embedding_attention_seq2seq model with the same hyper-parameters, it cannot work even if I set the batch size to 1 (128 before) and RNN hidden state dimension to 1 (256 before). So I think this is not a issue related with memory. Any other factors may cause this problem?", "body": "In fact, I run the embedding_rnn_seq2seq model before. It works well with my GPU. However, when I switch to embedding_attention_seq2seq model with the same hyper-parameters, it cannot work even if I set the batch size to 1 (128 before) and RNN hidden state dimension to 1 (256 before). So I think this is not a issue related with memory. Any other factors may cause this problem?\n"}