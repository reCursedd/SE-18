{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3582", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3582/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3582/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3582/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3582", "id": 168441344, "node_id": "MDU6SXNzdWUxNjg0NDEzNDQ=", "number": 3582, "title": "Loss not decreasing, GTX 1070, cuDNN 5.0 for RC 8.0, Cuda 8.0 RC", "user": {"login": "TylerBalsam", "id": 6165535, "node_id": "MDQ6VXNlcjYxNjU1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/6165535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TylerBalsam", "html_url": "https://github.com/TylerBalsam", "followers_url": "https://api.github.com/users/TylerBalsam/followers", "following_url": "https://api.github.com/users/TylerBalsam/following{/other_user}", "gists_url": "https://api.github.com/users/TylerBalsam/gists{/gist_id}", "starred_url": "https://api.github.com/users/TylerBalsam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TylerBalsam/subscriptions", "organizations_url": "https://api.github.com/users/TylerBalsam/orgs", "repos_url": "https://api.github.com/users/TylerBalsam/repos", "events_url": "https://api.github.com/users/TylerBalsam/events{/privacy}", "received_events_url": "https://api.github.com/users/TylerBalsam/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-07-30T01:24:31Z", "updated_at": "2017-11-09T03:35:02Z", "closed_at": "2016-08-17T16:45:11Z", "author_association": "NONE", "body_html": "<p>I've spent about 12 hours monkeying around before throwing in the towel here. I've checked the other similar threads on this and done the steps there. This includes <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"162587064\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3068\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3068/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3068\">#3068</a> and <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"167629897\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3507\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3507/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3507\">#3507</a>.</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04</p>\n<p>Installed version of CUDA and cuDNN:<br>\nCuda 8.0 RC<br>\ncuDNN 5.0 RC<br>\nOutput of libcudn*:<br>\n/usr/local/cuda/lib64/libcudadevrt.a       /usr/local/cuda/lib64/libcudnn.so<br>\n/usr/local/cuda/lib64/libcudart.so         /usr/local/cuda/lib64/libcudnn.so.5<br>\n/usr/local/cuda/lib64/libcudart.so.8.0     /usr/local/cuda/lib64/libcudnn.so.5.0.5<br>\n/usr/local/cuda/lib64/libcudart.so.8.0.27  /usr/local/cuda/lib64/libcudnn_static.a<br>\n/usr/local/cuda/lib64/libcudart_static.a</p>\n<p>Built from source</p>\n<ol>\n<li>The commit hash: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/5161e4c51b994b3feb93cdb851479c29a3450f31/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/5161e4c51b994b3feb93cdb851479c29a3450f31\"><tt>5161e4c</tt></a></li>\n<li>The output of <code>bazel version</code>:<br>\nBuild label: 0.3.0<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)<br>\nBuild timestamp: 1465558703<br>\nBuild timestamp as int: 1465558703</li>\n</ol>\n<h3>Notes</h3>\n<p>I've built and thrown out, built and thrown out, cleaned the slate and reinstalled in about as many permutations as I can think. However, with training, the loss freezes. I simplified my network all the way down to a shallow autoencoder where there is only one image in the training set, and still the loss freezes. This worked completely fine using the CPU for similar code from a regular old pip distro. I'm using the MSE loss on a 224*224 image scaled between 0 and 1 to try to keep things as regular as possible.</p>\n<p>Something odd to note is that no matter what my loss starts at, it always tends to 11893.53125. Ironically, if it starts below there, it goes up to it. Example of several 40 epoch runs: 11893.52930, 11893.53223, 11893.52930, 11893.52930, 11893.52930.</p>\n<p>My theory was that it was outputting 0s -- assuming an even distribution, the summed MSE loss would be 224x224x.5^2 = 12544, which is pretty close to our 11893.52930. Since regular images won't have an even pixel distribution, this makes sense. In practice, after testing the network, it outputs all 1s.</p>\n<p>A side note is that when running the trainer test, the lambda value was not always exactly 2 -- every few iterations it was off by a fraction.</p>\n<p>I'm really scratching my head here -- any folks with combat experience see any telltale signs of a solution? Thank you very much in advance for your time.</p>", "body_text": "I've spent about 12 hours monkeying around before throwing in the towel here. I've checked the other similar threads on this and done the steps there. This includes #3068 and #3507.\nEnvironment info\nOperating System: Ubuntu 14.04\nInstalled version of CUDA and cuDNN:\nCuda 8.0 RC\ncuDNN 5.0 RC\nOutput of libcudn*:\n/usr/local/cuda/lib64/libcudadevrt.a       /usr/local/cuda/lib64/libcudnn.so\n/usr/local/cuda/lib64/libcudart.so         /usr/local/cuda/lib64/libcudnn.so.5\n/usr/local/cuda/lib64/libcudart.so.8.0     /usr/local/cuda/lib64/libcudnn.so.5.0.5\n/usr/local/cuda/lib64/libcudart.so.8.0.27  /usr/local/cuda/lib64/libcudnn_static.a\n/usr/local/cuda/lib64/libcudart_static.a\nBuilt from source\n\nThe commit hash: 5161e4c\nThe output of bazel version:\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n\nNotes\nI've built and thrown out, built and thrown out, cleaned the slate and reinstalled in about as many permutations as I can think. However, with training, the loss freezes. I simplified my network all the way down to a shallow autoencoder where there is only one image in the training set, and still the loss freezes. This worked completely fine using the CPU for similar code from a regular old pip distro. I'm using the MSE loss on a 224*224 image scaled between 0 and 1 to try to keep things as regular as possible.\nSomething odd to note is that no matter what my loss starts at, it always tends to 11893.53125. Ironically, if it starts below there, it goes up to it. Example of several 40 epoch runs: 11893.52930, 11893.53223, 11893.52930, 11893.52930, 11893.52930.\nMy theory was that it was outputting 0s -- assuming an even distribution, the summed MSE loss would be 224x224x.5^2 = 12544, which is pretty close to our 11893.52930. Since regular images won't have an even pixel distribution, this makes sense. In practice, after testing the network, it outputs all 1s.\nA side note is that when running the trainer test, the lambda value was not always exactly 2 -- every few iterations it was off by a fraction.\nI'm really scratching my head here -- any folks with combat experience see any telltale signs of a solution? Thank you very much in advance for your time.", "body": "I've spent about 12 hours monkeying around before throwing in the towel here. I've checked the other similar threads on this and done the steps there. This includes #3068 and #3507. \n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \nCuda 8.0 RC\ncuDNN 5.0 RC\nOutput of libcudn*:\n/usr/local/cuda/lib64/libcudadevrt.a       /usr/local/cuda/lib64/libcudnn.so\n/usr/local/cuda/lib64/libcudart.so         /usr/local/cuda/lib64/libcudnn.so.5\n/usr/local/cuda/lib64/libcudart.so.8.0     /usr/local/cuda/lib64/libcudnn.so.5.0.5\n/usr/local/cuda/lib64/libcudart.so.8.0.27  /usr/local/cuda/lib64/libcudnn_static.a\n/usr/local/cuda/lib64/libcudart_static.a\n\nBuilt from source\n1. The commit hash: 5161e4c51b994b3feb93cdb851479c29a3450f31\n2. The output of `bazel version`:\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n### Notes\n\nI've built and thrown out, built and thrown out, cleaned the slate and reinstalled in about as many permutations as I can think. However, with training, the loss freezes. I simplified my network all the way down to a shallow autoencoder where there is only one image in the training set, and still the loss freezes. This worked completely fine using the CPU for similar code from a regular old pip distro. I'm using the MSE loss on a 224*224 image scaled between 0 and 1 to try to keep things as regular as possible.\n\nSomething odd to note is that no matter what my loss starts at, it always tends to 11893.53125. Ironically, if it starts below there, it goes up to it. Example of several 40 epoch runs: 11893.52930, 11893.53223, 11893.52930, 11893.52930, 11893.52930.\n\nMy theory was that it was outputting 0s -- assuming an even distribution, the summed MSE loss would be 224x224x.5^2 = 12544, which is pretty close to our 11893.52930. Since regular images won't have an even pixel distribution, this makes sense. In practice, after testing the network, it outputs all 1s.\n\nA side note is that when running the trainer test, the lambda value was not always exactly 2 -- every few iterations it was off by a fraction.\n\nI'm really scratching my head here -- any folks with combat experience see any telltale signs of a solution? Thank you very much in advance for your time.\n"}