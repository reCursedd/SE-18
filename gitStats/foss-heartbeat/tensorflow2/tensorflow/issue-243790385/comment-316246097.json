{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316246097", "html_url": "https://github.com/tensorflow/tensorflow/issues/11583#issuecomment-316246097", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11583", "id": 316246097, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjI0NjA5Nw==", "user": {"login": "Loohaze", "id": 18029266, "node_id": "MDQ6VXNlcjE4MDI5MjY2", "avatar_url": "https://avatars3.githubusercontent.com/u/18029266?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Loohaze", "html_url": "https://github.com/Loohaze", "followers_url": "https://api.github.com/users/Loohaze/followers", "following_url": "https://api.github.com/users/Loohaze/following{/other_user}", "gists_url": "https://api.github.com/users/Loohaze/gists{/gist_id}", "starred_url": "https://api.github.com/users/Loohaze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Loohaze/subscriptions", "organizations_url": "https://api.github.com/users/Loohaze/orgs", "repos_url": "https://api.github.com/users/Loohaze/repos", "events_url": "https://api.github.com/users/Loohaze/events{/privacy}", "received_events_url": "https://api.github.com/users/Loohaze/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-19T01:31:53Z", "updated_at": "2017-07-19T01:31:53Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a>  I checked the embedding_size's value is 256. the function embedding_attention_seq2seq is called and the parameters are</p>\n<pre><code>encoder_inputs,   &lt;class 'list'&gt;\ndecoder_inputs,   &lt;class 'list'&gt;\ncell,   cell:&lt;tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object&gt;\nnum_encoder_symbols,   60000\nnum_decoder_symbols,   60000\nembedding_size,   256\nnum_heads=1,   1\noutput_projection=None,    \nfeed_previous=False,    False\ndtype=None,    float32\nscope=None,   None\ninitial_state_attention=False   False\n</code></pre>", "body_text": "@drpngx  I checked the embedding_size's value is 256. the function embedding_attention_seq2seq is called and the parameters are\nencoder_inputs,   <class 'list'>\ndecoder_inputs,   <class 'list'>\ncell,   cell:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object>\nnum_encoder_symbols,   60000\nnum_decoder_symbols,   60000\nembedding_size,   256\nnum_heads=1,   1\noutput_projection=None,    \nfeed_previous=False,    False\ndtype=None,    float32\nscope=None,   None\ninitial_state_attention=False   False", "body": "@drpngx  I checked the embedding_size's value is 256. the function embedding_attention_seq2seq is called and the parameters are \r\n```\r\nencoder_inputs,   <class 'list'>\r\ndecoder_inputs,   <class 'list'>\r\ncell,   cell:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object>\r\nnum_encoder_symbols,   60000\r\nnum_decoder_symbols,   60000\r\nembedding_size,   256\r\nnum_heads=1,   1\r\noutput_projection=None,    \r\nfeed_previous=False,    False\r\ndtype=None,    float32\r\nscope=None,   None\r\ninitial_state_attention=False   False\r\n```"}