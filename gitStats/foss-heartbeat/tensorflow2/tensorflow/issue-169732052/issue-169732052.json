{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3676", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3676/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3676/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3676/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3676", "id": 169732052, "node_id": "MDU6SXNzdWUxNjk3MzIwNTI=", "number": 3676, "title": "BatchNorm on GPU become very slow in latest TF", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-08-06T06:06:37Z", "updated_at": "2016-08-08T06:59:32Z", "closed_at": "2016-08-08T06:59:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Nightly built, python2 gpu, cuda 7.5, cudnn 4.0.7, archlinux. TitanX.<br>\nI run <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/batch_norm_benchmark.py\">batch_norm_benchmark.py</a> and got the following output:</p>\n<pre><code>Forward convolution (lower layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.043865 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.053864 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.088060 secs\n=== op vs py: 22.8% ===\n=== py vs slow: 63.5% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.287913 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.284179 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.287409 secs\n=== op vs py: -1.3% ===\n=== py vs slow: 1.1% ===\nForward/backward convolution (lower layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.220112 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.201284 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.295103 secs\n=== op vs py: -8.6% ===\n=== py vs slow: 46.6% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 2.108785 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.578407 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.863753 secs\n=== op vs py: -59.0% ===\n=== py vs slow: -96.6% ===\nForward convolution (higher layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.025443 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.033344 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.048162 secs\n=== op vs py: 31.1% ===\n=== py vs slow: 44.4% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.164241 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.161473 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.163212 secs\n=== op vs py: -1.7% ===\n=== py vs slow: 1.1% ===\nForward/backward convolution (higher layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.111931 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.118556 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.163289 secs\n=== op vs py: 5.9% ===\n=== py vs slow: 37.7% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 1.194288 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.329403 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.491005 secs\n=== op vs py: -72.4% ===\n=== py vs slow: 49.1% ===\nForward fully-connected.\ncpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.001866 secs\ncpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002859 secs\n=== py vs slow: 53.2% ===\ngpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.002420 secs\ngpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002523 secs\n=== py vs slow: 4.3% ===\nForward/backward fully-connected.\ncpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.004973 secs\ncpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.006781 secs\n=== py vs slow: 36.4% ===\ngpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.006370 secs\ngpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.008182 secs\n=== py vs slow: 28.4% ===\n</code></pre>\n<p>Although <code>sess.run</code> in a loop might not be an accurate way to benchmark, I think the performance regression exists because running the same script with an earlier binary built (not sure which commit it is) is 10x-20x faster.</p>", "body_text": "Nightly built, python2 gpu, cuda 7.5, cudnn 4.0.7, archlinux. TitanX.\nI run batch_norm_benchmark.py and got the following output:\nForward convolution (lower layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.043865 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.053864 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.088060 secs\n=== op vs py: 22.8% ===\n=== py vs slow: 63.5% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.287913 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.284179 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.287409 secs\n=== op vs py: -1.3% ===\n=== py vs slow: 1.1% ===\nForward/backward convolution (lower layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.220112 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.201284 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.295103 secs\n=== op vs py: -8.6% ===\n=== py vs slow: 46.6% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 2.108785 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.578407 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.863753 secs\n=== op vs py: -59.0% ===\n=== py vs slow: -96.6% ===\nForward convolution (higher layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.025443 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.033344 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.048162 secs\n=== op vs py: 31.1% ===\n=== py vs slow: 44.4% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.164241 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.161473 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.163212 secs\n=== op vs py: -1.7% ===\n=== py vs slow: 1.1% ===\nForward/backward convolution (higher layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.111931 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.118556 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.163289 secs\n=== op vs py: 5.9% ===\n=== py vs slow: 37.7% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 1.194288 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.329403 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.491005 secs\n=== op vs py: -72.4% ===\n=== py vs slow: 49.1% ===\nForward fully-connected.\ncpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.001866 secs\ncpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002859 secs\n=== py vs slow: 53.2% ===\ngpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.002420 secs\ngpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002523 secs\n=== py vs slow: 4.3% ===\nForward/backward fully-connected.\ncpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.004973 secs\ncpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.006781 secs\n=== py vs slow: 36.4% ===\ngpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.006370 secs\ngpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.008182 secs\n=== py vs slow: 28.4% ===\n\nAlthough sess.run in a loop might not be an accurate way to benchmark, I think the performance regression exists because running the same script with an earlier binary built (not sure which commit it is) is 10x-20x faster.", "body": "Nightly built, python2 gpu, cuda 7.5, cudnn 4.0.7, archlinux. TitanX.\nI run [batch_norm_benchmark.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/batch_norm_benchmark.py) and got the following output:\n\n```\nForward convolution (lower layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.043865 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.053864 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.088060 secs\n=== op vs py: 22.8% ===\n=== py vs slow: 63.5% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.287913 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.284179 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.287409 secs\n=== op vs py: -1.3% ===\n=== py vs slow: 1.1% ===\nForward/backward convolution (lower layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.220112 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.201284 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.295103 secs\n=== op vs py: -8.6% ===\n=== py vs slow: 46.6% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 2.108785 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.578407 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.863753 secs\n=== op vs py: -59.0% ===\n=== py vs slow: -96.6% ===\nForward convolution (higher layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.025443 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.033344 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.048162 secs\n=== op vs py: 31.1% ===\n=== py vs slow: 44.4% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:False - 0.164241 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:False - 0.161473 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:False - 0.163212 secs\n=== op vs py: -1.7% ===\n=== py vs slow: 1.1% ===\nForward/backward convolution (higher layers).\ncpu shape:4/3 #layers:10 mode:op scale:True train:True - 0.111931 secs\ncpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.118556 secs\ncpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.163289 secs\n=== op vs py: 5.9% ===\n=== py vs slow: 37.7% ===\ngpu shape:4/3 #layers:10 mode:op scale:True train:True - 1.194288 secs\ngpu shape:4/3 #layers:10 mode:py scale:True train:True - 0.329403 secs\ngpu shape:4/3 #layers:10 mode:slow scale:True train:True - 0.491005 secs\n=== op vs py: -72.4% ===\n=== py vs slow: 49.1% ===\nForward fully-connected.\ncpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.001866 secs\ncpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002859 secs\n=== py vs slow: 53.2% ===\ngpu shape:2/1 #layers:10 mode:py scale:True train:False - 0.002420 secs\ngpu shape:2/1 #layers:10 mode:slow scale:True train:False - 0.002523 secs\n=== py vs slow: 4.3% ===\nForward/backward fully-connected.\ncpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.004973 secs\ncpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.006781 secs\n=== py vs slow: 36.4% ===\ngpu shape:2/1 #layers:10 mode:py scale:True train:True - 0.006370 secs\ngpu shape:2/1 #layers:10 mode:slow scale:True train:True - 0.008182 secs\n=== py vs slow: 28.4% ===\n```\n\nAlthough `sess.run` in a loop might not be an accurate way to benchmark, I think the performance regression exists because running the same script with an earlier binary built (not sure which commit it is) is 10x-20x faster.\n"}