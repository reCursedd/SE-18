{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/401527480", "html_url": "https://github.com/tensorflow/tensorflow/issues/3937#issuecomment-401527480", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3937", "id": 401527480, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTUyNzQ4MA==", "user": {"login": "burnpanck", "id": 1310437, "node_id": "MDQ6VXNlcjEzMTA0Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1310437?v=4", "gravatar_id": "", "url": "https://api.github.com/users/burnpanck", "html_url": "https://github.com/burnpanck", "followers_url": "https://api.github.com/users/burnpanck/followers", "following_url": "https://api.github.com/users/burnpanck/following{/other_user}", "gists_url": "https://api.github.com/users/burnpanck/gists{/gist_id}", "starred_url": "https://api.github.com/users/burnpanck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/burnpanck/subscriptions", "organizations_url": "https://api.github.com/users/burnpanck/orgs", "repos_url": "https://api.github.com/users/burnpanck/repos", "events_url": "https://api.github.com/users/burnpanck/events{/privacy}", "received_events_url": "https://api.github.com/users/burnpanck/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-30T08:45:08Z", "updated_at": "2018-06-30T08:45:08Z", "author_association": "NONE", "body_html": "<p>Just a +me-too here. The great thing with <code>tf.add_check_numerics</code> is that it allows one to relatively quickly identify where NaNs and Infs appear first, which is usually the place one is most interested in. However, this assumption fails for graphs, where NaN's and Infs a are expected and later explicitly handled, such as in the implementation of <code>tf.pow</code>. The problem is, as soon as I have one such case in my graph, I cannot use the great <code>tf.add_check_numerics</code> anymore, even when it would be perfectly fine for the 99 % remainder of the graph.<br>\nThe \"perfect\" generic \"magicaly debug my big hairy numeric algorithm\" solution would probably require something like a \"non-signalling\" assert, i.e. an exception that propagates through the graph and only is reported if it makes it to the end; exactly like fp non-signalling NaNs and Infs, but with the additional traceback/debugging information being pulled along, in order to still be able to identify the source. Unfortunately, that would be very difficult to implement.<br>\nOne workaround that would at least allow me to continue to use <code>tf.add_check_numerics</code> would be a way to explicitly silence/remove those checks from operations that I have explicitly identified as being unproblematic. Is there a way to accomplish this?<br>\nIn any case, a warning in the documentation of <code>tf.add_check_numerics</code> (ideally with a list of well known operations with this problem) would probably save many people hours of debugging such issues. Of course, a fused implementation of the gradient of pow would avoid this particular case, as would an optimiser identifying and replacing the very common case of  <code>x**2</code>.</p>", "body_text": "Just a +me-too here. The great thing with tf.add_check_numerics is that it allows one to relatively quickly identify where NaNs and Infs appear first, which is usually the place one is most interested in. However, this assumption fails for graphs, where NaN's and Infs a are expected and later explicitly handled, such as in the implementation of tf.pow. The problem is, as soon as I have one such case in my graph, I cannot use the great tf.add_check_numerics anymore, even when it would be perfectly fine for the 99 % remainder of the graph.\nThe \"perfect\" generic \"magicaly debug my big hairy numeric algorithm\" solution would probably require something like a \"non-signalling\" assert, i.e. an exception that propagates through the graph and only is reported if it makes it to the end; exactly like fp non-signalling NaNs and Infs, but with the additional traceback/debugging information being pulled along, in order to still be able to identify the source. Unfortunately, that would be very difficult to implement.\nOne workaround that would at least allow me to continue to use tf.add_check_numerics would be a way to explicitly silence/remove those checks from operations that I have explicitly identified as being unproblematic. Is there a way to accomplish this?\nIn any case, a warning in the documentation of tf.add_check_numerics (ideally with a list of well known operations with this problem) would probably save many people hours of debugging such issues. Of course, a fused implementation of the gradient of pow would avoid this particular case, as would an optimiser identifying and replacing the very common case of  x**2.", "body": "Just a +me-too here. The great thing with `tf.add_check_numerics` is that it allows one to relatively quickly identify where NaNs and Infs appear first, which is usually the place one is most interested in. However, this assumption fails for graphs, where NaN's and Infs a are expected and later explicitly handled, such as in the implementation of `tf.pow`. The problem is, as soon as I have one such case in my graph, I cannot use the great `tf.add_check_numerics` anymore, even when it would be perfectly fine for the 99 % remainder of the graph.\r\nThe \"perfect\" generic \"magicaly debug my big hairy numeric algorithm\" solution would probably require something like a \"non-signalling\" assert, i.e. an exception that propagates through the graph and only is reported if it makes it to the end; exactly like fp non-signalling NaNs and Infs, but with the additional traceback/debugging information being pulled along, in order to still be able to identify the source. Unfortunately, that would be very difficult to implement.\r\nOne workaround that would at least allow me to continue to use `tf.add_check_numerics` would be a way to explicitly silence/remove those checks from operations that I have explicitly identified as being unproblematic. Is there a way to accomplish this?\r\nIn any case, a warning in the documentation of `tf.add_check_numerics` (ideally with a list of well known operations with this problem) would probably save many people hours of debugging such issues. Of course, a fused implementation of the gradient of pow would avoid this particular case, as would an optimiser identifying and replacing the very common case of  `x**2`."}