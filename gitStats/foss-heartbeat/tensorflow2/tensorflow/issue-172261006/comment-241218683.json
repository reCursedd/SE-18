{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241218683", "html_url": "https://github.com/tensorflow/tensorflow/issues/3937#issuecomment-241218683", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3937", "id": 241218683, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTIxODY4Mw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-20T19:22:05Z", "updated_at": "2016-12-29T18:25:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>That's normal functionality of check_gradient -- you run it on a code which<br>\nshouldn't have any NaN or inf values, and it fails with<br>\nIllegalArgumentError on the first inf/nan it finds.</p>\n<p>However, having NaN or Inf in your computation isn't necessarily a problem.<br>\nYou can divide by inf to get 0, or you can add extra logic to replace it<br>\nwith regular value, which is what Pow gradient is doing</p>\n<p>Below is implementation of Pow grad<br>\n(tensorflow/python/ops/math_grad.py#L544). You can see it'll computes Log<br>\nof 0, but there's a \"where\" clause which replaces negative infinities with<br>\n0's so they don't break the result</p>\n<pre><code>  # Avoid false singularity at x = 0\n  log_x = math_ops.select(x &gt; 0, math_ops.log(x), array_ops.zeros_like(x))\n\n</code></pre>\n<p>On Sat, Aug 20, 2016 at 1:35 AM, Shi Jiaxin <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>Environment info</p>\n<p>Operating System: Ubuntu 16.04</p>\n<p>Installed version of CUDA and cuDNN: None<br>\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed. <a href=\"https://storage.googleapis\" rel=\"nofollow\">https://storage.googleapis</a>.<br>\ncom/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-<br>\nnone-linux_x86_64.whl<br>\n<a href=\"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl</a></li>\n<li>The output from python -c \"import tensorflow;<br>\nprint(tensorflow.<strong>version</strong>)\". 0.10.0rc0</li>\n</ol>\n<p>Steps to reproduce</p>\n<p>import tensorflow as tf<br>\nx = tf.Variable(0.)<br>\nx_grad = tf.gradients(x**2, x)<br>\ninit = tf.initialize_all_variables()<br>\ncheck_op = tf.add_check_numerics_ops()<br>\nsess = tf.InteractiveSession()<br>\nsess.run(init)<br>\nsess.run(x_grad + [check_op])# =&gt; InvalidArgumentError: gradients/pow_grad/Log:0 : Tensor had Inf values<br>\nsess.run(x_grad)# =&gt; [0.0]</p>\n<p>My guess is that the check_op forces the two factors in grad(x<strong>2) =<br>\n(e</strong>(2log x)) * (1./x) to separately evaluate. Then 1./x causes an Inf.</p>\n<p>\u2014<br>\nYou are receiving this because you are subscribed to this thread.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"172261006\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3937\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3937/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3937\">#3937</a>, or mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/AABaHGTAjWdQnsYYTdRjuMe_NripnAcdks5qhrw-gaJpZM4JpB78\">https://github.com/notifications/unsubscribe-auth/AABaHGTAjWdQnsYYTdRjuMe_NripnAcdks5qhrw-gaJpZM4JpB78</a><br>\n.</p>\n</blockquote>", "body_text": "That's normal functionality of check_gradient -- you run it on a code which\nshouldn't have any NaN or inf values, and it fails with\nIllegalArgumentError on the first inf/nan it finds.\nHowever, having NaN or Inf in your computation isn't necessarily a problem.\nYou can divide by inf to get 0, or you can add extra logic to replace it\nwith regular value, which is what Pow gradient is doing\nBelow is implementation of Pow grad\n(tensorflow/python/ops/math_grad.py#L544). You can see it'll computes Log\nof 0, but there's a \"where\" clause which replaces negative infinities with\n0's so they don't break the result\n  # Avoid false singularity at x = 0\n  log_x = math_ops.select(x > 0, math_ops.log(x), array_ops.zeros_like(x))\n\n\nOn Sat, Aug 20, 2016 at 1:35 AM, Shi Jiaxin notifications@github.com\nwrote:\n\nEnvironment info\nOperating System: Ubuntu 16.04\nInstalled version of CUDA and cuDNN: None\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nIf installed from binary pip package, provide:\n\nWhich pip package you installed. https://storage.googleapis.\ncom/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-\nnone-linux_x86_64.whl\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\nThe output from python -c \"import tensorflow;\nprint(tensorflow.version)\". 0.10.0rc0\n\nSteps to reproduce\nimport tensorflow as tf\nx = tf.Variable(0.)\nx_grad = tf.gradients(x**2, x)\ninit = tf.initialize_all_variables()\ncheck_op = tf.add_check_numerics_ops()\nsess = tf.InteractiveSession()\nsess.run(init)\nsess.run(x_grad + [check_op])# => InvalidArgumentError: gradients/pow_grad/Log:0 : Tensor had Inf values\nsess.run(x_grad)# => [0.0]\nMy guess is that the check_op forces the two factors in grad(x2) =\n(e(2log x)) * (1./x) to separately evaluate. Then 1./x causes an Inf.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n#3937, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABaHGTAjWdQnsYYTdRjuMe_NripnAcdks5qhrw-gaJpZM4JpB78\n.", "body": "That's normal functionality of check_gradient -- you run it on a code which\r\nshouldn't have any NaN or inf values, and it fails with\r\nIllegalArgumentError on the first inf/nan it finds.\r\n\r\nHowever, having NaN or Inf in your computation isn't necessarily a problem.\r\nYou can divide by inf to get 0, or you can add extra logic to replace it\r\nwith regular value, which is what Pow gradient is doing\r\n\r\nBelow is implementation of Pow grad\r\n(tensorflow/python/ops/math_grad.py#L544). You can see it'll computes Log\r\nof 0, but there's a \"where\" clause which replaces negative infinities with\r\n0's so they don't break the result\r\n\r\n```\r\n  # Avoid false singularity at x = 0\r\n  log_x = math_ops.select(x > 0, math_ops.log(x), array_ops.zeros_like(x))\r\n\r\n```\r\nOn Sat, Aug 20, 2016 at 1:35 AM, Shi Jiaxin notifications@github.com\r\nwrote:\r\n\r\n> Environment info\r\n> \r\n> Operating System: Ubuntu 16.04\r\n> \r\n> Installed version of CUDA and cuDNN: None\r\n> (please attach the output of ls -l /path/to/cuda/lib/libcud*):\r\n> \r\n> If installed from binary pip package, provide:\r\n> 1. Which pip package you installed. https://storage.googleapis.\r\n>    com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-\r\n>    none-linux_x86_64.whl\r\n>    https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\r\n> 2. The output from python -c \"import tensorflow;\r\n>    print(tensorflow.**version**)\". 0.10.0rc0\r\n> \r\n> Steps to reproduce\r\n> \r\n> import tensorflow as tf\r\n> x = tf.Variable(0.)\r\n> x_grad = tf.gradients(x**2, x)\r\n> init = tf.initialize_all_variables()\r\n> check_op = tf.add_check_numerics_ops()\r\n> sess = tf.InteractiveSession()\r\n> sess.run(init)\r\n> sess.run(x_grad + [check_op])# => InvalidArgumentError: gradients/pow_grad/Log:0 : Tensor had Inf values\r\n> sess.run(x_grad)# => [0.0]\r\n> \r\n> My guess is that the check_op forces the two factors in grad(x**2) =\r\n> (e**(2log x)) \\* (1./x) to separately evaluate. Then 1./x causes an Inf.\r\n> \r\n> \u2014\r\n> You are receiving this because you are subscribed to this thread.\r\n> Reply to this email directly, view it on GitHub\r\n> https://github.com/tensorflow/tensorflow/issues/3937, or mute the thread\r\n> https://github.com/notifications/unsubscribe-auth/AABaHGTAjWdQnsYYTdRjuMe_NripnAcdks5qhrw-gaJpZM4JpB78\r\n> .\r\n"}