{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/231805433", "html_url": "https://github.com/tensorflow/tensorflow/issues/3270#issuecomment-231805433", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3270", "id": 231805433, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMTgwNTQzMw==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-11T17:31:09Z", "updated_at": "2016-07-11T17:31:09Z", "author_association": "MEMBER", "body_html": "<p>As said in the other thread -- you want separate variables for each model. So you need to use \"with tf.variable_scope(...)\" before creating each model. And since you didn't do this before training, you'll have to re-name the variables you restore from the checkpoint. This can be done by passing a dict to the Saver object, rather than just a list of variables (as is done now). Let me know if you have more specific questions, I'll be happy to help, but here is the overview.</p>\n<p>(0) Currently both your models are trying to share parameters, that's where the error comes from (they cannot share because vocab sizes are different, but it's just a consequence of a different problem: you don't want to share the variables, you want 2 separate sets of them).</p>\n<p>(1) You cannot create 2 separate sets just with 2 sessions, you need to put them in variable_scopes. Just before calling create_model, open a scope, for example:<br>\nwith tf.variable_scope('en_fr'):<br>\nen_fr_model = create_model(...)<br>\nwith tf.variable_scope('fr_en'):<br>\nfr_en_model = create_model(...)</p>\n<p>(2) Now you have 2 sets of variables, but their names don't correspond to the names you've saved in checkpoints during training, because your names are now prefixed with 'en_fr/' and 'fr_en/' and those in checkpoint are not. So you need to add some logic to the Saver object in the model to tell it to prefix the variables from the checkpoint in the right way. This can be done by passing a dict to the Saver, see <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L791\">here</a></p>", "body_text": "As said in the other thread -- you want separate variables for each model. So you need to use \"with tf.variable_scope(...)\" before creating each model. And since you didn't do this before training, you'll have to re-name the variables you restore from the checkpoint. This can be done by passing a dict to the Saver object, rather than just a list of variables (as is done now). Let me know if you have more specific questions, I'll be happy to help, but here is the overview.\n(0) Currently both your models are trying to share parameters, that's where the error comes from (they cannot share because vocab sizes are different, but it's just a consequence of a different problem: you don't want to share the variables, you want 2 separate sets of them).\n(1) You cannot create 2 separate sets just with 2 sessions, you need to put them in variable_scopes. Just before calling create_model, open a scope, for example:\nwith tf.variable_scope('en_fr'):\nen_fr_model = create_model(...)\nwith tf.variable_scope('fr_en'):\nfr_en_model = create_model(...)\n(2) Now you have 2 sets of variables, but their names don't correspond to the names you've saved in checkpoints during training, because your names are now prefixed with 'en_fr/' and 'fr_en/' and those in checkpoint are not. So you need to add some logic to the Saver object in the model to tell it to prefix the variables from the checkpoint in the right way. This can be done by passing a dict to the Saver, see here", "body": "As said in the other thread -- you want separate variables for each model. So you need to use \"with tf.variable_scope(...)\" before creating each model. And since you didn't do this before training, you'll have to re-name the variables you restore from the checkpoint. This can be done by passing a dict to the Saver object, rather than just a list of variables (as is done now). Let me know if you have more specific questions, I'll be happy to help, but here is the overview.\n\n(0) Currently both your models are trying to share parameters, that's where the error comes from (they cannot share because vocab sizes are different, but it's just a consequence of a different problem: you don't want to share the variables, you want 2 separate sets of them).\n\n(1) You cannot create 2 separate sets just with 2 sessions, you need to put them in variable_scopes. Just before calling create_model, open a scope, for example:\n  with tf.variable_scope('en_fr'):\n    en_fr_model = create_model(...)\n  with tf.variable_scope('fr_en'):\n    fr_en_model = create_model(...)\n\n(2) Now you have 2 sets of variables, but their names don't correspond to the names you've saved in checkpoints during training, because your names are now prefixed with 'en_fr/' and 'fr_en/' and those in checkpoint are not. So you need to add some logic to the Saver object in the model to tell it to prefix the variables from the checkpoint in the right way. This can be done by passing a dict to the Saver, see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L791)\n"}