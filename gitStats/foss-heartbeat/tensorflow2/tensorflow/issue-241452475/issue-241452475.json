{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11375", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11375/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11375/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11375/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11375", "id": 241452475, "node_id": "MDU6SXNzdWUyNDE0NTI0NzU=", "number": 11375, "title": "tensorflow-\"ValueError: Operation 'init' has been marked as not fetchable\" Plz Help", "user": {"login": "Sudharsansai", "id": 8994711, "node_id": "MDQ6VXNlcjg5OTQ3MTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/8994711?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Sudharsansai", "html_url": "https://github.com/Sudharsansai", "followers_url": "https://api.github.com/users/Sudharsansai/followers", "following_url": "https://api.github.com/users/Sudharsansai/following{/other_user}", "gists_url": "https://api.github.com/users/Sudharsansai/gists{/gist_id}", "starred_url": "https://api.github.com/users/Sudharsansai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Sudharsansai/subscriptions", "organizations_url": "https://api.github.com/users/Sudharsansai/orgs", "repos_url": "https://api.github.com/users/Sudharsansai/repos", "events_url": "https://api.github.com/users/Sudharsansai/events{/privacy}", "received_events_url": "https://api.github.com/users/Sudharsansai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-07-08T14:06:39Z", "updated_at": "2017-07-08T14:10:53Z", "closed_at": "2017-07-08T14:10:53Z", "author_association": "NONE", "body_html": "<p>I started working with LSTMs for conversation modelling. I have got a sample piece of code with a persistent error. The code is given below.</p>\n<p>'''<br>\nA Dynamic Recurrent Neural Network (LSTM) implementation example using<br>\nTensorFlow library. This example is using a toy dataset to classify linear<br>\nsequences. The generated sequences have variable length.<br>\nLong Short Term Memory paper: <a href=\"http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\" rel=\"nofollow\">http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf</a><br>\nAuthor: Aymeric Damien<br>\nProject: <a href=\"https://github.com/aymericdamien/TensorFlow-Examples/\">https://github.com/aymericdamien/TensorFlow-Examples/</a><br>\n'''</p>\n<p>from <strong>future</strong> import print_function</p>\n<p>import tensorflow as tf<br>\nimport random<br>\ndata_path=\"C:/Users/AnacondaProjects/Convmodels/CleanedData/embeddings/\"</p>\n<h1>====================</h1>\n<h1>TOY DATA GENERATOR</h1>\n<h1>====================</h1>\n<p>class ToySequenceData(object):<br>\ndef <strong>init</strong>(self, n_samples=100, max_seq_len=10, min_seq_len=2):<br>\nself.data = []<br>\nself.labels = []<br>\nself.seqlen = []<br>\ndummy_vector=[float(0.0) for i in range(300)]<br>\nfor i in range(n_samples):<br>\nwith open(data_path+str(i)+\".txt\",\"r\",encoding=\"utf-8\") as inp:<br>\ninput_line=[[float(i) for i in line.split()] for line in inp]<br>\ncurrent_input=[]<br>\nfor j in range(min(10,len(input_line)-1)):<br>\ncurrent_input.append(input_line[j])<br>\ntemp_data=current_input[:]<br>\ntemp_data=temp_data+[dummy_vector[:] for k in range(max_seq_len-j-1)]<br>\ncurrent_input=temp_data[:]<br>\nself.data.append(temp_data)<br>\nself.labels.append(input_line[j+1])<br>\nself.seqlen.append(j+1)<br>\ni=i+(min(10,len(input_line)-1)-1)<br>\nself.batch_id = 0</p>\n<pre><code>def next(self, batch_size):\n    \"\"\" Return a batch of data. When dataset end is reached, start over.\n    \"\"\"\n    if self.batch_id == len(self.data):\n        self.batch_id = 0\n    batch_data = (self.data[self.batch_id:min(self.batch_id +\n                                              batch_size, len(self.data))])\n    batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n                                              batch_size, len(self.data))])\n    batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n                                              batch_size, len(self.data))])\n    self.batch_id = min(self.batch_id + batch_size, len(self.data))\n    return batch_data, batch_labels, batch_seqlen\n</code></pre>\n<h1>==========</h1>\n<h1>MODEL</h1>\n<h1>==========</h1>\n<h1>Parameters</h1>\n<p>learning_rate = 0.01<br>\ntraining_iters = 1000<br>\nbatch_size = 128<br>\ndisplay_step = 10</p>\n<h1>Network Parameters</h1>\n<p>seq_max_len = 10 # Sequence max length<br>\nn_hidden = 64 # hidden layer num of features<br>\nn_classes = 300 # linear sequence or not</p>\n<p>trainset = ToySequenceData(n_samples=100, max_seq_len=seq_max_len)<br>\ntestset = ToySequenceData(n_samples=20, max_seq_len=seq_max_len)</p>\n<h1>tf Graph input</h1>\n<p>x = tf.placeholder(\"float\", [None, seq_max_len, n_classes])<br>\ny = tf.placeholder(\"float\", [None, n_classes])</p>\n<h1>A placeholder for indicating each sequence length</h1>\n<p>seqlen = tf.placeholder(tf.int32, [None])</p>\n<h1>Define weights</h1>\n<p>Weights = {<br>\n'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))<br>\n}<br>\nBiases = {<br>\n'out': tf.Variable(tf.random_normal([n_classes]))<br>\n}</p>\n<p>def dynamicRNN(x, seqlen, Weights, Biases):</p>\n<pre><code># Prepare data shape to match `rnn` function requirements\n# Current data input shape: (batch_size, n_steps, n_input)\n# Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n\n# Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\nx = tf.unstack(x, seq_max_len, 1)\n\n# Define a lstm cell with tensorflow\nwith tf.variable_scope('lstm_cell_def'):\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n\n# Get lstm cell output, providing 'sequence_length' will perform dynamic\n# calculation.\nwith tf.variable_scope('rnn_cell_def',reuse=True): \n    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n                        sequence_length=seqlen)\n\n# When performing dynamic calculation, we must retrieve the last\n# dynamically computed output, i.e., if a sequence length is 10, we need\n# to retrieve the 10th output.\n# However TensorFlow doesn't support advanced indexing yet, so we build\n# a custom op that for each sample in batch size, get its length and\n# get the corresponding relevant output.\n\n# 'outputs' is a list of output at every timestep, we pack them in a Tensor\n# and change back dimension to [batch_size, n_step, n_input]\noutputs = tf.stack(outputs)\noutputs = tf.transpose(outputs, [1, 0, 2])\n\n# Hack to build the indexing and retrieve the right output.\nbatch_size = tf.shape(outputs)[0]\n# Start indices for each sample\nindex = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n# Indexing\noutputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n\n# Linear activation, using outputs computed above\nreturn tf.matmul(outputs, Weights['out']) + Biases['out']\n</code></pre>\n<p>pred = dynamicRNN(x, seqlen, Weights, Biases)</p>\n<h1>Define loss and optimizer</h1>\n<p>cos_dist=tf.losses.cosine_distance(predictions=pred,labels=y,dim=1)<br>\n#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))<br>\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cos_dist)</p>\n<h1>Evaluate model</h1>\n<p>#correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))<br>\nnormalize_pred = tf.nn.l2_normalize(pred,1)<br>\nnormalize_y = tf.nn.l2_normalize(y,1)<br>\ncorrect_pred=(1+tf.reduce_sum(tf.multiply(normalize_pred,normalize_y)))/2<br>\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</p>\n<h1>Initializing the variables</h1>\n<p>init = tf.global_variables_initializer()</p>\n<h1>Launch the graph</h1>\n<p>with tf.Session() as sess:<br>\nsess.run(init)<br>\nstep = 1<br>\n# Keep training until reach max iterations<br>\nwhile step * batch_size &lt; training_iters:<br>\nbatch_x, batch_y, batch_seqlen = trainset.next(batch_size)<br>\n# Run optimization op (backprop)<br>\nsess.run(optimizer, feed_dict={x: batch_x, y: batch_y,<br>\nseqlen: batch_seqlen})<br>\nif step % display_step == 0:<br>\n# Calculate batch accuracy<br>\nacc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,<br>\nseqlen: batch_seqlen})<br>\n# Calculate batch loss<br>\nloss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,<br>\nseqlen: batch_seqlen})<br>\nprint(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + <br>\n\"{:.6f}\".format(loss) + \", Training Accuracy= \" + <br>\n\"{:.5f}\".format(acc))<br>\nstep += 1<br>\nprint(\"Optimization Finished!\")</p>\n<pre><code># Calculate accuracy\ntest_data = testset.data\ntest_label = testset.labels\ntest_seqlen = testset.seqlen\nprint(\"Testing Accuracy:\", \\\n    sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n                                  seqlen: test_seqlen}))\n</code></pre>\n<p>When I run this, it get \" ValueError: Operation 'init' has been marked as not fetchable \" and pointing to line: \" sess.run(init) \" Please kindly help! Thanks in advance.</p>", "body_text": "I started working with LSTMs for conversation modelling. I have got a sample piece of code with a persistent error. The code is given below.\n'''\nA Dynamic Recurrent Neural Network (LSTM) implementation example using\nTensorFlow library. This example is using a toy dataset to classify linear\nsequences. The generated sequences have variable length.\nLong Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n'''\nfrom future import print_function\nimport tensorflow as tf\nimport random\ndata_path=\"C:/Users/AnacondaProjects/Convmodels/CleanedData/embeddings/\"\n====================\nTOY DATA GENERATOR\n====================\nclass ToySequenceData(object):\ndef init(self, n_samples=100, max_seq_len=10, min_seq_len=2):\nself.data = []\nself.labels = []\nself.seqlen = []\ndummy_vector=[float(0.0) for i in range(300)]\nfor i in range(n_samples):\nwith open(data_path+str(i)+\".txt\",\"r\",encoding=\"utf-8\") as inp:\ninput_line=[[float(i) for i in line.split()] for line in inp]\ncurrent_input=[]\nfor j in range(min(10,len(input_line)-1)):\ncurrent_input.append(input_line[j])\ntemp_data=current_input[:]\ntemp_data=temp_data+[dummy_vector[:] for k in range(max_seq_len-j-1)]\ncurrent_input=temp_data[:]\nself.data.append(temp_data)\nself.labels.append(input_line[j+1])\nself.seqlen.append(j+1)\ni=i+(min(10,len(input_line)-1)-1)\nself.batch_id = 0\ndef next(self, batch_size):\n    \"\"\" Return a batch of data. When dataset end is reached, start over.\n    \"\"\"\n    if self.batch_id == len(self.data):\n        self.batch_id = 0\n    batch_data = (self.data[self.batch_id:min(self.batch_id +\n                                              batch_size, len(self.data))])\n    batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n                                              batch_size, len(self.data))])\n    batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n                                              batch_size, len(self.data))])\n    self.batch_id = min(self.batch_id + batch_size, len(self.data))\n    return batch_data, batch_labels, batch_seqlen\n\n==========\nMODEL\n==========\nParameters\nlearning_rate = 0.01\ntraining_iters = 1000\nbatch_size = 128\ndisplay_step = 10\nNetwork Parameters\nseq_max_len = 10 # Sequence max length\nn_hidden = 64 # hidden layer num of features\nn_classes = 300 # linear sequence or not\ntrainset = ToySequenceData(n_samples=100, max_seq_len=seq_max_len)\ntestset = ToySequenceData(n_samples=20, max_seq_len=seq_max_len)\ntf Graph input\nx = tf.placeholder(\"float\", [None, seq_max_len, n_classes])\ny = tf.placeholder(\"float\", [None, n_classes])\nA placeholder for indicating each sequence length\nseqlen = tf.placeholder(tf.int32, [None])\nDefine weights\nWeights = {\n'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n}\nBiases = {\n'out': tf.Variable(tf.random_normal([n_classes]))\n}\ndef dynamicRNN(x, seqlen, Weights, Biases):\n# Prepare data shape to match `rnn` function requirements\n# Current data input shape: (batch_size, n_steps, n_input)\n# Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n\n# Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\nx = tf.unstack(x, seq_max_len, 1)\n\n# Define a lstm cell with tensorflow\nwith tf.variable_scope('lstm_cell_def'):\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n\n# Get lstm cell output, providing 'sequence_length' will perform dynamic\n# calculation.\nwith tf.variable_scope('rnn_cell_def',reuse=True): \n    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n                        sequence_length=seqlen)\n\n# When performing dynamic calculation, we must retrieve the last\n# dynamically computed output, i.e., if a sequence length is 10, we need\n# to retrieve the 10th output.\n# However TensorFlow doesn't support advanced indexing yet, so we build\n# a custom op that for each sample in batch size, get its length and\n# get the corresponding relevant output.\n\n# 'outputs' is a list of output at every timestep, we pack them in a Tensor\n# and change back dimension to [batch_size, n_step, n_input]\noutputs = tf.stack(outputs)\noutputs = tf.transpose(outputs, [1, 0, 2])\n\n# Hack to build the indexing and retrieve the right output.\nbatch_size = tf.shape(outputs)[0]\n# Start indices for each sample\nindex = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n# Indexing\noutputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n\n# Linear activation, using outputs computed above\nreturn tf.matmul(outputs, Weights['out']) + Biases['out']\n\npred = dynamicRNN(x, seqlen, Weights, Biases)\nDefine loss and optimizer\ncos_dist=tf.losses.cosine_distance(predictions=pred,labels=y,dim=1)\n#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cos_dist)\nEvaluate model\n#correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\nnormalize_pred = tf.nn.l2_normalize(pred,1)\nnormalize_y = tf.nn.l2_normalize(y,1)\ncorrect_pred=(1+tf.reduce_sum(tf.multiply(normalize_pred,normalize_y)))/2\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\nInitializing the variables\ninit = tf.global_variables_initializer()\nLaunch the graph\nwith tf.Session() as sess:\nsess.run(init)\nstep = 1\n# Keep training until reach max iterations\nwhile step * batch_size < training_iters:\nbatch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n# Run optimization op (backprop)\nsess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\nseqlen: batch_seqlen})\nif step % display_step == 0:\n# Calculate batch accuracy\nacc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,\nseqlen: batch_seqlen})\n# Calculate batch loss\nloss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,\nseqlen: batch_seqlen})\nprint(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \n\"{:.6f}\".format(loss) + \", Training Accuracy= \" + \n\"{:.5f}\".format(acc))\nstep += 1\nprint(\"Optimization Finished!\")\n# Calculate accuracy\ntest_data = testset.data\ntest_label = testset.labels\ntest_seqlen = testset.seqlen\nprint(\"Testing Accuracy:\", \\\n    sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n                                  seqlen: test_seqlen}))\n\nWhen I run this, it get \" ValueError: Operation 'init' has been marked as not fetchable \" and pointing to line: \" sess.run(init) \" Please kindly help! Thanks in advance.", "body": "I started working with LSTMs for conversation modelling. I have got a sample piece of code with a persistent error. The code is given below.\r\n\r\n'''\r\nA Dynamic Recurrent Neural Network (LSTM) implementation example using\r\nTensorFlow library. This example is using a toy dataset to classify linear\r\nsequences. The generated sequences have variable length.\r\nLong Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\r\nAuthor: Aymeric Damien\r\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\r\n'''\r\n\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport random\r\ndata_path=\"C:/Users/AnacondaProjects/Convmodels/CleanedData/embeddings/\"\r\n\r\n\r\n# ====================\r\n#  TOY DATA GENERATOR\r\n# ====================\r\nclass ToySequenceData(object):\r\n    def __init__(self, n_samples=100, max_seq_len=10, min_seq_len=2):\r\n        self.data = []\r\n        self.labels = []\r\n        self.seqlen = []\r\n        dummy_vector=[float(0.0) for i in range(300)]\r\n        for i in range(n_samples):\r\n            with open(data_path+str(i)+\".txt\",\"r\",encoding=\"utf-8\") as inp:\r\n                input_line=[[float(i) for i in line.split()] for line in inp]\r\n                current_input=[]\r\n                for j in range(min(10,len(input_line)-1)):\r\n                    current_input.append(input_line[j])\r\n                    temp_data=current_input[:]\r\n                    temp_data=temp_data+[dummy_vector[:] for k in range(max_seq_len-j-1)]\r\n                    current_input=temp_data[:]\r\n                    self.data.append(temp_data)\r\n                    self.labels.append(input_line[j+1])\r\n                    self.seqlen.append(j+1)\r\n                i=i+(min(10,len(input_line)-1)-1)\r\n        self.batch_id = 0\r\n\r\n    def next(self, batch_size):\r\n        \"\"\" Return a batch of data. When dataset end is reached, start over.\r\n        \"\"\"\r\n        if self.batch_id == len(self.data):\r\n            self.batch_id = 0\r\n        batch_data = (self.data[self.batch_id:min(self.batch_id +\r\n                                                  batch_size, len(self.data))])\r\n        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\r\n                                                  batch_size, len(self.data))])\r\n        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\r\n                                                  batch_size, len(self.data))])\r\n        self.batch_id = min(self.batch_id + batch_size, len(self.data))\r\n        return batch_data, batch_labels, batch_seqlen\r\n\r\n\r\n# ==========\r\n#   MODEL\r\n# ==========\r\n\r\n# Parameters\r\nlearning_rate = 0.01\r\ntraining_iters = 1000\r\nbatch_size = 128\r\ndisplay_step = 10\r\n\r\n# Network Parameters\r\nseq_max_len = 10 # Sequence max length\r\nn_hidden = 64 # hidden layer num of features\r\nn_classes = 300 # linear sequence or not\r\n\r\ntrainset = ToySequenceData(n_samples=100, max_seq_len=seq_max_len)\r\ntestset = ToySequenceData(n_samples=20, max_seq_len=seq_max_len)\r\n\r\n# tf Graph input\r\nx = tf.placeholder(\"float\", [None, seq_max_len, n_classes])\r\ny = tf.placeholder(\"float\", [None, n_classes])\r\n# A placeholder for indicating each sequence length\r\nseqlen = tf.placeholder(tf.int32, [None])\r\n\r\n# Define weights\r\nWeights = {\r\n    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\r\n}\r\nBiases = {\r\n    'out': tf.Variable(tf.random_normal([n_classes]))\r\n}\r\n\r\n\r\ndef dynamicRNN(x, seqlen, Weights, Biases):\r\n\r\n    # Prepare data shape to match `rnn` function requirements\r\n    # Current data input shape: (batch_size, n_steps, n_input)\r\n    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\r\n    \r\n    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\r\n    x = tf.unstack(x, seq_max_len, 1)\r\n    \r\n    # Define a lstm cell with tensorflow\r\n    with tf.variable_scope('lstm_cell_def'):\r\n        lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\r\n\r\n    # Get lstm cell output, providing 'sequence_length' will perform dynamic\r\n    # calculation.\r\n    with tf.variable_scope('rnn_cell_def',reuse=True): \r\n        outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\r\n                            sequence_length=seqlen)\r\n\r\n    # When performing dynamic calculation, we must retrieve the last\r\n    # dynamically computed output, i.e., if a sequence length is 10, we need\r\n    # to retrieve the 10th output.\r\n    # However TensorFlow doesn't support advanced indexing yet, so we build\r\n    # a custom op that for each sample in batch size, get its length and\r\n    # get the corresponding relevant output.\r\n\r\n    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\r\n    # and change back dimension to [batch_size, n_step, n_input]\r\n    outputs = tf.stack(outputs)\r\n    outputs = tf.transpose(outputs, [1, 0, 2])\r\n\r\n    # Hack to build the indexing and retrieve the right output.\r\n    batch_size = tf.shape(outputs)[0]\r\n    # Start indices for each sample\r\n    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\r\n    # Indexing\r\n    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\r\n\r\n    # Linear activation, using outputs computed above\r\n    return tf.matmul(outputs, Weights['out']) + Biases['out']\r\n\r\npred = dynamicRNN(x, seqlen, Weights, Biases)\r\n\r\n# Define loss and optimizer\r\ncos_dist=tf.losses.cosine_distance(predictions=pred,labels=y,dim=1)\r\n#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cos_dist)\r\n\r\n# Evaluate model\r\n#correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\r\nnormalize_pred = tf.nn.l2_normalize(pred,1)        \r\nnormalize_y = tf.nn.l2_normalize(y,1)\r\ncorrect_pred=(1+tf.reduce_sum(tf.multiply(normalize_pred,normalize_y)))/2\r\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n# Initializing the variables\r\ninit = tf.global_variables_initializer()\r\n\r\n# Launch the graph\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    step = 1\r\n    # Keep training until reach max iterations\r\n    while step * batch_size < training_iters:\r\n        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\r\n        # Run optimization op (backprop)\r\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\r\n                                       seqlen: batch_seqlen})\r\n        if step % display_step == 0:\r\n            # Calculate batch accuracy\r\n            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,\r\n                                                seqlen: batch_seqlen})\r\n            # Calculate batch loss\r\n            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,\r\n                                             seqlen: batch_seqlen})\r\n            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\r\n                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\r\n                  \"{:.5f}\".format(acc))\r\n        step += 1\r\n    print(\"Optimization Finished!\")\r\n\r\n    # Calculate accuracy\r\n    test_data = testset.data\r\n    test_label = testset.labels\r\n    test_seqlen = testset.seqlen\r\n    print(\"Testing Accuracy:\", \\\r\n        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\r\n                                      seqlen: test_seqlen}))\r\n\r\nWhen I run this, it get \" ValueError: Operation 'init' has been marked as not fetchable \" and pointing to line: \" sess.run(init) \" Please kindly help! Thanks in advance."}