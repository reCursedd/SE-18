{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/385918121", "html_url": "https://github.com/tensorflow/tensorflow/issues/19017#issuecomment-385918121", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19017", "id": 385918121, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTkxODEyMQ==", "user": {"login": "sleepfin", "id": 7370869, "node_id": "MDQ6VXNlcjczNzA4Njk=", "avatar_url": "https://avatars1.githubusercontent.com/u/7370869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sleepfin", "html_url": "https://github.com/sleepfin", "followers_url": "https://api.github.com/users/sleepfin/followers", "following_url": "https://api.github.com/users/sleepfin/following{/other_user}", "gists_url": "https://api.github.com/users/sleepfin/gists{/gist_id}", "starred_url": "https://api.github.com/users/sleepfin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sleepfin/subscriptions", "organizations_url": "https://api.github.com/users/sleepfin/orgs", "repos_url": "https://api.github.com/users/sleepfin/repos", "events_url": "https://api.github.com/users/sleepfin/events{/privacy}", "received_events_url": "https://api.github.com/users/sleepfin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-02T09:26:35Z", "updated_at": "2018-05-02T09:26:35Z", "author_association": "NONE", "body_html": "<p>I also tried another code which copy <code>a_cpu</code> from CPU to GPU:1 (not GPU:0)<br>\ncode:</p>\n<pre><code>import tensorflow as tf\nimport os\n\nfrom tensorflow.python.client import timeline\nslim = tf.contrib.slim\n\ntrain_ops = []\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n\nnet = tf.random_normal(shape=(32, 16, 16, 128))\n\nwith tf.device('/GPU:0'):\n\n  for i in range(10):\n    net = slim.conv2d(net, 128, [11, 11], padding='SAME', scope='conv_%s' % i)\n\n  loss = tf.reduce_mean(net, name='loss_func')\n  grad = tf.gradients(loss, tf.global_variables(), gate_gradients=True, name='my_gradients')\n\nvars = tf.trainable_variables()\nnum_vars = len(vars)\n\nfor i in range(num_vars - 1, -1, -1):\n\n  with tf.device('/CPU:0'):\n    a_cpu = tf.get_variable('a_cpu_%s' % i, initializer=vars[i].initial_value)\n  update_op = optimizer.apply_gradients([(grad[i], a_cpu)], name='apply_%s' % i)\n\n  with tf.control_dependencies([update_op]):\n    # Method-1 :\n    # with tf.device('/GPU:0'):\n    #   a_cpu_to_gpu = a_cpu.read_value()\n    # train_op = vars[i].assign(a_cpu_to_gpu).op\n    # train_ops.append(train_op)\n\n    # Method-2 :\n    # train_ops.append(vars[i].assign(a_cpu).op)\n\n    # Method-3 :\n    with tf.device('/GPU:1'):\n      a_cpu_to_gpu = a_cpu.read_value()\n    train_ops.append(a_cpu_to_gpu.op)\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  for i in range(10):\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n    sess.run(train_ops, options=run_options, run_metadata=run_metadata)\n    trace_filename = os.path.join('/tmp/delete_me/trace', 'trace-global-step-%d.json' % i)\n    if not tf.gfile.Exists(os.path.dirname(trace_filename)):\n      os.makedirs(os.path.dirname(trace_filename))\n    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n    with tf.gfile.Open(trace_filename, 'w') as trace_file:\n      trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7370869/39515871-ee9d8664-4e2d-11e8-9484-d3e6d6836760.png\"><img src=\"https://user-images.githubusercontent.com/7370869/39515871-ee9d8664-4e2d-11e8-9484-d3e6d6836760.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>In this way, MEMCPYHtoD (green bar) overlaps with compute_gradients.<br>\nBut in <code>Method-2</code>, MEMCPYHtoD (green bar) can not overlap with compute_gradients.</p>\n<p>It seems that MEMCPYHtoD (green bar) cannot overlap with the backward computation. But it can somehow overlap with forward computation (blue bars) in <code>Method-1</code>.</p>\n<p>If I change the <code>with tf.device('/GPU:1'):</code> to <code>with tf.device('/GPU:0'):</code>, the timeline is:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7370869/39515848-d8956864-4e2d-11e8-9d5d-f39fc6148a74.png\"><img src=\"https://user-images.githubusercontent.com/7370869/39515848-d8956864-4e2d-11e8-9d5d-f39fc6148a74.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>", "body_text": "I also tried another code which copy a_cpu from CPU to GPU:1 (not GPU:0)\ncode:\nimport tensorflow as tf\nimport os\n\nfrom tensorflow.python.client import timeline\nslim = tf.contrib.slim\n\ntrain_ops = []\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n\nnet = tf.random_normal(shape=(32, 16, 16, 128))\n\nwith tf.device('/GPU:0'):\n\n  for i in range(10):\n    net = slim.conv2d(net, 128, [11, 11], padding='SAME', scope='conv_%s' % i)\n\n  loss = tf.reduce_mean(net, name='loss_func')\n  grad = tf.gradients(loss, tf.global_variables(), gate_gradients=True, name='my_gradients')\n\nvars = tf.trainable_variables()\nnum_vars = len(vars)\n\nfor i in range(num_vars - 1, -1, -1):\n\n  with tf.device('/CPU:0'):\n    a_cpu = tf.get_variable('a_cpu_%s' % i, initializer=vars[i].initial_value)\n  update_op = optimizer.apply_gradients([(grad[i], a_cpu)], name='apply_%s' % i)\n\n  with tf.control_dependencies([update_op]):\n    # Method-1 :\n    # with tf.device('/GPU:0'):\n    #   a_cpu_to_gpu = a_cpu.read_value()\n    # train_op = vars[i].assign(a_cpu_to_gpu).op\n    # train_ops.append(train_op)\n\n    # Method-2 :\n    # train_ops.append(vars[i].assign(a_cpu).op)\n\n    # Method-3 :\n    with tf.device('/GPU:1'):\n      a_cpu_to_gpu = a_cpu.read_value()\n    train_ops.append(a_cpu_to_gpu.op)\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  for i in range(10):\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n    sess.run(train_ops, options=run_options, run_metadata=run_metadata)\n    trace_filename = os.path.join('/tmp/delete_me/trace', 'trace-global-step-%d.json' % i)\n    if not tf.gfile.Exists(os.path.dirname(trace_filename)):\n      os.makedirs(os.path.dirname(trace_filename))\n    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n    with tf.gfile.Open(trace_filename, 'w') as trace_file:\n      trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\n\n\nIn this way, MEMCPYHtoD (green bar) overlaps with compute_gradients.\nBut in Method-2, MEMCPYHtoD (green bar) can not overlap with compute_gradients.\nIt seems that MEMCPYHtoD (green bar) cannot overlap with the backward computation. But it can somehow overlap with forward computation (blue bars) in Method-1.\nIf I change the with tf.device('/GPU:1'): to with tf.device('/GPU:0'):, the timeline is:", "body": "I also tried another code which copy `a_cpu` from CPU to GPU:1 (not GPU:0)\r\ncode:\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nfrom tensorflow.python.client import timeline\r\nslim = tf.contrib.slim\r\n\r\ntrain_ops = []\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\r\n\r\nnet = tf.random_normal(shape=(32, 16, 16, 128))\r\n\r\nwith tf.device('/GPU:0'):\r\n\r\n  for i in range(10):\r\n    net = slim.conv2d(net, 128, [11, 11], padding='SAME', scope='conv_%s' % i)\r\n\r\n  loss = tf.reduce_mean(net, name='loss_func')\r\n  grad = tf.gradients(loss, tf.global_variables(), gate_gradients=True, name='my_gradients')\r\n\r\nvars = tf.trainable_variables()\r\nnum_vars = len(vars)\r\n\r\nfor i in range(num_vars - 1, -1, -1):\r\n\r\n  with tf.device('/CPU:0'):\r\n    a_cpu = tf.get_variable('a_cpu_%s' % i, initializer=vars[i].initial_value)\r\n  update_op = optimizer.apply_gradients([(grad[i], a_cpu)], name='apply_%s' % i)\r\n\r\n  with tf.control_dependencies([update_op]):\r\n    # Method-1 :\r\n    # with tf.device('/GPU:0'):\r\n    #   a_cpu_to_gpu = a_cpu.read_value()\r\n    # train_op = vars[i].assign(a_cpu_to_gpu).op\r\n    # train_ops.append(train_op)\r\n\r\n    # Method-2 :\r\n    # train_ops.append(vars[i].assign(a_cpu).op)\r\n\r\n    # Method-3 :\r\n    with tf.device('/GPU:1'):\r\n      a_cpu_to_gpu = a_cpu.read_value()\r\n    train_ops.append(a_cpu_to_gpu.op)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  for i in range(10):\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(train_ops, options=run_options, run_metadata=run_metadata)\r\n    trace_filename = os.path.join('/tmp/delete_me/trace', 'trace-global-step-%d.json' % i)\r\n    if not tf.gfile.Exists(os.path.dirname(trace_filename)):\r\n      os.makedirs(os.path.dirname(trace_filename))\r\n    trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n    with tf.gfile.Open(trace_filename, 'w') as trace_file:\r\n      trace_file.write(trace.generate_chrome_trace_format(show_memory=False))\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/7370869/39515871-ee9d8664-4e2d-11e8-9484-d3e6d6836760.png)\r\n\r\n\r\nIn this way, MEMCPYHtoD (green bar) overlaps with compute_gradients.\r\nBut in `Method-2`, MEMCPYHtoD (green bar) can not overlap with compute_gradients.\r\n\r\nIt seems that MEMCPYHtoD (green bar) cannot overlap with the backward computation. But it can somehow overlap with forward computation (blue bars) in `Method-1`.\r\n\r\nIf I change the `with tf.device('/GPU:1'):` to `with tf.device('/GPU:0'):`, the timeline is:\r\n\r\n![image](https://user-images.githubusercontent.com/7370869/39515848-d8956864-4e2d-11e8-9d5d-f39fc6148a74.png)\r\n"}