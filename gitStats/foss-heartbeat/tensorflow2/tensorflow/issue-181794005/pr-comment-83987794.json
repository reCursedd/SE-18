{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/83987794", "pull_request_review_id": 4794683, "id": 83987794, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDgzOTg3Nzk0", "diff_hunk": "@@ -94,48 +94,100 @@ def einsum(axes, *inputs):\n   \"\"\"\n   A generalized contraction between tensors of arbitrary dimension.\n \n-  Like numpy.einsum.\n+  Like `numpy.einsum`, but does not support:\n+  * Ellipses (subscripts like `ij...,jk...->ik...`)\n+  * Subscripts where an axis appears more than once for a single input (e.g. `ijj,jk->ik`).\n+\n+  Args:\n+    axes: a `str` describing the contraction, in the same format as `numpy.einsum`.\n+    inputs: the inputs to contract (each one a `Tensor`), whose shapes should be consistent with `axes`.\n+\n+  Returns:\n+    The contracted `Tensor`, with shape determined by `axes`.\n+\n+  Raises:\n+    ValueError: If the format of `axes` is incorrect,\n+                or the number of inputs implied by `axes` does not match `len(inputs)`,\n+                or an axis appears in the output subscripts but not in any of the inputs,\n+                or the number of dimensions of an input differs from the number of indices in its subscript,\n+                or the input shapes are inconsistent along a particular axis.\n   \"\"\"\n+  if '...' in axes:\n+    raise ValueError(\"Subscripts with ellipses are not yet supported.\")\n \n-  match = re.match('([a-z,]+)->([a-z]+)', axes)\n-  assert match, \\\n-    \"Indices have incorrect format: %s\" % axes\n+  match = re.match('([a-z,]+)(->[a-z]*)?', axes)\n+  if not match:\n+    raise ValueError(\n+      \"Indices have incorrect format: %s\" % axes\n+    )\n \n   inputs = list(inputs)\n   idx_in = match.group(1).split(',')\n-  idx_out = match.group(2)\n   idx_all = set(''.join(idx_in))\n+  indices = ''.join(sorted(idx_all))\n+\n+  if match.group(2):\n+    idx_out = match.group(2)[2:]\n \n+  else:\n+    # infer the output subscripts if not given, assume alphabetical order\n+    counts = {ax: 0 for ax in indices}\n+    for axes_ in idx_in:\n+      for ax in axes_:\n+        counts[ax] += 1\n \n-  assert len(idx_in) == len(inputs), \\\n-    \"Expected %d inputs but only got %d\" % (len(idx_in), len(inputs))\n+    idx_out = ''.join(sorted(\n+      ax for ax in indices\n+      if counts[ax] == 1\n+    ))\n \n-  # transpose inputs so axes are in alphabetical order\n+  if len(idx_in) != len(inputs):\n+    raise ValueError(\n+      \"Expected %d inputs but got %d\" % (len(idx_in), len(inputs))\n+    )\n+\n+  missing_idx = set(idx_out).difference(idx_all)\n+  if missing_idx:\n+    raise ValueError(\n+      \"Unknown ouput axes: %s\" % missing_idx\n+    )\n+\n+  axis_order = {}\n+  for ax in indices:\n+    if ax not in idx_out:\n+      axis_order[ax] = len(axis_order)\n+  for ax in idx_out:\n+    axis_order[ax] = len(axis_order)\n+\n+  # transpose inputs so axes are in order\n   for i, (input_, axes_) in enumerate(zip(inputs, idx_in)):\n-    assert input_.get_shape().ndims == len(axes_), \\\n-      \"Input %d with axes %s has incorrect\" \\\n-      \" number of dimensions (expected %d, got %d)\" % (\n-        i, axes_, len(axes_), input_.get_shape().ndims\n+    if input_.get_shape().ndims != len(axes_):\n+      raise ValueError(\n+        \"Input %d with axes %s has incorrect\" \\\n+        \" number of dimensions (expected %d, got %d)\" % (\n+          i, axes_, len(axes_), input_.get_shape().ndims\n+        )\n       )\n \n-    sorted_idx = sorted(axes_)\n+    sorted_idx = sorted(axes_, key=axis_order.get)\n+\n+    if len(set(axes_)) != len(axes_):\n+      raise ValueError(\n+        \"Subscript not supported: an axis appears more than once: %s\" % axes_\n+      )\n \n     if list(axes_) != sorted_idx:\n       permuted = [axes_.find(ax) for ax in sorted_idx]\n       inputs[i] = array_ops.transpose(input_, permuted)\n       idx_in[i] = sorted_idx\n \n-  missing_idx = set(idx_out).difference(idx_all)\n-  assert not missing_idx, \\\n-    \"Unknown ouput axes: %s\" % missing_idx\n-\n   reduction_idx = []\n   shapes = [[dim if dim else -1\n              for dim in tensor.get_shape().as_list()]", "path": "tensorflow/python/ops/special_math_ops.py", "position": 110, "original_position": 110, "commit_id": "9e79af1af8785ce89072e3ffcc1685f3f5e63e57", "original_commit_id": "9e79af1af8785ce89072e3ffcc1685f3f5e63e57", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "body": "I would like to propose one more fix. I think this should be changed from:\n\n```\nshapes = [[dim if dim else -1\n           for dim in tensor.get_shape().as_list()]\n           for tensor in inputs]\n```\n\nto:\n\n```\nshapes = [[dim if dim is not None else -1\n           for dim in array_ops.unpack(array_ops.shape(tensor))]\n           for tensor in inputs]\n```\n\nin order to allow for dynamic tensor shapes. I run into an issue while using `tf.einsum` and this change fixed it for me.\n", "created_at": "2016-10-19T02:24:29Z", "updated_at": "2016-10-19T02:24:29Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4837#discussion_r83987794", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4837", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/83987794"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4837#discussion_r83987794"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4837"}}, "body_html": "<p>I would like to propose one more fix. I think this should be changed from:</p>\n<pre><code>shapes = [[dim if dim else -1\n           for dim in tensor.get_shape().as_list()]\n           for tensor in inputs]\n</code></pre>\n<p>to:</p>\n<pre><code>shapes = [[dim if dim is not None else -1\n           for dim in array_ops.unpack(array_ops.shape(tensor))]\n           for tensor in inputs]\n</code></pre>\n<p>in order to allow for dynamic tensor shapes. I run into an issue while using <code>tf.einsum</code> and this change fixed it for me.</p>", "body_text": "I would like to propose one more fix. I think this should be changed from:\nshapes = [[dim if dim else -1\n           for dim in tensor.get_shape().as_list()]\n           for tensor in inputs]\n\nto:\nshapes = [[dim if dim is not None else -1\n           for dim in array_ops.unpack(array_ops.shape(tensor))]\n           for tensor in inputs]\n\nin order to allow for dynamic tensor shapes. I run into an issue while using tf.einsum and this change fixed it for me."}