{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22229", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22229/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22229/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22229/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22229", "id": 359359385, "node_id": "MDU6SXNzdWUzNTkzNTkzODU=", "number": 22229, "title": "intra_op_parallelism_threads will be invalid if call any API initialized device", "user": {"login": "Zantares", "id": 38638514, "node_id": "MDQ6VXNlcjM4NjM4NTE0", "avatar_url": "https://avatars0.githubusercontent.com/u/38638514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zantares", "html_url": "https://github.com/Zantares", "followers_url": "https://api.github.com/users/Zantares/followers", "following_url": "https://api.github.com/users/Zantares/following{/other_user}", "gists_url": "https://api.github.com/users/Zantares/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zantares/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zantares/subscriptions", "organizations_url": "https://api.github.com/users/Zantares/orgs", "repos_url": "https://api.github.com/users/Zantares/repos", "events_url": "https://api.github.com/users/Zantares/events{/privacy}", "received_events_url": "https://api.github.com/users/Zantares/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-09-12T07:59:54Z", "updated_at": "2018-11-15T19:04:39Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):</strong> Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):</strong> (Red Hat 4.8.5-16)</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile devic:</strong> n/a</li>\n<li><strong>TensorFlow installed from (source or binary):</strong> source</li>\n<li><strong>TensorFlow version (use command below):</strong> 1.10.0</li>\n<li><strong>python version:</strong> 3.4.5</li>\n<li><strong>Bazel version (if compiling from source):</strong> 0.15.1</li>\n<li><strong>GCC/Compiler version (if compiling from source):</strong> gcc version 6.3.1</li>\n<li><strong>CUDA/cuDNN version:</strong> n/a</li>\n<li><strong>GPU model and memory:</strong> n/a</li>\n<li><strong>Exact command to reproduce:</strong> Run included script</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I'm trying to run <a href=\"https://github.com/tensorflow/models/tree/master/official/recommendation\">NCF model </a> on X86 CPU, and add the <strong>intra_op_parallelism_threads</strong> to tune the performance. Then I found that if we call any API including <strong>list_devices()</strong>, TF would initialize a global intra thread pool and overwrite the configuration of <strong>intra_op_parallelism_threads</strong>.</p>\n<p>NCF will call <strong>is_gpu_available()</strong> before training, and its implementation includes <strong>list_devices()</strong>, so the <strong>intra_op_parallelism_threads</strong> option is invalid.</p>\n<p>My question is, dose this logic makes sense? If we want to run a model on CPU, the config may be invalid when we call some 'harmless' API and get no feedback.</p>\n<h3>Source code / logs</h3>\n<p>here's the modify of NCF to enable <strong>intra_op_parallelism_threads</strong>:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/2373942/ncf_intra.txt\">ncf_intra.txt</a></p>\n<p>I also add some code to print the intra thread pool status in TF, this patch has changed the global setting to false to solve the issue, revert it to true will reproduce the question:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/2373947/tf_intra.txt\">tf_intra.txt</a></p>\n<p>With the print code, you can see that TF will try to initialize a device with a global intra thread pool when call <strong>list_devices()</strong>. Once if the global intra thread pool was initialized, other device couldn't change the configuration any more.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): (Red Hat 4.8.5-16)\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile devic: n/a\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.10.0\npython version: 3.4.5\nBazel version (if compiling from source): 0.15.1\nGCC/Compiler version (if compiling from source): gcc version 6.3.1\nCUDA/cuDNN version: n/a\nGPU model and memory: n/a\nExact command to reproduce: Run included script\n\nDescribe the problem\nI'm trying to run NCF model  on X86 CPU, and add the intra_op_parallelism_threads to tune the performance. Then I found that if we call any API including list_devices(), TF would initialize a global intra thread pool and overwrite the configuration of intra_op_parallelism_threads.\nNCF will call is_gpu_available() before training, and its implementation includes list_devices(), so the intra_op_parallelism_threads option is invalid.\nMy question is, dose this logic makes sense? If we want to run a model on CPU, the config may be invalid when we call some 'harmless' API and get no feedback.\nSource code / logs\nhere's the modify of NCF to enable intra_op_parallelism_threads:\nncf_intra.txt\nI also add some code to print the intra thread pool status in TF, this patch has changed the global setting to false to solve the issue, revert it to true will reproduce the question:\ntf_intra.txt\nWith the print code, you can see that TF will try to initialize a device with a global intra thread pool when call list_devices(). Once if the global intra thread pool was initialized, other device couldn't change the configuration any more.", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** (Red Hat 4.8.5-16)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile devic:** n/a\r\n- **TensorFlow installed from (source or binary):** source\r\n- **TensorFlow version (use command below):** 1.10.0\r\n- **python version:** 3.4.5\r\n- **Bazel version (if compiling from source):** 0.15.1\r\n- **GCC/Compiler version (if compiling from source):** gcc version 6.3.1\r\n- **CUDA/cuDNN version:** n/a\r\n- **GPU model and memory:** n/a\r\n- **Exact command to reproduce:** Run included script\r\n\r\n### Describe the problem\r\n\r\nI'm trying to run [NCF model ](https://github.com/tensorflow/models/tree/master/official/recommendation) on X86 CPU, and add the **intra_op_parallelism_threads** to tune the performance. Then I found that if we call any API including **list_devices()**, TF would initialize a global intra thread pool and overwrite the configuration of **intra_op_parallelism_threads**.\r\n\r\nNCF will call **is_gpu_available()** before training, and its implementation includes **list_devices()**, so the **intra_op_parallelism_threads** option is invalid.\r\n\r\nMy question is, dose this logic makes sense? If we want to run a model on CPU, the config may be invalid when we call some 'harmless' API and get no feedback.\r\n\r\n### Source code / logs\r\n\r\nhere's the modify of NCF to enable **intra_op_parallelism_threads**:\r\n[ncf_intra.txt](https://github.com/tensorflow/tensorflow/files/2373942/ncf_intra.txt)\r\n\r\nI also add some code to print the intra thread pool status in TF, this patch has changed the global setting to false to solve the issue, revert it to true will reproduce the question:\r\n[tf_intra.txt](https://github.com/tensorflow/tensorflow/files/2373947/tf_intra.txt)\r\n\r\nWith the print code, you can see that TF will try to initialize a device with a global intra thread pool when call **list_devices()**. Once if the global intra thread pool was initialized, other device couldn't change the configuration any more.\r\n\r\n\r\n"}