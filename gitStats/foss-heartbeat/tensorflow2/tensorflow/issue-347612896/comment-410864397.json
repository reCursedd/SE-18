{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/410864397", "html_url": "https://github.com/tensorflow/tensorflow/issues/21380#issuecomment-410864397", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21380", "id": 410864397, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMDg2NDM5Nw==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-06T21:51:53Z", "updated_at": "2018-08-06T21:51:53Z", "author_association": "MEMBER", "body_html": "<p>I should mention that since the fix won't be in 1.10, the following workaround can be used till then:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@tf.custom_gradient</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">my_relu6</span>(<span class=\"pl-smi\">x</span>):\n    out <span class=\"pl-k\">=</span> tf.nn.relu6(x)\n    <span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> nn_ops\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">grad</span>(<span class=\"pl-smi\">dy</span>):\n        <span class=\"pl-k\">return</span> nn_ops.relu6_grad(dy, out)\n    <span class=\"pl-k\">return</span> out, grad</pre></div>\n<p>And then use <code>my_relu6</code> instead of <code>tf.nn.relu6</code>. For example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\ntf.enable_eager_execution()\nw <span class=\"pl-k\">=</span> tf.contrib.eager.Variable([[<span class=\"pl-c1\">1.0</span>]])\n<span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n    loss <span class=\"pl-k\">=</span> my_relu6(w <span class=\"pl-k\">*</span> w)\ngrad <span class=\"pl-k\">=</span> tape.gradient(loss, w)</pre></div>", "body_text": "I should mention that since the fix won't be in 1.10, the following workaround can be used till then:\n@tf.custom_gradient\ndef my_relu6(x):\n    out = tf.nn.relu6(x)\n    from tensorflow.python.ops import nn_ops\n    def grad(dy):\n        return nn_ops.relu6_grad(dy, out)\n    return out, grad\nAnd then use my_relu6 instead of tf.nn.relu6. For example:\nimport tensorflow as tf\n\ntf.enable_eager_execution()\nw = tf.contrib.eager.Variable([[1.0]])\nwith tf.GradientTape() as tape:\n    loss = my_relu6(w * w)\ngrad = tape.gradient(loss, w)", "body": "I should mention that since the fix won't be in 1.10, the following workaround can be used till then:\r\n\r\n```python\r\n@tf.custom_gradient\r\ndef my_relu6(x):\r\n    out = tf.nn.relu6(x)\r\n    from tensorflow.python.ops import nn_ops\r\n    def grad(dy):\r\n        return nn_ops.relu6_grad(dy, out)\r\n    return out, grad\r\n```\r\n\r\nAnd then use `my_relu6` instead of `tf.nn.relu6`. For example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\nw = tf.contrib.eager.Variable([[1.0]])\r\nwith tf.GradientTape() as tape:\r\n    loss = my_relu6(w * w)\r\ngrad = tape.gradient(loss, w)\r\n```"}