{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17438", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17438/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17438/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17438/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/17438", "id": 302290508, "node_id": "MDExOlB1bGxSZXF1ZXN0MTcyODYwMDY4", "number": 17438, "title": "Add optimizers with decoupled weight decay.", "user": {"login": "PhilJd", "id": 16101605, "node_id": "MDQ6VXNlcjE2MTAxNjA1", "avatar_url": "https://avatars2.githubusercontent.com/u/16101605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilJd", "html_url": "https://github.com/PhilJd", "followers_url": "https://api.github.com/users/PhilJd/followers", "following_url": "https://api.github.com/users/PhilJd/following{/other_user}", "gists_url": "https://api.github.com/users/PhilJd/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilJd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilJd/subscriptions", "organizations_url": "https://api.github.com/users/PhilJd/orgs", "repos_url": "https://api.github.com/users/PhilJd/repos", "events_url": "https://api.github.com/users/PhilJd/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilJd/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 419840263, "node_id": "MDU6TGFiZWw0MTk4NDAyNjM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20testing%20(then%20merge)", "name": "awaiting testing (then merge)", "color": "c2e0c6", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2018-03-05T12:59:02Z", "updated_at": "2018-07-20T07:04:56Z", "closed_at": "2018-06-13T18:48:21Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17438", "html_url": "https://github.com/tensorflow/tensorflow/pull/17438", "diff_url": "https://github.com/tensorflow/tensorflow/pull/17438.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/17438.patch"}, "body_html": "<p>This pull request implements decoupled weight decay as described in 'Fixing Weight Decay Regularization' by  Loshchilov &amp; Hutter <a href=\"https://arxiv.org/abs/1711.05101\" rel=\"nofollow\">https://arxiv.org/abs/1711.05101</a>.</p>\n<p>This paper shows that for adaptive gradient algorithms, the implemented method regularizes variables with large gradients more than L2 regularization would and that this  yields better training loss and generalization error.</p>\n<p>For SGD variants, this simplifies hyperparameter search since it decouples the settings of weight decay and learning rate, which is nicely visualized in Fig. 2 in the paper:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/16101605/36976180-f2fcbf92-207c-11e8-879e-4677381d892d.png\"><img src=\"https://user-images.githubusercontent.com/16101605/36976180-f2fcbf92-207c-11e8-879e-4677381d892d.png\" alt=\"adamw\" style=\"max-width:100%;\"></a></p>\n<p>This implementation explicitly adds the optimizers described in the paper (<code>AdamW</code> and <code>MomentumW</code>) to tf.contrib.opt and provides a factory function <code>extend_with_decoupled_weight_decay</code> that can be used to create a new optimizer class with decoupled weight decay.</p>\n<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #15237.\">Closes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"280728601\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/15237\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/15237/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/15237\">#15237</a>.</p>", "body_text": "This pull request implements decoupled weight decay as described in 'Fixing Weight Decay Regularization' by  Loshchilov & Hutter https://arxiv.org/abs/1711.05101.\nThis paper shows that for adaptive gradient algorithms, the implemented method regularizes variables with large gradients more than L2 regularization would and that this  yields better training loss and generalization error.\nFor SGD variants, this simplifies hyperparameter search since it decouples the settings of weight decay and learning rate, which is nicely visualized in Fig. 2 in the paper:\n\nThis implementation explicitly adds the optimizers described in the paper (AdamW and MomentumW) to tf.contrib.opt and provides a factory function extend_with_decoupled_weight_decay that can be used to create a new optimizer class with decoupled weight decay.\nCloses #15237.", "body": "This pull request implements decoupled weight decay as described in 'Fixing Weight Decay Regularization' by  Loshchilov & Hutter [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).\r\n\r\nThis paper shows that for adaptive gradient algorithms, the implemented method regularizes variables with large gradients more than L2 regularization would and that this  yields better training loss and generalization error.\r\n\r\nFor SGD variants, this simplifies hyperparameter search since it decouples the settings of weight decay and learning rate, which is nicely visualized in Fig. 2 in the paper: \r\n![adamw](https://user-images.githubusercontent.com/16101605/36976180-f2fcbf92-207c-11e8-879e-4677381d892d.png)\r\n\r\nThis implementation explicitly adds the optimizers described in the paper (`AdamW` and `MomentumW`) to tf.contrib.opt and provides a factory function `extend_with_decoupled_weight_decay` that can be used to create a new optimizer class with decoupled weight decay.\r\n\r\nCloses #15237."}