{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/381871259", "html_url": "https://github.com/tensorflow/tensorflow/pull/17438#issuecomment-381871259", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17438", "id": 381871259, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTg3MTI1OQ==", "user": {"login": "PhilJd", "id": 16101605, "node_id": "MDQ6VXNlcjE2MTAxNjA1", "avatar_url": "https://avatars2.githubusercontent.com/u/16101605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilJd", "html_url": "https://github.com/PhilJd", "followers_url": "https://api.github.com/users/PhilJd/followers", "following_url": "https://api.github.com/users/PhilJd/following{/other_user}", "gists_url": "https://api.github.com/users/PhilJd/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilJd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilJd/subscriptions", "organizations_url": "https://api.github.com/users/PhilJd/orgs", "repos_url": "https://api.github.com/users/PhilJd/repos", "events_url": "https://api.github.com/users/PhilJd/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilJd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-17T07:03:46Z", "updated_at": "2018-04-17T07:34:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Following the discussion in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"280728601\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/15237\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/15237/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/15237\">#15237</a> I re-checked the apply functions for other optimizers.<br>\nThis implementation uses the fact that the <code>apply_</code> functions already get the precomputed gradient and Adam + Momentum optimizers don't compute values based on var, so the pre-decay implementation is equivalent to the algorithm described in the paper for these optimizers.<br>\nHowever, I just checked the apply functions of other optimizers and e.g. Ftrl computes factors based on var, so decoupling decay for such optimizers would need a custom op.<br>\nMy question now is: Do you favor limiting the decoupled implementation to Adam + Momentum optimizers, i.e., remove the extend_with-decoupled_weight_decay function or do you think adding a warning in the documentation is enough, still allowing to create e.g., AdadeltaW, AdagradW, RMSPropW with one line of code?</p>", "body_text": "Following the discussion in #15237 I re-checked the apply functions for other optimizers.\nThis implementation uses the fact that the apply_ functions already get the precomputed gradient and Adam + Momentum optimizers don't compute values based on var, so the pre-decay implementation is equivalent to the algorithm described in the paper for these optimizers.\nHowever, I just checked the apply functions of other optimizers and e.g. Ftrl computes factors based on var, so decoupling decay for such optimizers would need a custom op.\nMy question now is: Do you favor limiting the decoupled implementation to Adam + Momentum optimizers, i.e., remove the extend_with-decoupled_weight_decay function or do you think adding a warning in the documentation is enough, still allowing to create e.g., AdadeltaW, AdagradW, RMSPropW with one line of code?", "body": "Following the discussion in #15237 I re-checked the apply functions for other optimizers.\r\nThis implementation uses the fact that the `apply_` functions already get the precomputed gradient and Adam + Momentum optimizers don't compute values based on var, so the pre-decay implementation is equivalent to the algorithm described in the paper for these optimizers.\r\nHowever, I just checked the apply functions of other optimizers and e.g. Ftrl computes factors based on var, so decoupling decay for such optimizers would need a custom op.\r\nMy question now is: Do you favor limiting the decoupled implementation to Adam + Momentum optimizers, i.e., remove the extend_with-decoupled_weight_decay function or do you think adding a warning in the documentation is enough, still allowing to create e.g., AdadeltaW, AdagradW, RMSPropW with one line of code?"}