{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/406475737", "html_url": "https://github.com/tensorflow/tensorflow/pull/17438#issuecomment-406475737", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17438", "id": 406475737, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjQ3NTczNw==", "user": {"login": "yym-ustc", "id": 38059108, "node_id": "MDQ6VXNlcjM4MDU5MTA4", "avatar_url": "https://avatars0.githubusercontent.com/u/38059108?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yym-ustc", "html_url": "https://github.com/yym-ustc", "followers_url": "https://api.github.com/users/yym-ustc/followers", "following_url": "https://api.github.com/users/yym-ustc/following{/other_user}", "gists_url": "https://api.github.com/users/yym-ustc/gists{/gist_id}", "starred_url": "https://api.github.com/users/yym-ustc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yym-ustc/subscriptions", "organizations_url": "https://api.github.com/users/yym-ustc/orgs", "repos_url": "https://api.github.com/users/yym-ustc/repos", "events_url": "https://api.github.com/users/yym-ustc/events{/privacy}", "received_events_url": "https://api.github.com/users/yym-ustc/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-20T03:06:07Z", "updated_at": "2018-07-20T03:06:07Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16101605\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PhilJd\">@PhilJd</a> It's ok to use the AdamOptimizer, but when change to use AdaWOptimizer, I got the error.<br>\nweights_var = tf.trainable_variables()<br>\ngradients = tf.gradients(cost, weights_var)<br>\n#optimizer = tf.train.AdamOptimizer(learning_rate=deep_learning_rate)<br>\nAdamWOptimizer = tf.contrib.opt.extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)<br>\noptimizer = AdamWOptimizer(weight_decay=weight_decay, learning_rate=deep_learning_rate)<br>\ntrain_op = optimizer.apply_gradients(zip(gradients, weights_var))</p>", "body_text": "@PhilJd It's ok to use the AdamOptimizer, but when change to use AdaWOptimizer, I got the error.\nweights_var = tf.trainable_variables()\ngradients = tf.gradients(cost, weights_var)\n#optimizer = tf.train.AdamOptimizer(learning_rate=deep_learning_rate)\nAdamWOptimizer = tf.contrib.opt.extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)\noptimizer = AdamWOptimizer(weight_decay=weight_decay, learning_rate=deep_learning_rate)\ntrain_op = optimizer.apply_gradients(zip(gradients, weights_var))", "body": "@PhilJd It's ok to use the AdamOptimizer, but when change to use AdaWOptimizer, I got the error.\r\nweights_var = tf.trainable_variables()\r\ngradients = tf.gradients(cost, weights_var)\r\n#optimizer = tf.train.AdamOptimizer(learning_rate=deep_learning_rate)\r\nAdamWOptimizer = tf.contrib.opt.extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)\r\noptimizer = AdamWOptimizer(weight_decay=weight_decay, learning_rate=deep_learning_rate)\r\ntrain_op = optimizer.apply_gradients(zip(gradients, weights_var))"}