{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/376745983", "html_url": "https://github.com/tensorflow/tensorflow/pull/17438#issuecomment-376745983", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17438", "id": 376745983, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Njc0NTk4Mw==", "user": {"login": "AntreasAntoniou", "id": 10792502, "node_id": "MDQ6VXNlcjEwNzkyNTAy", "avatar_url": "https://avatars3.githubusercontent.com/u/10792502?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AntreasAntoniou", "html_url": "https://github.com/AntreasAntoniou", "followers_url": "https://api.github.com/users/AntreasAntoniou/followers", "following_url": "https://api.github.com/users/AntreasAntoniou/following{/other_user}", "gists_url": "https://api.github.com/users/AntreasAntoniou/gists{/gist_id}", "starred_url": "https://api.github.com/users/AntreasAntoniou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AntreasAntoniou/subscriptions", "organizations_url": "https://api.github.com/users/AntreasAntoniou/orgs", "repos_url": "https://api.github.com/users/AntreasAntoniou/repos", "events_url": "https://api.github.com/users/AntreasAntoniou/events{/privacy}", "received_events_url": "https://api.github.com/users/AntreasAntoniou/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-28T03:07:11Z", "updated_at": "2018-03-28T03:07:11Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Thanks for your reply.\n\nI am using the cosine annealing with warm restarts learning rate scheduler.\nI use the normalized weight decay as specified in the paper. Furthermore,\nwhen I replace training=False to training=True at inference time the whole\nthing just works. So I suspect that the batch norm global statistics don't\nget updated somehow, but I will take a closer look to see what exactly is\noff. :)</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On 28 March 2018 at 03:59, Phil ***@***.***&gt; wrote:\n Thanks for trying this out! :)\n I tested this implementation adapting the official resnet\n &lt;<a href=\"https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py\">https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py</a>&gt;\n implementation and it worked for me, with and without decaying the batch\n norm variables.\n I suspect that your training diverged as the optimizer is not active in\n the forward pass (assuming you don't do inference on the backward pass).\n\n    - Did you adapt the weight decay rate? Most likely it should be lower\n    compared to l2 loss regularization.\n    - Do you decay all variables, i.e. including the batch norm variables?\n    Often it's useful to exclude the batch norm vars from weight decay.\n    - Do you schedule the weight decay similar to the learning rate? I.e.,\n    when lowering the learning rate, do you also lower the weight decay?\n\n \u2014\n You are receiving this because you commented.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"302290508\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/17438\" href=\"https://github.com/tensorflow/tensorflow/pull/17438#issuecomment-376744340\">#17438 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AKSuNsPDPuC1KT1EW4Tz4ohSa83vSsU7ks5tivytgaJpZM4ScPo0\">https://github.com/notifications/unsubscribe-auth/AKSuNsPDPuC1KT1EW4Tz4ohSa83vSsU7ks5tivytgaJpZM4ScPo0</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Thanks for your reply.\n\nI am using the cosine annealing with warm restarts learning rate scheduler.\nI use the normalized weight decay as specified in the paper. Furthermore,\nwhen I replace training=False to training=True at inference time the whole\nthing just works. So I suspect that the batch norm global statistics don't\nget updated somehow, but I will take a closer look to see what exactly is\noff. :)\n\u2026\nOn 28 March 2018 at 03:59, Phil ***@***.***> wrote:\n Thanks for trying this out! :)\n I tested this implementation adapting the official resnet\n <https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py>\n implementation and it worked for me, with and without decaying the batch\n norm variables.\n I suspect that your training diverged as the optimizer is not active in\n the forward pass (assuming you don't do inference on the backward pass).\n\n    - Did you adapt the weight decay rate? Most likely it should be lower\n    compared to l2 loss regularization.\n    - Do you decay all variables, i.e. including the batch norm variables?\n    Often it's useful to exclude the batch norm vars from weight decay.\n    - Do you schedule the weight decay similar to the learning rate? I.e.,\n    when lowering the learning rate, do you also lower the weight decay?\n\n \u2014\n You are receiving this because you commented.\n Reply to this email directly, view it on GitHub\n <#17438 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AKSuNsPDPuC1KT1EW4Tz4ohSa83vSsU7ks5tivytgaJpZM4ScPo0>\n .", "body": "Thanks for your reply.\n\nI am using the cosine annealing with warm restarts learning rate scheduler.\nI use the normalized weight decay as specified in the paper. Furthermore,\nwhen I replace training=False to training=True at inference time the whole\nthing just works. So I suspect that the batch norm global statistics don't\nget updated somehow, but I will take a closer look to see what exactly is\noff. :)\n\nOn 28 March 2018 at 03:59, Phil <notifications@github.com> wrote:\n\n> Thanks for trying this out! :)\n> I tested this implementation adapting the official resnet\n> <https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py>\n> implementation and it worked for me, with and without decaying the batch\n> norm variables.\n> I suspect that your training diverged as the optimizer is not active in\n> the forward pass (assuming you don't do inference on the backward pass).\n>\n>    - Did you adapt the weight decay rate? Most likely it should be lower\n>    compared to l2 loss regularization.\n>    - Do you decay all variables, i.e. including the batch norm variables?\n>    Often it's useful to exclude the batch norm vars from weight decay.\n>    - Do you schedule the weight decay similar to the learning rate? I.e.,\n>    when lowering the learning rate, do you also lower the weight decay?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/17438#issuecomment-376744340>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKSuNsPDPuC1KT1EW4Tz4ohSa83vSsU7ks5tivytgaJpZM4ScPo0>\n> .\n>\n"}