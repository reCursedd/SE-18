{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/376744340", "html_url": "https://github.com/tensorflow/tensorflow/pull/17438#issuecomment-376744340", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17438", "id": 376744340, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Njc0NDM0MA==", "user": {"login": "PhilJd", "id": 16101605, "node_id": "MDQ6VXNlcjE2MTAxNjA1", "avatar_url": "https://avatars2.githubusercontent.com/u/16101605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilJd", "html_url": "https://github.com/PhilJd", "followers_url": "https://api.github.com/users/PhilJd/followers", "following_url": "https://api.github.com/users/PhilJd/following{/other_user}", "gists_url": "https://api.github.com/users/PhilJd/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilJd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilJd/subscriptions", "organizations_url": "https://api.github.com/users/PhilJd/orgs", "repos_url": "https://api.github.com/users/PhilJd/repos", "events_url": "https://api.github.com/users/PhilJd/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilJd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-28T02:57:20Z", "updated_at": "2018-03-28T02:57:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for trying this out! :)<br>\nI tested this implementation adapting the <a href=\"https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py\">official resnet</a> implementation and it worked for me, with and without decaying the batch norm variables.<br>\nI suspect that your training diverged as the optimizer is not active in the forward pass (assuming you don't do inference on the backward pass).</p>\n<ul>\n<li>Did you adapt the weight decay rate? Most likely it should be lower compared to l2 loss regularization.</li>\n<li>Do you decay all variables, i.e. including the batch norm variables? Often it's useful to exclude the batch norm vars from weight decay.</li>\n<li>Do you schedule the weight decay similar to the learning rate? I.e., when lowering the learning rate, do you also lower the weight decay?</li>\n</ul>", "body_text": "Thanks for trying this out! :)\nI tested this implementation adapting the official resnet implementation and it worked for me, with and without decaying the batch norm variables.\nI suspect that your training diverged as the optimizer is not active in the forward pass (assuming you don't do inference on the backward pass).\n\nDid you adapt the weight decay rate? Most likely it should be lower compared to l2 loss regularization.\nDo you decay all variables, i.e. including the batch norm variables? Often it's useful to exclude the batch norm vars from weight decay.\nDo you schedule the weight decay similar to the learning rate? I.e., when lowering the learning rate, do you also lower the weight decay?", "body": "Thanks for trying this out! :)\r\nI tested this implementation adapting the [official resnet](https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py) implementation and it worked for me, with and without decaying the batch norm variables.\r\nI suspect that your training diverged as the optimizer is not active in the forward pass (assuming you don't do inference on the backward pass).\r\n- Did you adapt the weight decay rate? Most likely it should be lower compared to l2 loss regularization. \r\n- Do you decay all variables, i.e. including the batch norm variables? Often it's useful to exclude the batch norm vars from weight decay.\r\n- Do you schedule the weight decay similar to the learning rate? I.e., when lowering the learning rate, do you also lower the weight decay?"}