{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/182414533", "pull_request_review_id": 113203382, "id": 182414533, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MjQxNDUzMw==", "diff_hunk": "@@ -0,0 +1,296 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"Base class to make optimizers weight decay ready.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.python.framework import ops\n+from tensorflow.python.training import optimizer\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.training import adam, momentum\n+from tensorflow.python.util.tf_export import tf_export\n+\n+\n+def extend_with_decoupled_weight_decay(base_optimizer):\n+  \"\"\"Factory function returning an optimizer class with decoupled weight decay.\n+\n+  Returns an optimizer class. An instance of the returned class computes the\n+  update step of `base_optimizer` and additionally decays the weights.\n+  E.g., the class returned by\n+  `extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)` is equivalent to\n+  `tf.contrib.opt.AdamWOptimizer`.\n+\n+  The API of the new optimizer class slightly differs from the API of the\n+  base optimizer:\n+  - The first argument to the constructor is the weight decay rate.\n+  - `minimize` and `apply_gradients` accept the optional keyword argument\n+    `decay_var_list`, which specifies the variables that should be decayed.\n+    If `None`, all variables that are optimized are decayed.\n+\n+  Usage example:\n+  ```python\n+  # MyAdamW is a new class\n+  MyAdamW = extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)\n+  # Create a MyAdamW object\n+  optimizer = MyAdamW(weight_decay=0.001, learning_rate=0.001)\n+  sess.run(optimizer.minimize(loss, decay_variables=[var1, var2]))\n+  ```\n+\n+  Args:\n+    base_optimizer: An optimizer class that inherits from tf.train.Optimizer.\n+\n+  Returns:\n+    A new optimizer class that inherits from DecoupledWeightDecayExtension\n+    and base_optimizer.\n+  \"\"\"\n+  class OptimizerWithDecoupledWeightDecay(DecoupledWeightDecayExtension,\n+                                          base_optimizer):\n+    \"\"\"Base_optimizer with decoupled weight decay.\n+\n+    This class computes the update step of `base_optimizer` and\n+    additionally decays the variable with the weight decay being decoupled from\n+    the optimization steps w.r.t. to the loss function, as described by\n+    Loshchilov & Hutter (https://arxiv.org/pdf/1711.05101.pdf).\n+    For SGD variants, this simplifies hyperparameter search since\n+    it decouples the settings of weight decay and learning rate.\n+    For adaptive gradient algorithms, it regularizes variables with large\n+    gradients more than L2 regularization would, which was shown to yield\n+    better training loss and generalization error in the paper above.\n+    \"\"\"\n+\n+    def __init__(self, weight_decay, *args, **kwargs):\n+      super(OptimizerWithDecoupledWeightDecay, self).__init__(\n+          weight_decay, *args, **kwargs)\n+\n+  return OptimizerWithDecoupledWeightDecay\n+\n+\n+class DecoupledWeightDecayExtension(object):\n+  \"\"\"This class allows to extend optimizers with decoupled weight decay.\n+\n+  It implements the decoupled weight decay described by Loshchilov & Hutter\n+  (https://arxiv.org/pdf/1711.05101.pdf), in which the weight decay is\n+  decoupled from the optimization steps w.r.t. to the loss function.\n+  For SGD variants, this simplifies hyperparameter search since it decouples\n+  the settings of weight decay and learning rate.\n+  For adaptive gradient algorithms, it regularizes variables with large\n+  gradients more than L2 regularization would, which was shown to yield better\n+  training loss and generalization error in the paper above.\n+\n+  This class alone is not an optimizer but rather extends existing\n+  optimizers with decoupled weight decay. We explicitly define the two examples\n+  used in the above paper (SGDW and AdamW), but in general this can extend\n+  any OptimizerX by using\n+  `extend_with_weight_decay(OptimizerX, weight_decay=weight_decay)`.\n+  In order for it to work, it must be the first class the Optimizer with\n+  weight decay inherits from, e.g.\n+\n+  ```python\n+  class AdamWOptimizer(DecoupledWeightDecayExtension, adam.AdamOptimizer):\n+    def __init__(self, weight_decay, *args, **kwargs):\n+      super(AdamWOptimizer, self).__init__(weight_decay, *args, **kwargs).\n+  ```\n+  \"\"\"\n+\n+  def __init__(self, weight_decay, **kwargs):\n+    \"\"\"Construct the extension class that adds weight decay to an optimizer.\n+\n+    Args:\n+      weight_decay: A `Tensor` or a floating point value, the factor by which\n+        a variable is decayed in the update step.\n+      decay_var_list: Optional list or tuple or set of `Variable` objects to\n+        decay.\n+    \"\"\"\n+    self._decay_var_list = None  # is set in minimize or apply_gradients\n+    self._weight_decay = weight_decay\n+    # The tensors are initialized in call to _prepare\n+    self._weight_decay_tensor = None\n+    super(DecoupledWeightDecayExtension, self).__init__(**kwargs)\n+\n+  def minimize(self, loss, global_step=None, var_list=None,\n+               gate_gradients=optimizer.Optimizer.GATE_OP,\n+               aggregation_method=None, colocate_gradients_with_ops=False,\n+               name=None, grad_loss=None, decay_var_list=None):\n+    \"\"\"Add operations to minimize `loss` by updating `var_list` with decay.\n+\n+    This function is the same as Optimizer.minimize except that it allows to\n+    specify the variables that should be decayed using decay_var_list.\n+    If decay_var_list is None, all variables in var_list are decayed.\n+\n+    For more information see the documentation of Optimizer.minimize.\n+    \"\"\"\n+    self._decay_var_list = set(decay_var_list) if decay_var_list else False\n+    return super(DecoupledWeightDecayExtension, self).minimize(\n+        loss, global_step=global_step, var_list=var_list,\n+        gate_gradients=gate_gradients, aggregation_method=aggregation_method,\n+        colocate_gradients_with_ops=colocate_gradients_with_ops, name=name,\n+        grad_loss=grad_loss)\n+\n+  def apply_gradients(self, grads_and_vars, global_step=None, name=None,\n+                      decay_var_list=None):\n+    \"\"\"Apply gradients to variables and decay the variables.\n+\n+    This function is the same as Optimizer.apply_gradients except that it\n+    allows to specify the variables that should be decayed using\n+    decay_var_list. If decay_var_list is None, all variables in var_list\n+    are decayed.\n+\n+    For more information see the documentation of Optimizer.apply_gradients.\n+    \"\"\"\n+    self._decay_var_list = set(decay_var_list) if decay_var_list else False\n+    return super(DecoupledWeightDecayExtension, self).apply_gradients(\n+        grads_and_vars, global_step=global_step, name=name)\n+\n+  def _prepare(self):\n+    weight_decay = self._weight_decay\n+    if callable(weight_decay):\n+      weight_decay = weight_decay()\n+    self._weight_decay_tensor = ops.convert_to_tensor(\n+        weight_decay, name=\"weight_decay\")\n+    # Call the optimizers _prepare function.\n+    super(DecoupledWeightDecayExtension, self)._prepare()\n+\n+  def _decay_weights(self, var):\n+    if (not self._decay_var_list or\n+            (self._decay_var_list and var in self._decay_var_list)):\n+      return var.assign_sub(self._weight_decay * var, self._use_locking)\n+    return control_flow_ops.no_op()\n+\n+  # Overwrite the apply functions the base optimizer calls. super().apply_x\n+  # resolves to the apply_x function of the child's BaseOptimizer.\n+  def _apply_dense(self, grad, var):\n+    with ops.control_dependencies([self._decay_weights(var)]):\n+      return super(DecoupledWeightDecayExtension, self)._apply_dense(grad, var)\n+\n+  def _resource_apply_dense(self, grad, var):\n+    with ops.control_dependencies([self._decay_weights(var)]):\n+      return super(DecoupledWeightDecayExtension, self)._resource_apply_dense(\n+          grad, var)\n+\n+  def _apply_sparse(self, grad, var):", "path": "tensorflow/contrib/opt/python/training/weight_decay_optimizers.py", "position": null, "original_position": 184, "commit_id": "20b120c10c76e53873208fecaba4b7fc5263be6e", "original_commit_id": "87c9acba87f35298f859c42d6fa03e8c4e618831", "user": {"login": "PhilJd", "id": 16101605, "node_id": "MDQ6VXNlcjE2MTAxNjA1", "avatar_url": "https://avatars2.githubusercontent.com/u/16101605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilJd", "html_url": "https://github.com/PhilJd", "followers_url": "https://api.github.com/users/PhilJd/followers", "following_url": "https://api.github.com/users/PhilJd/following{/other_user}", "gists_url": "https://api.github.com/users/PhilJd/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilJd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilJd/subscriptions", "organizations_url": "https://api.github.com/users/PhilJd/orgs", "repos_url": "https://api.github.com/users/PhilJd/repos", "events_url": "https://api.github.com/users/PhilJd/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilJd/received_events", "type": "User", "site_admin": false}, "body": "I completely agree but I wasn't sure what would be your preference here as e.g. Adam does dense updates of momentum even when using `apply_sparse` if I remember correctly.\r\nWhat would you prefer:\r\n- add a warning\r\n- do sparse weight decay\r\n- add a flag `force_dense_decay` to `__init__` , which always computes dense decay if True and sparse decay for `apply_sparse` if false?", "created_at": "2018-04-18T12:53:33Z", "updated_at": "2018-06-13T16:13:47Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/17438#discussion_r182414533", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17438", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/182414533"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/17438#discussion_r182414533"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17438"}}, "body_html": "<p>I completely agree but I wasn't sure what would be your preference here as e.g. Adam does dense updates of momentum even when using <code>apply_sparse</code> if I remember correctly.<br>\nWhat would you prefer:</p>\n<ul>\n<li>add a warning</li>\n<li>do sparse weight decay</li>\n<li>add a flag <code>force_dense_decay</code> to <code>__init__</code> , which always computes dense decay if True and sparse decay for <code>apply_sparse</code> if false?</li>\n</ul>", "body_text": "I completely agree but I wasn't sure what would be your preference here as e.g. Adam does dense updates of momentum even when using apply_sparse if I remember correctly.\nWhat would you prefer:\n\nadd a warning\ndo sparse weight decay\nadd a flag force_dense_decay to __init__ , which always computes dense decay if True and sparse decay for apply_sparse if false?", "in_reply_to_id": 182403534}