{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23236", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23236/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23236/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23236/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23236", "id": 373721893, "node_id": "MDU6SXNzdWUzNzM3MjE4OTM=", "number": 23236, "title": "Unnecessary gpu allocations when reusing resource variables that are initialized from checkpoint", "user": {"login": "WuTheFWasThat", "id": 1479648, "node_id": "MDQ6VXNlcjE0Nzk2NDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1479648?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WuTheFWasThat", "html_url": "https://github.com/WuTheFWasThat", "followers_url": "https://api.github.com/users/WuTheFWasThat/followers", "following_url": "https://api.github.com/users/WuTheFWasThat/following{/other_user}", "gists_url": "https://api.github.com/users/WuTheFWasThat/gists{/gist_id}", "starred_url": "https://api.github.com/users/WuTheFWasThat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WuTheFWasThat/subscriptions", "organizations_url": "https://api.github.com/users/WuTheFWasThat/orgs", "repos_url": "https://api.github.com/users/WuTheFWasThat/repos", "events_url": "https://api.github.com/users/WuTheFWasThat/events{/privacy}", "received_events_url": "https://api.github.com/users/WuTheFWasThat/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097547538, "node_id": "MDU6TGFiZWwxMDk3NTQ3NTM4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:gpu", "name": "comp:gpu", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-10-24T23:59:47Z", "updated_at": "2018-11-05T23:12:45Z", "closed_at": "2018-11-05T23:12:45Z", "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>\n<p>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):<br>\n<strong>Yes</strong></p>\n</li>\n<li>\n<p>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):<br>\n<strong>Ubuntu 16.04</strong></p>\n</li>\n<li>\n<p>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:</p>\n</li>\n<li>\n<p>TensorFlow installed from (source or binary):<br>\n<strong>binary</strong></p>\n</li>\n<li>\n<p>TensorFlow version (use command below):<br>\n<strong>b'v1.11.0-rc1-0-ge4c4b20' 1.11.0-rc1</strong></p>\n</li>\n<li>\n<p>Python version:<br>\n<strong>3.6.6</strong></p>\n</li>\n<li>\n<p>Bazel version (if compiling from source):</p>\n</li>\n<li>\n<p>GCC/Compiler version (if compiling from source):</p>\n</li>\n<li>\n<p>CUDA/cuDNN version:<br>\n<strong>9.2</strong></p>\n</li>\n<li>\n<p>GPU model and memory:<br>\n<strong>Tesla K80</strong></p>\n</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nGPU memory is very high when using init_from_checkpoint and reuse=True with resource variables, in certain cases.</p>\n<p><strong>Describe the expected behavior</strong><br>\nGPU memory should be at most parameters+activations</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<pre><code>#!/usr/bin/env python3\n\n\"\"\"\n# first:\npython repro_bug.py --write_ckpt\n\n# then, compare:\npython repro_bug.py # good\npython repro_bug.py --use_ckpt # bad\n\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.contrib.training import HParams\nimport fire\n\ndef model(*, X, hparams):\n    with tf.variable_scope('model'):\n        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.02))\n        h = tf.gather(wte, X)\n        return tf.matmul(h, wte, transpose_b=True)\n\ndef main(\n        batch_size=20, \n        length=100, # increase this to leak more memory!\n        write_ckpt=False, use_ckpt=False, ckpt_path='/tmp/reprobug.ckpt',\n        # hparams\n        n_vocab=400, n_embd=300,\n):\n    hparams = HParams(n_vocab=n_vocab, n_embd=n_embd)\n\n    if write_ckpt:\n        with tf.Graph().as_default() as g:\n            X = tf.placeholder(shape=[batch_size], dtype=tf.int32)\n            results = model(X=X, hparams=hparams)\n            saver = tf.train.Saver()\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                saver.save(sess, ckpt_path)\n        return\n\n    X = tf.fill([batch_size], 0) # initial value\n    for _ in range(length):\n        # NOTE: doing something like reuse=(i &gt; 0) doesn't help\n        with tf.variable_scope('step', reuse=tf.AUTO_REUSE, use_resource=True):\n            logits_flat = model(X=X, hparams=hparams)\n        X = tf.squeeze(tf.multinomial(logits_flat, num_samples=1, output_dtype=tf.int32), axis=[1])\n\n    if use_ckpt:\n        print('setting initializers')\n        tf.train.init_from_checkpoint(ckpt_path, {'/': 'step/'})\n\n    config = tf.ConfigProto()\n    config.gpu_options.visible_device_list = '0'\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n\n        run_metadata = tf.RunMetadata()\n        sess.run(X, feed_dict={}, options=tf.RunOptions(trace_level=tf.RunOptions.HARDWARE_TRACE), run_metadata=run_metadata)\n        mbs_in_use = []\n        for dev_stat in run_metadata.step_stats.dev_stats[:]:\n            for node_stat in dev_stat.node_stats[:]:\n                for mem in node_stat.memory:\n                    if mem.allocator_bytes_in_use and mem.allocator_name.startswith(\"GPU\"):\n                        mbs_in_use.append(mem.allocator_bytes_in_use // 1000000)\n        print(\"mbs in use: \" , \" -&gt; \".join(map(str, mbs_in_use)))\n\nif __name__ == '__main__':\n    fire.Fire(main)\n</code></pre>\n<p><strong>Other info / logs</strong></p>\n<p>Output looks like this:</p>\n<pre><code>setting initializers\nmbs in use:  0 -&gt; 1 -&gt; 1 -&gt; 8 -&gt; 10 -&gt; 18 -&gt; 27 -&gt; 27 -&gt; 35 -&gt; [etc...]\n</code></pre>\n<p>without calling initialize_from_checkpoint, it looks like:</p>\n<pre><code>mbs in use:  0 -&gt; 0 -&gt; 0 -&gt; 0 -&gt; 0 -&gt; 0 -&gt; 0 -&gt; 0 -&gt; 0 -&gt; 0 -&gt; [etc...]\n</code></pre>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04\n\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n\n\nTensorFlow installed from (source or binary):\nbinary\n\n\nTensorFlow version (use command below):\nb'v1.11.0-rc1-0-ge4c4b20' 1.11.0-rc1\n\n\nPython version:\n3.6.6\n\n\nBazel version (if compiling from source):\n\n\nGCC/Compiler version (if compiling from source):\n\n\nCUDA/cuDNN version:\n9.2\n\n\nGPU model and memory:\nTesla K80\n\n\nDescribe the current behavior\nGPU memory is very high when using init_from_checkpoint and reuse=True with resource variables, in certain cases.\nDescribe the expected behavior\nGPU memory should be at most parameters+activations\nCode to reproduce the issue\n#!/usr/bin/env python3\n\n\"\"\"\n# first:\npython repro_bug.py --write_ckpt\n\n# then, compare:\npython repro_bug.py # good\npython repro_bug.py --use_ckpt # bad\n\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.contrib.training import HParams\nimport fire\n\ndef model(*, X, hparams):\n    with tf.variable_scope('model'):\n        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.02))\n        h = tf.gather(wte, X)\n        return tf.matmul(h, wte, transpose_b=True)\n\ndef main(\n        batch_size=20, \n        length=100, # increase this to leak more memory!\n        write_ckpt=False, use_ckpt=False, ckpt_path='/tmp/reprobug.ckpt',\n        # hparams\n        n_vocab=400, n_embd=300,\n):\n    hparams = HParams(n_vocab=n_vocab, n_embd=n_embd)\n\n    if write_ckpt:\n        with tf.Graph().as_default() as g:\n            X = tf.placeholder(shape=[batch_size], dtype=tf.int32)\n            results = model(X=X, hparams=hparams)\n            saver = tf.train.Saver()\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                saver.save(sess, ckpt_path)\n        return\n\n    X = tf.fill([batch_size], 0) # initial value\n    for _ in range(length):\n        # NOTE: doing something like reuse=(i > 0) doesn't help\n        with tf.variable_scope('step', reuse=tf.AUTO_REUSE, use_resource=True):\n            logits_flat = model(X=X, hparams=hparams)\n        X = tf.squeeze(tf.multinomial(logits_flat, num_samples=1, output_dtype=tf.int32), axis=[1])\n\n    if use_ckpt:\n        print('setting initializers')\n        tf.train.init_from_checkpoint(ckpt_path, {'/': 'step/'})\n\n    config = tf.ConfigProto()\n    config.gpu_options.visible_device_list = '0'\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n\n        run_metadata = tf.RunMetadata()\n        sess.run(X, feed_dict={}, options=tf.RunOptions(trace_level=tf.RunOptions.HARDWARE_TRACE), run_metadata=run_metadata)\n        mbs_in_use = []\n        for dev_stat in run_metadata.step_stats.dev_stats[:]:\n            for node_stat in dev_stat.node_stats[:]:\n                for mem in node_stat.memory:\n                    if mem.allocator_bytes_in_use and mem.allocator_name.startswith(\"GPU\"):\n                        mbs_in_use.append(mem.allocator_bytes_in_use // 1000000)\n        print(\"mbs in use: \" , \" -> \".join(map(str, mbs_in_use)))\n\nif __name__ == '__main__':\n    fire.Fire(main)\n\nOther info / logs\nOutput looks like this:\nsetting initializers\nmbs in use:  0 -> 1 -> 1 -> 8 -> 10 -> 18 -> 27 -> 27 -> 35 -> [etc...]\n\nwithout calling initialize_from_checkpoint, it looks like:\nmbs in use:  0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> [etc...]", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n**Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n**binary**\r\n- TensorFlow version (use command below):\r\n**b'v1.11.0-rc1-0-ge4c4b20' 1.11.0-rc1**\r\n\r\n- Python version:\r\n**3.6.6**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n**9.2**\r\n- GPU model and memory:\r\n**Tesla K80**\r\n\r\n**Describe the current behavior**\r\nGPU memory is very high when using init_from_checkpoint and reuse=True with resource variables, in certain cases.\r\n\r\n**Describe the expected behavior**\r\nGPU memory should be at most parameters+activations\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\n\"\"\"\r\n# first:\r\npython repro_bug.py --write_ckpt\r\n\r\n# then, compare:\r\npython repro_bug.py # good\r\npython repro_bug.py --use_ckpt # bad\r\n\"\"\"\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.training import HParams\r\nimport fire\r\n\r\ndef model(*, X, hparams):\r\n    with tf.variable_scope('model'):\r\n        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\r\n                             initializer=tf.random_normal_initializer(stddev=0.02))\r\n        h = tf.gather(wte, X)\r\n        return tf.matmul(h, wte, transpose_b=True)\r\n\r\ndef main(\r\n        batch_size=20, \r\n        length=100, # increase this to leak more memory!\r\n        write_ckpt=False, use_ckpt=False, ckpt_path='/tmp/reprobug.ckpt',\r\n        # hparams\r\n        n_vocab=400, n_embd=300,\r\n):\r\n    hparams = HParams(n_vocab=n_vocab, n_embd=n_embd)\r\n\r\n    if write_ckpt:\r\n        with tf.Graph().as_default() as g:\r\n            X = tf.placeholder(shape=[batch_size], dtype=tf.int32)\r\n            results = model(X=X, hparams=hparams)\r\n            saver = tf.train.Saver()\r\n            with tf.Session() as sess:\r\n                sess.run(tf.global_variables_initializer())\r\n                saver.save(sess, ckpt_path)\r\n        return\r\n\r\n    X = tf.fill([batch_size], 0) # initial value\r\n    for _ in range(length):\r\n        # NOTE: doing something like reuse=(i > 0) doesn't help\r\n        with tf.variable_scope('step', reuse=tf.AUTO_REUSE, use_resource=True):\r\n            logits_flat = model(X=X, hparams=hparams)\r\n        X = tf.squeeze(tf.multinomial(logits_flat, num_samples=1, output_dtype=tf.int32), axis=[1])\r\n\r\n    if use_ckpt:\r\n        print('setting initializers')\r\n        tf.train.init_from_checkpoint(ckpt_path, {'/': 'step/'})\r\n\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.visible_device_list = '0'\r\n    with tf.Session(config=config) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        run_metadata = tf.RunMetadata()\r\n        sess.run(X, feed_dict={}, options=tf.RunOptions(trace_level=tf.RunOptions.HARDWARE_TRACE), run_metadata=run_metadata)\r\n        mbs_in_use = []\r\n        for dev_stat in run_metadata.step_stats.dev_stats[:]:\r\n            for node_stat in dev_stat.node_stats[:]:\r\n                for mem in node_stat.memory:\r\n                    if mem.allocator_bytes_in_use and mem.allocator_name.startswith(\"GPU\"):\r\n                        mbs_in_use.append(mem.allocator_bytes_in_use // 1000000)\r\n        print(\"mbs in use: \" , \" -> \".join(map(str, mbs_in_use)))\r\n\r\nif __name__ == '__main__':\r\n    fire.Fire(main)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nOutput looks like this:\r\n```\r\nsetting initializers\r\nmbs in use:  0 -> 1 -> 1 -> 8 -> 10 -> 18 -> 27 -> 27 -> 35 -> [etc...]\r\n```\r\nwithout calling initialize_from_checkpoint, it looks like:\r\n```\r\nmbs in use:  0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> 0 -> [etc...]\r\n```\r\n"}