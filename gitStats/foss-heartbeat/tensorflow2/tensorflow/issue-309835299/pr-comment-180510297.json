{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/180510297", "pull_request_review_id": 110942322, "id": 180510297, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MDUxMDI5Nw==", "diff_hunk": "@@ -50,70 +50,178 @@ def rejection_resample(class_func, target_dist, initial_dist=None, seed=None):\n     A `Dataset` transformation function, which can be passed to\n     @{tf.data.Dataset.apply}.\n   \"\"\"\n-\n   def _apply_fn(dataset):\n     \"\"\"Function from `Dataset` to `Dataset` that applies the transformation.\"\"\"\n-    dist_estimation_batch_size = 32\n     target_dist_t = ops.convert_to_tensor(target_dist, name=\"target_dist\")\n     class_values_ds = dataset.map(class_func)\n+\n+    # Get initial distribution.\n     if initial_dist is not None:\n-      initial_dist_t = ops.convert_to_tensor(initial_dist, name=\"initial_dist\")\n+      initial_dist_t = ops.convert_to_tensor(\n+          initial_dist, name=\"initial_dist\")\n       acceptance_dist = _calculate_acceptance_probs(initial_dist_t,\n                                                     target_dist_t)\n       initial_dist_ds = dataset_ops.Dataset.from_tensors(\n           initial_dist_t).repeat()\n       acceptance_dist_ds = dataset_ops.Dataset.from_tensors(\n           acceptance_dist).repeat()\n     else:\n-      num_classes = (target_dist_t.shape[0].value or\n-                     array_ops.shape(target_dist_t)[0])\n-      smoothing_constant = 10\n-      initial_examples_per_class_seen = array_ops.fill(\n-          [num_classes], np.int64(smoothing_constant))\n-\n-      def update_estimate_and_tile(num_examples_per_class_seen, c):\n-        updated_examples_per_class_seen, dist = _estimate_data_distribution(\n-            c, num_examples_per_class_seen)\n-        tiled_dist = array_ops.tile(\n-            array_ops.expand_dims(dist, 0), [dist_estimation_batch_size, 1])\n-        return updated_examples_per_class_seen, tiled_dist\n-\n-      initial_dist_ds = (class_values_ds.batch(dist_estimation_batch_size)\n-                         .apply(scan_ops.scan(initial_examples_per_class_seen,\n-                                              update_estimate_and_tile))\n-                         .apply(batching.unbatch()))\n+      initial_dist_ds = _estimate_initial_dist_ds(\n+          target_dist_t, class_values_ds)\n       acceptance_dist_ds = initial_dist_ds.map(\n           lambda initial: _calculate_acceptance_probs(initial, target_dist_t))\n+    return _filter_ds(dataset, acceptance_dist_ds, initial_dist_ds,\n+                      class_values_ds, seed)\n+\n+  return _apply_fn\n+\n+\n+def rejection_resample_v2(class_func, target_dist, initial_dist=None,\n+                          seed=None):\n+  \"\"\"A transformation that resamples a dataset to achieve a target distribution.\n+\n+  This differs from v1 in that it will also sample from the original dataset\n+  with some probability, so it makes strictly fewer data rejections. Due to an\n+  implementation detail it must initialize a separate dataset initializer, so\n+  the dataset becomes stateful after this transformation is applied\n+  (`make_one_shot_iterator` won't work; users must use\n+  `make_initializable_iterator`). This transformation is faster than the\n+  original, except for overhead.\n+\n+  **NOTE** Resampling is performed via rejection sampling; some fraction\n+  of the input values will be dropped.\n+\n+  Args:\n+    class_func: A function mapping an element of the input dataset to a scalar\n+      `tf.int32` tensor. Values should be in `[0, num_classes)`.\n+    target_dist: A floating point type tensor, shaped `[num_classes]`.\n+    initial_dist: (Optional.)  A floating point type tensor, shaped\n+      `[num_classes]`.  If not provided, the true class distribution is\n+      estimated live in a streaming fashion.\n+    seed: (Optional.) Python integer seed for the resampler.\n \n-    def maybe_warn_on_large_rejection(accept_dist, initial_dist):\n-      proportion_rejected = math_ops.reduce_sum(\n-          (1 - accept_dist) * initial_dist)\n-      return control_flow_ops.cond(\n-          math_ops.less(proportion_rejected, .5),\n-          lambda: accept_dist,\n-          lambda: logging_ops.Print(  # pylint: disable=g-long-lambda\n-              accept_dist, [proportion_rejected, initial_dist, accept_dist],\n-              message=\"Proportion of examples rejected by sampler is high: \",\n-              summarize=100,\n-              first_n=10))\n-\n-    acceptance_dist_ds = (dataset_ops.Dataset.zip((acceptance_dist_ds,\n-                                                   initial_dist_ds))\n-                          .map(maybe_warn_on_large_rejection))\n-\n-    def _gather_and_copy(class_val, acceptance_prob, data):\n-      return (class_val, array_ops.gather(acceptance_prob, class_val), data)\n-    current_probabilities_and_class_and_data_ds = dataset_ops.Dataset.zip(\n-        (class_values_ds, acceptance_dist_ds, dataset)).map(_gather_and_copy)\n-    filtered_ds = (\n-        current_probabilities_and_class_and_data_ds\n-        .filter(lambda _1, p, _2: random_ops.random_uniform([], seed=seed) < p))\n-    return filtered_ds.map(lambda class_value, _, data: (class_value, data))\n+  Returns:\n+    A `Dataset` transformation function, which can be passed to\n+    @{tf.data.Dataset.apply}.\n+  \"\"\"\n+  def _apply_fn(dataset):\n+    \"\"\"Function from `Dataset` to `Dataset` that applies the transformation.\"\"\"\n+    target_dist_t = ops.convert_to_tensor(target_dist, name=\"target_dist\")\n+    class_values_ds = dataset.map(class_func)\n \n+    # Get initial distribution.\n+    if initial_dist is not None:\n+      initial_dist_t = ops.convert_to_tensor(\n+          initial_dist, name=\"initial_dist\")\n+      acceptance_dist, prob_of_original = (\n+          _calculate_acceptance_probs_with_mixing(initial_dist_t,\n+                                                  target_dist_t))\n+      initial_dist_ds = dataset_ops.Dataset.from_tensors(\n+          initial_dist_t).repeat()\n+      acceptance_dist_ds = dataset_ops.Dataset.from_tensors(\n+          acceptance_dist).repeat()\n+      prob_of_original_ds = dataset_ops.Dataset.from_tensors(\n+          prob_of_original).repeat()\n+    else:\n+      initial_dist_ds = _estimate_initial_dist_ds(\n+          target_dist_t, class_values_ds)\n+      acceptance_and_original_prob_ds = initial_dist_ds.map(\n+          lambda initial: _calculate_acceptance_probs_with_mixing(\n+              initial, target_dist_t))\n+      acceptance_dist_ds = acceptance_and_original_prob_ds.map(\n+          lambda accept_prob, _: accept_prob)\n+      prob_of_original_ds = acceptance_and_original_prob_ds.map(\n+          lambda _, prob_original: prob_original)\n+    filtered_ds = _filter_ds(dataset, acceptance_dist_ds, initial_dist_ds,\n+                             class_values_ds, seed)\n+\n+    # We need to carefully combine `filtered_ds` and the original `dataset` so\n+    # that we don't needlessly compute the filtering.\n+    num_filtered_to_prefetch = 3\n+    filtered_ds = filtered_ds.prefetch(num_filtered_to_prefetch)\n+    filtered_iterator = filtered_ds.make_one_shot_iterator()", "path": "tensorflow/contrib/data/python/ops/resampling.py", "position": 137, "original_position": 137, "commit_id": "93fb13d34c5575b484990adc6a1ae0d4ac077386", "original_commit_id": "962c7091ebffdfa1aa3ffec9a2c09dfd68ee846b", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "body": "This is unfortunate, because the iterator becomes an opaque piece of state in the pipeline and things like checkpointing and (ironically enough) `make_one_shot_iterator()` will no longer work.\r\n\r\nCan you find a way that works using pure dataset transformations? ", "created_at": "2018-04-10T17:45:02Z", "updated_at": "2018-04-19T19:35:53Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/18091#discussion_r180510297", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18091", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/180510297"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/18091#discussion_r180510297"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18091"}}, "body_html": "<p>This is unfortunate, because the iterator becomes an opaque piece of state in the pipeline and things like checkpointing and (ironically enough) <code>make_one_shot_iterator()</code> will no longer work.</p>\n<p>Can you find a way that works using pure dataset transformations?</p>", "body_text": "This is unfortunate, because the iterator becomes an opaque piece of state in the pipeline and things like checkpointing and (ironically enough) make_one_shot_iterator() will no longer work.\nCan you find a way that works using pure dataset transformations?"}