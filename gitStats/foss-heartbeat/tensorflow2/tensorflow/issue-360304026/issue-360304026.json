{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22277", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22277/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22277/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22277/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22277", "id": 360304026, "node_id": "MDU6SXNzdWUzNjAzMDQwMjY=", "number": 22277, "title": "How to compile the tensorflow-lite to get a smaller and faster library?", "user": {"login": "wzzju", "id": 17102274, "node_id": "MDQ6VXNlcjE3MTAyMjc0", "avatar_url": "https://avatars2.githubusercontent.com/u/17102274?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wzzju", "html_url": "https://github.com/wzzju", "followers_url": "https://api.github.com/users/wzzju/followers", "following_url": "https://api.github.com/users/wzzju/following{/other_user}", "gists_url": "https://api.github.com/users/wzzju/gists{/gist_id}", "starred_url": "https://api.github.com/users/wzzju/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wzzju/subscriptions", "organizations_url": "https://api.github.com/users/wzzju/orgs", "repos_url": "https://api.github.com/users/wzzju/repos", "events_url": "https://api.github.com/users/wzzju/events{/privacy}", "received_events_url": "https://api.github.com/users/wzzju/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "shashishekhar", "id": 1162712, "node_id": "MDQ6VXNlcjExNjI3MTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1162712?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shashishekhar", "html_url": "https://github.com/shashishekhar", "followers_url": "https://api.github.com/users/shashishekhar/followers", "following_url": "https://api.github.com/users/shashishekhar/following{/other_user}", "gists_url": "https://api.github.com/users/shashishekhar/gists{/gist_id}", "starred_url": "https://api.github.com/users/shashishekhar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shashishekhar/subscriptions", "organizations_url": "https://api.github.com/users/shashishekhar/orgs", "repos_url": "https://api.github.com/users/shashishekhar/repos", "events_url": "https://api.github.com/users/shashishekhar/events{/privacy}", "received_events_url": "https://api.github.com/users/shashishekhar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shashishekhar", "id": 1162712, "node_id": "MDQ6VXNlcjExNjI3MTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1162712?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shashishekhar", "html_url": "https://github.com/shashishekhar", "followers_url": "https://api.github.com/users/shashishekhar/followers", "following_url": "https://api.github.com/users/shashishekhar/following{/other_user}", "gists_url": "https://api.github.com/users/shashishekhar/gists{/gist_id}", "starred_url": "https://api.github.com/users/shashishekhar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shashishekhar/subscriptions", "organizations_url": "https://api.github.com/users/shashishekhar/orgs", "repos_url": "https://api.github.com/users/shashishekhar/repos", "events_url": "https://api.github.com/users/shashishekhar/events{/privacy}", "received_events_url": "https://api.github.com/users/shashishekhar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-09-14T13:25:45Z", "updated_at": "2018-10-02T16:27:41Z", "closed_at": "2018-10-02T16:27:41Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Mobile device</strong>:\n<ul>\n<li>HUAWEI Mate 9</li>\n<li>Xiaomi Mi 6</li>\n</ul>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>During Arm AI Developer Global Summit, the Google engineer said that TensorFlow-Lite can be compiled into an about 400KB library. However, when I compile the latest TensorFlow-Lite, I can only get a  1.2MB dynamic link library. The following script is my compiled command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>bazel build -c opt --cxxopt=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--std=c++11<span class=\"pl-pds\">'</span></span> --cxxopt=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-O3<span class=\"pl-pds\">'</span></span> --cxxopt=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-march=armv7-a<span class=\"pl-pds\">'</span></span> --cxxopt=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-mfpu=neon<span class=\"pl-pds\">'</span></span> --cxxopt=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-mfloat-abi=softfp<span class=\"pl-pds\">'</span></span> //tensorflow/contrib/lite/java:tensorflowlite --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a</pre></div>\n<p>What's wrong with this compiled command?</p>\n<p>Besides,  in order to test the performance of tensorflow-lite, I also compiled its benchmarks according to <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark\">this official doc</a> .The following script is my compiled command (offered by the official doc):</p>\n<pre><code>bazel build -c opt \\\n  --config=android_arm \\\n  --cxxopt='--std=c++11' \\\n  tensorflow/contrib/lite/tools/benchmark:benchmark_model\n</code></pre>\n<p>When running MobileNet_v1_1.0_224, its result is shown as follow:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/17102274/45552548-d9ca7f80-b863-11e8-8bd1-db14bf5c354f.png\"><img src=\"https://user-images.githubusercontent.com/17102274/45552548-d9ca7f80-b863-11e8-8bd1-db14bf5c354f.png\" alt=\"image\" style=\"max-width:100%;\"></a><br>\nI want to know how to get a faster result?</p>", "body_text": "System information\n\nMobile device:\n\nHUAWEI Mate 9\nXiaomi Mi 6\n\n\n\nDescribe the problem\nDuring Arm AI Developer Global Summit, the Google engineer said that TensorFlow-Lite can be compiled into an about 400KB library. However, when I compile the latest TensorFlow-Lite, I can only get a  1.2MB dynamic link library. The following script is my compiled command:\nbazel build -c opt --cxxopt='--std=c++11' --cxxopt='-O3' --cxxopt='-march=armv7-a' --cxxopt='-mfpu=neon' --cxxopt='-mfloat-abi=softfp' //tensorflow/contrib/lite/java:tensorflowlite --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a\nWhat's wrong with this compiled command?\nBesides,  in order to test the performance of tensorflow-lite, I also compiled its benchmarks according to this official doc .The following script is my compiled command (offered by the official doc):\nbazel build -c opt \\\n  --config=android_arm \\\n  --cxxopt='--std=c++11' \\\n  tensorflow/contrib/lite/tools/benchmark:benchmark_model\n\nWhen running MobileNet_v1_1.0_224, its result is shown as follow:\n\nI want to know how to get a faster result?", "body": "### System information\r\n- **Mobile device**: \r\n    * HUAWEI Mate 9\r\n    * Xiaomi Mi 6\r\n\r\n### Describe the problem\r\nDuring Arm AI Developer Global Summit, the Google engineer said that TensorFlow-Lite can be compiled into an about 400KB library. However, when I compile the latest TensorFlow-Lite, I can only get a  1.2MB dynamic link library. The following script is my compiled command:\r\n```bash\r\nbazel build -c opt --cxxopt='--std=c++11' --cxxopt='-O3' --cxxopt='-march=armv7-a' --cxxopt='-mfpu=neon' --cxxopt='-mfloat-abi=softfp' //tensorflow/contrib/lite/java:tensorflowlite --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a\r\n```\r\nWhat's wrong with this compiled command?\r\n\r\nBesides,  in order to test the performance of tensorflow-lite, I also compiled its benchmarks according to [this official doc](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark) .The following script is my compiled command (offered by the official doc):\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm \\\r\n  --cxxopt='--std=c++11' \\\r\n  tensorflow/contrib/lite/tools/benchmark:benchmark_model\r\n```\r\nWhen running MobileNet_v1_1.0_224, its result is shown as follow:\r\n![image](https://user-images.githubusercontent.com/17102274/45552548-d9ca7f80-b863-11e8-8bd1-db14bf5c354f.png)\r\nI want to know how to get a faster result?\r\n"}