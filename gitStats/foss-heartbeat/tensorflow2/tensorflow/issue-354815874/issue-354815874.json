{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21929", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21929/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21929/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21929/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21929", "id": 354815874, "node_id": "MDU6SXNzdWUzNTQ4MTU4NzQ=", "number": 21929, "title": "Poor performance of the model when enabling layer normalization with tf.contrib.rnn.LayerNormBasicLSTMCell", "user": {"login": "AzizCode92", "id": 19540527, "node_id": "MDQ6VXNlcjE5NTQwNTI3", "avatar_url": "https://avatars2.githubusercontent.com/u/19540527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AzizCode92", "html_url": "https://github.com/AzizCode92", "followers_url": "https://api.github.com/users/AzizCode92/followers", "following_url": "https://api.github.com/users/AzizCode92/following{/other_user}", "gists_url": "https://api.github.com/users/AzizCode92/gists{/gist_id}", "starred_url": "https://api.github.com/users/AzizCode92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AzizCode92/subscriptions", "organizations_url": "https://api.github.com/users/AzizCode92/orgs", "repos_url": "https://api.github.com/users/AzizCode92/repos", "events_url": "https://api.github.com/users/AzizCode92/events{/privacy}", "received_events_url": "https://api.github.com/users/AzizCode92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2018-08-28T16:47:52Z", "updated_at": "2018-11-01T00:48:39Z", "closed_at": "2018-09-07T20:48:50Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</p>\n</li>\n</ul>\n<p>VERSION=\"16.04.4 LTS (Xenial Xerus)\"<br>\nVERSION_ID=\"16.04\"<br>\nVERSION_CODENAME=xenial</p>\n<ul>\n<li>\n<p><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:<br>\nNo</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\ntf.VERSION = 1.9.0</p>\n</li>\n<li>\n<p><strong>Python version</strong>: python 2.7</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:<br>\n0.11.1</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\n9.1</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nTesla K80</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:<br>\n( No command)</p>\n</li>\n</ul>\n<hr>\n<p>Before writing my issue here,  I found a similar question on  <a href=\"https://stackoverflow.com/questions/45150101/why-is-layernormbasiclstmcell-much-slower-and-less-accurate-than-lstmcell\" rel=\"nofollow\">stackoverflow</a> where the author described the same problem I have but there were no valid answer and most of the comments where wrongly referring to technique related to batch_normalization which are non-applicable since we are talking about layer_normalization.<br>\n( <a href=\"https://theneuralperspective.com/2016/10/27/gradient-topics/\" rel=\"nofollow\">Difference between layer_normalzation and batch_normalization</a> )<br>\nI was trying to do some layer normalization with my model, and I have chosen the<br>\n<code>tf.contrib.rnn.LayerNormBasicLSTMCell</code> where <code>layer_norm</code> is set to True.<br>\nI started training the model and as unexpected the training/validation losses are decreasing slowly.<br>\nThe paper of layer normalization ( <a href=\"https://arxiv.org/pdf/1607.06450.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1607.06450.pdf</a> ) showed that this technique makes the training time much faster but the Tensorflow implementation showed the inverse case.<br>\nAre there please any hints on how to solve this problem? Any further explications on why the implementation makes the training slower?</p>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\n\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nNo\n\n\nTensorFlow installed from (source or binary):\nsource\n\n\nTensorFlow version (use command below):\ntf.VERSION = 1.9.0\n\n\nPython version: python 2.7\n\n\nBazel version (if compiling from source):\n0.11.1\n\n\nGCC/Compiler version (if compiling from source):\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\n\n\nCUDA/cuDNN version:\n9.1\n\n\nGPU model and memory:\nTesla K80\n\n\nExact command to reproduce:\n( No command)\n\n\n\nBefore writing my issue here,  I found a similar question on  stackoverflow where the author described the same problem I have but there were no valid answer and most of the comments where wrongly referring to technique related to batch_normalization which are non-applicable since we are talking about layer_normalization.\n( Difference between layer_normalzation and batch_normalization )\nI was trying to do some layer normalization with my model, and I have chosen the\ntf.contrib.rnn.LayerNormBasicLSTMCell where layer_norm is set to True.\nI started training the model and as unexpected the training/validation losses are decreasing slowly.\nThe paper of layer normalization ( https://arxiv.org/pdf/1607.06450.pdf ) showed that this technique makes the training time much faster but the Tensorflow implementation showed the inverse case.\nAre there please any hints on how to solve this problem? Any further explications on why the implementation makes the training slower?", "body": "-------------------------------------------------------------------------------------------------------\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\nNo\r\n- **TensorFlow installed from (source or binary)**:\r\n source\r\n- **TensorFlow version (use command below)**:  \r\ntf.VERSION = 1.9.0\r\n\r\n- **Python version**: python 2.7\r\n\r\n- **Bazel version (if compiling from source)**:  \r\n0.11.1\r\n\r\n- **GCC/Compiler version (if compiling from source)**:  \r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n\r\n- **CUDA/cuDNN version**: \r\n 9.1\r\n- **GPU model and memory**:  \r\nTesla K80\r\n- **Exact command to reproduce**:\r\n( No command)\r\n\r\n-------------------------------------------------------------------------------------------------------\r\nBefore writing my issue here,  I found a similar question on  [stackoverflow]( https://stackoverflow.com/questions/45150101/why-is-layernormbasiclstmcell-much-slower-and-less-accurate-than-lstmcell) where the author described the same problem I have but there were no valid answer and most of the comments where wrongly referring to technique related to batch_normalization which are non-applicable since we are talking about layer_normalization. \r\n( [Difference between layer_normalzation and batch_normalization](https://theneuralperspective.com/2016/10/27/gradient-topics/) )\r\nI was trying to do some layer normalization with my model, and I have chosen the \r\n`tf.contrib.rnn.LayerNormBasicLSTMCell` where `layer_norm` is set to True.\r\nI started training the model and as unexpected the training/validation losses are decreasing slowly.\r\nThe paper of layer normalization ( https://arxiv.org/pdf/1607.06450.pdf ) showed that this technique makes the training time much faster but the Tensorflow implementation showed the inverse case.\r\nAre there please any hints on how to solve this problem? Any further explications on why the implementation makes the training slower? "}