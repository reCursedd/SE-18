{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/418827279", "html_url": "https://github.com/tensorflow/tensorflow/issues/21929#issuecomment-418827279", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21929", "id": 418827279, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODgyNzI3OQ==", "user": {"login": "AzizCode92", "id": 19540527, "node_id": "MDQ6VXNlcjE5NTQwNTI3", "avatar_url": "https://avatars2.githubusercontent.com/u/19540527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AzizCode92", "html_url": "https://github.com/AzizCode92", "followers_url": "https://api.github.com/users/AzizCode92/followers", "following_url": "https://api.github.com/users/AzizCode92/following{/other_user}", "gists_url": "https://api.github.com/users/AzizCode92/gists{/gist_id}", "starred_url": "https://api.github.com/users/AzizCode92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AzizCode92/subscriptions", "organizations_url": "https://api.github.com/users/AzizCode92/orgs", "repos_url": "https://api.github.com/users/AzizCode92/repos", "events_url": "https://api.github.com/users/AzizCode92/events{/privacy}", "received_events_url": "https://api.github.com/users/AzizCode92/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-05T18:09:26Z", "updated_at": "2018-09-05T18:09:26Z", "author_association": "NONE", "body_html": "<p>here is the full code</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Recurrent Neural Network.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.</span>\n<span class=\"pl-s\">This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Links:</span>\n<span class=\"pl-s\">    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)</span>\n<span class=\"pl-s\">    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Author: Aymeric Damien</span>\n<span class=\"pl-s\">Project: https://github.com/aymericdamien/TensorFlow-Examples/</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib <span class=\"pl-k\">import</span> rnn\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Import MNIST data</span>\n<span class=\"pl-k\">from</span> tensorflow.examples.tutorials.mnist <span class=\"pl-k\">import</span> input_data\nmnist <span class=\"pl-k\">=</span> input_data.read_data_sets(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/tmp/data/<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">one_hot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">To classify images using a recurrent neural network, we consider every image</span>\n<span class=\"pl-s\">row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then</span>\n<span class=\"pl-s\">handle 28 sequences of 28 steps for every sample.</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Training Parameters</span>\nlearning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.001</span>\ntraining_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\ndisplay_step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">200</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Network Parameters</span>\nnum_input <span class=\"pl-k\">=</span> <span class=\"pl-c1\">28</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> MNIST data input (img shape: 28*28)</span>\ntimesteps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">28</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> timesteps</span>\nnum_hidden <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> hidden layer num of features</span>\nnum_classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> MNIST total classes (0-9 digits)</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> tf Graph input</span>\nX <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">None</span>, timesteps, num_input])\nY <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">None</span>, num_classes])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Define weights</span>\nweights <span class=\"pl-k\">=</span> {\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>out<span class=\"pl-pds\">'</span></span>: tf.Variable(tf.random_normal([num_hidden, num_classes]))\n}\nbiases <span class=\"pl-k\">=</span> {\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>out<span class=\"pl-pds\">'</span></span>: tf.Variable(tf.random_normal([num_classes]))\n}\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">RNN</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">weights</span>, <span class=\"pl-smi\">biases</span>):\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Prepare data shape to match `rnn` function requirements</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Current data input shape: (batch_size, timesteps, n_input)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Required shape: 'timesteps' tensors list of shape (batch_size, n_input)</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)</span>\n    x <span class=\"pl-k\">=</span> tf.unstack(x, timesteps, <span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define a lstm cell with tensorflow</span>\n    lstm_cell <span class=\"pl-k\">=</span> rnn.LayerNormBasicLSTMCell(num_hidden, <span class=\"pl-v\">forget_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get lstm cell output</span>\n    outputs, states <span class=\"pl-k\">=</span> rnn.static_rnn(lstm_cell, x, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Linear activation, using rnn inner loop last output</span>\n    <span class=\"pl-k\">return</span> tf.matmul(outputs[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], weights[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>out<span class=\"pl-pds\">'</span></span>]) <span class=\"pl-k\">+</span> biases[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>out<span class=\"pl-pds\">'</span></span>]\n\nlogits <span class=\"pl-k\">=</span> RNN(X, weights, biases)\nprediction <span class=\"pl-k\">=</span> tf.nn.softmax(logits)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Define loss and optimizer</span>\nloss_op <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>Y))\noptimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span>learning_rate)\ntrain_op <span class=\"pl-k\">=</span> optimizer.minimize(loss_op)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Evaluate model (with test logits, for dropout to be disabled)</span>\ncorrect_pred <span class=\"pl-k\">=</span> tf.equal(tf.argmax(prediction, <span class=\"pl-c1\">1</span>), tf.argmax(Y, <span class=\"pl-c1\">1</span>))\naccuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialize the variables (i.e. assign their default value)</span>\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Start training</span>\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the initializer</span>\n    sess.run(init)\n\n    <span class=\"pl-k\">for</span> step <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>, training_steps<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>):\n        batch_x, batch_y <span class=\"pl-k\">=</span> mnist.train.next_batch(batch_size)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Reshape data to get 28 seq of 28 elements</span>\n        batch_x <span class=\"pl-k\">=</span> batch_x.reshape((batch_size, timesteps, num_input))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Run optimization op (backprop)</span>\n        sess.run(train_op, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{X: batch_x, Y: batch_y})\n        <span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> display_step <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">or</span> step <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Calculate batch loss and accuracy</span>\n            loss, acc <span class=\"pl-k\">=</span> sess.run([loss_op, accuracy], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{X: batch_x,\n                                                                 Y: batch_y})\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Step <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(step) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>, Minibatch Loss= <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> \\\n                  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{<span class=\"pl-k\">:.4f</span>}</span><span class=\"pl-pds\">\"</span></span>.format(loss) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>, Training Accuracy= <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> \\\n                  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{<span class=\"pl-k\">:.3f</span>}</span><span class=\"pl-pds\">\"</span></span>.format(acc))\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Optimization Finished!<span class=\"pl-pds\">\"</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Calculate accuracy for 128 mnist test images</span>\n    test_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\n    test_data <span class=\"pl-k\">=</span> mnist.test.images[:test_len].reshape((<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, timesteps, num_input))\n    test_label <span class=\"pl-k\">=</span> mnist.test.labels[:test_len]\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Testing Accuracy:<span class=\"pl-pds\">\"</span></span>, \\\n        sess.run(accuracy, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{X: test_data, Y: test_label}))<span class=\"pl-bu\">```</span></pre></div>", "body_text": "here is the full code\n\"\"\" Recurrent Neural Network.\n\nA Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n\nLinks:\n    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\"\"\"\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\n'''\nTo classify images using a recurrent neural network, we consider every image\nrow as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\nhandle 28 sequences of 28 steps for every sample.\n'''\n\n# Training Parameters\nlearning_rate = 0.001\ntraining_steps = 10000\nbatch_size = 128\ndisplay_step = 200\n\n# Network Parameters\nnum_input = 28 # MNIST data input (img shape: 28*28)\ntimesteps = 28 # timesteps\nnum_hidden = 128 # hidden layer num of features\nnum_classes = 10 # MNIST total classes (0-9 digits)\n\n# tf Graph input\nX = tf.placeholder(\"float\", [None, timesteps, num_input])\nY = tf.placeholder(\"float\", [None, num_classes])\n\n# Define weights\nweights = {\n    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([num_classes]))\n}\n\n\ndef RNN(x, weights, biases):\n\n    # Prepare data shape to match `rnn` function requirements\n    # Current data input shape: (batch_size, timesteps, n_input)\n    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n\n    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n    x = tf.unstack(x, timesteps, 1)\n\n    # Define a lstm cell with tensorflow\n    lstm_cell = rnn.LayerNormBasicLSTMCell(num_hidden, forget_bias=1.0)\n\n    # Get lstm cell output\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n\n    # Linear activation, using rnn inner loop last output\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n\nlogits = RNN(X, weights, biases)\nprediction = tf.nn.softmax(logits)\n\n# Define loss and optimizer\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=logits, labels=Y))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n\n# Evaluate model (with test logits, for dropout to be disabled)\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initialize the variables (i.e. assign their default value)\ninit = tf.global_variables_initializer()\n\n# Start training\nwith tf.Session() as sess:\n\n    # Run the initializer\n    sess.run(init)\n\n    for step in range(1, training_steps+1):\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        # Reshape data to get 28 seq of 28 elements\n        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n        # Run optimization op (backprop)\n        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n        if step % display_step == 0 or step == 1:\n            # Calculate batch loss and accuracy\n            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n                                                                 Y: batch_y})\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.3f}\".format(acc))\n\n    print(\"Optimization Finished!\")\n\n    # Calculate accuracy for 128 mnist test images\n    test_len = 128\n    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n    test_label = mnist.test.labels[:test_len]\n    print(\"Testing Accuracy:\", \\\n        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))```", "body": "here is the full code \r\n```python\r\n\"\"\" Recurrent Neural Network.\r\n\r\nA Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\r\nThis example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\r\n\r\nLinks:\r\n    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\r\n    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\r\n\r\nAuthor: Aymeric Damien\r\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\r\n\"\"\"\r\n\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import rnn\r\n\r\n# Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n\r\n'''\r\nTo classify images using a recurrent neural network, we consider every image\r\nrow as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\r\nhandle 28 sequences of 28 steps for every sample.\r\n'''\r\n\r\n# Training Parameters\r\nlearning_rate = 0.001\r\ntraining_steps = 10000\r\nbatch_size = 128\r\ndisplay_step = 200\r\n\r\n# Network Parameters\r\nnum_input = 28 # MNIST data input (img shape: 28*28)\r\ntimesteps = 28 # timesteps\r\nnum_hidden = 128 # hidden layer num of features\r\nnum_classes = 10 # MNIST total classes (0-9 digits)\r\n\r\n# tf Graph input\r\nX = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY = tf.placeholder(\"float\", [None, num_classes])\r\n\r\n# Define weights\r\nweights = {\r\n    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\r\n}\r\nbiases = {\r\n    'out': tf.Variable(tf.random_normal([num_classes]))\r\n}\r\n\r\n\r\ndef RNN(x, weights, biases):\r\n\r\n    # Prepare data shape to match `rnn` function requirements\r\n    # Current data input shape: (batch_size, timesteps, n_input)\r\n    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\r\n\r\n    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\r\n    x = tf.unstack(x, timesteps, 1)\r\n\r\n    # Define a lstm cell with tensorflow\r\n    lstm_cell = rnn.LayerNormBasicLSTMCell(num_hidden, forget_bias=1.0)\r\n\r\n    # Get lstm cell output\r\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\r\n\r\n    # Linear activation, using rnn inner loop last output\r\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\r\n\r\nlogits = RNN(X, weights, biases)\r\nprediction = tf.nn.softmax(logits)\r\n\r\n# Define loss and optimizer\r\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\r\n    logits=logits, labels=Y))\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\r\ntrain_op = optimizer.minimize(loss_op)\r\n\r\n# Evaluate model (with test logits, for dropout to be disabled)\r\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n# Initialize the variables (i.e. assign their default value)\r\ninit = tf.global_variables_initializer()\r\n\r\n# Start training\r\nwith tf.Session() as sess:\r\n\r\n    # Run the initializer\r\n    sess.run(init)\r\n\r\n    for step in range(1, training_steps+1):\r\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\r\n        # Reshape data to get 28 seq of 28 elements\r\n        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\r\n        # Run optimization op (backprop)\r\n        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\r\n        if step % display_step == 0 or step == 1:\r\n            # Calculate batch loss and accuracy\r\n            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\r\n                                                                 Y: batch_y})\r\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\r\n                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\r\n                  \"{:.3f}\".format(acc))\r\n\r\n    print(\"Optimization Finished!\")\r\n\r\n    # Calculate accuracy for 128 mnist test images\r\n    test_len = 128\r\n    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\r\n    test_label = mnist.test.labels[:test_len]\r\n    print(\"Testing Accuracy:\", \\\r\n        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))```"}