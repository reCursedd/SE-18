{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/418063498", "html_url": "https://github.com/tensorflow/tensorflow/issues/21929#issuecomment-418063498", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21929", "id": 418063498, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODA2MzQ5OA==", "user": {"login": "AzizCode92", "id": 19540527, "node_id": "MDQ6VXNlcjE5NTQwNTI3", "avatar_url": "https://avatars2.githubusercontent.com/u/19540527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AzizCode92", "html_url": "https://github.com/AzizCode92", "followers_url": "https://api.github.com/users/AzizCode92/followers", "following_url": "https://api.github.com/users/AzizCode92/following{/other_user}", "gists_url": "https://api.github.com/users/AzizCode92/gists{/gist_id}", "starred_url": "https://api.github.com/users/AzizCode92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AzizCode92/subscriptions", "organizations_url": "https://api.github.com/users/AzizCode92/orgs", "repos_url": "https://api.github.com/users/AzizCode92/repos", "events_url": "https://api.github.com/users/AzizCode92/events{/privacy}", "received_events_url": "https://api.github.com/users/AzizCode92/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-03T09:52:09Z", "updated_at": "2018-09-04T13:26:28Z", "author_association": "NONE", "body_html": "<p>Here I added some results of the experiment I have done using the MNIST dataset.</p>\n<pre><code>   def RNN(x,weights,biaises):\n        x = tf.unstack(x,timesteps,1)\n        lstm_cell = rnn.LayerNormBasicLSTMCell(num_hidden, forget_bias=1.0,layer_norm=True)\n        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n</code></pre>\n<p>The output of the model was the following.</p>\n<blockquote>\n<p>Step 1, Minibatch Loss= nan, Training Accuracy= 0.070<br>\nStep 200, Minibatch Loss= nan, Training Accuracy= 0.117<br>\nStep 400, Minibatch Loss= nan, Training Accuracy= 0.195<br>\nStep 600, Minibatch Loss= nan, Training Accuracy= 0.117<br>\nStep 800, Minibatch Loss= nan, Training Accuracy= 0.086<br>\nStep 1000, Minibatch Loss= nan, Training Accuracy= 0.102<br>\nStep 1200, Minibatch Loss= nan, Training Accuracy= 0.109<br>\nStep 1400, Minibatch Loss= nan, Training Accuracy= 0.148<br>\nStep 1600, Minibatch Loss= nan, Training Accuracy= 0.133<br>\nStep 1800, Minibatch Loss= nan, Training Accuracy= 0.102<br>\nStep 2000, Minibatch Loss= nan, Training Accuracy= 0.086<br>\nStep 2200, Minibatch Loss= nan, Training Accuracy= 0.094<br>\nStep 2400, Minibatch Loss= nan, Training Accuracy= 0.062<br>\nStep 2600, Minibatch Loss= nan, Training Accuracy= 0.094<br>\nStep 2800, Minibatch Loss= nan, Training Accuracy= 0.094<br>\nStep 3000, Minibatch Loss= nan, Training Accuracy= 0.109<br>\nStep 3200, Minibatch Loss= nan, Training Accuracy= 0.086<br>\nStep 3400, Minibatch Loss= nan, Training Accuracy= 0.109<br>\nStep 3600, Minibatch Loss= nan, Training Accuracy= 0.148<br>\nStep 3800, Minibatch Loss= nan, Training Accuracy= 0.102<br>\nStep 4000, Minibatch Loss= nan, Training Accuracy= 0.117<br>\nStep 4200, Minibatch Loss= nan, Training Accuracy= 0.078<br>\nStep 4400, Minibatch Loss= nan, Training Accuracy= 0.102<br>\nStep 4600, Minibatch Loss= nan, Training Accuracy= 0.062<br>\nStep 4800, Minibatch Loss= nan, Training Accuracy= 0.078<br>\nStep 5000, Minibatch Loss= nan, Training Accuracy= 0.070<br>\nStep 5200, Minibatch Loss= nan, Training Accuracy= 0.062<br>\nStep 5400, Minibatch Loss= nan, Training Accuracy= 0.125<br>\nStep 5600, Minibatch Loss= nan, Training Accuracy= 0.109<br>\nStep 5800, Minibatch Loss= nan, Training Accuracy= 0.125<br>\nStep 6000, Minibatch Loss= nan, Training Accuracy= 0.062<br>\nStep 6200, Minibatch Loss= nan, Training Accuracy= 0.102<br>\nStep 6400, Minibatch Loss= nan, Training Accuracy= 0.094<br>\nStep 6600, Minibatch Loss= nan, Training Accuracy= 0.094<br>\nStep 6800, Minibatch Loss= nan, Training Accuracy= 0.102<br>\nStep 7000, Minibatch Loss= nan, Training Accuracy= 0.078<br>\nStep 7200, Minibatch Loss= nan, Training Accuracy= 0.148<br>\nStep 7400, Minibatch Loss= nan, Training Accuracy= 0.117<br>\nStep 7600, Minibatch Loss= nan, Training Accuracy= 0.125<br>\nStep 7800, Minibatch Loss= nan, Training Accuracy= 0.117<br>\nStep 8000, Minibatch Loss= nan, Training Accuracy= 0.102<br>\nStep 8200, Minibatch Loss= nan, Training Accuracy= 0.125<br>\nStep 8400, Minibatch Loss= nan, Training Accuracy= 0.062<br>\nStep 8600, Minibatch Loss= nan, Training Accuracy= 0.055<br>\nStep 8800, Minibatch Loss= nan, Training Accuracy= 0.094<br>\nStep 9000, Minibatch Loss= nan, Training Accuracy= 0.172<br>\nStep 9200, Minibatch Loss= nan, Training Accuracy= 0.078<br>\nStep 9400, Minibatch Loss= nan, Training Accuracy= 0.148<br>\nStep 9600, Minibatch Loss= nan, Training Accuracy= 0.086<br>\nStep 9800, Minibatch Loss= nan, Training Accuracy= 0.094<br>\nStep 10000, Minibatch Loss= nan, Training Accuracy= 0.078<br>\nOptimization Finished!<br>\nelapsed time is 6732.706034<br>\nTesting Accuracy: 0.078125</p>\n</blockquote>\n<p>Now working with the BasicLSTM implementation :</p>\n<pre><code> def RNN(x,weights,biaises):\n        x = tf.unstack(x,timesteps,1)\n        lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n</code></pre>\n<p>The results were like the following:</p>\n<blockquote>\n<p>Step 1, Minibatch Loss= 2.5853, Training Accuracy= 0.055<br>\nStep 200, Minibatch Loss= 2.0630, Training Accuracy= 0.344<br>\nStep 400, Minibatch Loss= 1.9385, Training Accuracy= 0.359<br>\nStep 600, Minibatch Loss= 1.8311, Training Accuracy= 0.398<br>\nStep 800, Minibatch Loss= 1.7100, Training Accuracy= 0.492<br>\nStep 1000, Minibatch Loss= 1.6623, Training Accuracy= 0.398<br>\nStep 1200, Minibatch Loss= 1.4859, Training Accuracy= 0.469<br>\nStep 1400, Minibatch Loss= 1.3768, Training Accuracy= 0.555<br>\nStep 1600, Minibatch Loss= 1.2733, Training Accuracy= 0.609<br>\nStep 1800, Minibatch Loss= 1.1450, Training Accuracy= 0.633<br>\nStep 2000, Minibatch Loss= 1.2749, Training Accuracy= 0.562<br>\nStep 2200, Minibatch Loss= 1.2194, Training Accuracy= 0.633<br>\nStep 2400, Minibatch Loss= 1.2644, Training Accuracy= 0.586<br>\nStep 2600, Minibatch Loss= 1.1235, Training Accuracy= 0.664<br>\nStep 2800, Minibatch Loss= 1.0690, Training Accuracy= 0.688<br>\nStep 3000, Minibatch Loss= 0.9766, Training Accuracy= 0.664<br>\nStep 3200, Minibatch Loss= 0.9989, Training Accuracy= 0.648<br>\nStep 3400, Minibatch Loss= 1.0919, Training Accuracy= 0.648<br>\nStep 3600, Minibatch Loss= 0.9082, Training Accuracy= 0.711<br>\nStep 3800, Minibatch Loss= 0.9113, Training Accuracy= 0.727<br>\nStep 4000, Minibatch Loss= 0.9754, Training Accuracy= 0.703<br>\nStep 4200, Minibatch Loss= 0.9138, Training Accuracy= 0.719<br>\nStep 4400, Minibatch Loss= 0.9403, Training Accuracy= 0.688<br>\nStep 4600, Minibatch Loss= 0.7376, Training Accuracy= 0.797<br>\nStep 4800, Minibatch Loss= 0.8453, Training Accuracy= 0.688<br>\nStep 5000, Minibatch Loss= 0.8530, Training Accuracy= 0.781<br>\nStep 5200, Minibatch Loss= 0.6303, Training Accuracy= 0.797<br>\nStep 5400, Minibatch Loss= 0.7195, Training Accuracy= 0.781<br>\nStep 5600, Minibatch Loss= 0.6891, Training Accuracy= 0.805<br>\nStep 5800, Minibatch Loss= 0.6978, Training Accuracy= 0.805<br>\nStep 6000, Minibatch Loss= 0.7251, Training Accuracy= 0.766<br>\nStep 6200, Minibatch Loss= 0.6651, Training Accuracy= 0.773<br>\nStep 6400, Minibatch Loss= 0.6404, Training Accuracy= 0.789<br>\nStep 6600, Minibatch Loss= 0.6093, Training Accuracy= 0.805<br>\nStep 6800, Minibatch Loss= 0.5060, Training Accuracy= 0.844<br>\nStep 7000, Minibatch Loss= 0.6833, Training Accuracy= 0.805<br>\nStep 7200, Minibatch Loss= 0.5854, Training Accuracy= 0.844<br>\nStep 7400, Minibatch Loss= 0.4906, Training Accuracy= 0.836<br>\nStep 7600, Minibatch Loss= 0.5958, Training Accuracy= 0.820<br>\nStep 7800, Minibatch Loss= 0.4900, Training Accuracy= 0.828<br>\nStep 8000, Minibatch Loss= 0.4880, Training Accuracy= 0.867<br>\nStep 8200, Minibatch Loss= 0.4877, Training Accuracy= 0.836<br>\nStep 8400, Minibatch Loss= 0.4955, Training Accuracy= 0.867<br>\nStep 8600, Minibatch Loss= 0.5071, Training Accuracy= 0.820<br>\nStep 8800, Minibatch Loss= 0.5158, Training Accuracy= 0.805<br>\nStep 9000, Minibatch Loss= 0.3802, Training Accuracy= 0.898<br>\nStep 9200, Minibatch Loss= 0.5845, Training Accuracy= 0.859<br>\nStep 9400, Minibatch Loss= 0.3615, Training Accuracy= 0.898<br>\nStep 9600, Minibatch Loss= 0.5044, Training Accuracy= 0.867<br>\nStep 9800, Minibatch Loss= 0.4280, Training Accuracy= 0.883<br>\nStep 10000, Minibatch Loss= 0.4171, Training Accuracy= 0.875<br>\nOptimization Finished!<br>\nelapsed time is 0.03865504264831543<br>\nTesting Accuracy: 0.8828125</p>\n</blockquote>", "body_text": "Here I added some results of the experiment I have done using the MNIST dataset.\n   def RNN(x,weights,biaises):\n        x = tf.unstack(x,timesteps,1)\n        lstm_cell = rnn.LayerNormBasicLSTMCell(num_hidden, forget_bias=1.0,layer_norm=True)\n        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n\nThe output of the model was the following.\n\nStep 1, Minibatch Loss= nan, Training Accuracy= 0.070\nStep 200, Minibatch Loss= nan, Training Accuracy= 0.117\nStep 400, Minibatch Loss= nan, Training Accuracy= 0.195\nStep 600, Minibatch Loss= nan, Training Accuracy= 0.117\nStep 800, Minibatch Loss= nan, Training Accuracy= 0.086\nStep 1000, Minibatch Loss= nan, Training Accuracy= 0.102\nStep 1200, Minibatch Loss= nan, Training Accuracy= 0.109\nStep 1400, Minibatch Loss= nan, Training Accuracy= 0.148\nStep 1600, Minibatch Loss= nan, Training Accuracy= 0.133\nStep 1800, Minibatch Loss= nan, Training Accuracy= 0.102\nStep 2000, Minibatch Loss= nan, Training Accuracy= 0.086\nStep 2200, Minibatch Loss= nan, Training Accuracy= 0.094\nStep 2400, Minibatch Loss= nan, Training Accuracy= 0.062\nStep 2600, Minibatch Loss= nan, Training Accuracy= 0.094\nStep 2800, Minibatch Loss= nan, Training Accuracy= 0.094\nStep 3000, Minibatch Loss= nan, Training Accuracy= 0.109\nStep 3200, Minibatch Loss= nan, Training Accuracy= 0.086\nStep 3400, Minibatch Loss= nan, Training Accuracy= 0.109\nStep 3600, Minibatch Loss= nan, Training Accuracy= 0.148\nStep 3800, Minibatch Loss= nan, Training Accuracy= 0.102\nStep 4000, Minibatch Loss= nan, Training Accuracy= 0.117\nStep 4200, Minibatch Loss= nan, Training Accuracy= 0.078\nStep 4400, Minibatch Loss= nan, Training Accuracy= 0.102\nStep 4600, Minibatch Loss= nan, Training Accuracy= 0.062\nStep 4800, Minibatch Loss= nan, Training Accuracy= 0.078\nStep 5000, Minibatch Loss= nan, Training Accuracy= 0.070\nStep 5200, Minibatch Loss= nan, Training Accuracy= 0.062\nStep 5400, Minibatch Loss= nan, Training Accuracy= 0.125\nStep 5600, Minibatch Loss= nan, Training Accuracy= 0.109\nStep 5800, Minibatch Loss= nan, Training Accuracy= 0.125\nStep 6000, Minibatch Loss= nan, Training Accuracy= 0.062\nStep 6200, Minibatch Loss= nan, Training Accuracy= 0.102\nStep 6400, Minibatch Loss= nan, Training Accuracy= 0.094\nStep 6600, Minibatch Loss= nan, Training Accuracy= 0.094\nStep 6800, Minibatch Loss= nan, Training Accuracy= 0.102\nStep 7000, Minibatch Loss= nan, Training Accuracy= 0.078\nStep 7200, Minibatch Loss= nan, Training Accuracy= 0.148\nStep 7400, Minibatch Loss= nan, Training Accuracy= 0.117\nStep 7600, Minibatch Loss= nan, Training Accuracy= 0.125\nStep 7800, Minibatch Loss= nan, Training Accuracy= 0.117\nStep 8000, Minibatch Loss= nan, Training Accuracy= 0.102\nStep 8200, Minibatch Loss= nan, Training Accuracy= 0.125\nStep 8400, Minibatch Loss= nan, Training Accuracy= 0.062\nStep 8600, Minibatch Loss= nan, Training Accuracy= 0.055\nStep 8800, Minibatch Loss= nan, Training Accuracy= 0.094\nStep 9000, Minibatch Loss= nan, Training Accuracy= 0.172\nStep 9200, Minibatch Loss= nan, Training Accuracy= 0.078\nStep 9400, Minibatch Loss= nan, Training Accuracy= 0.148\nStep 9600, Minibatch Loss= nan, Training Accuracy= 0.086\nStep 9800, Minibatch Loss= nan, Training Accuracy= 0.094\nStep 10000, Minibatch Loss= nan, Training Accuracy= 0.078\nOptimization Finished!\nelapsed time is 6732.706034\nTesting Accuracy: 0.078125\n\nNow working with the BasicLSTM implementation :\n def RNN(x,weights,biaises):\n        x = tf.unstack(x,timesteps,1)\n        lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n\nThe results were like the following:\n\nStep 1, Minibatch Loss= 2.5853, Training Accuracy= 0.055\nStep 200, Minibatch Loss= 2.0630, Training Accuracy= 0.344\nStep 400, Minibatch Loss= 1.9385, Training Accuracy= 0.359\nStep 600, Minibatch Loss= 1.8311, Training Accuracy= 0.398\nStep 800, Minibatch Loss= 1.7100, Training Accuracy= 0.492\nStep 1000, Minibatch Loss= 1.6623, Training Accuracy= 0.398\nStep 1200, Minibatch Loss= 1.4859, Training Accuracy= 0.469\nStep 1400, Minibatch Loss= 1.3768, Training Accuracy= 0.555\nStep 1600, Minibatch Loss= 1.2733, Training Accuracy= 0.609\nStep 1800, Minibatch Loss= 1.1450, Training Accuracy= 0.633\nStep 2000, Minibatch Loss= 1.2749, Training Accuracy= 0.562\nStep 2200, Minibatch Loss= 1.2194, Training Accuracy= 0.633\nStep 2400, Minibatch Loss= 1.2644, Training Accuracy= 0.586\nStep 2600, Minibatch Loss= 1.1235, Training Accuracy= 0.664\nStep 2800, Minibatch Loss= 1.0690, Training Accuracy= 0.688\nStep 3000, Minibatch Loss= 0.9766, Training Accuracy= 0.664\nStep 3200, Minibatch Loss= 0.9989, Training Accuracy= 0.648\nStep 3400, Minibatch Loss= 1.0919, Training Accuracy= 0.648\nStep 3600, Minibatch Loss= 0.9082, Training Accuracy= 0.711\nStep 3800, Minibatch Loss= 0.9113, Training Accuracy= 0.727\nStep 4000, Minibatch Loss= 0.9754, Training Accuracy= 0.703\nStep 4200, Minibatch Loss= 0.9138, Training Accuracy= 0.719\nStep 4400, Minibatch Loss= 0.9403, Training Accuracy= 0.688\nStep 4600, Minibatch Loss= 0.7376, Training Accuracy= 0.797\nStep 4800, Minibatch Loss= 0.8453, Training Accuracy= 0.688\nStep 5000, Minibatch Loss= 0.8530, Training Accuracy= 0.781\nStep 5200, Minibatch Loss= 0.6303, Training Accuracy= 0.797\nStep 5400, Minibatch Loss= 0.7195, Training Accuracy= 0.781\nStep 5600, Minibatch Loss= 0.6891, Training Accuracy= 0.805\nStep 5800, Minibatch Loss= 0.6978, Training Accuracy= 0.805\nStep 6000, Minibatch Loss= 0.7251, Training Accuracy= 0.766\nStep 6200, Minibatch Loss= 0.6651, Training Accuracy= 0.773\nStep 6400, Minibatch Loss= 0.6404, Training Accuracy= 0.789\nStep 6600, Minibatch Loss= 0.6093, Training Accuracy= 0.805\nStep 6800, Minibatch Loss= 0.5060, Training Accuracy= 0.844\nStep 7000, Minibatch Loss= 0.6833, Training Accuracy= 0.805\nStep 7200, Minibatch Loss= 0.5854, Training Accuracy= 0.844\nStep 7400, Minibatch Loss= 0.4906, Training Accuracy= 0.836\nStep 7600, Minibatch Loss= 0.5958, Training Accuracy= 0.820\nStep 7800, Minibatch Loss= 0.4900, Training Accuracy= 0.828\nStep 8000, Minibatch Loss= 0.4880, Training Accuracy= 0.867\nStep 8200, Minibatch Loss= 0.4877, Training Accuracy= 0.836\nStep 8400, Minibatch Loss= 0.4955, Training Accuracy= 0.867\nStep 8600, Minibatch Loss= 0.5071, Training Accuracy= 0.820\nStep 8800, Minibatch Loss= 0.5158, Training Accuracy= 0.805\nStep 9000, Minibatch Loss= 0.3802, Training Accuracy= 0.898\nStep 9200, Minibatch Loss= 0.5845, Training Accuracy= 0.859\nStep 9400, Minibatch Loss= 0.3615, Training Accuracy= 0.898\nStep 9600, Minibatch Loss= 0.5044, Training Accuracy= 0.867\nStep 9800, Minibatch Loss= 0.4280, Training Accuracy= 0.883\nStep 10000, Minibatch Loss= 0.4171, Training Accuracy= 0.875\nOptimization Finished!\nelapsed time is 0.03865504264831543\nTesting Accuracy: 0.8828125", "body": "Here I added some results of the experiment I have done using the MNIST dataset.\r\n\r\n\r\n\r\n\r\n\r\n       def RNN(x,weights,biaises):\r\n            x = tf.unstack(x,timesteps,1)\r\n            lstm_cell = rnn.LayerNormBasicLSTMCell(num_hidden, forget_bias=1.0,layer_norm=True)\r\n            outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\r\n            return tf.matmul(outputs[-1], weights['out']) + biases['out']\r\n\r\n\r\n\r\n\r\nThe output of the model was the following.   \r\n\r\n\r\n> Step 1, Minibatch Loss= nan, Training Accuracy= 0.070\r\nStep 200, Minibatch Loss= nan, Training Accuracy= 0.117\r\nStep 400, Minibatch Loss= nan, Training Accuracy= 0.195\r\nStep 600, Minibatch Loss= nan, Training Accuracy= 0.117\r\nStep 800, Minibatch Loss= nan, Training Accuracy= 0.086\r\nStep 1000, Minibatch Loss= nan, Training Accuracy= 0.102\r\nStep 1200, Minibatch Loss= nan, Training Accuracy= 0.109\r\nStep 1400, Minibatch Loss= nan, Training Accuracy= 0.148\r\nStep 1600, Minibatch Loss= nan, Training Accuracy= 0.133\r\nStep 1800, Minibatch Loss= nan, Training Accuracy= 0.102\r\nStep 2000, Minibatch Loss= nan, Training Accuracy= 0.086\r\nStep 2200, Minibatch Loss= nan, Training Accuracy= 0.094\r\nStep 2400, Minibatch Loss= nan, Training Accuracy= 0.062\r\nStep 2600, Minibatch Loss= nan, Training Accuracy= 0.094\r\nStep 2800, Minibatch Loss= nan, Training Accuracy= 0.094\r\nStep 3000, Minibatch Loss= nan, Training Accuracy= 0.109\r\nStep 3200, Minibatch Loss= nan, Training Accuracy= 0.086\r\nStep 3400, Minibatch Loss= nan, Training Accuracy= 0.109\r\nStep 3600, Minibatch Loss= nan, Training Accuracy= 0.148\r\nStep 3800, Minibatch Loss= nan, Training Accuracy= 0.102\r\nStep 4000, Minibatch Loss= nan, Training Accuracy= 0.117\r\nStep 4200, Minibatch Loss= nan, Training Accuracy= 0.078\r\nStep 4400, Minibatch Loss= nan, Training Accuracy= 0.102\r\nStep 4600, Minibatch Loss= nan, Training Accuracy= 0.062\r\nStep 4800, Minibatch Loss= nan, Training Accuracy= 0.078\r\nStep 5000, Minibatch Loss= nan, Training Accuracy= 0.070\r\nStep 5200, Minibatch Loss= nan, Training Accuracy= 0.062\r\nStep 5400, Minibatch Loss= nan, Training Accuracy= 0.125\r\nStep 5600, Minibatch Loss= nan, Training Accuracy= 0.109\r\nStep 5800, Minibatch Loss= nan, Training Accuracy= 0.125\r\nStep 6000, Minibatch Loss= nan, Training Accuracy= 0.062\r\nStep 6200, Minibatch Loss= nan, Training Accuracy= 0.102\r\nStep 6400, Minibatch Loss= nan, Training Accuracy= 0.094\r\nStep 6600, Minibatch Loss= nan, Training Accuracy= 0.094\r\nStep 6800, Minibatch Loss= nan, Training Accuracy= 0.102\r\nStep 7000, Minibatch Loss= nan, Training Accuracy= 0.078\r\nStep 7200, Minibatch Loss= nan, Training Accuracy= 0.148\r\nStep 7400, Minibatch Loss= nan, Training Accuracy= 0.117\r\nStep 7600, Minibatch Loss= nan, Training Accuracy= 0.125\r\nStep 7800, Minibatch Loss= nan, Training Accuracy= 0.117\r\nStep 8000, Minibatch Loss= nan, Training Accuracy= 0.102\r\nStep 8200, Minibatch Loss= nan, Training Accuracy= 0.125\r\nStep 8400, Minibatch Loss= nan, Training Accuracy= 0.062\r\nStep 8600, Minibatch Loss= nan, Training Accuracy= 0.055\r\nStep 8800, Minibatch Loss= nan, Training Accuracy= 0.094\r\nStep 9000, Minibatch Loss= nan, Training Accuracy= 0.172\r\nStep 9200, Minibatch Loss= nan, Training Accuracy= 0.078\r\nStep 9400, Minibatch Loss= nan, Training Accuracy= 0.148\r\nStep 9600, Minibatch Loss= nan, Training Accuracy= 0.086\r\nStep 9800, Minibatch Loss= nan, Training Accuracy= 0.094\r\nStep 10000, Minibatch Loss= nan, Training Accuracy= 0.078\r\nOptimization Finished!\r\nelapsed time is 6732.706034\r\nTesting Accuracy: 0.078125\r\n\r\nNow working with the BasicLSTM implementation : \r\n\r\n     def RNN(x,weights,biaises):\r\n            x = tf.unstack(x,timesteps,1)\r\n            lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\r\n            outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\r\n            return tf.matmul(outputs[-1], weights['out']) + biases['out']\r\n\r\n\r\nThe results were like the following:\r\n \r\n\r\n> Step 1, Minibatch Loss= 2.5853, Training Accuracy= 0.055\r\n> Step 200, Minibatch Loss= 2.0630, Training Accuracy= 0.344\r\n> Step 400, Minibatch Loss= 1.9385, Training Accuracy= 0.359\r\n> Step 600, Minibatch Loss= 1.8311, Training Accuracy= 0.398\r\n> Step 800, Minibatch Loss= 1.7100, Training Accuracy= 0.492\r\n> Step 1000, Minibatch Loss= 1.6623, Training Accuracy= 0.398\r\n> Step 1200, Minibatch Loss= 1.4859, Training Accuracy= 0.469\r\n> Step 1400, Minibatch Loss= 1.3768, Training Accuracy= 0.555\r\n> Step 1600, Minibatch Loss= 1.2733, Training Accuracy= 0.609\r\n> Step 1800, Minibatch Loss= 1.1450, Training Accuracy= 0.633\r\n> Step 2000, Minibatch Loss= 1.2749, Training Accuracy= 0.562\r\n> Step 2200, Minibatch Loss= 1.2194, Training Accuracy= 0.633\r\n> Step 2400, Minibatch Loss= 1.2644, Training Accuracy= 0.586\r\n> Step 2600, Minibatch Loss= 1.1235, Training Accuracy= 0.664\r\n> Step 2800, Minibatch Loss= 1.0690, Training Accuracy= 0.688\r\n> Step 3000, Minibatch Loss= 0.9766, Training Accuracy= 0.664\r\n> Step 3200, Minibatch Loss= 0.9989, Training Accuracy= 0.648\r\n> Step 3400, Minibatch Loss= 1.0919, Training Accuracy= 0.648\r\n> Step 3600, Minibatch Loss= 0.9082, Training Accuracy= 0.711\r\n> Step 3800, Minibatch Loss= 0.9113, Training Accuracy= 0.727\r\n> Step 4000, Minibatch Loss= 0.9754, Training Accuracy= 0.703\r\n> Step 4200, Minibatch Loss= 0.9138, Training Accuracy= 0.719\r\n> Step 4400, Minibatch Loss= 0.9403, Training Accuracy= 0.688\r\n> Step 4600, Minibatch Loss= 0.7376, Training Accuracy= 0.797\r\n> Step 4800, Minibatch Loss= 0.8453, Training Accuracy= 0.688\r\n> Step 5000, Minibatch Loss= 0.8530, Training Accuracy= 0.781\r\n> Step 5200, Minibatch Loss= 0.6303, Training Accuracy= 0.797\r\n> Step 5400, Minibatch Loss= 0.7195, Training Accuracy= 0.781\r\n> Step 5600, Minibatch Loss= 0.6891, Training Accuracy= 0.805\r\n> Step 5800, Minibatch Loss= 0.6978, Training Accuracy= 0.805\r\n> Step 6000, Minibatch Loss= 0.7251, Training Accuracy= 0.766\r\n> Step 6200, Minibatch Loss= 0.6651, Training Accuracy= 0.773\r\n> Step 6400, Minibatch Loss= 0.6404, Training Accuracy= 0.789\r\n> Step 6600, Minibatch Loss= 0.6093, Training Accuracy= 0.805\r\n> Step 6800, Minibatch Loss= 0.5060, Training Accuracy= 0.844\r\n> Step 7000, Minibatch Loss= 0.6833, Training Accuracy= 0.805\r\n> Step 7200, Minibatch Loss= 0.5854, Training Accuracy= 0.844\r\n> Step 7400, Minibatch Loss= 0.4906, Training Accuracy= 0.836\r\n> Step 7600, Minibatch Loss= 0.5958, Training Accuracy= 0.820\r\n> Step 7800, Minibatch Loss= 0.4900, Training Accuracy= 0.828\r\n> Step 8000, Minibatch Loss= 0.4880, Training Accuracy= 0.867\r\n> Step 8200, Minibatch Loss= 0.4877, Training Accuracy= 0.836\r\n> Step 8400, Minibatch Loss= 0.4955, Training Accuracy= 0.867\r\n> Step 8600, Minibatch Loss= 0.5071, Training Accuracy= 0.820\r\n> Step 8800, Minibatch Loss= 0.5158, Training Accuracy= 0.805\r\n> Step 9000, Minibatch Loss= 0.3802, Training Accuracy= 0.898\r\n> Step 9200, Minibatch Loss= 0.5845, Training Accuracy= 0.859\r\n> Step 9400, Minibatch Loss= 0.3615, Training Accuracy= 0.898\r\n> Step 9600, Minibatch Loss= 0.5044, Training Accuracy= 0.867\r\n> Step 9800, Minibatch Loss= 0.4280, Training Accuracy= 0.883\r\n> Step 10000, Minibatch Loss= 0.4171, Training Accuracy= 0.875\r\n> Optimization Finished!\r\n> elapsed time is 0.03865504264831543\r\n> Testing Accuracy: 0.8828125\r\n> \r\n\r\n\r\n"}