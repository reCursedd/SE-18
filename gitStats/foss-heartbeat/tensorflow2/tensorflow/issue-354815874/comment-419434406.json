{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/419434406", "html_url": "https://github.com/tensorflow/tensorflow/issues/21929#issuecomment-419434406", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21929", "id": 419434406, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTQzNDQwNg==", "user": {"login": "AzizCode92", "id": 19540527, "node_id": "MDQ6VXNlcjE5NTQwNTI3", "avatar_url": "https://avatars2.githubusercontent.com/u/19540527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AzizCode92", "html_url": "https://github.com/AzizCode92", "followers_url": "https://api.github.com/users/AzizCode92/followers", "following_url": "https://api.github.com/users/AzizCode92/following{/other_user}", "gists_url": "https://api.github.com/users/AzizCode92/gists{/gist_id}", "starred_url": "https://api.github.com/users/AzizCode92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AzizCode92/subscriptions", "organizations_url": "https://api.github.com/users/AzizCode92/orgs", "repos_url": "https://api.github.com/users/AzizCode92/repos", "events_url": "https://api.github.com/users/AzizCode92/events{/privacy}", "received_events_url": "https://api.github.com/users/AzizCode92/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-07T13:11:04Z", "updated_at": "2018-09-07T13:14:10Z", "author_association": "NONE", "body_html": "<p>This is a simple neural network with 5 layers on the MNIST dataset.<br>\nAt each layer I applied layer normalization using <code>tf.contrib.layers.layer_norm</code></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> tensorflow.examples.tutorials.mnist <span class=\"pl-k\">import</span> input_data\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nlearning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.003</span>\ntraining_epoch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\nmnist <span class=\"pl-k\">=</span> input_data.read_data_sets(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">one_hot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\nX_ <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">784</span>])\nY_ <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">10</span>])\n\nL <span class=\"pl-k\">=</span> <span class=\"pl-c1\">200</span>\nM <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nN <span class=\"pl-k\">=</span> <span class=\"pl-c1\">60</span>\nO <span class=\"pl-k\">=</span> <span class=\"pl-c1\">30</span>\n\nX <span class=\"pl-k\">=</span> tf.reshape(X_, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">784</span>])\nW1 <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([<span class=\"pl-c1\">784</span>, L], <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>))\nB1 <span class=\"pl-k\">=</span> tf.Variable(tf.ones([L])<span class=\"pl-k\">/</span><span class=\"pl-c1\">10</span>)\nW2 <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([L, M], <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>))\nB2 <span class=\"pl-k\">=</span> tf.Variable(tf.ones([M])<span class=\"pl-k\">/</span><span class=\"pl-c1\">10</span>)\nW3 <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([M, N], <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>))\nB3 <span class=\"pl-k\">=</span> tf.Variable(tf.ones([N])<span class=\"pl-k\">/</span><span class=\"pl-c1\">10</span>)\nW4 <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([N, O], <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>))\nB4 <span class=\"pl-k\">=</span> tf.Variable(tf.ones(O)<span class=\"pl-k\">/</span><span class=\"pl-c1\">10</span>)\nW5 <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([O, <span class=\"pl-c1\">10</span>], <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>))\nB5 <span class=\"pl-k\">=</span> tf.Variable(tf.zeros(<span class=\"pl-c1\">10</span>))\n\nY1 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(X, W1) <span class=\"pl-k\">+</span> B1)\nY1 <span class=\"pl-k\">=</span> tf.contrib.layers.layer_norm(Y1)\nY2 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(Y1, W2) <span class=\"pl-k\">+</span> B2)\nY2 <span class=\"pl-k\">=</span> tf.contrib.layers.layer_norm(Y2)\nY3 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(Y2, W3) <span class=\"pl-k\">+</span> B3)\nY3 <span class=\"pl-k\">=</span> tf.contrib.layers.layer_norm(Y3)\nY4 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(Y3, W4) <span class=\"pl-k\">+</span> B4)\nY4 <span class=\"pl-k\">=</span> tf.contrib.layers.layer_norm(Y4)\nYlogits <span class=\"pl-k\">=</span> tf.matmul(Y4, W5) <span class=\"pl-k\">+</span> B5\nY <span class=\"pl-k\">=</span> tf.nn.softmax(Ylogits)\n\n\ncross_entropy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>Y_, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>Ylogits))\ncorrect_prediction <span class=\"pl-k\">=</span> tf.equal(tf.argmax(Y, <span class=\"pl-c1\">1</span>), tf.argmax(Y_, <span class=\"pl-c1\">1</span>))\naccuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\ntrain_step <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n\tsess.run(tf.global_variables_initializer())\n\t<span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(training_epoch):\n\t\tbatch_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(mnist.train.num_examples<span class=\"pl-k\">/</span>batch_size)\n\t\t<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_count):\n\t\t\tbatch_x, batch_y <span class=\"pl-k\">=</span> mnist.train.next_batch(batch_size)\n\t\t\tsess.run([train_step], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span> {X: batch_x, Y_: batch_y})\n\t\t<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Epoch: <span class=\"pl-pds\">\"</span></span>, epoch, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>, Accuracy: <span class=\"pl-pds\">\"</span></span>, accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{X: mnist.test.images, Y_: mnist.test.labels}))\n</pre></div>\n<p>and the results were like this</p>\n<pre><code>Epoch:  0 , Accuracy:  0.9589\nEpoch:  1 , Accuracy:  0.9631\nEpoch:  2 , Accuracy:  0.9721\nEpoch:  3 , Accuracy:  0.9659\nEpoch:  4 , Accuracy:  0.9718\nEpoch:  5 , Accuracy:  0.9729\nEpoch:  6 , Accuracy:  0.9729\nEpoch:  7 , Accuracy:  0.9779\nEpoch:  8 , Accuracy:  0.9796\nEpoch:  9 , Accuracy:  0.9763\n</code></pre>\n<p>so applying the  <code>tf.contrib.layers.layer_norm</code>  works fine but applying the <code>tf.contrib.rnn.LayerNormBasicLSTMCell</code> to the MNIST dataset didn't work and maybe this is due to the nature of the problem.</p>", "body_text": "This is a simple neural network with 5 layers on the MNIST dataset.\nAt each layer I applied layer normalization using tf.contrib.layers.layer_norm\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\nbatch_size = 100\nlearning_rate = 0.003\ntraining_epoch = 10\n\nmnist = input_data.read_data_sets('data', one_hot=True)\n\nX_ = tf.placeholder(tf.float32, [None, 784])\nY_ = tf.placeholder(tf.float32, [None, 10])\n\nL = 200\nM = 100\nN = 60\nO = 30\n\nX = tf.reshape(X_, [-1, 784])\nW1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))\nB1 = tf.Variable(tf.ones([L])/10)\nW2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\nB2 = tf.Variable(tf.ones([M])/10)\nW3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\nB3 = tf.Variable(tf.ones([N])/10)\nW4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\nB4 = tf.Variable(tf.ones(O)/10)\nW5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\nB5 = tf.Variable(tf.zeros(10))\n\nY1 = tf.nn.relu(tf.matmul(X, W1) + B1)\nY1 = tf.contrib.layers.layer_norm(Y1)\nY2 = tf.nn.relu(tf.matmul(Y1, W2) + B2)\nY2 = tf.contrib.layers.layer_norm(Y2)\nY3 = tf.nn.relu(tf.matmul(Y2, W3) + B3)\nY3 = tf.contrib.layers.layer_norm(Y3)\nY4 = tf.nn.relu(tf.matmul(Y3, W4) + B4)\nY4 = tf.contrib.layers.layer_norm(Y4)\nYlogits = tf.matmul(Y4, W5) + B5\nY = tf.nn.softmax(Ylogits)\n\n\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_, logits=Ylogits))\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\ntrain_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n\nwith tf.Session() as sess:\n\tsess.run(tf.global_variables_initializer())\n\tfor epoch in range(training_epoch):\n\t\tbatch_count = int(mnist.train.num_examples/batch_size)\n\t\tfor i in range(batch_count):\n\t\t\tbatch_x, batch_y = mnist.train.next_batch(batch_size)\n\t\t\tsess.run([train_step], feed_dict= {X: batch_x, Y_: batch_y})\n\t\tprint(\"Epoch: \", epoch, \", Accuracy: \", accuracy.eval(feed_dict={X: mnist.test.images, Y_: mnist.test.labels}))\n\nand the results were like this\nEpoch:  0 , Accuracy:  0.9589\nEpoch:  1 , Accuracy:  0.9631\nEpoch:  2 , Accuracy:  0.9721\nEpoch:  3 , Accuracy:  0.9659\nEpoch:  4 , Accuracy:  0.9718\nEpoch:  5 , Accuracy:  0.9729\nEpoch:  6 , Accuracy:  0.9729\nEpoch:  7 , Accuracy:  0.9779\nEpoch:  8 , Accuracy:  0.9796\nEpoch:  9 , Accuracy:  0.9763\n\nso applying the  tf.contrib.layers.layer_norm  works fine but applying the tf.contrib.rnn.LayerNormBasicLSTMCell to the MNIST dataset didn't work and maybe this is due to the nature of the problem.", "body": "This is a simple neural network with 5 layers on the MNIST dataset.\r\nAt each layer I applied layer normalization using `tf.contrib.layers.layer_norm`\r\n\r\n```python\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nimport tensorflow as tf\r\n\r\nbatch_size = 100\r\nlearning_rate = 0.003\r\ntraining_epoch = 10\r\n\r\nmnist = input_data.read_data_sets('data', one_hot=True)\r\n\r\nX_ = tf.placeholder(tf.float32, [None, 784])\r\nY_ = tf.placeholder(tf.float32, [None, 10])\r\n\r\nL = 200\r\nM = 100\r\nN = 60\r\nO = 30\r\n\r\nX = tf.reshape(X_, [-1, 784])\r\nW1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))\r\nB1 = tf.Variable(tf.ones([L])/10)\r\nW2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\r\nB2 = tf.Variable(tf.ones([M])/10)\r\nW3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\r\nB3 = tf.Variable(tf.ones([N])/10)\r\nW4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\r\nB4 = tf.Variable(tf.ones(O)/10)\r\nW5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\r\nB5 = tf.Variable(tf.zeros(10))\r\n\r\nY1 = tf.nn.relu(tf.matmul(X, W1) + B1)\r\nY1 = tf.contrib.layers.layer_norm(Y1)\r\nY2 = tf.nn.relu(tf.matmul(Y1, W2) + B2)\r\nY2 = tf.contrib.layers.layer_norm(Y2)\r\nY3 = tf.nn.relu(tf.matmul(Y2, W3) + B3)\r\nY3 = tf.contrib.layers.layer_norm(Y3)\r\nY4 = tf.nn.relu(tf.matmul(Y3, W4) + B4)\r\nY4 = tf.contrib.layers.layer_norm(Y4)\r\nYlogits = tf.matmul(Y4, W5) + B5\r\nY = tf.nn.softmax(Ylogits)\r\n\r\n\r\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_, logits=Ylogits))\r\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\ntrain_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\r\n\r\nwith tf.Session() as sess:\r\n\tsess.run(tf.global_variables_initializer())\r\n\tfor epoch in range(training_epoch):\r\n\t\tbatch_count = int(mnist.train.num_examples/batch_size)\r\n\t\tfor i in range(batch_count):\r\n\t\t\tbatch_x, batch_y = mnist.train.next_batch(batch_size)\r\n\t\t\tsess.run([train_step], feed_dict= {X: batch_x, Y_: batch_y})\r\n\t\tprint(\"Epoch: \", epoch, \", Accuracy: \", accuracy.eval(feed_dict={X: mnist.test.images, Y_: mnist.test.labels}))\r\n\r\n```\r\nand the results were like this \r\n\r\n```\r\nEpoch:  0 , Accuracy:  0.9589\r\nEpoch:  1 , Accuracy:  0.9631\r\nEpoch:  2 , Accuracy:  0.9721\r\nEpoch:  3 , Accuracy:  0.9659\r\nEpoch:  4 , Accuracy:  0.9718\r\nEpoch:  5 , Accuracy:  0.9729\r\nEpoch:  6 , Accuracy:  0.9729\r\nEpoch:  7 , Accuracy:  0.9779\r\nEpoch:  8 , Accuracy:  0.9796\r\nEpoch:  9 , Accuracy:  0.9763\r\n```\r\nso applying the  `tf.contrib.layers.layer_norm`  works fine but applying the `tf.contrib.rnn.LayerNormBasicLSTMCell` to the MNIST dataset didn't work and maybe this is due to the nature of the problem. "}