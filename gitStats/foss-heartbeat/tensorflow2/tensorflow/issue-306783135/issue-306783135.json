{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17855", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17855/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17855/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17855/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17855", "id": 306783135, "node_id": "MDU6SXNzdWUzMDY3ODMxMzU=", "number": 17855, "title": "why training and validation loss are always zero during the training phase of deep neural network?", "user": {"login": "karthick96", "id": 32822620, "node_id": "MDQ6VXNlcjMyODIyNjIw", "avatar_url": "https://avatars3.githubusercontent.com/u/32822620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karthick96", "html_url": "https://github.com/karthick96", "followers_url": "https://api.github.com/users/karthick96/followers", "following_url": "https://api.github.com/users/karthick96/following{/other_user}", "gists_url": "https://api.github.com/users/karthick96/gists{/gist_id}", "starred_url": "https://api.github.com/users/karthick96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karthick96/subscriptions", "organizations_url": "https://api.github.com/users/karthick96/orgs", "repos_url": "https://api.github.com/users/karthick96/repos", "events_url": "https://api.github.com/users/karthick96/events{/privacy}", "received_events_url": "https://api.github.com/users/karthick96/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-03-20T09:17:23Z", "updated_at": "2018-03-29T18:32:59Z", "closed_at": "2018-03-29T18:32:59Z", "author_association": "NONE", "body_html": "<p>I'm new to machine learning and I'm trying to learn more about it. I have been facing many problems doing my project as DEEP NEURAL NETWORK Classifier (classes 0,1).<br>\nI'm using windows 8.1, 4GB ram, python 3.6.4(pip installation), tensorflow cpu version v1.4 (pip installation).<br>\nAll other packages required for this code also installed through pip installation and have been imported.<br>\nDuring training, Train loss and validation loss are always zero. I don't know why?<br>\nIf anyone know the problem, please let me know asap!! My project deadline is very near!!</p>\n<p>Things I've tried:</p>\n<ol>\n<li>different values for hidden_units, no. of hidden layers, batch_size, learning_rate</li>\n<li>Equally distributed my sample inputs according to the class labels.</li>\n<li>I gave the same data to LDA (Linear Discriminant Analysis) and it's able to classify at the accuracy of 99.6%</li>\n<li>I gave the same data to bernoulliRBM and it classifies all samples as class '0'</li>\n</ol>\n<h1>here is my CODE</h1>\n<pre><code>def build_neural_network(hidden_units_1=8, hidden_units_2=16):\n\n    tf.reset_default_graph()\n    inputs = tf.placeholder(tf.float64, shape=[None, train_x.shape[1]])\n    labels = tf.placeholder(tf.float64, shape=[None, 1])\n    learning_rate = tf.placeholder(tf.float64)\n    is_training=tf.Variable(True,dtype=tf.bool)\n    initializer = tf.contrib.layers.xavier_initializer()\n    fc = tf.layers.dense(inputs, hidden_units_1, activation=None,kernel_initializer=initializer)\n    fc=tf.layers.batch_normalization(fc, training=is_training)\n    fc=tf.nn.relu(fc)\n    fc = tf.layers.dense(fc, hidden_units_2, activation=None,kernel_initializer=initializer)\n    fc=tf.layers.batch_normalization(fc, training=is_training)\n    fc=tf.nn.relu(fc)\n    logits = tf.layers.dense(fc, 1, activation=None)\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n    cost = tf.reduce_mean(cross_entropy)\n   \n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n         optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    \n    predicted = tf.nn.softmax(logits)\n    correct_pred = tf.equal(tf.round(predicted), labels)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))\n\n    export_nodes = ['inputs', 'labels', 'learning_rate','is_training', 'logits',\n                'cost', 'optimizer', 'predicted', 'accuracy']\n    Graph = namedtuple('Graph', export_nodes)\n    local_dict = locals()\n    graph = Graph(*[local_dict[each] for each in export_nodes])\n    \n    return graph\n\nmodel = build_neural_network()\n\nepochs = 10\ntrain_collect = 10\ntrain_print=train_collect*2\nlearning_rate_value = 0.01 \nbatch_size=100\nx_collect = []\ntrain_loss_collect = []\ntrain_acc_collect = []\nvalid_loss_collect = []\nvalid_acc_collect = []\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, \"./save.ckpt\")\n    iteration=0\n    for e in range(epochs):\n          for batch_x,batch_y in get_batch(train_x,train_y,batch_size):\n                iteration+=1\n                feed = {model.inputs: train_x,\n                            model.labels: train_y,\n                            model.learning_rate: learning_rate_value,\n                            model.is_training:True,\n                            }\n                train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], \n                        feed_dict=feed)\n                \n                if iteration % train_collect == 0:\n                    x_collect.append(e)\n                    train_loss_collect.append(train_loss)\n                    train_acc_collect.append(train_acc)\n                \n                    if iteration % train_print==0:\n                        print(\"Epoch: {}/{}\".format(e + 1, epochs),\n                        \"Train Loss: {:.4f}\".format(train_loss),\n                        \"Train Acc: {:.4f}\".format(train_acc))\n                    \n                    feed = {model.inputs: valid_x,\n                                model.labels: valid_y,\n                                model.is_training:False\n                                }\n                    val_loss, val_acc = sess.run([model.cost, model.accuracy], feed_dict=feed)\n                    valid_loss_collect.append(val_loss)\n                    valid_acc_collect.append(val_acc)\n                                   \n                    if iteration % train_print==0:\n                        print(\"Epoch: {}/{}\".format(e + 1, epochs),\n                       \"Validation Loss: {:.4f}\".format(val_loss),\n                       \"Validation Acc: {:.4f}\".format(val_acc))\n            \n\nsaver.save(sess, \"./save.ckpt\")\n</code></pre>\n<p>#here is the OUTPUT of training:</p>\n<pre><code> Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n ...\n</code></pre>\n<p>(I'm getting the same result as above till epoch 10/10)</p>", "body_text": "I'm new to machine learning and I'm trying to learn more about it. I have been facing many problems doing my project as DEEP NEURAL NETWORK Classifier (classes 0,1).\nI'm using windows 8.1, 4GB ram, python 3.6.4(pip installation), tensorflow cpu version v1.4 (pip installation).\nAll other packages required for this code also installed through pip installation and have been imported.\nDuring training, Train loss and validation loss are always zero. I don't know why?\nIf anyone know the problem, please let me know asap!! My project deadline is very near!!\nThings I've tried:\n\ndifferent values for hidden_units, no. of hidden layers, batch_size, learning_rate\nEqually distributed my sample inputs according to the class labels.\nI gave the same data to LDA (Linear Discriminant Analysis) and it's able to classify at the accuracy of 99.6%\nI gave the same data to bernoulliRBM and it classifies all samples as class '0'\n\nhere is my CODE\ndef build_neural_network(hidden_units_1=8, hidden_units_2=16):\n\n    tf.reset_default_graph()\n    inputs = tf.placeholder(tf.float64, shape=[None, train_x.shape[1]])\n    labels = tf.placeholder(tf.float64, shape=[None, 1])\n    learning_rate = tf.placeholder(tf.float64)\n    is_training=tf.Variable(True,dtype=tf.bool)\n    initializer = tf.contrib.layers.xavier_initializer()\n    fc = tf.layers.dense(inputs, hidden_units_1, activation=None,kernel_initializer=initializer)\n    fc=tf.layers.batch_normalization(fc, training=is_training)\n    fc=tf.nn.relu(fc)\n    fc = tf.layers.dense(fc, hidden_units_2, activation=None,kernel_initializer=initializer)\n    fc=tf.layers.batch_normalization(fc, training=is_training)\n    fc=tf.nn.relu(fc)\n    logits = tf.layers.dense(fc, 1, activation=None)\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n    cost = tf.reduce_mean(cross_entropy)\n   \n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n         optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    \n    predicted = tf.nn.softmax(logits)\n    correct_pred = tf.equal(tf.round(predicted), labels)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))\n\n    export_nodes = ['inputs', 'labels', 'learning_rate','is_training', 'logits',\n                'cost', 'optimizer', 'predicted', 'accuracy']\n    Graph = namedtuple('Graph', export_nodes)\n    local_dict = locals()\n    graph = Graph(*[local_dict[each] for each in export_nodes])\n    \n    return graph\n\nmodel = build_neural_network()\n\nepochs = 10\ntrain_collect = 10\ntrain_print=train_collect*2\nlearning_rate_value = 0.01 \nbatch_size=100\nx_collect = []\ntrain_loss_collect = []\ntrain_acc_collect = []\nvalid_loss_collect = []\nvalid_acc_collect = []\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, \"./save.ckpt\")\n    iteration=0\n    for e in range(epochs):\n          for batch_x,batch_y in get_batch(train_x,train_y,batch_size):\n                iteration+=1\n                feed = {model.inputs: train_x,\n                            model.labels: train_y,\n                            model.learning_rate: learning_rate_value,\n                            model.is_training:True,\n                            }\n                train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], \n                        feed_dict=feed)\n                \n                if iteration % train_collect == 0:\n                    x_collect.append(e)\n                    train_loss_collect.append(train_loss)\n                    train_acc_collect.append(train_acc)\n                \n                    if iteration % train_print==0:\n                        print(\"Epoch: {}/{}\".format(e + 1, epochs),\n                        \"Train Loss: {:.4f}\".format(train_loss),\n                        \"Train Acc: {:.4f}\".format(train_acc))\n                    \n                    feed = {model.inputs: valid_x,\n                                model.labels: valid_y,\n                                model.is_training:False\n                                }\n                    val_loss, val_acc = sess.run([model.cost, model.accuracy], feed_dict=feed)\n                    valid_loss_collect.append(val_loss)\n                    valid_acc_collect.append(val_acc)\n                                   \n                    if iteration % train_print==0:\n                        print(\"Epoch: {}/{}\".format(e + 1, epochs),\n                       \"Validation Loss: {:.4f}\".format(val_loss),\n                       \"Validation Acc: {:.4f}\".format(val_acc))\n            \n\nsaver.save(sess, \"./save.ckpt\")\n\n#here is the OUTPUT of training:\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\n Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\n ...\n\n(I'm getting the same result as above till epoch 10/10)", "body": "I'm new to machine learning and I'm trying to learn more about it. I have been facing many problems doing my project as DEEP NEURAL NETWORK Classifier (classes 0,1). \r\nI'm using windows 8.1, 4GB ram, python 3.6.4(pip installation), tensorflow cpu version v1.4 (pip installation).\r\nAll other packages required for this code also installed through pip installation and have been imported.\r\nDuring training, Train loss and validation loss are always zero. I don't know why?\r\nIf anyone know the problem, please let me know asap!! My project deadline is very near!!\r\n\r\nThings I've tried:\r\n1. different values for hidden_units, no. of hidden layers, batch_size, learning_rate \r\n2. Equally distributed my sample inputs according to the class labels. \r\n3. I gave the same data to LDA (Linear Discriminant Analysis) and it's able to classify at the accuracy of 99.6%\r\n4. I gave the same data to bernoulliRBM and it classifies all samples as class '0'\r\n\r\n# here is my CODE\r\n\r\n    def build_neural_network(hidden_units_1=8, hidden_units_2=16):\r\n   \r\n        tf.reset_default_graph()\r\n        inputs = tf.placeholder(tf.float64, shape=[None, train_x.shape[1]])\r\n        labels = tf.placeholder(tf.float64, shape=[None, 1])\r\n        learning_rate = tf.placeholder(tf.float64)\r\n        is_training=tf.Variable(True,dtype=tf.bool)\r\n        initializer = tf.contrib.layers.xavier_initializer()\r\n        fc = tf.layers.dense(inputs, hidden_units_1, activation=None,kernel_initializer=initializer)\r\n        fc=tf.layers.batch_normalization(fc, training=is_training)\r\n        fc=tf.nn.relu(fc)\r\n        fc = tf.layers.dense(fc, hidden_units_2, activation=None,kernel_initializer=initializer)\r\n        fc=tf.layers.batch_normalization(fc, training=is_training)\r\n        fc=tf.nn.relu(fc)\r\n        logits = tf.layers.dense(fc, 1, activation=None)\r\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\r\n        cost = tf.reduce_mean(cross_entropy)\r\n       \r\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n             optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n        \r\n        predicted = tf.nn.softmax(logits)\r\n        correct_pred = tf.equal(tf.round(predicted), labels)\r\n        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))\r\n\r\n        export_nodes = ['inputs', 'labels', 'learning_rate','is_training', 'logits',\r\n                    'cost', 'optimizer', 'predicted', 'accuracy']\r\n        Graph = namedtuple('Graph', export_nodes)\r\n        local_dict = locals()\r\n        graph = Graph(*[local_dict[each] for each in export_nodes])\r\n        \r\n        return graph\r\n   \r\n    model = build_neural_network()\r\n\r\n    epochs = 10\r\n    train_collect = 10\r\n    train_print=train_collect*2\r\n    learning_rate_value = 0.01 \r\n    batch_size=100\r\n    x_collect = []\r\n    train_loss_collect = []\r\n    train_acc_collect = []\r\n    valid_loss_collect = []\r\n    valid_acc_collect = []\r\n\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, \"./save.ckpt\")\r\n        iteration=0\r\n        for e in range(epochs):\r\n              for batch_x,batch_y in get_batch(train_x,train_y,batch_size):\r\n                    iteration+=1\r\n                    feed = {model.inputs: train_x,\r\n                                model.labels: train_y,\r\n                                model.learning_rate: learning_rate_value,\r\n                                model.is_training:True,\r\n                                }\r\n                    train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], \r\n                            feed_dict=feed)\r\n                    \r\n                    if iteration % train_collect == 0:\r\n                        x_collect.append(e)\r\n                        train_loss_collect.append(train_loss)\r\n                        train_acc_collect.append(train_acc)\r\n                    \r\n                        if iteration % train_print==0:\r\n                            print(\"Epoch: {}/{}\".format(e + 1, epochs),\r\n                            \"Train Loss: {:.4f}\".format(train_loss),\r\n                            \"Train Acc: {:.4f}\".format(train_acc))\r\n                        \r\n                        feed = {model.inputs: valid_x,\r\n                                    model.labels: valid_y,\r\n                                    model.is_training:False\r\n                                    }\r\n                        val_loss, val_acc = sess.run([model.cost, model.accuracy], feed_dict=feed)\r\n                        valid_loss_collect.append(val_loss)\r\n                        valid_acc_collect.append(val_acc)\r\n                                       \r\n                        if iteration % train_print==0:\r\n                            print(\"Epoch: {}/{}\".format(e + 1, epochs),\r\n                           \"Validation Loss: {:.4f}\".format(val_loss),\r\n                           \"Validation Acc: {:.4f}\".format(val_acc))\r\n                \r\n\r\n    saver.save(sess, \"./save.ckpt\")\r\n\r\n#here is the OUTPUT of training:\r\n      \r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n    \r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     ...\r\n(I'm getting the same result as above till epoch 10/10)"}