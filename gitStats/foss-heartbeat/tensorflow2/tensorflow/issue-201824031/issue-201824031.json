{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6955", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6955/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6955/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6955/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6955", "id": 201824031, "node_id": "MDU6SXNzdWUyMDE4MjQwMzE=", "number": 6955, "title": "Better way to transfer data from memory to tensor in C++ API", "user": {"login": "ppries", "id": 7548590, "node_id": "MDQ6VXNlcjc1NDg1OTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/7548590?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppries", "html_url": "https://github.com/ppries", "followers_url": "https://api.github.com/users/ppries/followers", "following_url": "https://api.github.com/users/ppries/following{/other_user}", "gists_url": "https://api.github.com/users/ppries/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppries/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppries/subscriptions", "organizations_url": "https://api.github.com/users/ppries/orgs", "repos_url": "https://api.github.com/users/ppries/repos", "events_url": "https://api.github.com/users/ppries/events{/privacy}", "received_events_url": "https://api.github.com/users/ppries/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 20, "created_at": "2017-01-19T10:46:37Z", "updated_at": "2018-09-17T16:31:46Z", "closed_at": "2018-09-17T16:31:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is more of a general feature request for a better way to load data into a tensor in the C++ API, but I'll take our specific case as a reference.</p>\n<p>We currently train a model in Python, freeze it, and load it in a C++ production pipeline. This works fine, but it seems the only way of loading data into a tensor is by looping through every single element in the data and copying it to a tensor. In our case, we're dealing with a 1920x1080 ~30 fps video input signal (each frame coming as an OpenCV matrix) making it infeasible to do if we want to process a video within a reasonable amount of time.</p>\n<p>It seems there is a way of creating a tensor from a pointer in the C API (see <a href=\"http://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying\" rel=\"nofollow\">http://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying</a>), which we will test next, but it would be nice to also have this functionality in the C++ API.</p>\n<p>For reference, it takes approximately 900 ms to copy from an 1920x1080x3 OpenCV matrix to a tensor while it takes 315 ms to do session.run (which I assume includes transferring between CPU and GPU memory).</p>\n<p>An issue related to this is that we will probably already have the data on the GPU from some earlier preprocessing, so we would also be very interested in not having to transfer between CPU and GPU unnecessarily.</p>\n<p>So I guess it boils down to:</p>\n<ol>\n<li>Are there any plans for making it easier to load data already in memory to a tensor?</li>\n<li>How can we contribute?</li>\n</ol>", "body_text": "This is more of a general feature request for a better way to load data into a tensor in the C++ API, but I'll take our specific case as a reference.\nWe currently train a model in Python, freeze it, and load it in a C++ production pipeline. This works fine, but it seems the only way of loading data into a tensor is by looping through every single element in the data and copying it to a tensor. In our case, we're dealing with a 1920x1080 ~30 fps video input signal (each frame coming as an OpenCV matrix) making it infeasible to do if we want to process a video within a reasonable amount of time.\nIt seems there is a way of creating a tensor from a pointer in the C API (see http://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying), which we will test next, but it would be nice to also have this functionality in the C++ API.\nFor reference, it takes approximately 900 ms to copy from an 1920x1080x3 OpenCV matrix to a tensor while it takes 315 ms to do session.run (which I assume includes transferring between CPU and GPU memory).\nAn issue related to this is that we will probably already have the data on the GPU from some earlier preprocessing, so we would also be very interested in not having to transfer between CPU and GPU unnecessarily.\nSo I guess it boils down to:\n\nAre there any plans for making it easier to load data already in memory to a tensor?\nHow can we contribute?", "body": "This is more of a general feature request for a better way to load data into a tensor in the C++ API, but I'll take our specific case as a reference.\r\n\r\nWe currently train a model in Python, freeze it, and load it in a C++ production pipeline. This works fine, but it seems the only way of loading data into a tensor is by looping through every single element in the data and copying it to a tensor. In our case, we're dealing with a 1920x1080 ~30 fps video input signal (each frame coming as an OpenCV matrix) making it infeasible to do if we want to process a video within a reasonable amount of time.\r\n\r\nIt seems there is a way of creating a tensor from a pointer in the C API (see http://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying), which we will test next, but it would be nice to also have this functionality in the C++ API.\r\n\r\nFor reference, it takes approximately 900 ms to copy from an 1920x1080x3 OpenCV matrix to a tensor while it takes 315 ms to do session.run (which I assume includes transferring between CPU and GPU memory). \r\n\r\nAn issue related to this is that we will probably already have the data on the GPU from some earlier preprocessing, so we would also be very interested in not having to transfer between CPU and GPU unnecessarily.\r\n\r\nSo I guess it boils down to:\r\n\r\n1. Are there any plans for making it easier to load data already in memory to a tensor?\r\n2. How can we contribute?"}