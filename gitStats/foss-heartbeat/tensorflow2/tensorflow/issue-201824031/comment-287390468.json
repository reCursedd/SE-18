{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/287390468", "html_url": "https://github.com/tensorflow/tensorflow/issues/6955#issuecomment-287390468", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6955", "id": 287390468, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NzM5MDQ2OA==", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-17T15:43:51Z", "updated_at": "2017-03-17T15:44:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> Thanks for the reference you pointed to! Really helpful to what I am looking for.</p>\n<p>But to this issue per se, I guess parsing in a multi-node context is not exactly what the owner of this issue referred to.</p>\n<p>So currently there is no way to construct a tensor by moving an existing pointer to a memory buffer? How about constructing a tensor with empty underlying storage and take that storage for your own use, and give it back to TF?</p>\n<p><code>Allocator</code> is such a monolithic runtime so I guess it has to take control over the details of memory allocation, for example, alignment, similar to what <code>malloc</code>/<code>free</code> does. As it is simply not a good idea to glibc <code>free</code> a chunk of memory returned by another memory allocation library, say <code>jemalloc</code>.</p>", "body_text": "@yaroslavvb Thanks for the reference you pointed to! Really helpful to what I am looking for.\nBut to this issue per se, I guess parsing in a multi-node context is not exactly what the owner of this issue referred to.\nSo currently there is no way to construct a tensor by moving an existing pointer to a memory buffer? How about constructing a tensor with empty underlying storage and take that storage for your own use, and give it back to TF?\nAllocator is such a monolithic runtime so I guess it has to take control over the details of memory allocation, for example, alignment, similar to what malloc/free does. As it is simply not a good idea to glibc free a chunk of memory returned by another memory allocation library, say jemalloc.", "body": "@yaroslavvb Thanks for the reference you pointed to! Really helpful to what I am looking for. \r\n\r\nBut to this issue per se, I guess parsing in a multi-node context is not exactly what the owner of this issue referred to. \r\n\r\nSo currently there is no way to construct a tensor by moving an existing pointer to a memory buffer? How about constructing a tensor with empty underlying storage and take that storage for your own use, and give it back to TF? \r\n\r\n`Allocator` is such a monolithic runtime so I guess it has to take control over the details of memory allocation, for example, alignment, similar to what `malloc`/`free` does. As it is simply not a good idea to glibc `free` a chunk of memory returned by another memory allocation library, say `jemalloc`."}