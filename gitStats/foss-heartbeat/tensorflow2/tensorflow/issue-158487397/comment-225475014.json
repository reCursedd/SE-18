{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225475014", "html_url": "https://github.com/tensorflow/tensorflow/issues/2652#issuecomment-225475014", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2652", "id": 225475014, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTQ3NTAxNA==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-13T01:57:24Z", "updated_at": "2016-06-13T01:59:34Z", "author_association": "NONE", "body_html": "<p>Here's the link: <a href=\"https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/\" rel=\"nofollow\">https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/</a><br>\nUsing shfl_xor just keeps the same copy of data with in a warp (not mentioned in the article) and should be better in keep_dim situations. One example is in a <a href=\"https://github.com/ChenglongChen/word2vec_cbow/blob/master/cbow.cu\">fast CUDA word2vec implementation</a> that utilizes the warp shuffle to reach 3 million tokens/s on a Titan X with the default (as in the original implementation given by Mikolov) CBOW negative sampling setting</p>", "body_text": "Here's the link: https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/\nUsing shfl_xor just keeps the same copy of data with in a warp (not mentioned in the article) and should be better in keep_dim situations. One example is in a fast CUDA word2vec implementation that utilizes the warp shuffle to reach 3 million tokens/s on a Titan X with the default (as in the original implementation given by Mikolov) CBOW negative sampling setting", "body": "Here's the link: https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/\nUsing shfl_xor just keeps the same copy of data with in a warp (not mentioned in the article) and should be better in keep_dim situations. One example is in a [fast CUDA word2vec implementation](https://github.com/ChenglongChen/word2vec_cbow/blob/master/cbow.cu) that utilizes the warp shuffle to reach 3 million tokens/s on a Titan X with the default (as in the original implementation given by Mikolov) CBOW negative sampling setting\n"}