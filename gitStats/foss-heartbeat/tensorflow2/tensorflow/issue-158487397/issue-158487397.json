{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2652", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2652/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2652/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2652/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2652", "id": 158487397, "node_id": "MDU6SXNzdWUxNTg0ODczOTc=", "number": 2652, "title": "Backward pass of broadcasting on GPU is non-deterministic", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2016-06-04T04:51:13Z", "updated_at": "2017-05-12T15:07:44Z", "closed_at": "2016-06-08T05:53:10Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run</span>(<span class=\"pl-smi\">on_gpu</span>):\n    tf.reset_default_graph()\n    tf.set_random_seed(<span class=\"pl-c1\">42</span>)\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">if</span> on_gpu <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>):\n        a <span class=\"pl-k\">=</span> tf.random_normal([<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">16</span>])\n        b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>b<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span> <span class=\"pl-k\">=</span> [], <span class=\"pl-v\">initializer</span> <span class=\"pl-k\">=</span> tf.constant_initializer(<span class=\"pl-v\">value</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>))\n        c <span class=\"pl-k\">=</span> a<span class=\"pl-k\">*</span>b\n        grad <span class=\"pl-k\">=</span> tf.gradients(c, [b], <span class=\"pl-v\">gate_gradients</span><span class=\"pl-k\">=</span>tf.train.Optimizer.<span class=\"pl-c1\">GATE_GRAPH</span>)[<span class=\"pl-c1\">0</span>]\n\n    sess <span class=\"pl-k\">=</span> tf.Session()\n    sess.run(tf.initialize_all_variables())\n    grad_val <span class=\"pl-k\">=</span> sess.run(grad)\n    <span class=\"pl-k\">return</span> grad_val\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">20</span>):\n    <span class=\"pl-c1\">print</span> <span class=\"pl-c1\">repr</span>(run(<span class=\"pl-v\">on_gpu</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)),\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">20</span>):\n    <span class=\"pl-c1\">print</span> <span class=\"pl-c1\">repr</span>(run(<span class=\"pl-v\">on_gpu</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)),</pre></div>\n<p>Result:</p>\n<pre><code>23.066511 23.066511 23.066513 23.066513 23.066511 23.066513 23.066509 23.066513 23.066513 23.066511 23.066513 23.066511 23.066513 23.066513 23.066509 23.066511 23.066513 23.066513 23.066511 23.066511 \n23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509\n</code></pre>\n<p>As you can see, consistent result across CPU runs but inconsistent result across GPU runs.</p>\n<p>No doubt a CUDA reduction order issue, but it'd be really nice if we can have deterministic reduction. I am using tf 0.8.0 (self-compiled against CuDNN v5). CuDNN version is 5005 (not rc)</p>", "body_text": "import tensorflow as tf\n\ndef run(on_gpu):\n    tf.reset_default_graph()\n    tf.set_random_seed(42)\n    with tf.device('/gpu:0' if on_gpu else '/cpu:0'):\n        a = tf.random_normal([16, 16])\n        b = tf.get_variable('b', shape = [], initializer = tf.constant_initializer(value = 0.0))\n        c = a*b\n        grad = tf.gradients(c, [b], gate_gradients=tf.train.Optimizer.GATE_GRAPH)[0]\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    grad_val = sess.run(grad)\n    return grad_val\n\nfor i in xrange(20):\n    print repr(run(on_gpu=True)),\nprint ''\nfor i in xrange(20):\n    print repr(run(on_gpu=False)),\nResult:\n23.066511 23.066511 23.066513 23.066513 23.066511 23.066513 23.066509 23.066513 23.066513 23.066511 23.066513 23.066511 23.066513 23.066513 23.066509 23.066511 23.066513 23.066513 23.066511 23.066511 \n23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509\n\nAs you can see, consistent result across CPU runs but inconsistent result across GPU runs.\nNo doubt a CUDA reduction order issue, but it'd be really nice if we can have deterministic reduction. I am using tf 0.8.0 (self-compiled against CuDNN v5). CuDNN version is 5005 (not rc)", "body": "``` python\nimport tensorflow as tf\n\ndef run(on_gpu):\n    tf.reset_default_graph()\n    tf.set_random_seed(42)\n    with tf.device('/gpu:0' if on_gpu else '/cpu:0'):\n        a = tf.random_normal([16, 16])\n        b = tf.get_variable('b', shape = [], initializer = tf.constant_initializer(value = 0.0))\n        c = a*b\n        grad = tf.gradients(c, [b], gate_gradients=tf.train.Optimizer.GATE_GRAPH)[0]\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    grad_val = sess.run(grad)\n    return grad_val\n\nfor i in xrange(20):\n    print repr(run(on_gpu=True)),\nprint ''\nfor i in xrange(20):\n    print repr(run(on_gpu=False)),\n```\n\nResult:\n\n```\n23.066511 23.066511 23.066513 23.066513 23.066511 23.066513 23.066509 23.066513 23.066513 23.066511 23.066513 23.066511 23.066513 23.066513 23.066509 23.066511 23.066513 23.066513 23.066511 23.066511 \n23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509\n```\n\nAs you can see, consistent result across CPU runs but inconsistent result across GPU runs.\n\nNo doubt a CUDA reduction order issue, but it'd be really nice if we can have deterministic reduction. I am using tf 0.8.0 (self-compiled against CuDNN v5). CuDNN version is 5005 (not rc)\n"}