{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11143", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11143/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11143/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11143/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11143", "id": 239538563, "node_id": "MDU6SXNzdWUyMzk1Mzg1NjM=", "number": 11143, "title": "Parameter server of distributed Tensorflow computes unexpected training operations when using Estimator", "user": {"login": "jongsae", "id": 13041074, "node_id": "MDQ6VXNlcjEzMDQxMDc0", "avatar_url": "https://avatars0.githubusercontent.com/u/13041074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jongsae", "html_url": "https://github.com/jongsae", "followers_url": "https://api.github.com/users/jongsae/followers", "following_url": "https://api.github.com/users/jongsae/following{/other_user}", "gists_url": "https://api.github.com/users/jongsae/gists{/gist_id}", "starred_url": "https://api.github.com/users/jongsae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jongsae/subscriptions", "organizations_url": "https://api.github.com/users/jongsae/orgs", "repos_url": "https://api.github.com/users/jongsae/repos", "events_url": "https://api.github.com/users/jongsae/events{/privacy}", "received_events_url": "https://api.github.com/users/jongsae/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-29T16:33:37Z", "updated_at": "2017-07-14T13:27:21Z", "closed_at": "2017-06-29T16:53:37Z", "author_association": "NONE", "body_html": "<p>Hi, I am trying to use multi GPUs for the Google's seq2seq training (<a href=\"https://github.com/google/seq2seq\">https://github.com/google/seq2seq</a>) through distributed Tensorflow (data parallelism).</p>\n<p>I launched a parameter server (PS) and three worker processes on a machine equipped with 4 GPUs.<br>\nI have each process (including PS) to run on separate GPU through the CUDA_VISIBLE_DEVICES.<br>\nI successfully trained the model faster than the single-node version; however, I noticed a weird behavior.</p>\n<p>The way of enabling data parallelism was to set the ClusterConfig like below:<br>\n<code># ps_hosts, worker_hosts, job_name, and task_index are given as program arguments</code><br>\n<code>ps_hosts = FLAGS.ps_hosts.split(\",\")</code><br>\n<code>worker_hosts = FLAGS.worker_hosts.split(\",\")</code><br>\n<code>cluster = {\"ps\": ps_hosts, \"worker\": worker_hosts}</code><br>\n<code>os.environ['TF_CONFIG'] = json.dumps({</code><br>\n<code>    'cluster': cluster,</code><br>\n<code>    'task': {</code><br>\n<code>        'type': FLAGS.job_name,</code><br>\n<code>        'index': FLAGS.task_index</code><br>\n<code>    }</code><br>\n<code>})</code><br>\n<code>config = run_config.RunConfig( ..... ) </code><br>\n<code>estimator = tf.contrib.learn.Estimator(....., config=config)</code><br>\n<code>experiment = tf.contrib.learn.Experiment(estimator=estimator, .....) learn_runner.run(experiment=experiment, .....)</code></p>\n<p>I profiled the training of this machine using nvprof and noticed that the parameter server process also uses its GPU for training. I looked into the device placement log messages and there is no MatMul ops associated with the /job:ps but for some reason, GPU calls many gemm calls.</p>\n<p>I think this issue is not specific to GPU because I also ran the same experiment with PS mapped to CPU but the PS also computes training operations.</p>\n<p>Is there anybody else who has experienced this? Is this the Estimator's bug?<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L201\">_get_replica_device_setter</a> seems to pass /job:ps/task:%d as worker_device and maybe this makes this problem?</p>\n<p>I also thought about the possibility that I did something wrong in deploying distributed Tensorflow but as mentioned earlier, the model is trained successfully with higher performance so I am confused if this is a bug or a feature.</p>\n<p>I would really appreciate any feedback/comments/help. Please let me know if I am misunderstanding anything.</p>", "body_text": "Hi, I am trying to use multi GPUs for the Google's seq2seq training (https://github.com/google/seq2seq) through distributed Tensorflow (data parallelism).\nI launched a parameter server (PS) and three worker processes on a machine equipped with 4 GPUs.\nI have each process (including PS) to run on separate GPU through the CUDA_VISIBLE_DEVICES.\nI successfully trained the model faster than the single-node version; however, I noticed a weird behavior.\nThe way of enabling data parallelism was to set the ClusterConfig like below:\n# ps_hosts, worker_hosts, job_name, and task_index are given as program arguments\nps_hosts = FLAGS.ps_hosts.split(\",\")\nworker_hosts = FLAGS.worker_hosts.split(\",\")\ncluster = {\"ps\": ps_hosts, \"worker\": worker_hosts}\nos.environ['TF_CONFIG'] = json.dumps({\n    'cluster': cluster,\n    'task': {\n        'type': FLAGS.job_name,\n        'index': FLAGS.task_index\n    }\n})\nconfig = run_config.RunConfig( ..... ) \nestimator = tf.contrib.learn.Estimator(....., config=config)\nexperiment = tf.contrib.learn.Experiment(estimator=estimator, .....) learn_runner.run(experiment=experiment, .....)\nI profiled the training of this machine using nvprof and noticed that the parameter server process also uses its GPU for training. I looked into the device placement log messages and there is no MatMul ops associated with the /job:ps but for some reason, GPU calls many gemm calls.\nI think this issue is not specific to GPU because I also ran the same experiment with PS mapped to CPU but the PS also computes training operations.\nIs there anybody else who has experienced this? Is this the Estimator's bug?\n_get_replica_device_setter seems to pass /job:ps/task:%d as worker_device and maybe this makes this problem?\nI also thought about the possibility that I did something wrong in deploying distributed Tensorflow but as mentioned earlier, the model is trained successfully with higher performance so I am confused if this is a bug or a feature.\nI would really appreciate any feedback/comments/help. Please let me know if I am misunderstanding anything.", "body": "Hi, I am trying to use multi GPUs for the Google's seq2seq training (https://github.com/google/seq2seq) through distributed Tensorflow (data parallelism). \r\n\r\nI launched a parameter server (PS) and three worker processes on a machine equipped with 4 GPUs. \r\nI have each process (including PS) to run on separate GPU through the CUDA_VISIBLE_DEVICES. \r\nI successfully trained the model faster than the single-node version; however, I noticed a weird behavior. \r\n\r\nThe way of enabling data parallelism was to set the ClusterConfig like below:  \r\n`# ps_hosts, worker_hosts, job_name, and task_index are given as program arguments`\r\n`ps_hosts = FLAGS.ps_hosts.split(\",\")`\r\n`worker_hosts = FLAGS.worker_hosts.split(\",\")`\r\n`cluster = {\"ps\": ps_hosts, \"worker\": worker_hosts}`\r\n`os.environ['TF_CONFIG'] = json.dumps({`\r\n`    'cluster': cluster,`\r\n`    'task': {`\r\n`        'type': FLAGS.job_name,`\r\n`        'index': FLAGS.task_index`\r\n`    }`\r\n`})`\r\n`config = run_config.RunConfig( ..... ) `\r\n`estimator = tf.contrib.learn.Estimator(....., config=config)`\r\n`experiment = tf.contrib.learn.Experiment(estimator=estimator, .....)\r\nlearn_runner.run(experiment=experiment, .....)`\r\n\r\nI profiled the training of this machine using nvprof and noticed that the parameter server process also uses its GPU for training. I looked into the device placement log messages and there is no MatMul ops associated with the /job:ps but for some reason, GPU calls many gemm calls. \r\n\r\nI think this issue is not specific to GPU because I also ran the same experiment with PS mapped to CPU but the PS also computes training operations. \r\n\r\nIs there anybody else who has experienced this? Is this the Estimator's bug? \r\n[_get_replica_device_setter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L201) seems to pass /job:ps/task:%d as worker_device and maybe this makes this problem? \r\n\r\nI also thought about the possibility that I did something wrong in deploying distributed Tensorflow but as mentioned earlier, the model is trained successfully with higher performance so I am confused if this is a bug or a feature. \r\n\r\nI would really appreciate any feedback/comments/help. Please let me know if I am misunderstanding anything.  \r\n\r\n"}