{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6599", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6599/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6599/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6599/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6599", "id": 198335710, "node_id": "MDU6SXNzdWUxOTgzMzU3MTA=", "number": 6599, "title": "memory leak in tensorflow_gpu 0.12.1", "user": {"login": "Mahdizade", "id": 5513062, "node_id": "MDQ6VXNlcjU1MTMwNjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/5513062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mahdizade", "html_url": "https://github.com/Mahdizade", "followers_url": "https://api.github.com/users/Mahdizade/followers", "following_url": "https://api.github.com/users/Mahdizade/following{/other_user}", "gists_url": "https://api.github.com/users/Mahdizade/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mahdizade/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mahdizade/subscriptions", "organizations_url": "https://api.github.com/users/Mahdizade/orgs", "repos_url": "https://api.github.com/users/Mahdizade/repos", "events_url": "https://api.github.com/users/Mahdizade/events{/privacy}", "received_events_url": "https://api.github.com/users/Mahdizade/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 28, "created_at": "2017-01-02T13:28:25Z", "updated_at": "2017-05-28T16:27:49Z", "closed_at": "2017-05-28T16:27:49Z", "author_association": "NONE", "body_html": "<p><strong>Hi, We use tensorflow for training our OCR system models. I simply train models in tensorflow 0.9 and former versions. after some upgrade in cuda and tensorflow, I see large memory leak in our server with 32GB RAM.</strong></p>\n<h3>I have tried all of suggestions in <a href=\"http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201612280142239281993\" rel=\"nofollow\">How to debug a memory leak in TensorFlow</a> and some other github issues and stackoverflow posts. No of them worked.</h3>\n<p>train code:</p>\n<pre><code>#!/bin/env python\nimport tensorflow as tf\n\nimport Config\nimport Utilities\nfrom Dataset import Dataset\nfrom Model import Model\n\ndataset = Dataset()\n\nsess_config = tf.ConfigProto()\nsess_config.gpu_options.allow_growth = True\nsess = tf.Session(config=sess_config)\nimages, labels = dataset.train_images_labels()\nmodel = Model(images, labels, training=True)\ntf.train.start_queue_runners(sess=sess)\n\n\ndef main(global_step=0):\n    if global_step == 0:\n        init_op = tf.global_variables_initializer()\n        sess.run(init_op)\n    else:\n        checkpoint_path = Utilities.get_checkpoint_path(global_step)\n        model.saver.restore(sess, checkpoint_path)\n        Utilities.log_checkpoint_load(checkpoint_path)\n\n    loss = 0\n    train_iterations = global_step\n    while train_iterations &lt; Config.train_max_iterations:\n        # train\n        l = model.train(sess)\n        loss += l\n        train_iterations += 1\n\n        # show train loss\n        if train_iterations % Config.display_intervals == 0:\n            loss /= float(Config.display_intervals)\n            Utilities.log_train_loss(train_iterations, loss)\n            loss = 0\n\n        # save checkpoint\n        if train_iterations % Config.checkpoint_intervals == 0:\n            checkpoint_path = model.saver.save(sess, Utilities.get_checkpoint_path(train_iterations))\n            Utilities.log_checkpoint_save(checkpoint_path)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n<p>images, labels are queues from tf.train.shuffle_batch function. I used Graph.finalize() and I am sure no new operation added to graph in training because the size of stored models' files are equal. I guess the reason of leak is tensorflow queues.</p>\n<h3>Environment info</h3>\n<p>Ubuntu Server 14.04.5 LTS</p>\n<p>Installed version of CUDA and cuDNN:<br>\nCUDA V8.0.44, cuDNN V5.1.5.</p>\n<p>installed tensorflow from PyPI.</p>\n<ol>\n<li>sudo pip install --upgrade tensorflow_gpu.</li>\n<li>tensorflow 0.12.1.</li>\n</ol>", "body_text": "Hi, We use tensorflow for training our OCR system models. I simply train models in tensorflow 0.9 and former versions. after some upgrade in cuda and tensorflow, I see large memory leak in our server with 32GB RAM.\nI have tried all of suggestions in How to debug a memory leak in TensorFlow and some other github issues and stackoverflow posts. No of them worked.\ntrain code:\n#!/bin/env python\nimport tensorflow as tf\n\nimport Config\nimport Utilities\nfrom Dataset import Dataset\nfrom Model import Model\n\ndataset = Dataset()\n\nsess_config = tf.ConfigProto()\nsess_config.gpu_options.allow_growth = True\nsess = tf.Session(config=sess_config)\nimages, labels = dataset.train_images_labels()\nmodel = Model(images, labels, training=True)\ntf.train.start_queue_runners(sess=sess)\n\n\ndef main(global_step=0):\n    if global_step == 0:\n        init_op = tf.global_variables_initializer()\n        sess.run(init_op)\n    else:\n        checkpoint_path = Utilities.get_checkpoint_path(global_step)\n        model.saver.restore(sess, checkpoint_path)\n        Utilities.log_checkpoint_load(checkpoint_path)\n\n    loss = 0\n    train_iterations = global_step\n    while train_iterations < Config.train_max_iterations:\n        # train\n        l = model.train(sess)\n        loss += l\n        train_iterations += 1\n\n        # show train loss\n        if train_iterations % Config.display_intervals == 0:\n            loss /= float(Config.display_intervals)\n            Utilities.log_train_loss(train_iterations, loss)\n            loss = 0\n\n        # save checkpoint\n        if train_iterations % Config.checkpoint_intervals == 0:\n            checkpoint_path = model.saver.save(sess, Utilities.get_checkpoint_path(train_iterations))\n            Utilities.log_checkpoint_save(checkpoint_path)\n\n\nif __name__ == '__main__':\n    main()\n\nimages, labels are queues from tf.train.shuffle_batch function. I used Graph.finalize() and I am sure no new operation added to graph in training because the size of stored models' files are equal. I guess the reason of leak is tensorflow queues.\nEnvironment info\nUbuntu Server 14.04.5 LTS\nInstalled version of CUDA and cuDNN:\nCUDA V8.0.44, cuDNN V5.1.5.\ninstalled tensorflow from PyPI.\n\nsudo pip install --upgrade tensorflow_gpu.\ntensorflow 0.12.1.", "body": "**Hi, We use tensorflow for training our OCR system models. I simply train models in tensorflow 0.9 and former versions. after some upgrade in cuda and tensorflow, I see large memory leak in our server with 32GB RAM.**\r\n\r\n### I have tried all of suggestions in [How to debug a memory leak in TensorFlow](http://stackoverflow.com/documentation/tensorflow/3883/how-to-debug-a-memory-leak-in-tensorflow#t=201612280142239281993) and some other github issues and stackoverflow posts. No of them worked.\r\n\r\ntrain code:\r\n\r\n```\r\n#!/bin/env python\r\nimport tensorflow as tf\r\n\r\nimport Config\r\nimport Utilities\r\nfrom Dataset import Dataset\r\nfrom Model import Model\r\n\r\ndataset = Dataset()\r\n\r\nsess_config = tf.ConfigProto()\r\nsess_config.gpu_options.allow_growth = True\r\nsess = tf.Session(config=sess_config)\r\nimages, labels = dataset.train_images_labels()\r\nmodel = Model(images, labels, training=True)\r\ntf.train.start_queue_runners(sess=sess)\r\n\r\n\r\ndef main(global_step=0):\r\n    if global_step == 0:\r\n        init_op = tf.global_variables_initializer()\r\n        sess.run(init_op)\r\n    else:\r\n        checkpoint_path = Utilities.get_checkpoint_path(global_step)\r\n        model.saver.restore(sess, checkpoint_path)\r\n        Utilities.log_checkpoint_load(checkpoint_path)\r\n\r\n    loss = 0\r\n    train_iterations = global_step\r\n    while train_iterations < Config.train_max_iterations:\r\n        # train\r\n        l = model.train(sess)\r\n        loss += l\r\n        train_iterations += 1\r\n\r\n        # show train loss\r\n        if train_iterations % Config.display_intervals == 0:\r\n            loss /= float(Config.display_intervals)\r\n            Utilities.log_train_loss(train_iterations, loss)\r\n            loss = 0\r\n\r\n        # save checkpoint\r\n        if train_iterations % Config.checkpoint_intervals == 0:\r\n            checkpoint_path = model.saver.save(sess, Utilities.get_checkpoint_path(train_iterations))\r\n            Utilities.log_checkpoint_save(checkpoint_path)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nimages, labels are queues from tf.train.shuffle_batch function. I used Graph.finalize() and I am sure no new operation added to graph in training because the size of stored models' files are equal. I guess the reason of leak is tensorflow queues.\r\n\r\n\r\n### Environment info\r\nUbuntu Server 14.04.5 LTS \r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA V8.0.44, cuDNN V5.1.5.\r\n\r\ninstalled tensorflow from PyPI.\r\n1. sudo pip install --upgrade tensorflow_gpu.\r\n2. tensorflow 0.12.1."}