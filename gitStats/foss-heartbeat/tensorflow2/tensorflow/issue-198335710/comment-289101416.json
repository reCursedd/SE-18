{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289101416", "html_url": "https://github.com/tensorflow/tensorflow/issues/6599#issuecomment-289101416", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6599", "id": 289101416, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTEwMTQxNg==", "user": {"login": "chwang85", "id": 16256959, "node_id": "MDQ6VXNlcjE2MjU2OTU5", "avatar_url": "https://avatars0.githubusercontent.com/u/16256959?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chwang85", "html_url": "https://github.com/chwang85", "followers_url": "https://api.github.com/users/chwang85/followers", "following_url": "https://api.github.com/users/chwang85/following{/other_user}", "gists_url": "https://api.github.com/users/chwang85/gists{/gist_id}", "starred_url": "https://api.github.com/users/chwang85/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chwang85/subscriptions", "organizations_url": "https://api.github.com/users/chwang85/orgs", "repos_url": "https://api.github.com/users/chwang85/repos", "events_url": "https://api.github.com/users/chwang85/events{/privacy}", "received_events_url": "https://api.github.com/users/chwang85/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-24T18:05:06Z", "updated_at": "2017-03-24T18:05:06Z", "author_association": "NONE", "body_html": "<p>Look at this code:<br>\n<code>result = tf.contrib.layers.bias_add(inputs=tf.matmul(a=left, b=right), activation_fn=tf.nn.softmax)</code><br>\nIf I use \"softmax\" as the activation function, there will be a memory leak, but if I change the activation function to \"relu\" or \"softplus\" or \"sigmoid\", the memory leak will disappear.<br>\nBy the way, this \"softmax\" is not the one used at the last layer for classification.</p>\n<p>Platform: CentOS 7, x86_64<br>\nTensorFlow version: 1.0_gpu</p>", "body_text": "Look at this code:\nresult = tf.contrib.layers.bias_add(inputs=tf.matmul(a=left, b=right), activation_fn=tf.nn.softmax)\nIf I use \"softmax\" as the activation function, there will be a memory leak, but if I change the activation function to \"relu\" or \"softplus\" or \"sigmoid\", the memory leak will disappear.\nBy the way, this \"softmax\" is not the one used at the last layer for classification.\nPlatform: CentOS 7, x86_64\nTensorFlow version: 1.0_gpu", "body": "Look at this code:\r\n`result = tf.contrib.layers.bias_add(inputs=tf.matmul(a=left, b=right), activation_fn=tf.nn.softmax)`\r\nIf I use \"softmax\" as the activation function, there will be a memory leak, but if I change the activation function to \"relu\" or \"softplus\" or \"sigmoid\", the memory leak will disappear.\r\nBy the way, this \"softmax\" is not the one used at the last layer for classification.\r\n\r\nPlatform: CentOS 7, x86_64\r\nTensorFlow version: 1.0_gpu"}