{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4540", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4540/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4540/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4540/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4540", "id": 178772203, "node_id": "MDU6SXNzdWUxNzg3NzIyMDM=", "number": 4540, "title": "error when using embedding_lookup with random initial embeddings", "user": {"login": "lan2720", "id": 5330101, "node_id": "MDQ6VXNlcjUzMzAxMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/5330101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lan2720", "html_url": "https://github.com/lan2720", "followers_url": "https://api.github.com/users/lan2720/followers", "following_url": "https://api.github.com/users/lan2720/following{/other_user}", "gists_url": "https://api.github.com/users/lan2720/gists{/gist_id}", "starred_url": "https://api.github.com/users/lan2720/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lan2720/subscriptions", "organizations_url": "https://api.github.com/users/lan2720/orgs", "repos_url": "https://api.github.com/users/lan2720/repos", "events_url": "https://api.github.com/users/lan2720/events{/privacy}", "received_events_url": "https://api.github.com/users/lan2720/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-09-23T02:51:59Z", "updated_at": "2018-06-09T04:46:45Z", "closed_at": "2017-01-27T20:05:53Z", "author_association": "NONE", "body_html": "<p>Hi all. I'm working on variable-length sequence data. eg. a sequence <code>data = np.array([4,2,3,2,0,-1,-1])</code>(padded to length 7, the real sequence length is 5). They are word ids. So I can get word embeddings through <code>tf.nn.embedding_lookup</code>.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport os\nimport cPickle\n\n\nvocab_size = 105374\nembed_dim = 200\ninput_x = tf.placeholder(dtype=tf.int32, shape=[7], name=\"x\")\nembedding_matrix = tf.placeholder(dtype=tf.float32, shape=[vocab_size, embed_dim], name=\"embeddings\")\nwith tf.device(\"/cpu:0\"), tf.variable_scope(\"embedding\"):\n    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)\n\n\n## Test\ndata = np.array([4,2,3,2,0,-1,-1])\nembedding_path = os.path.join(\"/home/lan/data/dataset-TSU\", \"IMDB\", \"embinit.save\")\nwith open(embedding_path, 'rb') as f:\n    embed_init = cPickle.load(f)\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print sess.run(embedded_inputs, feed_dict={input_x:data, embedding_matrix:embed_init})\n</code></pre>\n<p>If I use <code>placeholder</code> to hold my pre-trained word vectors <code>embinit.save</code>, <code>id = -1</code> is ok to work.<br>\nI got result from the code above like that. As I expected, the last two ids are both -1, which cannot be found in word embedding matrix. So it returned all zeros.</p>\n<pre><code>[[ 0.26877201  0.239695   -0.08297    ..., -0.030947    0.199618\n  -0.24129499]\n [ 0.115232    0.123859   -0.055312   ..., -0.040216    0.176268   -0.259417  ]\n [ 0.132403    0.134068   -0.059557   ..., -0.010268    0.16242801\n  -0.240173  ]\n ..., \n [ 0.20500401  0.125486   -0.116052   ...,  0.066659    0.15658499\n  -0.227872  ]\n [ 0.          0.          0.         ...,  0.          0.          0.        ]\n [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n</code></pre>\n<p>But if I randomly initialized word embedding matrix, like that</p>\n<pre><code>vocab_size = 105374\nembed_dim = 200\ninput_x = tf.placeholder(dtype=tf.int32, shape=[7], name=\"x\")\nwith tf.device(\"/cpu:0\"), tf.variable_scope(\"embedding\"):\n    embedding_matrix = tf.get_variable(name=\"embedding_matrix\",\n                                            dtype=tf.float32,\n                                            shape=[vocab_size,embed_dim],\n                                            initializer=tf.random_uniform_initializer(-1.0, 1.0))\n    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)\n</code></pre>\n<p>I got such an error:</p>\n<pre><code>tensorflow.python.framework.errors.InvalidArgumentError: indices[5] = -1 is not in [0, 105374)\n</code></pre>\n<p>That's to say I cannot use id=-1 in this way. Why?? Is there any solution?</p>", "body_text": "Hi all. I'm working on variable-length sequence data. eg. a sequence data = np.array([4,2,3,2,0,-1,-1])(padded to length 7, the real sequence length is 5). They are word ids. So I can get word embeddings through tf.nn.embedding_lookup.\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport cPickle\n\n\nvocab_size = 105374\nembed_dim = 200\ninput_x = tf.placeholder(dtype=tf.int32, shape=[7], name=\"x\")\nembedding_matrix = tf.placeholder(dtype=tf.float32, shape=[vocab_size, embed_dim], name=\"embeddings\")\nwith tf.device(\"/cpu:0\"), tf.variable_scope(\"embedding\"):\n    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)\n\n\n## Test\ndata = np.array([4,2,3,2,0,-1,-1])\nembedding_path = os.path.join(\"/home/lan/data/dataset-TSU\", \"IMDB\", \"embinit.save\")\nwith open(embedding_path, 'rb') as f:\n    embed_init = cPickle.load(f)\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print sess.run(embedded_inputs, feed_dict={input_x:data, embedding_matrix:embed_init})\n\nIf I use placeholder to hold my pre-trained word vectors embinit.save, id = -1 is ok to work.\nI got result from the code above like that. As I expected, the last two ids are both -1, which cannot be found in word embedding matrix. So it returned all zeros.\n[[ 0.26877201  0.239695   -0.08297    ..., -0.030947    0.199618\n  -0.24129499]\n [ 0.115232    0.123859   -0.055312   ..., -0.040216    0.176268   -0.259417  ]\n [ 0.132403    0.134068   -0.059557   ..., -0.010268    0.16242801\n  -0.240173  ]\n ..., \n [ 0.20500401  0.125486   -0.116052   ...,  0.066659    0.15658499\n  -0.227872  ]\n [ 0.          0.          0.         ...,  0.          0.          0.        ]\n [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n\nBut if I randomly initialized word embedding matrix, like that\nvocab_size = 105374\nembed_dim = 200\ninput_x = tf.placeholder(dtype=tf.int32, shape=[7], name=\"x\")\nwith tf.device(\"/cpu:0\"), tf.variable_scope(\"embedding\"):\n    embedding_matrix = tf.get_variable(name=\"embedding_matrix\",\n                                            dtype=tf.float32,\n                                            shape=[vocab_size,embed_dim],\n                                            initializer=tf.random_uniform_initializer(-1.0, 1.0))\n    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)\n\nI got such an error:\ntensorflow.python.framework.errors.InvalidArgumentError: indices[5] = -1 is not in [0, 105374)\n\nThat's to say I cannot use id=-1 in this way. Why?? Is there any solution?", "body": "Hi all. I'm working on variable-length sequence data. eg. a sequence `data = np.array([4,2,3,2,0,-1,-1])`(padded to length 7, the real sequence length is 5). They are word ids. So I can get word embeddings through `tf.nn.embedding_lookup`. \n\n```\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport cPickle\n\n\nvocab_size = 105374\nembed_dim = 200\ninput_x = tf.placeholder(dtype=tf.int32, shape=[7], name=\"x\")\nembedding_matrix = tf.placeholder(dtype=tf.float32, shape=[vocab_size, embed_dim], name=\"embeddings\")\nwith tf.device(\"/cpu:0\"), tf.variable_scope(\"embedding\"):\n    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)\n\n\n## Test\ndata = np.array([4,2,3,2,0,-1,-1])\nembedding_path = os.path.join(\"/home/lan/data/dataset-TSU\", \"IMDB\", \"embinit.save\")\nwith open(embedding_path, 'rb') as f:\n    embed_init = cPickle.load(f)\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print sess.run(embedded_inputs, feed_dict={input_x:data, embedding_matrix:embed_init})\n```\n\nIf I use `placeholder` to hold my pre-trained word vectors `embinit.save`, `id = -1` is ok to work.\nI got result from the code above like that. As I expected, the last two ids are both -1, which cannot be found in word embedding matrix. So it returned all zeros.\n\n```\n[[ 0.26877201  0.239695   -0.08297    ..., -0.030947    0.199618\n  -0.24129499]\n [ 0.115232    0.123859   -0.055312   ..., -0.040216    0.176268   -0.259417  ]\n [ 0.132403    0.134068   -0.059557   ..., -0.010268    0.16242801\n  -0.240173  ]\n ..., \n [ 0.20500401  0.125486   -0.116052   ...,  0.066659    0.15658499\n  -0.227872  ]\n [ 0.          0.          0.         ...,  0.          0.          0.        ]\n [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n```\n\nBut if I randomly initialized word embedding matrix, like that \n\n```\nvocab_size = 105374\nembed_dim = 200\ninput_x = tf.placeholder(dtype=tf.int32, shape=[7], name=\"x\")\nwith tf.device(\"/cpu:0\"), tf.variable_scope(\"embedding\"):\n    embedding_matrix = tf.get_variable(name=\"embedding_matrix\",\n                                            dtype=tf.float32,\n                                            shape=[vocab_size,embed_dim],\n                                            initializer=tf.random_uniform_initializer(-1.0, 1.0))\n    embedded_inputs = tf.nn.embedding_lookup(embedding_matrix, input_x)\n```\n\nI got such an error:\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: indices[5] = -1 is not in [0, 105374)\n```\n\nThat's to say I cannot use id=-1 in this way. Why?? Is there any solution?\n"}