{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324891245", "html_url": "https://github.com/tensorflow/tensorflow/issues/12569#issuecomment-324891245", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12569", "id": 324891245, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDg5MTI0NQ==", "user": {"login": "malsulaimi", "id": 31324944, "node_id": "MDQ6VXNlcjMxMzI0OTQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/31324944?v=4", "gravatar_id": "", "url": "https://api.github.com/users/malsulaimi", "html_url": "https://github.com/malsulaimi", "followers_url": "https://api.github.com/users/malsulaimi/followers", "following_url": "https://api.github.com/users/malsulaimi/following{/other_user}", "gists_url": "https://api.github.com/users/malsulaimi/gists{/gist_id}", "starred_url": "https://api.github.com/users/malsulaimi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/malsulaimi/subscriptions", "organizations_url": "https://api.github.com/users/malsulaimi/orgs", "repos_url": "https://api.github.com/users/malsulaimi/repos", "events_url": "https://api.github.com/users/malsulaimi/events{/privacy}", "received_events_url": "https://api.github.com/users/malsulaimi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-25T11:19:37Z", "updated_at": "2017-08-25T11:19:37Z", "author_association": "NONE", "body_html": "<p>I did not know that this would return a AttenionWrapperState , I though that this would return a normal initial state  , and thus I was using it as the below :</p>\n<pre><code>def decoding_layer(dec_input, encoder_state,\n                   target_sequence_length, max_target_sequence_length,\n                   rnn_size,\n                   num_layers, target_vocab_to_int, target_vocab_size,\n                   batch_size, keep_prob, decoding_embedding_size , encoder_outputs):\n    \"\"\"\n    Create decoding layer\n    :param dec_input: Decoder input\n    :param encoder_state: Encoder state\n    :param target_sequence_length: The lengths of each sequence in the target batch\n    :param max_target_sequence_length: Maximum length of target sequences\n    :param rnn_size: RNN Size\n    :param num_layers: Number of layers\n    :param target_vocab_to_int: Dictionary to go from the target words to an id\n    :param target_vocab_size: Size of target vocabulary\n    :param batch_size: The size of the batch\n    :param keep_prob: Dropout keep probability\n    :param decoding_embedding_size: Decoding embedding size\n    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n    \"\"\"\n    # 1. Decoder Embedding\n    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n\n    # 2. Construct the decoder cell\n    def create_cell(rnn_size):\n        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n                                            initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n        drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n        return drop\n\n\n    dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n    #dec_cell = tf.contrib.rnn.MultiRNNCell(cells_a)  \n\n    #attention details \n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs) \n\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size/2)\n\nattn_zero = attn_cell.zero_state(batch_size , tf.float32 )\n\nattn_zero = attn_zero.clone(cell_state = encoder_state)\n\nnew_state = tf.contrib.seq2seq.AttentionWrapperState(cell_state = encoder_state, attention = attn_zero  , time = 0 ,alignments=None , alignment_history=())\n\n\"\"\"out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n            attn_cell, target_vocab_size, reuse=True\n        )\"\"\"\n\n    #end of attention \n\n    output_layer = Dense(target_vocab_size,\n                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n\n    with tf.variable_scope(\"decode\"):\n        train_decoder_out = decoding_layer_train(new_state, attn_cell, dec_embed_input, \n                         target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\n\n    with tf.variable_scope(\"decode\", reuse=True):\n        infer_decoder_out = decoding_layer_infer(new_state, attn_cell, dec_embeddings, \n                             target_vocab_to_int['&lt;GO&gt;'], target_vocab_to_int['&lt;EOS&gt;'], max_target_sequence_length, \n                             target_vocab_size, output_layer, batch_size, keep_prob)\n\n    return (train_decoder_out, infer_decoder_out)\n\n\"\"\"\nDON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n\"\"\"\n#tests.test_decoding_layer(decoding_layer)\n</code></pre>", "body_text": "I did not know that this would return a AttenionWrapperState , I though that this would return a normal initial state  , and thus I was using it as the below :\ndef decoding_layer(dec_input, encoder_state,\n                   target_sequence_length, max_target_sequence_length,\n                   rnn_size,\n                   num_layers, target_vocab_to_int, target_vocab_size,\n                   batch_size, keep_prob, decoding_embedding_size , encoder_outputs):\n    \"\"\"\n    Create decoding layer\n    :param dec_input: Decoder input\n    :param encoder_state: Encoder state\n    :param target_sequence_length: The lengths of each sequence in the target batch\n    :param max_target_sequence_length: Maximum length of target sequences\n    :param rnn_size: RNN Size\n    :param num_layers: Number of layers\n    :param target_vocab_to_int: Dictionary to go from the target words to an id\n    :param target_vocab_size: Size of target vocabulary\n    :param batch_size: The size of the batch\n    :param keep_prob: Dropout keep probability\n    :param decoding_embedding_size: Decoding embedding size\n    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n    \"\"\"\n    # 1. Decoder Embedding\n    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n\n    # 2. Construct the decoder cell\n    def create_cell(rnn_size):\n        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n                                            initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n        drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n        return drop\n\n\n    dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n    #dec_cell = tf.contrib.rnn.MultiRNNCell(cells_a)  \n\n    #attention details \n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs) \n\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size/2)\n\nattn_zero = attn_cell.zero_state(batch_size , tf.float32 )\n\nattn_zero = attn_zero.clone(cell_state = encoder_state)\n\nnew_state = tf.contrib.seq2seq.AttentionWrapperState(cell_state = encoder_state, attention = attn_zero  , time = 0 ,alignments=None , alignment_history=())\n\n\"\"\"out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n            attn_cell, target_vocab_size, reuse=True\n        )\"\"\"\n\n    #end of attention \n\n    output_layer = Dense(target_vocab_size,\n                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n\n    with tf.variable_scope(\"decode\"):\n        train_decoder_out = decoding_layer_train(new_state, attn_cell, dec_embed_input, \n                         target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\n\n    with tf.variable_scope(\"decode\", reuse=True):\n        infer_decoder_out = decoding_layer_infer(new_state, attn_cell, dec_embeddings, \n                             target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], max_target_sequence_length, \n                             target_vocab_size, output_layer, batch_size, keep_prob)\n\n    return (train_decoder_out, infer_decoder_out)\n\n\"\"\"\nDON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n\"\"\"\n#tests.test_decoding_layer(decoding_layer)", "body": "I did not know that this would return a AttenionWrapperState , I though that this would return a normal initial state  , and thus I was using it as the below : \r\n```\r\ndef decoding_layer(dec_input, encoder_state,\r\n                   target_sequence_length, max_target_sequence_length,\r\n                   rnn_size,\r\n                   num_layers, target_vocab_to_int, target_vocab_size,\r\n                   batch_size, keep_prob, decoding_embedding_size , encoder_outputs):\r\n    \"\"\"\r\n    Create decoding layer\r\n    :param dec_input: Decoder input\r\n    :param encoder_state: Encoder state\r\n    :param target_sequence_length: The lengths of each sequence in the target batch\r\n    :param max_target_sequence_length: Maximum length of target sequences\r\n    :param rnn_size: RNN Size\r\n    :param num_layers: Number of layers\r\n    :param target_vocab_to_int: Dictionary to go from the target words to an id\r\n    :param target_vocab_size: Size of target vocabulary\r\n    :param batch_size: The size of the batch\r\n    :param keep_prob: Dropout keep probability\r\n    :param decoding_embedding_size: Decoding embedding size\r\n    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\r\n    \"\"\"\r\n    # 1. Decoder Embedding\r\n    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\r\n    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\r\n\r\n    # 2. Construct the decoder cell\r\n    def create_cell(rnn_size):\r\n        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,\r\n                                            initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\r\n        drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\r\n        return drop\r\n\r\n\r\n    dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\r\n    #dec_cell = tf.contrib.rnn.MultiRNNCell(cells_a)  \r\n\r\n    #attention details \r\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs) \r\n\r\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size/2)\r\n\r\nattn_zero = attn_cell.zero_state(batch_size , tf.float32 )\r\n\r\nattn_zero = attn_zero.clone(cell_state = encoder_state)\r\n\r\nnew_state = tf.contrib.seq2seq.AttentionWrapperState(cell_state = encoder_state, attention = attn_zero  , time = 0 ,alignments=None , alignment_history=())\r\n\r\n\"\"\"out_cell = tf.contrib.rnn.OutputProjectionWrapper(\r\n            attn_cell, target_vocab_size, reuse=True\r\n        )\"\"\"\r\n\r\n    #end of attention \r\n\r\n    output_layer = Dense(target_vocab_size,\r\n                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\r\n\r\n    with tf.variable_scope(\"decode\"):\r\n        train_decoder_out = decoding_layer_train(new_state, attn_cell, dec_embed_input, \r\n                         target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\r\n\r\n    with tf.variable_scope(\"decode\", reuse=True):\r\n        infer_decoder_out = decoding_layer_infer(new_state, attn_cell, dec_embeddings, \r\n                             target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], max_target_sequence_length, \r\n                             target_vocab_size, output_layer, batch_size, keep_prob)\r\n\r\n    return (train_decoder_out, infer_decoder_out)\r\n\r\n\"\"\"\r\nDON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\r\n\"\"\"\r\n#tests.test_decoding_layer(decoding_layer)\r\n```\r\n"}