{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/234296270", "html_url": "https://github.com/tensorflow/tensorflow/issues/3373#issuecomment-234296270", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3373", "id": 234296270, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDI5NjI3MA==", "user": {"login": "eamartin", "id": 287200, "node_id": "MDQ6VXNlcjI4NzIwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eamartin", "html_url": "https://github.com/eamartin", "followers_url": "https://api.github.com/users/eamartin/followers", "following_url": "https://api.github.com/users/eamartin/following{/other_user}", "gists_url": "https://api.github.com/users/eamartin/gists{/gist_id}", "starred_url": "https://api.github.com/users/eamartin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eamartin/subscriptions", "organizations_url": "https://api.github.com/users/eamartin/orgs", "repos_url": "https://api.github.com/users/eamartin/repos", "events_url": "https://api.github.com/users/eamartin/events{/privacy}", "received_events_url": "https://api.github.com/users/eamartin/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-21T15:48:12Z", "updated_at": "2016-07-21T18:10:27Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=592670\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/concretevitamin\">@concretevitamin</a> Initializing a &gt;50% GPU memory size array from <code>tf.zeros</code> does not work either.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a> Assuming a reasonable implementation of scatter_update, that would work if any type of initialization worked (or if there wasn't an error thrown while trying to use an uninitialized variable).</p>\n<p>The following is my attempt to use tf.zeros and scatter_update. If I comment out the <code>sess.run(data.initializer)</code> line, I get an error about using a non-initialized variable. If I don't comment out the line, I get a very similar OOM error message to what I put in the initial issue description.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> allocate 1K x 2M matrix (8GB)                                                                                                             </span>\n_data <span class=\"pl-k\">=</span> np.ones((<span class=\"pl-c1\">1</span> <span class=\"pl-k\">&lt;&lt;</span> <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">1</span> <span class=\"pl-k\">&lt;&lt;</span> <span class=\"pl-c1\">21</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n\ndata <span class=\"pl-k\">=</span> tf.Variable(tf.zeros(_data.shape), <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">collections</span><span class=\"pl-k\">=</span>[])\n\nidx_ph <span class=\"pl-k\">=</span> tf.placeholder(tf.int32)\ndata_ph <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[_data.shape[<span class=\"pl-c1\">1</span>]])\nscatter_update_op <span class=\"pl-k\">=</span> tf.scatter_update(data, idx_ph, data_ph)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(data.initializer)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> assign 1 row at a time                                                                                                                </span>\n    <span class=\"pl-k\">for</span> r <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">&lt;&lt;</span> <span class=\"pl-c1\">10</span>):\n        sess.run(scatter_update_op, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{idx_ph: r, data_ph: _data[r, :]})\n\n    out <span class=\"pl-k\">=</span> sess.run(data)</pre></div>\n<p>Are there any hacks to disable variable initialization checks? If not, seems like the problem is there's no copy free initialization of variables.</p>", "body_text": "@yaroslavvb @concretevitamin Initializing a >50% GPU memory size array from tf.zeros does not work either.\n@zheng-xq Assuming a reasonable implementation of scatter_update, that would work if any type of initialization worked (or if there wasn't an error thrown while trying to use an uninitialized variable).\nThe following is my attempt to use tf.zeros and scatter_update. If I comment out the sess.run(data.initializer) line, I get an error about using a non-initialized variable. If I don't comment out the line, I get a very similar OOM error message to what I put in the initial issue description.\nimport numpy as np\nimport tensorflow as tf\n\n# allocate 1K x 2M matrix (8GB)                                                                                                             \n_data = np.ones((1 << 10, 1 << 21), dtype=np.float32)\n\ndata = tf.Variable(tf.zeros(_data.shape), trainable=False, collections=[])\n\nidx_ph = tf.placeholder(tf.int32)\ndata_ph = tf.placeholder(tf.float32, shape=[_data.shape[1]])\nscatter_update_op = tf.scatter_update(data, idx_ph, data_ph)\n\nwith tf.Session() as sess:\n    sess.run(data.initializer)\n\n    # assign 1 row at a time                                                                                                                \n    for r in xrange(1 << 10):\n        sess.run(scatter_update_op, feed_dict={idx_ph: r, data_ph: _data[r, :]})\n\n    out = sess.run(data)\nAre there any hacks to disable variable initialization checks? If not, seems like the problem is there's no copy free initialization of variables.", "body": "@yaroslavvb @concretevitamin Initializing a >50% GPU memory size array from `tf.zeros` does not work either.\n\n@zheng-xq Assuming a reasonable implementation of scatter_update, that would work if any type of initialization worked (or if there wasn't an error thrown while trying to use an uninitialized variable).\n\nThe following is my attempt to use tf.zeros and scatter_update. If I comment out the `sess.run(data.initializer)` line, I get an error about using a non-initialized variable. If I don't comment out the line, I get a very similar OOM error message to what I put in the initial issue description.\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\n# allocate 1K x 2M matrix (8GB)                                                                                                             \n_data = np.ones((1 << 10, 1 << 21), dtype=np.float32)\n\ndata = tf.Variable(tf.zeros(_data.shape), trainable=False, collections=[])\n\nidx_ph = tf.placeholder(tf.int32)\ndata_ph = tf.placeholder(tf.float32, shape=[_data.shape[1]])\nscatter_update_op = tf.scatter_update(data, idx_ph, data_ph)\n\nwith tf.Session() as sess:\n    sess.run(data.initializer)\n\n    # assign 1 row at a time                                                                                                                \n    for r in xrange(1 << 10):\n        sess.run(scatter_update_op, feed_dict={idx_ph: r, data_ph: _data[r, :]})\n\n    out = sess.run(data)\n```\n\nAre there any hacks to disable variable initialization checks? If not, seems like the problem is there's no copy free initialization of variables.\n"}