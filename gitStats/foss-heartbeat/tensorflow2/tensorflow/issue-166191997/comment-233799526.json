{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/233799526", "html_url": "https://github.com/tensorflow/tensorflow/issues/3373#issuecomment-233799526", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3373", "id": 233799526, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMzc5OTUyNg==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-19T23:44:51Z", "updated_at": "2016-07-19T23:44:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=592670\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/concretevitamin\">@concretevitamin</a>, the initializer is fundamentally a tf.Assign. So there will be at least two copies of the tensors in that operation, lhs is the variable, and rhs is the initial value. Since TensorFlow didn't get all the GPU memory, it only allocates about 11GB memory. So half of that is about 5.5GB. If you change the size of your variable to (1400 &lt;&lt; (1 &lt;&lt;20)), it should work.</p>\n<p>Short of an in-place initialization kernel, there is no current way to save the second copy of tensor for initial values. In most cases, this is not a problem, since the memory is only temporary, and will be released immediately after the assignment.</p>", "body_text": "@concretevitamin, the initializer is fundamentally a tf.Assign. So there will be at least two copies of the tensors in that operation, lhs is the variable, and rhs is the initial value. Since TensorFlow didn't get all the GPU memory, it only allocates about 11GB memory. So half of that is about 5.5GB. If you change the size of your variable to (1400 << (1 <<20)), it should work.\nShort of an in-place initialization kernel, there is no current way to save the second copy of tensor for initial values. In most cases, this is not a problem, since the memory is only temporary, and will be released immediately after the assignment.", "body": "@concretevitamin, the initializer is fundamentally a tf.Assign. So there will be at least two copies of the tensors in that operation, lhs is the variable, and rhs is the initial value. Since TensorFlow didn't get all the GPU memory, it only allocates about 11GB memory. So half of that is about 5.5GB. If you change the size of your variable to (1400 << (1 <<20)), it should work.\n\nShort of an in-place initialization kernel, there is no current way to save the second copy of tensor for initial values. In most cases, this is not a problem, since the memory is only temporary, and will be released immediately after the assignment. \n"}