{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22825", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22825/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22825/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22825/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22825", "id": 368024621, "node_id": "MDU6SXNzdWUzNjgwMjQ2MjE=", "number": 22825, "title": "Second order derivative not supported for LRN (tf.nn.lrn)", "user": {"login": "vipinpillai", "id": 1018780, "node_id": "MDQ6VXNlcjEwMTg3ODA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1018780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vipinpillai", "html_url": "https://github.com/vipinpillai", "followers_url": "https://api.github.com/users/vipinpillai/followers", "following_url": "https://api.github.com/users/vipinpillai/following{/other_user}", "gists_url": "https://api.github.com/users/vipinpillai/gists{/gist_id}", "starred_url": "https://api.github.com/users/vipinpillai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vipinpillai/subscriptions", "organizations_url": "https://api.github.com/users/vipinpillai/orgs", "repos_url": "https://api.github.com/users/vipinpillai/repos", "events_url": "https://api.github.com/users/vipinpillai/events{/privacy}", "received_events_url": "https://api.github.com/users/vipinpillai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2018-10-09T03:13:36Z", "updated_at": "2018-10-24T20:21:25Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: No</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.11.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:  CUDA 9.0, cuDNN - 7</li>\n<li><strong>GPU model and memory</strong>: Titan X</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I have a network architecture which has a LRN layer (tf.nn.lrn). I am using the same network definition from the CIFAR 10 tutorial: <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py\">https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py</a> .<br>\nI have a loss term which involves the gradient of the cross entropy loss with respect to the input. This throws an error during graph construction:<br>\nLookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)</p>\n<h3>Source code</h3>\n<p>I have reused the same code from the CIFAR10 tutorial with minimal change to the loss function.<br>\nI have modified the loss function to have an additional term which penalizes the L2 norm of the gradient of the cross entropy loss with respect to the input.<br>\nFollowing is the change to the loss function made to the cifar10.py from the tutorial.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">loss</span>(<span class=\"pl-smi\">images</span>, <span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">labels</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Please note that the caller needs to feed the input images as well.</span>\n<span class=\"pl-s\">  Args:</span>\n<span class=\"pl-s\">    images: Input images to be used for gradient computation</span>\n<span class=\"pl-s\">    logits: Logits from inference().</span>\n<span class=\"pl-s\">    labels: Labels from distorted_inputs or inputs(). 1-D tensor</span>\n<span class=\"pl-s\">            of shape [batch_size]</span>\n<span class=\"pl-s\">  Returns:</span>\n<span class=\"pl-s\">    Loss tensor of type float.</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n  labels_onehot <span class=\"pl-k\">=</span> tf.one_hot(labels, <span class=\"pl-v\">depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">off_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">on_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n  cross_entropy <span class=\"pl-k\">=</span> tf.nn.softmax_cross_entropy_with_logits(\n      <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels_onehot, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cross_entropy_per_example<span class=\"pl-pds\">'</span></span>)\n  cross_entropy_mean <span class=\"pl-k\">=</span> tf.reduce_mean(cross_entropy, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cross_entropy<span class=\"pl-pds\">'</span></span>)\n  \n  cross_entropy_grads, <span class=\"pl-k\">=</span> tf.gradients(cross_entropy_mean, images)\n  xent_grad_norm <span class=\"pl-k\">=</span> tf.nn.l2_loss(cross_entropy_grads)\n\n  tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>losses<span class=\"pl-pds\">'</span></span>, cross_entropy_mean)\n  tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>losses<span class=\"pl-pds\">'</span></span>, xent_grad_norm)\n\n  <span class=\"pl-k\">return</span> tf.add_n(tf.get_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>losses<span class=\"pl-pds\">'</span></span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>total_loss<span class=\"pl-pds\">'</span></span>)</pre></div>\n<h3>Logs:</h3>\n<pre><code>Traceback (most recent call last):\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 730, in _GradientsHelper\n    grad_fn = ops.get_gradient_function(op)\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2460, in get_gradient_function\n    return _gradient_registry.lookup(op_type)\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\", line 93, in lookup\n    \"%s registry has no entry for: %s\" % (self._name, name))\nLookupError: gradient registry has no entry for: LRNGrad\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n..............................................\n..............................................\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 734, in _GradientsHelper\n    (op.name, op.type))\nLookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.11.0\nPython version: 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:  CUDA 9.0, cuDNN - 7\nGPU model and memory: Titan X\nExact command to reproduce:\n\nDescribe the problem\nI have a network architecture which has a LRN layer (tf.nn.lrn). I am using the same network definition from the CIFAR 10 tutorial: https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py .\nI have a loss term which involves the gradient of the cross entropy loss with respect to the input. This throws an error during graph construction:\nLookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)\nSource code\nI have reused the same code from the CIFAR10 tutorial with minimal change to the loss function.\nI have modified the loss function to have an additional term which penalizes the L2 norm of the gradient of the cross entropy loss with respect to the input.\nFollowing is the change to the loss function made to the cifar10.py from the tutorial.\ndef loss(images, logits, labels):\n  \"\"\"Please note that the caller needs to feed the input images as well.\n  Args:\n    images: Input images to be used for gradient computation\n    logits: Logits from inference().\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n  Returns:\n    Loss tensor of type float.\n  \"\"\"\n  labels_onehot = tf.one_hot(labels, depth=10, off_value=0.0, on_value=1.0, dtype=tf.float32)\n  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n      labels=labels_onehot, logits=logits, name='cross_entropy_per_example')\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n  \n  cross_entropy_grads, = tf.gradients(cross_entropy_mean, images)\n  xent_grad_norm = tf.nn.l2_loss(cross_entropy_grads)\n\n  tf.add_to_collection('losses', cross_entropy_mean)\n  tf.add_to_collection('losses', xent_grad_norm)\n\n  return tf.add_n(tf.get_collection('losses'), name='total_loss')\nLogs:\nTraceback (most recent call last):\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 730, in _GradientsHelper\n    grad_fn = ops.get_gradient_function(op)\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2460, in get_gradient_function\n    return _gradient_registry.lookup(op_type)\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\", line 93, in lookup\n    \"%s registry has no entry for: %s\" % (self._name, name))\nLookupError: gradient registry has no entry for: LRNGrad\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n..............................................\n..............................................\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 734, in _GradientsHelper\n    (op.name, op.type))\nLookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: binary \r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  CUDA 9.0, cuDNN - 7\r\n- **GPU model and memory**: Titan X\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a network architecture which has a LRN layer (tf.nn.lrn). I am using the same network definition from the CIFAR 10 tutorial: https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py .\r\nI have a loss term which involves the gradient of the cross entropy loss with respect to the input. This throws an error during graph construction:\r\nLookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)\r\n\r\n\r\n### Source code\r\nI have reused the same code from the CIFAR10 tutorial with minimal change to the loss function.\r\nI have modified the loss function to have an additional term which penalizes the L2 norm of the gradient of the cross entropy loss with respect to the input.\r\nFollowing is the change to the loss function made to the cifar10.py from the tutorial.\r\n```python\r\ndef loss(images, logits, labels):\r\n  \"\"\"Please note that the caller needs to feed the input images as well.\r\n  Args:\r\n    images: Input images to be used for gradient computation\r\n    logits: Logits from inference().\r\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\r\n            of shape [batch_size]\r\n  Returns:\r\n    Loss tensor of type float.\r\n  \"\"\"\r\n  labels_onehot = tf.one_hot(labels, depth=10, off_value=0.0, on_value=1.0, dtype=tf.float32)\r\n  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\r\n      labels=labels_onehot, logits=logits, name='cross_entropy_per_example')\r\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\r\n  \r\n  cross_entropy_grads, = tf.gradients(cross_entropy_mean, images)\r\n  xent_grad_norm = tf.nn.l2_loss(cross_entropy_grads)\r\n\r\n  tf.add_to_collection('losses', cross_entropy_mean)\r\n  tf.add_to_collection('losses', xent_grad_norm)\r\n\r\n  return tf.add_n(tf.get_collection('losses'), name='total_loss')\r\n```\r\n### Logs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 730, in _GradientsHelper\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2460, in get_gradient_function\r\n    return _gradient_registry.lookup(op_type)\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\", line 93, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: LRNGrad\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n..............................................\r\n..............................................\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 734, in _GradientsHelper\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)\r\n```\r\n\r\n"}