{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/431520793", "html_url": "https://github.com/tensorflow/tensorflow/issues/22825#issuecomment-431520793", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22825", "id": 431520793, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTUyMDc5Mw==", "user": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-19T22:50:35Z", "updated_at": "2018-10-19T22:50:35Z", "author_association": "MEMBER", "body_html": "<p>Yes exactly. You would register an LRNGradGrad op here: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L684\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L684</a></p>\n<p>And then implement the kernel here: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lrn_op.cc\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lrn_op.cc</a></p>\n<p>And then register a Python gradient function here: <a href=\"http://go/gh/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L579\" rel=\"nofollow\">http://go/gh/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L579</a></p>\n<p>I'm not familiar with the specifics of LRN, so I can't really give more context than that. It looks like the current LRN kernels are pretty fancy so this might be tricky, although it's possible you can make a simpler GradGrad kernel that's not as performant but still works.</p>\n<p>Alternatively, you can possibly implement the gradient entirely in the Python gradient function using lower-level ops (i.e. create the grad grad computation using the standard TF API). This would have the advantage that it could in turn be differentiated (assuming you use ops that have gradients defined), and might be easier to implement.</p>\n<p>Please let me know if you have any questions!</p>", "body_text": "Yes exactly. You would register an LRNGradGrad op here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L684\nAnd then implement the kernel here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lrn_op.cc\nAnd then register a Python gradient function here: http://go/gh/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L579\nI'm not familiar with the specifics of LRN, so I can't really give more context than that. It looks like the current LRN kernels are pretty fancy so this might be tricky, although it's possible you can make a simpler GradGrad kernel that's not as performant but still works.\nAlternatively, you can possibly implement the gradient entirely in the Python gradient function using lower-level ops (i.e. create the grad grad computation using the standard TF API). This would have the advantage that it could in turn be differentiated (assuming you use ops that have gradients defined), and might be easier to implement.\nPlease let me know if you have any questions!", "body": "Yes exactly. You would register an LRNGradGrad op here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L684\r\n\r\nAnd then implement the kernel here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lrn_op.cc\r\n\r\nAnd then register a Python gradient function here: http://go/gh/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L579\r\n\r\nI'm not familiar with the specifics of LRN, so I can't really give more context than that. It looks like the current LRN kernels are pretty fancy so this might be tricky, although it's possible you can make a simpler GradGrad kernel that's not as performant but still works.\r\n\r\nAlternatively, you can possibly implement the gradient entirely in the Python gradient function using lower-level ops (i.e. create the grad grad computation using the standard TF API). This would have the advantage that it could in turn be differentiated (assuming you use ops that have gradients defined), and might be easier to implement.\r\n\r\nPlease let me know if you have any questions!"}