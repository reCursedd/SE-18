{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10222", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10222/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10222/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10222/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10222", "id": 231663167, "node_id": "MDU6SXNzdWUyMzE2NjMxNjc=", "number": 10222, "title": "tf.nn.moments with tf.concat numerical ambiguity", "user": {"login": "hunsteve", "id": 15277286, "node_id": "MDQ6VXNlcjE1Mjc3Mjg2", "avatar_url": "https://avatars0.githubusercontent.com/u/15277286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hunsteve", "html_url": "https://github.com/hunsteve", "followers_url": "https://api.github.com/users/hunsteve/followers", "following_url": "https://api.github.com/users/hunsteve/following{/other_user}", "gists_url": "https://api.github.com/users/hunsteve/gists{/gist_id}", "starred_url": "https://api.github.com/users/hunsteve/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hunsteve/subscriptions", "organizations_url": "https://api.github.com/users/hunsteve/orgs", "repos_url": "https://api.github.com/users/hunsteve/repos", "events_url": "https://api.github.com/users/hunsteve/events{/privacy}", "received_events_url": "https://api.github.com/users/hunsteve/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-05-26T15:41:33Z", "updated_at": "2017-06-16T18:47:00Z", "closed_at": "2017-06-16T18:47:00Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: custom code, see below</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04 LTE</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')</li>\n<li><strong>Bazel version (if compiling from source)</strong>: n/a</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA: 8.0 / cuDNN: 5.1</li>\n<li><strong>GPU model and memory</strong>: GTX960m 4GB and GTX1080 8GB</li>\n<li><strong>Exact command to reproduce</strong>: run the code below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p><strong>tf.nn.moments</strong> GPU version produces different mean and variance values for numerically same input tensors. The only difference between the inputs is that they are the outputs of one and two <strong>tf.concat</strong> operations (see the code below). CPU version works well.</p>\n<p>The bad output is:</p>\n<pre><code>input diff:0.0\nmean diff:2.98023223877e-08\nvar diff:7.45058059692e-09\n</code></pre>\n<p>The correct output should be:</p>\n<pre><code>input diff:0.0\nmean diff:0.0\nvar diff:0.0\n</code></pre>\n<h3>Source code / logs</h3>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nwith tf.device(\"/gpu:0\"):\n    input = tf.placeholder(shape=[16, 4, 4, 8], dtype=tf.float32)\n    input1 = tf.concat([input], axis=0)\n    input2 = tf.concat([tf.concat([input], axis=0)], axis=0)\n\n    mean1, var1 = tf.nn.moments(input1, axes=[0,1,2])\n    mean2, var2 = tf.nn.moments(input2, axes=[0,1,2])\n\n    input_diff_max = tf.reduce_max(tf.abs(input1 - input2))\n    mean_diff_max = tf.reduce_max(tf.abs(mean1 - mean2))\n    var_diff_max = tf.reduce_max(tf.abs(var1 - var2))\n\n    with tf.Session() as sess:\n        i_v, m_v, v_v = sess.run([input_diff_max, mean_diff_max, var_diff_max], feed_dict={input: np.random.rand(16, 4, 4, 8)})\n        print(\"input diff:{}\".format(i_v))\n        print(\"mean diff:{}\".format(m_v))\n        print(\"var diff:{}\".format(v_v))\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code, see below\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTE\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\nBazel version (if compiling from source): n/a\nCUDA/cuDNN version: CUDA: 8.0 / cuDNN: 5.1\nGPU model and memory: GTX960m 4GB and GTX1080 8GB\nExact command to reproduce: run the code below\n\nDescribe the problem\ntf.nn.moments GPU version produces different mean and variance values for numerically same input tensors. The only difference between the inputs is that they are the outputs of one and two tf.concat operations (see the code below). CPU version works well.\nThe bad output is:\ninput diff:0.0\nmean diff:2.98023223877e-08\nvar diff:7.45058059692e-09\n\nThe correct output should be:\ninput diff:0.0\nmean diff:0.0\nvar diff:0.0\n\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\n\nwith tf.device(\"/gpu:0\"):\n    input = tf.placeholder(shape=[16, 4, 4, 8], dtype=tf.float32)\n    input1 = tf.concat([input], axis=0)\n    input2 = tf.concat([tf.concat([input], axis=0)], axis=0)\n\n    mean1, var1 = tf.nn.moments(input1, axes=[0,1,2])\n    mean2, var2 = tf.nn.moments(input2, axes=[0,1,2])\n\n    input_diff_max = tf.reduce_max(tf.abs(input1 - input2))\n    mean_diff_max = tf.reduce_max(tf.abs(mean1 - mean2))\n    var_diff_max = tf.reduce_max(tf.abs(var1 - var2))\n\n    with tf.Session() as sess:\n        i_v, m_v, v_v = sess.run([input_diff_max, mean_diff_max, var_diff_max], feed_dict={input: np.random.rand(16, 4, 4, 8)})\n        print(\"input diff:{}\".format(i_v))\n        print(\"mean diff:{}\".format(m_v))\n        print(\"var diff:{}\".format(v_v))", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTE\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: CUDA: 8.0 / cuDNN: 5.1\r\n- **GPU model and memory**: GTX960m 4GB and GTX1080 8GB\r\n- **Exact command to reproduce**: run the code below\r\n\r\n### Describe the problem\r\n**tf.nn.moments** GPU version produces different mean and variance values for numerically same input tensors. The only difference between the inputs is that they are the outputs of one and two **tf.concat** operations (see the code below). CPU version works well.\r\n\r\nThe bad output is:\r\n```\r\ninput diff:0.0\r\nmean diff:2.98023223877e-08\r\nvar diff:7.45058059692e-09\r\n```\r\nThe correct output should be:\r\n```\r\ninput diff:0.0\r\nmean diff:0.0\r\nvar diff:0.0\r\n```\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    input = tf.placeholder(shape=[16, 4, 4, 8], dtype=tf.float32)\r\n    input1 = tf.concat([input], axis=0)\r\n    input2 = tf.concat([tf.concat([input], axis=0)], axis=0)\r\n\r\n    mean1, var1 = tf.nn.moments(input1, axes=[0,1,2])\r\n    mean2, var2 = tf.nn.moments(input2, axes=[0,1,2])\r\n\r\n    input_diff_max = tf.reduce_max(tf.abs(input1 - input2))\r\n    mean_diff_max = tf.reduce_max(tf.abs(mean1 - mean2))\r\n    var_diff_max = tf.reduce_max(tf.abs(var1 - var2))\r\n\r\n    with tf.Session() as sess:\r\n        i_v, m_v, v_v = sess.run([input_diff_max, mean_diff_max, var_diff_max], feed_dict={input: np.random.rand(16, 4, 4, 8)})\r\n        print(\"input diff:{}\".format(i_v))\r\n        print(\"mean diff:{}\".format(m_v))\r\n        print(\"var diff:{}\".format(v_v))\r\n```\r\n"}