{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9696", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9696/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9696/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9696/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9696", "id": 226650147, "node_id": "MDU6SXNzdWUyMjY2NTAxNDc=", "number": 9696, "title": "Epoch is not working properly for tensorflow keras -  KerasRegressor", "user": {"login": "KishoreKarunakaran", "id": 10724627, "node_id": "MDQ6VXNlcjEwNzI0NjI3", "avatar_url": "https://avatars3.githubusercontent.com/u/10724627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KishoreKarunakaran", "html_url": "https://github.com/KishoreKarunakaran", "followers_url": "https://api.github.com/users/KishoreKarunakaran/followers", "following_url": "https://api.github.com/users/KishoreKarunakaran/following{/other_user}", "gists_url": "https://api.github.com/users/KishoreKarunakaran/gists{/gist_id}", "starred_url": "https://api.github.com/users/KishoreKarunakaran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KishoreKarunakaran/subscriptions", "organizations_url": "https://api.github.com/users/KishoreKarunakaran/orgs", "repos_url": "https://api.github.com/users/KishoreKarunakaran/repos", "events_url": "https://api.github.com/users/KishoreKarunakaran/events{/privacy}", "received_events_url": "https://api.github.com/users/KishoreKarunakaran/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2017-05-05T18:09:58Z", "updated_at": "2017-05-08T04:40:02Z", "closed_at": "2017-05-05T20:30:53Z", "author_association": "NONE", "body_html": "<p>I'm just getting start with Keras using tensorflow, In the code code I have specified <strong>nb_epoch=100</strong> but it's not running for 100 epoch, instead it's executing only for 10 epoch. I wonder is there any bug in my code. Also attaching the output in the bottom</p>\n<p><strong>Code:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> matplotlib.pyplot <span class=\"pl-k\">as</span> plt\n<span class=\"pl-k\">import</span> pandas <span class=\"pl-k\">as</span> pd\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> sklearn.model_selection <span class=\"pl-k\">import</span> train_test_split\n<span class=\"pl-k\">import</span> sklearn.metrics <span class=\"pl-k\">as</span> metrics\n<span class=\"pl-k\">from</span> utils <span class=\"pl-k\">import</span> encode_numeric_zscore, to_xy\n\ndataset <span class=\"pl-k\">=</span> pd.read_csv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>data/boston-housing.csv<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n\n<span class=\"pl-k\">for</span> col <span class=\"pl-k\">in</span> dataset.columns:\n    <span class=\"pl-k\">if</span> col <span class=\"pl-k\">!=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>medv<span class=\"pl-pds\">'</span></span>:\n        encode_numeric_zscore(dataset, col)\n\nfeatures, target <span class=\"pl-k\">=</span> to_xy(dataset, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>medv<span class=\"pl-pds\">'</span></span>)\n\nX_train, X_test, Y_train, Y_test <span class=\"pl-k\">=</span> train_test_split(features, target, <span class=\"pl-v\">test_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.25</span>, <span class=\"pl-v\">random_state</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">42</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">chart_regression</span>(<span class=\"pl-smi\">pred</span>, <span class=\"pl-smi\">y</span>):\n    t <span class=\"pl-k\">=</span> pd.DataFrame({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pred<span class=\"pl-pds\">'</span></span>: pred, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>: Y_test.flatten()})\n    t.sort_values(<span class=\"pl-v\">by</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>], <span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    a <span class=\"pl-k\">=</span> plt.plot(t[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>].tolist(), <span class=\"pl-v\">label</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>expected<span class=\"pl-pds\">'</span></span>)\n    b <span class=\"pl-k\">=</span> plt.plot(t[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pred<span class=\"pl-pds\">'</span></span>].tolist(), <span class=\"pl-v\">label</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>prediction<span class=\"pl-pds\">'</span></span>)\n    plt.ylabel(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>output<span class=\"pl-pds\">'</span></span>)\n    plt.legend()\n    plt.show()\n\n\n<span class=\"pl-k\">from</span> tensorflow.contrib.keras.api.keras.models <span class=\"pl-k\">import</span> Sequential\n<span class=\"pl-k\">from</span> tensorflow.contrib.keras.api.keras.layers <span class=\"pl-k\">import</span> Dense, Dropout\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build_nn</span>():\n    model <span class=\"pl-k\">=</span> Sequential()\n    model.add(Dense(<span class=\"pl-c1\">75</span>, <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">13</span>,), <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>glorot_uniform<span class=\"pl-pds\">\"</span></span>))\n    model.add(Dense(<span class=\"pl-c1\">55</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>))\n    model.add(Dropout(<span class=\"pl-c1\">0.005</span>))\n    model.add(Dense(<span class=\"pl-c1\">35</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>))\n    model.add(Dropout(<span class=\"pl-c1\">0.005</span>))\n    model.add(Dense(<span class=\"pl-c1\">11</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>))\n    model.add(Dropout(<span class=\"pl-c1\">0.005</span>))\n    model.add(Dense(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>normal<span class=\"pl-pds\">\"</span></span>))\n    model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>mean_squared_error<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>adam<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> model\n\n\nseed <span class=\"pl-k\">=</span> <span class=\"pl-c1\">7</span>\nnp.random.seed(seed)\n\n<span class=\"pl-k\">from</span> tensorflow.contrib.keras.api.keras.wrappers.scikit_learn <span class=\"pl-k\">import</span> KerasRegressor\n\nregressor <span class=\"pl-k\">=</span> KerasRegressor(<span class=\"pl-v\">build_fn</span><span class=\"pl-k\">=</span>build_nn, <span class=\"pl-v\">nb_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\nregressor.fit(X_train, Y_train)\n\nprediction <span class=\"pl-k\">=</span> regressor.predict(X_test, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\nscore <span class=\"pl-k\">=</span> metrics.mean_squared_error(Y_test, prediction)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Final score (MSE): <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(score))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n\nscore <span class=\"pl-k\">=</span> np.sqrt(metrics.mean_squared_error(prediction, Y_test))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Final score (RMSE): <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(score))\n\nchart_regression(prediction, Y_test)</pre></div>\n<p><strong>Output:</strong></p>\n<pre><code>Epoch 1/10\n2017-05-05 15:30:37.711608: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.711981: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.712355: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.712721: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.713094: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.713463: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.713839: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.714213: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n  1/379 [..............................] - ETA: 107s - loss: 492.8400\n 60/379 [===&gt;..........................] - ETA: 1s - loss: 669.9998  \n109/379 [=======&gt;......................] - ETA: 0s - loss: 625.8113\n172/379 [============&gt;.................] - ETA: 0s - loss: 461.0169\n233/379 [=================&gt;............] - ETA: 0s - loss: 353.9863\n286/379 [=====================&gt;........] - ETA: 0s - loss: 294.3637\n336/379 [=========================&gt;....] - ETA: 0s - loss: 254.4181\n379/379 [==============================] - 0s - loss: 230.0349     \nEpoch 2/10\n  1/379 [..............................] - ETA: 0s - loss: 8.0337\n 55/379 [===&gt;..........................] - ETA: 0s - loss: 26.7809\n104/379 [=======&gt;......................] - ETA: 0s - loss: 22.0314\n171/379 [============&gt;.................] - ETA: 0s - loss: 25.4361\n219/379 [================&gt;.............] - ETA: 0s - loss: 28.0947\n285/379 [=====================&gt;........] - ETA: 0s - loss: 26.3540\n338/379 [=========================&gt;....] - ETA: 0s - loss: 26.1902\n379/379 [==============================] - 0s - loss: 24.4095     \nEpoch 3/10\n  1/379 [..............................] - ETA: 0s - loss: 9.8365\n 42/379 [==&gt;...........................] - ETA: 0s - loss: 20.2841\n108/379 [=======&gt;......................] - ETA: 0s - loss: 16.0308\n161/379 [===========&gt;..................] - ETA: 0s - loss: 18.4792\n225/379 [================&gt;.............] - ETA: 0s - loss: 18.9558\n288/379 [=====================&gt;........] - ETA: 0s - loss: 16.6939\n379/379 [==============================] - 0s - loss: 18.3086     \nEpoch 4/10\n  1/379 [..............................] - ETA: 0s - loss: 13.7463\n 59/379 [===&gt;..........................] - ETA: 0s - loss: 12.0900\n115/379 [========&gt;.....................] - ETA: 0s - loss: 18.4202\n181/379 [=============&gt;................] - ETA: 0s - loss: 16.6087\n244/379 [==================&gt;...........] - ETA: 0s - loss: 15.0061\n302/379 [======================&gt;.......] - ETA: 0s - loss: 13.2268\n379/379 [==============================] - 0s - loss: 16.0613     \nEpoch 5/10\n  1/379 [..............................] - ETA: 0s - loss: 25.8912\n 65/379 [====&gt;.........................] - ETA: 0s - loss: 28.8039\n123/379 [========&gt;.....................] - ETA: 0s - loss: 21.3528\n199/379 [==============&gt;...............] - ETA: 0s - loss: 17.8586\n256/379 [===================&gt;..........] - ETA: 0s - loss: 19.4171\n327/379 [========================&gt;.....] - ETA: 0s - loss: 17.9864\n379/379 [==============================] - 0s - loss: 18.2714     \nEpoch 6/10\n  1/379 [..............................] - ETA: 0s - loss: 2.0872\n 61/379 [===&gt;..........................] - ETA: 0s - loss: 18.4408\n115/379 [========&gt;.....................] - ETA: 0s - loss: 16.9127\n184/379 [=============&gt;................] - ETA: 0s - loss: 16.9419\n247/379 [==================&gt;...........] - ETA: 0s - loss: 16.2630\n314/379 [=======================&gt;......] - ETA: 0s - loss: 14.9947\n379/379 [==============================] - 0s - loss: 13.5761     \nEpoch 7/10\n  1/379 [..............................] - ETA: 0s - loss: 0.3704\n 59/379 [===&gt;..........................] - ETA: 0s - loss: 15.2191\n118/379 [========&gt;.....................] - ETA: 0s - loss: 11.8381\n188/379 [=============&gt;................] - ETA: 0s - loss: 10.9623\n247/379 [==================&gt;...........] - ETA: 0s - loss: 10.0409\n313/379 [=======================&gt;......] - ETA: 0s - loss: 12.4478\n379/379 [==============================] - 0s - loss: 13.8573     \nEpoch 8/10\n  1/379 [..............................] - ETA: 0s - loss: 1.2173\n 44/379 [==&gt;...........................] - ETA: 0s - loss: 12.5116\n 96/379 [======&gt;.......................] - ETA: 0s - loss: 10.2578\n170/379 [============&gt;.................] - ETA: 0s - loss: 11.9057\n227/379 [================&gt;.............] - ETA: 0s - loss: 12.1961\n287/379 [=====================&gt;........] - ETA: 0s - loss: 14.4951\n341/379 [=========================&gt;....] - ETA: 0s - loss: 13.7772\n379/379 [==============================] - 0s - loss: 13.2340     \nEpoch 9/10\n  1/379 [..............................] - ETA: 0s - loss: 0.1982\n 53/379 [===&gt;..........................] - ETA: 0s - loss: 9.8250\n124/379 [========&gt;.....................] - ETA: 0s - loss: 9.8112\n175/379 [============&gt;.................] - ETA: 0s - loss: 10.9075\n238/379 [=================&gt;............] - ETA: 0s - loss: 11.3589\n303/379 [======================&gt;.......] - ETA: 0s - loss: 13.8970\n379/379 [==============================] - 0s - loss: 14.5469     \nEpoch 10/10\n  1/379 [..............................] - ETA: 0s - loss: 3.7164\n 49/379 [==&gt;...........................] - ETA: 0s - loss: 14.6596\n116/379 [========&gt;.....................] - ETA: 0s - loss: 15.5960\n172/379 [============&gt;.................] - ETA: 0s - loss: 13.6525\n223/379 [================&gt;.............] - ETA: 0s - loss: 12.9588\n275/379 [====================&gt;.........] - ETA: 0s - loss: 12.4905\n347/379 [==========================&gt;...] - ETA: 0s - loss: 13.5629\n379/379 [==============================] - 0s - loss: 13.1696     \n  1/127 [..............................] - ETA: 1s\n\nFinal score (MSE): 11.964310646057129\n\n\nFinal score (RMSE): 3.458946466445923\n\n</code></pre>", "body_text": "I'm just getting start with Keras using tensorflow, In the code code I have specified nb_epoch=100 but it's not running for 100 epoch, instead it's executing only for 10 epoch. I wonder is there any bug in my code. Also attaching the output in the bottom\nCode:\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom utils import encode_numeric_zscore, to_xy\n\ndataset = pd.read_csv('data/boston-housing.csv', dtype=np.float32)\n\nfor col in dataset.columns:\n    if col != 'medv':\n        encode_numeric_zscore(dataset, col)\n\nfeatures, target = to_xy(dataset, 'medv')\n\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.25, random_state=42)\n\n\ndef chart_regression(pred, y):\n    t = pd.DataFrame({'pred': pred, 'y': Y_test.flatten()})\n    t.sort_values(by=['y'], inplace=True)\n    a = plt.plot(t['y'].tolist(), label='expected')\n    b = plt.plot(t['pred'].tolist(), label='prediction')\n    plt.ylabel('output')\n    plt.legend()\n    plt.show()\n\n\nfrom tensorflow.contrib.keras.api.keras.models import Sequential\nfrom tensorflow.contrib.keras.api.keras.layers import Dense, Dropout\n\n\ndef build_nn():\n    model = Sequential()\n    model.add(Dense(75, input_shape=(13,), activation=\"relu\", kernel_initializer=\"glorot_uniform\"))\n    model.add(Dense(55, activation=\"relu\"))\n    model.add(Dropout(0.005))\n    model.add(Dense(35, activation=\"relu\"))\n    model.add(Dropout(0.005))\n    model.add(Dense(11, activation=\"relu\"))\n    model.add(Dropout(0.005))\n    model.add(Dense(1, activation='relu', kernel_initializer=\"normal\"))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\n\nseed = 7\nnp.random.seed(seed)\n\nfrom tensorflow.contrib.keras.api.keras.wrappers.scikit_learn import KerasRegressor\n\nregressor = KerasRegressor(build_fn=build_nn, nb_epoch=100, batch_size=1, verbose=1)\n\nregressor.fit(X_train, Y_train)\n\nprediction = regressor.predict(X_test, batch_size=1)\n\nscore = metrics.mean_squared_error(Y_test, prediction)\n\nprint('\\n')\n\nprint(\"Final score (MSE): {}\".format(score))\n\nprint('\\n')\n\nscore = np.sqrt(metrics.mean_squared_error(prediction, Y_test))\nprint(\"Final score (RMSE): {}\".format(score))\n\nchart_regression(prediction, Y_test)\nOutput:\nEpoch 1/10\n2017-05-05 15:30:37.711608: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.711981: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.712355: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.712721: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.713094: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.713463: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.713839: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-05 15:30:37.714213: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n  1/379 [..............................] - ETA: 107s - loss: 492.8400\n 60/379 [===>..........................] - ETA: 1s - loss: 669.9998  \n109/379 [=======>......................] - ETA: 0s - loss: 625.8113\n172/379 [============>.................] - ETA: 0s - loss: 461.0169\n233/379 [=================>............] - ETA: 0s - loss: 353.9863\n286/379 [=====================>........] - ETA: 0s - loss: 294.3637\n336/379 [=========================>....] - ETA: 0s - loss: 254.4181\n379/379 [==============================] - 0s - loss: 230.0349     \nEpoch 2/10\n  1/379 [..............................] - ETA: 0s - loss: 8.0337\n 55/379 [===>..........................] - ETA: 0s - loss: 26.7809\n104/379 [=======>......................] - ETA: 0s - loss: 22.0314\n171/379 [============>.................] - ETA: 0s - loss: 25.4361\n219/379 [================>.............] - ETA: 0s - loss: 28.0947\n285/379 [=====================>........] - ETA: 0s - loss: 26.3540\n338/379 [=========================>....] - ETA: 0s - loss: 26.1902\n379/379 [==============================] - 0s - loss: 24.4095     \nEpoch 3/10\n  1/379 [..............................] - ETA: 0s - loss: 9.8365\n 42/379 [==>...........................] - ETA: 0s - loss: 20.2841\n108/379 [=======>......................] - ETA: 0s - loss: 16.0308\n161/379 [===========>..................] - ETA: 0s - loss: 18.4792\n225/379 [================>.............] - ETA: 0s - loss: 18.9558\n288/379 [=====================>........] - ETA: 0s - loss: 16.6939\n379/379 [==============================] - 0s - loss: 18.3086     \nEpoch 4/10\n  1/379 [..............................] - ETA: 0s - loss: 13.7463\n 59/379 [===>..........................] - ETA: 0s - loss: 12.0900\n115/379 [========>.....................] - ETA: 0s - loss: 18.4202\n181/379 [=============>................] - ETA: 0s - loss: 16.6087\n244/379 [==================>...........] - ETA: 0s - loss: 15.0061\n302/379 [======================>.......] - ETA: 0s - loss: 13.2268\n379/379 [==============================] - 0s - loss: 16.0613     \nEpoch 5/10\n  1/379 [..............................] - ETA: 0s - loss: 25.8912\n 65/379 [====>.........................] - ETA: 0s - loss: 28.8039\n123/379 [========>.....................] - ETA: 0s - loss: 21.3528\n199/379 [==============>...............] - ETA: 0s - loss: 17.8586\n256/379 [===================>..........] - ETA: 0s - loss: 19.4171\n327/379 [========================>.....] - ETA: 0s - loss: 17.9864\n379/379 [==============================] - 0s - loss: 18.2714     \nEpoch 6/10\n  1/379 [..............................] - ETA: 0s - loss: 2.0872\n 61/379 [===>..........................] - ETA: 0s - loss: 18.4408\n115/379 [========>.....................] - ETA: 0s - loss: 16.9127\n184/379 [=============>................] - ETA: 0s - loss: 16.9419\n247/379 [==================>...........] - ETA: 0s - loss: 16.2630\n314/379 [=======================>......] - ETA: 0s - loss: 14.9947\n379/379 [==============================] - 0s - loss: 13.5761     \nEpoch 7/10\n  1/379 [..............................] - ETA: 0s - loss: 0.3704\n 59/379 [===>..........................] - ETA: 0s - loss: 15.2191\n118/379 [========>.....................] - ETA: 0s - loss: 11.8381\n188/379 [=============>................] - ETA: 0s - loss: 10.9623\n247/379 [==================>...........] - ETA: 0s - loss: 10.0409\n313/379 [=======================>......] - ETA: 0s - loss: 12.4478\n379/379 [==============================] - 0s - loss: 13.8573     \nEpoch 8/10\n  1/379 [..............................] - ETA: 0s - loss: 1.2173\n 44/379 [==>...........................] - ETA: 0s - loss: 12.5116\n 96/379 [======>.......................] - ETA: 0s - loss: 10.2578\n170/379 [============>.................] - ETA: 0s - loss: 11.9057\n227/379 [================>.............] - ETA: 0s - loss: 12.1961\n287/379 [=====================>........] - ETA: 0s - loss: 14.4951\n341/379 [=========================>....] - ETA: 0s - loss: 13.7772\n379/379 [==============================] - 0s - loss: 13.2340     \nEpoch 9/10\n  1/379 [..............................] - ETA: 0s - loss: 0.1982\n 53/379 [===>..........................] - ETA: 0s - loss: 9.8250\n124/379 [========>.....................] - ETA: 0s - loss: 9.8112\n175/379 [============>.................] - ETA: 0s - loss: 10.9075\n238/379 [=================>............] - ETA: 0s - loss: 11.3589\n303/379 [======================>.......] - ETA: 0s - loss: 13.8970\n379/379 [==============================] - 0s - loss: 14.5469     \nEpoch 10/10\n  1/379 [..............................] - ETA: 0s - loss: 3.7164\n 49/379 [==>...........................] - ETA: 0s - loss: 14.6596\n116/379 [========>.....................] - ETA: 0s - loss: 15.5960\n172/379 [============>.................] - ETA: 0s - loss: 13.6525\n223/379 [================>.............] - ETA: 0s - loss: 12.9588\n275/379 [====================>.........] - ETA: 0s - loss: 12.4905\n347/379 [==========================>...] - ETA: 0s - loss: 13.5629\n379/379 [==============================] - 0s - loss: 13.1696     \n  1/127 [..............................] - ETA: 1s\n\nFinal score (MSE): 11.964310646057129\n\n\nFinal score (RMSE): 3.458946466445923", "body": "I'm just getting start with Keras using tensorflow, In the code code I have specified **nb_epoch=100** but it's not running for 100 epoch, instead it's executing only for 10 epoch. I wonder is there any bug in my code. Also attaching the output in the bottom\r\n\r\n**Code:**\r\n\r\n```python\r\n\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nimport sklearn.metrics as metrics\r\nfrom utils import encode_numeric_zscore, to_xy\r\n\r\ndataset = pd.read_csv('data/boston-housing.csv', dtype=np.float32)\r\n\r\nfor col in dataset.columns:\r\n    if col != 'medv':\r\n        encode_numeric_zscore(dataset, col)\r\n\r\nfeatures, target = to_xy(dataset, 'medv')\r\n\r\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.25, random_state=42)\r\n\r\n\r\ndef chart_regression(pred, y):\r\n    t = pd.DataFrame({'pred': pred, 'y': Y_test.flatten()})\r\n    t.sort_values(by=['y'], inplace=True)\r\n    a = plt.plot(t['y'].tolist(), label='expected')\r\n    b = plt.plot(t['pred'].tolist(), label='prediction')\r\n    plt.ylabel('output')\r\n    plt.legend()\r\n    plt.show()\r\n\r\n\r\nfrom tensorflow.contrib.keras.api.keras.models import Sequential\r\nfrom tensorflow.contrib.keras.api.keras.layers import Dense, Dropout\r\n\r\n\r\ndef build_nn():\r\n    model = Sequential()\r\n    model.add(Dense(75, input_shape=(13,), activation=\"relu\", kernel_initializer=\"glorot_uniform\"))\r\n    model.add(Dense(55, activation=\"relu\"))\r\n    model.add(Dropout(0.005))\r\n    model.add(Dense(35, activation=\"relu\"))\r\n    model.add(Dropout(0.005))\r\n    model.add(Dense(11, activation=\"relu\"))\r\n    model.add(Dropout(0.005))\r\n    model.add(Dense(1, activation='relu', kernel_initializer=\"normal\"))\r\n    model.compile(loss='mean_squared_error', optimizer='adam')\r\n    return model\r\n\r\n\r\nseed = 7\r\nnp.random.seed(seed)\r\n\r\nfrom tensorflow.contrib.keras.api.keras.wrappers.scikit_learn import KerasRegressor\r\n\r\nregressor = KerasRegressor(build_fn=build_nn, nb_epoch=100, batch_size=1, verbose=1)\r\n\r\nregressor.fit(X_train, Y_train)\r\n\r\nprediction = regressor.predict(X_test, batch_size=1)\r\n\r\nscore = metrics.mean_squared_error(Y_test, prediction)\r\n\r\nprint('\\n')\r\n\r\nprint(\"Final score (MSE): {}\".format(score))\r\n\r\nprint('\\n')\r\n\r\nscore = np.sqrt(metrics.mean_squared_error(prediction, Y_test))\r\nprint(\"Final score (RMSE): {}\".format(score))\r\n\r\nchart_regression(prediction, Y_test)\r\n```\r\n\r\n**Output:**\r\n\r\n```\r\nEpoch 1/10\r\n2017-05-05 15:30:37.711608: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-05 15:30:37.711981: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-05 15:30:37.712355: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-05 15:30:37.712721: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-05 15:30:37.713094: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-05 15:30:37.713463: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-05 15:30:37.713839: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-05 15:30:37.714213: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n  1/379 [..............................] - ETA: 107s - loss: 492.8400\r\n 60/379 [===>..........................] - ETA: 1s - loss: 669.9998  \r\n109/379 [=======>......................] - ETA: 0s - loss: 625.8113\r\n172/379 [============>.................] - ETA: 0s - loss: 461.0169\r\n233/379 [=================>............] - ETA: 0s - loss: 353.9863\r\n286/379 [=====================>........] - ETA: 0s - loss: 294.3637\r\n336/379 [=========================>....] - ETA: 0s - loss: 254.4181\r\n379/379 [==============================] - 0s - loss: 230.0349     \r\nEpoch 2/10\r\n  1/379 [..............................] - ETA: 0s - loss: 8.0337\r\n 55/379 [===>..........................] - ETA: 0s - loss: 26.7809\r\n104/379 [=======>......................] - ETA: 0s - loss: 22.0314\r\n171/379 [============>.................] - ETA: 0s - loss: 25.4361\r\n219/379 [================>.............] - ETA: 0s - loss: 28.0947\r\n285/379 [=====================>........] - ETA: 0s - loss: 26.3540\r\n338/379 [=========================>....] - ETA: 0s - loss: 26.1902\r\n379/379 [==============================] - 0s - loss: 24.4095     \r\nEpoch 3/10\r\n  1/379 [..............................] - ETA: 0s - loss: 9.8365\r\n 42/379 [==>...........................] - ETA: 0s - loss: 20.2841\r\n108/379 [=======>......................] - ETA: 0s - loss: 16.0308\r\n161/379 [===========>..................] - ETA: 0s - loss: 18.4792\r\n225/379 [================>.............] - ETA: 0s - loss: 18.9558\r\n288/379 [=====================>........] - ETA: 0s - loss: 16.6939\r\n379/379 [==============================] - 0s - loss: 18.3086     \r\nEpoch 4/10\r\n  1/379 [..............................] - ETA: 0s - loss: 13.7463\r\n 59/379 [===>..........................] - ETA: 0s - loss: 12.0900\r\n115/379 [========>.....................] - ETA: 0s - loss: 18.4202\r\n181/379 [=============>................] - ETA: 0s - loss: 16.6087\r\n244/379 [==================>...........] - ETA: 0s - loss: 15.0061\r\n302/379 [======================>.......] - ETA: 0s - loss: 13.2268\r\n379/379 [==============================] - 0s - loss: 16.0613     \r\nEpoch 5/10\r\n  1/379 [..............................] - ETA: 0s - loss: 25.8912\r\n 65/379 [====>.........................] - ETA: 0s - loss: 28.8039\r\n123/379 [========>.....................] - ETA: 0s - loss: 21.3528\r\n199/379 [==============>...............] - ETA: 0s - loss: 17.8586\r\n256/379 [===================>..........] - ETA: 0s - loss: 19.4171\r\n327/379 [========================>.....] - ETA: 0s - loss: 17.9864\r\n379/379 [==============================] - 0s - loss: 18.2714     \r\nEpoch 6/10\r\n  1/379 [..............................] - ETA: 0s - loss: 2.0872\r\n 61/379 [===>..........................] - ETA: 0s - loss: 18.4408\r\n115/379 [========>.....................] - ETA: 0s - loss: 16.9127\r\n184/379 [=============>................] - ETA: 0s - loss: 16.9419\r\n247/379 [==================>...........] - ETA: 0s - loss: 16.2630\r\n314/379 [=======================>......] - ETA: 0s - loss: 14.9947\r\n379/379 [==============================] - 0s - loss: 13.5761     \r\nEpoch 7/10\r\n  1/379 [..............................] - ETA: 0s - loss: 0.3704\r\n 59/379 [===>..........................] - ETA: 0s - loss: 15.2191\r\n118/379 [========>.....................] - ETA: 0s - loss: 11.8381\r\n188/379 [=============>................] - ETA: 0s - loss: 10.9623\r\n247/379 [==================>...........] - ETA: 0s - loss: 10.0409\r\n313/379 [=======================>......] - ETA: 0s - loss: 12.4478\r\n379/379 [==============================] - 0s - loss: 13.8573     \r\nEpoch 8/10\r\n  1/379 [..............................] - ETA: 0s - loss: 1.2173\r\n 44/379 [==>...........................] - ETA: 0s - loss: 12.5116\r\n 96/379 [======>.......................] - ETA: 0s - loss: 10.2578\r\n170/379 [============>.................] - ETA: 0s - loss: 11.9057\r\n227/379 [================>.............] - ETA: 0s - loss: 12.1961\r\n287/379 [=====================>........] - ETA: 0s - loss: 14.4951\r\n341/379 [=========================>....] - ETA: 0s - loss: 13.7772\r\n379/379 [==============================] - 0s - loss: 13.2340     \r\nEpoch 9/10\r\n  1/379 [..............................] - ETA: 0s - loss: 0.1982\r\n 53/379 [===>..........................] - ETA: 0s - loss: 9.8250\r\n124/379 [========>.....................] - ETA: 0s - loss: 9.8112\r\n175/379 [============>.................] - ETA: 0s - loss: 10.9075\r\n238/379 [=================>............] - ETA: 0s - loss: 11.3589\r\n303/379 [======================>.......] - ETA: 0s - loss: 13.8970\r\n379/379 [==============================] - 0s - loss: 14.5469     \r\nEpoch 10/10\r\n  1/379 [..............................] - ETA: 0s - loss: 3.7164\r\n 49/379 [==>...........................] - ETA: 0s - loss: 14.6596\r\n116/379 [========>.....................] - ETA: 0s - loss: 15.5960\r\n172/379 [============>.................] - ETA: 0s - loss: 13.6525\r\n223/379 [================>.............] - ETA: 0s - loss: 12.9588\r\n275/379 [====================>.........] - ETA: 0s - loss: 12.4905\r\n347/379 [==========================>...] - ETA: 0s - loss: 13.5629\r\n379/379 [==============================] - 0s - loss: 13.1696     \r\n  1/127 [..............................] - ETA: 1s\r\n\r\nFinal score (MSE): 11.964310646057129\r\n\r\n\r\nFinal score (RMSE): 3.458946466445923\r\n\r\n```"}