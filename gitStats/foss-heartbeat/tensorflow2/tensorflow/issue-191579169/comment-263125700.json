{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263125700", "html_url": "https://github.com/tensorflow/tensorflow/issues/5834#issuecomment-263125700", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5834", "id": 263125700, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzEyNTcwMA==", "user": {"login": "gilberthendry", "id": 18124217, "node_id": "MDQ6VXNlcjE4MTI0MjE3", "avatar_url": "https://avatars0.githubusercontent.com/u/18124217?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gilberthendry", "html_url": "https://github.com/gilberthendry", "followers_url": "https://api.github.com/users/gilberthendry/followers", "following_url": "https://api.github.com/users/gilberthendry/following{/other_user}", "gists_url": "https://api.github.com/users/gilberthendry/gists{/gist_id}", "starred_url": "https://api.github.com/users/gilberthendry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gilberthendry/subscriptions", "organizations_url": "https://api.github.com/users/gilberthendry/orgs", "repos_url": "https://api.github.com/users/gilberthendry/repos", "events_url": "https://api.github.com/users/gilberthendry/events{/privacy}", "received_events_url": "https://api.github.com/users/gilberthendry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-27T14:37:02Z", "updated_at": "2016-11-27T14:37:02Z", "author_association": "NONE", "body_html": "<p>As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a> points out, this isn't the usual usage of an input function (returning a tf.constant).  For starters, you won't get any mini-batching.  It looks like you'd be processing the whole dataset on every step during training (which is ok if that's what you want, but is a not very efficient thing to do for online extremely random forests).</p>\n<p>Without trying to reproduce this myself, I suspect that evaluate might never terminate because it never throws the exception that indicates that the input is out of data (I think it still works this way).  You can verify this by passing max_steps=1 to evaluate.</p>\n<p>Still, it seems strange that the input_fn is being called more than once, resulting in your memory leak and such.</p>\n<p>I don't know what the migration plan is for other estimators to switch to input_fn, but TensorForestEstimator will continue to support x= and y= for datasets that easily fit in memory.  We will do the conversion to the right kind of tf.learn.Estimator under the hood (there will be an explicit class for this).  So my advice is to keep using x= and y= unless you want to use the IO ops mentioned by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a>, which is really only necessary for large datasets.</p>", "body_text": "As @prb12 points out, this isn't the usual usage of an input function (returning a tf.constant).  For starters, you won't get any mini-batching.  It looks like you'd be processing the whole dataset on every step during training (which is ok if that's what you want, but is a not very efficient thing to do for online extremely random forests).\nWithout trying to reproduce this myself, I suspect that evaluate might never terminate because it never throws the exception that indicates that the input is out of data (I think it still works this way).  You can verify this by passing max_steps=1 to evaluate.\nStill, it seems strange that the input_fn is being called more than once, resulting in your memory leak and such.\nI don't know what the migration plan is for other estimators to switch to input_fn, but TensorForestEstimator will continue to support x= and y= for datasets that easily fit in memory.  We will do the conversion to the right kind of tf.learn.Estimator under the hood (there will be an explicit class for this).  So my advice is to keep using x= and y= unless you want to use the IO ops mentioned by @prb12, which is really only necessary for large datasets.", "body": "As @prb12 points out, this isn't the usual usage of an input function (returning a tf.constant).  For starters, you won't get any mini-batching.  It looks like you'd be processing the whole dataset on every step during training (which is ok if that's what you want, but is a not very efficient thing to do for online extremely random forests). \r\n\r\nWithout trying to reproduce this myself, I suspect that evaluate might never terminate because it never throws the exception that indicates that the input is out of data (I think it still works this way).  You can verify this by passing max_steps=1 to evaluate.  \r\n\r\nStill, it seems strange that the input_fn is being called more than once, resulting in your memory leak and such.  \r\n\r\nI don't know what the migration plan is for other estimators to switch to input_fn, but TensorForestEstimator will continue to support x= and y= for datasets that easily fit in memory.  We will do the conversion to the right kind of tf.learn.Estimator under the hood (there will be an explicit class for this).  So my advice is to keep using x= and y= unless you want to use the IO ops mentioned by @prb12, which is really only necessary for large datasets."}