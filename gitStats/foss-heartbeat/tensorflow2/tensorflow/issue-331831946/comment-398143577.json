{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/398143577", "html_url": "https://github.com/tensorflow/tensorflow/issues/19967#issuecomment-398143577", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19967", "id": 398143577, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODE0MzU3Nw==", "user": {"login": "pawarrick", "id": 8933353, "node_id": "MDQ6VXNlcjg5MzMzNTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8933353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pawarrick", "html_url": "https://github.com/pawarrick", "followers_url": "https://api.github.com/users/pawarrick/followers", "following_url": "https://api.github.com/users/pawarrick/following{/other_user}", "gists_url": "https://api.github.com/users/pawarrick/gists{/gist_id}", "starred_url": "https://api.github.com/users/pawarrick/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pawarrick/subscriptions", "organizations_url": "https://api.github.com/users/pawarrick/orgs", "repos_url": "https://api.github.com/users/pawarrick/repos", "events_url": "https://api.github.com/users/pawarrick/events{/privacy}", "received_events_url": "https://api.github.com/users/pawarrick/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-18T18:04:30Z", "updated_at": "2018-06-18T18:04:30Z", "author_association": "NONE", "body_html": "<p>Here is the updated code with CuDNNLSTM added:</p>\n<pre><code>import sys\n\nimport numpy as np\nimport tensorflow as tf\n\nUSE_ESTIMATOR = True\n#USE_ESTIMATOR = False\n\n#USE_CUDNN_LSTM = True\nUSE_CUDNN_LSTM = False\n\nif USE_ESTIMATOR:\n  from tensorflow.python import keras\n  from tensorflow.python.keras.models import Sequential\n  from tensorflow.python.keras.layers import Dense, LSTM, BatchNormalization, CuDNNLSTM\n  from tensorflow.python.keras.optimizers import SGD, RMSprop\n  if USE_CUDNN_LSTM:\n    N_RECORDS = 35\n    BATCH_SIZE = 35\n  else:\n    N_RECORDS = 1\n    BATCH_SIZE = 1\n    \nelse:\n  import keras\n  import keras.backend.tensorflow_backend as K\n  from keras.models import Sequential\n  from keras.layers import Dense, LSTM, BatchNormalization\n  from keras.optimizers import SGD, RMSprop\n  N_RECORDS = 20\n  BATCH_SIZE = 10\n\n#SEQ_LENGTH=1000\n#SEQ_LENGTH=5000\n#SEQ_LENGTH=10000\nSEQ_LENGTH=24000\n\nINPUT_DIM = 598\nOUTPUT_DIM = 3\n\nNP_DTYPE = np.float32\nTF_DTYPE = tf.float32\n  \nTRAIN_EPOCHS = 2\nDEVICE_ID = '/gpu:0'\n\n\n\nclass ModelLSTM():\n  def __init__(self, batch_size, max_length=None, device_id='/cpu:0', n_input_dim=1, n_output_dim=2):  \n    \n    self.batch_size = batch_size\n    self.max_length = max_length\n    self.device_id = device_id\n    self.n_input_dim = n_input_dim\n    self.n_output_dim = n_output_dim\n\n    self.n_layers = 3\n    self.lstm_n_cell=[100, 100, 100] \n    self.dropout=0.1 \n    self.recurrent_dropout=0.1\n    \n    self.create_model()\n        \n  def create_model(self):        \n    with tf.device(self.device_id):\n    \n      print('Creating Model')\n      model = Sequential()\n      \n      for i_layer in range(0, self.n_layers):\n        if USE_CUDNN_LSTM:\n          model.add(keras.layers.CuDNNLSTM(self.lstm_n_cell[i_layer],\n                          return_sequences=True,\n                          stateful=False,\n                          kernel_initializer='he_normal',\n                          batch_input_shape=(self.batch_size, self.max_length, self.n_input_dim)))\n        else:\n          model.add(LSTM(self.lstm_n_cell[i_layer],\n                          return_sequences=True,\n                          stateful=False,\n                          kernel_initializer='he_normal',\n                          activation='tanh',\n                          dropout = self.dropout, \n                          recurrent_dropout = self.recurrent_dropout,\n                          batch_input_shape=(self.batch_size, self.max_length, self.n_input_dim)))\n          \n  \n        model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \n                                     scale=True, beta_initializer='zeros', gamma_initializer='ones', \n                                     moving_mean_initializer='zeros', moving_variance_initializer='ones'))      \n       \n      model.add(Dense(self.n_output_dim, kernel_initializer='he_normal',\n                                      activation='softmax')) \n      \n      #print (model.summary())\n      opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n  \n      model.compile(loss='categorical_crossentropy',\n                    optimizer=opt,\n                    metrics=['accuracy'],\n                    weighted_metrics=['accuracy'],\n                    sample_weight_mode='temporal')\n    \n    self.model = model\n    return self\n  \n  \nclass TestKerasAndEstimator():\n  def __init__(self):\n    self.device_id = DEVICE_ID\n      \n  def set_device(self, id):\n    self.device_id = id\n          \n  def the_input_fn(self, filenames, perform_shuffle=False, repeat_count=1, batch_size=1):\n    def _set_shapes(features, labels):\n      features.set_shape([SEQ_LENGTH, 598])\n      labels.set_shape([SEQ_LENGTH, 3])\n      return features, labels\n\n    def _my_parse_function(filename, label=None):\n      \n      print('Input File:')\n      print(filename)\n      \n      dec_filename = filename.decode(sys.getdefaultencoding())\n      print('Decoded File:')\n      print(dec_filename)\n      \n      # stub for testing, but normally read data from file here \n      features = np.zeros((SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\n      labels = np.zeros((SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\n\n      print('Features:')\n      print(features)\n      print('Labels:')\n      print(labels)\n      \n      return features, labels \n      \n     \n    labels = [0]*len(filenames)\n    labels = np.array(labels)\n    labels = tf.constant(labels)\n    labels = tf.cast(labels, TF_DTYPE)\n    \n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))  \n    \n    dataset = dataset.map(\n      lambda filename, label: tuple(tf.py_func(\n        _my_parse_function, [filename, label], [TF_DTYPE, label.dtype])))\n    \n    print(dataset)\n\n    dataset = dataset.map(_set_shapes)\n    \n    print(\"Dataset point 1:\")\n    print(dataset)\n    \n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=batch_size)\n    print(\"Dataset point 2:\")\n    print(dataset)\n    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n    print(\"Dataset point 3:\")\n    print(dataset)\n    dataset = dataset.batch(batch_size)  # Batch size to use\n    the_iterator = dataset.make_one_shot_iterator()    \n    batch_features, batch_labels = the_iterator.get_next()\n    print('Batch features') \n    print(batch_features) \n    print('Batch labels') \n    print(batch_labels) \n    return batch_features, batch_labels\n  \n  def test_keras_estimator(self, n_records=1, batch_size=1):\n    gpu_options = tf.GPUOptions(allow_growth=True) \n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \n    run_config = tf.estimator.RunConfig(session_config=sess_config)   \n    \n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\n    \n        \n    self.estimator = tf.keras.estimator.model_to_estimator(keras_model=train_model.model,\n                                                           model_dir='models', config=run_config)\n    train_records = list()\n    for i in range(0, n_records):\n      train_records.append('record_' + str(i))\n      \n    train_spec = tf.estimator.TrainSpec(input_fn=lambda: \n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size), \n                                        max_steps=TRAIN_EPOCHS)\n    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: \n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size))\n\n    tf.estimator.train_and_evaluate(self.estimator, train_spec, eval_spec)\n    #print (train_model.model.summary())\n      \n    \n     \n  def test_keras(self, n_records=1, batch_size=1):\n    gpu_options = tf.GPUOptions(allow_growth=True) \n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \n    K.set_session(tf.Session(config=sess_config))\n      \n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\n    \n    # stub for testing, but normally read data from file here \n    features = np.zeros((n_records, SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\n    labels = np.zeros((n_records, SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\n\n    train_model.model.fit(x=features, y=labels, batch_size=batch_size, epochs=TRAIN_EPOCHS, verbose=1)\n    train_model.model.evaluate(x=features, y=labels, batch_size=batch_size, verbose=1)    \n     \n     \n     \nif __name__ == \"__main__\":\n  mt = TestKerasAndEstimator()\n  if USE_ESTIMATOR:\n      mt.test_keras_estimator(n_records=N_RECORDS, batch_size=BATCH_SIZE)\n  else:\n      mt.test_keras(n_records=N_RECORDS, batch_size=BATCH_SIZE)\n\n    \n</code></pre>", "body_text": "Here is the updated code with CuDNNLSTM added:\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nUSE_ESTIMATOR = True\n#USE_ESTIMATOR = False\n\n#USE_CUDNN_LSTM = True\nUSE_CUDNN_LSTM = False\n\nif USE_ESTIMATOR:\n  from tensorflow.python import keras\n  from tensorflow.python.keras.models import Sequential\n  from tensorflow.python.keras.layers import Dense, LSTM, BatchNormalization, CuDNNLSTM\n  from tensorflow.python.keras.optimizers import SGD, RMSprop\n  if USE_CUDNN_LSTM:\n    N_RECORDS = 35\n    BATCH_SIZE = 35\n  else:\n    N_RECORDS = 1\n    BATCH_SIZE = 1\n    \nelse:\n  import keras\n  import keras.backend.tensorflow_backend as K\n  from keras.models import Sequential\n  from keras.layers import Dense, LSTM, BatchNormalization\n  from keras.optimizers import SGD, RMSprop\n  N_RECORDS = 20\n  BATCH_SIZE = 10\n\n#SEQ_LENGTH=1000\n#SEQ_LENGTH=5000\n#SEQ_LENGTH=10000\nSEQ_LENGTH=24000\n\nINPUT_DIM = 598\nOUTPUT_DIM = 3\n\nNP_DTYPE = np.float32\nTF_DTYPE = tf.float32\n  \nTRAIN_EPOCHS = 2\nDEVICE_ID = '/gpu:0'\n\n\n\nclass ModelLSTM():\n  def __init__(self, batch_size, max_length=None, device_id='/cpu:0', n_input_dim=1, n_output_dim=2):  \n    \n    self.batch_size = batch_size\n    self.max_length = max_length\n    self.device_id = device_id\n    self.n_input_dim = n_input_dim\n    self.n_output_dim = n_output_dim\n\n    self.n_layers = 3\n    self.lstm_n_cell=[100, 100, 100] \n    self.dropout=0.1 \n    self.recurrent_dropout=0.1\n    \n    self.create_model()\n        \n  def create_model(self):        \n    with tf.device(self.device_id):\n    \n      print('Creating Model')\n      model = Sequential()\n      \n      for i_layer in range(0, self.n_layers):\n        if USE_CUDNN_LSTM:\n          model.add(keras.layers.CuDNNLSTM(self.lstm_n_cell[i_layer],\n                          return_sequences=True,\n                          stateful=False,\n                          kernel_initializer='he_normal',\n                          batch_input_shape=(self.batch_size, self.max_length, self.n_input_dim)))\n        else:\n          model.add(LSTM(self.lstm_n_cell[i_layer],\n                          return_sequences=True,\n                          stateful=False,\n                          kernel_initializer='he_normal',\n                          activation='tanh',\n                          dropout = self.dropout, \n                          recurrent_dropout = self.recurrent_dropout,\n                          batch_input_shape=(self.batch_size, self.max_length, self.n_input_dim)))\n          \n  \n        model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \n                                     scale=True, beta_initializer='zeros', gamma_initializer='ones', \n                                     moving_mean_initializer='zeros', moving_variance_initializer='ones'))      \n       \n      model.add(Dense(self.n_output_dim, kernel_initializer='he_normal',\n                                      activation='softmax')) \n      \n      #print (model.summary())\n      opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n  \n      model.compile(loss='categorical_crossentropy',\n                    optimizer=opt,\n                    metrics=['accuracy'],\n                    weighted_metrics=['accuracy'],\n                    sample_weight_mode='temporal')\n    \n    self.model = model\n    return self\n  \n  \nclass TestKerasAndEstimator():\n  def __init__(self):\n    self.device_id = DEVICE_ID\n      \n  def set_device(self, id):\n    self.device_id = id\n          \n  def the_input_fn(self, filenames, perform_shuffle=False, repeat_count=1, batch_size=1):\n    def _set_shapes(features, labels):\n      features.set_shape([SEQ_LENGTH, 598])\n      labels.set_shape([SEQ_LENGTH, 3])\n      return features, labels\n\n    def _my_parse_function(filename, label=None):\n      \n      print('Input File:')\n      print(filename)\n      \n      dec_filename = filename.decode(sys.getdefaultencoding())\n      print('Decoded File:')\n      print(dec_filename)\n      \n      # stub for testing, but normally read data from file here \n      features = np.zeros((SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\n      labels = np.zeros((SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\n\n      print('Features:')\n      print(features)\n      print('Labels:')\n      print(labels)\n      \n      return features, labels \n      \n     \n    labels = [0]*len(filenames)\n    labels = np.array(labels)\n    labels = tf.constant(labels)\n    labels = tf.cast(labels, TF_DTYPE)\n    \n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))  \n    \n    dataset = dataset.map(\n      lambda filename, label: tuple(tf.py_func(\n        _my_parse_function, [filename, label], [TF_DTYPE, label.dtype])))\n    \n    print(dataset)\n\n    dataset = dataset.map(_set_shapes)\n    \n    print(\"Dataset point 1:\")\n    print(dataset)\n    \n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=batch_size)\n    print(\"Dataset point 2:\")\n    print(dataset)\n    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n    print(\"Dataset point 3:\")\n    print(dataset)\n    dataset = dataset.batch(batch_size)  # Batch size to use\n    the_iterator = dataset.make_one_shot_iterator()    \n    batch_features, batch_labels = the_iterator.get_next()\n    print('Batch features') \n    print(batch_features) \n    print('Batch labels') \n    print(batch_labels) \n    return batch_features, batch_labels\n  \n  def test_keras_estimator(self, n_records=1, batch_size=1):\n    gpu_options = tf.GPUOptions(allow_growth=True) \n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \n    run_config = tf.estimator.RunConfig(session_config=sess_config)   \n    \n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\n    \n        \n    self.estimator = tf.keras.estimator.model_to_estimator(keras_model=train_model.model,\n                                                           model_dir='models', config=run_config)\n    train_records = list()\n    for i in range(0, n_records):\n      train_records.append('record_' + str(i))\n      \n    train_spec = tf.estimator.TrainSpec(input_fn=lambda: \n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size), \n                                        max_steps=TRAIN_EPOCHS)\n    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: \n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size))\n\n    tf.estimator.train_and_evaluate(self.estimator, train_spec, eval_spec)\n    #print (train_model.model.summary())\n      \n    \n     \n  def test_keras(self, n_records=1, batch_size=1):\n    gpu_options = tf.GPUOptions(allow_growth=True) \n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \n    K.set_session(tf.Session(config=sess_config))\n      \n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\n    \n    # stub for testing, but normally read data from file here \n    features = np.zeros((n_records, SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\n    labels = np.zeros((n_records, SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\n\n    train_model.model.fit(x=features, y=labels, batch_size=batch_size, epochs=TRAIN_EPOCHS, verbose=1)\n    train_model.model.evaluate(x=features, y=labels, batch_size=batch_size, verbose=1)    \n     \n     \n     \nif __name__ == \"__main__\":\n  mt = TestKerasAndEstimator()\n  if USE_ESTIMATOR:\n      mt.test_keras_estimator(n_records=N_RECORDS, batch_size=BATCH_SIZE)\n  else:\n      mt.test_keras(n_records=N_RECORDS, batch_size=BATCH_SIZE)", "body": "Here is the updated code with CuDNNLSTM added:\r\n\r\n```\r\nimport sys\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nUSE_ESTIMATOR = True\r\n#USE_ESTIMATOR = False\r\n\r\n#USE_CUDNN_LSTM = True\r\nUSE_CUDNN_LSTM = False\r\n\r\nif USE_ESTIMATOR:\r\n  from tensorflow.python import keras\r\n  from tensorflow.python.keras.models import Sequential\r\n  from tensorflow.python.keras.layers import Dense, LSTM, BatchNormalization, CuDNNLSTM\r\n  from tensorflow.python.keras.optimizers import SGD, RMSprop\r\n  if USE_CUDNN_LSTM:\r\n    N_RECORDS = 35\r\n    BATCH_SIZE = 35\r\n  else:\r\n    N_RECORDS = 1\r\n    BATCH_SIZE = 1\r\n    \r\nelse:\r\n  import keras\r\n  import keras.backend.tensorflow_backend as K\r\n  from keras.models import Sequential\r\n  from keras.layers import Dense, LSTM, BatchNormalization\r\n  from keras.optimizers import SGD, RMSprop\r\n  N_RECORDS = 20\r\n  BATCH_SIZE = 10\r\n\r\n#SEQ_LENGTH=1000\r\n#SEQ_LENGTH=5000\r\n#SEQ_LENGTH=10000\r\nSEQ_LENGTH=24000\r\n\r\nINPUT_DIM = 598\r\nOUTPUT_DIM = 3\r\n\r\nNP_DTYPE = np.float32\r\nTF_DTYPE = tf.float32\r\n  \r\nTRAIN_EPOCHS = 2\r\nDEVICE_ID = '/gpu:0'\r\n\r\n\r\n\r\nclass ModelLSTM():\r\n  def __init__(self, batch_size, max_length=None, device_id='/cpu:0', n_input_dim=1, n_output_dim=2):  \r\n    \r\n    self.batch_size = batch_size\r\n    self.max_length = max_length\r\n    self.device_id = device_id\r\n    self.n_input_dim = n_input_dim\r\n    self.n_output_dim = n_output_dim\r\n\r\n    self.n_layers = 3\r\n    self.lstm_n_cell=[100, 100, 100] \r\n    self.dropout=0.1 \r\n    self.recurrent_dropout=0.1\r\n    \r\n    self.create_model()\r\n        \r\n  def create_model(self):        \r\n    with tf.device(self.device_id):\r\n    \r\n      print('Creating Model')\r\n      model = Sequential()\r\n      \r\n      for i_layer in range(0, self.n_layers):\r\n        if USE_CUDNN_LSTM:\r\n          model.add(keras.layers.CuDNNLSTM(self.lstm_n_cell[i_layer],\r\n                          return_sequences=True,\r\n                          stateful=False,\r\n                          kernel_initializer='he_normal',\r\n                          batch_input_shape=(self.batch_size, self.max_length, self.n_input_dim)))\r\n        else:\r\n          model.add(LSTM(self.lstm_n_cell[i_layer],\r\n                          return_sequences=True,\r\n                          stateful=False,\r\n                          kernel_initializer='he_normal',\r\n                          activation='tanh',\r\n                          dropout = self.dropout, \r\n                          recurrent_dropout = self.recurrent_dropout,\r\n                          batch_input_shape=(self.batch_size, self.max_length, self.n_input_dim)))\r\n          \r\n  \r\n        model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \r\n                                     scale=True, beta_initializer='zeros', gamma_initializer='ones', \r\n                                     moving_mean_initializer='zeros', moving_variance_initializer='ones'))      \r\n       \r\n      model.add(Dense(self.n_output_dim, kernel_initializer='he_normal',\r\n                                      activation='softmax')) \r\n      \r\n      #print (model.summary())\r\n      opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\r\n  \r\n      model.compile(loss='categorical_crossentropy',\r\n                    optimizer=opt,\r\n                    metrics=['accuracy'],\r\n                    weighted_metrics=['accuracy'],\r\n                    sample_weight_mode='temporal')\r\n    \r\n    self.model = model\r\n    return self\r\n  \r\n  \r\nclass TestKerasAndEstimator():\r\n  def __init__(self):\r\n    self.device_id = DEVICE_ID\r\n      \r\n  def set_device(self, id):\r\n    self.device_id = id\r\n          \r\n  def the_input_fn(self, filenames, perform_shuffle=False, repeat_count=1, batch_size=1):\r\n    def _set_shapes(features, labels):\r\n      features.set_shape([SEQ_LENGTH, 598])\r\n      labels.set_shape([SEQ_LENGTH, 3])\r\n      return features, labels\r\n\r\n    def _my_parse_function(filename, label=None):\r\n      \r\n      print('Input File:')\r\n      print(filename)\r\n      \r\n      dec_filename = filename.decode(sys.getdefaultencoding())\r\n      print('Decoded File:')\r\n      print(dec_filename)\r\n      \r\n      # stub for testing, but normally read data from file here \r\n      features = np.zeros((SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\r\n      labels = np.zeros((SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\r\n\r\n      print('Features:')\r\n      print(features)\r\n      print('Labels:')\r\n      print(labels)\r\n      \r\n      return features, labels \r\n      \r\n     \r\n    labels = [0]*len(filenames)\r\n    labels = np.array(labels)\r\n    labels = tf.constant(labels)\r\n    labels = tf.cast(labels, TF_DTYPE)\r\n    \r\n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))  \r\n    \r\n    dataset = dataset.map(\r\n      lambda filename, label: tuple(tf.py_func(\r\n        _my_parse_function, [filename, label], [TF_DTYPE, label.dtype])))\r\n    \r\n    print(dataset)\r\n\r\n    dataset = dataset.map(_set_shapes)\r\n    \r\n    print(\"Dataset point 1:\")\r\n    print(dataset)\r\n    \r\n    if perform_shuffle:\r\n        dataset = dataset.shuffle(buffer_size=batch_size)\r\n    print(\"Dataset point 2:\")\r\n    print(dataset)\r\n    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\r\n    print(\"Dataset point 3:\")\r\n    print(dataset)\r\n    dataset = dataset.batch(batch_size)  # Batch size to use\r\n    the_iterator = dataset.make_one_shot_iterator()    \r\n    batch_features, batch_labels = the_iterator.get_next()\r\n    print('Batch features') \r\n    print(batch_features) \r\n    print('Batch labels') \r\n    print(batch_labels) \r\n    return batch_features, batch_labels\r\n  \r\n  def test_keras_estimator(self, n_records=1, batch_size=1):\r\n    gpu_options = tf.GPUOptions(allow_growth=True) \r\n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \r\n    run_config = tf.estimator.RunConfig(session_config=sess_config)   \r\n    \r\n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \r\n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\r\n    \r\n        \r\n    self.estimator = tf.keras.estimator.model_to_estimator(keras_model=train_model.model,\r\n                                                           model_dir='models', config=run_config)\r\n    train_records = list()\r\n    for i in range(0, n_records):\r\n      train_records.append('record_' + str(i))\r\n      \r\n    train_spec = tf.estimator.TrainSpec(input_fn=lambda: \r\n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size), \r\n                                        max_steps=TRAIN_EPOCHS)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: \r\n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size))\r\n\r\n    tf.estimator.train_and_evaluate(self.estimator, train_spec, eval_spec)\r\n    #print (train_model.model.summary())\r\n      \r\n    \r\n     \r\n  def test_keras(self, n_records=1, batch_size=1):\r\n    gpu_options = tf.GPUOptions(allow_growth=True) \r\n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \r\n    K.set_session(tf.Session(config=sess_config))\r\n      \r\n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \r\n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\r\n    \r\n    # stub for testing, but normally read data from file here \r\n    features = np.zeros((n_records, SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\r\n    labels = np.zeros((n_records, SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\r\n\r\n    train_model.model.fit(x=features, y=labels, batch_size=batch_size, epochs=TRAIN_EPOCHS, verbose=1)\r\n    train_model.model.evaluate(x=features, y=labels, batch_size=batch_size, verbose=1)    \r\n     \r\n     \r\n     \r\nif __name__ == \"__main__\":\r\n  mt = TestKerasAndEstimator()\r\n  if USE_ESTIMATOR:\r\n      mt.test_keras_estimator(n_records=N_RECORDS, batch_size=BATCH_SIZE)\r\n  else:\r\n      mt.test_keras(n_records=N_RECORDS, batch_size=BATCH_SIZE)\r\n\r\n    \r\n  ```"}