{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19967", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19967/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19967/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19967/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19967", "id": 331831946, "node_id": "MDU6SXNzdWUzMzE4MzE5NDY=", "number": 19967, "title": "GPU OOM with Keras and Estimator, fine with Keras alone", "user": {"login": "pawarrick", "id": 8933353, "node_id": "MDQ6VXNlcjg5MzMzNTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8933353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pawarrick", "html_url": "https://github.com/pawarrick", "followers_url": "https://api.github.com/users/pawarrick/followers", "following_url": "https://api.github.com/users/pawarrick/following{/other_user}", "gists_url": "https://api.github.com/users/pawarrick/gists{/gist_id}", "starred_url": "https://api.github.com/users/pawarrick/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pawarrick/subscriptions", "organizations_url": "https://api.github.com/users/pawarrick/orgs", "repos_url": "https://api.github.com/users/pawarrick/repos", "events_url": "https://api.github.com/users/pawarrick/events{/privacy}", "received_events_url": "https://api.github.com/users/pawarrick/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, {"login": "tanzhenyu", "id": 15220929, "node_id": "MDQ6VXNlcjE1MjIwOTI5", "avatar_url": "https://avatars3.githubusercontent.com/u/15220929?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanzhenyu", "html_url": "https://github.com/tanzhenyu", "followers_url": "https://api.github.com/users/tanzhenyu/followers", "following_url": "https://api.github.com/users/tanzhenyu/following{/other_user}", "gists_url": "https://api.github.com/users/tanzhenyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanzhenyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanzhenyu/subscriptions", "organizations_url": "https://api.github.com/users/tanzhenyu/orgs", "repos_url": "https://api.github.com/users/tanzhenyu/repos", "events_url": "https://api.github.com/users/tanzhenyu/events{/privacy}", "received_events_url": "https://api.github.com/users/tanzhenyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2018-06-13T03:49:56Z", "updated_at": "2018-11-23T18:38:42Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflowGPU_1.7</li>\n<li><strong>Python version</strong>: 3.5.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.0/cuDNN7.0</li>\n<li><strong>GPU model and memory</strong>:NVidia Titan X (Pascal) 12GB</li>\n<li><strong>Exact command to reproduce</strong>:python TestKerasAndEstimator.py</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I have developed a Keras model and used it successfully with only Keras train and evaluate calls. Now I would like to use the same model in an Estimator context using tf.keras.estimator.model_to_estimator.</p>\n<p>It's quite a large model (3 LSTM layers with 100 units each, input dimension 598), but I can run it in the Keras-alone context with a batch_size up to 10.  However, with an Estimator, I get an OOM on the GPU even with a batch_size of 1.  If I reduce the sequence length (SEQ_LEN) from 24000 to 5000 however, it will run with an Estimator.</p>\n<p>Here is the output log with Keras alone (USE_ESTIMATOR=False)</p>\n<pre><code>[Console output redirected to file:C:\\Users\\philip.LMS\\git\\GIT_RD_Python\\Ch2018\\ch2018_train\\TestEstimator_20180612_2256_log.txt]\nUsing TensorFlow backend.\n2018-06-12 22:57:11.753350: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n2018-06-12 22:57:12.396391: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:03:00.0\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\n2018-06-12 22:57:12.624894: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 1 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:a1:00.0\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\n2018-06-12 22:57:12.625753: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0, 1\n2018-06-12 22:57:17.075745: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-06-12 22:57:17.076181: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 1 \n2018-06-12 22:57:17.076457: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N N \n2018-06-12 22:57:17.076746: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 1:   N N \n2018-06-12 22:57:17.077336: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -&gt; physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)\n2018-06-12 22:57:17.079930: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9619 MB memory) -&gt; physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:a1:00.0, compute capability: 6.1)\nCreating Model\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (10, 24000, 100)          279600    \n_________________________________________________________________\nbatch_normalization_1 (Batch (10, 24000, 100)          400       \n_________________________________________________________________\nlstm_2 (LSTM)                (10, 24000, 100)          80400     \n_________________________________________________________________\nbatch_normalization_2 (Batch (10, 24000, 100)          400       \n_________________________________________________________________\nlstm_3 (LSTM)                (10, 24000, 100)          80400     \n_________________________________________________________________\nbatch_normalization_3 (Batch (10, 24000, 100)          400       \n_________________________________________________________________\ndense_1 (Dense)              (10, 24000, 3)            303       \n=================================================================\nTotal params: 441,903\nTrainable params: 441,303\nNon-trainable params: 600\n_________________________________________________________________\nNone\nEpoch 1/2\n\n10/20 [==============&gt;...............] - ETA: 6:45 - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\n20/20 [==============================] - 904s 45s/step - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\nEpoch 2/2\n\n10/20 [==============&gt;...............] - ETA: 8:59 - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\n20/20 [==============================] - 1059s 53s/step - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\n\n10/20 [==============&gt;...............] - ETA: 43s\n20/20 [==============================] - 86s 4s/step\n</code></pre>\n<p>And here is the output log with Keras and Estimator (USE_ESTIMATOR=True)</p>\n<pre><code>[Console output redirected to file:C:\\Users\\philip.LMS\\git\\GIT_RD_Python\\Ch2018\\ch2018_train\\TestEstimator_20180612_2338_log.txt]\nCreating Model\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (1, 24000, 100)           279600    \n_________________________________________________________________\nbatch_normalization_1 (Batch (1, 24000, 100)           400       \n_________________________________________________________________\nlstm_2 (LSTM)                (1, 24000, 100)           80400     \n_________________________________________________________________\nbatch_normalization_2 (Batch (1, 24000, 100)           400       \n_________________________________________________________________\nlstm_3 (LSTM)                (1, 24000, 100)           80400     \n_________________________________________________________________\nbatch_normalization_3 (Batch (1, 24000, 100)           400       \n_________________________________________________________________\ndense_1 (Dense)              (1, 24000, 3)             303       \n=================================================================\nTotal params: 441,903\nTrainable params: 441,303\nNon-trainable params: 600\n_________________________________________________________________\nNone\n2018-06-12 23:39:11.126586: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n2018-06-12 23:39:11.751372: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:03:00.0\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\n2018-06-12 23:39:11.981018: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 1 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:a1:00.0\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\n2018-06-12 23:39:11.981905: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0, 1\n2018-06-12 23:39:17.295319: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-06-12 23:39:17.295766: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 1 \n2018-06-12 23:39:17.296030: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N N \n2018-06-12 23:39:17.296314: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 1:   N N \n2018-06-12 23:39:17.296888: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -&gt; physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)\n2018-06-12 23:39:17.299820: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9619 MB memory) -&gt; physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:a1:00.0, compute capability: 6.1)\n2018-06-12 23:39:17.644506: I T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:159] Creating CudaSolver handles for stream 000001E5C8B53C10\n&lt;MapDataset shapes: (&lt;unknown&gt;, &lt;unknown&gt;), types: (tf.float32, tf.float32)&gt;\nDataset point 1:\n&lt;MapDataset shapes: ((24000, 598), (24000, 3)), types: (tf.float32, tf.float32)&gt;\nDataset point 2:\n&lt;MapDataset shapes: ((24000, 598), (24000, 3)), types: (tf.float32, tf.float32)&gt;\nDataset point 3:\n&lt;RepeatDataset shapes: ((24000, 598), (24000, 3)), types: (tf.float32, tf.float32)&gt;\nBatch features\nTensor(\"IteratorGetNext:0\", shape=(?, 24000, 598), dtype=float32, device=/device:CPU:0)\nBatch labels\nTensor(\"IteratorGetNext:1\", shape=(?, 24000, 3), dtype=float32, device=/device:CPU:0)\n2018-06-12 23:39:26.545626: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0, 1\n2018-06-12 23:39:26.546174: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-06-12 23:39:26.546739: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 1 \n2018-06-12 23:39:26.547090: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N N \n2018-06-12 23:39:26.547462: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 1:   N N \n2018-06-12 23:39:26.548091: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -&gt; physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)\n2018-06-12 23:39:26.549907: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9619 MB memory) -&gt; physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:a1:00.0, compute capability: 6.1)\nInput File:\nb'record_0'\nDecoded File:\nrecord_0\nFeatures:\n[[ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n ..., \n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\nLabels:\n[[ 0.  0.  0.]\n [ 0.  0.  0.]\n [ 0.  0.  0.]\n ..., \n [ 0.  0.  0.]\n [ 0.  0.  0.]\n [ 0.  0.  0.]]\n2018-06-12 23:40:47.905054: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:967] failed to alloc 4294967296 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\n2018-06-12 23:40:47.905614: W T:\\src\\github\\tensorflow\\tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 4294967296\n2018-06-12 23:40:47.906068: E \n...\n</code></pre>\n<p>Here is the python test file TestKerasAndEstimator.py:</p>\n<pre><code>import sys\n\nimport numpy as np\nimport tensorflow as tf\n\nUSE_ESTIMATOR = True\n#USE_ESTIMATOR = False\n\nif USE_ESTIMATOR:\n  from tensorflow.python import keras\n  from tensorflow.python.keras.models import Sequential\n  from tensorflow.python.keras.layers import Dense, LSTM, BatchNormalization\n  from tensorflow.python.keras.optimizers import SGD, RMSprop\n  N_RECORDS = 1\n  BATCH_SIZE = 1\nelse:\n  import keras\n  import keras.backend.tensorflow_backend as K\n  from keras.models import Sequential\n  from keras.layers import Dense, LSTM, BatchNormalization\n  from keras.optimizers import SGD, RMSprop\n  N_RECORDS = 20\n  BATCH_SIZE = 10\n\n#SEQ_LENGTH=1000\n#SEQ_LENGTH=5000\n#SEQ_LENGTH=10000\nSEQ_LENGTH=24000\n\nINPUT_DIM = 598\nOUTPUT_DIM = 3\n\nNP_DTYPE = np.float32\nTF_DTYPE = tf.float32\n  \nTRAIN_EPOCHS = 2\nDEVICE_ID = '/gpu:0'\n\nclass ModelLSTM():\n  def __init__(self, batch_size, max_length=None, device_id='/cpu:0', n_input_dim=1, n_output_dim=2):  \n    \n    self.batch_size = batch_size\n    self.max_length = max_length\n    self.device_id = device_id\n    self.n_input_dim = n_input_dim\n    self.n_output_dim = n_output_dim\n\n    self.lstm_n_cell=[100, 100, 100] \n    self.dropout=0.1 \n    self.recurrent_dropout=0.1\n    \n    self.create_model()\n        \n  def create_model(self):        \n      \n    with tf.device(self.device_id):\n    \n      print('Creating Model')\n      model = Sequential()\n      model.add(LSTM(self.lstm_n_cell[0],\n                      return_sequences=True,\n                      stateful=False,\n                      kernel_initializer='he_normal',\n                      activation='tanh',\n                      dropout = self.dropout, \n                      recurrent_dropout = self.recurrent_dropout,\n                      batch_input_shape=(self.batch_size, self.max_length, self.n_input_dim)))\n      model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \n                                   scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n      model.add(LSTM(self.lstm_n_cell[1],\n                     return_sequences=True,\n                     stateful=False,\n                     kernel_initializer='he_normal',\n                      activation='tanh',\n                      dropout = self.dropout, \n                      recurrent_dropout = self.recurrent_dropout))\n      model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \n                                   scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n      model.add(LSTM(self.lstm_n_cell[2],\n                     return_sequences=True,\n                     stateful=False,\n                     kernel_initializer='he_normal',\n                      activation='tanh',\n                      dropout = self.dropout, \n                      recurrent_dropout = self.recurrent_dropout))\n      model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \n                                   scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n       \n      model.add(Dense(self.n_output_dim, kernel_initializer='he_normal',\n                                      activation='softmax')) \n      \n      print (model.summary())\n      \n      opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n  \n      model.compile(loss='categorical_crossentropy',\n                    optimizer=opt,\n                    metrics=['accuracy'],\n                    weighted_metrics=['accuracy'],\n                    sample_weight_mode='temporal')\n    \n    self.model = model\n    return self\n  \n  \nclass TestKerasAndEstimator():\n  def __init__(self):\n    self.device_id = DEVICE_ID\n      \n  def set_device(self, id):\n    self.device_id = id\n          \n  def the_input_fn(self, filenames, perform_shuffle=False, repeat_count=1, batch_size=1):\n    def _set_shapes(features, labels):\n      features.set_shape([SEQ_LENGTH, 598])\n      labels.set_shape([SEQ_LENGTH, 3])\n      return features, labels\n\n    def _my_parse_function(filename, label=None):\n      \n      print('Input File:')\n      print(filename)\n      \n      dec_filename = filename.decode(sys.getdefaultencoding())\n      print('Decoded File:')\n      print(dec_filename)\n      \n      features = np.zeros((SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\n      labels = np.zeros((SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\n\n      print('Features:')\n      print(features)\n      print('Labels:')\n      print(labels)\n      \n      return features, labels \n      \n     \n    labels = [0]*len(filenames)\n    labels = np.array(labels)\n    labels = tf.constant(labels)\n    labels = tf.cast(labels, TF_DTYPE)\n    \n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))  \n    \n    dataset = dataset.map(\n      lambda filename, label: tuple(tf.py_func(\n        _my_parse_function, [filename, label], [TF_DTYPE, label.dtype])))\n    \n    print(dataset)\n\n    dataset = dataset.map(_set_shapes)\n    \n    print(\"Dataset point 1:\")\n    print(dataset)\n    \n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=batch_size)\n    print(\"Dataset point 2:\")\n    print(dataset)\n    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n    print(\"Dataset point 3:\")\n    print(dataset)\n    dataset = dataset.batch(batch_size)  # Batch size to use\n    the_iterator = dataset.make_one_shot_iterator()    \n    batch_features, batch_labels = the_iterator.get_next()\n    print('Batch features') \n    print(batch_features) \n    print('Batch labels') \n    print(batch_labels) \n    return batch_features, batch_labels\n  \n  def test_keras_estimator(self, n_records=1, batch_size=1):\n    gpu_options = tf.GPUOptions(allow_growth=True) \n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \n    run_config = tf.estimator.RunConfig(session_config=sess_config)   \n    \n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\n    self.estimator = tf.keras.estimator.model_to_estimator(keras_model=train_model.model,\n                                                           model_dir='.', config=run_config)\n    train_records = list()\n    for i in range(0, n_records):\n      train_records.append('record_' + str(i))\n      \n    train_spec = tf.estimator.TrainSpec(input_fn=lambda: \n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size), \n                                        max_steps=TRAIN_EPOCHS)\n    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: \n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size))\n\n    tf.estimator.train_and_evaluate(self.estimator, train_spec, eval_spec)\n    \n     \n  def test_keras(self, n_records=1, batch_size=1):\n    gpu_options = tf.GPUOptions(allow_growth=True) \n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \n    K.set_session(tf.Session(config=sess_config))\n      \n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\n    features = np.zeros((n_records, SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\n    labels = np.zeros((n_records, SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\n\n    train_model.model.fit(x=features, y=labels, batch_size=batch_size, epochs=TRAIN_EPOCHS, verbose=1)\n    train_model.model.evaluate(x=features, y=labels, batch_size=batch_size, verbose=1)    \n     \n     \n     \nif __name__ == \"__main__\":\n  mt = TestKerasAndEstimator()\n  if USE_ESTIMATOR:\n      mt.test_keras_estimator(n_records=N_RECORDS, batch_size=BATCH_SIZE)\n  else:\n      mt.test_keras(n_records=N_RECORDS, batch_size=BATCH_SIZE)\n\n    \n  \n</code></pre>\n<p>Thanks!</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): tensorflowGPU_1.7\nPython version: 3.5.4\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: CUDA 9.0/cuDNN7.0\nGPU model and memory:NVidia Titan X (Pascal) 12GB\nExact command to reproduce:python TestKerasAndEstimator.py\n\nDescribe the problem\nI have developed a Keras model and used it successfully with only Keras train and evaluate calls. Now I would like to use the same model in an Estimator context using tf.keras.estimator.model_to_estimator.\nIt's quite a large model (3 LSTM layers with 100 units each, input dimension 598), but I can run it in the Keras-alone context with a batch_size up to 10.  However, with an Estimator, I get an OOM on the GPU even with a batch_size of 1.  If I reduce the sequence length (SEQ_LEN) from 24000 to 5000 however, it will run with an Estimator.\nHere is the output log with Keras alone (USE_ESTIMATOR=False)\n[Console output redirected to file:C:\\Users\\philip.LMS\\git\\GIT_RD_Python\\Ch2018\\ch2018_train\\TestEstimator_20180612_2256_log.txt]\nUsing TensorFlow backend.\n2018-06-12 22:57:11.753350: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n2018-06-12 22:57:12.396391: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:03:00.0\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\n2018-06-12 22:57:12.624894: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 1 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:a1:00.0\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\n2018-06-12 22:57:12.625753: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0, 1\n2018-06-12 22:57:17.075745: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-06-12 22:57:17.076181: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 1 \n2018-06-12 22:57:17.076457: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N N \n2018-06-12 22:57:17.076746: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 1:   N N \n2018-06-12 22:57:17.077336: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)\n2018-06-12 22:57:17.079930: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9619 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:a1:00.0, compute capability: 6.1)\nCreating Model\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (10, 24000, 100)          279600    \n_________________________________________________________________\nbatch_normalization_1 (Batch (10, 24000, 100)          400       \n_________________________________________________________________\nlstm_2 (LSTM)                (10, 24000, 100)          80400     \n_________________________________________________________________\nbatch_normalization_2 (Batch (10, 24000, 100)          400       \n_________________________________________________________________\nlstm_3 (LSTM)                (10, 24000, 100)          80400     \n_________________________________________________________________\nbatch_normalization_3 (Batch (10, 24000, 100)          400       \n_________________________________________________________________\ndense_1 (Dense)              (10, 24000, 3)            303       \n=================================================================\nTotal params: 441,903\nTrainable params: 441,303\nNon-trainable params: 600\n_________________________________________________________________\nNone\nEpoch 1/2\n\n10/20 [==============>...............] - ETA: 6:45 - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\n20/20 [==============================] - 904s 45s/step - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\nEpoch 2/2\n\n10/20 [==============>...............] - ETA: 8:59 - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\n20/20 [==============================] - 1059s 53s/step - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\n\n10/20 [==============>...............] - ETA: 43s\n20/20 [==============================] - 86s 4s/step\n\nAnd here is the output log with Keras and Estimator (USE_ESTIMATOR=True)\n[Console output redirected to file:C:\\Users\\philip.LMS\\git\\GIT_RD_Python\\Ch2018\\ch2018_train\\TestEstimator_20180612_2338_log.txt]\nCreating Model\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (1, 24000, 100)           279600    \n_________________________________________________________________\nbatch_normalization_1 (Batch (1, 24000, 100)           400       \n_________________________________________________________________\nlstm_2 (LSTM)                (1, 24000, 100)           80400     \n_________________________________________________________________\nbatch_normalization_2 (Batch (1, 24000, 100)           400       \n_________________________________________________________________\nlstm_3 (LSTM)                (1, 24000, 100)           80400     \n_________________________________________________________________\nbatch_normalization_3 (Batch (1, 24000, 100)           400       \n_________________________________________________________________\ndense_1 (Dense)              (1, 24000, 3)             303       \n=================================================================\nTotal params: 441,903\nTrainable params: 441,303\nNon-trainable params: 600\n_________________________________________________________________\nNone\n2018-06-12 23:39:11.126586: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n2018-06-12 23:39:11.751372: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:03:00.0\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\n2018-06-12 23:39:11.981018: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 1 with properties: \nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:a1:00.0\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\n2018-06-12 23:39:11.981905: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0, 1\n2018-06-12 23:39:17.295319: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-06-12 23:39:17.295766: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 1 \n2018-06-12 23:39:17.296030: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N N \n2018-06-12 23:39:17.296314: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 1:   N N \n2018-06-12 23:39:17.296888: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)\n2018-06-12 23:39:17.299820: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9619 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:a1:00.0, compute capability: 6.1)\n2018-06-12 23:39:17.644506: I T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:159] Creating CudaSolver handles for stream 000001E5C8B53C10\n<MapDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.float32)>\nDataset point 1:\n<MapDataset shapes: ((24000, 598), (24000, 3)), types: (tf.float32, tf.float32)>\nDataset point 2:\n<MapDataset shapes: ((24000, 598), (24000, 3)), types: (tf.float32, tf.float32)>\nDataset point 3:\n<RepeatDataset shapes: ((24000, 598), (24000, 3)), types: (tf.float32, tf.float32)>\nBatch features\nTensor(\"IteratorGetNext:0\", shape=(?, 24000, 598), dtype=float32, device=/device:CPU:0)\nBatch labels\nTensor(\"IteratorGetNext:1\", shape=(?, 24000, 3), dtype=float32, device=/device:CPU:0)\n2018-06-12 23:39:26.545626: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0, 1\n2018-06-12 23:39:26.546174: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-06-12 23:39:26.546739: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 1 \n2018-06-12 23:39:26.547090: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N N \n2018-06-12 23:39:26.547462: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 1:   N N \n2018-06-12 23:39:26.548091: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)\n2018-06-12 23:39:26.549907: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9619 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:a1:00.0, compute capability: 6.1)\nInput File:\nb'record_0'\nDecoded File:\nrecord_0\nFeatures:\n[[ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n ..., \n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\nLabels:\n[[ 0.  0.  0.]\n [ 0.  0.  0.]\n [ 0.  0.  0.]\n ..., \n [ 0.  0.  0.]\n [ 0.  0.  0.]\n [ 0.  0.  0.]]\n2018-06-12 23:40:47.905054: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:967] failed to alloc 4294967296 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\n2018-06-12 23:40:47.905614: W T:\\src\\github\\tensorflow\\tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 4294967296\n2018-06-12 23:40:47.906068: E \n...\n\nHere is the python test file TestKerasAndEstimator.py:\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nUSE_ESTIMATOR = True\n#USE_ESTIMATOR = False\n\nif USE_ESTIMATOR:\n  from tensorflow.python import keras\n  from tensorflow.python.keras.models import Sequential\n  from tensorflow.python.keras.layers import Dense, LSTM, BatchNormalization\n  from tensorflow.python.keras.optimizers import SGD, RMSprop\n  N_RECORDS = 1\n  BATCH_SIZE = 1\nelse:\n  import keras\n  import keras.backend.tensorflow_backend as K\n  from keras.models import Sequential\n  from keras.layers import Dense, LSTM, BatchNormalization\n  from keras.optimizers import SGD, RMSprop\n  N_RECORDS = 20\n  BATCH_SIZE = 10\n\n#SEQ_LENGTH=1000\n#SEQ_LENGTH=5000\n#SEQ_LENGTH=10000\nSEQ_LENGTH=24000\n\nINPUT_DIM = 598\nOUTPUT_DIM = 3\n\nNP_DTYPE = np.float32\nTF_DTYPE = tf.float32\n  \nTRAIN_EPOCHS = 2\nDEVICE_ID = '/gpu:0'\n\nclass ModelLSTM():\n  def __init__(self, batch_size, max_length=None, device_id='/cpu:0', n_input_dim=1, n_output_dim=2):  \n    \n    self.batch_size = batch_size\n    self.max_length = max_length\n    self.device_id = device_id\n    self.n_input_dim = n_input_dim\n    self.n_output_dim = n_output_dim\n\n    self.lstm_n_cell=[100, 100, 100] \n    self.dropout=0.1 \n    self.recurrent_dropout=0.1\n    \n    self.create_model()\n        \n  def create_model(self):        \n      \n    with tf.device(self.device_id):\n    \n      print('Creating Model')\n      model = Sequential()\n      model.add(LSTM(self.lstm_n_cell[0],\n                      return_sequences=True,\n                      stateful=False,\n                      kernel_initializer='he_normal',\n                      activation='tanh',\n                      dropout = self.dropout, \n                      recurrent_dropout = self.recurrent_dropout,\n                      batch_input_shape=(self.batch_size, self.max_length, self.n_input_dim)))\n      model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \n                                   scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n      model.add(LSTM(self.lstm_n_cell[1],\n                     return_sequences=True,\n                     stateful=False,\n                     kernel_initializer='he_normal',\n                      activation='tanh',\n                      dropout = self.dropout, \n                      recurrent_dropout = self.recurrent_dropout))\n      model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \n                                   scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n      model.add(LSTM(self.lstm_n_cell[2],\n                     return_sequences=True,\n                     stateful=False,\n                     kernel_initializer='he_normal',\n                      activation='tanh',\n                      dropout = self.dropout, \n                      recurrent_dropout = self.recurrent_dropout))\n      model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \n                                   scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\n       \n      model.add(Dense(self.n_output_dim, kernel_initializer='he_normal',\n                                      activation='softmax')) \n      \n      print (model.summary())\n      \n      opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n  \n      model.compile(loss='categorical_crossentropy',\n                    optimizer=opt,\n                    metrics=['accuracy'],\n                    weighted_metrics=['accuracy'],\n                    sample_weight_mode='temporal')\n    \n    self.model = model\n    return self\n  \n  \nclass TestKerasAndEstimator():\n  def __init__(self):\n    self.device_id = DEVICE_ID\n      \n  def set_device(self, id):\n    self.device_id = id\n          \n  def the_input_fn(self, filenames, perform_shuffle=False, repeat_count=1, batch_size=1):\n    def _set_shapes(features, labels):\n      features.set_shape([SEQ_LENGTH, 598])\n      labels.set_shape([SEQ_LENGTH, 3])\n      return features, labels\n\n    def _my_parse_function(filename, label=None):\n      \n      print('Input File:')\n      print(filename)\n      \n      dec_filename = filename.decode(sys.getdefaultencoding())\n      print('Decoded File:')\n      print(dec_filename)\n      \n      features = np.zeros((SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\n      labels = np.zeros((SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\n\n      print('Features:')\n      print(features)\n      print('Labels:')\n      print(labels)\n      \n      return features, labels \n      \n     \n    labels = [0]*len(filenames)\n    labels = np.array(labels)\n    labels = tf.constant(labels)\n    labels = tf.cast(labels, TF_DTYPE)\n    \n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))  \n    \n    dataset = dataset.map(\n      lambda filename, label: tuple(tf.py_func(\n        _my_parse_function, [filename, label], [TF_DTYPE, label.dtype])))\n    \n    print(dataset)\n\n    dataset = dataset.map(_set_shapes)\n    \n    print(\"Dataset point 1:\")\n    print(dataset)\n    \n    if perform_shuffle:\n        dataset = dataset.shuffle(buffer_size=batch_size)\n    print(\"Dataset point 2:\")\n    print(dataset)\n    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n    print(\"Dataset point 3:\")\n    print(dataset)\n    dataset = dataset.batch(batch_size)  # Batch size to use\n    the_iterator = dataset.make_one_shot_iterator()    \n    batch_features, batch_labels = the_iterator.get_next()\n    print('Batch features') \n    print(batch_features) \n    print('Batch labels') \n    print(batch_labels) \n    return batch_features, batch_labels\n  \n  def test_keras_estimator(self, n_records=1, batch_size=1):\n    gpu_options = tf.GPUOptions(allow_growth=True) \n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \n    run_config = tf.estimator.RunConfig(session_config=sess_config)   \n    \n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\n    self.estimator = tf.keras.estimator.model_to_estimator(keras_model=train_model.model,\n                                                           model_dir='.', config=run_config)\n    train_records = list()\n    for i in range(0, n_records):\n      train_records.append('record_' + str(i))\n      \n    train_spec = tf.estimator.TrainSpec(input_fn=lambda: \n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size), \n                                        max_steps=TRAIN_EPOCHS)\n    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: \n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size))\n\n    tf.estimator.train_and_evaluate(self.estimator, train_spec, eval_spec)\n    \n     \n  def test_keras(self, n_records=1, batch_size=1):\n    gpu_options = tf.GPUOptions(allow_growth=True) \n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \n    K.set_session(tf.Session(config=sess_config))\n      \n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\n    features = np.zeros((n_records, SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\n    labels = np.zeros((n_records, SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\n\n    train_model.model.fit(x=features, y=labels, batch_size=batch_size, epochs=TRAIN_EPOCHS, verbose=1)\n    train_model.model.evaluate(x=features, y=labels, batch_size=batch_size, verbose=1)    \n     \n     \n     \nif __name__ == \"__main__\":\n  mt = TestKerasAndEstimator()\n  if USE_ESTIMATOR:\n      mt.test_keras_estimator(n_records=N_RECORDS, batch_size=BATCH_SIZE)\n  else:\n      mt.test_keras(n_records=N_RECORDS, batch_size=BATCH_SIZE)\n\n    \n  \n\nThanks!", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tensorflowGPU_1.7\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 9.0/cuDNN7.0\r\n- **GPU model and memory**:NVidia Titan X (Pascal) 12GB\r\n- **Exact command to reproduce**:python TestKerasAndEstimator.py\r\n\r\n\r\n### Describe the problem\r\nI have developed a Keras model and used it successfully with only Keras train and evaluate calls. Now I would like to use the same model in an Estimator context using tf.keras.estimator.model_to_estimator.\r\n\r\nIt's quite a large model (3 LSTM layers with 100 units each, input dimension 598), but I can run it in the Keras-alone context with a batch_size up to 10.  However, with an Estimator, I get an OOM on the GPU even with a batch_size of 1.  If I reduce the sequence length (SEQ_LEN) from 24000 to 5000 however, it will run with an Estimator.\r\n\r\nHere is the output log with Keras alone (USE_ESTIMATOR=False)\r\n```\r\n[Console output redirected to file:C:\\Users\\philip.LMS\\git\\GIT_RD_Python\\Ch2018\\ch2018_train\\TestEstimator_20180612_2256_log.txt]\r\nUsing TensorFlow backend.\r\n2018-06-12 22:57:11.753350: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-06-12 22:57:12.396391: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\r\n2018-06-12 22:57:12.624894: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 1 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:a1:00.0\r\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\r\n2018-06-12 22:57:12.625753: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0, 1\r\n2018-06-12 22:57:17.075745: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-12 22:57:17.076181: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 1 \r\n2018-06-12 22:57:17.076457: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N N \r\n2018-06-12 22:57:17.076746: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 1:   N N \r\n2018-06-12 22:57:17.077336: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-06-12 22:57:17.079930: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9619 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:a1:00.0, compute capability: 6.1)\r\nCreating Model\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nlstm_1 (LSTM)                (10, 24000, 100)          279600    \r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (10, 24000, 100)          400       \r\n_________________________________________________________________\r\nlstm_2 (LSTM)                (10, 24000, 100)          80400     \r\n_________________________________________________________________\r\nbatch_normalization_2 (Batch (10, 24000, 100)          400       \r\n_________________________________________________________________\r\nlstm_3 (LSTM)                (10, 24000, 100)          80400     \r\n_________________________________________________________________\r\nbatch_normalization_3 (Batch (10, 24000, 100)          400       \r\n_________________________________________________________________\r\ndense_1 (Dense)              (10, 24000, 3)            303       \r\n=================================================================\r\nTotal params: 441,903\r\nTrainable params: 441,303\r\nNon-trainable params: 600\r\n_________________________________________________________________\r\nNone\r\nEpoch 1/2\r\n\r\n10/20 [==============>...............] - ETA: 6:45 - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\r\n20/20 [==============================] - 904s 45s/step - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\r\nEpoch 2/2\r\n\r\n10/20 [==============>...............] - ETA: 8:59 - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\r\n20/20 [==============================] - 1059s 53s/step - loss: 0.0000e+00 - acc: 1.0000 - weighted_acc: 1.0000\r\n\r\n10/20 [==============>...............] - ETA: 43s\r\n20/20 [==============================] - 86s 4s/step\r\n```\r\n\r\nAnd here is the output log with Keras and Estimator (USE_ESTIMATOR=True)\r\n\r\n```\r\n[Console output redirected to file:C:\\Users\\philip.LMS\\git\\GIT_RD_Python\\Ch2018\\ch2018_train\\TestEstimator_20180612_2338_log.txt]\r\nCreating Model\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nlstm_1 (LSTM)                (1, 24000, 100)           279600    \r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (1, 24000, 100)           400       \r\n_________________________________________________________________\r\nlstm_2 (LSTM)                (1, 24000, 100)           80400     \r\n_________________________________________________________________\r\nbatch_normalization_2 (Batch (1, 24000, 100)           400       \r\n_________________________________________________________________\r\nlstm_3 (LSTM)                (1, 24000, 100)           80400     \r\n_________________________________________________________________\r\nbatch_normalization_3 (Batch (1, 24000, 100)           400       \r\n_________________________________________________________________\r\ndense_1 (Dense)              (1, 24000, 3)             303       \r\n=================================================================\r\nTotal params: 441,903\r\nTrainable params: 441,303\r\nNon-trainable params: 600\r\n_________________________________________________________________\r\nNone\r\n2018-06-12 23:39:11.126586: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-06-12 23:39:11.751372: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\r\n2018-06-12 23:39:11.981018: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 1 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:a1:00.0\r\ntotalMemory: 12.00GiB freeMemory: 9.93GiB\r\n2018-06-12 23:39:11.981905: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0, 1\r\n2018-06-12 23:39:17.295319: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-12 23:39:17.295766: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 1 \r\n2018-06-12 23:39:17.296030: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N N \r\n2018-06-12 23:39:17.296314: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 1:   N N \r\n2018-06-12 23:39:17.296888: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-06-12 23:39:17.299820: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9619 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:a1:00.0, compute capability: 6.1)\r\n2018-06-12 23:39:17.644506: I T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:159] Creating CudaSolver handles for stream 000001E5C8B53C10\r\n<MapDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.float32)>\r\nDataset point 1:\r\n<MapDataset shapes: ((24000, 598), (24000, 3)), types: (tf.float32, tf.float32)>\r\nDataset point 2:\r\n<MapDataset shapes: ((24000, 598), (24000, 3)), types: (tf.float32, tf.float32)>\r\nDataset point 3:\r\n<RepeatDataset shapes: ((24000, 598), (24000, 3)), types: (tf.float32, tf.float32)>\r\nBatch features\r\nTensor(\"IteratorGetNext:0\", shape=(?, 24000, 598), dtype=float32, device=/device:CPU:0)\r\nBatch labels\r\nTensor(\"IteratorGetNext:1\", shape=(?, 24000, 3), dtype=float32, device=/device:CPU:0)\r\n2018-06-12 23:39:26.545626: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0, 1\r\n2018-06-12 23:39:26.546174: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-12 23:39:26.546739: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 1 \r\n2018-06-12 23:39:26.547090: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N N \r\n2018-06-12 23:39:26.547462: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 1:   N N \r\n2018-06-12 23:39:26.548091: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9618 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-06-12 23:39:26.549907: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9619 MB memory) -> physical GPU (device: 1, name: TITAN X (Pascal), pci bus id: 0000:a1:00.0, compute capability: 6.1)\r\nInput File:\r\nb'record_0'\r\nDecoded File:\r\nrecord_0\r\nFeatures:\r\n[[ 0.  0.  0. ...,  0.  0.  0.]\r\n [ 0.  0.  0. ...,  0.  0.  0.]\r\n [ 0.  0.  0. ...,  0.  0.  0.]\r\n ..., \r\n [ 0.  0.  0. ...,  0.  0.  0.]\r\n [ 0.  0.  0. ...,  0.  0.  0.]\r\n [ 0.  0.  0. ...,  0.  0.  0.]]\r\nLabels:\r\n[[ 0.  0.  0.]\r\n [ 0.  0.  0.]\r\n [ 0.  0.  0.]\r\n ..., \r\n [ 0.  0.  0.]\r\n [ 0.  0.  0.]\r\n [ 0.  0.  0.]]\r\n2018-06-12 23:40:47.905054: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:967] failed to alloc 4294967296 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\r\n2018-06-12 23:40:47.905614: W T:\\src\\github\\tensorflow\\tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 4294967296\r\n2018-06-12 23:40:47.906068: E \r\n...\r\n```\r\n\r\nHere is the python test file TestKerasAndEstimator.py:\r\n```\r\nimport sys\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nUSE_ESTIMATOR = True\r\n#USE_ESTIMATOR = False\r\n\r\nif USE_ESTIMATOR:\r\n  from tensorflow.python import keras\r\n  from tensorflow.python.keras.models import Sequential\r\n  from tensorflow.python.keras.layers import Dense, LSTM, BatchNormalization\r\n  from tensorflow.python.keras.optimizers import SGD, RMSprop\r\n  N_RECORDS = 1\r\n  BATCH_SIZE = 1\r\nelse:\r\n  import keras\r\n  import keras.backend.tensorflow_backend as K\r\n  from keras.models import Sequential\r\n  from keras.layers import Dense, LSTM, BatchNormalization\r\n  from keras.optimizers import SGD, RMSprop\r\n  N_RECORDS = 20\r\n  BATCH_SIZE = 10\r\n\r\n#SEQ_LENGTH=1000\r\n#SEQ_LENGTH=5000\r\n#SEQ_LENGTH=10000\r\nSEQ_LENGTH=24000\r\n\r\nINPUT_DIM = 598\r\nOUTPUT_DIM = 3\r\n\r\nNP_DTYPE = np.float32\r\nTF_DTYPE = tf.float32\r\n  \r\nTRAIN_EPOCHS = 2\r\nDEVICE_ID = '/gpu:0'\r\n\r\nclass ModelLSTM():\r\n  def __init__(self, batch_size, max_length=None, device_id='/cpu:0', n_input_dim=1, n_output_dim=2):  \r\n    \r\n    self.batch_size = batch_size\r\n    self.max_length = max_length\r\n    self.device_id = device_id\r\n    self.n_input_dim = n_input_dim\r\n    self.n_output_dim = n_output_dim\r\n\r\n    self.lstm_n_cell=[100, 100, 100] \r\n    self.dropout=0.1 \r\n    self.recurrent_dropout=0.1\r\n    \r\n    self.create_model()\r\n        \r\n  def create_model(self):        \r\n      \r\n    with tf.device(self.device_id):\r\n    \r\n      print('Creating Model')\r\n      model = Sequential()\r\n      model.add(LSTM(self.lstm_n_cell[0],\r\n                      return_sequences=True,\r\n                      stateful=False,\r\n                      kernel_initializer='he_normal',\r\n                      activation='tanh',\r\n                      dropout = self.dropout, \r\n                      recurrent_dropout = self.recurrent_dropout,\r\n                      batch_input_shape=(self.batch_size, self.max_length, self.n_input_dim)))\r\n      model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \r\n                                   scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\r\n      model.add(LSTM(self.lstm_n_cell[1],\r\n                     return_sequences=True,\r\n                     stateful=False,\r\n                     kernel_initializer='he_normal',\r\n                      activation='tanh',\r\n                      dropout = self.dropout, \r\n                      recurrent_dropout = self.recurrent_dropout))\r\n      model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \r\n                                   scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\r\n      model.add(LSTM(self.lstm_n_cell[2],\r\n                     return_sequences=True,\r\n                     stateful=False,\r\n                     kernel_initializer='he_normal',\r\n                      activation='tanh',\r\n                      dropout = self.dropout, \r\n                      recurrent_dropout = self.recurrent_dropout))\r\n      model.add(BatchNormalization(momentum=0.99, epsilon=0.001, center=True, \r\n                                   scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones'))\r\n       \r\n      model.add(Dense(self.n_output_dim, kernel_initializer='he_normal',\r\n                                      activation='softmax')) \r\n      \r\n      print (model.summary())\r\n      \r\n      opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\r\n  \r\n      model.compile(loss='categorical_crossentropy',\r\n                    optimizer=opt,\r\n                    metrics=['accuracy'],\r\n                    weighted_metrics=['accuracy'],\r\n                    sample_weight_mode='temporal')\r\n    \r\n    self.model = model\r\n    return self\r\n  \r\n  \r\nclass TestKerasAndEstimator():\r\n  def __init__(self):\r\n    self.device_id = DEVICE_ID\r\n      \r\n  def set_device(self, id):\r\n    self.device_id = id\r\n          \r\n  def the_input_fn(self, filenames, perform_shuffle=False, repeat_count=1, batch_size=1):\r\n    def _set_shapes(features, labels):\r\n      features.set_shape([SEQ_LENGTH, 598])\r\n      labels.set_shape([SEQ_LENGTH, 3])\r\n      return features, labels\r\n\r\n    def _my_parse_function(filename, label=None):\r\n      \r\n      print('Input File:')\r\n      print(filename)\r\n      \r\n      dec_filename = filename.decode(sys.getdefaultencoding())\r\n      print('Decoded File:')\r\n      print(dec_filename)\r\n      \r\n      features = np.zeros((SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\r\n      labels = np.zeros((SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\r\n\r\n      print('Features:')\r\n      print(features)\r\n      print('Labels:')\r\n      print(labels)\r\n      \r\n      return features, labels \r\n      \r\n     \r\n    labels = [0]*len(filenames)\r\n    labels = np.array(labels)\r\n    labels = tf.constant(labels)\r\n    labels = tf.cast(labels, TF_DTYPE)\r\n    \r\n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))  \r\n    \r\n    dataset = dataset.map(\r\n      lambda filename, label: tuple(tf.py_func(\r\n        _my_parse_function, [filename, label], [TF_DTYPE, label.dtype])))\r\n    \r\n    print(dataset)\r\n\r\n    dataset = dataset.map(_set_shapes)\r\n    \r\n    print(\"Dataset point 1:\")\r\n    print(dataset)\r\n    \r\n    if perform_shuffle:\r\n        dataset = dataset.shuffle(buffer_size=batch_size)\r\n    print(\"Dataset point 2:\")\r\n    print(dataset)\r\n    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\r\n    print(\"Dataset point 3:\")\r\n    print(dataset)\r\n    dataset = dataset.batch(batch_size)  # Batch size to use\r\n    the_iterator = dataset.make_one_shot_iterator()    \r\n    batch_features, batch_labels = the_iterator.get_next()\r\n    print('Batch features') \r\n    print(batch_features) \r\n    print('Batch labels') \r\n    print(batch_labels) \r\n    return batch_features, batch_labels\r\n  \r\n  def test_keras_estimator(self, n_records=1, batch_size=1):\r\n    gpu_options = tf.GPUOptions(allow_growth=True) \r\n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \r\n    run_config = tf.estimator.RunConfig(session_config=sess_config)   \r\n    \r\n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \r\n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\r\n    self.estimator = tf.keras.estimator.model_to_estimator(keras_model=train_model.model,\r\n                                                           model_dir='.', config=run_config)\r\n    train_records = list()\r\n    for i in range(0, n_records):\r\n      train_records.append('record_' + str(i))\r\n      \r\n    train_spec = tf.estimator.TrainSpec(input_fn=lambda: \r\n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size), \r\n                                        max_steps=TRAIN_EPOCHS)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: \r\n                                        self.the_input_fn(train_records, perform_shuffle=False, batch_size=batch_size))\r\n\r\n    tf.estimator.train_and_evaluate(self.estimator, train_spec, eval_spec)\r\n    \r\n     \r\n  def test_keras(self, n_records=1, batch_size=1):\r\n    gpu_options = tf.GPUOptions(allow_growth=True) \r\n    sess_config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True, log_device_placement=False)        \r\n    K.set_session(tf.Session(config=sess_config))\r\n      \r\n    train_model = ModelLSTM(batch_size=batch_size, max_length=SEQ_LENGTH, device_id=self.device_id, \r\n                            n_input_dim=INPUT_DIM, n_output_dim=OUTPUT_DIM)\r\n    features = np.zeros((n_records, SEQ_LENGTH, INPUT_DIM), dtype=NP_DTYPE)\r\n    labels = np.zeros((n_records, SEQ_LENGTH, OUTPUT_DIM), dtype=NP_DTYPE)\r\n\r\n    train_model.model.fit(x=features, y=labels, batch_size=batch_size, epochs=TRAIN_EPOCHS, verbose=1)\r\n    train_model.model.evaluate(x=features, y=labels, batch_size=batch_size, verbose=1)    \r\n     \r\n     \r\n     \r\nif __name__ == \"__main__\":\r\n  mt = TestKerasAndEstimator()\r\n  if USE_ESTIMATOR:\r\n      mt.test_keras_estimator(n_records=N_RECORDS, batch_size=BATCH_SIZE)\r\n  else:\r\n      mt.test_keras(n_records=N_RECORDS, batch_size=BATCH_SIZE)\r\n\r\n    \r\n  \r\n```\r\nThanks!"}