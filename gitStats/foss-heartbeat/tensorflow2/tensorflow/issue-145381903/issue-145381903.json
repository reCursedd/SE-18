{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1748", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1748/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1748/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1748/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1748", "id": 145381903, "node_id": "MDU6SXNzdWUxNDUzODE5MDM=", "number": 1748, "title": "Optimizers incompatible with sampling -- missing docs?", "user": {"login": "elanmart", "id": 10772830, "node_id": "MDQ6VXNlcjEwNzcyODMw", "avatar_url": "https://avatars3.githubusercontent.com/u/10772830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elanmart", "html_url": "https://github.com/elanmart", "followers_url": "https://api.github.com/users/elanmart/followers", "following_url": "https://api.github.com/users/elanmart/following{/other_user}", "gists_url": "https://api.github.com/users/elanmart/gists{/gist_id}", "starred_url": "https://api.github.com/users/elanmart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elanmart/subscriptions", "organizations_url": "https://api.github.com/users/elanmart/orgs", "repos_url": "https://api.github.com/users/elanmart/repos", "events_url": "https://api.github.com/users/elanmart/events{/privacy}", "received_events_url": "https://api.github.com/users/elanmart/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-04-02T15:05:39Z", "updated_at": "2017-02-09T22:02:18Z", "closed_at": "2016-06-06T20:51:06Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>perhaps Tensorflow docs should mention that 5 out of 7 available optimizers will not work with sampling losses? For now, they fail with mysterious messages.</p>\n<p>The following script will fail for Momentum, AdaGrad, AdaDelta, RMSProp and FTRL.</p>\n<p>Also, where should I look if I'd like to implement my own optimizers for GPU?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy.random <span class=\"pl-k\">as</span> nr\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> config </span>\nnum_classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\nnum_sampled <span class=\"pl-k\">=</span> <span class=\"pl-c1\">512</span>\nnum_true <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nactivation_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">512</span>\nbatch_sz <span class=\"pl-k\">=</span> <span class=\"pl-c1\">16</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> \"model\" setup</span>\nactivations <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, activation_dim))\nlabels <span class=\"pl-k\">=</span> tf.placeholder(tf.int64, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, num_true))\n\nnce_W <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal((num_classes, activation_dim)))\nnce_b <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal((num_classes, )))\n\nnce_loss <span class=\"pl-k\">=</span> tf.reduce_mean(\n                tf.nn.nce_loss(nce_W, nce_b, \n                               activations, labels, \n                               num_sampled, num_classes, num_true))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> optimizer setup</span>\nglobal_step   <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">1</span>)\ninitial_alpha <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0.1</span>)\nalpha         <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0.01</span>)\n\noptimizer <span class=\"pl-k\">=</span> tf.train.FtrlOptimizer(alpha)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> fetches</span>\nstep <span class=\"pl-k\">=</span> optimizer.minimize(nce_loss, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sgd_step<span class=\"pl-pds\">'</span></span>) \ninit <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> synthetic data</span>\nX <span class=\"pl-k\">=</span> nr.randn(batch_sz, activation_dim).astype(np.float32)\ny <span class=\"pl-k\">=</span> nr.randint(<span class=\"pl-c1\">0</span>, num_classes, (batch_sz, num_true)).astype(np.int64)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(init)\n    sess.run(step, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{activations:X, labels:y})</pre></div>\n<p>The error message is:</p>\n<pre><code>---------------------------------------------------------------------------\nStatusNotOK                               Traceback (most recent call last)\nStatusNotOK: Invalid argument: Cannot assign a device to node 'Variable_1/read': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n     [[Node: Variable_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_1\"]](Variable_1)]]\n</code></pre>", "body_text": "Hi,\nperhaps Tensorflow docs should mention that 5 out of 7 available optimizers will not work with sampling losses? For now, they fail with mysterious messages.\nThe following script will fail for Momentum, AdaGrad, AdaDelta, RMSProp and FTRL.\nAlso, where should I look if I'd like to implement my own optimizers for GPU?\nimport tensorflow as tf\nimport numpy.random as nr\nimport numpy as np\n\n# config \nnum_classes = 10000\nnum_sampled = 512\nnum_true = 32\nactivation_dim = 512\nbatch_sz = 16\n\n# \"model\" setup\nactivations = tf.placeholder(tf.float32, shape=(None, activation_dim))\nlabels = tf.placeholder(tf.int64, shape=(None, num_true))\n\nnce_W = tf.Variable(tf.truncated_normal((num_classes, activation_dim)))\nnce_b = tf.Variable(tf.truncated_normal((num_classes, )))\n\nnce_loss = tf.reduce_mean(\n                tf.nn.nce_loss(nce_W, nce_b, \n                               activations, labels, \n                               num_sampled, num_classes, num_true))\n\n# optimizer setup\nglobal_step   = tf.Variable(1)\ninitial_alpha = tf.Variable(0.1)\nalpha         = tf.Variable(0.01)\n\noptimizer = tf.train.FtrlOptimizer(alpha)\n\n# fetches\nstep = optimizer.minimize(nce_loss, global_step=global_step, name='sgd_step') \ninit = tf.initialize_all_variables()\n\n# synthetic data\nX = nr.randn(batch_sz, activation_dim).astype(np.float32)\ny = nr.randint(0, num_classes, (batch_sz, num_true)).astype(np.int64)\n\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(step, feed_dict={activations:X, labels:y})\nThe error message is:\n---------------------------------------------------------------------------\nStatusNotOK                               Traceback (most recent call last)\nStatusNotOK: Invalid argument: Cannot assign a device to node 'Variable_1/read': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n     [[Node: Variable_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_1\"]](Variable_1)]]", "body": "Hi,\n\nperhaps Tensorflow docs should mention that 5 out of 7 available optimizers will not work with sampling losses? For now, they fail with mysterious messages.\n\nThe following script will fail for Momentum, AdaGrad, AdaDelta, RMSProp and FTRL.\n\nAlso, where should I look if I'd like to implement my own optimizers for GPU?\n\n``` python\nimport tensorflow as tf\nimport numpy.random as nr\nimport numpy as np\n\n# config \nnum_classes = 10000\nnum_sampled = 512\nnum_true = 32\nactivation_dim = 512\nbatch_sz = 16\n\n# \"model\" setup\nactivations = tf.placeholder(tf.float32, shape=(None, activation_dim))\nlabels = tf.placeholder(tf.int64, shape=(None, num_true))\n\nnce_W = tf.Variable(tf.truncated_normal((num_classes, activation_dim)))\nnce_b = tf.Variable(tf.truncated_normal((num_classes, )))\n\nnce_loss = tf.reduce_mean(\n                tf.nn.nce_loss(nce_W, nce_b, \n                               activations, labels, \n                               num_sampled, num_classes, num_true))\n\n# optimizer setup\nglobal_step   = tf.Variable(1)\ninitial_alpha = tf.Variable(0.1)\nalpha         = tf.Variable(0.01)\n\noptimizer = tf.train.FtrlOptimizer(alpha)\n\n# fetches\nstep = optimizer.minimize(nce_loss, global_step=global_step, name='sgd_step') \ninit = tf.initialize_all_variables()\n\n# synthetic data\nX = nr.randn(batch_sz, activation_dim).astype(np.float32)\ny = nr.randint(0, num_classes, (batch_sz, num_true)).astype(np.int64)\n\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(step, feed_dict={activations:X, labels:y})\n```\n\nThe error message is:\n\n```\n---------------------------------------------------------------------------\nStatusNotOK                               Traceback (most recent call last)\nStatusNotOK: Invalid argument: Cannot assign a device to node 'Variable_1/read': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n     [[Node: Variable_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_1\"]](Variable_1)]]\n```\n"}