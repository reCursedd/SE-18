{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11709", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11709/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11709/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11709/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11709", "id": 245004538, "node_id": "MDU6SXNzdWUyNDUwMDQ1Mzg=", "number": 11709, "title": "Most of embedding_lookup/embedding_lookup_sparse computations are on CPU,  Most of the GPU time is transferring data.", "user": {"login": "nolanliou", "id": 30223680, "node_id": "MDQ6VXNlcjMwMjIzNjgw", "avatar_url": "https://avatars3.githubusercontent.com/u/30223680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nolanliou", "html_url": "https://github.com/nolanliou", "followers_url": "https://api.github.com/users/nolanliou/followers", "following_url": "https://api.github.com/users/nolanliou/following{/other_user}", "gists_url": "https://api.github.com/users/nolanliou/gists{/gist_id}", "starred_url": "https://api.github.com/users/nolanliou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nolanliou/subscriptions", "organizations_url": "https://api.github.com/users/nolanliou/orgs", "repos_url": "https://api.github.com/users/nolanliou/repos", "events_url": "https://api.github.com/users/nolanliou/events{/privacy}", "received_events_url": "https://api.github.com/users/nolanliou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-07-24T08:19:52Z", "updated_at": "2017-08-17T10:44:22Z", "closed_at": "2017-08-17T10:44:13Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Centos 7</li>\n<li><strong>TenorFlow installed from (source or binary)</strong>:<br>\nsource</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nr.12</li>\n<li><strong>Python version</strong>:<br>\n2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n8.0</li>\n<li><strong>GPU model and memory</strong>:<br>\nP40</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I have done some profiling about <em>embedding_lookup_sparse</em>, i have use <code>tf.device('\\gpu:0')</code> to specify where the ops run. But the results show:</p>\n<ul>\n<li>Most of the operations are performed on the CPU.</li>\n<li>GPU was transferring data at most of time.</li>\n</ul>\n<p>After I looked into the codes, there are some Ops without GPU version, including <code>dynamic_partition</code>,  <code>segment_sum</code>, <code>strided_slice</code>. So I'm a little confused.</p>\n<p>Is this intended behavior? Or are these Ops not implemented yet? Thanks for the explanations.</p>\n<p>Here is some profiling result:</p>\n<h3>tfprof</h3>\n<pre><code>I tensorflow/core/common_runtime/simple_placer.cc:841] hidden/embed/embedding_lookup_sparse/embedding_lookup/DynamicPartition:(DynamicPartition)/job:localhost/replica:0/task:0/cpu:0\nhidden/embed/embedding_lookup_sparse: (SegmentSum): /job:localhost/replica:0/task:0/cpu:0\nhidden/embed/embedding_lookup_sparse/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0\n</code></pre>\n<h3>nvprof</h3>\n<pre><code>==4762== Profiling result:\nTime(%)      Time     Calls       Avg       Min       Max  Name\n 44.07%  130.029s    240276  541.17us     768ns  8.1151ms  [CUDA memcpy DtoH]\n 25.20%  74.3504s    480619  154.70us     992ns  8.9016ms  [CUDA memcpy HtoD]\n  9.84%  29.0339s    330000  87.981us  61.922us  610.68us  void tensorflow::GatherOpKernel&lt;float, int&gt;(float const *, int const *, tensorflow::GatherOpKernel&lt;float, int&gt;*, __int64, __int64, __int64)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Centos 7\nTenorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\nr.12\nPython version:\n2.7\nBazel version (if compiling from source):\n0.5.2\nCUDA/cuDNN version:\n8.0\nGPU model and memory:\nP40\n\nDescribe the problem\nI have done some profiling about embedding_lookup_sparse, i have use tf.device('\\gpu:0') to specify where the ops run. But the results show:\n\nMost of the operations are performed on the CPU.\nGPU was transferring data at most of time.\n\nAfter I looked into the codes, there are some Ops without GPU version, including dynamic_partition,  segment_sum, strided_slice. So I'm a little confused.\nIs this intended behavior? Or are these Ops not implemented yet? Thanks for the explanations.\nHere is some profiling result:\ntfprof\nI tensorflow/core/common_runtime/simple_placer.cc:841] hidden/embed/embedding_lookup_sparse/embedding_lookup/DynamicPartition:(DynamicPartition)/job:localhost/replica:0/task:0/cpu:0\nhidden/embed/embedding_lookup_sparse: (SegmentSum): /job:localhost/replica:0/task:0/cpu:0\nhidden/embed/embedding_lookup_sparse/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0\n\nnvprof\n==4762== Profiling result:\nTime(%)      Time     Calls       Avg       Min       Max  Name\n 44.07%  130.029s    240276  541.17us     768ns  8.1151ms  [CUDA memcpy DtoH]\n 25.20%  74.3504s    480619  154.70us     992ns  8.9016ms  [CUDA memcpy HtoD]\n  9.84%  29.0339s    330000  87.981us  61.922us  610.68us  void tensorflow::GatherOpKernel<float, int>(float const *, int const *, tensorflow::GatherOpKernel<float, int>*, __int64, __int64, __int64)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Centos 7\r\n- **TenorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nr.12\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.5.2\r\n- **CUDA/cuDNN version**:\r\n8.0\r\n- **GPU model and memory**:\r\nP40\r\n\r\n\r\n### Describe the problem\r\nI have done some profiling about *embedding_lookup_sparse*, i have use `tf.device('\\gpu:0')` to specify where the ops run. But the results show:\r\n\r\n- Most of the operations are performed on the CPU.\r\n- GPU was transferring data at most of time.\r\n\r\nAfter I looked into the codes, there are some Ops without GPU version, including `dynamic_partition`,  `segment_sum`, `strided_slice`. So I'm a little confused.\r\n\r\nIs this intended behavior? Or are these Ops not implemented yet? Thanks for the explanations.\r\n\r\nHere is some profiling result:\r\n### tfprof\r\n```\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] hidden/embed/embedding_lookup_sparse/embedding_lookup/DynamicPartition:(DynamicPartition)/job:localhost/replica:0/task:0/cpu:0\r\nhidden/embed/embedding_lookup_sparse: (SegmentSum): /job:localhost/replica:0/task:0/cpu:0\r\nhidden/embed/embedding_lookup_sparse/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0\r\n```\r\n### nvprof\r\n```\r\n==4762== Profiling result:\r\nTime(%)      Time     Calls       Avg       Min       Max  Name\r\n 44.07%  130.029s    240276  541.17us     768ns  8.1151ms  [CUDA memcpy DtoH]\r\n 25.20%  74.3504s    480619  154.70us     992ns  8.9016ms  [CUDA memcpy HtoD]\r\n  9.84%  29.0339s    330000  87.981us  61.922us  610.68us  void tensorflow::GatherOpKernel<float, int>(float const *, int const *, tensorflow::GatherOpKernel<float, int>*, __int64, __int64, __int64)\r\n```\r\n"}