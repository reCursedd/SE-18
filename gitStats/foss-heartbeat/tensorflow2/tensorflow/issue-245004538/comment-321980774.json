{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321980774", "html_url": "https://github.com/tensorflow/tensorflow/issues/11709#issuecomment-321980774", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11709", "id": 321980774, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTk4MDc3NA==", "user": {"login": "nolanliou", "id": 30223680, "node_id": "MDQ6VXNlcjMwMjIzNjgw", "avatar_url": "https://avatars3.githubusercontent.com/u/30223680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nolanliou", "html_url": "https://github.com/nolanliou", "followers_url": "https://api.github.com/users/nolanliou/followers", "following_url": "https://api.github.com/users/nolanliou/following{/other_user}", "gists_url": "https://api.github.com/users/nolanliou/gists{/gist_id}", "starred_url": "https://api.github.com/users/nolanliou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nolanliou/subscriptions", "organizations_url": "https://api.github.com/users/nolanliou/orgs", "repos_url": "https://api.github.com/users/nolanliou/repos", "events_url": "https://api.github.com/users/nolanliou/events{/privacy}", "received_events_url": "https://api.github.com/users/nolanliou/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-12T13:23:04Z", "updated_at": "2017-08-12T13:23:04Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a></p>\n<h3>System information</h3>\n<ul>\n<li><em><strong>Code</strong></em><br>\n<a href=\"https://gist.github.com/nolanliou/c00af5938b2aecfdc5ea1189426b8624\">embedding_lookup_sparse</a></li>\n<li><em><strong>Params</strong></em><br>\n100M * 1K</li>\n<li><em><strong>Id</strong></em><br>\n256 * 1000</li>\n<li><em><strong>CPU</strong></em><br>\n32-cores</li>\n<li><em><strong>Tensorflow</strong></em><br>\nr1.3</li>\n</ul>\n<h3>Conclusion</h3>\n<p>On a single machine,  profiling the <code>embedding_lookup_sparse</code> with tfprof, the result shows that <code>gather</code> Op takes a lot of time. I checked the code and found that the CPU version <code>gather</code> op is single-thread.<br>\nThen I modify the <code>gather</code> Op to multi-threads, the result shows about 6x speedup, which benifites a lot of cases. If neccessary, I can post a PR.</p>\n<h3>Profiling result</h3>\n<pre><code>node name | requested bytes | total execution time | accelerator execution t    ime | cpu execution time`\n# old `gather`\nGather     2048.00MB (100.00%, 2.27%),   622.96ms (99.99%, 22.97%),    0us (0.00%, 0.00%),  622.96ms (99.99%, 22.97%)\n# multi-thread `gather`\nGather    2048.00MB (100.00%, 49.96%),   107.91ms (99.99%, 5.50%),     0us (0.00%, 0.00%),   107.91ms (99.99%, 5.50%)\n</code></pre>", "body_text": "@alextp\nSystem information\n\nCode\nembedding_lookup_sparse\nParams\n100M * 1K\nId\n256 * 1000\nCPU\n32-cores\nTensorflow\nr1.3\n\nConclusion\nOn a single machine,  profiling the embedding_lookup_sparse with tfprof, the result shows that gather Op takes a lot of time. I checked the code and found that the CPU version gather op is single-thread.\nThen I modify the gather Op to multi-threads, the result shows about 6x speedup, which benifites a lot of cases. If neccessary, I can post a PR.\nProfiling result\nnode name | requested bytes | total execution time | accelerator execution t    ime | cpu execution time`\n# old `gather`\nGather     2048.00MB (100.00%, 2.27%),   622.96ms (99.99%, 22.97%),    0us (0.00%, 0.00%),  622.96ms (99.99%, 22.97%)\n# multi-thread `gather`\nGather    2048.00MB (100.00%, 49.96%),   107.91ms (99.99%, 5.50%),     0us (0.00%, 0.00%),   107.91ms (99.99%, 5.50%)", "body": "@alextp\r\n### System information\r\n* ***Code***\r\n[embedding_lookup_sparse](https://gist.github.com/nolanliou/c00af5938b2aecfdc5ea1189426b8624)\r\n* ***Params***\r\n100M * 1K\r\n* ***Id***\r\n256 * 1000\r\n* ***CPU***\r\n32-cores\r\n* ***Tensorflow***\r\nr1.3\r\n### Conclusion\r\nOn a single machine,  profiling the `embedding_lookup_sparse` with tfprof, the result shows that `gather` Op takes a lot of time. I checked the code and found that the CPU version `gather` op is single-thread. \r\nThen I modify the `gather` Op to multi-threads, the result shows about 6x speedup, which benifites a lot of cases. If neccessary, I can post a PR.\r\n\r\n### Profiling result\r\n```\r\nnode name | requested bytes | total execution time | accelerator execution t    ime | cpu execution time`\r\n# old `gather`\r\nGather     2048.00MB (100.00%, 2.27%),   622.96ms (99.99%, 22.97%),    0us (0.00%, 0.00%),  622.96ms (99.99%, 22.97%)\r\n# multi-thread `gather`\r\nGather    2048.00MB (100.00%, 49.96%),   107.91ms (99.99%, 5.50%),     0us (0.00%, 0.00%),   107.91ms (99.99%, 5.50%)\r\n```"}