{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/155906081", "html_url": "https://github.com/tensorflow/tensorflow/issues/120#issuecomment-155906081", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/120", "id": 155906081, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NTkwNjA4MQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-11T20:51:22Z", "updated_at": "2015-11-11T20:51:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'd define it in terms of the number of floating-point operations needed to compute the result of the step. A 3 x 3 matrix multiplication (to take the <code>time_matmul</code> benchmark as an example) uses very few floating-point operations compared to the constant framework overhead. It would be quicker to do the computation than dispatch it to another framework. Similarly, it would almost certainly not be worth offloading that computation to a GPU, because of the overheads in dispatching and fetching the results of a kernel. By contrast, larger matrix multiplications an convolutions have a high flop count, and will tend to benefit from this approach.</p>\n<p>It would still be very informative to learn how the overhead changes with the size of the data. Would you consider adding different sizes of input for each of the workloads?</p>", "body_text": "I'd define it in terms of the number of floating-point operations needed to compute the result of the step. A 3 x 3 matrix multiplication (to take the time_matmul benchmark as an example) uses very few floating-point operations compared to the constant framework overhead. It would be quicker to do the computation than dispatch it to another framework. Similarly, it would almost certainly not be worth offloading that computation to a GPU, because of the overheads in dispatching and fetching the results of a kernel. By contrast, larger matrix multiplications an convolutions have a high flop count, and will tend to benefit from this approach.\nIt would still be very informative to learn how the overhead changes with the size of the data. Would you consider adding different sizes of input for each of the workloads?", "body": "I'd define it in terms of the number of floating-point operations needed to compute the result of the step. A 3 x 3 matrix multiplication (to take the `time_matmul` benchmark as an example) uses very few floating-point operations compared to the constant framework overhead. It would be quicker to do the computation than dispatch it to another framework. Similarly, it would almost certainly not be worth offloading that computation to a GPU, because of the overheads in dispatching and fetching the results of a kernel. By contrast, larger matrix multiplications an convolutions have a high flop count, and will tend to benefit from this approach.\n\nIt would still be very informative to learn how the overhead changes with the size of the data. Would you consider adding different sizes of input for each of the workloads?\n"}