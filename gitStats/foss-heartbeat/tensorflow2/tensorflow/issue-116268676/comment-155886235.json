{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/155886235", "html_url": "https://github.com/tensorflow/tensorflow/issues/120#issuecomment-155886235", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/120", "id": 155886235, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NTg4NjIzNQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-11T19:27:38Z", "updated_at": "2015-11-11T19:27:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Your benchmarks seem to align with what I can measure on my laptop. For example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">with</span> tf.Graph().as_default():\n<span class=\"pl-c1\">...</span>   x <span class=\"pl-k\">=</span> tf.constant(np.random.rand(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>).astype(np.float32))\n<span class=\"pl-c1\">...</span>   y <span class=\"pl-k\">=</span> tf.constant(np.random.rand(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>).astype(np.float32))\n<span class=\"pl-c1\">...</span>   z <span class=\"pl-k\">=</span> tf.matmul(x, y)\n<span class=\"pl-c1\">...</span>   <span class=\"pl-k\">with</span> tf.Session():\n<span class=\"pl-c1\">...</span>     start <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-c1\">...</span>     <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">10000</span>):\n<span class=\"pl-c1\">...</span>       _ <span class=\"pl-k\">=</span> z.eval()\n<span class=\"pl-c1\">...</span>     end <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> end <span class=\"pl-k\">-</span> start\n<span class=\"pl-c1\">1.6025779247283936</span></pre></div>\n<p>I should add that the operations in your benchmark are very small - computable in a time that is on the order of microseconds or nanoseconds - compared to the size of computation for which TensorFlow is designed. While it would be nice to reduce any unnecessary overhead from step and op dispatch, it is unlikely that doing so would dramatically reduce the time taken to run an inference or training step in a realistic neural network. With that said, thanks for looking into this, and if you have any suggestions for how to reduce this overhead, we would be glad to hear them!</p>", "body_text": "Your benchmarks seem to align with what I can measure on my laptop. For example:\n>>> with tf.Graph().as_default():\n...   x = tf.constant(np.random.rand(3, 3).astype(np.float32))\n...   y = tf.constant(np.random.rand(3, 3).astype(np.float32))\n...   z = tf.matmul(x, y)\n...   with tf.Session():\n...     start = time.time()\n...     for _ in xrange(10000):\n...       _ = z.eval()\n...     end = time.time()\n>>> end - start\n1.6025779247283936\nI should add that the operations in your benchmark are very small - computable in a time that is on the order of microseconds or nanoseconds - compared to the size of computation for which TensorFlow is designed. While it would be nice to reduce any unnecessary overhead from step and op dispatch, it is unlikely that doing so would dramatically reduce the time taken to run an inference or training step in a realistic neural network. With that said, thanks for looking into this, and if you have any suggestions for how to reduce this overhead, we would be glad to hear them!", "body": "Your benchmarks seem to align with what I can measure on my laptop. For example:\n\n``` python\n>>> with tf.Graph().as_default():\n...   x = tf.constant(np.random.rand(3, 3).astype(np.float32))\n...   y = tf.constant(np.random.rand(3, 3).astype(np.float32))\n...   z = tf.matmul(x, y)\n...   with tf.Session():\n...     start = time.time()\n...     for _ in xrange(10000):\n...       _ = z.eval()\n...     end = time.time()\n>>> end - start\n1.6025779247283936\n```\n\nI should add that the operations in your benchmark are very small - computable in a time that is on the order of microseconds or nanoseconds - compared to the size of computation for which TensorFlow is designed. While it would be nice to reduce any unnecessary overhead from step and op dispatch, it is unlikely that doing so would dramatically reduce the time taken to run an inference or training step in a realistic neural network. With that said, thanks for looking into this, and if you have any suggestions for how to reduce this overhead, we would be glad to hear them!\n"}