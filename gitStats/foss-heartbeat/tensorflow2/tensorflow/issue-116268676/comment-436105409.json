{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436105409", "html_url": "https://github.com/tensorflow/tensorflow/issues/120#issuecomment-436105409", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/120", "id": 436105409, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjEwNTQwOQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-06T02:03:43Z", "updated_at": "2018-11-06T02:03:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7721540\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/TimZaman\">@TimZaman</a> Just speculating, but I think that a large fraction of the cost might come from the first call to <code>c.eval()</code>, which performs various one-time startup activities (and has generally grown in responsibility since 2015). I'd hope that the subsequent steps are faster than 200us per call. You might be interested in looking here to see some of the ways to reduce the overhead of invoking a graph:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session_benchmark.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session_benchmark.py</a></p>\n<p>However, the recent engineering focus has been on making single-op execution fast in eager mode, and using TensorFlow functions as a replacement for <code>sess.run()</code>.</p>", "body_text": "@TimZaman Just speculating, but I think that a large fraction of the cost might come from the first call to c.eval(), which performs various one-time startup activities (and has generally grown in responsibility since 2015). I'd hope that the subsequent steps are faster than 200us per call. You might be interested in looking here to see some of the ways to reduce the overhead of invoking a graph:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session_benchmark.py\nHowever, the recent engineering focus has been on making single-op execution fast in eager mode, and using TensorFlow functions as a replacement for sess.run().", "body": "@TimZaman Just speculating, but I think that a large fraction of the cost might come from the first call to `c.eval()`, which performs various one-time startup activities (and has generally grown in responsibility since 2015). I'd hope that the subsequent steps are faster than 200us per call. You might be interested in looking here to see some of the ways to reduce the overhead of invoking a graph:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session_benchmark.py\r\n\r\nHowever, the recent engineering focus has been on making single-op execution fast in eager mode, and using TensorFlow functions as a replacement for `sess.run()`."}