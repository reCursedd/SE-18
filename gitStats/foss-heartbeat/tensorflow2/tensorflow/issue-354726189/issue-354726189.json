{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21923", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21923/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21923/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21923/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21923", "id": 354726189, "node_id": "MDU6SXNzdWUzNTQ3MjYxODk=", "number": 21923, "title": "\u00c0 trous 1D convolution slow in certain scenarios ", "user": {"login": "zbitouzakaria", "id": 12535244, "node_id": "MDQ6VXNlcjEyNTM1MjQ0", "avatar_url": "https://avatars0.githubusercontent.com/u/12535244?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zbitouzakaria", "html_url": "https://github.com/zbitouzakaria", "followers_url": "https://api.github.com/users/zbitouzakaria/followers", "following_url": "https://api.github.com/users/zbitouzakaria/following{/other_user}", "gists_url": "https://api.github.com/users/zbitouzakaria/gists{/gist_id}", "starred_url": "https://api.github.com/users/zbitouzakaria/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zbitouzakaria/subscriptions", "organizations_url": "https://api.github.com/users/zbitouzakaria/orgs", "repos_url": "https://api.github.com/users/zbitouzakaria/repos", "events_url": "https://api.github.com/users/zbitouzakaria/events{/privacy}", "received_events_url": "https://api.github.com/users/zbitouzakaria/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-08-28T13:15:35Z", "updated_at": "2018-11-23T18:39:03Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10.1</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0.176</li>\n<li><strong>GPU model and memory</strong>: CPU</li>\n<li><strong>Exact command to reproduce</strong>: See below</li>\n<li><strong>Bazel version</strong>: N/A</li>\n<li><strong>Mobile device</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Running</p>\n<p><code>tf.nn.atrous_conv2d(images, filters, rate=dilation_rate, padding=\"SAME\")</code></p>\n<p>and</p>\n<p><code>tf.nn.convolution(images, filters, dilation_rate=[dilation_rate, dilation_rate], padding=\"SAME\")</code></p>\n<p>takes much more time to run than:</p>\n<p><code>tf.nn.convolution(images, filters, dilation_rate=[1, dilation_rate], padding=\"SAME\")</code></p>\n<p>for <code>1D</code> convolutions.</p>\n<p>I don't think this is supposed to behave like this, provided that the input data is one dimensional along the width or the height (i.e. <code>shape = [batch, 1, width, channels] or [batch, height, 1, channels]</code>) since in this case, no dilation is needed on the corresponding axis.</p>\n<p>A person using <code>tf.nn.atrous_conv2d</code> for <code>1D</code> \"\u00e0 trous\" convolutions might never notice the issue here, and his network will be really slow to train.</p>\n<h3>Source code / logs</h3>\n<p>A small snippet to compare the performance of the different methods</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> time\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Scenario</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nchannels <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\no_channels <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nimage_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">160</span>\nfilter_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Dummy images and filters</span>\nimages <span class=\"pl-k\">=</span> tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[batch_size, <span class=\"pl-c1\">1</span>, image_size, channels],  <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nfilters <span class=\"pl-k\">=</span> tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, filter_size, channels, o_channels], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Ops definition</span>\nnormal_conv <span class=\"pl-k\">=</span> tf.nn.convolution(images, filters, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SAME<span class=\"pl-pds\">\"</span></span>)\natrous_convs <span class=\"pl-k\">=</span> []\natrous_convs_wrapper <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">50</span>]:\n    atrous_convs.append({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dilation<span class=\"pl-pds\">\"</span></span>: i, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>op<span class=\"pl-pds\">\"</span></span>: tf.nn.convolution(images, filters, <span class=\"pl-v\">dilation_rate</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, i], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SAME<span class=\"pl-pds\">\"</span></span>)})\n    atrous_convs_wrapper.append({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dilation<span class=\"pl-pds\">\"</span></span>: i, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>op<span class=\"pl-pds\">\"</span></span>: tf.nn.convolution(images, filters, <span class=\"pl-v\">dilation_rate</span><span class=\"pl-k\">=</span>[i, i], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SAME<span class=\"pl-pds\">\"</span></span>)})\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Or</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> atrous_convs_wrapper.append({\"dilation\": i, \"op\": tf.nn.atrous_conv2d(images, filters, rate=i, padding=\"SAME\")})</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Test</span>\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    repeats <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span>\n    \n    <span class=\"pl-k\">for</span> conv <span class=\"pl-k\">in</span> atrous_convs:\n        op1 <span class=\"pl-k\">=</span> conv[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>op<span class=\"pl-pds\">\"</span></span>]\n        op2 <span class=\"pl-k\">=</span> [elt <span class=\"pl-k\">for</span> elt <span class=\"pl-k\">in</span> atrous_convs_wrapper <span class=\"pl-k\">if</span> elt[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dilation<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">==</span> conv[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dilation<span class=\"pl-pds\">\"</span></span>]][<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>op<span class=\"pl-pds\">\"</span></span>]\n        op1_res, op2_res <span class=\"pl-k\">=</span> sess.run([op1, op2])\n        np.testing.assert_equal(op1_res, op2_res)\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Benchmark normal method</span>\n    start <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(repeats):\n        _ <span class=\"pl-k\">=</span> sess.run(normal_conv)\n    end <span class=\"pl-k\">=</span> time.time()\n    normal_conv_time <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>((end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">/</span> repeats <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Benchmark atrous conv method</span>\n    <span class=\"pl-k\">for</span> conv <span class=\"pl-k\">in</span> atrous_convs:\n        start <span class=\"pl-k\">=</span> time.time()\n        <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(repeats):\n            _ <span class=\"pl-k\">=</span> sess.run(conv[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>op<span class=\"pl-pds\">\"</span></span>])\n        end <span class=\"pl-k\">=</span> time.time()\n        conv.update({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">int</span>((end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">/</span> repeats <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>)})\n    \n    <span class=\"pl-k\">for</span> conv <span class=\"pl-k\">in</span> atrous_convs_wrapper:\n        start <span class=\"pl-k\">=</span> time.time()\n        <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(repeats):\n            _ <span class=\"pl-k\">=</span> sess.run(conv[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>op<span class=\"pl-pds\">\"</span></span>])\n        end <span class=\"pl-k\">=</span> time.time()\n        conv.update({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">int</span>((end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">/</span> repeats <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>)})\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Normal conv: <span class=\"pl-c1\">{}</span>ms<span class=\"pl-pds\">\"</span></span>.format(normal_conv_time))\n    <span class=\"pl-k\">for</span> conv <span class=\"pl-k\">in</span> atrous_convs:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Atrous conv w/ dilation <span class=\"pl-c1\">{}</span>: <span class=\"pl-c1\">{}</span>ms<span class=\"pl-pds\">\"</span></span>.format(conv[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dilation<span class=\"pl-pds\">\"</span></span>], conv[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span>]))\n    <span class=\"pl-k\">for</span> conv <span class=\"pl-k\">in</span> atrous_convs_wrapper:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Atrous conv(wrapper) w/ dilation <span class=\"pl-c1\">{}</span>: <span class=\"pl-c1\">{}</span>ms<span class=\"pl-pds\">\"</span></span>.format(conv[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dilation<span class=\"pl-pds\">\"</span></span>], conv[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span>]))\n</pre></div>\n<p>returns</p>\n<pre><code>Normal conv: 1ms\nAtrous conv w/ dilation 5: 1ms\nAtrous conv w/ dilation 10: 2ms\nAtrous conv w/ dilation 20: 1ms\nAtrous conv w/ dilation 50: 2ms\nAtrous conv(wrapper) w/ dilation 5: 4ms\nAtrous conv(wrapper) w/ dilation 10: 9ms\nAtrous conv(wrapper) w/ dilation 20: 15ms\nAtrous conv(wrapper) w/ dilation 50: 47ms\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.10.1\nPython version: 3.5.2\nCUDA/cuDNN version: 9.0.176\nGPU model and memory: CPU\nExact command to reproduce: See below\nBazel version: N/A\nMobile device: N/A\n\nDescribe the problem\nRunning\ntf.nn.atrous_conv2d(images, filters, rate=dilation_rate, padding=\"SAME\")\nand\ntf.nn.convolution(images, filters, dilation_rate=[dilation_rate, dilation_rate], padding=\"SAME\")\ntakes much more time to run than:\ntf.nn.convolution(images, filters, dilation_rate=[1, dilation_rate], padding=\"SAME\")\nfor 1D convolutions.\nI don't think this is supposed to behave like this, provided that the input data is one dimensional along the width or the height (i.e. shape = [batch, 1, width, channels] or [batch, height, 1, channels]) since in this case, no dilation is needed on the corresponding axis.\nA person using tf.nn.atrous_conv2d for 1D \"\u00e0 trous\" convolutions might never notice the issue here, and his network will be really slow to train.\nSource code / logs\nA small snippet to compare the performance of the different methods\nimport tensorflow as tf\nimport numpy as np\nimport time\n\n# Scenario\nbatch_size = 100\nchannels = 3\no_channels = 10\nimage_size = 160\nfilter_size = 3\n\n# Dummy images and filters\nimages = tf.random_normal(shape=[batch_size, 1, image_size, channels],  dtype=tf.float32)\nfilters = tf.random_normal(shape=[1, filter_size, channels, o_channels], dtype=tf.float32)\n\n# Ops definition\nnormal_conv = tf.nn.convolution(images, filters, padding=\"SAME\")\natrous_convs = []\natrous_convs_wrapper = []\nfor i in [5, 10, 20, 50]:\n    atrous_convs.append({\"dilation\": i, \"op\": tf.nn.convolution(images, filters, dilation_rate=[1, i], padding=\"SAME\")})\n    atrous_convs_wrapper.append({\"dilation\": i, \"op\": tf.nn.convolution(images, filters, dilation_rate=[i, i], padding=\"SAME\")})\n    # Or\n    # atrous_convs_wrapper.append({\"dilation\": i, \"op\": tf.nn.atrous_conv2d(images, filters, rate=i, padding=\"SAME\")})\n# Test\nwith tf.Session() as sess:\n    repeats = 50\n    \n    for conv in atrous_convs:\n        op1 = conv[\"op\"]\n        op2 = [elt for elt in atrous_convs_wrapper if elt[\"dilation\"] == conv[\"dilation\"]][0][\"op\"]\n        op1_res, op2_res = sess.run([op1, op2])\n        np.testing.assert_equal(op1_res, op2_res)\n    \n    # Benchmark normal method\n    start = time.time()\n    for _ in range(repeats):\n        _ = sess.run(normal_conv)\n    end = time.time()\n    normal_conv_time = int((end - start) / repeats * 1000)\n\n    # Benchmark atrous conv method\n    for conv in atrous_convs:\n        start = time.time()\n        for _ in range(repeats):\n            _ = sess.run(conv[\"op\"])\n        end = time.time()\n        conv.update({\"time\": int((end - start) / repeats * 1000)})\n    \n    for conv in atrous_convs_wrapper:\n        start = time.time()\n        for _ in range(repeats):\n            _ = sess.run(conv[\"op\"])\n        end = time.time()\n        conv.update({\"time\": int((end - start) / repeats * 1000)})\n\n    print(\"Normal conv: {}ms\".format(normal_conv_time))\n    for conv in atrous_convs:\n        print(\"Atrous conv w/ dilation {}: {}ms\".format(conv[\"dilation\"], conv[\"time\"]))\n    for conv in atrous_convs_wrapper:\n        print(\"Atrous conv(wrapper) w/ dilation {}: {}ms\".format(conv[\"dilation\"], conv[\"time\"]))\n\nreturns\nNormal conv: 1ms\nAtrous conv w/ dilation 5: 1ms\nAtrous conv w/ dilation 10: 2ms\nAtrous conv w/ dilation 20: 1ms\nAtrous conv w/ dilation 50: 2ms\nAtrous conv(wrapper) w/ dilation 5: 4ms\nAtrous conv(wrapper) w/ dilation 10: 9ms\nAtrous conv(wrapper) w/ dilation 20: 15ms\nAtrous conv(wrapper) w/ dilation 50: 47ms", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: 9.0.176\r\n- **GPU model and memory**: CPU\r\n- **Exact command to reproduce**: See below\r\n- **Bazel version**: N/A\r\n- **Mobile device**: N/A\r\n\r\n### Describe the problem\r\nRunning\r\n\r\n``\r\ntf.nn.atrous_conv2d(images, filters, rate=dilation_rate, padding=\"SAME\")\r\n``\r\n\r\nand\r\n\r\n``\r\ntf.nn.convolution(images, filters, dilation_rate=[dilation_rate, dilation_rate], padding=\"SAME\")\r\n``\r\n\r\ntakes much more time to run than:\r\n\r\n``\r\ntf.nn.convolution(images, filters, dilation_rate=[1, dilation_rate], padding=\"SAME\")\r\n``\r\n\r\nfor `1D` convolutions.\r\n\r\nI don't think this is supposed to behave like this, provided that the input data is one dimensional along the width or the height (i.e. ``shape = [batch, 1, width, channels] or [batch, height, 1, channels]``) since in this case, no dilation is needed on the corresponding axis.\r\n\r\nA person using `tf.nn.atrous_conv2d` for `1D` \"\u00e0 trous\" convolutions might never notice the issue here, and his network will be really slow to train.\r\n\r\n### Source code / logs\r\nA small snippet to compare the performance of the different methods\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\n# Scenario\r\nbatch_size = 100\r\nchannels = 3\r\no_channels = 10\r\nimage_size = 160\r\nfilter_size = 3\r\n\r\n# Dummy images and filters\r\nimages = tf.random_normal(shape=[batch_size, 1, image_size, channels],  dtype=tf.float32)\r\nfilters = tf.random_normal(shape=[1, filter_size, channels, o_channels], dtype=tf.float32)\r\n\r\n# Ops definition\r\nnormal_conv = tf.nn.convolution(images, filters, padding=\"SAME\")\r\natrous_convs = []\r\natrous_convs_wrapper = []\r\nfor i in [5, 10, 20, 50]:\r\n    atrous_convs.append({\"dilation\": i, \"op\": tf.nn.convolution(images, filters, dilation_rate=[1, i], padding=\"SAME\")})\r\n    atrous_convs_wrapper.append({\"dilation\": i, \"op\": tf.nn.convolution(images, filters, dilation_rate=[i, i], padding=\"SAME\")})\r\n    # Or\r\n    # atrous_convs_wrapper.append({\"dilation\": i, \"op\": tf.nn.atrous_conv2d(images, filters, rate=i, padding=\"SAME\")})\r\n# Test\r\nwith tf.Session() as sess:\r\n    repeats = 50\r\n    \r\n    for conv in atrous_convs:\r\n        op1 = conv[\"op\"]\r\n        op2 = [elt for elt in atrous_convs_wrapper if elt[\"dilation\"] == conv[\"dilation\"]][0][\"op\"]\r\n        op1_res, op2_res = sess.run([op1, op2])\r\n        np.testing.assert_equal(op1_res, op2_res)\r\n    \r\n    # Benchmark normal method\r\n    start = time.time()\r\n    for _ in range(repeats):\r\n        _ = sess.run(normal_conv)\r\n    end = time.time()\r\n    normal_conv_time = int((end - start) / repeats * 1000)\r\n\r\n    # Benchmark atrous conv method\r\n    for conv in atrous_convs:\r\n        start = time.time()\r\n        for _ in range(repeats):\r\n            _ = sess.run(conv[\"op\"])\r\n        end = time.time()\r\n        conv.update({\"time\": int((end - start) / repeats * 1000)})\r\n    \r\n    for conv in atrous_convs_wrapper:\r\n        start = time.time()\r\n        for _ in range(repeats):\r\n            _ = sess.run(conv[\"op\"])\r\n        end = time.time()\r\n        conv.update({\"time\": int((end - start) / repeats * 1000)})\r\n\r\n    print(\"Normal conv: {}ms\".format(normal_conv_time))\r\n    for conv in atrous_convs:\r\n        print(\"Atrous conv w/ dilation {}: {}ms\".format(conv[\"dilation\"], conv[\"time\"]))\r\n    for conv in atrous_convs_wrapper:\r\n        print(\"Atrous conv(wrapper) w/ dilation {}: {}ms\".format(conv[\"dilation\"], conv[\"time\"]))\r\n\r\n```\r\n\r\nreturns\r\n\r\n```\r\nNormal conv: 1ms\r\nAtrous conv w/ dilation 5: 1ms\r\nAtrous conv w/ dilation 10: 2ms\r\nAtrous conv w/ dilation 20: 1ms\r\nAtrous conv w/ dilation 50: 2ms\r\nAtrous conv(wrapper) w/ dilation 5: 4ms\r\nAtrous conv(wrapper) w/ dilation 10: 9ms\r\nAtrous conv(wrapper) w/ dilation 20: 15ms\r\nAtrous conv(wrapper) w/ dilation 50: 47ms\r\n```"}