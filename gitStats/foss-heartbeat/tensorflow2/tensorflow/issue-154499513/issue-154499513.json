{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2337", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2337/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2337/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2337/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/2337", "id": 154499513, "node_id": "MDExOlB1bGxSZXF1ZXN0Njk4NDgzNDI=", "number": 2337, "title": "Update seq2seq", "user": {"login": "raingo", "id": 606565, "node_id": "MDQ6VXNlcjYwNjU2NQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/606565?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raingo", "html_url": "https://github.com/raingo", "followers_url": "https://api.github.com/users/raingo/followers", "following_url": "https://api.github.com/users/raingo/following{/other_user}", "gists_url": "https://api.github.com/users/raingo/gists{/gist_id}", "starred_url": "https://api.github.com/users/raingo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raingo/subscriptions", "organizations_url": "https://api.github.com/users/raingo/orgs", "repos_url": "https://api.github.com/users/raingo/repos", "events_url": "https://api.github.com/users/raingo/events{/privacy}", "received_events_url": "https://api.github.com/users/raingo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-05-12T14:39:54Z", "updated_at": "2016-05-25T00:05:08Z", "closed_at": "2016-05-25T00:05:08Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2337", "html_url": "https://github.com/tensorflow/tensorflow/pull/2337", "diff_url": "https://github.com/tensorflow/tensorflow/pull/2337.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/2337.patch"}, "body_html": "<p>This PR brings updates to the seq2seq examples, removing all of the <code>with ops.device(\"/cpu:0\"):</code>.</p>\n<p>There are many ops implemented with cuda kernels recently, so maybe it's time to remove some cpu device specifications from the translate.py examples.</p>\n<p>I don't have profile numbers for the translate.py example, but according to a similar experiment, this will boost the speed 5~6 times on a NVIDIA Titan GPU, especially when there are more than one independent runs going together.</p>\n<p>Side note: when training with multiple cards (independent runs), the frequent data transfer between gpu and cpu is particularly harmful in my case.</p>", "body_text": "This PR brings updates to the seq2seq examples, removing all of the with ops.device(\"/cpu:0\"):.\nThere are many ops implemented with cuda kernels recently, so maybe it's time to remove some cpu device specifications from the translate.py examples.\nI don't have profile numbers for the translate.py example, but according to a similar experiment, this will boost the speed 5~6 times on a NVIDIA Titan GPU, especially when there are more than one independent runs going together.\nSide note: when training with multiple cards (independent runs), the frequent data transfer between gpu and cpu is particularly harmful in my case.", "body": "This PR brings updates to the seq2seq examples, removing all of the `with ops.device(\"/cpu:0\"):`. \n\nThere are many ops implemented with cuda kernels recently, so maybe it's time to remove some cpu device specifications from the translate.py examples.\n\nI don't have profile numbers for the translate.py example, but according to a similar experiment, this will boost the speed 5~6 times on a NVIDIA Titan GPU, especially when there are more than one independent runs going together.\n\nSide note: when training with multiple cards (independent runs), the frequent data transfer between gpu and cpu is particularly harmful in my case.\n"}