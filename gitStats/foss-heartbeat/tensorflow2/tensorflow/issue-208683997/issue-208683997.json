{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7658", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7658/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7658/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7658/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7658", "id": 208683997, "node_id": "MDU6SXNzdWUyMDg2ODM5OTc=", "number": 7658, "title": "Optimizer var_list does not have effect on the excluded variable!", "user": {"login": "saman-aghazadeh", "id": 3586023, "node_id": "MDQ6VXNlcjM1ODYwMjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3586023?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saman-aghazadeh", "html_url": "https://github.com/saman-aghazadeh", "followers_url": "https://api.github.com/users/saman-aghazadeh/followers", "following_url": "https://api.github.com/users/saman-aghazadeh/following{/other_user}", "gists_url": "https://api.github.com/users/saman-aghazadeh/gists{/gist_id}", "starred_url": "https://api.github.com/users/saman-aghazadeh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saman-aghazadeh/subscriptions", "organizations_url": "https://api.github.com/users/saman-aghazadeh/orgs", "repos_url": "https://api.github.com/users/saman-aghazadeh/repos", "events_url": "https://api.github.com/users/saman-aghazadeh/events{/privacy}", "received_events_url": "https://api.github.com/users/saman-aghazadeh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-02-19T00:50:43Z", "updated_at": "2017-06-16T18:37:55Z", "closed_at": "2017-06-16T18:37:55Z", "author_association": "NONE", "body_html": "<p>I have written piece of TensorFlow code which has two optimizers and I would like to exclude specific variables from being updated while calling \"run\" on any of these optimizers. As suggested by the TensorFlow documentation, I have specifically generated a list of variables to be updated for each optimizer, like below:</p>\n<pre><code>```\n</code></pre>\n<p>mentor_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"mentor\")<br>\ntrain_op_mentor = mnist.training(loss_mentor, FLAGS.learning_rate, mentor_training_vars)<br>\nmentee_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"mentee\")<br>\ntrain_op_mentee = mnist.training(loss_mentee, FLAGS.learning_rate, mentee_training_vars)<br>\nmentee_indep_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"mentee_indep\")<br>\ntrain_op_mentee_indep = mnist.training(loss_mentee_indep, FLAGS.learning_rate, mentee_indep_training_vars)</p>\n<pre><code>\nThe training functions in the mnist object is defined as:L\n\ndef training(loss, learning_rate, var_list):\n  # Add a scalar summary for the snapshot loss.\n  tf.summary.scalar('loss', loss)\n  # Create the gradient descent optimizer with the given learning rate.\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  # Create a variable to track the global step.\n  global_step = tf.Variable(0, name='global_step', trainable=False)\n  # Use the optimizer to apply the gradients that minimize the loss\n  # (and also increment the global step counter) as a single training step.\n  train_op = optimizer.minimize(loss, global_step=global_step, var_list=var_list)\n  return train_op\n\n\n</code></pre>\n<p>As it's clear in the above code, I have three namescopes, where each has their own variables.</p>\n<p>Now, let's say I only want to train the mentor variables. When I put a breakpoint after running session on the mentor optimizer, I can see that the mentee variables content is being changed after each run. Now I'm wondering whether I'm using this feature correctly, or there is something wrong with this API?</p>", "body_text": "I have written piece of TensorFlow code which has two optimizers and I would like to exclude specific variables from being updated while calling \"run\" on any of these optimizers. As suggested by the TensorFlow documentation, I have specifically generated a list of variables to be updated for each optimizer, like below:\n```\n\nmentor_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"mentor\")\ntrain_op_mentor = mnist.training(loss_mentor, FLAGS.learning_rate, mentor_training_vars)\nmentee_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"mentee\")\ntrain_op_mentee = mnist.training(loss_mentee, FLAGS.learning_rate, mentee_training_vars)\nmentee_indep_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"mentee_indep\")\ntrain_op_mentee_indep = mnist.training(loss_mentee_indep, FLAGS.learning_rate, mentee_indep_training_vars)\n\nThe training functions in the mnist object is defined as:L\n\ndef training(loss, learning_rate, var_list):\n  # Add a scalar summary for the snapshot loss.\n  tf.summary.scalar('loss', loss)\n  # Create the gradient descent optimizer with the given learning rate.\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  # Create a variable to track the global step.\n  global_step = tf.Variable(0, name='global_step', trainable=False)\n  # Use the optimizer to apply the gradients that minimize the loss\n  # (and also increment the global step counter) as a single training step.\n  train_op = optimizer.minimize(loss, global_step=global_step, var_list=var_list)\n  return train_op\n\n\n\nAs it's clear in the above code, I have three namescopes, where each has their own variables.\nNow, let's say I only want to train the mentor variables. When I put a breakpoint after running session on the mentor optimizer, I can see that the mentee variables content is being changed after each run. Now I'm wondering whether I'm using this feature correctly, or there is something wrong with this API?", "body": "I have written piece of TensorFlow code which has two optimizers and I would like to exclude specific variables from being updated while calling \"run\" on any of these optimizers. As suggested by the TensorFlow documentation, I have specifically generated a list of variables to be updated for each optimizer, like below:\r\n\r\n    ```\r\nmentor_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"mentor\")\r\n    train_op_mentor = mnist.training(loss_mentor, FLAGS.learning_rate, mentor_training_vars)\r\n    mentee_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"mentee\")\r\n    train_op_mentee = mnist.training(loss_mentee, FLAGS.learning_rate, mentee_training_vars)\r\n    mentee_indep_training_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"mentee_indep\")\r\n    train_op_mentee_indep = mnist.training(loss_mentee_indep, FLAGS.learning_rate, mentee_indep_training_vars)\r\n```\r\n\r\nThe training functions in the mnist object is defined as:L\r\n\r\ndef training(loss, learning_rate, var_list):\r\n  # Add a scalar summary for the snapshot loss.\r\n  tf.summary.scalar('loss', loss)\r\n  # Create the gradient descent optimizer with the given learning rate.\r\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n  # Create a variable to track the global step.\r\n  global_step = tf.Variable(0, name='global_step', trainable=False)\r\n  # Use the optimizer to apply the gradients that minimize the loss\r\n  # (and also increment the global step counter) as a single training step.\r\n  train_op = optimizer.minimize(loss, global_step=global_step, var_list=var_list)\r\n  return train_op\r\n\r\n\r\n```\r\n\r\nAs it's clear in the above code, I have three namescopes, where each has their own variables.\r\n\r\nNow, let's say I only want to train the mentor variables. When I put a breakpoint after running session on the mentor optimizer, I can see that the mentee variables content is being changed after each run. Now I'm wondering whether I'm using this feature correctly, or there is something wrong with this API?"}