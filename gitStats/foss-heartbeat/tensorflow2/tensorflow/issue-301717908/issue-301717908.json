{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17375", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17375/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17375/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17375/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17375", "id": 301717908, "node_id": "MDU6SXNzdWUzMDE3MTc5MDg=", "number": 17375, "title": "tensorflow lite cannot use my own model, crashed without error log.", "user": {"login": "BKZero", "id": 18680778, "node_id": "MDQ6VXNlcjE4NjgwNzc4", "avatar_url": "https://avatars2.githubusercontent.com/u/18680778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BKZero", "html_url": "https://github.com/BKZero", "followers_url": "https://api.github.com/users/BKZero/followers", "following_url": "https://api.github.com/users/BKZero/following{/other_user}", "gists_url": "https://api.github.com/users/BKZero/gists{/gist_id}", "starred_url": "https://api.github.com/users/BKZero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BKZero/subscriptions", "organizations_url": "https://api.github.com/users/BKZero/orgs", "repos_url": "https://api.github.com/users/BKZero/repos", "events_url": "https://api.github.com/users/BKZero/events{/privacy}", "received_events_url": "https://api.github.com/users/BKZero/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-02T10:18:28Z", "updated_at": "2018-03-08T17:23:38Z", "closed_at": "2018-03-08T17:23:31Z", "author_association": "NONE", "body_html": "<p><strong>Version Info:</strong><br>\ntensorflow r1.5<br>\nubuntu 14.04<br>\narmv7 platform<br>\nandroid 6.0.1 and pc<br>\nndk version: r14<br>\nandroid studio 2.3.1</p>\n<p><strong>Describe the problem</strong><br>\ni write the code in c++, it can get results when using the mobilenet model from the tutorails(link: <a href=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip\" rel=\"nofollow\">https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip</a>)<br>\nbut when i use my own model converted by toco, it crashed without any information. set input and load model seemed correct, but invoke failed.</p>\n<p><strong>code</strong></p>\n<p><strong>network define</strong><br>\nnetwork is defined by tf.slim framework:</p>\n<pre><code> def lite_v1(images, num_classes=10, is_training=False, dropout_keep_prob=0.5, prediction_fn=slim.softmax, scope='lite_v1'):\n    end_points = {}\n      with tf.variable_scope(scope, 'lite_v1', [images, num_classes]):\n        images_input = tf.placeholder_with_default(images, shape=[None, 39, 39, 1], name='InputPlaceholder')\n        net = slim.conv2d(images_input, 16, [4, 4], padding='VALID', scope='conv1')\n        end_points['conv1'] = net\n        feature = slim.flatten(net)\n    with tf.variable_scope('Logits'):\n      net = slim.fully_connected(feature, 100, scope='fc3')\n      logits = slim.fully_connected(net, num_classes,\n                              biases_initializer=tf.zeros_initializer(),\n                              weights_regularizer=None,\n                              activation_fn=None,\n                              scope='logits')\n      end_points['Logits'] = logits\n    output = tf.multiply(logits, 1, name=\"Output\")\n    end_points['Output'] = output\n  return logits, end_points\n</code></pre>\n<p><strong>model convert</strong><br>\ni trained it and use the freeze_graph.py and optimize_for_inference.py scripe in tensorflow/python/tools/ and get a frozen .pb file. then i convert it to a .tflite file by following the tutorials in github: <a href=\"https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/lite\">https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/lite</a> just like:</p>\n<pre><code>bazel build tensorflow/contrib/lite/toco:toco\nbazel-bin/tensorflow/contrib/lite/toco/toco -- \\\n  --input_file=$(pwd)/mobilenet_v1_1.0_224/frozen_graph.pb \\\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\n  --output_file=/tmp/mobilenet_v1_1.0_224.lite --inference_type=FLOAT \\\n  --input_type=FLOAT --input_arrays=input \\\n  --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3\n</code></pre>\n<p><strong>c++ test code</strong><br>\nafter get a .tflite file. and then i wrote the c++ code to read the model and image from sdcard:</p>\n<pre><code>void LoadImageFromFile(std::string file_name, std::vector&lt;uint8_t&gt;&amp; output, int&amp; out_width, int&amp; out_height, int&amp; out_channels)\n{\ncv::Mat image = cv::imread(file_name);\ncv::Mat gray;\ncv::cvtColor(image, gray, cv::COLOR_BGR2GRAY);\n\nout_width = gray.cols;\nout_height = gray.rows;\nout_channels = gray.channels();\nfor(int nrow =0; nrow &lt; out_height; nrow++)\n    for(int ncol = 0; ncol &lt; out_width; ncol++)\n        output.push_back(gray.at&lt;unsigned char&gt;(nrow,ncol));\n}\nsize_t writeByteBuffer(uint8_t* in, char** dst, int dst_size) {\n  char* buf = (char*)in;\n  if (!buf) {\n    return 0;\n  }\n  *dst = buf;\n  return dst_size;\n}\n\nint RunInferenceOnImage()\n{\nconst int num_threads = 1;\nstd::string input_layer_type = \"float\";\nstd::vector&lt;int&gt; sizes = {1, 39, 39, 1};\n//    std::vector&lt;int&gt; sizes = {1, 224, 224, 3};\nstd::string graph_path = \"/storage/emulated/0/DCIM/fastnetv2.tflite\"; //mobilenet_quant_v1_224.tflite\nstd::unique_ptr&lt;tflite::FlatBufferModel&gt; model(tflite::FlatBufferModel::BuildFromFile(graph_path.c_str()));\nif (!model)\n{\n    LOGD(\"bkzero jni: , Failed to mmap model %s\", graph_path.c_str());\n}\nmodel-&gt;error_reporter();\n\n#ifdef TFLITE_CUSTOM_OPS_HEADER\n  tflite::MutableOpResolver resolver;\n  RegisterSelectedOps(&amp;resolver);\n#else\n  tflite::ops::builtin::BuiltinOpResolver resolver;\n#endif\n\nLOGD(\"bkzero jni: , %s\", \"resolved reporter end\");\n\nstd::unique_ptr&lt;tflite::Interpreter&gt; interpreter;\ntflite::InterpreterBuilder(*model, resolver)(&amp;interpreter);\nif (!interpreter)\n{\n    LOGD(\"bkzero jni: , %s\", \"Failed to construct interpreter\");\n}\nif (num_threads != -1) {\n  interpreter-&gt;SetNumThreads(num_threads);\n}\nint input = interpreter-&gt;inputs()[0];\nif (input_layer_type != \"string\") {\n  interpreter-&gt;ResizeInputTensor(input, sizes);\n}\n\nif (interpreter-&gt;AllocateTensors() != kTfLiteOk)\n{\n    LOGD(\"bkzero jni: , %s\", \"Failed to allocate tensors!\");\n}\nstd::string image_path = \"/storage/emulated/0/DCIM/test2.jpg\";\nint image_width;\nint image_height;\nint image_channels;\nstd::vector&lt;uint8_t&gt; image_data;\nLoadImageFromFile(image_path, image_data, image_width, image_height, image_channels);\n\n//    const int wanted_width = 224;\n//    const int wanted_height = 224;\n//    const int wanted_channels = 3;\n//    const float input_mean = 127.5f;\n//    const float input_std = 127.5f;\n\nconst int wanted_width = 39;\nconst int wanted_height = 39;\nconst int wanted_channels = 1;\nconst float input_mean = 128.0f;\nconst float input_std = 64.0f;\nassert(image_channels &gt;= wanted_channels);\nuint8_t* in = image_data.data();\nfloat* out = interpreter-&gt;typed_tensor&lt;float&gt;(input);\nint input_idx = interpreter-&gt;inputs()[0];\nTfLiteTensor* target = interpreter-&gt;tensor(input_idx);\nint num_bytes = sizes[0]*sizes[1]*sizes[2]*sizes[3];\nwriteByteBuffer(in, &amp;(target-&gt;data.raw), static_cast&lt;int&gt;(num_bytes));\n    if (interpreter-&gt;Invoke() != kTfLiteOk)\n    {\n        LOGD(\"bkzero jni: , %s\", \"Failed to invoke!\");\n    }\n    const std::vector&lt;int&gt;&amp; results = interpreter-&gt;outputs();\n    if (results.empty()) {\n      LOGD(\"bkzero jni: , %s\", \"results.empty()\");\n    }\n    long outputs[results.size()];\n    size_t size = results.size();\n    for (int i = 0; i &lt; size; ++i) {\n      TfLiteTensor* source = interpreter-&gt;tensor(results[i]);\n      outputs[i] = reinterpret_cast&lt;long&gt;(source);\n    }\n    uint8_t* final = (uint8_t*)outputs[0];\n    for(int i = 0; i &lt; 10; i++)\n        LOGD(\"bkzero jni: results, %f\", (final[i]/255.0));\n    LOGD(\"bkzero jni: , %s\", \"interpreter outputs read finished\");\n}\n</code></pre>\n<p>this code can read and ouput result correct when run the example mobilenet file, mobilenet_quant_v1_224.tflite. but crash when i run my own model and give the error: Fatal signal 7 (SIGBUS), code 1, fault addr 0xdd3dd008 in tid 10528</p>\n<p>and on PC, it also crashes, without more information.</p>", "body_text": "Version Info:\ntensorflow r1.5\nubuntu 14.04\narmv7 platform\nandroid 6.0.1 and pc\nndk version: r14\nandroid studio 2.3.1\nDescribe the problem\ni write the code in c++, it can get results when using the mobilenet model from the tutorails(link: https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip)\nbut when i use my own model converted by toco, it crashed without any information. set input and load model seemed correct, but invoke failed.\ncode\nnetwork define\nnetwork is defined by tf.slim framework:\n def lite_v1(images, num_classes=10, is_training=False, dropout_keep_prob=0.5, prediction_fn=slim.softmax, scope='lite_v1'):\n    end_points = {}\n      with tf.variable_scope(scope, 'lite_v1', [images, num_classes]):\n        images_input = tf.placeholder_with_default(images, shape=[None, 39, 39, 1], name='InputPlaceholder')\n        net = slim.conv2d(images_input, 16, [4, 4], padding='VALID', scope='conv1')\n        end_points['conv1'] = net\n        feature = slim.flatten(net)\n    with tf.variable_scope('Logits'):\n      net = slim.fully_connected(feature, 100, scope='fc3')\n      logits = slim.fully_connected(net, num_classes,\n                              biases_initializer=tf.zeros_initializer(),\n                              weights_regularizer=None,\n                              activation_fn=None,\n                              scope='logits')\n      end_points['Logits'] = logits\n    output = tf.multiply(logits, 1, name=\"Output\")\n    end_points['Output'] = output\n  return logits, end_points\n\nmodel convert\ni trained it and use the freeze_graph.py and optimize_for_inference.py scripe in tensorflow/python/tools/ and get a frozen .pb file. then i convert it to a .tflite file by following the tutorials in github: https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/lite just like:\nbazel build tensorflow/contrib/lite/toco:toco\nbazel-bin/tensorflow/contrib/lite/toco/toco -- \\\n  --input_file=$(pwd)/mobilenet_v1_1.0_224/frozen_graph.pb \\\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\n  --output_file=/tmp/mobilenet_v1_1.0_224.lite --inference_type=FLOAT \\\n  --input_type=FLOAT --input_arrays=input \\\n  --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3\n\nc++ test code\nafter get a .tflite file. and then i wrote the c++ code to read the model and image from sdcard:\nvoid LoadImageFromFile(std::string file_name, std::vector<uint8_t>& output, int& out_width, int& out_height, int& out_channels)\n{\ncv::Mat image = cv::imread(file_name);\ncv::Mat gray;\ncv::cvtColor(image, gray, cv::COLOR_BGR2GRAY);\n\nout_width = gray.cols;\nout_height = gray.rows;\nout_channels = gray.channels();\nfor(int nrow =0; nrow < out_height; nrow++)\n    for(int ncol = 0; ncol < out_width; ncol++)\n        output.push_back(gray.at<unsigned char>(nrow,ncol));\n}\nsize_t writeByteBuffer(uint8_t* in, char** dst, int dst_size) {\n  char* buf = (char*)in;\n  if (!buf) {\n    return 0;\n  }\n  *dst = buf;\n  return dst_size;\n}\n\nint RunInferenceOnImage()\n{\nconst int num_threads = 1;\nstd::string input_layer_type = \"float\";\nstd::vector<int> sizes = {1, 39, 39, 1};\n//    std::vector<int> sizes = {1, 224, 224, 3};\nstd::string graph_path = \"/storage/emulated/0/DCIM/fastnetv2.tflite\"; //mobilenet_quant_v1_224.tflite\nstd::unique_ptr<tflite::FlatBufferModel> model(tflite::FlatBufferModel::BuildFromFile(graph_path.c_str()));\nif (!model)\n{\n    LOGD(\"bkzero jni: , Failed to mmap model %s\", graph_path.c_str());\n}\nmodel->error_reporter();\n\n#ifdef TFLITE_CUSTOM_OPS_HEADER\n  tflite::MutableOpResolver resolver;\n  RegisterSelectedOps(&resolver);\n#else\n  tflite::ops::builtin::BuiltinOpResolver resolver;\n#endif\n\nLOGD(\"bkzero jni: , %s\", \"resolved reporter end\");\n\nstd::unique_ptr<tflite::Interpreter> interpreter;\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\nif (!interpreter)\n{\n    LOGD(\"bkzero jni: , %s\", \"Failed to construct interpreter\");\n}\nif (num_threads != -1) {\n  interpreter->SetNumThreads(num_threads);\n}\nint input = interpreter->inputs()[0];\nif (input_layer_type != \"string\") {\n  interpreter->ResizeInputTensor(input, sizes);\n}\n\nif (interpreter->AllocateTensors() != kTfLiteOk)\n{\n    LOGD(\"bkzero jni: , %s\", \"Failed to allocate tensors!\");\n}\nstd::string image_path = \"/storage/emulated/0/DCIM/test2.jpg\";\nint image_width;\nint image_height;\nint image_channels;\nstd::vector<uint8_t> image_data;\nLoadImageFromFile(image_path, image_data, image_width, image_height, image_channels);\n\n//    const int wanted_width = 224;\n//    const int wanted_height = 224;\n//    const int wanted_channels = 3;\n//    const float input_mean = 127.5f;\n//    const float input_std = 127.5f;\n\nconst int wanted_width = 39;\nconst int wanted_height = 39;\nconst int wanted_channels = 1;\nconst float input_mean = 128.0f;\nconst float input_std = 64.0f;\nassert(image_channels >= wanted_channels);\nuint8_t* in = image_data.data();\nfloat* out = interpreter->typed_tensor<float>(input);\nint input_idx = interpreter->inputs()[0];\nTfLiteTensor* target = interpreter->tensor(input_idx);\nint num_bytes = sizes[0]*sizes[1]*sizes[2]*sizes[3];\nwriteByteBuffer(in, &(target->data.raw), static_cast<int>(num_bytes));\n    if (interpreter->Invoke() != kTfLiteOk)\n    {\n        LOGD(\"bkzero jni: , %s\", \"Failed to invoke!\");\n    }\n    const std::vector<int>& results = interpreter->outputs();\n    if (results.empty()) {\n      LOGD(\"bkzero jni: , %s\", \"results.empty()\");\n    }\n    long outputs[results.size()];\n    size_t size = results.size();\n    for (int i = 0; i < size; ++i) {\n      TfLiteTensor* source = interpreter->tensor(results[i]);\n      outputs[i] = reinterpret_cast<long>(source);\n    }\n    uint8_t* final = (uint8_t*)outputs[0];\n    for(int i = 0; i < 10; i++)\n        LOGD(\"bkzero jni: results, %f\", (final[i]/255.0));\n    LOGD(\"bkzero jni: , %s\", \"interpreter outputs read finished\");\n}\n\nthis code can read and ouput result correct when run the example mobilenet file, mobilenet_quant_v1_224.tflite. but crash when i run my own model and give the error: Fatal signal 7 (SIGBUS), code 1, fault addr 0xdd3dd008 in tid 10528\nand on PC, it also crashes, without more information.", "body": "**Version Info:** \r\ntensorflow r1.5 \r\nubuntu 14.04 \r\narmv7 platform \r\nandroid 6.0.1 and pc\r\nndk version: r14 \r\nandroid studio 2.3.1 \r\n\r\n**Describe the problem**\r\ni write the code in c++, it can get results when using the mobilenet model from the tutorails(link: https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip)\r\nbut when i use my own model converted by toco, it crashed without any information. set input and load model seemed correct, but invoke failed.\r\n\r\n**code**\r\n\r\n**network define**\r\nnetwork is defined by tf.slim framework:\r\n```\r\n def lite_v1(images, num_classes=10, is_training=False, dropout_keep_prob=0.5, prediction_fn=slim.softmax, scope='lite_v1'):\r\n    end_points = {}\r\n      with tf.variable_scope(scope, 'lite_v1', [images, num_classes]):\r\n        images_input = tf.placeholder_with_default(images, shape=[None, 39, 39, 1], name='InputPlaceholder')\r\n        net = slim.conv2d(images_input, 16, [4, 4], padding='VALID', scope='conv1')\r\n        end_points['conv1'] = net\r\n        feature = slim.flatten(net)\r\n    with tf.variable_scope('Logits'):\r\n      net = slim.fully_connected(feature, 100, scope='fc3')\r\n      logits = slim.fully_connected(net, num_classes,\r\n                              biases_initializer=tf.zeros_initializer(),\r\n                              weights_regularizer=None,\r\n                              activation_fn=None,\r\n                              scope='logits')\r\n      end_points['Logits'] = logits\r\n    output = tf.multiply(logits, 1, name=\"Output\")\r\n    end_points['Output'] = output\r\n  return logits, end_points\r\n```\r\n**model convert**\r\ni trained it and use the freeze_graph.py and optimize_for_inference.py scripe in tensorflow/python/tools/ and get a frozen .pb file. then i convert it to a .tflite file by following the tutorials in github: https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/lite just like:\r\n```\r\nbazel build tensorflow/contrib/lite/toco:toco\r\nbazel-bin/tensorflow/contrib/lite/toco/toco -- \\\r\n  --input_file=$(pwd)/mobilenet_v1_1.0_224/frozen_graph.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n  --output_file=/tmp/mobilenet_v1_1.0_224.lite --inference_type=FLOAT \\\r\n  --input_type=FLOAT --input_arrays=input \\\r\n  --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3\r\n```\r\n\r\n**c++ test code**\r\nafter get a .tflite file. and then i wrote the c++ code to read the model and image from sdcard:\r\n```\r\nvoid LoadImageFromFile(std::string file_name, std::vector<uint8_t>& output, int& out_width, int& out_height, int& out_channels)\r\n{\r\ncv::Mat image = cv::imread(file_name);\r\ncv::Mat gray;\r\ncv::cvtColor(image, gray, cv::COLOR_BGR2GRAY);\r\n\r\nout_width = gray.cols;\r\nout_height = gray.rows;\r\nout_channels = gray.channels();\r\nfor(int nrow =0; nrow < out_height; nrow++)\r\n    for(int ncol = 0; ncol < out_width; ncol++)\r\n        output.push_back(gray.at<unsigned char>(nrow,ncol));\r\n}\r\nsize_t writeByteBuffer(uint8_t* in, char** dst, int dst_size) {\r\n  char* buf = (char*)in;\r\n  if (!buf) {\r\n    return 0;\r\n  }\r\n  *dst = buf;\r\n  return dst_size;\r\n}\r\n\r\nint RunInferenceOnImage()\r\n{\r\nconst int num_threads = 1;\r\nstd::string input_layer_type = \"float\";\r\nstd::vector<int> sizes = {1, 39, 39, 1};\r\n//    std::vector<int> sizes = {1, 224, 224, 3};\r\nstd::string graph_path = \"/storage/emulated/0/DCIM/fastnetv2.tflite\"; //mobilenet_quant_v1_224.tflite\r\nstd::unique_ptr<tflite::FlatBufferModel> model(tflite::FlatBufferModel::BuildFromFile(graph_path.c_str()));\r\nif (!model)\r\n{\r\n    LOGD(\"bkzero jni: , Failed to mmap model %s\", graph_path.c_str());\r\n}\r\nmodel->error_reporter();\r\n\r\n#ifdef TFLITE_CUSTOM_OPS_HEADER\r\n  tflite::MutableOpResolver resolver;\r\n  RegisterSelectedOps(&resolver);\r\n#else\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n#endif\r\n\r\nLOGD(\"bkzero jni: , %s\", \"resolved reporter end\");\r\n\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\nif (!interpreter)\r\n{\r\n    LOGD(\"bkzero jni: , %s\", \"Failed to construct interpreter\");\r\n}\r\nif (num_threads != -1) {\r\n  interpreter->SetNumThreads(num_threads);\r\n}\r\nint input = interpreter->inputs()[0];\r\nif (input_layer_type != \"string\") {\r\n  interpreter->ResizeInputTensor(input, sizes);\r\n}\r\n\r\nif (interpreter->AllocateTensors() != kTfLiteOk)\r\n{\r\n    LOGD(\"bkzero jni: , %s\", \"Failed to allocate tensors!\");\r\n}\r\nstd::string image_path = \"/storage/emulated/0/DCIM/test2.jpg\";\r\nint image_width;\r\nint image_height;\r\nint image_channels;\r\nstd::vector<uint8_t> image_data;\r\nLoadImageFromFile(image_path, image_data, image_width, image_height, image_channels);\r\n\r\n//    const int wanted_width = 224;\r\n//    const int wanted_height = 224;\r\n//    const int wanted_channels = 3;\r\n//    const float input_mean = 127.5f;\r\n//    const float input_std = 127.5f;\r\n\r\nconst int wanted_width = 39;\r\nconst int wanted_height = 39;\r\nconst int wanted_channels = 1;\r\nconst float input_mean = 128.0f;\r\nconst float input_std = 64.0f;\r\nassert(image_channels >= wanted_channels);\r\nuint8_t* in = image_data.data();\r\nfloat* out = interpreter->typed_tensor<float>(input);\r\nint input_idx = interpreter->inputs()[0];\r\nTfLiteTensor* target = interpreter->tensor(input_idx);\r\nint num_bytes = sizes[0]*sizes[1]*sizes[2]*sizes[3];\r\nwriteByteBuffer(in, &(target->data.raw), static_cast<int>(num_bytes));\r\n    if (interpreter->Invoke() != kTfLiteOk)\r\n    {\r\n        LOGD(\"bkzero jni: , %s\", \"Failed to invoke!\");\r\n    }\r\n    const std::vector<int>& results = interpreter->outputs();\r\n    if (results.empty()) {\r\n      LOGD(\"bkzero jni: , %s\", \"results.empty()\");\r\n    }\r\n    long outputs[results.size()];\r\n    size_t size = results.size();\r\n    for (int i = 0; i < size; ++i) {\r\n      TfLiteTensor* source = interpreter->tensor(results[i]);\r\n      outputs[i] = reinterpret_cast<long>(source);\r\n    }\r\n    uint8_t* final = (uint8_t*)outputs[0];\r\n    for(int i = 0; i < 10; i++)\r\n        LOGD(\"bkzero jni: results, %f\", (final[i]/255.0));\r\n    LOGD(\"bkzero jni: , %s\", \"interpreter outputs read finished\");\r\n}\r\n```\r\n\r\nthis code can read and ouput result correct when run the example mobilenet file, mobilenet_quant_v1_224.tflite. but crash when i run my own model and give the error: Fatal signal 7 (SIGBUS), code 1, fault addr 0xdd3dd008 in tid 10528\r\n\r\nand on PC, it also crashes, without more information."}