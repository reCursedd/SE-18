{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17317", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17317/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17317/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17317/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17317", "id": 300881084, "node_id": "MDU6SXNzdWUzMDA4ODEwODQ=", "number": 17317, "title": "How to Split up/Unstack/Partition a dynamic 3D Tensor into subtensors?", "user": {"login": "Aashit-Sharma", "id": 29089622, "node_id": "MDQ6VXNlcjI5MDg5NjIy", "avatar_url": "https://avatars0.githubusercontent.com/u/29089622?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aashit-Sharma", "html_url": "https://github.com/Aashit-Sharma", "followers_url": "https://api.github.com/users/Aashit-Sharma/followers", "following_url": "https://api.github.com/users/Aashit-Sharma/following{/other_user}", "gists_url": "https://api.github.com/users/Aashit-Sharma/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aashit-Sharma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aashit-Sharma/subscriptions", "organizations_url": "https://api.github.com/users/Aashit-Sharma/orgs", "repos_url": "https://api.github.com/users/Aashit-Sharma/repos", "events_url": "https://api.github.com/users/Aashit-Sharma/events{/privacy}", "received_events_url": "https://api.github.com/users/Aashit-Sharma/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-02-28T03:44:49Z", "updated_at": "2018-06-04T06:10:20Z", "closed_at": "2018-06-04T06:10:20Z", "author_association": "NONE", "body_html": "<p>In this <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/seq2seq/python/ops/loss.py\">code from google</a> , sequence loss is being calculated by passing in 3 variables : logits , weights and targets.</p>\n<p>How logits are defined:</p>\n<blockquote>\n<p>logits: A Tensor of shape<br>\n<code>[batch_size, sequence_length, num_decoder_symbols]</code> and dtype float.<br>\nThe logits correspond to the prediction across all classes at each<br>\ntimestep.</p>\n</blockquote>\n<p>My intentions were to get <strong>3 different tensors of shapes</strong> batch_size , sequence_length and dec_symbols  and then use them in <strong>tf.scan</strong> (using sequence_length as elems)</p>\n<p>If I print the logits tensor , this is what i get :</p>\n<blockquote>\n<p>shape=(?, ?, 300)</p>\n</blockquote>\n<p>Which means tf.unstack is out of the equation(as it has a variable shape)</p>\n<p>So my first question is , is it even possible ?</p>\n<p>If yes , any suggestions ?</p>\n<p>Thanks !</p>\n<p>PS: Maybe Google can add a swap_memory parameter to the seq2seq.sequence_loss function ,to avoid OOM errors while calculating losses [which is what we are trying to overcome using an iterator inside]</p>", "body_text": "In this code from google , sequence loss is being calculated by passing in 3 variables : logits , weights and targets.\nHow logits are defined:\n\nlogits: A Tensor of shape\n[batch_size, sequence_length, num_decoder_symbols] and dtype float.\nThe logits correspond to the prediction across all classes at each\ntimestep.\n\nMy intentions were to get 3 different tensors of shapes batch_size , sequence_length and dec_symbols  and then use them in tf.scan (using sequence_length as elems)\nIf I print the logits tensor , this is what i get :\n\nshape=(?, ?, 300)\n\nWhich means tf.unstack is out of the equation(as it has a variable shape)\nSo my first question is , is it even possible ?\nIf yes , any suggestions ?\nThanks !\nPS: Maybe Google can add a swap_memory parameter to the seq2seq.sequence_loss function ,to avoid OOM errors while calculating losses [which is what we are trying to overcome using an iterator inside]", "body": "In this [code from google][1] , sequence loss is being calculated by passing in 3 variables : logits , weights and targets.\r\n\r\nHow logits are defined:\r\n> logits: A Tensor of shape\r\n>       `[batch_size, sequence_length, num_decoder_symbols]` and dtype float.\r\n>       The logits correspond to the prediction across all classes at each\r\n>       timestep.\r\n\r\nMy intentions were to get **3 different tensors of shapes** batch_size , sequence_length and dec_symbols  and then use them in **tf.scan** (using sequence_length as elems)\r\n\r\nIf I print the logits tensor , this is what i get : \r\n> shape=(?, ?, 300) \r\n\r\nWhich means tf.unstack is out of the equation(as it has a variable shape)\r\n\r\nSo my first question is , is it even possible ?\r\n\r\nIf yes , any suggestions ?\r\n\r\nThanks !\r\n\r\nPS: Maybe Google can add a swap_memory parameter to the seq2seq.sequence_loss function ,to avoid OOM errors while calculating losses [which is what we are trying to overcome using an iterator inside]\r\n\r\n  [1]: https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/seq2seq/python/ops/loss.py"}