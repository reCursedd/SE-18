{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5417", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5417/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5417/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5417/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5417", "id": 187525884, "node_id": "MDU6SXNzdWUxODc1MjU4ODQ=", "number": 5417, "title": "1% GPU usage and slow training times after unknown DSO update?", "user": {"login": "jubbens", "id": 1747256, "node_id": "MDQ6VXNlcjE3NDcyNTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1747256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jubbens", "html_url": "https://github.com/jubbens", "followers_url": "https://api.github.com/users/jubbens/followers", "following_url": "https://api.github.com/users/jubbens/following{/other_user}", "gists_url": "https://api.github.com/users/jubbens/gists{/gist_id}", "starred_url": "https://api.github.com/users/jubbens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jubbens/subscriptions", "organizations_url": "https://api.github.com/users/jubbens/orgs", "repos_url": "https://api.github.com/users/jubbens/repos", "events_url": "https://api.github.com/users/jubbens/events{/privacy}", "received_events_url": "https://api.github.com/users/jubbens/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173351, "node_id": "MDU6TGFiZWw0NzMxNzMzNTE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:build/install", "name": "type:build/install", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-11-05T21:56:18Z", "updated_at": "2016-11-07T21:31:22Z", "closed_at": "2016-11-07T21:31:22Z", "author_association": "NONE", "body_html": "<p>Tensorflow with CUDA was working fine. Cross-entropy loss was going to zero (CIFAR-10 and a very simple CNN), but training error stayed around the same as random chance. I mention this in case it may be related to my issue.</p>\n<p>In between runs, not 5 minutes after it was just working, I mysteriously got this error message about my video driver:</p>\n<p><code>kernel version 361.42.0 does not match DSO version 367.57.0</code></p>\n<p>I never updated anything, so that's strange. I update my video driver using <code>apt-get</code> and the Ubuntu ppa repo, then restart. I didn't check what version the video driver was before the update but I assume it was 361.42?</p>\n<p>The error message is gone, but <strong>training is now an order of magnitude slower with ~1% GPU usage and ~5% CPU usage</strong>. <code>nvidia-smi</code> indicates 90% memory usage, which means the model is in memory, but what before took ~0.4 seconds per batch is now taking 4+ seconds per batch.</p>\n<p>I tried re-installing CUDA and cuDNN from official sources but no change.<br>\nCUDA bandwidth test and deviceQuery results normal.</p>\n<p>Ubuntu 16.04<br>\nTensorflow 0.10.0 (built from source w/ CC 3.5 and 5.0)<br>\nnvidia Quadro K2200<br>\nCUDA 8.0<br>\ncuDNN 5.1.5<br>\nPython 2.7</p>", "body_text": "Tensorflow with CUDA was working fine. Cross-entropy loss was going to zero (CIFAR-10 and a very simple CNN), but training error stayed around the same as random chance. I mention this in case it may be related to my issue.\nIn between runs, not 5 minutes after it was just working, I mysteriously got this error message about my video driver:\nkernel version 361.42.0 does not match DSO version 367.57.0\nI never updated anything, so that's strange. I update my video driver using apt-get and the Ubuntu ppa repo, then restart. I didn't check what version the video driver was before the update but I assume it was 361.42?\nThe error message is gone, but training is now an order of magnitude slower with ~1% GPU usage and ~5% CPU usage. nvidia-smi indicates 90% memory usage, which means the model is in memory, but what before took ~0.4 seconds per batch is now taking 4+ seconds per batch.\nI tried re-installing CUDA and cuDNN from official sources but no change.\nCUDA bandwidth test and deviceQuery results normal.\nUbuntu 16.04\nTensorflow 0.10.0 (built from source w/ CC 3.5 and 5.0)\nnvidia Quadro K2200\nCUDA 8.0\ncuDNN 5.1.5\nPython 2.7", "body": "Tensorflow with CUDA was working fine. Cross-entropy loss was going to zero (CIFAR-10 and a very simple CNN), but training error stayed around the same as random chance. I mention this in case it may be related to my issue.\r\n\r\nIn between runs, not 5 minutes after it was just working, I mysteriously got this error message about my video driver: \r\n\r\n`kernel version 361.42.0 does not match DSO version 367.57.0`\r\n\r\nI never updated anything, so that's strange. I update my video driver using `apt-get` and the Ubuntu ppa repo, then restart. I didn't check what version the video driver was before the update but I assume it was 361.42?\r\n\r\nThe error message is gone, but **training is now an order of magnitude slower with ~1% GPU usage and ~5% CPU usage**. `nvidia-smi` indicates 90% memory usage, which means the model is in memory, but what before took ~0.4 seconds per batch is now taking 4+ seconds per batch.\r\n\r\nI tried re-installing CUDA and cuDNN from official sources but no change.\r\nCUDA bandwidth test and deviceQuery results normal.\r\n\r\nUbuntu 16.04\r\nTensorflow 0.10.0 (built from source w/ CC 3.5 and 5.0)\r\nnvidia Quadro K2200\r\nCUDA 8.0\r\ncuDNN 5.1.5\r\nPython 2.7"}