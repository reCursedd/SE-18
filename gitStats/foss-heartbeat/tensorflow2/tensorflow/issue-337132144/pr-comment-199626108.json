{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/199626108", "pull_request_review_id": 133780834, "id": 199626108, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5OTYyNjEwOA==", "diff_hunk": "@@ -0,0 +1,1188 @@\n+{\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0,\n+  \"metadata\": {\n+    \"colab\": {\n+      \"name\": \"image_captioning_with_attention.ipynb\",\n+      \"version\": \"0.3.2\",\n+      \"views\": {},\n+      \"default_view\": {},\n+      \"provenance\": [\n+        {\n+          \"file_id\": \"1HI8OK2sMjcx9CTWVn0122QAHOuXaOaMg\",\n+          \"timestamp\": 1530222436922\n+        }\n+      ],\n+      \"private_outputs\": true,\n+      \"collapsed_sections\": [],\n+      \"toc_visible\": true\n+    },\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"accelerator\": \"GPU\"\n+  },\n+  \"cells\": [\n+    {\n+      \"metadata\": {\n+        \"id\": \"K2s1A9eLRPEj\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"##### Copyright 2018 The TensorFlow Authors.\\n\",\n+        \"\\n\",\n+        \"Licensed under the Apache License, Version 2.0 (the \\\"License\\\").\\n\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"Cffg2i257iMS\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"# Image Captioning with Attention\\n\",\n+        \"\\n\",\n+        \"<table class=\\\"tfo-notebook-buttons\\\" align=\\\"left\\\"><td>\\n\",\n+        \"<a target=\\\"_blank\\\"  href=\\\"https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb\\\">\\n\",\n+        \"    <img src=\\\"https://www.tensorflow.org/images/colab_logo_32px.png\\\" /><span>Run in Google Colab</span></a>  \\n\",\n+        \"</td><td>\\n\",\n+        \"<a target=\\\"_blank\\\"  href=\\\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb\\\"><img width=32px src=\\\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\\\" /><span>View source on GitHub</span></a></td></table>\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"QASbY_HGo4Lq\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"Image captioning is the task of generating a caption for an image. Given an image like this:\\n\",\n+        \"\\n\",\n+        \"![alt text](https://tensorflow.org/images/surf.jpg) \\n\",\n+        \"\\n\",\n+        \"NOTE: This image was originally hosted at [https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg](https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg)\\n\",\n+        \"\\n\",\n+        \"Our goal is generate a caption, such as \\\"a surfer riding on a wave\\\". Here, we'll use an attention based model. This enables us to see which parts of the image the model focuses on as it generates a caption.\\n\",\n+        \"\\n\",\n+        \"![alt text](https://tensorflow.org/images/imcap_prediction.png)\\n\",\n+        \"\\n\",\n+        \"This model architecture below is similar to [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044). \\n\",\n+        \"\\n\",\n+        \"The code uses [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager), which you can learn more about in the linked guides.\\n\",\n+        \"\\n\",\n+        \"This notebook is an end-to-end example. If you run it, it will download the  [MS-COCO](http://cocodataset.org/#home) dataset, preprocess and cache a subset of the images using Inception V3, train an encoder-decoder model, and use it to generate captions on new images.\\n\",\n+        \"\\n\",\n+        \"The code requires TensorFlow version >=1.9. If you're running this in [Colab]()\\n\",\n+        \"\\n\",\n+        \"In this example, we're training on a relatively small amount of data as an example. On a single P100 GPU, this example will take about ~2 hours to train. We train on the first 30,000 captions (corresponding to about ~20,000 images depending on shuffling, as there are multiple captions per image in the dataset)\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"U8l4RJ0XRPEm\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# Import TensorFlow and enable eager execution\\n\",\n+        \"# This code requires TensorFlow version >=1.9\\n\",\n+        \"import tensorflow as tf\\n\",\n+        \"tf.enable_eager_execution()\\n\",\n+        \"\\n\",\n+        \"# We'll generate plots of attention in order to see which parts of an image\\n\",\n+        \"# our model focuses on during captioning\\n\",\n+        \"import matplotlib.pyplot as plt\\n\",\n+        \"\\n\",\n+        \"# Scikit-learn includes many helpful utilities\\n\",\n+        \"from sklearn.model_selection import train_test_split\\n\",\n+        \"from sklearn.utils import shuffle\\n\",\n+        \"\\n\",\n+        \"import re\\n\",\n+        \"import numpy as np\\n\",\n+        \"import os\\n\",\n+        \"import time\\n\",\n+        \"import json\\n\",\n+        \"from glob import glob\\n\",\n+        \"from PIL import Image\\n\",\n+        \"import pickle\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"b6qbGw8MRPE5\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Download and prepare the MS-COCO dataset\\n\",\n+        \"\\n\",\n+        \"We will use the [MS-COCO dataset](http://cocodataset.org/#home) to train our model. This dataset contains >82,000 images, each of which has been annotated with at least 5 different captions. The code code below will download and extract the dataset automatically.  \\n\",\n+        \"\\n\",\n+        \"**Caution: large download ahead**. We'll use the training set, it's a 13GB file.\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"krQuPYTtRPE7\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"annotation_zip = tf.keras.utils.get_file('captions.zip', \\n\",\n+        \"                                          cache_subdir=os.path.abspath('.'),\\n\",\n+        \"                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\\n\",\n+        \"                                          extract = True)\\n\",\n+        \"annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\\n\",\n+        \"\\n\",\n+        \"name_of_zip = 'train2014.zip'\\n\",\n+        \"if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\\n\",\n+        \"  image_zip = tf.keras.utils.get_file(name_of_zip, \\n\",\n+        \"                                      cache_subdir=os.path.abspath('.'),\\n\",\n+        \"                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\\n\",\n+        \"                                      extract = True)\\n\",\n+        \"  PATH = os.path.dirname(image_zip)+'/train2014/'\\n\",\n+        \"else:\\n\",\n+        \"  PATH = os.path.abspath('.')+'/train2014/'\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"aANEzb5WwSzg\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Optionally, limit the size of the training set for faster training\\n\",\n+        \"For this example, we'll select a subset of 30,000 captions and use these and the corresponding images to train our model. As always, captioning quality will improve if you choose to use more data.\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"4G3b8x8_RPFD\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# read the json file\\n\",\n+        \"with open(annotation_file, 'r') as f:\\n\",\n+        \"    annotations = json.load(f)\\n\",\n+        \"\\n\",\n+        \"# storing the captions and the image name in vectors\\n\",\n+        \"all_captions = []\\n\",\n+        \"all_img_name_vector = []\\n\",\n+        \"\\n\",\n+        \"for annot in annotations['annotations']:\\n\",\n+        \"    caption = '<start> ' + annot['caption'] + ' <end>'\\n\",\n+        \"    image_id = annot['image_id']\\n\",\n+        \"    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\\n\",\n+        \"    \\n\",\n+        \"    all_img_name_vector.append(full_coco_image_path)\\n\",\n+        \"    all_captions.append(caption)\\n\",\n+        \"\\n\",\n+        \"# shuffling the captions and image_names together\\n\",\n+        \"# setting a random state\\n\",\n+        \"train_captions, img_name_vector = shuffle(all_captions,\\n\",\n+        \"                                          all_img_name_vector,\\n\",\n+        \"                                          random_state=1)\\n\",\n+        \"\\n\",\n+        \"# selecting the first 30000 captions from the shuffled set\\n\",\n+        \"num_examples = 30000\\n\",\n+        \"train_captions = train_captions[:num_examples]\\n\",\n+        \"img_name_vector = img_name_vector[:num_examples]\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"5fOGoXVLxEid\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Inspect the data: display a caption and the corresponding image\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"JuC7WcHSRPFG\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"plt.imshow(Image.open(img_name_vector[0]))\\n\",", "path": "tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb", "position": null, "original_position": 245, "commit_id": "25d69fd6ec7fa299df997f49f6be8bc0b6f87c67", "original_commit_id": "12b8fd801f592f9825da3e739d1cb146d50c8f34", "user": {"login": "MarkDaoust", "id": 1414837, "node_id": "MDQ6VXNlcjE0MTQ4Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1414837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarkDaoust", "html_url": "https://github.com/MarkDaoust", "followers_url": "https://api.github.com/users/MarkDaoust/followers", "following_url": "https://api.github.com/users/MarkDaoust/following{/other_user}", "gists_url": "https://api.github.com/users/MarkDaoust/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarkDaoust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarkDaoust/subscriptions", "organizations_url": "https://api.github.com/users/MarkDaoust/orgs", "repos_url": "https://api.github.com/users/MarkDaoust/repos", "events_url": "https://api.github.com/users/MarkDaoust/events{/privacy}", "received_events_url": "https://api.github.com/users/MarkDaoust/received_events", "type": "User", "site_admin": false}, "body": "Note: We can't render this onto tensorflow.org if it's displaying images from the dataset. So we need to be careful here.", "created_at": "2018-07-02T21:27:21Z", "updated_at": "2018-07-09T22:47:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20429#discussion_r199626108", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20429", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/199626108"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20429#discussion_r199626108"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20429"}}, "body_html": "<p>Note: We can't render this onto tensorflow.org if it's displaying images from the dataset. So we need to be careful here.</p>", "body_text": "Note: We can't render this onto tensorflow.org if it's displaying images from the dataset. So we need to be careful here."}