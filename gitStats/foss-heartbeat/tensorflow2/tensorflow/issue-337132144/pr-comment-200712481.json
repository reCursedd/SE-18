{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/200712481", "pull_request_review_id": 135087195, "id": 200712481, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMDcxMjQ4MQ==", "diff_hunk": "@@ -0,0 +1,1159 @@\n+{\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0,\n+  \"metadata\": {\n+    \"colab\": {\n+      \"name\": \"image_captioning_with_attention.ipynb\",\n+      \"version\": \"0.3.2\",\n+      \"views\": {},\n+      \"default_view\": {},\n+      \"provenance\": [\n+        {\n+          \"file_id\": \"1HI8OK2sMjcx9CTWVn0122QAHOuXaOaMg\",\n+          \"timestamp\": 1530222436922\n+        }\n+      ],\n+      \"private_outputs\": true,\n+      \"collapsed_sections\": [],\n+      \"toc_visible\": true\n+    },\n+    \"kernelspec\": {\n+      \"display_name\": \"Python 3\",\n+      \"language\": \"python\",\n+      \"name\": \"python3\"\n+    },\n+    \"accelerator\": \"GPU\"\n+  },\n+  \"cells\": [\n+    {\n+      \"metadata\": {\n+        \"id\": \"K2s1A9eLRPEj\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"##### Copyright 2018 The TensorFlow Authors.\\n\",\n+        \"\\n\",\n+        \"Licensed under the Apache License, Version 2.0 (the \\\"License\\\").\\n\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"Cffg2i257iMS\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"# Image Captioning with Attention\\n\",\n+        \"\\n\",\n+        \"<table class=\\\"tfo-notebook-buttons\\\" align=\\\"left\\\"><td>\\n\",\n+        \"<a target=\\\"_blank\\\"  href=\\\"https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb\\\">\\n\",\n+        \"    <img src=\\\"https://www.tensorflow.org/images/colab_logo_32px.png\\\" /><span>Run in Google Colab</span></a>  \\n\",\n+        \"</td><td>\\n\",\n+        \"<a target=\\\"_blank\\\"  href=\\\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb\\\"><img width=32px src=\\\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\\\" /><span>View source on GitHub</span></a></td></table>\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"QASbY_HGo4Lq\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"Image captioning is the task of generating a caption for an image. Given an image like this:\\n\",\n+        \"\\n\",\n+        \"![Man Surfing](https://tensorflow.org/images/surf.jpg) \\n\",\n+        \"\\n\",\n+        \"[Image Source](https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg), License: Public Domain\\n\",\n+        \"\\n\",\n+        \"Our goal is generate a caption, such as \\\"a surfer riding on a wave\\\". Here, we'll use an attention based model. This enables us to see which parts of the image the model focuses on as it generates a caption.\\n\",\n+        \"\\n\",\n+        \"![Prediction](https://tensorflow.org/images/imcap_prediction.png)\\n\",\n+        \"\\n\",\n+        \"This model architecture below is similar to [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044). \\n\",\n+        \"\\n\",\n+        \"The code uses [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager), which you can learn more about in the linked guides.\\n\",\n+        \"\\n\",\n+        \"This notebook is an end-to-end example. If you run it, it will download the  [MS-COCO](http://cocodataset.org/#home) dataset, preprocess and cache a subset of the images using Inception V3, train an encoder-decoder model, and use it to generate captions on new images.\\n\",\n+        \"\\n\",\n+        \"The code requires TensorFlow version >=1.9. If you're running this in [Colab]()\\n\",\n+        \"\\n\",\n+        \"In this example, we're training on a relatively small amount of data as an example. On a single P100 GPU, this example will take about ~2 hours to train. We train on the first 30,000 captions (corresponding to about ~20,000 images depending on shuffling, as there are multiple captions per image in the dataset)\\n\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"U8l4RJ0XRPEm\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# Import TensorFlow and enable eager execution\\n\",\n+        \"# This code requires TensorFlow version >=1.9\\n\",\n+        \"import tensorflow as tf\\n\",\n+        \"tf.enable_eager_execution()\\n\",\n+        \"\\n\",\n+        \"# We'll generate plots of attention in order to see which parts of an image\\n\",\n+        \"# our model focuses on during captioning\\n\",\n+        \"import matplotlib.pyplot as plt\\n\",\n+        \"\\n\",\n+        \"# Scikit-learn includes many helpful utilities\\n\",\n+        \"from sklearn.model_selection import train_test_split\\n\",\n+        \"from sklearn.utils import shuffle\\n\",\n+        \"\\n\",\n+        \"import re\\n\",\n+        \"import numpy as np\\n\",\n+        \"import os\\n\",\n+        \"import time\\n\",\n+        \"import json\\n\",\n+        \"from glob import glob\\n\",\n+        \"from PIL import Image\\n\",\n+        \"import pickle\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"b6qbGw8MRPE5\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Download and prepare the MS-COCO dataset\\n\",\n+        \"\\n\",\n+        \"We will use the [MS-COCO dataset](http://cocodataset.org/#home) to train our model. This dataset contains >82,000 images, each of which has been annotated with at least 5 different captions. The code code below will download and extract the dataset automatically.  \\n\",\n+        \"\\n\",\n+        \"**Caution: large download ahead**. We'll use the training set, it's a 13GB file.\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"krQuPYTtRPE7\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"annotation_zip = tf.keras.utils.get_file('captions.zip', \\n\",\n+        \"                                          cache_subdir=os.path.abspath('.'),\\n\",\n+        \"                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\\n\",\n+        \"                                          extract = True)\\n\",\n+        \"annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\\n\",\n+        \"\\n\",\n+        \"name_of_zip = 'train2014.zip'\\n\",\n+        \"if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\\n\",\n+        \"  image_zip = tf.keras.utils.get_file(name_of_zip, \\n\",\n+        \"                                      cache_subdir=os.path.abspath('.'),\\n\",\n+        \"                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\\n\",\n+        \"                                      extract = True)\\n\",\n+        \"  PATH = os.path.dirname(image_zip)+'/train2014/'\\n\",\n+        \"else:\\n\",\n+        \"  PATH = os.path.abspath('.')+'/train2014/'\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"aANEzb5WwSzg\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Optionally, limit the size of the training set for faster training\\n\",\n+        \"For this example, we'll select a subset of 30,000 captions and use these and the corresponding images to train our model. As always, captioning quality will improve if you choose to use more data.\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"4G3b8x8_RPFD\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# read the json file\\n\",\n+        \"with open(annotation_file, 'r') as f:\\n\",\n+        \"    annotations = json.load(f)\\n\",\n+        \"\\n\",\n+        \"# storing the captions and the image name in vectors\\n\",\n+        \"all_captions = []\\n\",\n+        \"all_img_name_vector = []\\n\",\n+        \"\\n\",\n+        \"for annot in annotations['annotations']:\\n\",\n+        \"    caption = '<start> ' + annot['caption'] + ' <end>'\\n\",\n+        \"    image_id = annot['image_id']\\n\",\n+        \"    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\\n\",\n+        \"    \\n\",\n+        \"    all_img_name_vector.append(full_coco_image_path)\\n\",\n+        \"    all_captions.append(caption)\\n\",\n+        \"\\n\",\n+        \"# shuffling the captions and image_names together\\n\",\n+        \"# setting a random state\\n\",\n+        \"train_captions, img_name_vector = shuffle(all_captions,\\n\",\n+        \"                                          all_img_name_vector,\\n\",\n+        \"                                          random_state=1)\\n\",\n+        \"\\n\",\n+        \"# selecting the first 30000 captions from the shuffled set\\n\",\n+        \"num_examples = 30000\\n\",\n+        \"train_captions = train_captions[:num_examples]\\n\",\n+        \"img_name_vector = img_name_vector[:num_examples]\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"mPBMgK34RPFL\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"len(train_captions), len(all_captions)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"8cSW4u-ORPFQ\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Preprocess the images using InceptionV3\\n\",\n+        \"Next, we will use InceptionV3 (pretrained on Imagenet) to classify each image. We will extract features from the last convolutional layer. \\n\",\n+        \"\\n\",\n+        \"First, we will need to convert the images into the format inceptionV3 expects by:\\n\",\n+        \"* Resizing the image to (299, 299)\\n\",\n+        \"* Using the [preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input) method to place the pixels in the range of -1 to 1 (to match the format of the images used to train InceptionV3).\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"zXR0217aRPFR\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"def load_image(image_path):\\n\",\n+        \"    img = tf.read_file(image_path)\\n\",\n+        \"    img = tf.image.decode_jpeg(img, channels=3)\\n\",\n+        \"    img = tf.image.resize_images(img, (299, 299))\\n\",\n+        \"    img = tf.keras.applications.inception_v3.preprocess_input(img)\\n\",\n+        \"    return img, image_path\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"MDvIu4sXRPFV\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Initialize InceptionV3 and load the pretrained Imagenet weights\\n\",\n+        \"\\n\",\n+        \"To do so, we'll create a tf.keras model where the output layer is the last convolutional layer in the InceptionV3 architecture. \\n\",\n+        \"* Each image is forwarded through the network and the vector that we get at the end is stored in a dictionary (image_name --> feature_vector). \\n\",\n+        \"* We use the last convolutional layer because we are using attention in this example. The shape of the output of this layer is ```8x8x2048```. \\n\",\n+        \"* We avoid doing this during training so it does not become a bottleneck. \\n\",\n+        \"* After all the images are passed through the network, we pickle the dictionary and save it to disk.\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"RD3vW4SsRPFW\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"image_model = tf.keras.applications.InceptionV3(include_top=False, \\n\",\n+        \"                                                weights='imagenet')\\n\",\n+        \"new_input = image_model.input\\n\",\n+        \"hidden_layer = image_model.layers[-1].output\\n\",\n+        \"\\n\",\n+        \"image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"nyqH3zFwRPFi\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Preprocess and tokenize the captions\\n\",", "path": "tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb", "position": 378, "original_position": 324, "commit_id": "25d69fd6ec7fa299df997f49f6be8bc0b6f87c67", "original_commit_id": "cf94ebc53b95faa000df176d6c9a1c6d94b59e0a", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "body": "It's weird that this section is here but it has nothing to do with inception v3. Maybe move it above / below the others using inception?", "created_at": "2018-07-06T16:57:55Z", "updated_at": "2018-07-09T22:47:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20429#discussion_r200712481", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20429", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/200712481"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20429#discussion_r200712481"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20429"}}, "body_html": "<p>It's weird that this section is here but it has nothing to do with inception v3. Maybe move it above / below the others using inception?</p>", "body_text": "It's weird that this section is here but it has nothing to do with inception v3. Maybe move it above / below the others using inception?"}