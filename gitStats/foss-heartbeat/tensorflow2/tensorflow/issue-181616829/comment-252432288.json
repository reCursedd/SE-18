{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/252432288", "html_url": "https://github.com/tensorflow/tensorflow/issues/4815#issuecomment-252432288", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4815", "id": 252432288, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MjQzMjI4OA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-08T15:55:28Z", "updated_at": "2016-10-08T15:57:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>IMHO standard is hard to make useful at this point since the set of useful neural network primitives is changing so fast. Two years ago people wouldn't have thought about adding GRU unit, a year ago people would miss some neural attention primitives, and next year there may be categorically new ops or ops that unify existing ops redundant. Like with ReLU -&gt; ReLU6 -&gt; ReLUN, then ReLUN with trainable N which technically makes all other versions of ReLU redundant. So a standard would end up either missing someone's favorite primitive, or grow so large that it's not practical to be implemented by everyone (ie, like with TensorFlow GraphDef format which grew to incude 450+ different op types)</p>", "body_text": "IMHO standard is hard to make useful at this point since the set of useful neural network primitives is changing so fast. Two years ago people wouldn't have thought about adding GRU unit, a year ago people would miss some neural attention primitives, and next year there may be categorically new ops or ops that unify existing ops redundant. Like with ReLU -> ReLU6 -> ReLUN, then ReLUN with trainable N which technically makes all other versions of ReLU redundant. So a standard would end up either missing someone's favorite primitive, or grow so large that it's not practical to be implemented by everyone (ie, like with TensorFlow GraphDef format which grew to incude 450+ different op types)", "body": "IMHO standard is hard to make useful at this point since the set of useful neural network primitives is changing so fast. Two years ago people wouldn't have thought about adding GRU unit, a year ago people would miss some neural attention primitives, and next year there may be categorically new ops or ops that unify existing ops redundant. Like with ReLU -> ReLU6 -> ReLUN, then ReLUN with trainable N which technically makes all other versions of ReLU redundant. So a standard would end up either missing someone's favorite primitive, or grow so large that it's not practical to be implemented by everyone (ie, like with TensorFlow GraphDef format which grew to incude 450+ different op types)\n"}