{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107119740", "pull_request_review_id": 28071024, "id": 107119740, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNzExOTc0MA==", "diff_hunk": "@@ -199,32 +199,111 @@ __global__ void MaxPoolBackward(const int nthreads, const dtype* top_diff,\n   }\n }\n \n-#undef CUDA_1D_KERNEL_LOOP\n-}  // namespace\n+template <typename dtype>\n+__global__ void MaxPoolGradBackwardNoMaskNCHW(\n+    const int nthreads, const dtype* bottom_data, const dtype* output_data,\n+    const int pooled_height, const int pooled_width, const int channels,\n+    const int height, const int width, const int kernel_h, const int kernel_w,\n+    const int stride_h, const int stride_w, const int pad_t, const int pad_l,\n+    const dtype* top_diff, dtype* bottom_diff) {\n+  CUDA_1D_KERNEL_LOOP(index, nthreads) {\n+    // First find out the index to the maximum, since we have no mask.\n+    int pw = index % pooled_width;\n+    int ph = (index / pooled_width) % pooled_height;\n+    int c = (index / pooled_width / pooled_height) % channels;\n+    int n = index / pooled_width / pooled_height / channels;\n+    int hstart = ph * stride_h - pad_t;\n+    int wstart = pw * stride_w - pad_l;\n+    const int hend = min(hstart + kernel_h, height);\n+    const int wend = min(wstart + kernel_w, width);\n+    hstart = max(hstart, 0);\n+    wstart = max(wstart, 0);\n+    bool should_stop = false;\n+    int maxidx = -1;\n+    const dtype* bottom_data_n = bottom_data + n * channels * height * width;\n+    // Propagate only first value from top_diff corresponding to the maximum.\n+    for (int h = hstart; h < hend && !should_stop; ++h) {\n+      for (int w = wstart; w < wend && !should_stop; ++w) {\n+        int idx = c * height * width + h * width + w;\n+        if (output_data[index] == bottom_data_n[idx]) {\n+          maxidx = idx;\n+          should_stop = true;\n+        }\n+      }\n+    }\n+    // Set the bottom diff (atomic is not necessary). The index could still be\n+    // uninitialized, if all the bottom_data are NaN.\n+    if (maxidx != -1) {\n+      bottom_diff[index] = top_diff[n * channels * height * width + maxidx];\n+    }\n+  }\n+}\n \n-bool MaxPoolForwardWithOptionalArgmax(\n-    const float* bottom_data, const int batch, const int height,\n-    const int width, const int channels, const int pooled_height,\n-    const int pooled_width, const int kernel_h, const int kernel_w,\n+template <typename dtype>\n+__global__ void MaxPoolGradBackwardNoMaskNHWC(\n+    const int nthreads, const dtype* bottom_data, const dtype* output_data,\n+    const int pooled_height, const int pooled_width, const int channels,\n+    const int height, const int width, const int kernel_h, const int kernel_w,\n     const int stride_h, const int stride_w, const int pad_t, const int pad_l,\n-    float* top_data, int64* mask, const Eigen::GpuDevice& d) {\n-  const int kThreadsPerBlock = 1024;\n-  const int output_size = batch * channels * pooled_height * pooled_width;\n+    const dtype* top_diff, dtype* bottom_diff) {\n+  CUDA_1D_KERNEL_LOOP(index, nthreads) {\n+    // First find out the index to the maximum, since we have no mask.\n+    int n = index;\n+    int c = n % channels;\n+    n /= channels;\n+    int wstart = (n % pooled_width) * stride_w - pad_l;\n+    n /= pooled_width;\n+    int hstart = (n % pooled_height) * stride_h - pad_t;\n+    n /= pooled_height;\n+    int hend = min(hstart + kernel_h, height);\n+    int wend = min(wstart + kernel_w, width);\n+    hstart = max(hstart, 0);\n+    wstart = max(wstart, 0);\n+    bool should_stop = false;\n+    int maxidx = -1;\n+    const dtype* bottom_data_n = bottom_data + n * height * width * channels;\n+    // Propagate only first value from top_diff corresponding to the maximum.\n+    for (int h = hstart; h < hend && !should_stop; ++h) {\n+      for (int w = wstart; w < wend && !should_stop; ++w) {\n+        int idx = (h * width + w) * channels + c;\n+        if (output_data[index] == bottom_data_n[idx]) {\n+          maxidx = idx;\n+          should_stop = true;\n+        }\n+      }\n+    }\n+    // Set the bottom diff (atomic is not necessary). The index could still be\n+    // uninitialized, if all the bottom_data are NaN.\n+    if (maxidx != -1) {\n+      bottom_diff[index] = top_diff[n * height * width * channels + maxidx];\n+    }\n+  }\n+}\n \n-  MaxPoolForwardNHWC<<<(output_size + kThreadsPerBlock - 1) / kThreadsPerBlock,\n-                       kThreadsPerBlock, 0, d.stream()>>>(\n-      output_size, bottom_data, height, width, channels, pooled_height,\n-      pooled_width, kernel_h, kernel_w, stride_h, stride_w, pad_t, pad_l,\n-      top_data, mask);\n-  return d.ok();\n+template <typename dtype>\n+__global__ void MaxPoolGradBackward(const int nthreads, const dtype* top_diff,\n+                                    const int64* mask, const int top_offset,", "path": "tensorflow/core/kernels/maxpooling_op_gpu.cu.cc", "position": null, "original_position": 102, "commit_id": "44449164baa4a004f69b5975a23c6335bc09797f", "original_commit_id": "3fdc5a2152162c0854194b3a84e4f2f89296e1a7", "user": {"login": "aam-at", "id": 486336, "node_id": "MDQ6VXNlcjQ4NjMzNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/486336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aam-at", "html_url": "https://github.com/aam-at", "followers_url": "https://api.github.com/users/aam-at/followers", "following_url": "https://api.github.com/users/aam-at/following{/other_user}", "gists_url": "https://api.github.com/users/aam-at/gists{/gist_id}", "starred_url": "https://api.github.com/users/aam-at/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aam-at/subscriptions", "organizations_url": "https://api.github.com/users/aam-at/orgs", "repos_url": "https://api.github.com/users/aam-at/repos", "events_url": "https://api.github.com/users/aam-at/events{/privacy}", "received_events_url": "https://api.github.com/users/aam-at/received_events", "type": "User", "site_admin": false}, "body": "It has the same size as the output `N*C*Hout*Wout` and contains the corresponding index in the input for each element in the output.", "created_at": "2017-03-21T10:29:06Z", "updated_at": "2017-03-28T04:33:24Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r107119740", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107119740"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r107119740"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664"}}, "body_html": "<p>It has the same size as the output <code>N*C*Hout*Wout</code> and contains the corresponding index in the input for each element in the output.</p>", "body_text": "It has the same size as the output N*C*Hout*Wout and contains the corresponding index in the input for each element in the output.", "in_reply_to_id": 106951346}