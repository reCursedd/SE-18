{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107065529", "pull_request_review_id": 28015437, "id": 107065529, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNzA2NTUyOQ==", "diff_hunk": "@@ -403,12 +364,260 @@ class MaxPoolingGradOp<Eigen::GpuDevice, T> : public OpKernel {\n   bool use_dnn_;\n };\n \n-REGISTER_KERNEL_BUILDER(\n-    Name(\"MaxPoolGrad\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n-    MaxPoolingGradOp<Eigen::GpuDevice, float>);\n-REGISTER_KERNEL_BUILDER(\n-    Name(\"MaxPoolGrad\").Device(DEVICE_GPU).TypeConstraint<Eigen::half>(\"T\"),\n-    MaxPoolingGradOp<Eigen::GpuDevice, Eigen::half>);\n+#endif  // GOOGLE_CUDA\n+\n+// The operation to compute gradient of MaxPool gradients.\n+// It takes three inputs:\n+//   - The original input tensor\n+//   - The original output tensor\n+//   - Backprop tensor for output gradients\n+// It produces one output: backprop tensor for output gradient.\n+template <class Device, class T>\n+class MaxPoolingGradGradOp : public OpKernel {\n+ public:\n+  explicit MaxPoolingGradGradOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    string data_format;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n+    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n+                errors::InvalidArgument(\"Invalid data format\"));\n+    OP_REQUIRES(\n+        context, data_format_ == FORMAT_NHWC,\n+        errors::InvalidArgument(\n+            \"Default MaxPoolinGradGradOp only supports NHWC \",\n+            \"on device type \", DeviceTypeString(context->device_type())));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n+    OP_REQUIRES(context, ksize_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window ksize field must \"\n+                                        \"specify 4 dimensions\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n+    OP_REQUIRES(context, stride_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window strides field must \"\n+                                        \"specify 4 dimensions\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n+    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n+                errors::Unimplemented(\n+                    \"Pooling is not yet supported on the batch dimension.\"));\n+    OP_REQUIRES(\n+        context, ksize_[3] == 1 && stride_[3] == 1,\n+        errors::Unimplemented(\n+            \"MaxPoolingGradGrad is not yet supported on the depth dimension.\"));\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    const Tensor& tensor_in = context->input(0);\n+    const Tensor& tensor_out = context->input(1);\n+    const Tensor& out_grad_backprop = context->input(2);\n+\n+    // For maxpooling, tensor_in should have 4 dimensions.\n+    OP_REQUIRES(context, tensor_in.dims() == 4,\n+                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n+    OP_REQUIRES(context, tensor_out.dims() == 4,\n+                errors::InvalidArgument(\"tensor_out must be 4-dimensional\"));\n+    // For maxpooling, out_grad_backprop should have 4 dimensions.\n+    OP_REQUIRES(\n+        context, out_grad_backprop.dims() == 4,\n+        errors::InvalidArgument(\"out_grad_backprop must be 4-dimensional\"));\n+\n+    PoolParameters params{context,  ksize_,      stride_,\n+                          padding_, FORMAT_NHWC, tensor_in.shape()};\n+    Tensor* output = nullptr;\n+    OP_REQUIRES_OK(context,\n+                   context->allocate_output(0, tensor_out.shape(), &output));\n+    if (!context->status().ok()) {", "path": "tensorflow/core/kernels/maxpooling_op.cc", "position": null, "original_position": 150, "commit_id": "44449164baa4a004f69b5975a23c6335bc09797f", "original_commit_id": "3fdc5a2152162c0854194b3a84e4f2f89296e1a7", "user": {"login": "aam-at", "id": 486336, "node_id": "MDQ6VXNlcjQ4NjMzNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/486336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aam-at", "html_url": "https://github.com/aam-at", "followers_url": "https://api.github.com/users/aam-at/followers", "following_url": "https://api.github.com/users/aam-at/following{/other_user}", "gists_url": "https://api.github.com/users/aam-at/gists{/gist_id}", "starred_url": "https://api.github.com/users/aam-at/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aam-at/subscriptions", "organizations_url": "https://api.github.com/users/aam-at/orgs", "repos_url": "https://api.github.com/users/aam-at/repos", "events_url": "https://api.github.com/users/aam-at/events{/privacy}", "received_events_url": "https://api.github.com/users/aam-at/received_events", "type": "User", "site_admin": false}, "body": "Okay. I suspected that but previous implementation of GradOp had an explicit checks. I will remove unnecessary checks throughout `maxpooling` file then.\r\nUpd:\r\nAfter rebase, it seems that unnecessary checks in GradOp were removed.", "created_at": "2017-03-21T03:18:32Z", "updated_at": "2017-03-28T04:33:24Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r107065529", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107065529"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r107065529"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664"}}, "body_html": "<p>Okay. I suspected that but previous implementation of GradOp had an explicit checks. I will remove unnecessary checks throughout <code>maxpooling</code> file then.<br>\nUpd:<br>\nAfter rebase, it seems that unnecessary checks in GradOp were removed.</p>", "body_text": "Okay. I suspected that but previous implementation of GradOp had an explicit checks. I will remove unnecessary checks throughout maxpooling file then.\nUpd:\nAfter rebase, it seems that unnecessary checks in GradOp were removed.", "in_reply_to_id": 106947613}