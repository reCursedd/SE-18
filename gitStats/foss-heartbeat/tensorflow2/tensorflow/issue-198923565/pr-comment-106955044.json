{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/106955044", "pull_request_review_id": 27901342, "id": 106955044, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNjk1NTA0NA==", "diff_hunk": "@@ -426,13 +492,216 @@ class AvgPooling3dGradOp : public OpKernel {\n   std::vector<int32> ksize_;\n   std::vector<int32> stride_;\n   Padding padding_;\n+  TensorFormat data_format_;\n };\n \n-REGISTER_KERNEL_BUILDER(Name(\"AvgPool3DGrad\")\n-                            .Device(DEVICE_CPU)\n-                            .TypeConstraint<float>(\"T\")\n-                            .HostMemory(\"orig_input_shape\"),\n-                        AvgPooling3dGradOp<CPUDevice, float>);\n+template <typename Device, typename T>\n+struct LaunchMaxPooling3dGradGradOp;\n+\n+template <typename T>\n+struct LaunchMaxPooling3dGradGradOp<CPUDevice, T> {\n+  static void launch(OpKernelContext* context, const Pool3dParameters& params,\n+                     const Tensor& tensor_in, const Tensor& tensor_out,\n+                     const Tensor& tensor_top_diff,\n+                     Tensor* tensor_bottom_diff) {\n+    OP_REQUIRES(\n+        context, params.data_format == FORMAT_NHWC,\n+        errors::InvalidArgument(\"Default MaxPoolingGradGradOp only supports\",\n+                                \"NHWC on CPU device type\"));\n+\n+    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n+        ConstEigenMatrixMap;\n+    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n+        EigenMatrixMap;\n+\n+    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), params.depth,\n+                               params.tensor_in_planes * params.tensor_in_cols *\n+                                   params.tensor_in_rows *\n+                                   params.tensor_in_batch);\n+    ConstEigenMatrixMap out_mat(tensor_out.flat<T>().data(), params.depth,\n+                                params.out_plane * params.out_width *\n+                                    params.out_height * params.tensor_in_batch);\n+    ConstEigenMatrixMap top_diff_mat(\n+        tensor_top_diff.flat<T>().data(), params.depth,\n+        params.tensor_in_planes * params.tensor_in_cols *\n+            params.tensor_in_rows * params.tensor_in_batch);\n+    EigenMatrixMap bottom_diff_mat(\n+        tensor_bottom_diff->flat<T>().data(), params.depth,\n+        params.out_plane * params.out_width * params.out_height *\n+            params.tensor_in_batch);\n+\n+    const DeviceBase::CpuWorkerThreads& worker_threads =\n+      *(context->device()->tensorflow_cpu_worker_threads());\n+\n+    auto shard = [&params, &in_mat, &out_mat, &top_diff_mat, &bottom_diff_mat](\n+        int64 start, int64 limit) {\n+\n+      const int32 depth = params.depth;\n+      const int32 in_planes = params.tensor_in_planes;\n+      const int32 in_rows = params.tensor_in_rows;\n+      const int32 in_cols = params.tensor_in_cols;\n+      const int32 pad_planes = params.pad_planes;\n+      const int32 pad_rows = params.pad_rows;\n+      const int32 pad_cols = params.pad_cols;\n+      const int32 window_planes = params.window_planes;\n+      const int32 window_rows = params.window_rows;\n+      const int32 window_cols = params.window_cols;\n+      const int32 plane_stride = params.plane_stride;\n+      const int32 row_stride = params.row_stride;\n+      const int32 col_stride = params.col_stride;\n+      const int32 out_plane = params.out_plane;\n+      const int32 out_height = params.out_height;\n+      const int32 out_width = params.out_width;\n+\n+      {\n+        // Initializes the output grad backprop tensor with 0.\n+        const int32 output_image_size =\n+            out_plane * out_height * out_width * params.depth;\n+        EigenMatrixMap bottom_diff_shard(\n+            bottom_diff_mat.data() + start * output_image_size, 1,\n+            (limit - start) * output_image_size);\n+        bottom_diff_shard.setZero();\n+      }\n+\n+      for (int b = start; b < limit; ++b) {\n+        for (int pp = 0; pp < out_plane; ++pp) {\n+          for (int ph = 0; ph < out_height; ++ph) {\n+            for (int pw = 0; pw < out_width; ++pw) {\n+              // (p_start, p_end) * (h_start, h_end) * (w_start, w_end) is the\n+              // range that the input vector projects to.\n+              int p_start = pp * plane_stride - pad_planes;\n+              const int p_end = std::min(p_start + window_planes, in_planes);\n+              int h_start = ph * row_stride - pad_rows;\n+              const int h_end = std::min(h_start + window_rows, in_rows);\n+              int w_start = pw * col_stride - pad_cols;\n+              const int w_end = std::min(w_start + window_cols, in_cols);\n+              p_start = std::max(p_start, 0);\n+              h_start = std::max(h_start, 0);\n+              w_start = std::max(w_start, 0);\n+              const int out_index =\n+                  ((b * out_plane + pp) * out_height + ph) * out_width + pw;\n+              // Find value corresponding to the input maximum in top_diff.\n+              for (int d = 0; d < depth; ++d) {\n+                const T& output_ref = out_mat.coeffRef(d, out_index);\n+                bool should_stop = false;\n+                for (int p = p_start; p < p_end && !should_stop; ++p) {\n+                  for (int h = h_start; h < h_end && !should_stop; ++h) {\n+                    for (int w = w_start; w < w_end && !should_stop; ++w) {\n+                      const int in_index =\n+                          ((b * in_planes + p) * in_rows + h) * in_cols + w;\n+                      const T& input_ref = in_mat.coeffRef(d, in_index);\n+                      if (output_ref == input_ref) {\n+                        T& bottom_diff_ref =\n+                            bottom_diff_mat.coeffRef(d, out_index);\n+                        bottom_diff_ref = top_diff_mat.coeffRef(d, in_index);\n+                        should_stop = true;\n+                      }\n+                    }\n+                  }\n+                }\n+              }\n+            }\n+          }\n+        }\n+      }\n+    };\n+    const int64 shard_cost =\n+        params.out_plane * params.out_height * params.out_width * params.depth *\n+        params.window_planes * params.window_rows * params.window_cols;\n+    Shard(worker_threads.num_threads, worker_threads.workers,\n+          params.tensor_in_batch, shard_cost, shard);\n+  }\n+};\n+\n+template <class Device, class T>\n+class MaxPooling3dGradGradOp : public OpKernel {\n+ public:\n+  explicit MaxPooling3dGradGradOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    string data_format;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n+    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n+                errors::InvalidArgument(\"Invalid data format\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n+    OP_REQUIRES(context, ksize_.size() == 5,\n+                errors::InvalidArgument(\"Sliding window ksize field must \"\n+                                        \"specify 5 dimensions\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n+    OP_REQUIRES(context, stride_.size() == 5,\n+                errors::InvalidArgument(\"Sliding window strides field must \"\n+                                        \"specify 5 dimensions\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n+    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n+                errors::Unimplemented(\n+                    \"Pooling is not yet supported on the batch dimension.\"));\n+    const int32 ksize_c = GetTensorDim3(ksize_, data_format_, 'C');\n+    const int32 stride_c = GetTensorDim3(stride_, data_format_, 'C');\n+    OP_REQUIRES(\n+        context, ksize_c == 1 && stride_c == 1,\n+        errors::Unimplemented(\n+            \"MaxPoolingGradGrad is not yet supported on the depth dimension.\"));\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    const Tensor& tensor_in = context->input(0);\n+    const Tensor& tensor_out = context->input(1);\n+    const Tensor& out_grad_backprop = context->input(2);\n+\n+    // For maxpooling3d, tensor_in should have 5 dimensions.\n+    OP_REQUIRES(context, tensor_in.dims() == 5,\n+                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n+    OP_REQUIRES(context, tensor_out.dims() == 5,\n+                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n+    // For maxpooling3d, out_grad_backprop should have 5 dimensions.\n+    OP_REQUIRES(\n+        context, out_grad_backprop.dims() == 5,\n+        errors::InvalidArgument(\"out_grad_backprop must be 5-dimensional\"));\n+\n+    Pool3dParameters params{context, ksize_, stride_,\n+                            padding_, data_format_, tensor_in.shape()};\n+\n+    Tensor* output = nullptr;\n+    OP_REQUIRES_OK(context,\n+                   context->allocate_output(0, tensor_out.shape(), &output));\n+    if (!context->status().ok()) {", "path": "tensorflow/core/kernels/pooling_ops_3d.cc", "position": null, "original_position": 333, "commit_id": "44449164baa4a004f69b5975a23c6335bc09797f", "original_commit_id": "3fdc5a2152162c0854194b3a84e4f2f89296e1a7", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "No need to check again.", "created_at": "2017-03-20T16:54:48Z", "updated_at": "2017-03-28T04:33:24Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r106955044", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/106955044"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r106955044"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664"}}, "body_html": "<p>No need to check again.</p>", "body_text": "No need to check again."}