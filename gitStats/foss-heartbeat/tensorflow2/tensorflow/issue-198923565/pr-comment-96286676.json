{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/96286676", "pull_request_review_id": 16864222, "id": 96286676, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk2Mjg2Njc2", "diff_hunk": "@@ -1102,45 +1112,84 @@ REGISTER_OP(\"MaxPool3D\")\n     .Attr(\"ksize: list(int) >= 5\")\n     .Attr(\"strides: list(int) >= 5\")\n     .Attr(GetPaddingAttrString())\n+    .Attr(GetConvnetDataFormatAttrString())\n     .Attr(\"T: numbertype\")\n     .SetShapeFn(shape_inference::Pool3DShape)\n     .Doc(R\"doc(\n Performs 3D max pooling on the input.\n \n-ksize: 1-D tensor of length 5. The size of the window for each dimension of\n-  the input tensor. Must have `ksize[0] = ksize[4] = 1`.\n-strides: 1-D tensor of length 5. The stride of the sliding window for each\n-  dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n+ksize: The size of the window for each dimension of the input tensor.\n+strides: The stride of the sliding window for each dimension of the\n+  input tensor.\n padding: The type of padding algorithm to use.\n+data_format: Specify the data format of the input and output data. With the\n+    default format \"NHWC\", the data is stored in the order of:\n+        [batch, in_planes, in_height, in_width, in_channels].\n+    Alternatively, the format could be \"NCHW\", the data storage order of:\n+        [batch, in_channels, in_planes, in_height, in_width].\n input: Shape `[batch, depth, rows, cols, channels]` tensor to pool over.\n output: The max pooled output tensor.\n )doc\");\n \n REGISTER_OP(\"MaxPool3DGrad\")\n-    .Input(\"orig_input: float\")\n-    .Input(\"orig_output: float\")\n+    .Input(\"orig_input: T\")\n+    .Input(\"orig_output: T\")\n     .Input(\"grad: T\")\n     .Output(\"output: T\")\n     .Attr(\"ksize: list(int) >= 5 \")\n     .Attr(\"strides: list(int) >= 5\")\n     .Attr(GetPaddingAttrString())\n+    .Attr(GetConvnetDataFormatAttrString())\n     .Attr(\"T: numbertype\")\n     .SetShapeFn([](InferenceContext* c) {\n       return UnchangedShapeWithRank(c, 5);\n     })\n     .Doc(R\"doc(\n Computes gradients of max pooling function.\n \n-ksize: 1-D tensor of length 5. The size of the window for each dimension of\n-  the input tensor. Must have `ksize[0] = ksize[4] = 1`.\n-strides: 1-D tensor of length 5. The stride of the sliding window for each\n-  dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n+ksize: The size of the window for each dimension of the input tensor.\n+strides: The stride of the sliding window for each dimension of the\n+  input tensor.\n+data_format: Specify the data format of the input and output data. With the\n+    default format \"NHWC\", the data is stored in the order of:\n+        [batch, in_planes, in_height, in_width, in_channels].\n+    Alternatively, the format could be \"NCHW\", the data storage order of:\n+        [batch, in_channels, in_planes, in_height, in_width].\n padding: The type of padding algorithm to use.\n orig_input: The original input tensor.\n orig_output: The original output tensor.\n grad: Output backprop of shape `[batch, depth, rows, cols, channels]`.\n )doc\");\n \n+REGISTER_OP(\"MaxPool3DGradGrad\")\n+    .Input(\"orig_input: T\")\n+    .Input(\"orig_output: T\")\n+    .Input(\"grad: T\")\n+    .Output(\"output: T\")\n+    .Attr(\"ksize: list(int) >= 5 \")\n+    .Attr(\"strides: list(int) >= 5\")\n+    .Attr(GetPaddingAttrString())\n+    .Attr(GetConvnetDataFormatAttrString())\n+    .Attr(\"T: realnumbertype\")\n+    .SetShapeFn(shape_inference::Pool3DShape)", "path": "tensorflow/core/ops/nn_ops.cc", "position": null, "original_position": 144, "commit_id": "44449164baa4a004f69b5975a23c6335bc09797f", "original_commit_id": "f64d63e1c671108e53762d49a4036b152069ba8c", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "body": "small note: this is doing validation that 'orig_input' and 'output' follow Pool3DShape's shape contract, but doesn't validate orig_output and 'grad'\r\n\r\nI would recommend augmenting this registration with something like:\r\n\r\n```\r\n  .SetShapeFn([](InferenceContext* c) {\r\n    TF_RETURN_IF_ERROR(shape_inference::Pool3DShape(c));\r\n    ShapeHandle unused;\r\n    // Validate 'grad' is the same shape as 'orig_input'\r\n    TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\r\n    // Validate 'orig_output' is same shape as 'output'\r\n    TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\r\n    return Status::OK();\r\n  })\r\n```\r\n\r\nor something like that\r\n\r\nand same with the other new ops you added", "created_at": "2017-01-16T18:46:13Z", "updated_at": "2017-03-28T04:33:24Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r96286676", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/96286676"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r96286676"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664"}}, "body_html": "<p>small note: this is doing validation that 'orig_input' and 'output' follow Pool3DShape's shape contract, but doesn't validate orig_output and 'grad'</p>\n<p>I would recommend augmenting this registration with something like:</p>\n<pre><code>  .SetShapeFn([](InferenceContext* c) {\n    TF_RETURN_IF_ERROR(shape_inference::Pool3DShape(c));\n    ShapeHandle unused;\n    // Validate 'grad' is the same shape as 'orig_input'\n    TF_RETURN_IF_ERROR(c-&gt;Merge(c-&gt;input(0), c-&gt;input(2), &amp;unused));\n    // Validate 'orig_output' is same shape as 'output'\n    TF_RETURN_IF_ERROR(c-&gt;Merge(c-&gt;input(1), c-&gt;output(0), &amp;unused));\n    return Status::OK();\n  })\n</code></pre>\n<p>or something like that</p>\n<p>and same with the other new ops you added</p>", "body_text": "small note: this is doing validation that 'orig_input' and 'output' follow Pool3DShape's shape contract, but doesn't validate orig_output and 'grad'\nI would recommend augmenting this registration with something like:\n  .SetShapeFn([](InferenceContext* c) {\n    TF_RETURN_IF_ERROR(shape_inference::Pool3DShape(c));\n    ShapeHandle unused;\n    // Validate 'grad' is the same shape as 'orig_input'\n    TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n    // Validate 'orig_output' is same shape as 'output'\n    TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n    return Status::OK();\n  })\n\nor something like that\nand same with the other new ops you added"}