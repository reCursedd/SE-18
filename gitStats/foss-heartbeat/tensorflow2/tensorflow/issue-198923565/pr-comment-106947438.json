{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/106947438", "pull_request_review_id": 27882337, "id": 106947438, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNjk0NzQzOA==", "diff_hunk": "@@ -403,12 +364,260 @@ class MaxPoolingGradOp<Eigen::GpuDevice, T> : public OpKernel {\n   bool use_dnn_;\n };\n \n-REGISTER_KERNEL_BUILDER(\n-    Name(\"MaxPoolGrad\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n-    MaxPoolingGradOp<Eigen::GpuDevice, float>);\n-REGISTER_KERNEL_BUILDER(\n-    Name(\"MaxPoolGrad\").Device(DEVICE_GPU).TypeConstraint<Eigen::half>(\"T\"),\n-    MaxPoolingGradOp<Eigen::GpuDevice, Eigen::half>);\n+#endif  // GOOGLE_CUDA\n+\n+// The operation to compute gradient of MaxPool gradients.\n+// It takes three inputs:\n+//   - The original input tensor\n+//   - The original output tensor\n+//   - Backprop tensor for output gradients\n+// It produces one output: backprop tensor for output gradient.\n+template <class Device, class T>\n+class MaxPoolingGradGradOp : public OpKernel {\n+ public:\n+  explicit MaxPoolingGradGradOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    string data_format;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n+    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n+                errors::InvalidArgument(\"Invalid data format\"));\n+    OP_REQUIRES(\n+        context, data_format_ == FORMAT_NHWC,\n+        errors::InvalidArgument(\n+            \"Default MaxPoolinGradGradOp only supports NHWC \",\n+            \"on device type \", DeviceTypeString(context->device_type())));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n+    OP_REQUIRES(context, ksize_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window ksize field must \"\n+                                        \"specify 4 dimensions\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n+    OP_REQUIRES(context, stride_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window strides field must \"\n+                                        \"specify 4 dimensions\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n+    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n+                errors::Unimplemented(\n+                    \"Pooling is not yet supported on the batch dimension.\"));\n+    OP_REQUIRES(\n+        context, ksize_[3] == 1 && stride_[3] == 1,\n+        errors::Unimplemented(\n+            \"MaxPoolingGradGrad is not yet supported on the depth dimension.\"));\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    const Tensor& tensor_in = context->input(0);\n+    const Tensor& tensor_out = context->input(1);\n+    const Tensor& out_grad_backprop = context->input(2);\n+\n+    // For maxpooling, tensor_in should have 4 dimensions.\n+    OP_REQUIRES(context, tensor_in.dims() == 4,\n+                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n+    OP_REQUIRES(context, tensor_out.dims() == 4,\n+                errors::InvalidArgument(\"tensor_out must be 4-dimensional\"));\n+    // For maxpooling, out_grad_backprop should have 4 dimensions.\n+    OP_REQUIRES(\n+        context, out_grad_backprop.dims() == 4,\n+        errors::InvalidArgument(\"out_grad_backprop must be 4-dimensional\"));\n+\n+    PoolParameters params{context,  ksize_,      stride_,\n+                          padding_, FORMAT_NHWC, tensor_in.shape()};\n+    Tensor* output = nullptr;\n+    OP_REQUIRES_OK(context,\n+                   context->allocate_output(0, tensor_out.shape(), &output));\n+    if (!context->status().ok()) {\n+      return;\n+    }\n+\n+    SpatialMaxPoolGradGrad(context, output, tensor_in, tensor_out,\n+                           out_grad_backprop, params, padding_);\n+  }\n+\n+ private:\n+  void SpatialMaxPoolGradGrad(OpKernelContext* context, Tensor* bottom_diff,\n+                              const Tensor& tensor_in, const Tensor& tensor_out,\n+                              const Tensor& top_diff,\n+                              const PoolParameters& params,\n+                              const Padding& padding) {\n+    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n+        ConstEigenMatrixMap;\n+    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n+        EigenMatrixMap;\n+\n+    ConstEigenMatrixMap in_mat(\n+        tensor_in.flat<T>().data(), params.depth,\n+        params.tensor_in_cols * params.tensor_in_rows * params.tensor_in_batch);\n+    ConstEigenMatrixMap out_mat(\n+        tensor_out.flat<T>().data(), params.depth,\n+        params.out_width * params.out_height * params.tensor_in_batch);\n+    ConstEigenMatrixMap top_diff_mat(\n+        top_diff.flat<T>().data(), params.depth,\n+        params.tensor_in_cols * params.tensor_in_rows * params.tensor_in_batch);\n+    EigenMatrixMap bottom_diff_mat(\n+        bottom_diff->flat<T>().data(), params.depth,\n+        params.out_width * params.out_height * params.tensor_in_batch);\n+\n+    const DeviceBase::CpuWorkerThreads& worker_threads =\n+        *(context->device()->tensorflow_cpu_worker_threads());\n+\n+    // The following code basically does the following:\n+    // 1. Flattens the input, output, top_diff and bottom_diff tensors into\n+    //    two dimensional arrays.\n+    //    tensor_in_as_matrix:\n+    //      depth by (tensor_in_cols * tensor_in_rows * tensor_in_batch)\n+    //    tensor_out_as_matrix:\n+    //      depth by (out_width * out_height * tensor_in_batch)\n+    //    top_diff_as_matrix:\n+    //      depth by (tensor_in_cols * tensor_in_rows * tensor_in_batch)\n+    //    bottom_diff_as_matrix:\n+    //      depth by (out_width * out_height * tensor_in_batch)\n+    //\n+    // 2. Walks through the set of columns in the flattened\n+    //    tensor_in_as_matrix, tensor_out_as_matrix, top_diff_as_matrix\n+    //    and updates the column(s) corresponding to the maximum values in\n+    //    tensor_out_as_matrix with the corresponding values in\n+    //    top_diff_as_matrix.\n+    auto shard = [&params, &in_mat, &out_mat, &top_diff_mat, &bottom_diff_mat](\n+        int64 start, int64 limit) {\n+\n+      const int32 depth = params.depth;\n+      const int32 in_rows = params.tensor_in_rows;\n+      const int32 in_cols = params.tensor_in_cols;\n+      const int32 pad_rows = params.pad_rows;\n+      const int32 pad_cols = params.pad_cols;\n+      const int32 window_rows = params.window_rows;\n+      const int32 window_cols = params.window_cols;\n+      const int32 row_stride = params.row_stride;\n+      const int32 col_stride = params.col_stride;\n+      const int32 out_height = params.out_height;\n+      const int32 out_width = params.out_width;\n+\n+      {\n+        // Initializes the output grad backprop tensor with 0.\n+        const int32 output_image_size = out_height * out_width * params.depth;\n+        EigenMatrixMap bottom_diff_shard(\n+            bottom_diff_mat.data() + start * output_image_size, 1,\n+            (limit - start) * output_image_size);\n+        bottom_diff_shard.setZero();\n+      }\n+\n+      for (int b = start; b < limit; ++b) {\n+        for (int ph = 0; ph < out_height; ++ph) {\n+          for (int pw = 0; pw < out_width; ++pw) {\n+            // (h_start, h_end) * (w_start, w_end) is the range that the input\n+            // vector projects to.\n+            int h_start = ph * row_stride - pad_rows;\n+            const int h_end = std::min(h_start + window_rows, in_rows);\n+            int w_start = pw * col_stride - pad_cols;\n+            const int w_end = std::min(w_start + window_cols, in_cols);\n+            h_start = std::max(h_start, 0);\n+            w_start = std::max(w_start, 0);\n+            const int out_index = (b * out_height + ph) * out_width + pw;\n+            // Find value corresponding to the input maximum in top_diff.\n+            for (int d = 0; d < depth; ++d) {\n+              const T& output_ref = out_mat.coeffRef(d, out_index);\n+              bool should_stop = false;\n+              for (int h = h_start; h < h_end && !should_stop; ++h) {\n+                for (int w = w_start; w < w_end && !should_stop; ++w) {\n+                  const int in_index = (b * in_rows + h) * in_cols + w;\n+                  const T& input_ref = in_mat.coeffRef(d, in_index);\n+                  if (output_ref == input_ref) {\n+                    T& bottom_diff_ref = bottom_diff_mat.coeffRef(d, out_index);\n+                    bottom_diff_ref = top_diff_mat.coeffRef(d, in_index);\n+                    should_stop = true;\n+                  }\n+                }\n+              }\n+            }\n+          }\n+        }\n+      }\n+    };\n+\n+    const int64 shard_cost = params.out_width * params.out_height *\n+                             params.depth * params.window_rows *\n+                             params.window_cols;\n+    Shard(worker_threads.num_threads, worker_threads.workers,\n+          params.tensor_in_batch, shard_cost, shard);\n+  }\n+\n+  std::vector<int32> ksize_;\n+  std::vector<int32> stride_;\n+  Padding padding_;\n+  TensorFormat data_format_;\n+};\n+\n+#ifdef GOOGLE_CUDA\n+\n+template <class T>\n+class MaxPoolingGradGradOp<Eigen::GpuDevice, T> : public OpKernel {\n+ public:\n+  typedef Eigen::GpuDevice Device;\n+\n+  explicit MaxPoolingGradGradOp(OpKernelConstruction* context)\n+      : OpKernel(context) {\n+    string data_format;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n+    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n+                errors::InvalidArgument(\"Invalid data format\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n+    OP_REQUIRES(context, ksize_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window ksize field must \"\n+                                        \"specify 4 dimensions\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n+    OP_REQUIRES(context, stride_.size() == 4,\n+                errors::InvalidArgument(\"Sliding window strides field must \"\n+                                        \"specify 4 dimensions\"));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n+    const int32 ksize_n = GetTensorDim(ksize_, data_format_, 'N');\n+    const int32 stride_n = GetTensorDim(stride_, data_format_, 'N');\n+    OP_REQUIRES(context, ksize_n == 1 && stride_n == 1,\n+                errors::Unimplemented(\n+                    \"Pooling is not yet supported on the batch dimension.\"));\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    const Tensor& tensor_in = context->input(0);\n+    const Tensor& tensor_out = context->input(1);\n+    const Tensor& out_grad_backprop = context->input(2);\n+\n+    // For maxpooling, tensor_in should have 4 dimensions.\n+    OP_REQUIRES(context, tensor_in.dims() == 4,\n+                errors::InvalidArgument(\"tensor_in must be 4-dimensional 4\"));\n+    OP_REQUIRES(context, tensor_out.dims() == 4,\n+                errors::InvalidArgument(\"tensor_out must be 4-dimensional\"));\n+    // For maxpooling, out_grad_backprop should have 4 dimensions.\n+    OP_REQUIRES(\n+        context, out_grad_backprop.dims() == 4,\n+        errors::InvalidArgument(\"out_grad_backprop must be 4-dimensional\"));\n+\n+    TensorShape input_shape = tensor_in.shape();\n+    TensorShape output_shape = tensor_out.shape();\n+    Tensor* output = nullptr;\n+    OP_REQUIRES_OK(context, context->allocate_output(0, output_shape, &output));\n+    if (!context->status().ok()) {", "path": "tensorflow/core/kernels/maxpooling_op.cc", "position": null, "original_position": 320, "commit_id": "44449164baa4a004f69b5975a23c6335bc09797f", "original_commit_id": "3fdc5a2152162c0854194b3a84e4f2f89296e1a7", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "you don't need this. The OP_REQUIRES_OK macro tests the status and returns if not OK.", "created_at": "2017-03-20T16:24:49Z", "updated_at": "2017-03-28T04:33:24Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r106947438", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/106947438"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/6664#discussion_r106947438"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/6664"}}, "body_html": "<p>you don't need this. The OP_REQUIRES_OK macro tests the status and returns if not OK.</p>", "body_text": "you don't need this. The OP_REQUIRES_OK macro tests the status and returns if not OK."}