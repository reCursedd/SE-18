{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/370883948", "html_url": "https://github.com/tensorflow/tensorflow/issues/17460#issuecomment-370883948", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17460", "id": 370883948, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDg4Mzk0OA==", "user": {"login": "jparkhill", "id": 4132346, "node_id": "MDQ6VXNlcjQxMzIzNDY=", "avatar_url": "https://avatars0.githubusercontent.com/u/4132346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jparkhill", "html_url": "https://github.com/jparkhill", "followers_url": "https://api.github.com/users/jparkhill/followers", "following_url": "https://api.github.com/users/jparkhill/following{/other_user}", "gists_url": "https://api.github.com/users/jparkhill/gists{/gist_id}", "starred_url": "https://api.github.com/users/jparkhill/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jparkhill/subscriptions", "organizations_url": "https://api.github.com/users/jparkhill/orgs", "repos_url": "https://api.github.com/users/jparkhill/repos", "events_url": "https://api.github.com/users/jparkhill/events{/privacy}", "received_events_url": "https://api.github.com/users/jparkhill/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-06T18:40:05Z", "updated_at": "2018-03-06T18:40:05Z", "author_association": "NONE", "body_html": "<p>Yaroslavv-<br>\nThanks for the rapid response and the phenomenal worldly good you're doing with TF.<br>\nI have found that a workaround is to use tf.clip_by_value():</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">TFDistance</span>(<span class=\"pl-smi\">A</span>):\n\t<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">\tCompute a distance matrix of A, a coordinate matrix</span>\n<span class=\"pl-s\">\tUsing the factorization:</span>\n<span class=\"pl-s\">\tDij = &lt;i|i&gt; - 2&lt;i|j&gt; + &lt;j,j&gt;</span>\n<span class=\"pl-s\">\tArgs:</span>\n<span class=\"pl-s\">\t\tA: a Nx3 matrix</span>\n<span class=\"pl-s\">\tReturns:</span>\n<span class=\"pl-s\">\t\tD: a NxN matrix</span>\n<span class=\"pl-s\">\t<span class=\"pl-pds\">\"\"\"</span></span>\n\tr <span class=\"pl-k\">=</span> tf.reduce_sum(A<span class=\"pl-k\">*</span>A, <span class=\"pl-c1\">1</span>)\n\tr <span class=\"pl-k\">=</span> tf.reshape(r, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> For the later broadcast.</span>\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> Tensorflow can only reverse mode grad the sqrt if all these elements</span>\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> are nonzero</span>\n\tD <span class=\"pl-k\">=</span> r <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>tf.matmul(A, tf.transpose(A)) <span class=\"pl-k\">+</span> tf.transpose(r)\n\t<span class=\"pl-k\">return</span> tf.sqrt(tf.clip_by_value(D,<span class=\"pl-c1\">1e-36</span>,<span class=\"pl-c1\">1e36</span>))\nxyzs <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>],<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float64)<span class=\"pl-k\">*</span><span class=\"pl-c1\">10.0</span>\nsess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\nsess.run(init)\nsess.run([TFDistance(xyzs),tf.gradients(TFDistance(xyzs),xyzs)],<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x:<span class=\"pl-c1\">0.2</span>})</pre></div>\n<p>Still this change is mildly disturbing because I worry some lower precision math is now implicated in the calculation, and I feel like some unit-test should have caught this. I'm sure anyone whose code computes d(sqrt()) or similar unstable functions is having similar issues.</p>", "body_text": "Yaroslavv-\nThanks for the rapid response and the phenomenal worldly good you're doing with TF.\nI have found that a workaround is to use tf.clip_by_value():\ndef TFDistance(A):\n\t\"\"\"\n\tCompute a distance matrix of A, a coordinate matrix\n\tUsing the factorization:\n\tDij = <i|i> - 2<i|j> + <j,j>\n\tArgs:\n\t\tA: a Nx3 matrix\n\tReturns:\n\t\tD: a NxN matrix\n\t\"\"\"\n\tr = tf.reduce_sum(A*A, 1)\n\tr = tf.reshape(r, [-1, 1]) # For the later broadcast.\n\t# Tensorflow can only reverse mode grad the sqrt if all these elements\n\t# are nonzero\n\tD = r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r)\n\treturn tf.sqrt(tf.clip_by_value(D,1e-36,1e36))\nxyzs = tf.random_uniform([2,3],dtype=tf.float64)*10.0\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\ninit = tf.global_variables_initializer()\nsess.run(init)\nsess.run([TFDistance(xyzs),tf.gradients(TFDistance(xyzs),xyzs)],feed_dict={x:0.2})\nStill this change is mildly disturbing because I worry some lower precision math is now implicated in the calculation, and I feel like some unit-test should have caught this. I'm sure anyone whose code computes d(sqrt()) or similar unstable functions is having similar issues.", "body": "Yaroslavv- \r\n    Thanks for the rapid response and the phenomenal worldly good you're doing with TF. \r\nI have found that a workaround is to use tf.clip_by_value(): \r\n\r\n```python\r\ndef TFDistance(A):\r\n\t\"\"\"\r\n\tCompute a distance matrix of A, a coordinate matrix\r\n\tUsing the factorization:\r\n\tDij = <i|i> - 2<i|j> + <j,j>\r\n\tArgs:\r\n\t\tA: a Nx3 matrix\r\n\tReturns:\r\n\t\tD: a NxN matrix\r\n\t\"\"\"\r\n\tr = tf.reduce_sum(A*A, 1)\r\n\tr = tf.reshape(r, [-1, 1]) # For the later broadcast.\r\n\t# Tensorflow can only reverse mode grad the sqrt if all these elements\r\n\t# are nonzero\r\n\tD = r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r)\r\n\treturn tf.sqrt(tf.clip_by_value(D,1e-36,1e36))\r\nxyzs = tf.random_uniform([2,3],dtype=tf.float64)*10.0\r\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\nsess.run([TFDistance(xyzs),tf.gradients(TFDistance(xyzs),xyzs)],feed_dict={x:0.2})\r\n```\r\nStill this change is mildly disturbing because I worry some lower precision math is now implicated in the calculation, and I feel like some unit-test should have caught this. I'm sure anyone whose code computes d(sqrt()) or similar unstable functions is having similar issues. "}