{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17460", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17460/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17460/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17460/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17460", "id": 302534608, "node_id": "MDU6SXNzdWUzMDI1MzQ2MDg=", "number": 17460, "title": "Precision in tf.gradients changed in 1.6 vs. <1.6", "user": {"login": "jparkhill", "id": 4132346, "node_id": "MDQ6VXNlcjQxMzIzNDY=", "avatar_url": "https://avatars0.githubusercontent.com/u/4132346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jparkhill", "html_url": "https://github.com/jparkhill", "followers_url": "https://api.github.com/users/jparkhill/followers", "following_url": "https://api.github.com/users/jparkhill/following{/other_user}", "gists_url": "https://api.github.com/users/jparkhill/gists{/gist_id}", "starred_url": "https://api.github.com/users/jparkhill/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jparkhill/subscriptions", "organizations_url": "https://api.github.com/users/jparkhill/orgs", "repos_url": "https://api.github.com/users/jparkhill/repos", "events_url": "https://api.github.com/users/jparkhill/events{/privacy}", "received_events_url": "https://api.github.com/users/jparkhill/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-06T02:02:00Z", "updated_at": "2018-03-06T18:40:05Z", "closed_at": "2018-03-06T18:40:05Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>Try the following code using different values for the infinitesimal 1e-15:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">TFDistance</span>(<span class=\"pl-smi\">A</span>):\n\t<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">\tCompute a distance matrix of A, a coordinate matrix</span>\n<span class=\"pl-s\">\tUsing the factorization:</span>\n<span class=\"pl-s\">\tDij = &lt;i|i&gt; - 2&lt;i|j&gt; + &lt;j,j&gt;</span>\n<span class=\"pl-s\">\tArgs:</span>\n<span class=\"pl-s\">\t\tA: a Nx3 matrix</span>\n<span class=\"pl-s\">\tReturns:</span>\n<span class=\"pl-s\">\t\tD: a NxN matrix</span>\n<span class=\"pl-s\">\t<span class=\"pl-pds\">\"\"\"</span></span>\n\tr <span class=\"pl-k\">=</span> tf.reduce_sum(A<span class=\"pl-k\">*</span>A, <span class=\"pl-c1\">1</span>)\n\tr <span class=\"pl-k\">=</span> tf.reshape(r, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>]) \n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> Tensorflow can only reverse mode grad the sqrt if all these elements</span>\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> are nonzero so add a small infinitesimal </span>\n\tD <span class=\"pl-k\">=</span> r <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>tf.matmul(A, tf.transpose(A)) <span class=\"pl-k\">+</span> tf.transpose(r) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1e-15</span>\n\t<span class=\"pl-k\">return</span> tf.sqrt(D)\nxyzs <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>],<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float64)<span class=\"pl-k\">*</span><span class=\"pl-c1\">10.0</span>\nsess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\nsess.run(init)\nsess.run([TFDistance(xyzs),tf.gradients(TFDistance(xyzs),xyzs)],<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x:<span class=\"pl-c1\">0.2</span>})</pre></div>\n<p>The issue is that in tf &lt; 1.6 a small infinitesimal  (&lt; 1e-15) was enough to avoid nan in tf.gradients due to 1/sqrt(0.0). For some reason in tf 1.6 this is returning a nan for anything less than 1e-13, which produces an unacceptable error in the result.</p>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nOSX, pip and pip3 tf 1.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n'v1.6.0-0-gd2e24b6039', '1.6.0')</li>\n<li><strong>Python version</strong>:<br>\n2.7 and 3.3</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nCPU</li>\n<li><strong>Exact command to reproduce</strong>:<br>\n(See script above)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>In versions of tensorflow previous to 1.6 this code would not issue a nan gradient for reasonable infinitesimals (&lt;1e-26). Suddenly the way numerical precision is handled in the gradient computation has obviously changed, and changed in a way which is clipping things near zero and doing so differently depending on if colocation of gradients is requested....</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>", "body_text": "System information\nTry the following code using different values for the infinitesimal 1e-15:\ndef TFDistance(A):\n\t\"\"\"\n\tCompute a distance matrix of A, a coordinate matrix\n\tUsing the factorization:\n\tDij = <i|i> - 2<i|j> + <j,j>\n\tArgs:\n\t\tA: a Nx3 matrix\n\tReturns:\n\t\tD: a NxN matrix\n\t\"\"\"\n\tr = tf.reduce_sum(A*A, 1)\n\tr = tf.reshape(r, [-1, 1]) \n\t# Tensorflow can only reverse mode grad the sqrt if all these elements\n\t# are nonzero so add a small infinitesimal \n\tD = r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r) + 1e-15\n\treturn tf.sqrt(D)\nxyzs = tf.random_uniform([2,3],dtype=tf.float64)*10.0\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\ninit = tf.global_variables_initializer()\nsess.run(init)\nsess.run([TFDistance(xyzs),tf.gradients(TFDistance(xyzs),xyzs)],feed_dict={x:0.2})\nThe issue is that in tf < 1.6 a small infinitesimal  (< 1e-15) was enough to avoid nan in tf.gradients due to 1/sqrt(0.0). For some reason in tf 1.6 this is returning a nan for anything less than 1e-13, which produces an unacceptable error in the result.\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nOSX, pip and pip3 tf 1.6\nTensorFlow installed from (source or binary):\nbinary\nTensorFlow version (use command below):\n'v1.6.0-0-gd2e24b6039', '1.6.0')\nPython version:\n2.7 and 3.3\nCUDA/cuDNN version:\nCPU\nExact command to reproduce:\n(See script above)\n\nDescribe the problem\nIn versions of tensorflow previous to 1.6 this code would not issue a nan gradient for reasonable infinitesimals (<1e-26). Suddenly the way numerical precision is handled in the gradient computation has obviously changed, and changed in a way which is clipping things near zero and doing so differently depending on if colocation of gradients is requested....\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "body": "### System information\r\nTry the following code using different values for the infinitesimal 1e-15: \r\n\r\n```python\r\ndef TFDistance(A):\r\n\t\"\"\"\r\n\tCompute a distance matrix of A, a coordinate matrix\r\n\tUsing the factorization:\r\n\tDij = <i|i> - 2<i|j> + <j,j>\r\n\tArgs:\r\n\t\tA: a Nx3 matrix\r\n\tReturns:\r\n\t\tD: a NxN matrix\r\n\t\"\"\"\r\n\tr = tf.reduce_sum(A*A, 1)\r\n\tr = tf.reshape(r, [-1, 1]) \r\n\t# Tensorflow can only reverse mode grad the sqrt if all these elements\r\n\t# are nonzero so add a small infinitesimal \r\n\tD = r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r) + 1e-15\r\n\treturn tf.sqrt(D)\r\nxyzs = tf.random_uniform([2,3],dtype=tf.float64)*10.0\r\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\nsess.run([TFDistance(xyzs),tf.gradients(TFDistance(xyzs),xyzs)],feed_dict={x:0.2})\r\n```\r\nThe issue is that in tf < 1.6 a small infinitesimal  (< 1e-15) was enough to avoid nan in tf.gradients due to 1/sqrt(0.0). For some reason in tf 1.6 this is returning a nan for anything less than 1e-13, which produces an unacceptable error in the result. \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nOSX, pip and pip3 tf 1.6\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n'v1.6.0-0-gd2e24b6039', '1.6.0')\r\n- **Python version**: \r\n2.7 and 3.3\r\n- **CUDA/cuDNN version**:\r\nCPU \r\n- **Exact command to reproduce**:\r\n(See script above) \r\n### Describe the problem\r\nIn versions of tensorflow previous to 1.6 this code would not issue a nan gradient for reasonable infinitesimals (<1e-26). Suddenly the way numerical precision is handled in the gradient computation has obviously changed, and changed in a way which is clipping things near zero and doing so differently depending on if colocation of gradients is requested....   \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n"}