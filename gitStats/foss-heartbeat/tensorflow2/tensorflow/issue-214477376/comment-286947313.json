{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/286947313", "html_url": "https://github.com/tensorflow/tensorflow/issues/8441#issuecomment-286947313", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8441", "id": 286947313, "node_id": "MDEyOklzc3VlQ29tbWVudDI4Njk0NzMxMw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-16T03:23:41Z", "updated_at": "2017-03-16T03:35:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> Hm, good point. For your proposed alternative I could also use it through ctypes and <code>compiled.so</code> would be a C++ binary that I created which uses TF C++ API and implements those 4 modes: variable in, forward, train op, variable out</p>\n<p>I was assuming that using XLA AOT for this is more efficient because it gets rid of unneeded TF runtime bits. (ie, tiny networks perform best when restricted to 1 core, so don't need parallel scheduling)</p>\n<p>Implementing this feature would require changes to <code>tfcompile</code> to support Variables and multiple fetch configurations. On other hand it would make XLA AOT compiler more useful/widely applicable.</p>\n<p>I'll leave it to XLA maintainers to decide if this feature fits into the XLA vision/design</p>", "body_text": "@asimshankar Hm, good point. For your proposed alternative I could also use it through ctypes and compiled.so would be a C++ binary that I created which uses TF C++ API and implements those 4 modes: variable in, forward, train op, variable out\nI was assuming that using XLA AOT for this is more efficient because it gets rid of unneeded TF runtime bits. (ie, tiny networks perform best when restricted to 1 core, so don't need parallel scheduling)\nImplementing this feature would require changes to tfcompile to support Variables and multiple fetch configurations. On other hand it would make XLA AOT compiler more useful/widely applicable.\nI'll leave it to XLA maintainers to decide if this feature fits into the XLA vision/design", "body": "@asimshankar Hm, good point. For your proposed alternative I could also use it through ctypes and `compiled.so` would be a C++ binary that I created which uses TF C++ API and implements those 4 modes: variable in, forward, train op, variable out\r\n\r\nI was assuming that using XLA AOT for this is more efficient because it gets rid of unneeded TF runtime bits. (ie, tiny networks perform best when restricted to 1 core, so don't need parallel scheduling)\r\n\r\nImplementing this feature would require changes to `tfcompile` to support Variables and multiple fetch configurations. On other hand it would make XLA AOT compiler more useful/widely applicable.\r\n\r\nI'll leave it to XLA maintainers to decide if this feature fits into the XLA vision/design"}