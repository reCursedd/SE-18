{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/114099150", "pull_request_review_id": 35540130, "id": 114099150, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNDA5OTE1MA==", "diff_hunk": "@@ -1859,3 +1859,115 @@ def __call__(self, inputs, state, scope=None):\n       new_state = core_rnn_cell.LSTMStateTuple(new_c, new_h)\n \n       return new_h, new_state\n+\n+class BasicConvLSTMCell(core_rnn_cell.RNNCell):\n+  \"\"\"Basic Convolutional LSTM recurrent network cell.\n+\n+  https://arxiv.org/pdf/1506.04214v1.pdf\n+  \"\"\"\n+\n+  def __init__(self,\n+               shape,\n+               filter_size,\n+               num_features,\n+               forget_bias=1.0,\n+               activation=math_ops.tanh,\n+               reuse=None):\n+    \"\"\"Initialize the basic Conv LSTM cell.\n+    Args:\n+      shape: int tuple thats the height and width of the cell\n+      filter_size: int tuple thats the height and width of the filter\n+      num_features: int thats the depth of the cell \n+      forget_bias: float, The bias added to forget gates (see above).\n+      input_size: Deprecated and unused.\n+      state_is_tuple: If True, accepted and returned states are 2-tuples of\n+        the `c_state` and `m_state`.  If False, they are concatenated\n+        along the column axis.  The latter behavior will soon be deprecated.\n+      activation: Activation function of the inner states.\n+    \"\"\"\n+    self._shape = shape \n+    self._filter_size = filter_size\n+    self._num_features = num_features \n+    self._forget_bias = forget_bias\n+    self._reuse = reuse\n+    self._activation = activation\n+\n+  @property\n+  def state_size(self):\n+    return core_rnn_cell.LSTMStateTuple(self._shape, self._shape)\n+\n+  @property\n+  def output_size(self):\n+    return self._shape\n+\n+  def zero_state(self, batch_size, dtype):\n+    shape = self._shape \n+    num_features = self._num_features\n+    zero_c = array_ops.zeros([batch_size, shape[0], shape[1], num_features], dtype=dtype)\n+    zero_h = array_ops.zeros([batch_size, shape[0], shape[1], num_features], dtype=dtype)\n+    zero_state = core_rnn_cell.LSTMStateTuple(zero_c, zero_h)\n+    return zero_state\n+\n+  def __call__(self, inputs, state, scope=None):\n+    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n+    with vs.variable_scope(scope or \"conv_lstm_cell\", reuse=self._reuse):  # \"BasicLSTMCell\"\n+      # Parameters of gates are concatenated into one multiply for efficiency.\n+      (c, h) = state\n+\n+      concat = _conv_linear([inputs, h], self._filter_size, self._num_features * 4, True)\n+\n+      i, j, f, o = array_ops.split(axis=3, num_or_size_splits=4, value=concat)\n+\n+      new_c = (c * math_ops.sigmoid(f + self._forget_bias) + math_ops.sigmoid(i) *\n+               self._activation(j))\n+      new_h = self._activation(new_c) * math_ops.sigmoid(o)\n+\n+      new_state = core_rnn_cell.LSTMStateTuple(new_c, new_h)\n+      return new_h, new_state\n+\n+def _conv_linear(args, filter_size, num_features, bias, bias_start=0.0, scope=None):\n+  \"\"\"convolution:\n+  Args:\n+    args: a 4D Tensor or a list of 4D, batch x n, Tensors.\n+    filter_size: int tuple of filter height and width.\n+    num_features: int, number of features.\n+    bias_start: starting value to initialize the bias; 0 by default.\n+    scope: VariableScope for the created subgraph; defaults to \"Conv\".\n+  Returns:\n+    A 4D Tensor with shape [batch h w num_features]\n+  Raises:\n+    ValueError: if some of the arguments has unspecified or wrong shape.\n+  \"\"\"\n+\n+  # Calculate the total size of arguments on dimension 1.\n+  total_arg_size_depth = 0\n+  shapes = [a.get_shape().as_list() for a in args]\n+  for shape in shapes:\n+    if len(shape) != 4:\n+      raise ValueError(\"Conv Linear is expecting 4D arguments: %s\" % str(shapes))\n+    if not shape[3]:\n+      raise ValueError(\"Conv Linear expects shape[4] of arguments: %s\" % str(shapes))\n+    else:\n+      total_arg_size_depth += shape[3]\n+\n+  dtype = [a.dtype for a in args][0]\n+\n+  # Now the computation.\n+  with vs.variable_scope(scope or \"Conv\"):\n+    matrix = vs.get_variable(\n+        \"Matrix\", [filter_size[0], filter_size[1], total_arg_size_depth, num_features], dtype=dtype)\n+    if len(args) == 1:\n+      res = nn_ops.conv2d(args[0], matrix, strides=[1, 1, 1, 1], padding='SAME')\n+    else:\n+      res = nn_ops.conv2d(array_ops.concat(axis=3, values=args), matrix, strides=[1, 1, 1, 1], padding='SAME')\n+    if not bias:\n+      return res", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 106, "commit_id": "a884a920d776b48310a79cf82fa7813fe24451df", "original_commit_id": "3e99f7440f86bfd5536cededb298d7e768439497", "user": {"login": "loliverhennigh", "id": 4978408, "node_id": "MDQ6VXNlcjQ5Nzg0MDg=", "avatar_url": "https://avatars1.githubusercontent.com/u/4978408?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loliverhennigh", "html_url": "https://github.com/loliverhennigh", "followers_url": "https://api.github.com/users/loliverhennigh/followers", "following_url": "https://api.github.com/users/loliverhennigh/following{/other_user}", "gists_url": "https://api.github.com/users/loliverhennigh/gists{/gist_id}", "starred_url": "https://api.github.com/users/loliverhennigh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loliverhennigh/subscriptions", "organizations_url": "https://api.github.com/users/loliverhennigh/orgs", "repos_url": "https://api.github.com/users/loliverhennigh/repos", "events_url": "https://api.github.com/users/loliverhennigh/events{/privacy}", "received_events_url": "https://api.github.com/users/loliverhennigh/received_events", "type": "User", "site_admin": false}, "body": "I cleaned this up a little bit however the function still has the 2 return statements. The reason I wrote it this way was because the `_linear` function written in `tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py` has 2 return statements. I can definitely change it but it seemed more consistent to do it this way.", "created_at": "2017-05-01T05:08:53Z", "updated_at": "2017-08-07T15:19:17Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8891#discussion_r114099150", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8891", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/114099150"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8891#discussion_r114099150"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8891"}}, "body_html": "<p>I cleaned this up a little bit however the function still has the 2 return statements. The reason I wrote it this way was because the <code>_linear</code> function written in <code>tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py</code> has 2 return statements. I can definitely change it but it seemed more consistent to do it this way.</p>", "body_text": "I cleaned this up a little bit however the function still has the 2 return statements. The reason I wrote it this way was because the _linear function written in tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py has 2 return statements. I can definitely change it but it seemed more consistent to do it this way.", "in_reply_to_id": 110891302}