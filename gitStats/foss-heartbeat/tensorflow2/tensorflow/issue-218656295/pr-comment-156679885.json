{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/156679885", "pull_request_review_id": 83195833, "id": 156679885, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NjY3OTg4NQ==", "diff_hunk": "@@ -1921,6 +1922,181 @@ def call(self, inputs, state):\n \n     return new_h, new_state\n \n+class ConvLSTMCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Convolutional LSTM recurrent network cell.\n+\n+  https://arxiv.org/pdf/1506.04214v1.pdf\n+  \"\"\"\n+\n+  def __init__(self,\n+               conv_ndims,\n+               input_shape,\n+               output_channels,\n+               kernel_shape,\n+               use_bias=True,\n+               skip_connection=False,\n+               forget_bias=1.0,\n+               initializers=None,\n+               name=\"conv_lstm_cell\"):\n+    \"\"\"Construct ConvLSTMCell.\n+    Args:\n+      conv_ndims: Convolution dimensionality (1, 2 or 3).\n+      input_shape: Shape of the input as int tuple, excluding the batch size.\n+      output_channels: int, number of output channels of the conv LSTM.\n+      kernel_shape: Shape of kernel as in tuple (of size 1,2 or 3).\n+      use_bias: Use bias in convolutions.\n+      skip_connection: If set to `True`, concatenate the input to the\n+      output of the conv LSTM. Default: `False`.\n+      forget_bias: Forget bias.\n+      name: Name of the module.\n+    Raises:\n+      ValueError: If `skip_connection` is `True` and stride is different from 1\n+        or if `input_shape` is incompatible with `conv_ndims`.\n+    \"\"\"\n+    super(ConvLSTMCell, self).__init__(name=name)\n+\n+    if conv_ndims != len(input_shape)-1:\n+      raise ValueError(\"Invalid input_shape {} for conv_ndims={}.\".format(\n+          input_shape, conv_ndims))\n+\n+    self._conv_ndims = conv_ndims\n+    self._input_shape = input_shape\n+    self._output_channels = output_channels\n+    self._kernel_shape = kernel_shape\n+    self._use_bias = use_bias\n+    self._forget_bias = forget_bias\n+    self._skip_connection = skip_connection\n+\n+    self._total_output_channels = output_channels\n+    if self._skip_connection:\n+      self._total_output_channels += self._input_shape[-1]\n+\n+    state_size = tensor_shape.TensorShape(self._input_shape[:-1] \n+                                          + [self._output_channels])\n+    self._state_size = rnn_cell_impl.LSTMStateTuple(state_size, state_size)\n+    self._output_size = tensor_shape.TensorShape(self._input_shape[:-1]\n+                                                 + [self._total_output_channels])\n+\n+  @property\n+  def output_size(self):\n+    return self._output_size\n+\n+  @property\n+  def state_size(self):\n+    return self._state_size\n+\n+  def call(self, inputs, state, scope=None):\n+    cell, hidden = state\n+    new_hidden = _conv([inputs, hidden],\n+                       self._kernel_shape,\n+                       4*self._output_channels,\n+                       self._use_bias)\n+    gates = array_ops.split(value=new_hidden,\n+                            num_or_size_splits=4,\n+                            axis=self._conv_ndims+1)\n+\n+    input_gate, new_input, forget_gate, output_gate = gates\n+    new_cell = math_ops.sigmoid(forget_gate + self._forget_bias) * cell\n+    new_cell += math_ops.sigmoid(input_gate) * math_ops.tanh(new_input)\n+    output = math_ops.tanh(new_cell) * math_ops.sigmoid(output_gate)\n+\n+    if self._skip_connection:\n+      output = array_ops.concat([output, inputs], axis=-1)\n+    new_state = rnn_cell_impl.LSTMStateTuple(new_cell, output)\n+    return output, new_state\n+\n+class Conv1DLSTMCell(ConvLSTMCell):\n+  \"\"\"1D Convolutional LSTM recurrent network cell.\n+\n+  https://arxiv.org/pdf/1506.04214v1.pdf\n+  \"\"\"\n+  def __init__(self, name=\"conv_1d_lstm_cell\", **kwargs):\n+    \"\"\"Construct Conv1DLSTM. See `ConvLSTMCell` for more details.\"\"\"\n+    super(Conv1DLSTMCell, self).__init__(conv_ndims=1, **kwargs)\n+\n+class Conv2DLSTMCell(ConvLSTMCell):\n+  \"\"\"2D Convolutional LSTM recurrent network cell.\n+\n+  https://arxiv.org/pdf/1506.04214v1.pdf\n+  \"\"\"\n+  def __init__(self, name=\"conv_2d_lstm_cell\", **kwargs):\n+    \"\"\"Construct Conv2DLSTM. See `ConvLSTMCell` for more details.\"\"\"\n+    super(Conv2DLSTMCell, self).__init__(conv_ndims=2, **kwargs)\n+\n+class Conv3DLSTMCell(ConvLSTMCell):\n+  \"\"\"3D Convolutional LSTM recurrent network cell.\n+\n+  https://arxiv.org/pdf/1506.04214v1.pdf\n+  \"\"\"\n+  def __init__(self, name=\"conv_3d_lstm_cell\", **kwargs):\n+    \"\"\"Construct Conv3DLSTM. See `ConvLSTMCell` for more details.\"\"\"\n+    super(Conv3DLSTMCell, self).__init__(conv_ndims=3, **kwargs)\n+\n+def _conv(args, \n+          filter_size,\n+          num_features,\n+          bias,\n+          bias_start=0.0):\n+  \"\"\"convolution:\n+  Args:\n+    args: a Tensor or a list of Tensors of dimension 3D, 4D or 5D, \n+    batch x n, Tensors.\n+    filter_size: int tuple of filter height and width.\n+    num_features: int, number of features.\n+    bias_start: starting value to initialize the bias; 0 by default.\n+  Returns:\n+    A 3D, 4D, or 5D Tensor with shape [batch ... num_features]\n+  Raises:\n+    ValueError: if some of the arguments has unspecified or wrong shape.\n+  \"\"\"\n+\n+  # Calculate the total size of arguments on dimension 1.\n+  total_arg_size_depth = 0\n+  shapes = [a.get_shape().as_list() for a in args]\n+  shape_length = len(shapes[0])\n+  for shape in shapes:\n+    if len(shape) not in [3,4,5]:\n+      raise ValueError(\"Conv Linear expects 3D, 4D or 5D arguments: %s\" % str(shapes))\n+    if len(shape) != len(shapes[0]):", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": 147, "original_position": 147, "commit_id": "a884a920d776b48310a79cf82fa7813fe24451df", "original_commit_id": "a884a920d776b48310a79cf82fa7813fe24451df", "user": {"login": "AllInNVDA", "id": 7636434, "node_id": "MDQ6VXNlcjc2MzY0MzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/7636434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AllInNVDA", "html_url": "https://github.com/AllInNVDA", "followers_url": "https://api.github.com/users/AllInNVDA/followers", "following_url": "https://api.github.com/users/AllInNVDA/following{/other_user}", "gists_url": "https://api.github.com/users/AllInNVDA/gists{/gist_id}", "starred_url": "https://api.github.com/users/AllInNVDA/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AllInNVDA/subscriptions", "organizations_url": "https://api.github.com/users/AllInNVDA/orgs", "repos_url": "https://api.github.com/users/AllInNVDA/repos", "events_url": "https://api.github.com/users/AllInNVDA/events{/privacy}", "received_events_url": "https://api.github.com/users/AllInNVDA/received_events", "type": "User", "site_admin": false}, "body": "You could just use ```shape_length``` instead of recalculating it again.", "created_at": "2017-12-13T14:51:59Z", "updated_at": "2017-12-13T14:51:59Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8891#discussion_r156679885", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8891", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/156679885"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8891#discussion_r156679885"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8891"}}, "body_html": "<p>You could just use <code>shape_length</code> instead of recalculating it again.</p>", "body_text": "You could just use shape_length instead of recalculating it again."}