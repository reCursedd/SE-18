{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/328707523", "html_url": "https://github.com/tensorflow/tensorflow/issues/12960#issuecomment-328707523", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12960", "id": 328707523, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODcwNzUyMw==", "user": {"login": "chaos-dd", "id": 6304566, "node_id": "MDQ6VXNlcjYzMDQ1NjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/6304566?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chaos-dd", "html_url": "https://github.com/chaos-dd", "followers_url": "https://api.github.com/users/chaos-dd/followers", "following_url": "https://api.github.com/users/chaos-dd/following{/other_user}", "gists_url": "https://api.github.com/users/chaos-dd/gists{/gist_id}", "starred_url": "https://api.github.com/users/chaos-dd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chaos-dd/subscriptions", "organizations_url": "https://api.github.com/users/chaos-dd/orgs", "repos_url": "https://api.github.com/users/chaos-dd/repos", "events_url": "https://api.github.com/users/chaos-dd/events{/privacy}", "received_events_url": "https://api.github.com/users/chaos-dd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-12T01:48:42Z", "updated_at": "2017-09-12T01:52:02Z", "author_association": "NONE", "body_html": "<p>I already try to add  thread num and buffer size param to dataset method map, but it doed not speed up.  It seems that the two params  does not  used for pre read data accoding to the docs.</p>\n<p>My problem is when I have multiple gpu in one machine, I want  data parallism to speed up training speed. So I add multiple dataset node in the tensorflow graph  one for a gpu, but it seems that multiple dataset read data seqnencely and in one dataset there's no data pre reading. Multiple gpu can not be fully used as reading data into tf takes so much time.</p>", "body_text": "I already try to add  thread num and buffer size param to dataset method map, but it doed not speed up.  It seems that the two params  does not  used for pre read data accoding to the docs.\nMy problem is when I have multiple gpu in one machine, I want  data parallism to speed up training speed. So I add multiple dataset node in the tensorflow graph  one for a gpu, but it seems that multiple dataset read data seqnencely and in one dataset there's no data pre reading. Multiple gpu can not be fully used as reading data into tf takes so much time.", "body": "I already try to add  thread num and buffer size param to dataset method map, but it doed not speed up.  It seems that the two params  does not  used for pre read data accoding to the docs.\r\n\r\nMy problem is when I have multiple gpu in one machine, I want  data parallism to speed up training speed. So I add multiple dataset node in the tensorflow graph  one for a gpu, but it seems that multiple dataset read data seqnencely and in one dataset there's no data pre reading. Multiple gpu can not be fully used as reading data into tf takes so much time.\r\n"}