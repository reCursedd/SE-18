{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/178431482", "html_url": "https://github.com/tensorflow/tensorflow/issues/550#issuecomment-178431482", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/550", "id": 178431482, "node_id": "MDEyOklzc3VlQ29tbWVudDE3ODQzMTQ4Mg==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-02T07:52:35Z", "updated_at": "2016-02-02T07:52:35Z", "author_association": "MEMBER", "body_html": "<p>I think the comment about the difference between your dev and train data came from your log output like this:</p>\n<pre><code>global step 374600 learning rate 0.0069 step-time 1.92 perplexity 1.02\n  eval: bucket 0 perplexity 137268.32\n</code></pre>\n<p>Your training perplexity is 1.02 -- the model is basically perfect on the data it receives for training. But your dev perplexity is enormous, the model does not work at all. How did it look in earlier epochs? I would suspect that there is some mismatch in something. Maybe the tokenization is different for train and dev? Maybe the sizes of the buckets from the original translation model are not appropriate for your use-case? If you share more details, I'll be happy to take a look and try to help (maybe better on the original bug).</p>", "body_text": "I think the comment about the difference between your dev and train data came from your log output like this:\nglobal step 374600 learning rate 0.0069 step-time 1.92 perplexity 1.02\n  eval: bucket 0 perplexity 137268.32\n\nYour training perplexity is 1.02 -- the model is basically perfect on the data it receives for training. But your dev perplexity is enormous, the model does not work at all. How did it look in earlier epochs? I would suspect that there is some mismatch in something. Maybe the tokenization is different for train and dev? Maybe the sizes of the buckets from the original translation model are not appropriate for your use-case? If you share more details, I'll be happy to take a look and try to help (maybe better on the original bug).", "body": "I think the comment about the difference between your dev and train data came from your log output like this:\n\n```\nglobal step 374600 learning rate 0.0069 step-time 1.92 perplexity 1.02\n  eval: bucket 0 perplexity 137268.32\n```\n\nYour training perplexity is 1.02 -- the model is basically perfect on the data it receives for training. But your dev perplexity is enormous, the model does not work at all. How did it look in earlier epochs? I would suspect that there is some mismatch in something. Maybe the tokenization is different for train and dev? Maybe the sizes of the buckets from the original translation model are not appropriate for your use-case? If you share more details, I'll be happy to take a look and try to help (maybe better on the original bug).\n"}