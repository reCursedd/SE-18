{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/173501276", "html_url": "https://github.com/tensorflow/tensorflow/issues/550#issuecomment-173501276", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/550", "id": 173501276, "node_id": "MDEyOklzc3VlQ29tbWVudDE3MzUwMTI3Ng==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-21T08:45:19Z", "updated_at": "2016-01-21T08:45:19Z", "author_association": "MEMBER", "body_html": "<p>One thing that needs to be remembered is that neural translation is a young and active area of reseach -- it's not like there are established ways for everything. Let me comment on two points.</p>\n<p>(1) Perplexity vs BLEU. In a number of recent papers on neural translation many researchers report that perplexity correlates well with BLEU scores. Some papers even claim that perplexity correlates better with human judgement of translations than BLEU. But these are all just imperfect measures, sometimes imperfect in different ways. Looking at a few self-chosen samples is also an imperfect measure: the ones you consider might just be more outside of the training data (which comes from newspapers if I understand right), but not really harder. So in the end one needs to look at all of them to get a reasonable measurement and think hard what they mean.</p>\n<p>(2) OOV words. This is a well-known problem in neural translation models and a there are ongoing research efforts to deal with it. One way that seems to work well is having multiple UNK tokens and doing some replacements, see this paper: <a href=\"http://arxiv.org/abs/1410.8206\" rel=\"nofollow\">http://arxiv.org/abs/1410.8206</a> . In a basic model like the one in the tutorial the UNK is heavily over-represented in the training data which leads to bad decodings and a bit \"cheated\" perplexity, as you said. You can try to just decrease UNK probability in the training by using a few different UNKs, or the method from the above paper, or something else entirely.</p>\n<p>I'm writing about these points to illustrate one thing: this is an ongoing research effort. We constructed the tutorial to show how to get started with these kinds of models in TensorFlow, but all these research questions remain open. You can now see where the problems are and start working on your own solutions -- that's the intention!</p>", "body_text": "One thing that needs to be remembered is that neural translation is a young and active area of reseach -- it's not like there are established ways for everything. Let me comment on two points.\n(1) Perplexity vs BLEU. In a number of recent papers on neural translation many researchers report that perplexity correlates well with BLEU scores. Some papers even claim that perplexity correlates better with human judgement of translations than BLEU. But these are all just imperfect measures, sometimes imperfect in different ways. Looking at a few self-chosen samples is also an imperfect measure: the ones you consider might just be more outside of the training data (which comes from newspapers if I understand right), but not really harder. So in the end one needs to look at all of them to get a reasonable measurement and think hard what they mean.\n(2) OOV words. This is a well-known problem in neural translation models and a there are ongoing research efforts to deal with it. One way that seems to work well is having multiple UNK tokens and doing some replacements, see this paper: http://arxiv.org/abs/1410.8206 . In a basic model like the one in the tutorial the UNK is heavily over-represented in the training data which leads to bad decodings and a bit \"cheated\" perplexity, as you said. You can try to just decrease UNK probability in the training by using a few different UNKs, or the method from the above paper, or something else entirely.\nI'm writing about these points to illustrate one thing: this is an ongoing research effort. We constructed the tutorial to show how to get started with these kinds of models in TensorFlow, but all these research questions remain open. You can now see where the problems are and start working on your own solutions -- that's the intention!", "body": "One thing that needs to be remembered is that neural translation is a young and active area of reseach -- it's not like there are established ways for everything. Let me comment on two points.\n\n(1) Perplexity vs BLEU. In a number of recent papers on neural translation many researchers report that perplexity correlates well with BLEU scores. Some papers even claim that perplexity correlates better with human judgement of translations than BLEU. But these are all just imperfect measures, sometimes imperfect in different ways. Looking at a few self-chosen samples is also an imperfect measure: the ones you consider might just be more outside of the training data (which comes from newspapers if I understand right), but not really harder. So in the end one needs to look at all of them to get a reasonable measurement and think hard what they mean.\n\n(2) OOV words. This is a well-known problem in neural translation models and a there are ongoing research efforts to deal with it. One way that seems to work well is having multiple UNK tokens and doing some replacements, see this paper: http://arxiv.org/abs/1410.8206 . In a basic model like the one in the tutorial the UNK is heavily over-represented in the training data which leads to bad decodings and a bit \"cheated\" perplexity, as you said. You can try to just decrease UNK probability in the training by using a few different UNKs, or the method from the above paper, or something else entirely.\n\nI'm writing about these points to illustrate one thing: this is an ongoing research effort. We constructed the tutorial to show how to get started with these kinds of models in TensorFlow, but all these research questions remain open. You can now see where the problems are and start working on your own solutions -- that's the intention!\n"}