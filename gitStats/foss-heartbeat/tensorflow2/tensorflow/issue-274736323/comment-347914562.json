{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/347914562", "html_url": "https://github.com/tensorflow/tensorflow/issues/14642#issuecomment-347914562", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14642", "id": 347914562, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NzkxNDU2Mg==", "user": {"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T16:24:59Z", "updated_at": "2017-11-29T16:24:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>No need for a different issue for iOS since this is platform-independent.</p>\n<p>The conversion tool uses std_value and mean_value to map raw input values into real values:<br>\nreal_value = (raw_input_value - mean_value) / std_value<br>\nThe default values of 0 and 1 yield real_value = raw_input_value, which in conjunction with -<br>\ninference_type=QUANTIZED_UINT8 means the input will range from 0.0 to 255.0, while the output will use the default ranges or 0.0 to 6.0.</p>\n<p>By passing '--std_values=128' '--mean_values=128'  you are telling the conversion tool that the real input values are between -1.0 and 1.0 (for the corresponding uint8 0 and 255), which satisfies the quantization constraint that input_product_scale &lt; output_scale. (That's the gist of it, the details are slightly  more complex).</p>\n<p>We are in the process of documenting the details on how to retain a quantized model for TF Lite. Meanwhile, if you are interested there are more details here: <a href=\"https://stackoverflow.com/questions/47463204\" rel=\"nofollow\">https://stackoverflow.com/questions/47463204</a></p>", "body_text": "No need for a different issue for iOS since this is platform-independent.\nThe conversion tool uses std_value and mean_value to map raw input values into real values:\nreal_value = (raw_input_value - mean_value) / std_value\nThe default values of 0 and 1 yield real_value = raw_input_value, which in conjunction with -\ninference_type=QUANTIZED_UINT8 means the input will range from 0.0 to 255.0, while the output will use the default ranges or 0.0 to 6.0.\nBy passing '--std_values=128' '--mean_values=128'  you are telling the conversion tool that the real input values are between -1.0 and 1.0 (for the corresponding uint8 0 and 255), which satisfies the quantization constraint that input_product_scale < output_scale. (That's the gist of it, the details are slightly  more complex).\nWe are in the process of documenting the details on how to retain a quantized model for TF Lite. Meanwhile, if you are interested there are more details here: https://stackoverflow.com/questions/47463204", "body": "No need for a different issue for iOS since this is platform-independent.\r\n\r\nThe conversion tool uses std_value and mean_value to map raw input values into real values:\r\n    real_value = (raw_input_value - mean_value) / std_value\r\nThe default values of 0 and 1 yield real_value = raw_input_value, which in conjunction with -\r\n inference_type=QUANTIZED_UINT8 means the input will range from 0.0 to 255.0, while the output will use the default ranges or 0.0 to 6.0.\r\n\r\nBy passing '--std_values=128' '--mean_values=128'  you are telling the conversion tool that the real input values are between -1.0 and 1.0 (for the corresponding uint8 0 and 255), which satisfies the quantization constraint that input_product_scale < output_scale. (That's the gist of it, the details are slightly  more complex).\r\n\r\nWe are in the process of documenting the details on how to retain a quantized model for TF Lite. Meanwhile, if you are interested there are more details here: https://stackoverflow.com/questions/47463204\r\n"}