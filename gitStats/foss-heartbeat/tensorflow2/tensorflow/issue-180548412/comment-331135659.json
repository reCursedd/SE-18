{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/331135659", "html_url": "https://github.com/tensorflow/tensorflow/issues/4722#issuecomment-331135659", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4722", "id": 331135659, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTEzNTY1OQ==", "user": {"login": "dsalaj", "id": 8525546, "node_id": "MDQ6VXNlcjg1MjU1NDY=", "avatar_url": "https://avatars1.githubusercontent.com/u/8525546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dsalaj", "html_url": "https://github.com/dsalaj", "followers_url": "https://api.github.com/users/dsalaj/followers", "following_url": "https://api.github.com/users/dsalaj/following{/other_user}", "gists_url": "https://api.github.com/users/dsalaj/gists{/gist_id}", "starred_url": "https://api.github.com/users/dsalaj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dsalaj/subscriptions", "organizations_url": "https://api.github.com/users/dsalaj/orgs", "repos_url": "https://api.github.com/users/dsalaj/repos", "events_url": "https://api.github.com/users/dsalaj/events{/privacy}", "received_events_url": "https://api.github.com/users/dsalaj/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-21T12:01:19Z", "updated_at": "2017-09-21T12:58:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7218057\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fps7806\">@fps7806</a> I am currently using <code>einsum</code> with dynamic batch dimension like so:<br>\n<code>input = tf.placeholder(dtype=tf.float32, shape=(None, n_step, n_in))</code><br>\n<code>weights = tf.Variable(tf.truncated_normal((n_in, 1), stddev=1.0/np.sqrt(n_in)))</code><br>\n<code>Y_predict = tf.einsum('ijk,kl-&gt;ijl', input, weights)</code><br>\nIt works without errors which contradicts your previous statement. Has this been patched in the meantime?</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10172392\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xuancong84\">@xuancong84</a>  I am also possibly experiencing 'bad' gradients when using <code>einsum</code>. Since the <code>einsum</code>is syntactic sugar for <code>matmul</code> I assumed there should be no problems with back propagation. But when using <code>einsum</code> even with untrainable weights (same example as above) the networks fails to learn. Is <code>einsum</code> verified to work well in backprop?<br>\nI can confirm that if in my example above, <code>einsum</code> is replaced by reshape matmul alternative:<br>\n<code>input_ = tf.reshape(input, [-1, n_in])</code><br>\n<code>Y_predict_ = tf.matmul(input_ , weights)</code><br>\n<code>Y_predict = tf.reshape(Y_predict_, [-1, n_step])</code><br>\nthe network learns as expected. Is this a bug in <code>einsum</code>?</p>", "body_text": "@fps7806 I am currently using einsum with dynamic batch dimension like so:\ninput = tf.placeholder(dtype=tf.float32, shape=(None, n_step, n_in))\nweights = tf.Variable(tf.truncated_normal((n_in, 1), stddev=1.0/np.sqrt(n_in)))\nY_predict = tf.einsum('ijk,kl->ijl', input, weights)\nIt works without errors which contradicts your previous statement. Has this been patched in the meantime?\n@xuancong84  I am also possibly experiencing 'bad' gradients when using einsum. Since the einsumis syntactic sugar for matmul I assumed there should be no problems with back propagation. But when using einsum even with untrainable weights (same example as above) the networks fails to learn. Is einsum verified to work well in backprop?\nI can confirm that if in my example above, einsum is replaced by reshape matmul alternative:\ninput_ = tf.reshape(input, [-1, n_in])\nY_predict_ = tf.matmul(input_ , weights)\nY_predict = tf.reshape(Y_predict_, [-1, n_step])\nthe network learns as expected. Is this a bug in einsum?", "body": "@fps7806 I am currently using `einsum` with dynamic batch dimension like so:\r\n`input = tf.placeholder(dtype=tf.float32, shape=(None, n_step, n_in))`\r\n`weights = tf.Variable(tf.truncated_normal((n_in, 1), stddev=1.0/np.sqrt(n_in)))`\r\n`Y_predict = tf.einsum('ijk,kl->ijl', input, weights)`\r\nIt works without errors which contradicts your previous statement. Has this been patched in the meantime?\r\n\r\n@xuancong84  I am also possibly experiencing 'bad' gradients when using `einsum`. Since the `einsum`is syntactic sugar for `matmul` I assumed there should be no problems with back propagation. But when using `einsum` even with untrainable weights (same example as above) the networks fails to learn. Is `einsum` verified to work well in backprop?\r\nI can confirm that if in my example above, `einsum` is replaced by reshape matmul alternative:\r\n`input_ = tf.reshape(input, [-1, n_in])`\r\n`Y_predict_ = tf.matmul(input_ , weights)`\r\n`Y_predict = tf.reshape(Y_predict_, [-1, n_step])`\r\nthe network learns as expected. Is this a bug in `einsum`?"}