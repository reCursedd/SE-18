{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/251902625", "html_url": "https://github.com/tensorflow/tensorflow/issues/4722#issuecomment-251902625", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4722", "id": 251902625, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MTkwMjYyNQ==", "user": {"login": "xuancong84", "id": 10172392, "node_id": "MDQ6VXNlcjEwMTcyMzky", "avatar_url": "https://avatars0.githubusercontent.com/u/10172392?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuancong84", "html_url": "https://github.com/xuancong84", "followers_url": "https://api.github.com/users/xuancong84/followers", "following_url": "https://api.github.com/users/xuancong84/following{/other_user}", "gists_url": "https://api.github.com/users/xuancong84/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuancong84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuancong84/subscriptions", "organizations_url": "https://api.github.com/users/xuancong84/orgs", "repos_url": "https://api.github.com/users/xuancong84/repos", "events_url": "https://api.github.com/users/xuancong84/events{/privacy}", "received_events_url": "https://api.github.com/users/xuancong84/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-06T08:46:50Z", "updated_at": "2016-10-06T08:46:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9015977\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nikhilmishra000\">@nikhilmishra000</a> Yes, the forward is correct. You don't need to verify, I have verified by myself. The forward calculation of the following two gives exactly the same results up to all significant figures:</p>\n<pre><code>if True:                                                                                                          \n    batch_Wy1 = tf.reshape(tf.tile(weights['Wy1'],[current_batch_size,1]),[-1, hiddenSize, hiddenSize], name=\"Wy1\")\n    WyY1 = tf.batch_matmul(outputs2,batch_Wy1, name=\"WyY1\")                                                        \nelse:                                                                                                              \n    WyY1 = tf.einsum('ijk,kl-&gt;ijl',outputs2,weights['Wy1'])      \n</code></pre>\n<p>However, if I use Adam loss, the update gives slightly different results. I didn't try other loss functions, and this is not necessarily a bug.</p>", "body_text": "@nikhilmishra000 Yes, the forward is correct. You don't need to verify, I have verified by myself. The forward calculation of the following two gives exactly the same results up to all significant figures:\nif True:                                                                                                          \n    batch_Wy1 = tf.reshape(tf.tile(weights['Wy1'],[current_batch_size,1]),[-1, hiddenSize, hiddenSize], name=\"Wy1\")\n    WyY1 = tf.batch_matmul(outputs2,batch_Wy1, name=\"WyY1\")                                                        \nelse:                                                                                                              \n    WyY1 = tf.einsum('ijk,kl->ijl',outputs2,weights['Wy1'])      \n\nHowever, if I use Adam loss, the update gives slightly different results. I didn't try other loss functions, and this is not necessarily a bug.", "body": "@nikhilmishra000 Yes, the forward is correct. You don't need to verify, I have verified by myself. The forward calculation of the following two gives exactly the same results up to all significant figures:\n\n```\nif True:                                                                                                          \n    batch_Wy1 = tf.reshape(tf.tile(weights['Wy1'],[current_batch_size,1]),[-1, hiddenSize, hiddenSize], name=\"Wy1\")\n    WyY1 = tf.batch_matmul(outputs2,batch_Wy1, name=\"WyY1\")                                                        \nelse:                                                                                                              \n    WyY1 = tf.einsum('ijk,kl->ijl',outputs2,weights['Wy1'])      \n```\n\nHowever, if I use Adam loss, the update gives slightly different results. I didn't try other loss functions, and this is not necessarily a bug.\n"}