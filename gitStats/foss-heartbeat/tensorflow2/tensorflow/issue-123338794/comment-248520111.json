{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/248520111", "html_url": "https://github.com/tensorflow/tensorflow/issues/582#issuecomment-248520111", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/582", "id": 248520111, "node_id": "MDEyOklzc3VlQ29tbWVudDI0ODUyMDExMQ==", "user": {"login": "waichee", "id": 929147, "node_id": "MDQ6VXNlcjkyOTE0Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/929147?v=4", "gravatar_id": "", "url": "https://api.github.com/users/waichee", "html_url": "https://github.com/waichee", "followers_url": "https://api.github.com/users/waichee/followers", "following_url": "https://api.github.com/users/waichee/following{/other_user}", "gists_url": "https://api.github.com/users/waichee/gists{/gist_id}", "starred_url": "https://api.github.com/users/waichee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/waichee/subscriptions", "organizations_url": "https://api.github.com/users/waichee/orgs", "repos_url": "https://api.github.com/users/waichee/repos", "events_url": "https://api.github.com/users/waichee/events{/privacy}", "received_events_url": "https://api.github.com/users/waichee/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-21T06:06:34Z", "updated_at": "2016-09-21T06:06:34Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=606565\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/raingo\">@raingo</a>  I'm on the latest tensorflow_serving master. We are still having issue with TS serving throwing this error when loading large models. we have tried the various approaches mentioned above and they dont seem to work.</p>\n<ul>\n<li>PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python still getting the same error</li>\n<li>the custom wheel package for protobuf specified here <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues</a> This doesnt work because bazel installs the submodules (tensorflow and protofbuf) when building tensorflow_serving from source</li>\n<li><code>use_fast_cpp_protos=true</code> and <code>allow_oversize_protos=true</code> when running bazel as mentioned here <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"143406270\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/serving/issues/24\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/serving/issues/24/hovercard\" href=\"https://github.com/tensorflow/serving/issues/24\">tensorflow/serving#24</a></li>\n</ul>\n<p>Is there any other alternative approaches that doesn't involved manually changing the coded_stream.h file in the bazel cache?</p>", "body_text": "@vrv @raingo  I'm on the latest tensorflow_serving master. We are still having issue with TS serving throwing this error when loading large models. we have tried the various approaches mentioned above and they dont seem to work.\n\nPROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python still getting the same error\nthe custom wheel package for protobuf specified here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues This doesnt work because bazel installs the submodules (tensorflow and protofbuf) when building tensorflow_serving from source\nuse_fast_cpp_protos=true and allow_oversize_protos=true when running bazel as mentioned here tensorflow/serving#24\n\nIs there any other alternative approaches that doesn't involved manually changing the coded_stream.h file in the bazel cache?", "body": "@vrv @raingo  I'm on the latest tensorflow_serving master. We are still having issue with TS serving throwing this error when loading large models. we have tried the various approaches mentioned above and they dont seem to work.\n- PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python still getting the same error\n- the custom wheel package for protobuf specified here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues This doesnt work because bazel installs the submodules (tensorflow and protofbuf) when building tensorflow_serving from source\n- `use_fast_cpp_protos=true` and `allow_oversize_protos=true` when running bazel as mentioned here https://github.com/tensorflow/serving/issues/24\n\nIs there any other alternative approaches that doesn't involved manually changing the coded_stream.h file in the bazel cache?\n"}