{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/288378199", "html_url": "https://github.com/tensorflow/tensorflow/issues/8265#issuecomment-288378199", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8265", "id": 288378199, "node_id": "MDEyOklzc3VlQ29tbWVudDI4ODM3ODE5OQ==", "user": {"login": "BhagyeshVikani", "id": 20687525, "node_id": "MDQ6VXNlcjIwNjg3NTI1", "avatar_url": "https://avatars1.githubusercontent.com/u/20687525?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BhagyeshVikani", "html_url": "https://github.com/BhagyeshVikani", "followers_url": "https://api.github.com/users/BhagyeshVikani/followers", "following_url": "https://api.github.com/users/BhagyeshVikani/following{/other_user}", "gists_url": "https://api.github.com/users/BhagyeshVikani/gists{/gist_id}", "starred_url": "https://api.github.com/users/BhagyeshVikani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BhagyeshVikani/subscriptions", "organizations_url": "https://api.github.com/users/BhagyeshVikani/orgs", "repos_url": "https://api.github.com/users/BhagyeshVikani/repos", "events_url": "https://api.github.com/users/BhagyeshVikani/events{/privacy}", "received_events_url": "https://api.github.com/users/BhagyeshVikani/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-22T12:02:41Z", "updated_at": "2017-03-22T12:02:41Z", "author_association": "NONE", "body_html": "<p>Digging a bit more into the first case <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7119249\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/falaktheoptimist\">@falaktheoptimist</a>  shared above, we found that the memory hogging is happening at two stages - first when computing the <code>image_summary</code> (using session.run) and second by the <code>add_summary</code> function of Filewriter. Below are the profiler outputs showing half of the memory(~285 MB) occupied by <code>sess.run</code> and other half being occupied by <code>file_writer.add_summary</code> call.</p>\n<p>Case-1:</p>\n<pre><code>Filename: test.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7   2394.4 MiB      0.0 MiB   @profile\n     8                             def _write_into_log(images):\n     9   2394.4 MiB      0.0 MiB       with tf.Graph().as_default() as g:\n    10   2398.7 MiB      4.3 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\n    11   2398.8 MiB      0.1 MiB           image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\n    12                             \n    13   2407.3 MiB      8.5 MiB           with tf.Session() as sess:\n    14   2691.9 MiB    284.6 MiB               sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\n\n\nFilename: test.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n    17     89.4 MiB      0.0 MiB   @profile\n    18                             def main():\n    19   2394.2 MiB   2304.8 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\n    20   2691.9 MiB    297.7 MiB       _write_into_log(out)\n    21    387.2 MiB  -2304.7 MiB       out = None\n</code></pre>\n<p>Case-2:</p>\n<pre><code>Filename: test.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7   2394.2 MiB      0.0 MiB   @profile\n     8                             def _write_into_log(images):\n     9   2394.2 MiB      0.0 MiB       path_logdir = os.path.join(\"./MiniExample\")\n    10   2394.2 MiB      0.0 MiB       if not os.path.exists(path_logdir):\n    11                                     os.makedirs(path_logdir)\n    12                             \n    13   2394.2 MiB      0.1 MiB       with tf.Graph().as_default() as g:\n    14   2398.6 MiB      4.4 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\n    15                             \n    16   2398.7 MiB      0.1 MiB           image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\n    17                             \n    18   2409.4 MiB     10.7 MiB           with tf.Session() as sess:\n    19   2409.7 MiB      0.3 MiB               file_writer = tf.summary.FileWriter(path_logdir, g)\n    20   2982.4 MiB    572.7 MiB               sss = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\n    21   3271.2 MiB    288.8 MiB               file_writer.add_summary(sss)\n    22   3271.2 MiB      0.0 MiB               file_writer.close()\n    23   3271.2 MiB      0.0 MiB               file_writer = None\n    24   3271.2 MiB      0.0 MiB               image_summary = None\n    25   2983.0 MiB   -288.2 MiB               sss = None\n\nFilename: test.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n    27     89.3 MiB      0.0 MiB   @profile\n    28                             def main():\n    29   2394.2 MiB   2304.9 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\n    30   2983.0 MiB    588.8 MiB       _write_into_log(out)\n    31    678.3 MiB  -2304.7 MiB       out = None\n</code></pre>", "body_text": "Digging a bit more into the first case @falaktheoptimist  shared above, we found that the memory hogging is happening at two stages - first when computing the image_summary (using session.run) and second by the add_summary function of Filewriter. Below are the profiler outputs showing half of the memory(~285 MB) occupied by sess.run and other half being occupied by file_writer.add_summary call.\nCase-1:\nFilename: test.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7   2394.4 MiB      0.0 MiB   @profile\n     8                             def _write_into_log(images):\n     9   2394.4 MiB      0.0 MiB       with tf.Graph().as_default() as g:\n    10   2398.7 MiB      4.3 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\n    11   2398.8 MiB      0.1 MiB           image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\n    12                             \n    13   2407.3 MiB      8.5 MiB           with tf.Session() as sess:\n    14   2691.9 MiB    284.6 MiB               sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\n\n\nFilename: test.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n    17     89.4 MiB      0.0 MiB   @profile\n    18                             def main():\n    19   2394.2 MiB   2304.8 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\n    20   2691.9 MiB    297.7 MiB       _write_into_log(out)\n    21    387.2 MiB  -2304.7 MiB       out = None\n\nCase-2:\nFilename: test.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     7   2394.2 MiB      0.0 MiB   @profile\n     8                             def _write_into_log(images):\n     9   2394.2 MiB      0.0 MiB       path_logdir = os.path.join(\"./MiniExample\")\n    10   2394.2 MiB      0.0 MiB       if not os.path.exists(path_logdir):\n    11                                     os.makedirs(path_logdir)\n    12                             \n    13   2394.2 MiB      0.1 MiB       with tf.Graph().as_default() as g:\n    14   2398.6 MiB      4.4 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\n    15                             \n    16   2398.7 MiB      0.1 MiB           image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\n    17                             \n    18   2409.4 MiB     10.7 MiB           with tf.Session() as sess:\n    19   2409.7 MiB      0.3 MiB               file_writer = tf.summary.FileWriter(path_logdir, g)\n    20   2982.4 MiB    572.7 MiB               sss = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\n    21   3271.2 MiB    288.8 MiB               file_writer.add_summary(sss)\n    22   3271.2 MiB      0.0 MiB               file_writer.close()\n    23   3271.2 MiB      0.0 MiB               file_writer = None\n    24   3271.2 MiB      0.0 MiB               image_summary = None\n    25   2983.0 MiB   -288.2 MiB               sss = None\n\nFilename: test.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n    27     89.3 MiB      0.0 MiB   @profile\n    28                             def main():\n    29   2394.2 MiB   2304.9 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\n    30   2983.0 MiB    588.8 MiB       _write_into_log(out)\n    31    678.3 MiB  -2304.7 MiB       out = None", "body": "Digging a bit more into the first case @falaktheoptimist  shared above, we found that the memory hogging is happening at two stages - first when computing the `image_summary` (using session.run) and second by the `add_summary` function of Filewriter. Below are the profiler outputs showing half of the memory(~285 MB) occupied by `sess.run` and other half being occupied by `file_writer.add_summary` call.\r\n\r\nCase-1:\r\n```\r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7   2394.4 MiB      0.0 MiB   @profile\r\n     8                             def _write_into_log(images):\r\n     9   2394.4 MiB      0.0 MiB       with tf.Graph().as_default() as g:\r\n    10   2398.7 MiB      4.3 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\r\n    11   2398.8 MiB      0.1 MiB           image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\r\n    12                             \r\n    13   2407.3 MiB      8.5 MiB           with tf.Session() as sess:\r\n    14   2691.9 MiB    284.6 MiB               sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\r\n\r\n\r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    17     89.4 MiB      0.0 MiB   @profile\r\n    18                             def main():\r\n    19   2394.2 MiB   2304.8 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\r\n    20   2691.9 MiB    297.7 MiB       _write_into_log(out)\r\n    21    387.2 MiB  -2304.7 MiB       out = None\r\n```\r\nCase-2:\r\n```     \r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7   2394.2 MiB      0.0 MiB   @profile\r\n     8                             def _write_into_log(images):\r\n     9   2394.2 MiB      0.0 MiB       path_logdir = os.path.join(\"./MiniExample\")\r\n    10   2394.2 MiB      0.0 MiB       if not os.path.exists(path_logdir):\r\n    11                                     os.makedirs(path_logdir)\r\n    12                             \r\n    13   2394.2 MiB      0.1 MiB       with tf.Graph().as_default() as g:\r\n    14   2398.6 MiB      4.4 MiB           image = tf.placeholder(tf.float32, shape = [None, None, None, 3])\r\n    15                             \r\n    16   2398.7 MiB      0.1 MiB           image_summary = tf.summary.image(name = \"Images\", tensor = image, max_outputs = 2000)\r\n    17                             \r\n    18   2409.4 MiB     10.7 MiB           with tf.Session() as sess:\r\n    19   2409.7 MiB      0.3 MiB               file_writer = tf.summary.FileWriter(path_logdir, g)\r\n    20   2982.4 MiB    572.7 MiB               sss = sess.run(image_summary, feed_dict = {image : np.concatenate(images, axis = 0)})\r\n    21   3271.2 MiB    288.8 MiB               file_writer.add_summary(sss)\r\n    22   3271.2 MiB      0.0 MiB               file_writer.close()\r\n    23   3271.2 MiB      0.0 MiB               file_writer = None\r\n    24   3271.2 MiB      0.0 MiB               image_summary = None\r\n    25   2983.0 MiB   -288.2 MiB               sss = None\r\n\r\nFilename: test.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n    27     89.3 MiB      0.0 MiB   @profile\r\n    28                             def main():\r\n    29   2394.2 MiB   2304.9 MiB       out = [np.random.random((1, 224, 224, 3)) for i in range(2000)]\r\n    30   2983.0 MiB    588.8 MiB       _write_into_log(out)\r\n    31    678.3 MiB  -2304.7 MiB       out = None\r\n```\r\n\r\n"}