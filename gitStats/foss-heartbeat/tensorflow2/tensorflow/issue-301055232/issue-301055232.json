{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17327", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17327/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17327/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17327/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17327", "id": 301055232, "node_id": "MDU6SXNzdWUzMDEwNTUyMzI=", "number": 17327, "title": "tf.data.Dataset.padded_batch() should support padding to nearest N bytes", "user": {"login": "soCzech", "id": 7052917, "node_id": "MDQ6VXNlcjcwNTI5MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7052917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soCzech", "html_url": "https://github.com/soCzech", "followers_url": "https://api.github.com/users/soCzech/followers", "following_url": "https://api.github.com/users/soCzech/following{/other_user}", "gists_url": "https://api.github.com/users/soCzech/gists{/gist_id}", "starred_url": "https://api.github.com/users/soCzech/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soCzech/subscriptions", "organizations_url": "https://api.github.com/users/soCzech/orgs", "repos_url": "https://api.github.com/users/soCzech/repos", "events_url": "https://api.github.com/users/soCzech/events{/privacy}", "received_events_url": "https://api.github.com/users/soCzech/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-02-28T14:57:12Z", "updated_at": "2018-04-18T19:48:13Z", "closed_at": "2018-04-18T19:48:13Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>TF 1.5<br>\nPython 3.6.4 (Anaconda)</p>\n<h3>Problem description</h3>\n<p>If using <code>tf.data.Dataset.padded_batch()</code> on input with variable size, it looks like there is significant overhead first time a batch with examples of size X is used on GPU. If batch with examples of size X is used second time, the execution is approx. 4x faster. (I suppose it is due to a way tensorflow handles data.)</p>\n<p>In case the input size varies a lot, this slows the computation enormously. It could be solved by padding the batch to nearest N bytes so that there is only limited number of sizes pushed onto GPU and overhead thus becomes negligible.</p>\n<h3>Solution</h3>\n<p>It would be great if <code>padded_batch</code> supported argument <code>pad_to_nearest_bytes</code> or/and <code>allowed_batch_sizes</code> for enumeration of possible sizes.</p>\n<p>Proof:</p>\n<pre><code>TIME    BATCH_SIZE (2nd dimension = the data size)\n0.15205 20480\n0.06180 20480\n0.80608 147456\n0.24141 147456\n0.74360 135168\n0.21659 135168\n0.58724 98304\n0.16206 98304\n0.05387 20480\n0.05694 20480\n0.53993 90112\n0.15452 90112\n0.23547 147456\n0.23576 147456\n</code></pre>", "body_text": "System information\nTF 1.5\nPython 3.6.4 (Anaconda)\nProblem description\nIf using tf.data.Dataset.padded_batch() on input with variable size, it looks like there is significant overhead first time a batch with examples of size X is used on GPU. If batch with examples of size X is used second time, the execution is approx. 4x faster. (I suppose it is due to a way tensorflow handles data.)\nIn case the input size varies a lot, this slows the computation enormously. It could be solved by padding the batch to nearest N bytes so that there is only limited number of sizes pushed onto GPU and overhead thus becomes negligible.\nSolution\nIt would be great if padded_batch supported argument pad_to_nearest_bytes or/and allowed_batch_sizes for enumeration of possible sizes.\nProof:\nTIME    BATCH_SIZE (2nd dimension = the data size)\n0.15205 20480\n0.06180 20480\n0.80608 147456\n0.24141 147456\n0.74360 135168\n0.21659 135168\n0.58724 98304\n0.16206 98304\n0.05387 20480\n0.05694 20480\n0.53993 90112\n0.15452 90112\n0.23547 147456\n0.23576 147456", "body": "### System information\r\nTF 1.5\r\nPython 3.6.4 (Anaconda)\r\n\r\n### Problem description\r\nIf using `tf.data.Dataset.padded_batch()` on input with variable size, it looks like there is significant overhead first time a batch with examples of size X is used on GPU. If batch with examples of size X is used second time, the execution is approx. 4x faster. (I suppose it is due to a way tensorflow handles data.)\r\n\r\nIn case the input size varies a lot, this slows the computation enormously. It could be solved by padding the batch to nearest N bytes so that there is only limited number of sizes pushed onto GPU and overhead thus becomes negligible.\r\n\r\n### Solution\r\nIt would be great if `padded_batch` supported argument `pad_to_nearest_bytes` or/and `allowed_batch_sizes` for enumeration of possible sizes.\r\n\r\nProof:\r\n```\r\nTIME    BATCH_SIZE (2nd dimension = the data size)\r\n0.15205 20480\r\n0.06180 20480\r\n0.80608 147456\r\n0.24141 147456\r\n0.74360 135168\r\n0.21659 135168\r\n0.58724 98304\r\n0.16206 98304\r\n0.05387 20480\r\n0.05694 20480\r\n0.53993 90112\r\n0.15452 90112\r\n0.23547 147456\r\n0.23576 147456\r\n```"}