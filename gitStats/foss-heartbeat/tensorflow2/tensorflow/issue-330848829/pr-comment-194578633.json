{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194578633", "pull_request_review_id": 127778175, "id": 194578633, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDU3ODYzMw==", "diff_hunk": "@@ -141,7 +164,15 @@ def py3string(inp):\n     to_string = py2string\n   else:\n     to_string = py3string\n-\n+  is_calib_graph = False\n+  for n in calibration_graph_def.node:\n+    if n.op == \"TRTEngineOp\":\n+      is_calib_graph = len(n.attr[\"calibration_data\"].s) == 0\n+      break\n+  if not is_calib_graph:\n+    tf_logging.error(\n+        \"Not a calib graph. Doesn't seem to contain any calibration nodes.\")", "path": "tensorflow/contrib/tensorrt/python/trt_convert.py", "position": null, "original_position": 75, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "d5aaf3fa4a4851abc6a0e5600474f7674f1adb93", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "body": "Do we foresee a scenario where graph is composed of int8 and FP32 nodes? Right now all nodes are of the same precision and it is not possible to mix different precision engines.", "created_at": "2018-06-11T23:36:51Z", "updated_at": "2018-06-21T22:23:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194578633", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194578633"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194578633"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>Do we foresee a scenario where graph is composed of int8 and FP32 nodes? Right now all nodes are of the same precision and it is not possible to mix different precision engines.</p>", "body_text": "Do we foresee a scenario where graph is composed of int8 and FP32 nodes? Right now all nodes are of the same precision and it is not possible to mix different precision engines.", "in_reply_to_id": 194552209}