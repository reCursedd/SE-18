{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194911967", "pull_request_review_id": 128071489, "id": 194911967, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDkxMTk2Nw==", "diff_hunk": "@@ -318,96 +119,503 @@ tensorflow::Status BuildNodeMap(\n }\n \n }  // namespace\n+// Function to get calibration from ResourceMgr and put them into nodedef.\n tensorflow::Status ConvertCalibGraphToInferGraph(\n     const tensorflow::GraphDef& graph_def, tensorflow::GraphDef* infer_graph) {\n   VLOG(0) << \"Starting Calib Conversion\";\n-  tensorflow::Graph graph(tensorflow::OpRegistry::Global());\n-  TF_RETURN_IF_ERROR(tensorflow::ConvertGraphDefToGraph(\n-      tensorflow::GraphConstructorOptions(), graph_def, &graph));\n-  //  get calib nodes\n-  std::vector<tensorflow::Node*> calib_nodes;\n-  std::vector<tensorflow::Node*> topo_order;\n-  tensorflow::GetPostOrder(graph, &topo_order);\n-  for (auto rit = topo_order.rbegin(); rit != topo_order.rend(); ++rit) {\n-    auto node = *rit;\n-    if (node->type_string() == \"TRTCalibOp\") {\n-      VLOG(1) << \"Found Calib Node \" << node->name();\n-      calib_nodes.push_back(node);\n+  infer_graph->CopyFrom(graph_def);\n+  auto trt_rm = tensorflow::tensorrt::TRTResourceManager::instance();\n+  auto calib_rm = trt_rm->getManager(\"TRTCalibration\");\n+  int num_nodes = infer_graph->node_size();\n+  for (int i = 0; i < num_nodes; ++i) {\n+    auto n = infer_graph->mutable_node(i);\n+    if (n->op() == \"TRTEngineOp\") {\n+      VLOG(1) << \"Processing \" << n->name();\n+      string container_name = n->attr().at(\"segment_funcdef_name\").s();\n+      tensorflow::tensorrt::TRTCalibrationResource* cres = nullptr;\n+      auto status = calib_rm->Lookup(container_name, \"Calibrator\", &cres);\n+      if (!status.ok()) {\n+        LOG(ERROR) << \"Could not get Calibration information. Did you run with \"\n+                      \"calibration data?\";\n+        return tensorflow::errors::FailedPrecondition(\n+            \"Need to run graph with calibration data first!\");\n+      }\n+      if (cres->calibrator_) {\n+        cres->calibrator_->setDone();\n+        cres->thr_->join();\n+        auto calibration_table =\n+            cres->calibrator_->getCalibrationTableAsString();\n+        if (!calibration_table.size()) {\n+          LOG(ERROR) << \"Calibration table is empty\";\n+          return tensorflow::errors::Unknown(\n+              \"Calibration table is missing. This shouldn't have happened!\");\n+        }\n+        n->mutable_attr()->at(\"calibration_data\").set_s(calibration_table);\n+      } else {\n+        LOG(ERROR) << \"Can't get TRTCalibrator from resource manager!\";\n+        return tensorflow::errors::Unknown(\n+            \"Can't get TRTCalibrator from resource manager!\");\n+      }\n+      cres->Unref();\n     }\n   }\n-  VLOG(0) << \"Num Calib nodes in graph= \" << calib_nodes.size();\n-  if (calib_nodes.size() == 0)\n-    return tensorflow::errors::FailedPrecondition(\n-        \"Graph doesn't contain any calibration nodes!.\"\n-        \" Please generate calibration graph and run calibration first\");\n-  for (auto n : calib_nodes) {\n-    TF_RETURN_IF_ERROR(\n-        tensorrt::convert::ConvertCalibrationNodeToEngineNode(graph, n));\n-  }\n-  graph.ToGraphDef(infer_graph);\n   return tensorflow::Status::OK();\n }\n \n+// Entry function from Python.\n tensorflow::Status ConvertGraphDefToTensorRT(\n     const tensorflow::GraphDef& graph_def,\n     const std::vector<string>& output_names, size_t max_batch_size,\n     size_t max_workspace_size_bytes, tensorflow::GraphDef* new_graph_def,\n-    int precision_mode = FP32MODE, int minimum_segment_size = 3) {\n+    int precision_mode, int minimum_segment_size, bool is_dyn_op,\n+    int max_cached_engines, std::vector<int> cached_engine_batches) {\n   // optimization pass\n   tensorflow::grappler::GrapplerItem item;\n   item.fetch = output_names;\n   item.graph = graph_def;\n-\n+  // grappler requires a virtual cluster with a proper GPU device\n+  // in order to calculate flops>0 or fails with FATAL\n+  // We add numbers from a Pascal card here to have flops>0\n   tensorflow::DeviceProperties device_properties;\n   device_properties.set_type(\"GPU\");\n   device_properties.mutable_environment()->insert({\"architecture\", \"6\"});\n-  tensorflow::grappler::Cluster* cluster =\n-      new tensorflow::grappler::VirtualCluster({{\"/GPU:0\", device_properties}});\n+  device_properties.set_num_cores(3584);\n+  device_properties.set_frequency(1531);\n+  std::unique_ptr<tensorflow::grappler::Cluster> cluster(\n+      new tensorflow::grappler::VirtualCluster(\n+          {{\"/GPU:0\", device_properties}}));\n \n   // single machine\n   int num_cpu_cores = tensorflow::grappler::GetNumAvailableLogicalCPUCores();\n   int num_gpus = tensorflow::grappler::GetNumAvailableGPUs();\n   VLOG(2) << \"cpu_cores: \" << num_cpu_cores;\n   VLOG(2) << \"gpus: \" << num_gpus;\n   tensorflow::RewriterConfig rw_cfg;\n+  // use only const folding and layout for the time being since new optimizers\n+  // break the graph for us\n+  rw_cfg.add_optimizers(\"constfold\");\n+  rw_cfg.add_optimizers(\"layout\");\n+  rw_cfg.set_meta_optimizer_iterations(tensorflow::RewriterConfig::ONE);\n   tensorflow::grappler::MetaOptimizer meta_opt(nullptr, rw_cfg);\n   tensorflow::GraphDef gdef;\n-  TF_RETURN_IF_ERROR(meta_opt.Optimize(cluster, item, &gdef));\n+  TF_RETURN_IF_ERROR(meta_opt.Optimize(cluster.get(), item, &gdef));\n   item.graph = gdef;\n \n   // AJ refactoring shape inference through grappler/GraphProperties.\n   tensorflow::grappler::GraphProperties static_graph_properties(item);\n   TF_RETURN_IF_ERROR(static_graph_properties.InferStatically(true));\n   // Build full graph\n-\n-  return ConvertAfterShapes(gdef, output_names, max_batch_size,\n-                            max_workspace_size_bytes, new_graph_def,\n-                            precision_mode, minimum_segment_size,\n-                            static_graph_properties, nullptr);\n+  ConversionParams cp;\n+  cp.input_graph_def = &gdef;\n+  cp.output_names = &output_names;\n+  cp.max_batch_size = max_batch_size;\n+  cp.output_graph_def = new_graph_def;\n+  cp.precision_mode = precision_mode;\n+  cp.is_dyn_op = is_dyn_op;\n+  cp.max_cached_engines = max_cached_engines;\n+  cp.cached_engine_batches = cached_engine_batches;\n+  cp.minimum_segment_size = minimum_segment_size;\n+  cp.graph_properties = &static_graph_properties;\n+  cp.max_workspace_size_bytes = max_workspace_size_bytes;\n+  if (VLOG_IS_ON(5)) {\n+    std::fstream f;\n+    f.open(\"TRTConversionInput.pb\",\n+           std::fstream::out | std::fstream::binary | std::fstream::trunc);\n+    f << gdef.SerializeAsString();\n+    f.close();\n+  }\n+  return ConvertAfterShapes(cp);\n }\n \n-tensorflow::Status ConvertAfterShapes(\n-    const tensorflow::GraphDef& gdef, const std::vector<string>& output_names,\n-    size_t max_batch_size, size_t max_workspace_size_bytes,\n-    tensorflow::GraphDef* new_graph_def, int precision_mode,\n-    int minimum_segment_size,\n+// Function to get subsegment information structure.\n+EngineInfo GetEngineInfo(\n+    const tensorflow::Graph* g,\n     const tensorflow::grappler::GraphProperties& graph_properties,\n-    const tensorflow::grappler::Cluster* cluster) {\n+    const std::set<string>& segment_nodes,\n+    const std::unordered_map<string, tensorflow::Node*>& node_map,\n+    const std::vector<tensorflow::Node*>& topological_order) {\n+  std::vector<int> subgraph_node_ids;\n+  EngineInfo info;\n+  std::set<string> segment_devices;\n+  int input_port = 0;\n+  int output_port = 0;\n+  std::unordered_map<string, int> created_edges;\n+  for (auto it = topological_order.rbegin(); it != topological_order.rend();\n+       ++it) {\n+    auto node_name = (*it)->name();\n+\n+    if (segment_nodes.count(node_name) == 0) continue;\n+    auto node = node_map.at(node_name);\n+    auto node_device = node->requested_device();\n+    if (!node_device.empty()) {\n+      segment_devices.insert(node_device);\n+    }\n+    int node_id = node->id();\n+    subgraph_node_ids.push_back(node_id);\n+    for (const auto edge : node->in_edges()) {\n+      auto input_node = edge->src();\n+      if (segment_nodes.count(input_node->name()) == 0) {\n+        if (input_node->type_string() ==\n+            \"Const\") {  // Add constant input into segment\n+          subgraph_node_ids.push_back(input_node->id());\n+        } else if (!edge->IsControlEdge() && !input_node->IsSource()) {\n+          string s(input_node->name());\n+          StrAppend(&s, \":\", edge->src_output());\n+          VLOG(1) << \"Input edge = \" << s;\n+          int port = input_port;\n+          if (created_edges.count(s)) {\n+            port = created_edges.at(s);\n+          } else {\n+            created_edges.insert({s, port});\n+            input_port++;\n+          }\n+          EngineConnections ec(input_node->name(), input_node->id(),\n+                               edge->src_output(), node_name, node_id,\n+                               edge->dst_input(), true, port);\n+          ec.connection_type = input_node->output_type(edge->src_output());\n+\n+          info.connections.emplace_back(std::move(ec));\n+        }\n+      }\n+    }\n+    for (const auto edge : node->out_edges()) {\n+      auto output_node = edge->dst();\n+      if (segment_nodes.count(output_node->name()) == 0 &&\n+          !edge->IsControlEdge() && !output_node->IsSink()) {\n+        string s(node_name);\n+        StrAppend(&s, \":\", edge->src_output());\n+        VLOG(1) << \"Output edge = \" << s;\n+        int port = output_port;\n+        if (created_edges.count(s)) {\n+          port = created_edges.at(s);\n+        } else {\n+          created_edges.insert({s, port});\n+          output_port++;\n+        }\n+        info.connections.emplace_back(output_node->name(), output_node->id(),\n+                                      edge->dst_input(), node_name, node_id,\n+                                      edge->src_output(), false, port);\n+      }\n+    }\n+  }\n+\n+  ConvertSegmentToGraphDef(g, graph_properties, subgraph_node_ids,\n+                           &info.connections, &info.segment_graph_def,\n+                           &info.engine_name);\n+  info.engine_type = EngineInfo::EngineType::TRTStatic;\n+  if (segment_devices.size() > 1) {\n+    LOG(WARNING) << \"Detected multiple(\" << segment_devices.size()\n+                 << \") devices for the segment. Picking first one to continue \"\n+                 << \"but this shouldn't have happened\";\n+    info.device = *segment_devices.begin();\n+  }\n+  return info;\n+}\n+\n+// Function to insert a TRT node into the graph.\n+tensorflow::Status CreateTRTNode(tensorflow::Graph* graph,\n+                                 const std::vector<EngineInfo>& infos, int pos,\n+                                 tensorflow::NodeDef* trtNode,\n+                                 nvinfer1::IGpuAllocator* alloc,\n+                                 int max_batch_size) {\n+  auto& info = infos.at(pos);\n+  std::vector<tensorflow::TensorShapeProto> out_shapes;\n+  std::vector<tensorflow::TensorShapeProto> input_shapes;\n+  std::vector<tensorflow::PartialTensorShape> shapes;\n+  std::vector<tensorflow::NodeDefBuilder::NodeOut> inputs;\n+  std::vector<tensorflow::DataType> out_types;\n+  VLOG(1) << \"Processing \" << info.engine_name;\n+  for (const auto conn : info.connections) {\n+    if (!conn.is_input_edge) {  // output edge\n+      tensorflow::TensorShapeProto out_shape;\n+      conn.inside_shape.AsProto(\n+          &out_shape);  // shape of the output node inside segment\n+      if (out_shapes.size() <= conn.port_number) {\n+        out_shapes.resize(conn.port_number + 1);\n+        out_types.resize(conn.port_number + 1);\n+      }\n+      out_shapes.at(conn.port_number) = out_shape;\n+      out_types.at(conn.port_number) = conn.connection_type;\n+      continue;\n+    } else {  // input edge\n+      tensorflow::TensorShapeProto in_shape;\n+      conn.outside_shape.AsProto(&in_shape);\n+\n+      if (input_shapes.size() <= conn.port_number) {\n+        input_shapes.resize(conn.port_number + 1);\n+        shapes.resize(conn.port_number + 1);\n+      }\n+      input_shapes.at(conn.port_number) = in_shape;\n+      shapes.at(conn.port_number) = conn.outside_shape;\n+    }\n+    string input_node = conn.outside_node_name;", "path": "tensorflow/contrib/tensorrt/convert/convert_graph.cc", "position": null, "original_position": 562, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "565640eae327b092edf43613f77ba5ab0747d20d", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "Please add const for following local variable as well so we know it will not be changed.", "created_at": "2018-06-12T22:49:48Z", "updated_at": "2018-06-21T22:23:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194911967", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194911967"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194911967"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>Please add const for following local variable as well so we know it will not be changed.</p>", "body_text": "Please add const for following local variable as well so we know it will not be changed.", "in_reply_to_id": 194911870}