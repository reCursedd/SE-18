{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/195232927", "pull_request_review_id": 128560914, "id": 195232927, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTIzMjkyNw==", "diff_hunk": "@@ -117,51 +345,246 @@ void TRTEngineOp::Compute(OpKernelContext* context) {\n       std::vector<int> trt_shape(dims.nbDims + 1);\n       trt_shape[0] = num_batch;\n       for (int j = 0; j < dims.nbDims; j++) trt_shape[j + 1] = dims.d[j];\n-      OP_REQUIRES_OK(context,\n-                     TensorShapeUtils::MakeShape(\n-                         trt_shape.data(), trt_shape.size(), &output_shape));\n+      OP_REQUIRES_OK(\n+          ctx, TensorShapeUtils::MakeShape(trt_shape.data(), trt_shape.size(),\n+                                           &output_shape));\n     } else {\n-      LOG(FATAL) << \"output node not found, at \" << output_nodes_[i];\n-      break;\n+      LOG(ERROR) << \"output node not found, at \" << output_name;\n+      ctx->SetStatus(tensorflow::errors::Internal(\"output \" + output_name +\n+                                                  \" but couldn't be found!\"));\n+      return;\n+    }\n+    auto status = ctx->allocate_output(i, output_shape, &output_tensor);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Allocating output failed with \" << status;\n+      ctx->SetStatus(status);\n+      return;\n     }\n-\n-    OP_REQUIRES_OK(context,\n-                   context->allocate_output(i, output_shape, &output_tensor));\n     auto dtype = trt_engine_ptr_->getBindingDataType(binding_index);\n     switch (dtype) {\n       case nvinfer1::DataType::kFLOAT:\n         buffers[binding_index] =\n             reinterpret_cast<void*>(output_tensor->flat<float>().data());\n         break;\n       case nvinfer1::DataType::kHALF:\n-        LOG(FATAL) << \"half size is not supported yet!\";\n+        LOG(ERROR) << \"half size is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Half outputs are not supported!\"));\n+        return;\n         break;\n       case nvinfer1::DataType::kINT8:\n-        LOG(FATAL) << \"int8 is not supported yet!\";\n+        LOG(ERROR) << \"int8 is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"INT8 outputs are not supported!\"));\n+        return;\n         break;\n       default:\n-        LOG(FATAL) << \"Unknown data type: \" << int(dtype);\n+        LOG(ERROR) << \"Unknown TRT data type: \" << int(dtype);\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Unsupported output data type! \" + int(dtype)));\n+        return;\n         break;\n     }\n   }\n   // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n   const cudaStream_t* stream = CHECK_NOTNULL(\n-      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\n+      reinterpret_cast<const cudaStream_t*>(ctx->op_device_context()\n                                                 ->stream()\n                                                 ->implementation()\n                                                 ->CudaStreamMemberHack()));\n \n   // TODO(jie): trt enqueue does not return error\n-  auto ret = trt_execution_context_ptr_->enqueue(num_batch, &buffers[0],\n-                                                 *stream, nullptr);\n+  auto trt_execution_context_ptr = engine_ctx_pair.second;\n+  auto ret = trt_execution_context_ptr->enqueue(num_batch, &buffers[0], *stream,\n+                                                nullptr);\n   VLOG(2) << \"enqueue returns: \" << ret;\n   // sync should be done by TF.\n }\n+\n TRTEngineOp::~TRTEngineOp() {\n   // Order matters!\n-  trt_execution_context_ptr_.reset();\n-  trt_engine_ptr_.reset();\n-  allocator_.reset();\n+  for (auto eng : engine_map_) {\n+    eng.second.first.reset();\n+    eng.second.second.reset();\n+  }\n+  for (auto alloc : allocators_) alloc.second.reset();\n+}\n+\n+TRTEngineOp::EngineCtxPair TRTEngineOp::GetEngine(int batch_size,\n+                                                  OpKernelContext* ctx,\n+                                                  bool ignore_dim_change) {\n+  // TODO(sami): This method needs to be re-written to use resource manager and\n+  // with LRU mechanism option.\n+  tensorflow::mutex_lock lock(engine_mutex_);\n+  if (static_engine_) {", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 466, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "ee169363b5583ae7e16461aaf1588d6a0a9aa710", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "body": "There are cases where initialization of the engine is taking in the order or tens of minutes. It is not acceptable to ask every instantiation of an inference network wait for tens of minutes before it can process any input. For such networks, portability is not an issue, there is only one target device. Portability and maintenance argument applies to anything in TensorFlow. If there are backward incompatible changes, user needs to update the graph. Also the flag is there for a reason. There are users which want dynamic construction and there are users which want static construction. Each has pros and cons and users are free to choose whichever is suitable for them. We want users to be able to choose. Static engine is backwards compatible behavior. We may switch the default to dynamic engine but I would prefer doing that gradually, after users have a chance to try and see which one is most suitable for them. In short static construction is a needed feature.", "created_at": "2018-06-13T20:55:21Z", "updated_at": "2018-06-21T22:23:15Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r195232927", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/195232927"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r195232927"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>There are cases where initialization of the engine is taking in the order or tens of minutes. It is not acceptable to ask every instantiation of an inference network wait for tens of minutes before it can process any input. For such networks, portability is not an issue, there is only one target device. Portability and maintenance argument applies to anything in TensorFlow. If there are backward incompatible changes, user needs to update the graph. Also the flag is there for a reason. There are users which want dynamic construction and there are users which want static construction. Each has pros and cons and users are free to choose whichever is suitable for them. We want users to be able to choose. Static engine is backwards compatible behavior. We may switch the default to dynamic engine but I would prefer doing that gradually, after users have a chance to try and see which one is most suitable for them. In short static construction is a needed feature.</p>", "body_text": "There are cases where initialization of the engine is taking in the order or tens of minutes. It is not acceptable to ask every instantiation of an inference network wait for tens of minutes before it can process any input. For such networks, portability is not an issue, there is only one target device. Portability and maintenance argument applies to anything in TensorFlow. If there are backward incompatible changes, user needs to update the graph. Also the flag is there for a reason. There are users which want dynamic construction and there are users which want static construction. Each has pros and cons and users are free to choose whichever is suitable for them. We want users to be able to choose. Static engine is backwards compatible behavior. We may switch the default to dynamic engine but I would prefer doing that gradually, after users have a chance to try and see which one is most suitable for them. In short static construction is a needed feature.", "in_reply_to_id": 194886209}