{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194588063", "pull_request_review_id": 127788657, "id": 194588063, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDU4ODA2Mw==", "diff_hunk": "@@ -30,85 +37,300 @@ using IRuntime = nvinfer1::IRuntime;\n using Dims = nvinfer1::Dims;\n \n namespace tensorrt {\n+using tensorflow::strings::StrAppend;\n+using tensorflow::strings::StrCat;\n+class AsyncHelper : public tensorflow::core::RefCounted {\n+ public:\n+  AsyncHelper(tensorflow::AsyncOpKernel::DoneCallback done) { done_ = done; }\n+  ~AsyncHelper() override { done_(); }\n \n-TRTEngineOp::TRTEngineOp(OpKernelConstruction* context) : OpKernel(context) {\n+ private:\n+  tensorflow::AsyncOpKernel::DoneCallback done_;\n+};\n+\n+#define TYPECASE(dt, X, Y)                                                \\\n+  case dt: {                                                              \\\n+    return (void*)X->flat<tensorflow::EnumToDataType<dt>::Type>().data(); \\\n+  }\n+\n+void* GetTensorAddress(const Tensor* tensor_ptr) {\n+  auto tensor_type = tensor_ptr->dtype();\n+  switch (tensor_type) {\n+    TYPECASE(tensorflow::DT_FLOAT, tensor_ptr, dest_ptr);\n+    TYPECASE(tensorflow::DT_HALF, tensor_ptr, dest_ptr);\n+    TYPECASE(tensorflow::DT_INT8, tensor_ptr, dest_ptr);\n+    default: {\n+      LOG(FATAL) << \"Unsupported Data type \"\n+                 << tensorflow::DataTypeString(tensor_type);\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+tensorflow::Status TRTEngineOp::ConstructFunctionHandle(OpKernelContext* ctx) {\n+  VLOG(1) << \"Constructing function handle\";\n+  auto lib = ctx->function_library();\n+  if (lib == nullptr) {\n+    return tensorflow::errors::Internal(\"Context function library is null\");\n+  }\n+  auto fdef = lib->GetFunctionLibraryDefinition()->Find(funcdef_name_);\n+  if (fdef == nullptr) {\n+    return tensorflow::errors::Internal(\n+        StrCat(\"Native FunctionDef \", funcdef_name_,\n+               \" can't be found in function library\"));\n+  }\n+  tensorflow::FunctionLibraryRuntime::InstantiateOptions inst_ops;\n+  inst_ops.overlay_lib = nullptr;\n+  inst_ops.state_handle = \"\";\n+  inst_ops.target = ctx->device()->name();\n+  native_func_ = 0;\n+  auto status = lib->Instantiate(funcdef_name_, AttrSlice(&fdef->attr()),\n+                                 inst_ops, &native_func_);\n+  if (!status.ok()) {\n+    LOG(ERROR) << \" Instantiating native function \" << funcdef_name_\n+               << \" failed!\";\n+  }\n+  return status;\n+}\n+\n+TRTEngineOp::TRTEngineOp(OpKernelConstruction* context)\n+    : AsyncOpKernel(context) {\n   // read serialized_engine\n   OP_REQUIRES_OK(context,\n-                 context->GetAttr(\"serialized_engine\", &serialized_engine_));\n+                 context->GetAttr(\"serialized_segment\", &serialized_segment_));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"workspace_size_bytes\", &workspace_size_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"static_engine\", &static_engine));\n+  if (!static_engine) {\n+    if (!segment_graph_.ParseFromString(serialized_segment_)) {\n+      LOG(ERROR) << \"Parsing segment graph failed!\";\n+      context->SetStatus(tensorflow::errors::InvalidArgument(\n+          \"Failed to parse segment graphdef!\"));\n+      return;\n+    }\n+    serialized_segment_.resize(0);\n+  }\n \n-  // register input output node name in trt_sub_graph\n-  OP_REQUIRES_OK(context, context->GetAttr(\"input_nodes\", &input_nodes_));\n-  OP_REQUIRES_OK(context, context->GetAttr(\"output_nodes\", &output_nodes_));\n+  string precision_string;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"precision_mode\", &precision_string));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"calibration_data\", &calibration_data_));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"segment_funcdef_name\", &funcdef_name_));\n+  if (precision_string == \"FP32\") {\n+    precision_mode = tensorflow::tensorrt::convert::FP32MODE;\n+  } else if (precision_string == \"FP16\") {\n+    precision_mode = tensorflow::tensorrt::convert::FP16MODE;\n+  } else if (precision_string == \"INT8\") {\n+    precision_mode = tensorflow::tensorrt::convert::INT8MODE;\n+  }\n+  calibration_mode =\n+      precision_mode == tensorflow::tensorrt::convert::INT8MODE &&\n+      calibration_data_.size() == 0;\n+  if (calibration_data_.size()) {\n+    calibrator_.reset(new TRTInt8Calibrator(calibration_data_));\n+    calibration_data_.resize(0);\n+  }\n+  native_func_ = tensorflow::kInvalidHandle;\n+  OP_REQUIRES_OK(context, context->GetAttr(\"max_cached_engines_count\",\n+                                           &max_cached_engines));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"fixed_input_size\", &fixed_input_size));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"cached_engine_batches\",\n+                                           &cached_engine_batches));\n+  std::sort(cached_engine_batches.begin(), cached_engine_batches.end());\n+  if (VLOG_IS_ON(1)) {\n+    string s(\"Engine Batches= \");\n+    for (auto i : cached_engine_batches) {\n+      StrAppend(&s, i, \" \");\n+    }\n+    VLOG(1) << s;\n+  }\n }\n \n-void TRTEngineOp::Compute(OpKernelContext* context) {\n-  // TODO(samikama) runtime should be taken from a resourcemanager as well.\n-  // Only engine should be in the op and context and runtime should be taken\n-  // from resourcemanager\n+void TRTEngineOp::ExecuteNativeSegment(tensorflow::OpKernelContext* ctx,\n+                                       AsyncHelper* ah) {\n+  if (!calibration_mode) {\n+    VLOG(1) << \"Executing native engine\";\n+  }\n+  std::vector<Tensor> inputs;\n+  std::vector<Tensor>* outputs = new std::vector<Tensor>();\n+  if (native_func_ == tensorflow::kInvalidHandle) {\n+    auto status = ConstructFunctionHandle(ctx);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Couldn't construct function handle \" << funcdef_name_;\n+      ctx->SetStatus(status);\n+      return;\n+    }\n+  }\n+  auto lib = ctx->function_library();\n+  tensorflow::FunctionLibraryRuntime::Options opts;\n+  opts.step_id = ctx->step_id();\n+  opts.rendezvous = ctx->rendezvous();\n+  opts.cancellation_manager = ctx->cancellation_manager();\n+  opts.runner = ctx->runner();\n+  for (int i = 0; i < ctx->num_inputs(); i++) {\n+    inputs.push_back(ctx->input(i));\n+  }\n+  ah->Ref();  // Increment count for calculating native graph\n+  VLOG(1) << \"Executing native segment \" << name();\n+  lib->Run(opts, native_func_, inputs, outputs,\n+           [ctx, outputs, ah](const tensorflow::Status& s) {\n+             tensorflow::core::ScopedUnref SC(ah);\n+             VLOG(1) << \"Native Segment completed\";\n+             if (!s.ok()) {\n+               ctx->SetStatus(s);\n+               return;\n+             }\n+             for (size_t t = 0; t < outputs->size(); ++t) {\n+               ctx->set_output(t, outputs->at(t));\n+             }\n+             delete outputs;\n+             return;\n+           });\n+  return;\n+}\n \n-  if (!trt_execution_context_ptr_) {\n-    IRuntime* infer = nvinfer1::createInferRuntime(logger);\n-#if NV_TENSORRT_MAJOR > 3\n-    auto device = context->device();\n-    auto dev_allocator =\n-        device->GetAllocator(tensorflow::AllocatorAttributes());\n-    if (!dev_allocator) {\n-      LOG(FATAL) << \"Can't find device allocator for gpu device \"\n-                 << device->name();\n-    }\n-    allocator_ = std::make_shared<TRTDeviceAllocator>(dev_allocator);\n-    infer->setGpuAllocator(allocator_.get());\n-#endif\n-    trt_engine_ptr_.reset(infer->deserializeCudaEngine(\n-        serialized_engine_.c_str(), serialized_engine_.size(),\n-        PluginFactoryTensorRT::GetInstance()));\n-    trt_execution_context_ptr_.reset(trt_engine_ptr_->createExecutionContext());\n-    // Runtime is safe to delete after engine creation\n-    infer->destroy();\n-    serialized_engine_.clear();\n-  }\n-  int num_binding = context->num_inputs() + context->num_outputs();\n+void TRTEngineOp::ComputeAsync(tensorflow::OpKernelContext* ctx,\n+                               tensorflow::AsyncOpKernel::DoneCallback done) {\n+  auto ah = new AsyncHelper(done);\n+  tensorflow::core::ScopedUnref SC(ah);\n+  if (calibration_mode) {\n+    auto TRT_RM = tensorflow::tensorrt::TRTResourceManager::instance();\n+    auto res_mgr = TRT_RM->getManager(\"TRTCalibration\");\n+    tensorflow::tensorrt::TRTCalibrationResource* calib_res = nullptr;\n+    auto status = res_mgr->LookupOrCreate(\n+        funcdef_name_, \"Calibrator\", &calib_res,\n+        {[ctx, this](tensorflow::tensorrt::TRTCalibrationResource** cr)\n+             -> tensorflow::Status {\n+          return this->AllocateCalibrationResources(ctx, cr);\n+        }});\n+    if (!status.ok()) {\n+      ctx->SetStatus(status);\n+      return;\n+    }\n+    ExecuteNativeSegment(ctx, ah);\n+    int num_inputs = ctx->num_inputs();\n+    // Pass input data to calibrator\n+    std::unordered_map<string, void*> input_data;\n+    for (int i = 0; i < num_inputs; i++) {\n+      const Tensor& t = ctx->input(i);\n+      void* data_address = GetTensorAddress(&t);\n+      const auto device_tensor = dev_tensors_.at(i).AccessTensor(ctx);\n+      CHECK_EQ(t.TotalBytes(),\n+               device_tensor->TotalBytes());  // use the tensor so FW keeps it\n+      input_data.emplace(StrCat(\"InputPH_\", i), data_address);\n+    }\n+    VLOG(2) << \"Filled map for sending\";\n+    // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n+    const cudaStream_t* stream = CHECK_NOTNULL(\n+        reinterpret_cast<const cudaStream_t*>(ctx->op_device_context()\n+                                                  ->stream()\n+                                                  ->implementation()\n+                                                  ->CudaStreamMemberHack()));\n+    ah->Ref();  // Increment count for calculating calibration data\n+    calib_res->calibrator_->setBatch(input_data, *stream, ah);\n+    VLOG(2) << \"Passed calibration data\";\n+    return;\n+  }\n+  int num_binding = ctx->num_inputs() + ctx->num_outputs();\n   std::vector<void*> buffers(num_binding);\n \n   size_t binding_index;\n-  int num_batch = 0;\n-  for (int i = 0; i < context->num_inputs(); i++) {\n+  int num_batch = ctx->input(0).shape().dim_size(0);\n+  int smallest_engine = 0;\n+  for (const auto i : cached_engine_batches) {\n+    if (i >= num_batch) {\n+      smallest_engine = i;\n+      break;\n+    }\n+  }\n+  // TODO(sami): Need an LRU here\n+  if (smallest_engine == 0) {\n+    if (max_cached_engines > cached_engine_batches.size()) {\n+      smallest_engine = num_batch;\n+      cached_engine_batches.push_back(num_batch);\n+      std::sort(cached_engine_batches.begin(), cached_engine_batches.end());\n+      VLOG(1) << \"Running with batch size \" << num_batch;\n+    } else {\n+      string s(\"Engine buffer is full. buffer limit= \");\n+      StrAppend(&s, max_cached_engines, \", current entries= \");\n+      for (auto i : cached_engine_batches) StrAppend(&s, i, \", \");\n+      StrAppend(&s, \"Requested batch= \", num_batch);\n+      LOG(ERROR) << s;\n+      ctx->SetStatus(tensorflow::errors::ResourceExhausted(\n+          \"Requested batch size is not available and engine cache is full\"));\n+      return;\n+    }\n+  }\n+  auto engine_ctx_pair = get_engine(smallest_engine, ctx, fixed_input_size);\n+  auto trt_engine_ptr_ = engine_ctx_pair.first;\n+  if (!trt_engine_ptr_) {\n+    LOG(WARNING) << \"Engine retrieval for batch size \" << num_batch\n+                 << \" failed Running native segment\";\n+    ExecuteNativeSegment(ctx, ah);\n+    return;\n+    // ctx->SetStatus(tensorflow::errors::Unavailable(\n+    //     StrCat(\"Engine retrieval for batch \", num_batch, \" Failed\")));\n+    // return;\n+  }\n+  for (int i = 0; i < ctx->num_inputs(); i++) {\n+    string inp_name = \"InputPH_\";\n     // Grab the input tensor\n-    binding_index = trt_engine_ptr_->getBindingIndex(input_nodes_[i].c_str());\n+    tensorflow::strings::StrAppend(&inp_name, i);\n+    binding_index = trt_engine_ptr_->getBindingIndex(inp_name.c_str());\n \n-    const Tensor& input_tensor = context->input(i);\n+    const Tensor& input_tensor = ctx->input(i);\n     const TensorShape& input_shape = input_tensor.shape();\n     if (i == 0) {\n       num_batch = input_shape.dim_size(0);\n       if (num_batch > trt_engine_ptr_->getMaxBatchSize()) {\n-        LOG(FATAL) << \"input tensor batch larger than max_batch_size: \"\n+        LOG(ERROR) << \"input tensor batch \" << num_batch", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 306, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "d5aaf3fa4a4851abc6a0e5600474f7674f1adb93", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "body": "It should not be possible. Removed", "created_at": "2018-06-12T00:45:52Z", "updated_at": "2018-06-21T22:23:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194588063", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194588063"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194588063"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>It should not be possible. Removed</p>", "body_text": "It should not be possible. Removed", "in_reply_to_id": 194562161}