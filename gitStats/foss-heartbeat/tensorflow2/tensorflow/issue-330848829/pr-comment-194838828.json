{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194838828", "pull_request_review_id": 128071489, "id": 194838828, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDgzODgyOA==", "diff_hunk": "@@ -117,51 +340,238 @@ void TRTEngineOp::Compute(OpKernelContext* context) {\n       std::vector<int> trt_shape(dims.nbDims + 1);\n       trt_shape[0] = num_batch;\n       for (int j = 0; j < dims.nbDims; j++) trt_shape[j + 1] = dims.d[j];\n-      OP_REQUIRES_OK(context,\n-                     TensorShapeUtils::MakeShape(\n-                         trt_shape.data(), trt_shape.size(), &output_shape));\n+      OP_REQUIRES_OK(\n+          ctx, TensorShapeUtils::MakeShape(trt_shape.data(), trt_shape.size(),\n+                                           &output_shape));\n     } else {\n-      LOG(FATAL) << \"output node not found, at \" << output_nodes_[i];\n-      break;\n+      LOG(ERROR) << \"output node not found, at \" << output_name;\n+      ctx->SetStatus(tensorflow::errors::Internal(\"output \" + output_name +\n+                                                  \" but couldn't be found!\"));\n+      return;\n+    }\n+    auto status = ctx->allocate_output(i, output_shape, &output_tensor);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Allocating output failed with \" << status;\n+      ctx->SetStatus(status);\n+      return;\n     }\n-\n-    OP_REQUIRES_OK(context,\n-                   context->allocate_output(i, output_shape, &output_tensor));\n     auto dtype = trt_engine_ptr_->getBindingDataType(binding_index);\n     switch (dtype) {\n       case nvinfer1::DataType::kFLOAT:\n         buffers[binding_index] =\n             reinterpret_cast<void*>(output_tensor->flat<float>().data());\n         break;\n       case nvinfer1::DataType::kHALF:\n-        LOG(FATAL) << \"half size is not supported yet!\";\n+        LOG(ERROR) << \"half size is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Half outputs are not supported!\"));\n+        return;\n         break;\n       case nvinfer1::DataType::kINT8:\n-        LOG(FATAL) << \"int8 is not supported yet!\";\n+        LOG(ERROR) << \"int8 is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"INT8 outputs are not supported!\"));\n+        return;\n         break;\n       default:\n-        LOG(FATAL) << \"Unknown data type: \" << int(dtype);\n+        LOG(ERROR) << \"Unknown TRT data type: \" << int(dtype);\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Unsupported output data type! \" + int(dtype)));\n+        return;\n         break;\n     }\n   }\n   // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n   const cudaStream_t* stream = CHECK_NOTNULL(\n-      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\n+      reinterpret_cast<const cudaStream_t*>(ctx->op_device_context()\n                                                 ->stream()\n                                                 ->implementation()\n                                                 ->CudaStreamMemberHack()));\n \n   // TODO(jie): trt enqueue does not return error\n-  auto ret = trt_execution_context_ptr_->enqueue(num_batch, &buffers[0],\n-                                                 *stream, nullptr);\n+  auto trt_execution_context_ptr = engine_ctx_pair.second;\n+  auto ret = trt_execution_context_ptr->enqueue(num_batch, &buffers[0], *stream,\n+                                                nullptr);\n   VLOG(2) << \"enqueue returns: \" << ret;\n   // sync should be done by TF.\n }\n+\n TRTEngineOp::~TRTEngineOp() {\n   // Order matters!\n-  trt_execution_context_ptr_.reset();\n-  trt_engine_ptr_.reset();\n-  allocator_.reset();\n+  for (auto eng : engine_map_) {\n+    eng.second.first.reset();\n+    eng.second.second.reset();\n+  }\n+  for (auto alloc : allocators_) alloc.second.reset();\n+}\n+\n+TRTEngineOp::EngineCtxPair TRTEngineOp::GetEngine(int batch_size,\n+                                                   OpKernelContext* ctx,\n+                                                   bool ignore_dim_change) {\n+  tensorflow::mutex_lock lock(engine_mutex_);\n+  if (static_engine_) {\n+    if (engine_map_.size()) {\n+      if (engine_map_.begin()->first >= batch_size) {\n+        return engine_map_.begin()->second;\n+      } else {\n+        return {nullptr, nullptr};\n+      }\n+    } else {\n+      IRuntime* infer = nvinfer1::createInferRuntime(logger);\n+#if NV_TENSORRT_MAJOR > 3\n+      auto device = ctx->device();\n+      auto dev_allocator =\n+          device->GetAllocator(tensorflow::AllocatorAttributes());\n+      if (!dev_allocator) {\n+        LOG(FATAL) << \"Can't find device allocator for gpu device \"\n+                   << device->name();\n+      }\n+      allocator_ = std::make_shared<TRTDeviceAllocator>(dev_allocator);", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 475, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "ae13b0560666df62967d87072e85619083a2f44b", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "Where is allocator_ defined?", "created_at": "2018-06-12T18:12:57Z", "updated_at": "2018-06-21T22:23:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194838828", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194838828"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194838828"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>Where is allocator_ defined?</p>", "body_text": "Where is allocator_ defined?"}