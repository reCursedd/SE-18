{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194587917", "pull_request_review_id": 127788486, "id": 194587917, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDU4NzkxNw==", "diff_hunk": "@@ -117,51 +339,238 @@ void TRTEngineOp::Compute(OpKernelContext* context) {\n       std::vector<int> trt_shape(dims.nbDims + 1);\n       trt_shape[0] = num_batch;\n       for (int j = 0; j < dims.nbDims; j++) trt_shape[j + 1] = dims.d[j];\n-      OP_REQUIRES_OK(context,\n-                     TensorShapeUtils::MakeShape(\n-                         trt_shape.data(), trt_shape.size(), &output_shape));\n+      OP_REQUIRES_OK(\n+          ctx, TensorShapeUtils::MakeShape(trt_shape.data(), trt_shape.size(),\n+                                           &output_shape));\n     } else {\n-      LOG(FATAL) << \"output node not found, at \" << output_nodes_[i];\n-      break;\n+      LOG(ERROR) << \"output node not found, at \" << output_name;\n+      ctx->SetStatus(tensorflow::errors::Internal(\"output \" + output_name +\n+                                                  \" but couldn't be found!\"));\n+      return;\n+    }\n+    auto status = ctx->allocate_output(i, output_shape, &output_tensor);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Allocating output failed with \" << status;\n+      ctx->SetStatus(status);\n+      return;\n     }\n-\n-    OP_REQUIRES_OK(context,\n-                   context->allocate_output(i, output_shape, &output_tensor));\n     auto dtype = trt_engine_ptr_->getBindingDataType(binding_index);\n     switch (dtype) {\n       case nvinfer1::DataType::kFLOAT:\n         buffers[binding_index] =\n             reinterpret_cast<void*>(output_tensor->flat<float>().data());\n         break;\n       case nvinfer1::DataType::kHALF:\n-        LOG(FATAL) << \"half size is not supported yet!\";\n+        LOG(ERROR) << \"half size is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Half outputs are not supported!\"));\n+        return;\n         break;\n       case nvinfer1::DataType::kINT8:\n-        LOG(FATAL) << \"int8 is not supported yet!\";\n+        LOG(ERROR) << \"int8 is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"INT8 outputs are not supported!\"));\n+        return;\n         break;\n       default:\n-        LOG(FATAL) << \"Unknown data type: \" << int(dtype);\n+        LOG(ERROR) << \"Unknown TRT data type: \" << int(dtype);\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Unsupported output data type! \" + int(dtype)));\n+        return;\n         break;\n     }\n   }\n   // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n   const cudaStream_t* stream = CHECK_NOTNULL(\n-      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\n+      reinterpret_cast<const cudaStream_t*>(ctx->op_device_context()\n                                                 ->stream()\n                                                 ->implementation()\n                                                 ->CudaStreamMemberHack()));\n \n   // TODO(jie): trt enqueue does not return error\n-  auto ret = trt_execution_context_ptr_->enqueue(num_batch, &buffers[0],\n-                                                 *stream, nullptr);\n+  auto trt_execution_context_ptr = engine_ctx_pair.second;\n+  auto ret = trt_execution_context_ptr->enqueue(num_batch, &buffers[0], *stream,\n+                                                nullptr);\n   VLOG(2) << \"enqueue returns: \" << ret;\n   // sync should be done by TF.\n-}\n+}  // namespace tensorrt\n TRTEngineOp::~TRTEngineOp() {\n   // Order matters!\n-  trt_execution_context_ptr_.reset();\n-  trt_engine_ptr_.reset();\n-  allocator_.reset();\n+  for (auto eng : engine_map) {\n+    eng.second.first.reset();\n+    eng.second.second.reset();\n+  }\n+  for (auto alloc : allocators_) alloc.second.reset();\n+}\n+// template <typename T>\n+// using destroyed_ptr = std::shared_ptr<T, TRTEngineOp::Destroyer<T>>;\n+TRTEngineOp::EngineCtxPair TRTEngineOp::get_engine(int batch_size,\n+                                                   OpKernelContext* ctx,\n+                                                   bool ignore_dim_change) {", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 450, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "d5aaf3fa4a4851abc6a0e5600474f7674f1adb93", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "body": "Engine generation logic needs expansion which is left to another PR due to already large size of this PR, where this will be needed", "created_at": "2018-06-12T00:44:49Z", "updated_at": "2018-06-21T22:23:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194587917", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194587917"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194587917"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>Engine generation logic needs expansion which is left to another PR due to already large size of this PR, where this will be needed</p>", "body_text": "Engine generation logic needs expansion which is left to another PR due to already large size of this PR, where this will be needed", "in_reply_to_id": 194563491}