{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/195129673", "pull_request_review_id": 128390653, "id": 195129673, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTEyOTY3Mw==", "diff_hunk": "@@ -30,138 +37,548 @@ using IRuntime = nvinfer1::IRuntime;\n using Dims = nvinfer1::Dims;\n \n namespace tensorrt {\n+using tensorflow::strings::StrAppend;\n+using tensorflow::strings::StrCat;\n+// A helper class to call done() for asynchronous execution.\n+// Helps simultaneous execution of native and TRT engines.\n+class AsyncHelper : public tensorflow::core::RefCounted {\n+ public:\n+  AsyncHelper(tensorflow::AsyncOpKernel::DoneCallback done) { done_ = done; }\n+  ~AsyncHelper() override { done_(); }\n \n-TRTEngineOp::TRTEngineOp(OpKernelConstruction* context) : OpKernel(context) {\n+ private:\n+  tensorflow::AsyncOpKernel::DoneCallback done_;\n+};\n+\n+#define TYPECASE(dt, X, Y)                                                \\\n+  case dt: {                                                              \\\n+    return (void*)X->flat<tensorflow::EnumToDataType<dt>::Type>().data(); \\\n+  }\n+\n+void* GetTensorAddress(const Tensor* tensor_ptr) {\n+  auto tensor_type = tensor_ptr->dtype();\n+  switch (tensor_type) {\n+    TYPECASE(tensorflow::DT_FLOAT, tensor_ptr, dest_ptr);\n+    TYPECASE(tensorflow::DT_HALF, tensor_ptr, dest_ptr);\n+    TYPECASE(tensorflow::DT_INT8, tensor_ptr, dest_ptr);\n+    default: {\n+      LOG(ERROR) << \"Unsupported Data type \"\n+                 << tensorflow::DataTypeString(tensor_type);\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+tensorflow::Status TRTEngineOp::ConstructFunctionHandle(OpKernelContext* ctx) {\n+  VLOG(1) << \"Constructing function handle\";\n+  auto lib = ctx->function_library();\n+  if (lib == nullptr) {\n+    return tensorflow::errors::Internal(\"Context function library is null\");\n+  }\n+  auto fdef = lib->GetFunctionLibraryDefinition()->Find(funcdef_name_);\n+  if (fdef == nullptr) {\n+    return tensorflow::errors::Internal(\n+        StrCat(\"Native FunctionDef \", funcdef_name_,\n+               \" can't be found in function library\"));\n+  }\n+  tensorflow::FunctionLibraryRuntime::InstantiateOptions inst_ops;\n+  inst_ops.overlay_lib = nullptr;\n+  inst_ops.state_handle = \"\";\n+  inst_ops.target = ctx->device()->name();\n+  native_func_ = 0;\n+  auto status = lib->Instantiate(funcdef_name_, AttrSlice(&fdef->attr()),\n+                                 inst_ops, &native_func_);\n+  if (!status.ok()) {\n+    LOG(ERROR) << \" Instantiating native function \" << funcdef_name_\n+               << \" failed!\";\n+  }\n+  return status;\n+}\n+\n+TRTEngineOp::TRTEngineOp(OpKernelConstruction* context)\n+    : AsyncOpKernel(context) {\n   // read serialized_engine\n   OP_REQUIRES_OK(context,\n-                 context->GetAttr(\"serialized_engine\", &serialized_engine_));\n+                 context->GetAttr(\"serialized_segment\", &serialized_segment_));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"workspace_size_bytes\", &workspace_size_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"static_engine\", &static_engine_));\n+  if (!static_engine_) {\n+    if (!segment_graph_.ParseFromString(serialized_segment_)) {\n+      LOG(ERROR) << \"Parsing segment graph failed!\";\n+      context->SetStatus(tensorflow::errors::InvalidArgument(\n+          \"Failed to parse segment graphdef!\"));\n+      return;\n+    }\n+    serialized_segment_.resize(0);\n+  }\n \n-  // register input output node name in trt_sub_graph\n-  OP_REQUIRES_OK(context, context->GetAttr(\"input_nodes\", &input_nodes_));\n-  OP_REQUIRES_OK(context, context->GetAttr(\"output_nodes\", &output_nodes_));\n+  string precision_string;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"precision_mode\", &precision_string));\n+  string calibration_data;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"calibration_data\", &calibration_data));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"segment_funcdef_name\", &funcdef_name_));\n+  if (precision_string == \"FP32\") {\n+    precision_mode_ = tensorflow::tensorrt::convert::FP32MODE;\n+  } else if (precision_string == \"FP16\") {\n+    precision_mode_ = tensorflow::tensorrt::convert::FP16MODE;\n+  } else if (precision_string == \"INT8\") {\n+    precision_mode_ = tensorflow::tensorrt::convert::INT8MODE;\n+  }\n+  calibration_mode_ =\n+      precision_mode_ == tensorflow::tensorrt::convert::INT8MODE &&\n+      calibration_data.size() == 0;\n+  if (calibration_data.size()) {\n+    calibrator_.reset(new TRTInt8Calibrator(calibration_data));\n+    calibration_data.resize(0);\n+  }\n+  native_func_ = tensorflow::kInvalidHandle;\n+  OP_REQUIRES_OK(context, context->GetAttr(\"max_cached_engines_count\",\n+                                           &max_cached_engines_));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"fixed_input_size\", &fixed_input_size_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"cached_engine_batches\",\n+                                           &cached_engine_batches_));\n+  std::sort(cached_engine_batches_.begin(), cached_engine_batches_.end());\n+  if (VLOG_IS_ON(1)) {\n+    string s(\"Engine Batches= \");\n+    for (auto i : cached_engine_batches_) {\n+      StrAppend(&s, i, \" \");\n+    }\n+    VLOG(1) << s;\n+  }\n }\n \n-void TRTEngineOp::Compute(OpKernelContext* context) {\n-  // TODO(samikama) runtime should be taken from a resourcemanager as well.\n-  // Only engine should be in the op and context and runtime should be taken\n-  // from resourcemanager\n+void TRTEngineOp::ExecuteNativeSegment(tensorflow::OpKernelContext* ctx,\n+                                       AsyncHelper* helper) {\n+  if (!calibration_mode_) {\n+    VLOG(1) << \"Executing native engine\";\n+  }\n+  std::vector<Tensor> inputs;\n+  std::vector<Tensor>* outputs = new std::vector<Tensor>();\n+  if (native_func_ == tensorflow::kInvalidHandle) {\n+    auto status = ConstructFunctionHandle(ctx);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Couldn't construct function handle \" << funcdef_name_;\n+      ctx->SetStatus(status);\n+      return;\n+    }\n+  }\n+  auto lib = ctx->function_library();\n+  tensorflow::FunctionLibraryRuntime::Options opts;\n+  opts.step_id = ctx->step_id();\n+  opts.rendezvous = ctx->rendezvous();\n+  opts.cancellation_manager = ctx->cancellation_manager();\n+  opts.runner = ctx->runner();\n+  for (int i = 0; i < ctx->num_inputs(); i++) {\n+    inputs.push_back(ctx->input(i));\n+  }\n+  helper->Ref();  // Increment count for calculating native graph\n+  VLOG(1) << \"Executing native segment \" << name();\n+  lib->Run(opts, native_func_, inputs, outputs,\n+           [ctx, outputs, helper](const tensorflow::Status& s) {\n+             tensorflow::core::ScopedUnref sc(helper);\n+             VLOG(1) << \"Native Segment completed\";\n+             if (!s.ok()) {\n+               ctx->SetStatus(s);\n+               return;\n+             }\n+             for (size_t t = 0; t < outputs->size(); ++t) {\n+               ctx->set_output(t, outputs->at(t));\n+             }\n+             delete outputs;\n+             return;\n+           });\n+  return;\n+}\n \n-  if (!trt_execution_context_ptr_) {\n-    IRuntime* infer = nvinfer1::createInferRuntime(logger);\n-#if NV_TENSORRT_MAJOR > 3\n-    auto device = context->device();\n-    auto dev_allocator =\n-        device->GetAllocator(tensorflow::AllocatorAttributes());\n-    if (!dev_allocator) {\n-      LOG(FATAL) << \"Can't find device allocator for gpu device \"\n-                 << device->name();\n-    }\n-    allocator_ = std::make_shared<TRTDeviceAllocator>(dev_allocator);\n-    infer->setGpuAllocator(allocator_.get());\n-#endif\n-    trt_engine_ptr_.reset(infer->deserializeCudaEngine(\n-        serialized_engine_.c_str(), serialized_engine_.size(),\n-        PluginFactoryTensorRT::GetInstance()));\n-    trt_execution_context_ptr_.reset(trt_engine_ptr_->createExecutionContext());\n-    // Runtime is safe to delete after engine creation\n-    infer->destroy();\n-    serialized_engine_.clear();\n-  }\n-  int num_binding = context->num_inputs() + context->num_outputs();\n-  std::vector<void*> buffers(num_binding);\n+void TRTEngineOp::ExecuteCalibration(tensorflow::OpKernelContext* ctx,\n+                                     AsyncHelper* helper) {\n+  tensorflow::core::ScopedUnref sc(helper);\n+  auto TRT_RM = tensorflow::tensorrt::TRTResourceManager::instance();", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 214, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "d56675f6662b651dc996786262a6d28ccf9e06e7", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "trt_rm", "created_at": "2018-06-13T15:28:38Z", "updated_at": "2018-06-21T22:23:15Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r195129673", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/195129673"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r195129673"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>trt_rm</p>", "body_text": "trt_rm"}