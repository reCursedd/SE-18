{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194914905", "pull_request_review_id": 128071489, "id": 194914905, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDkxNDkwNQ==", "diff_hunk": "@@ -318,96 +119,503 @@ tensorflow::Status BuildNodeMap(\n }\n \n }  // namespace\n+// Function to get calibration from ResourceMgr and put them into nodedef.\n tensorflow::Status ConvertCalibGraphToInferGraph(\n     const tensorflow::GraphDef& graph_def, tensorflow::GraphDef* infer_graph) {\n   VLOG(0) << \"Starting Calib Conversion\";\n-  tensorflow::Graph graph(tensorflow::OpRegistry::Global());\n-  TF_RETURN_IF_ERROR(tensorflow::ConvertGraphDefToGraph(\n-      tensorflow::GraphConstructorOptions(), graph_def, &graph));\n-  //  get calib nodes\n-  std::vector<tensorflow::Node*> calib_nodes;\n-  std::vector<tensorflow::Node*> topo_order;\n-  tensorflow::GetPostOrder(graph, &topo_order);\n-  for (auto rit = topo_order.rbegin(); rit != topo_order.rend(); ++rit) {\n-    auto node = *rit;\n-    if (node->type_string() == \"TRTCalibOp\") {\n-      VLOG(1) << \"Found Calib Node \" << node->name();\n-      calib_nodes.push_back(node);\n+  infer_graph->CopyFrom(graph_def);\n+  auto trt_rm = tensorflow::tensorrt::TRTResourceManager::instance();\n+  auto calib_rm = trt_rm->getManager(\"TRTCalibration\");\n+  int num_nodes = infer_graph->node_size();\n+  for (int i = 0; i < num_nodes; ++i) {\n+    auto n = infer_graph->mutable_node(i);\n+    if (n->op() == \"TRTEngineOp\") {\n+      VLOG(1) << \"Processing \" << n->name();\n+      string container_name = n->attr().at(\"segment_funcdef_name\").s();\n+      tensorflow::tensorrt::TRTCalibrationResource* cres = nullptr;\n+      auto status = calib_rm->Lookup(container_name, \"Calibrator\", &cres);\n+      if (!status.ok()) {\n+        LOG(ERROR) << \"Could not get Calibration information. Did you run with \"\n+                      \"calibration data?\";\n+        return tensorflow::errors::FailedPrecondition(\n+            \"Need to run graph with calibration data first!\");\n+      }\n+      if (cres->calibrator_) {\n+        cres->calibrator_->setDone();\n+        cres->thr_->join();\n+        auto calibration_table =\n+            cres->calibrator_->getCalibrationTableAsString();\n+        if (!calibration_table.size()) {\n+          LOG(ERROR) << \"Calibration table is empty\";\n+          return tensorflow::errors::Unknown(\n+              \"Calibration table is missing. This shouldn't have happened!\");\n+        }\n+        n->mutable_attr()->at(\"calibration_data\").set_s(calibration_table);\n+      } else {\n+        LOG(ERROR) << \"Can't get TRTCalibrator from resource manager!\";\n+        return tensorflow::errors::Unknown(\n+            \"Can't get TRTCalibrator from resource manager!\");\n+      }\n+      cres->Unref();\n     }\n   }\n-  VLOG(0) << \"Num Calib nodes in graph= \" << calib_nodes.size();\n-  if (calib_nodes.size() == 0)\n-    return tensorflow::errors::FailedPrecondition(\n-        \"Graph doesn't contain any calibration nodes!.\"\n-        \" Please generate calibration graph and run calibration first\");\n-  for (auto n : calib_nodes) {\n-    TF_RETURN_IF_ERROR(\n-        tensorrt::convert::ConvertCalibrationNodeToEngineNode(graph, n));\n-  }\n-  graph.ToGraphDef(infer_graph);\n   return tensorflow::Status::OK();\n }\n \n+// Entry function from Python.\n tensorflow::Status ConvertGraphDefToTensorRT(\n     const tensorflow::GraphDef& graph_def,\n     const std::vector<string>& output_names, size_t max_batch_size,\n     size_t max_workspace_size_bytes, tensorflow::GraphDef* new_graph_def,\n-    int precision_mode = FP32MODE, int minimum_segment_size = 3) {\n+    int precision_mode, int minimum_segment_size, bool is_dyn_op,\n+    int max_cached_engines, std::vector<int> cached_engine_batches) {\n   // optimization pass\n   tensorflow::grappler::GrapplerItem item;\n   item.fetch = output_names;\n   item.graph = graph_def;\n-\n+  // grappler requires a virtual cluster with a proper GPU device\n+  // in order to calculate flops>0 or fails with FATAL\n+  // We add numbers from a Pascal card here to have flops>0\n   tensorflow::DeviceProperties device_properties;\n   device_properties.set_type(\"GPU\");\n   device_properties.mutable_environment()->insert({\"architecture\", \"6\"});\n-  tensorflow::grappler::Cluster* cluster =\n-      new tensorflow::grappler::VirtualCluster({{\"/GPU:0\", device_properties}});\n+  device_properties.set_num_cores(3584);\n+  device_properties.set_frequency(1531);\n+  std::unique_ptr<tensorflow::grappler::Cluster> cluster(\n+      new tensorflow::grappler::VirtualCluster(\n+          {{\"/GPU:0\", device_properties}}));\n \n   // single machine\n   int num_cpu_cores = tensorflow::grappler::GetNumAvailableLogicalCPUCores();\n   int num_gpus = tensorflow::grappler::GetNumAvailableGPUs();\n   VLOG(2) << \"cpu_cores: \" << num_cpu_cores;\n   VLOG(2) << \"gpus: \" << num_gpus;\n   tensorflow::RewriterConfig rw_cfg;\n+  // use only const folding and layout for the time being since new optimizers\n+  // break the graph for us\n+  rw_cfg.add_optimizers(\"constfold\");\n+  rw_cfg.add_optimizers(\"layout\");\n+  rw_cfg.set_meta_optimizer_iterations(tensorflow::RewriterConfig::ONE);\n   tensorflow::grappler::MetaOptimizer meta_opt(nullptr, rw_cfg);\n   tensorflow::GraphDef gdef;\n-  TF_RETURN_IF_ERROR(meta_opt.Optimize(cluster, item, &gdef));\n+  TF_RETURN_IF_ERROR(meta_opt.Optimize(cluster.get(), item, &gdef));\n   item.graph = gdef;\n \n   // AJ refactoring shape inference through grappler/GraphProperties.\n   tensorflow::grappler::GraphProperties static_graph_properties(item);\n   TF_RETURN_IF_ERROR(static_graph_properties.InferStatically(true));\n   // Build full graph\n-\n-  return ConvertAfterShapes(gdef, output_names, max_batch_size,\n-                            max_workspace_size_bytes, new_graph_def,\n-                            precision_mode, minimum_segment_size,\n-                            static_graph_properties, nullptr);\n+  ConversionParams cp;\n+  cp.input_graph_def = &gdef;\n+  cp.output_names = &output_names;\n+  cp.max_batch_size = max_batch_size;\n+  cp.output_graph_def = new_graph_def;\n+  cp.precision_mode = precision_mode;\n+  cp.is_dyn_op = is_dyn_op;\n+  cp.max_cached_engines = max_cached_engines;\n+  cp.cached_engine_batches = cached_engine_batches;\n+  cp.minimum_segment_size = minimum_segment_size;\n+  cp.graph_properties = &static_graph_properties;\n+  cp.max_workspace_size_bytes = max_workspace_size_bytes;\n+  if (VLOG_IS_ON(5)) {\n+    std::fstream f;\n+    f.open(\"TRTConversionInput.pb\",\n+           std::fstream::out | std::fstream::binary | std::fstream::trunc);\n+    f << gdef.SerializeAsString();\n+    f.close();\n+  }\n+  return ConvertAfterShapes(cp);\n }\n \n-tensorflow::Status ConvertAfterShapes(\n-    const tensorflow::GraphDef& gdef, const std::vector<string>& output_names,\n-    size_t max_batch_size, size_t max_workspace_size_bytes,\n-    tensorflow::GraphDef* new_graph_def, int precision_mode,\n-    int minimum_segment_size,\n+// Function to get subsegment information structure.\n+EngineInfo GetEngineInfo(\n+    const tensorflow::Graph* g,\n     const tensorflow::grappler::GraphProperties& graph_properties,\n-    const tensorflow::grappler::Cluster* cluster) {\n+    const std::set<string>& segment_nodes,\n+    const std::unordered_map<string, tensorflow::Node*>& node_map,\n+    const std::vector<tensorflow::Node*>& topological_order) {\n+  std::vector<int> subgraph_node_ids;\n+  EngineInfo info;\n+  std::set<string> segment_devices;\n+  int input_port = 0;\n+  int output_port = 0;\n+  std::unordered_map<string, int> created_edges;\n+  for (auto it = topological_order.rbegin(); it != topological_order.rend();\n+       ++it) {\n+    auto node_name = (*it)->name();\n+\n+    if (segment_nodes.count(node_name) == 0) continue;\n+    auto node = node_map.at(node_name);\n+    auto node_device = node->requested_device();\n+    if (!node_device.empty()) {\n+      segment_devices.insert(node_device);\n+    }\n+    int node_id = node->id();\n+    subgraph_node_ids.push_back(node_id);\n+    for (const auto edge : node->in_edges()) {\n+      auto input_node = edge->src();\n+      if (segment_nodes.count(input_node->name()) == 0) {\n+        if (input_node->type_string() ==\n+            \"Const\") {  // Add constant input into segment\n+          subgraph_node_ids.push_back(input_node->id());\n+        } else if (!edge->IsControlEdge() && !input_node->IsSource()) {\n+          string s(input_node->name());\n+          StrAppend(&s, \":\", edge->src_output());\n+          VLOG(1) << \"Input edge = \" << s;\n+          int port = input_port;\n+          if (created_edges.count(s)) {\n+            port = created_edges.at(s);\n+          } else {\n+            created_edges.insert({s, port});\n+            input_port++;\n+          }\n+          EngineConnections ec(input_node->name(), input_node->id(),\n+                               edge->src_output(), node_name, node_id,\n+                               edge->dst_input(), true, port);\n+          ec.connection_type = input_node->output_type(edge->src_output());\n+\n+          info.connections.emplace_back(std::move(ec));\n+        }\n+      }\n+    }\n+    for (const auto edge : node->out_edges()) {\n+      auto output_node = edge->dst();\n+      if (segment_nodes.count(output_node->name()) == 0 &&\n+          !edge->IsControlEdge() && !output_node->IsSink()) {\n+        string s(node_name);\n+        StrAppend(&s, \":\", edge->src_output());\n+        VLOG(1) << \"Output edge = \" << s;\n+        int port = output_port;\n+        if (created_edges.count(s)) {\n+          port = created_edges.at(s);\n+        } else {\n+          created_edges.insert({s, port});\n+          output_port++;\n+        }\n+        info.connections.emplace_back(output_node->name(), output_node->id(),\n+                                      edge->dst_input(), node_name, node_id,\n+                                      edge->src_output(), false, port);\n+      }\n+    }\n+  }\n+\n+  ConvertSegmentToGraphDef(g, graph_properties, subgraph_node_ids,\n+                           &info.connections, &info.segment_graph_def,\n+                           &info.engine_name);\n+  info.engine_type = EngineInfo::EngineType::TRTStatic;\n+  if (segment_devices.size() > 1) {\n+    LOG(WARNING) << \"Detected multiple(\" << segment_devices.size()\n+                 << \") devices for the segment. Picking first one to continue \"\n+                 << \"but this shouldn't have happened\";\n+    info.device = *segment_devices.begin();\n+  }\n+  return info;\n+}\n+\n+// Function to insert a TRT node into the graph.\n+tensorflow::Status CreateTRTNode(tensorflow::Graph* graph,\n+                                 const std::vector<EngineInfo>& infos, int pos,\n+                                 tensorflow::NodeDef* trtNode,\n+                                 nvinfer1::IGpuAllocator* alloc,\n+                                 int max_batch_size) {\n+  auto& info = infos.at(pos);\n+  std::vector<tensorflow::TensorShapeProto> out_shapes;\n+  std::vector<tensorflow::TensorShapeProto> input_shapes;\n+  std::vector<tensorflow::PartialTensorShape> shapes;\n+  std::vector<tensorflow::NodeDefBuilder::NodeOut> inputs;\n+  std::vector<tensorflow::DataType> out_types;\n+  VLOG(1) << \"Processing \" << info.engine_name;\n+  for (const auto conn : info.connections) {\n+    if (!conn.is_input_edge) {  // output edge\n+      tensorflow::TensorShapeProto out_shape;\n+      conn.inside_shape.AsProto(\n+          &out_shape);  // shape of the output node inside segment\n+      if (out_shapes.size() <= conn.port_number) {\n+        out_shapes.resize(conn.port_number + 1);\n+        out_types.resize(conn.port_number + 1);\n+      }\n+      out_shapes.at(conn.port_number) = out_shape;\n+      out_types.at(conn.port_number) = conn.connection_type;\n+      continue;\n+    } else {  // input edge\n+      tensorflow::TensorShapeProto in_shape;\n+      conn.outside_shape.AsProto(&in_shape);\n+\n+      if (input_shapes.size() <= conn.port_number) {\n+        input_shapes.resize(conn.port_number + 1);\n+        shapes.resize(conn.port_number + 1);\n+      }\n+      input_shapes.at(conn.port_number) = in_shape;\n+      shapes.at(conn.port_number) = conn.outside_shape;\n+    }\n+    string input_node = conn.outside_node_name;\n+    int input_port = conn.outside_port;\n+    auto dtype = conn.connection_type;\n+    bool found_engine = false;\n+    // Rewire the inputs to other engines if they contain original input node\n+    for (size_t t = 0; t < infos.size(); ++t) {\n+      if (t == pos) {\n+        continue;\n+      }\n+      auto& engine_info = infos.at(t);\n+      for (const auto& eng_conn : engine_info.connections) {\n+        if (eng_conn.is_input_edge) {\n+          continue;\n+        }\n+        if (eng_conn.inside_node_name == input_node) {\n+          input_node = engine_info.engine_name;\n+          if (eng_conn.inside_port == input_port) {\n+            input_port = eng_conn.port_number;\n+            found_engine = true;\n+            break;\n+          }\n+        }\n+      }\n+      if (found_engine) break;\n+    }\n+    VLOG(1) << \"Engine Input \" << input_node << \":\" << input_port << \" -> \"\n+            << info.engine_name << \":\" << inputs.size();\n+    bool new_input = true;\n+    for (const auto& inp : inputs) {\n+      if (inp.node == input_node && inp.index == input_port) {\n+        new_input = false;\n+        break;\n+      }\n+    }\n+    if (new_input) {\n+      inputs.emplace_back(input_node, input_port, dtype);\n+    }\n+  }\n+  string segment_string;\n+  if (info.engine_type == EngineInfo::EngineType::TRTStatic ||\n+      info.precision_mode == INT8MODE) {\n+    // Create static engine and for int8 test validity of the engine.\n+    tensorflow::tensorrt::Logger trt_logger;\n+    auto builder = std::shared_ptr<nvinfer1::IBuilder>(\n+        nvinfer1::createInferBuilder(trt_logger), [](nvinfer1::IBuilder* p) {\n+          if (p) p->destroy();\n+        });\n+    builder->setMaxBatchSize(max_batch_size);\n+    if (info.precision_mode == tensorflow::tensorrt::convert::FP16MODE) {\n+      builder->setHalf2Mode(true);\n+    }\n+    builder->setMaxWorkspaceSize(info.max_workspace_size_bytes);\n+    nvinfer1::ICudaEngine* engine = nullptr;\n+    // TODO(sami): What happens if 1st dim is not batch?\n+    auto status = ConvertSubgraphToEngine(info.segment_graph_def, builder.get(),\n+                                          shapes, &engine, info.precision_mode);\n+    if (!status.ok()) {\n+      return status;\n+    }\n+    if (engine) {\n+      auto engine_data = std::shared_ptr<nvinfer1::IHostMemory>(\n+          engine->serialize(), [](nvinfer1::IHostMemory* p) {\n+            if (p) p->destroy();\n+          });\n+      segment_string =\n+          string((const char*)engine_data->data(), engine_data->size());\n+      engine->destroy();\n+    }\n+    if (info.precision_mode == INT8MODE) {\n+      segment_string = info.segment_graph_def.SerializeAsString();\n+    }\n+  } else {\n+    segment_string = info.segment_graph_def.SerializeAsString();\n+  }\n+  string prec_string;\n+  switch (info.precision_mode) {\n+    case FP32MODE: {\n+      prec_string = \"FP32\";\n+      break;\n+    }\n+    case FP16MODE: {\n+      prec_string = \"FP16\";\n+      break;\n+    }\n+    case INT8MODE: {\n+      prec_string = \"INT8\";\n+      auto trt_rm = tensorflow::tensorrt::TRTResourceManager::instance();\n+      auto calib_rm = trt_rm->getManager(\"TRTCalibration\");\n+      if (!calib_rm) {\n+        LOG(ERROR) << \"Failed to construct calibration storage\";\n+      }\n+      break;\n+    }\n+    default: {\n+      return tensorflow::errors::OutOfRange(\"Unknown precision mode\");\n+    }\n+  }\n+  tensorflow::Status status;\n+  tensorflow::Node* engine_node = nullptr;\n+  tensorflow::NodeDefBuilder node_builder(info.engine_name, \"TRTEngineOp\");\n+  if (!info.device.empty()) {\n+    node_builder.Device(info.device);\n+  }\n+  if (VLOG_IS_ON(1)) {\n+    string ins(info.engine_name);\n+    for (const auto& ii : inputs) {\n+      StrAppend(&ins, ii.node, \":\", ii.index, \" \");\n+    }\n+    VLOG(1) << ins;\n+  }\n+  node_builder.Input(inputs);\n+  if (info.engine_type == EngineInfo::EngineType::TRTStatic) {\n+    if (info.cached_engine_batches.size()) {\n+      LOG(WARNING) << \"Cached engine batches are ignored for static engines\";\n+    }\n+  }\n+  status = node_builder.Attr(\"input_shapes\", input_shapes)\n+               .Attr(\"output_shapes\", out_shapes)\n+               .Attr(\"static_engine\",\n+                     info.engine_type == EngineInfo::EngineType::TRTStatic)\n+               .Attr(\"segment_funcdef_name\",\n+                     StrCat(info.engine_name, \"_native_segment\"))\n+               .Attr(\"serialized_segment\", segment_string)\n+               .Attr(\"calibration_data\", \"\")\n+               .Attr(\"max_cached_engines_count\", info.maximum_cached_engines)\n+               .Attr(\"cached_engine_batches\", {max_batch_size})\n+               .Attr(\"workspace_size_bytes\", info.max_workspace_size_bytes)\n+               .Attr(\"precision_mode\", prec_string)\n+               .Attr(\"OutT\", out_types)\n+               .Finalize(trtNode);\n+  if (!status.ok()) {\n+    LOG(ERROR) << \"Node construction failed with\" << status;\n+    return status;\n+  }\n+  VLOG(1) << \"Adding TRTEngine \" << info.engine_name << \" to graph\";\n+  engine_node = graph->AddNode(*trtNode, &status);\n+  if (!status.ok()) {\n+    LOG(ERROR) << \"Adding node failed \" << status;\n+    return status;\n+  }\n+\n+  for (auto& conn : info.connections) {\n+    if (conn.is_input_edge) continue;\n+    VLOG(1) << \" Updating DBG \" << engine_node->name() << \" out_port \"\n+            << conn.port_number << \" out_id \" << conn.outside_id\n+            << \" name=\" << conn.outside_node_name;\n+    auto dst_node = graph->FindNodeId(conn.outside_id);\n+    if (!dst_node) {  // node removed skip.\n+      continue;\n+    }\n+    VLOG(1) << \"Updating \" << engine_node->name() << \":\" << conn.port_number\n+            << \" to \" << dst_node->name() << \":\" << conn.outside_port;\n+    status = graph->UpdateEdge(engine_node, conn.port_number, dst_node,\n+                               conn.outside_port);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Edge update failed \" << engine_node->name() << \":\"\n+                 << conn.port_number << \" -> \" << dst_node->name() << \":\"\n+                 << conn.outside_port << \" status= \" << status;\n+    }\n+  }\n+  return status;\n+}\n+\n+// Function to construct a funcdef from the segment and add it to the graph.\n+tensorflow::Status RegisterSegmentFunctionToFunctionLibrary(\n+    tensorflow::Graph* graph, const tensorflow::GraphDef& segment,\n+    const string& name) {\n+  tensorflow::Graph sgraph(graph->flib_def());\n+  tensorflow::GraphConstructorOptions gcopts;\n+  TF_RETURN_IF_ERROR(\n+      tensorflow::ConvertGraphDefToGraph(gcopts, segment, &sgraph));\n+  std::map<string, tensorflow::Node*> io_nodes;\n+  int num_inputs = 0;\n+  for (auto n : sgraph.op_nodes()) {\n+    if (tensorflow::str_util::StartsWith(n->name(), \"InputPH_\")) {\n+      num_inputs++;\n+      io_nodes.insert({n->name(), n});\n+    } else if (tensorflow::str_util::StartsWith(n->name(), \"OutputPH_\")) {\n+      io_nodes.insert({n->name(), n});\n+    }\n+  }\n+\n+  for (int i = 0; i < num_inputs; ++i) {\n+    auto name = StrCat(\"InputPH_\", i);", "path": "tensorflow/contrib/tensorrt/convert/convert_graph.cc", "position": null, "original_position": 745, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "565640eae327b092edf43613f77ba5ab0747d20d", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "I don't quite understand: why do the conversion from InputPH_ to _Arg here? Why not directly construct the _Arg nodes from the very beginning?", "created_at": "2018-06-12T23:07:32Z", "updated_at": "2018-06-21T22:23:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194914905", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194914905"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194914905"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>I don't quite understand: why do the conversion from InputPH_ to _Arg here? Why not directly construct the _Arg nodes from the very beginning?</p>", "body_text": "I don't quite understand: why do the conversion from InputPH_ to _Arg here? Why not directly construct the _Arg nodes from the very beginning?"}