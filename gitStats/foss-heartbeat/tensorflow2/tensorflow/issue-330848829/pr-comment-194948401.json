{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194948401", "pull_request_review_id": 128218125, "id": 194948401, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDk0ODQwMQ==", "diff_hunk": "@@ -117,51 +345,246 @@ void TRTEngineOp::Compute(OpKernelContext* context) {\n       std::vector<int> trt_shape(dims.nbDims + 1);\n       trt_shape[0] = num_batch;\n       for (int j = 0; j < dims.nbDims; j++) trt_shape[j + 1] = dims.d[j];\n-      OP_REQUIRES_OK(context,\n-                     TensorShapeUtils::MakeShape(\n-                         trt_shape.data(), trt_shape.size(), &output_shape));\n+      OP_REQUIRES_OK(\n+          ctx, TensorShapeUtils::MakeShape(trt_shape.data(), trt_shape.size(),\n+                                           &output_shape));\n     } else {\n-      LOG(FATAL) << \"output node not found, at \" << output_nodes_[i];\n-      break;\n+      LOG(ERROR) << \"output node not found, at \" << output_name;\n+      ctx->SetStatus(tensorflow::errors::Internal(\"output \" + output_name +\n+                                                  \" but couldn't be found!\"));\n+      return;\n+    }\n+    auto status = ctx->allocate_output(i, output_shape, &output_tensor);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Allocating output failed with \" << status;\n+      ctx->SetStatus(status);\n+      return;\n     }\n-\n-    OP_REQUIRES_OK(context,\n-                   context->allocate_output(i, output_shape, &output_tensor));\n     auto dtype = trt_engine_ptr_->getBindingDataType(binding_index);\n     switch (dtype) {\n       case nvinfer1::DataType::kFLOAT:\n         buffers[binding_index] =\n             reinterpret_cast<void*>(output_tensor->flat<float>().data());\n         break;\n       case nvinfer1::DataType::kHALF:\n-        LOG(FATAL) << \"half size is not supported yet!\";\n+        LOG(ERROR) << \"half size is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Half outputs are not supported!\"));\n+        return;\n         break;\n       case nvinfer1::DataType::kINT8:\n-        LOG(FATAL) << \"int8 is not supported yet!\";\n+        LOG(ERROR) << \"int8 is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"INT8 outputs are not supported!\"));\n+        return;\n         break;\n       default:\n-        LOG(FATAL) << \"Unknown data type: \" << int(dtype);\n+        LOG(ERROR) << \"Unknown TRT data type: \" << int(dtype);\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Unsupported output data type! \" + int(dtype)));\n+        return;\n         break;\n     }\n   }\n   // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n   const cudaStream_t* stream = CHECK_NOTNULL(\n-      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\n+      reinterpret_cast<const cudaStream_t*>(ctx->op_device_context()\n                                                 ->stream()\n                                                 ->implementation()\n                                                 ->CudaStreamMemberHack()));\n \n   // TODO(jie): trt enqueue does not return error\n-  auto ret = trt_execution_context_ptr_->enqueue(num_batch, &buffers[0],\n-                                                 *stream, nullptr);\n+  auto trt_execution_context_ptr = engine_ctx_pair.second;\n+  auto ret = trt_execution_context_ptr->enqueue(num_batch, &buffers[0], *stream,\n+                                                nullptr);\n   VLOG(2) << \"enqueue returns: \" << ret;\n   // sync should be done by TF.\n }\n+\n TRTEngineOp::~TRTEngineOp() {\n   // Order matters!\n-  trt_execution_context_ptr_.reset();\n-  trt_engine_ptr_.reset();\n-  allocator_.reset();\n+  for (auto eng : engine_map_) {\n+    eng.second.first.reset();\n+    eng.second.second.reset();\n+  }\n+  for (auto alloc : allocators_) alloc.second.reset();\n+}\n+\n+TRTEngineOp::EngineCtxPair TRTEngineOp::GetEngine(int batch_size,\n+                                                  OpKernelContext* ctx,\n+                                                  bool ignore_dim_change) {\n+  // TODO(sami): This method needs to be re-written to use resource manager and\n+  // with LRU mechanism option.\n+  tensorflow::mutex_lock lock(engine_mutex_);\n+  if (static_engine_) {\n+    if (engine_map_.size()) {\n+      if (engine_map_.begin()->first >= batch_size) {\n+        return engine_map_.begin()->second;\n+      } else {\n+        return {nullptr, nullptr};\n+      }\n+    } else {\n+      IRuntime* infer = nvinfer1::createInferRuntime(logger);\n+#if NV_TENSORRT_MAJOR > 3\n+      auto device = ctx->device();\n+      auto dev_allocator =\n+          device->GetAllocator(tensorflow::AllocatorAttributes());\n+      if (!dev_allocator) {\n+        LOG(FATAL) << \"Can't find device allocator for gpu device \"\n+                   << device->name();\n+      }\n+      allocator_ = std::make_shared<TRTDeviceAllocator>(dev_allocator);\n+      infer->setGpuAllocator(allocator_.get());\n+#endif\n+      std::shared_ptr<nvinfer1::ICudaEngine> static_engine(\n+          infer->deserializeCudaEngine(serialized_segment_.c_str(),\n+                                       serialized_segment_.size(), nullptr),\n+          Destroyer<nvinfer1::ICudaEngine>());\n+      engine_map_.insert({static_engine->getMaxBatchSize(),\n+                          {static_engine,\n+                           {static_engine->createExecutionContext(),\n+                            Destroyer<nvinfer1::IExecutionContext>()}}});\n+      // Runtime is safe to delete after engine creation\n+      infer->destroy();\n+      serialized_segment_.clear();\n+      if (static_engine->getMaxBatchSize() < batch_size) {\n+        return {nullptr, nullptr};\n+      }\n+      return engine_map_.at(static_engine->getMaxBatchSize());\n+    }\n+  } else {\n+    auto engine_it = engine_map_.find(batch_size);\n+    if (engine_it == engine_map_.end() &&\n+        engine_map_.size() < (size_t)max_cached_engines_) {\n+      auto builder_ = std::shared_ptr<nvinfer1::IBuilder>(\n+          nvinfer1::createInferBuilder(logger),\n+          Destroyer<nvinfer1::IBuilder>());  // reset the builder to ensure\n+                                             // device is correct\n+#if NV_TENSORRT_MAJOR > 3\n+      auto device = context->device();\n+      auto device_name = device->name();\n+      if (allocators_.count(device_name)) {\n+        builder_->setGpuAllocator(allocators_.at(device_name).get());\n+      } else {\n+        std::make_shared<TRTDeviceAllocator> auto dev_allocator =\n+            device->GetAllocator(tensorflow::AllocatorAttributes());\n+        if (!dev_allocator) {\n+          LOG(ERROR) << \"Can't find device allocator for gpu device \"\n+                     << device->name();\n+          ctx->SetStatus(\n+              tensorflow::errors::Internal(\"Can't get device allocator\"));\n+          return nullptr;\n+        }\n+        auto allocator_ = std::make_shared<TRTDeviceAllocator>(dev_allocator);\n+        builder_->setGpuAllocator(allocator_.get());\n+        allocators_.insert({device_name, allocator});\n+      }\n+#endif\n+      VLOG(1) << name() << \" Constructing a new engine with batch size \"\n+              << batch_size;\n+      builder_->setMaxBatchSize(batch_size);\n+      if (precision_mode_ == tensorflow::tensorrt::convert::FP16MODE) {\n+        builder_->setHalf2Mode(true);\n+      } else if (precision_mode_ == tensorflow::tensorrt::convert::INT8MODE) {\n+        builder_->setInt8Mode(true);\n+        builder_->setInt8Calibrator(calibrator_.get());\n+      }\n+      builder_->setMaxWorkspaceSize(workspace_size_);\n+      nvinfer1::ICudaEngine* engine = nullptr;\n+      std::vector<tensorflow::PartialTensorShape> shapes;\n+      for (int i = 0; i < ctx->num_inputs(); ++i) {\n+        shapes.emplace_back(ctx->input(i).shape());\n+      }\n+      auto status = tensorflow::tensorrt::convert::ConvertSubgraphToEngine(\n+          segment_graph_, builder_.get(), shapes, &engine, precision_mode_);\n+      if (engine) {\n+        engine_map_[batch_size] = {\n+            std::shared_ptr<nvinfer1::ICudaEngine>(\n+                engine, Destroyer<nvinfer1::ICudaEngine>()),\n+            std::shared_ptr<nvinfer1::IExecutionContext>(\n+                engine->createExecutionContext(),\n+                Destroyer<nvinfer1::IExecutionContext>())};\n+      } else {\n+        LOG(ERROR) << \"Engine creation for batch size \" << batch_size\n+                   << \" failed\";\n+        ctx->SetStatus(tensorflow::errors::Internal(\"Engine creation failed!\"));\n+        engine_map_[batch_size] = {nullptr, nullptr};\n+        return {nullptr, nullptr};\n+      }\n+    }\n+    return engine_map_.at(batch_size);\n+  }\n+}\n+\n+tensorflow::Status TRTEngineOp::AllocateCalibrationResources(\n+    tensorflow::OpKernelContext* ctx,\n+    tensorflow::tensorrt::TRTCalibrationResource** cr) {\n+  auto cres = new TRTCalibrationResource();\n+  *cr = cres;\n+  cres->logger_ = new tensorflow::tensorrt::Logger();\n+  cres->builder_ = nvinfer1::createInferBuilder(*(cres->logger_));\n+#if NV_TENSORRT_MAJOR > 3\n+  auto dev = ctx->device();\n+  auto dev_allocator = dev->GetAllocator(tensorflow::AllocatorAttributes());\n+  if (!dev_allocator) {\n+    LOG(WARNING) << \"Can't get device allocator will not be able to \"\n+                    \"allocate memory from TensorFlow memory pool\";\n+    cres->allocator_ =\n+        std::make_shared<tensorflow::tensorrt::TRTCudaAllocator>();\n+  } else {\n+    cres->allocator_ =\n+        std::make_shared<tensorflow::tensorrt::TRTDeviceAllocator>(\n+            dev_allocator);\n+  }\n+  cres->builder_->setGpuAllocator(cres->allocator_.get());\n+#endif\n+  int batch_size = ctx->input(0).dim_size(0);\n+  cres->builder_->setMaxBatchSize(batch_size);\n+  cres->builder_->setInt8Mode(true);\n+  cres->builder_->setMaxWorkspaceSize(workspace_size_);\n+  cres->engine_ = nullptr;\n+  std::vector<tensorflow::PartialTensorShape> shapes;\n+  int num_inputs = ctx->num_inputs();\n+  // first run instantiate calibrator\n+  dev_tensors_.resize(num_inputs);\n+  VLOG(1) << \" Constructing calibrator\";\n+  for (int i = 0; i < num_inputs; i++) {\n+    // allocate workspace on device for inputs\n+    const tensorflow::Tensor& t = ctx->input(i);\n+    shapes.emplace_back(t.shape());\n+    TF_RETURN_IF_ERROR(ctx->allocate_persistent(t.dtype(), t.shape(),\n+                                                &dev_tensors_.at(i), nullptr));\n+    const auto device_tensor = dev_tensors_.at(i).AccessTensor(ctx);\n+    CHECK_EQ(t.TotalBytes(), device_tensor->TotalBytes());\n+    void* device_address = GetTensorAddress(device_tensor);\n+    if (device_address == nullptr) {\n+      return tensorflow::errors::InvalidArgument(\n+          StrCat(\"Unsupported data type encountered in input \", i));\n+    }\n+    device_buffers_.emplace(\n+        StrCat(\"InputPH_\", i),\n+        std::pair<void*, size_t>(device_address, device_tensor->TotalBytes()));\n+  }\n+  cres->calibrator_ =\n+      new TRTInt8Calibrator(device_buffers_, batch_size, name());\n+  cres->builder_->setInt8Calibrator(cres->calibrator_);\n+  string label(name());\n+  auto segment_graph = &segment_graph_;\n+  cres->thr_ = new std::thread([cres, label, segment_graph, shapes]() {\n+    VLOG(1) << \"Starting calibration thread, Calibration Resource @ \" << cres;\n+    auto s = tensorflow::tensorrt::convert::ConvertSubgraphToEngine(\n+        *segment_graph, cres->builder_, shapes, &cres->engine_,\n+        tensorflow::tensorrt::convert::INT8MODE);  // will loop until we", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 624, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "565640eae327b092edf43613f77ba5ab0747d20d", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "body": "It triggers calibrator pull, which continues until calibrator returns false;", "created_at": "2018-06-13T03:37:41Z", "updated_at": "2018-06-21T22:23:15Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194948401", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194948401"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194948401"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>It triggers calibrator pull, which continues until calibrator returns false;</p>", "body_text": "It triggers calibrator pull, which continues until calibrator returns false;", "in_reply_to_id": 194891832}