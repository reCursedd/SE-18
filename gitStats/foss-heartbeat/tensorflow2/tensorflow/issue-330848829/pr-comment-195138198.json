{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/195138198", "pull_request_review_id": 128390653, "id": 195138198, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTEzODE5OA==", "diff_hunk": "@@ -30,138 +37,548 @@ using IRuntime = nvinfer1::IRuntime;\n using Dims = nvinfer1::Dims;\n \n namespace tensorrt {\n+using tensorflow::strings::StrAppend;\n+using tensorflow::strings::StrCat;\n+// A helper class to call done() for asynchronous execution.\n+// Helps simultaneous execution of native and TRT engines.\n+class AsyncHelper : public tensorflow::core::RefCounted {\n+ public:\n+  AsyncHelper(tensorflow::AsyncOpKernel::DoneCallback done) { done_ = done; }\n+  ~AsyncHelper() override { done_(); }\n \n-TRTEngineOp::TRTEngineOp(OpKernelConstruction* context) : OpKernel(context) {\n+ private:\n+  tensorflow::AsyncOpKernel::DoneCallback done_;\n+};\n+\n+#define TYPECASE(dt, X, Y)                                                \\\n+  case dt: {                                                              \\\n+    return (void*)X->flat<tensorflow::EnumToDataType<dt>::Type>().data(); \\\n+  }\n+\n+void* GetTensorAddress(const Tensor* tensor_ptr) {\n+  auto tensor_type = tensor_ptr->dtype();\n+  switch (tensor_type) {\n+    TYPECASE(tensorflow::DT_FLOAT, tensor_ptr, dest_ptr);\n+    TYPECASE(tensorflow::DT_HALF, tensor_ptr, dest_ptr);\n+    TYPECASE(tensorflow::DT_INT8, tensor_ptr, dest_ptr);\n+    default: {\n+      LOG(ERROR) << \"Unsupported Data type \"\n+                 << tensorflow::DataTypeString(tensor_type);\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+tensorflow::Status TRTEngineOp::ConstructFunctionHandle(OpKernelContext* ctx) {\n+  VLOG(1) << \"Constructing function handle\";\n+  auto lib = ctx->function_library();\n+  if (lib == nullptr) {\n+    return tensorflow::errors::Internal(\"Context function library is null\");\n+  }\n+  auto fdef = lib->GetFunctionLibraryDefinition()->Find(funcdef_name_);\n+  if (fdef == nullptr) {\n+    return tensorflow::errors::Internal(\n+        StrCat(\"Native FunctionDef \", funcdef_name_,\n+               \" can't be found in function library\"));\n+  }\n+  tensorflow::FunctionLibraryRuntime::InstantiateOptions inst_ops;\n+  inst_ops.overlay_lib = nullptr;\n+  inst_ops.state_handle = \"\";\n+  inst_ops.target = ctx->device()->name();\n+  native_func_ = 0;\n+  auto status = lib->Instantiate(funcdef_name_, AttrSlice(&fdef->attr()),\n+                                 inst_ops, &native_func_);\n+  if (!status.ok()) {\n+    LOG(ERROR) << \" Instantiating native function \" << funcdef_name_\n+               << \" failed!\";\n+  }\n+  return status;\n+}\n+\n+TRTEngineOp::TRTEngineOp(OpKernelConstruction* context)\n+    : AsyncOpKernel(context) {\n   // read serialized_engine\n   OP_REQUIRES_OK(context,\n-                 context->GetAttr(\"serialized_engine\", &serialized_engine_));\n+                 context->GetAttr(\"serialized_segment\", &serialized_segment_));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"workspace_size_bytes\", &workspace_size_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"static_engine\", &static_engine_));\n+  if (!static_engine_) {\n+    if (!segment_graph_.ParseFromString(serialized_segment_)) {\n+      LOG(ERROR) << \"Parsing segment graph failed!\";\n+      context->SetStatus(tensorflow::errors::InvalidArgument(\n+          \"Failed to parse segment graphdef!\"));\n+      return;\n+    }\n+    serialized_segment_.resize(0);\n+  }\n \n-  // register input output node name in trt_sub_graph\n-  OP_REQUIRES_OK(context, context->GetAttr(\"input_nodes\", &input_nodes_));\n-  OP_REQUIRES_OK(context, context->GetAttr(\"output_nodes\", &output_nodes_));\n+  string precision_string;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"precision_mode\", &precision_string));\n+  string calibration_data;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"calibration_data\", &calibration_data));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"segment_funcdef_name\", &funcdef_name_));\n+  if (precision_string == \"FP32\") {\n+    precision_mode_ = tensorflow::tensorrt::convert::FP32MODE;\n+  } else if (precision_string == \"FP16\") {\n+    precision_mode_ = tensorflow::tensorrt::convert::FP16MODE;\n+  } else if (precision_string == \"INT8\") {\n+    precision_mode_ = tensorflow::tensorrt::convert::INT8MODE;\n+  }\n+  calibration_mode_ =\n+      precision_mode_ == tensorflow::tensorrt::convert::INT8MODE &&\n+      calibration_data.size() == 0;\n+  if (calibration_data.size()) {\n+    calibrator_.reset(new TRTInt8Calibrator(calibration_data));\n+    calibration_data.resize(0);\n+  }\n+  native_func_ = tensorflow::kInvalidHandle;\n+  OP_REQUIRES_OK(context, context->GetAttr(\"max_cached_engines_count\",\n+                                           &max_cached_engines_));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"fixed_input_size\", &fixed_input_size_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"cached_engine_batches\",\n+                                           &cached_engine_batches_));\n+  std::sort(cached_engine_batches_.begin(), cached_engine_batches_.end());\n+  if (VLOG_IS_ON(1)) {\n+    string s(\"Engine Batches= \");\n+    for (auto i : cached_engine_batches_) {\n+      StrAppend(&s, i, \" \");\n+    }\n+    VLOG(1) << s;\n+  }\n }\n \n-void TRTEngineOp::Compute(OpKernelContext* context) {\n-  // TODO(samikama) runtime should be taken from a resourcemanager as well.\n-  // Only engine should be in the op and context and runtime should be taken\n-  // from resourcemanager\n+void TRTEngineOp::ExecuteNativeSegment(tensorflow::OpKernelContext* ctx,\n+                                       AsyncHelper* helper) {\n+  if (!calibration_mode_) {\n+    VLOG(1) << \"Executing native engine\";\n+  }\n+  std::vector<Tensor> inputs;\n+  std::vector<Tensor>* outputs = new std::vector<Tensor>();\n+  if (native_func_ == tensorflow::kInvalidHandle) {\n+    auto status = ConstructFunctionHandle(ctx);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Couldn't construct function handle \" << funcdef_name_;\n+      ctx->SetStatus(status);\n+      return;\n+    }\n+  }\n+  auto lib = ctx->function_library();\n+  tensorflow::FunctionLibraryRuntime::Options opts;\n+  opts.step_id = ctx->step_id();\n+  opts.rendezvous = ctx->rendezvous();\n+  opts.cancellation_manager = ctx->cancellation_manager();\n+  opts.runner = ctx->runner();\n+  for (int i = 0; i < ctx->num_inputs(); i++) {\n+    inputs.push_back(ctx->input(i));\n+  }\n+  helper->Ref();  // Increment count for calculating native graph\n+  VLOG(1) << \"Executing native segment \" << name();\n+  lib->Run(opts, native_func_, inputs, outputs,\n+           [ctx, outputs, helper](const tensorflow::Status& s) {\n+             tensorflow::core::ScopedUnref sc(helper);\n+             VLOG(1) << \"Native Segment completed\";\n+             if (!s.ok()) {\n+               ctx->SetStatus(s);\n+               return;\n+             }\n+             for (size_t t = 0; t < outputs->size(); ++t) {\n+               ctx->set_output(t, outputs->at(t));\n+             }\n+             delete outputs;\n+             return;\n+           });\n+  return;\n+}\n \n-  if (!trt_execution_context_ptr_) {\n-    IRuntime* infer = nvinfer1::createInferRuntime(logger);\n-#if NV_TENSORRT_MAJOR > 3\n-    auto device = context->device();\n-    auto dev_allocator =\n-        device->GetAllocator(tensorflow::AllocatorAttributes());\n-    if (!dev_allocator) {\n-      LOG(FATAL) << \"Can't find device allocator for gpu device \"\n-                 << device->name();\n-    }\n-    allocator_ = std::make_shared<TRTDeviceAllocator>(dev_allocator);\n-    infer->setGpuAllocator(allocator_.get());\n-#endif\n-    trt_engine_ptr_.reset(infer->deserializeCudaEngine(\n-        serialized_engine_.c_str(), serialized_engine_.size(),\n-        PluginFactoryTensorRT::GetInstance()));\n-    trt_execution_context_ptr_.reset(trt_engine_ptr_->createExecutionContext());\n-    // Runtime is safe to delete after engine creation\n-    infer->destroy();\n-    serialized_engine_.clear();\n-  }\n-  int num_binding = context->num_inputs() + context->num_outputs();\n-  std::vector<void*> buffers(num_binding);\n+void TRTEngineOp::ExecuteCalibration(tensorflow::OpKernelContext* ctx,\n+                                     AsyncHelper* helper) {\n+  tensorflow::core::ScopedUnref sc(helper);\n+  auto TRT_RM = tensorflow::tensorrt::TRTResourceManager::instance();\n+  auto res_mgr = TRT_RM->getManager(\"TRTCalibration\");\n+  tensorflow::tensorrt::TRTCalibrationResource* calib_res = nullptr;\n+  auto status = res_mgr->LookupOrCreate(\n+      funcdef_name_, \"Calibrator\", &calib_res,\n+      {[ctx, this](tensorflow::tensorrt::TRTCalibrationResource** cr)\n+           -> tensorflow::Status {\n+        return this->AllocateCalibrationResources(ctx, cr);\n+      }});\n+  if (!status.ok()) {\n+    ctx->SetStatus(status);\n+    return;\n+  }\n+  ExecuteNativeSegment(ctx, helper);\n+  int num_inputs = ctx->num_inputs();\n+  // Pass input data to calibrator\n+  std::unordered_map<string, void*> input_data;\n+  for (int i = 0; i < num_inputs; i++) {\n+    const Tensor& t = ctx->input(i);\n+    void* data_address = GetTensorAddress(&t);\n+    if (data_address == nullptr) {\n+      ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+          StrCat(\"Unsupported data type encountered in input \", i)));\n+      return;\n+    }\n+    const auto device_tensor = dev_tensors_.at(i).AccessTensor(ctx);\n+    CHECK_EQ(t.TotalBytes(),\n+             device_tensor->TotalBytes());  // use the tensor so FW keeps it\n+    input_data.emplace(StrCat(kInputPHName, i), data_address);\n+  }\n+  VLOG(2) << \"Filled map for sending\";\n+  // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n+  const cudaStream_t* stream = CHECK_NOTNULL(\n+      reinterpret_cast<const cudaStream_t*>(ctx->op_device_context()\n+                                                ->stream()\n+                                                ->implementation()\n+                                                ->CudaStreamMemberHack()));\n+  calib_res->calibrator_->setBatch(input_data, *stream);\n+  VLOG(2) << \"Passed calibration data\";\n+  return;\n+}\n \n+int TRTEngineOp::GetEngineBatch(tensorflow::OpKernelContext* ctx) {\n+  int num_batch = ctx->input(0).shape().dim_size(0);\n+  int smallest_engine = 0;\n+  for (const auto i : cached_engine_batches_) {\n+    if (i >= num_batch) {\n+      smallest_engine = i;\n+      break;\n+    }\n+  }\n+  // TODO(sami): Need an LRU here\n+  if (smallest_engine == 0) {\n+    if (max_cached_engines_ > cached_engine_batches_.size()) {\n+      smallest_engine = num_batch;\n+      cached_engine_batches_.push_back(num_batch);\n+      VLOG(1) << \"Running with batch size \" << num_batch;\n+    } else {\n+      string s(\"Engine buffer is full. buffer limit= \");\n+      StrAppend(&s, max_cached_engines_, \", current entries= \");\n+      for (auto i : cached_engine_batches_) StrAppend(&s, i, \", \");\n+      StrAppend(&s, \"Requested batch= \", num_batch);\n+      LOG(ERROR) << s;\n+      ctx->SetStatus(tensorflow::errors::ResourceExhausted(\n+          \"Requested batch size is not available and engine cache is full\"));\n+      return -1;\n+    }\n+  }\n+  return smallest_engine;\n+}\n+\n+void TRTEngineOp::ComputeAsync(tensorflow::OpKernelContext* ctx,\n+                               tensorflow::AsyncOpKernel::DoneCallback done) {\n+  auto helper = new AsyncHelper(done);\n+  tensorflow::core::ScopedUnref sc(helper);\n+  if (calibration_mode_) {\n+    helper->Ref();\n+    ExecuteCalibration(ctx, helper);\n+    return;\n+  }\n+  int num_binding = ctx->num_inputs() + ctx->num_outputs();\n+  std::vector<void*> buffers(num_binding);\n+  int smallest_engine = GetEngineBatch(ctx);\n+  if (smallest_engine < 0) return;\n+  int num_batch = ctx->input(0).shape().dim_size(0);\n   size_t binding_index;\n-  int num_batch = 0;\n-  for (int i = 0; i < context->num_inputs(); i++) {\n-    // Grab the input tensor\n-    binding_index = trt_engine_ptr_->getBindingIndex(input_nodes_[i].c_str());\n+  auto engine_ctx_pair = GetEngine(smallest_engine, ctx, fixed_input_size_);\n+  auto trt_engine_ptr = engine_ctx_pair.first;\n+  if (!trt_engine_ptr) {\n+    LOG(WARNING) << \"Engine retrieval for batch size \" << num_batch\n+                 << \" failed Running native segment\";\n+    ExecuteNativeSegment(ctx, helper);\n+    return;\n+  }\n+  for (int i = 0; i < ctx->num_inputs(); i++) {\n+    string inp_name = StrCat(kInputPHName, i);\n+    binding_index = trt_engine_ptr->getBindingIndex(inp_name.c_str());\n \n-    const Tensor& input_tensor = context->input(i);\n+    const Tensor& input_tensor = ctx->input(i);\n     const TensorShape& input_shape = input_tensor.shape();\n-    if (i == 0) {\n-      num_batch = input_shape.dim_size(0);\n-      if (num_batch > trt_engine_ptr_->getMaxBatchSize()) {\n-        LOG(FATAL) << \"input tensor batch larger than max_batch_size: \"\n-                   << trt_engine_ptr_->getMaxBatchSize();\n-      }\n-    } else if (num_batch != input_shape.dim_size(0)) {\n-      LOG(FATAL) << \"input data inconsistent batch size\";\n-      break;\n+    if (num_batch != input_shape.dim_size(0)) {\n+      LOG(ERROR) << \"input data inconsistent batch size\";\n+      ctx->SetStatus(tensorflow::errors::FailedPrecondition(\n+          \"Different batch sizes between input tensors\"));\n+      return;\n     }\n-    auto dtype = trt_engine_ptr_->getBindingDataType(binding_index);\n+    auto dtype = trt_engine_ptr->getBindingDataType(binding_index);\n     switch (dtype) {\n       case nvinfer1::DataType::kFLOAT:\n         buffers[binding_index] = (void*)(input_tensor.flat<float>().data());\n         break;\n       case nvinfer1::DataType::kHALF:\n-        LOG(FATAL) << \"half size is not supported yet!\";\n-        break;\n+        LOG(ERROR) << \"FP16 inputs are not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"FP16 inputs are not supported!\"));\n+        return;\n       case nvinfer1::DataType::kINT8:\n-        LOG(FATAL) << \"int8 is not supported yet!\";\n-        break;\n+        LOG(ERROR) << \"INT8 inputs are not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"INT8 inputs are not supported!\"));\n+        return;\n       default:\n-        LOG(FATAL) << \"Unknown data type: \" << int(dtype);\n-        break;\n+        LOG(ERROR) << \"Unknown TRT data type: \" << int(dtype);\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Unknown ouput TRT data type! \" + int(dtype)));\n+        return;\n     }\n   }\n \n-  for (int i = 0; i < static_cast<int>(output_nodes_.size()); i++) {\n+  for (int i = 0; i < ctx->num_outputs(); i++) {\n     // This is bad that we have to reallocate output buffer every run.\n     // Create an output tensor\n-    binding_index = trt_engine_ptr_->getBindingIndex(output_nodes_[i].c_str());\n+    \n+    auto output_name=StrCat(kOutputPHName, i);\n+    binding_index = trt_engine_ptr->getBindingIndex(output_name.c_str());\n     Tensor* output_tensor = nullptr;\n \n     TensorShape output_shape;\n     if (binding_index != -1) {\n-      auto dims = trt_engine_ptr_->getBindingDimensions(binding_index);\n+      auto dims = trt_engine_ptr->getBindingDimensions(binding_index);\n       std::vector<int> trt_shape(dims.nbDims + 1);\n       trt_shape[0] = num_batch;\n       for (int j = 0; j < dims.nbDims; j++) trt_shape[j + 1] = dims.d[j];\n-      OP_REQUIRES_OK(context,\n-                     TensorShapeUtils::MakeShape(\n-                         trt_shape.data(), trt_shape.size(), &output_shape));\n+      OP_REQUIRES_OK(\n+          ctx, TensorShapeUtils::MakeShape(trt_shape.data(), trt_shape.size(),\n+                                           &output_shape));\n     } else {\n-      LOG(FATAL) << \"output node not found, at \" << output_nodes_[i];\n-      break;\n+      LOG(ERROR) << \"output node not found, at \" << output_name;\n+      ctx->SetStatus(tensorflow::errors::Internal(\"output \" + output_name +\n+                                                  \" but couldn't be found!\"));\n+      return;\n     }\n-\n-    OP_REQUIRES_OK(context,\n-                   context->allocate_output(i, output_shape, &output_tensor));\n-    auto dtype = trt_engine_ptr_->getBindingDataType(binding_index);\n+    auto status = ctx->allocate_output(i, output_shape, &output_tensor);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Allocating output failed with \" << status;\n+      ctx->SetStatus(status);\n+      return;\n+    }\n+    auto dtype = trt_engine_ptr->getBindingDataType(binding_index);\n     switch (dtype) {\n       case nvinfer1::DataType::kFLOAT:\n         buffers[binding_index] =\n             reinterpret_cast<void*>(output_tensor->flat<float>().data());\n         break;\n       case nvinfer1::DataType::kHALF:\n-        LOG(FATAL) << \"half size is not supported yet!\";\n-        break;\n+        LOG(ERROR) << \"half size is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Half outputs are not supported!\"));\n+        return;\n       case nvinfer1::DataType::kINT8:\n-        LOG(FATAL) << \"int8 is not supported yet!\";\n-        break;\n+        LOG(ERROR) << \"int8 is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"INT8 outputs are not supported!\"));\n+        return;\n       default:\n-        LOG(FATAL) << \"Unknown data type: \" << int(dtype);\n-        break;\n+        LOG(ERROR) << \"Unknown TRT data type: \" << int(dtype);\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Unsupported output data type! \" + int(dtype)));\n+        return;\n     }\n   }\n   // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n   const cudaStream_t* stream = CHECK_NOTNULL(\n-      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\n+      reinterpret_cast<const cudaStream_t*>(ctx->op_device_context()\n                                                 ->stream()\n                                                 ->implementation()\n                                                 ->CudaStreamMemberHack()));\n \n   // TODO(jie): trt enqueue does not return error\n-  auto ret = trt_execution_context_ptr_->enqueue(num_batch, &buffers[0],\n-                                                 *stream, nullptr);\n+  auto trt_execution_context_ptr = engine_ctx_pair.second;\n+  auto ret = trt_execution_context_ptr->enqueue(num_batch, &buffers[0], *stream,\n+                                                nullptr);\n   VLOG(2) << \"enqueue returns: \" << ret;\n   // sync should be done by TF.\n }\n+\n TRTEngineOp::~TRTEngineOp() {\n   // Order matters!\n-  trt_execution_context_ptr_.reset();\n-  trt_engine_ptr_.reset();\n-  allocator_.reset();\n+  for (auto eng : engine_map_) {\n+    eng.second.first.reset();\n+    eng.second.second.reset();\n+  }\n+  for (auto alloc : allocators_) alloc.second.reset();\n+}\n+nvinfer1::IGpuAllocator* TRTEngineOp::GetAllocator(OpKernelContext* ctx) {\n+  auto device = ctx->device();\n+  const auto& device_name = device->name();\n+  if (allocators_.count(device_name)) {\n+    return allocators_.at(device_name).get();\n+  }\n+  auto dev_allocator = device->GetAllocator(tensorflow::AllocatorAttributes());\n+  if (!dev_allocator) {\n+    LOG(ERROR) << \"Can't find device allocator for gpu device \"\n+               << device->name();\n+    ctx->SetStatus(tensorflow::errors::Internal(\n+        StrCat(\"Can't get device allocator for device \", device_name)));\n+    return nullptr;\n+  }\n+  auto allocator = std::make_shared<TRTDeviceAllocator>(dev_allocator);\n+  allocators_.insert({device_name, allocator});\n+  return allocator.get();\n+}\n+\n+TRTEngineOp::EngineCtxPair TRTEngineOp::GetEngine(int batch_size,\n+                                                  OpKernelContext* ctx,\n+                                                  bool ignore_dim_change) {\n+  // TODO(sami): This method needs to be re-written to use resource manager and\n+  // with LRU mechanism option.\n+  tensorflow::mutex_lock lock(engine_mutex_);\n+  if (static_engine_) {\n+    if (engine_map_.size()) {\n+      if (engine_map_.begin()->first >= batch_size) {\n+        return engine_map_.begin()->second;\n+      } else {\n+        return {nullptr, nullptr};\n+      }\n+    } else {\n+      IRuntime* infer = nvinfer1::createInferRuntime(logger);\n+#if NV_TENSORRT_MAJOR > 3\n+      auto allocator = GetAllocator(ctx);\n+      if (allocator == nullptr) {\n+        return {nullptr, nullptr};\n+      };\n+      infer->setGpuAllocator(allocator);\n+#endif\n+      std::shared_ptr<nvinfer1::ICudaEngine> static_engine(\n+          infer->deserializeCudaEngine(serialized_segment_.c_str(),\n+                                       serialized_segment_.size(), nullptr),\n+          Destroyer<nvinfer1::ICudaEngine>());\n+      engine_map_.insert({static_engine->getMaxBatchSize(),\n+                          {static_engine,\n+                           {static_engine->createExecutionContext(),\n+                            Destroyer<nvinfer1::IExecutionContext>()}}});\n+      // Runtime is safe to delete after engine creation\n+      infer->destroy();\n+      serialized_segment_.clear();\n+      if (static_engine->getMaxBatchSize() < batch_size) {\n+        return {nullptr, nullptr};\n+      }\n+      return engine_map_.at(static_engine->getMaxBatchSize());\n+    }\n+  } else {\n+    auto engine_it = engine_map_.find(batch_size);\n+    if (engine_it == engine_map_.end() &&\n+        engine_map_.size() < (size_t)max_cached_engines_) {\n+      auto builder = std::shared_ptr<nvinfer1::IBuilder>(\n+          nvinfer1::createInferBuilder(logger),\n+          Destroyer<nvinfer1::IBuilder>());  // reset the builder to ensure\n+                                             // device is correct\n+#if NV_TENSORRT_MAJOR > 3\n+      auto allocator = GetAllocator(ctx);\n+      if (allocator == nullptr) {\n+        return {nullptr, nullptr};\n+      }\n+      builder->setGpuAllocator(GetAllocator(ctx));\n+#endif\n+      VLOG(1) << name() << \" Constructing a new engine with batch size \"\n+              << batch_size;\n+      builder->setMaxBatchSize(batch_size);\n+      if (precision_mode_ == tensorflow::tensorrt::convert::FP16MODE) {\n+        builder->setHalf2Mode(true);\n+      } else if (precision_mode_ == tensorflow::tensorrt::convert::INT8MODE) {\n+        builder->setInt8Mode(true);\n+        builder->setInt8Calibrator(calibrator_.get());\n+      }\n+      builder->setMaxWorkspaceSize(workspace_size_);\n+      nvinfer1::ICudaEngine* engine = nullptr;\n+      std::vector<tensorflow::PartialTensorShape> shapes;\n+      for (int i = 0; i < ctx->num_inputs(); ++i) {\n+        shapes.emplace_back(ctx->input(i).shape());\n+      }\n+      auto status = tensorflow::tensorrt::convert::ConvertSubgraphToEngine(\n+          segment_graph_, builder.get(), shapes, &engine, precision_mode_);\n+      if (engine) {\n+        engine_map_[batch_size] = {\n+            std::shared_ptr<nvinfer1::ICudaEngine>(\n+                engine, Destroyer<nvinfer1::ICudaEngine>()),\n+            std::shared_ptr<nvinfer1::IExecutionContext>(\n+                engine->createExecutionContext(),\n+                Destroyer<nvinfer1::IExecutionContext>())};\n+      } else {\n+        LOG(ERROR) << \"Engine creation for batch size \" << batch_size\n+                   << \" failed\";\n+        ctx->SetStatus(tensorflow::errors::Internal(\"Engine creation failed!\"));\n+        engine_map_[batch_size] = {nullptr, nullptr};\n+        return {nullptr, nullptr};\n+      }\n+    }\n+    return engine_map_.at(batch_size);\n+  }\n+}\n+\n+tensorflow::Status TRTEngineOp::AllocateCalibrationResources(\n+    tensorflow::OpKernelContext* ctx,\n+    tensorflow::tensorrt::TRTCalibrationResource** cr) {\n+  auto cres = new TRTCalibrationResource();\n+  *cr = cres;\n+  cres->logger_ = new tensorflow::tensorrt::Logger();\n+  cres->builder_ = nvinfer1::createInferBuilder(*(cres->logger_));\n+#if NV_TENSORRT_MAJOR > 3\n+  auto dev = ctx->device();\n+  auto dev_allocator = dev->GetAllocator(tensorflow::AllocatorAttributes());\n+  if (!dev_allocator) {\n+    LOG(WARNING) << \"Can't get device allocator will not be able to \"\n+                    \"allocate memory from TensorFlow memory pool\";\n+    cres->allocator_ =\n+        std::make_shared<tensorflow::tensorrt::TRTCudaAllocator>();\n+  } else {\n+    cres->allocator_ =\n+        std::make_shared<tensorflow::tensorrt::TRTDeviceAllocator>(\n+            dev_allocator);\n+  }\n+  cres->builder_->setGpuAllocator(cres->allocator_.get());\n+#endif\n+  int batch_size = ctx->input(0).dim_size(0);\n+  cres->builder_->setMaxBatchSize(batch_size);\n+  cres->builder_->setInt8Mode(true);\n+  cres->builder_->setMaxWorkspaceSize(workspace_size_);\n+  cres->engine_ = nullptr;\n+  std::vector<tensorflow::PartialTensorShape> shapes;\n+  int num_inputs = ctx->num_inputs();\n+  // first run instantiate calibrator\n+  dev_tensors_.resize(num_inputs);\n+  VLOG(1) << \" Constructing calibrator\";\n+  for (int i = 0; i < num_inputs; i++) {\n+    // allocate workspace on device for inputs\n+    const tensorflow::Tensor& t = ctx->input(i);\n+    shapes.emplace_back(t.shape());\n+    Tensor* device_tensor;\n+    TF_RETURN_IF_ERROR(ctx->allocate_persistent(t.dtype(), t.shape(),\n+                                                &dev_tensors_.at(i), &device_tensor));\n+    CHECK_EQ(t.TotalBytes(), device_tensor->TotalBytes());\n+    void* device_address = GetTensorAddress(device_tensor);\n+    if (device_address == nullptr) {\n+      return tensorflow::errors::InvalidArgument(\n+          StrCat(\"Unsupported data type encountered in input \", i));\n+    }\n+    device_buffers_.emplace(\n+        StrCat(kInputPHName, i),\n+        std::pair<void*, size_t>(device_address, device_tensor->TotalBytes()));\n+  }\n+  cres->calibrator_ =\n+      new TRTInt8Calibrator(device_buffers_, batch_size, name());\n+  cres->builder_->setInt8Calibrator(cres->calibrator_);\n+  string label(name());\n+  auto segment_graph = &segment_graph_;\n+  cres->thr_ = new std::thread([cres, label, segment_graph, shapes]() {\n+    VLOG(1) << \"Starting calibration thread, Calibration Resource @ \" << cres;\n+    auto s = tensorflow::tensorrt::convert::ConvertSubgraphToEngine(\n+        *segment_graph, cres->builder_, shapes, &cres->engine_,\n+        tensorflow::tensorrt::convert::INT8MODE);  // calibrator will loop until we", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 629, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "d56675f6662b651dc996786262a6d28ccf9e06e7", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "I think I'm still confused, please add more comment on how this works. More specifically, which nvinfer method called by this ConvertSubgraphToEngine() will trigger the calibrator loop? And who terminates the calibration (I know this answer but other developers may not)? And what happen to ConvertSubgraphToEngine and here after that?", "created_at": "2018-06-13T15:49:52Z", "updated_at": "2018-06-21T22:23:15Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r195138198", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/195138198"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r195138198"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>I think I'm still confused, please add more comment on how this works. More specifically, which nvinfer method called by this ConvertSubgraphToEngine() will trigger the calibrator loop? And who terminates the calibration (I know this answer but other developers may not)? And what happen to ConvertSubgraphToEngine and here after that?</p>", "body_text": "I think I'm still confused, please add more comment on how this works. More specifically, which nvinfer method called by this ConvertSubgraphToEngine() will trigger the calibrator loop? And who terminates the calibration (I know this answer but other developers may not)? And what happen to ConvertSubgraphToEngine and here after that?"}