{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194908769", "pull_request_review_id": 128071489, "id": 194908769, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDkwODc2OQ==", "diff_hunk": "@@ -2140,562 +2144,238 @@ void Converter::register_op_converters() {\n \n }  // namespace\n \n-tensorflow::Status ConvertCalibrationNodeToEngineNode(\n-    tensorflow::Graph& graph, tensorflow::Node* c_node) {\n-  const auto ndef = c_node->def();\n-\n-  TFAttrs attrs(ndef);\n-  std::vector<string> segment_nodes(\n-      attrs.get<std::vector<string>>(\"segment_nodes\"));\n-  std::vector<string> output_nodes(\n-      attrs.get<std::vector<string>>(\"segment_output_names\"));\n-  std::vector<string> input_names(\n-      attrs.get<std::vector<string>>(\"input_names\"));\n-  string res_name = attrs.get<string>(\"resource_name\");\n-  VLOG(1) << \"Node name \" << c_node->name() << \" res_name \" << res_name;\n-  string engine_name = \"my_trt_op\";\n-  {\n-    const auto node_id = tensorflow::str_util::Split(res_name, \"_\");\n-    engine_name += node_id.back();\n-  }\n-  std::map<string, tensorflow::Node*> node_maps;\n-\n-  for (auto n : graph.op_nodes()) {\n-    node_maps.insert({n->name(), n});\n-  }\n-  std::set<int> subgraph_ids;\n-  for (const auto internal_node : segment_nodes) {\n-    subgraph_ids.insert(node_maps.at(internal_node)->id());\n-  }\n-  if (VLOG_IS_ON(2)) {\n-    string node_names = StrCat(c_node->name(), \" segment nodes= \");\n-\n-    for (const auto& node_name : segment_nodes) {\n-      StrAppend(&node_names, node_name, \", \");\n-    }\n-    VLOG(2) << node_names;\n+// Converts given subgraph to a TRT engine.\n+tensorflow::Status ConvertSubgraphToEngine(\n+    const tensorflow::GraphDef& gdef, nvinfer1::IBuilder* builder,\n+    const std::vector<tensorflow::PartialTensorShape>& input_shapes,\n+    nvinfer1::ICudaEngine** engine, int precision_mode) {\n+  auto trt_network = infer_object(builder->createNetwork());\n+  if (!trt_network) {\n+    return tensorflow::errors::Internal(\n+        \"Failed to create TensorRT network object\");\n   }\n-\n-  VLOG(1) << \"Output Nodes:\";\n-  std::vector<tensorflow::DataType> out_types;\n-  std::vector<const tensorflow::Edge*> out_edges;\n-\n-  for (auto& i : output_nodes) {\n-    auto node_port = tensorflow::str_util::Split(i, \":\");\n-    VLOG(1) << \" \" << i << \" in graph \" << node_maps.count(i);\n-    auto out_node_name = node_port.at(0);\n-    if (node_port.size() > 1) {\n-      VLOG(1) << \"Multi port output\" << node_port.at(0) << \" \"\n-              << node_port.at(1) << \" size=\" << node_port.size();\n-    }\n-    auto node_it = node_maps.find(out_node_name);\n-    if (node_it != node_maps.end()) {\n-      tensorflow::Node* out_node = node_it->second;\n-      int port = 0;\n-      if (node_port.size() == 2) {\n-        port = std::strtoul(node_port.at(1).c_str(), nullptr, 10);\n-        out_types.push_back(out_node->output_type(port));\n-      } else {\n-        out_types.push_back(out_node->output_type(0));\n+  auto ws = std::unique_ptr<tensorflow::tensorrt::TRTWeightStore>(\n+      new TRTWeightStore());\n+  // Build the network\n+  VLOG(1) << \"Starting engine conversion \";\n+  Converter converter(trt_network.get(), ws.get(), precision_mode == FP16MODE);\n+  std::vector<std::pair<string, string>> output_tensors;\n+  for (const auto& node_def : gdef.node()) {\n+    string node_name = node_def.name();\n+    VLOG(1) << \"Converting op name=\" << node_name << \", op=\" << node_def.op();\n+    if (tensorflow::str_util::StartsWith(node_name, \"InputPH_\") &&\n+        (node_def.op() == \"Placeholder\")) {\n+      nvinfer1::DimsCHW input_dim_pseudo_chw;\n+      for (int i = 0; i < 8; i++) input_dim_pseudo_chw.d[i] = 0;\n+      nvinfer1::DataType dtype(nvinfer1::DataType::kFLOAT);\n+      auto type_status =\n+          ConvertDType(node_def.attr().at(\"dtype\").type(), &dtype);\n+      if (type_status != tensorflow::Status::OK()) {\n+        LOG(WARNING) << \"Type conversion failed for \" << node_name;\n+        return type_status;\n+      }\n+      int32 slot_number = -1;\n+      if (!tensorflow::strings::safe_strto32(node_name.c_str() + 8,\n+                                             &slot_number)) {\n+        LOG(ERROR) << \"Failed to parse slot number from \" << node_name\n+                   << \" +8= \" << node_name.c_str() + 8;\n       }\n-      for (auto out_edge : out_node->out_edges()) {\n-        if (subgraph_ids.count(out_edge->dst()->id()))\n-          continue;  // skip internal edges;\n-        if (out_edge->src_output() == port) {\n-          out_edges.push_back(out_edge);\n-          VLOG(1) << \"OUTPUT EDGE \" << out_edge->src()->name() << \":\"\n-                  << out_edge->src_output() << \" -> \" << out_edge->dst()->name()\n-                  << \":\" << out_edge->dst_input();\n+      auto shape = input_shapes.at(slot_number);\n+      if (shape.dims() > 8) {\n+        LOG(ERROR) << \"Tensor rank is greater than 8 for \" << node_name\n+                   << \" at input slot \" << slot_number;\n+        return tensorflow::errors::OutOfRange(\n+            \"Input tensor rank is greater than 8\");\n+      }\n+      if (VLOG_IS_ON(1)) {\n+        string dim_str(\"dims=\");\n+        StrAppend(&dim_str, \"[ \", shape.dim_size(0));\n+        for (int i = 1; i < shape.dims(); i++) {\n+          StrAppend(&dim_str, \", \", shape.dim_size(i));\n+          input_dim_pseudo_chw.d[i - 1] = shape.dim_size(i);\n+        }\n+        StrAppend(&dim_str, \" ]\");\n+        VLOG(1) << dim_str;\n+      } else {\n+        for (int i = 1; i < shape.dims(); i++) {\n+          input_dim_pseudo_chw.d[i - 1] = shape.dim_size(i);\n         }\n       }\n+      input_dim_pseudo_chw.nbDims = shape.dims() - 1;\n+      nvinfer1::ITensor* input_tensor = converter.network()->addInput(\n+          node_name.c_str(), dtype, input_dim_pseudo_chw);\n+      if (!input_tensor) {\n+        return tensorflow::errors::InvalidArgument(\n+            StrCat(\"Failed to create Input layer tensor \", node_name,\n+                   \" rank=\", shape.dims()-1));\n+      }\n+      VLOG(1) << \"Input tensor name :\" << node_name;\n+      if (!converter.insert_input_tensor(node_name, input_tensor)) {\n+        return tensorflow::errors::AlreadyExists(\n+            \"Output tensor already exists for op: \" + node_name);\n+      }\n+    } else if (tensorflow::str_util::StartsWith(node_name, \"OutputPH_\") &&\n+               (node_def.op() == \"Identity\")) {\n+      tensorflow::int32 slot_number = -1;\n+      if (!tensorflow::strings::safe_strto32(node_name.c_str() + 9,\n+                                             &slot_number)) {\n+        LOG(ERROR) << \"Failed to parse slot number from \" << node_name\n+                   << \" +9=\" << node_name.c_str() + 9;\n+      }\n+      if (output_tensors.size() <= slot_number)\n+        output_tensors.resize(slot_number + 1);\n+      output_tensors.at(slot_number) = {node_def.input(0), node_name};\n     } else {\n-      LOG(WARNING) << \" couldn't find output node \" << out_node_name;\n-    }\n-  }\n-  if (VLOG_IS_ON(1)) {\n-    VLOG(1) << c_node->name() << \" Input Nodes:\";\n-    for (auto& i : input_names) {\n-      VLOG(1) << \" Input \" << i << \" in graph \" << node_maps.count(i);\n+      VLOG(2) << \"Converting node: \" << node_def.name() << \" , \"\n+              << node_def.op();\n+      TF_RETURN_IF_ERROR(converter.convert_node(node_def));\n     }\n   }\n-  auto trt_rm = tensorflow::tensorrt::TRTResourceManager::instance();\n-  auto resmgr = trt_rm->getManager(\"TRTCalibOps\");\n-  tensorflow::tensorrt::TRTCalibrationResource* calib_res = nullptr;\n-  auto status = resmgr->Lookup(res_name, res_name, &calib_res);\n-  if (!status.ok() || !calib_res->calibrator_) {\n-    return tensorflow::errors::FailedPrecondition(\n-        \"You must run calibration\"\n-        \" and inference conversion in the same process\");\n-  }\n-\n-  calib_res->calibrator_->setDone();\n-  calib_res->thr_->join();\n-  delete calib_res->thr_;\n-  if (!calib_res->engine_) {\n-    LOG(ERROR) << \"Calibration failed!, engine does not exist. Did you run \"\n-                  \"calibration graph?\";\n-    return tensorflow::errors::FailedPrecondition(\n-        \"Calibration graph needs to be executed on\"\n-        \" calibration data before convertsion to inference graph\");\n-  }\n-  auto weight_rmgr = trt_rm->getManager(\"WeightStore\");\n-  TF_CHECK_OK(weight_rmgr->Delete<tensorflow::tensorrt::TRTWeightStore>(\n-      res_name, res_name));\n-  auto engine_plan = calib_res->engine_->serialize();\n-  calib_res->engine_->destroy();\n-  calib_res->network_->destroy();\n-  calib_res->builder_->destroy();\n-  calib_res->thr_ = nullptr;\n-  calib_res->engine_ = nullptr;\n-  calib_res->builder_ = nullptr;\n-  tensorflow::NodeDefBuilder op_builder(engine_name, \"TRTEngineOp\");\n-  std::vector<tensorflow::NodeDefBuilder::NodeOut> income_edges;\n-  income_edges.resize(c_node->num_inputs());\n-  for (const auto in_edge : c_node->in_edges()) {\n-    auto src = in_edge->src();\n-    int dest_port = in_edge->dst_input();\n-    VLOG(1) << \"Incoming connection \" << src->name() << \":\"\n-            << in_edge->src_output() << \" -> \" << c_node->name() << \":\"\n-            << dest_port;\n-    income_edges.at(dest_port) = {src->name(), in_edge->src_output(),\n-                                  c_node->input_type(dest_port)};\n-  }\n-  tensorflow::gtl::ArraySlice<tensorflow::NodeDefBuilder::NodeOut> input_list(\n-      income_edges);\n-  if (VLOG_IS_ON(2)) {\n-    for (const auto& inp : input_list) {\n-      VLOG(2) << \" Input from inputlist \" << inp.node << \":\" << inp.index << \" \"\n-              << tensorflow::DataTypeString(inp.data_type);\n-    }\n-  }\n-  op_builder.Input(input_list);\n-  tensorflow::NodeDef engine_node;\n-  const char* engine_plan_data = static_cast<const char*>(engine_plan->data());\n-  string engine_plan_string(engine_plan_data,\n-                            engine_plan_data + engine_plan->size());\n-  status = op_builder.Attr(\"serialized_engine\", engine_plan_string)\n-               .Attr(\"input_nodes\", input_names)\n-               .Attr(\"output_nodes\", output_nodes)\n-               .Attr(\"OutT\", out_types)\n-               .Finalize(&engine_node);\n-  if (!status.ok()) {\n-    LOG(ERROR) << \"Engine Node creation failed\";\n-    return status;\n-  }\n-  auto trt_engine_node = graph.AddNode(engine_node, &status);\n-  TF_RETURN_IF_ERROR(status);\n-  std::map<string, int> port_map;\n-  for (size_t t = 0; t < output_nodes.size(); t++) {\n-    port_map.insert({output_nodes.at(t), t});\n-  }\n-  for (auto& i : out_edges) {\n-    string s(i->src()->name());\n-    if (i->src_output()) StrAppend(&s, \":\", i->src_output());\n-    int out_port = port_map.at(s);\n-    VLOG(1) << \"Connecting \" << trt_engine_node->name() << \":\" << out_port\n-            << \" -> \" << i->dst()->name() << \":\" << i->dst_input();\n-    TF_RETURN_IF_ERROR(\n-        graph.UpdateEdge(trt_engine_node, out_port, i->dst(), i->dst_input()));\n-  }\n-  for (const auto ed : trt_engine_node->in_edges()) {\n-    VLOG(1) << \"In Edge  \" << ed->src()->name() << \":\" << ed->src_output()\n-            << \" -> \" << ed->dst()->name() << \":\" << ed->dst_input();\n-  }\n-  for (const auto ed : trt_engine_node->out_edges()) {\n-    VLOG(1) << \"Out Edge \" << ed->src()->name() << \":\" << ed->src_output()\n-            << \" -> \" << ed->dst()->name() << \":\" << ed->dst_input();\n-  }\n-  VLOG(1) << \"Segment nodes:\";\n-  for (auto& i : segment_nodes) {\n-    VLOG(1) << \" \" << i << \" in graph \" << node_maps.count(i);\n-    auto it = node_maps.find(i);\n-    if (it != node_maps.end()) {\n-      graph.RemoveNode(it->second);\n-    }\n-  }\n-  graph.RemoveNode(c_node);\n-  return tensorflow::Status::OK();\n-}\n-\n-tensorflow::Status ReverseTopologicalSort(\n-    const tensorrt::convert::SubGraphParams& s,\n-    std::list<tensorflow::Node*>* order) {\n-  std::vector<tensorflow::Node*> order_vec;\n-  tensorflow::GetPostOrder(s.graph, &order_vec);\n-  // Select just the subgraph\n-  for (tensorflow::Node* node : order_vec) {\n-    if (s.subgraph_node_ids.count(node->id())) {\n-      // We want topological order to contstruct the\n-      // network layer by layer\n-      order->push_front(node);\n-    }\n-  }\n-  return tensorflow::Status::OK();\n-}\n-\n-tensorflow::Status SetInputList(\n-    const tensorrt::convert::SubGraphParams& s,\n-    tensorflow::NodeDefBuilder* op_builder,\n-    const std::vector<string>* input_names,\n-    std::vector<tensorflow::DataType>* input_dtypes) {\n-  std::vector<tensorflow::NodeDefBuilder::NodeOut> income_edges;\n-  VLOG(2) << \"input edge size: \" << input_names->size();\n-  for (size_t i = 0; i < input_names->size(); ++i) {\n-    VLOG(2) << \"input edges: \" << i << \" \" << input_names->at(i);\n-    int output_idx = s.input_inds.at(i).second;\n-    // we wired up the input here already, it is redundant to do it again in\n-    //  ConvertSubGraphToTensorRT(convert_graph.cc)\n-    auto incoming_edge = tensorflow::NodeDefBuilder::NodeOut(\n-        input_names->at(i), output_idx, input_dtypes->at(i));\n-    income_edges.push_back(incoming_edge);\n-  }\n-  tensorflow::gtl::ArraySlice<tensorflow::NodeDefBuilder::NodeOut> input_list(\n-      income_edges);\n-  op_builder->Input(input_list);\n-  return tensorflow::Status::OK();\n-}\n-\n-string SubgraphNameScopeGenerator(const std::list<tensorflow::Node*>* order) {\n-  string subgraph_name_scope;\n-  if (!order->empty()) {\n-    subgraph_name_scope = order->front()->name();\n-  }\n-  for (const tensorflow::Node* node : *order) {\n-    subgraph_name_scope = GetCommonNameScope(subgraph_name_scope, node->name());\n-  }\n-  // TODO(sami,ben,jie): proper naming!\n-  return subgraph_name_scope;\n-}\n-\n-tensorflow::Status ConvertSubgraph(\n-    Converter& converter, tensorrt::convert::SubGraphParams& s,\n-    std::list<tensorflow::Node*>* order, std::vector<string>* input_names,\n-    std::vector<tensorflow::DataType>* input_dtypes,\n-    std::vector<string>* output_names,\n-    std::vector<tensorflow::DataType>* output_dtypes,\n-    const string& engine_name) {\n-  std::set<string> added_tensors;\n-  for (const std::pair<int, int>& input : s.input_inds) {\n-    VLOG(2) << \"parsing input. Node id= \" << input.first;\n-    int node_id = input.first;\n-    int output_idx = input.second;\n-    tensorflow::Node* node = s.graph.FindNodeId(node_id);\n-    auto node_name = node->name();\n-    // input_names should use the node name in the graph\n-    // here it should be the input tensor name -> matching the binding\n-    // insert original node name without port\n-    auto tensor_name = node_name;\n-    if (output_idx != 0) {\n-      tensor_name = StrCat(tensor_name, \":\", output_idx);\n-    }\n-\n-    VLOG(2) << \"input name: \" << node_name << \" tensor_name: \" << tensor_name\n-            << \" idx: \" << output_idx;\n-\n-    auto shape_inference_node_name = node_name;\n-    auto shape_inference_output_idx = output_idx;\n-    // rewire the shape inference to original node in the graph\n-    if (s.output_edge_map->count(tensor_name)) {\n-      shape_inference_node_name = s.output_edge_map->at(tensor_name).second;\n-      shape_inference_output_idx = s.output_edge_map->at(tensor_name).first;\n-    }\n-    if (shape_inference_output_idx < 0) continue;\n-    VLOG(2) << \"shapeinference name: \" << shape_inference_node_name\n-            << \" idx: \" << shape_inference_output_idx;\n-\n-    if (!s.graph_properties.HasOutputProperties(shape_inference_node_name))\n-      return tensorflow::errors::Internal(\"failed to find input node: \" +\n-                                          shape_inference_node_name);\n-\n-    auto op_info_vec =\n-        s.graph_properties.GetOutputProperties(shape_inference_node_name);\n-    if (static_cast<int>(op_info_vec.size()) <= shape_inference_output_idx)\n-      return tensorflow::errors::Internal(\n-          \"accessing output index of: \", shape_inference_output_idx,\n-          \", at node: \", shape_inference_node_name,\n-          \" with output entry from shape_map: \", op_info_vec.size());\n-\n-    auto op_info = op_info_vec.at(shape_inference_output_idx);\n-    tensorflow::DataType tf_dtype = op_info.dtype();\n-\n-    nvinfer1::DataType dtype(nvinfer1::DataType::kFLOAT);\n-    auto type_status = ConvertDType(tf_dtype, &dtype);\n-    if (type_status != tensorflow::Status::OK()) {\n-      LOG(WARNING) << \"Type conversion failed for \" << node_name;\n-      return type_status;\n-    }\n-\n-    VLOG(2) << \"Accessing output index of: \" << output_idx\n-            << \", at node: \" << node_name\n-            << \" with output entry from shape_map: \" << op_info_vec.size();\n-    // TODO(ben,jie): update TRT input format/dimension\n-    nvinfer1::DimsCHW input_dim_pseudo_chw;\n-    for (int i = 0; i < 3; i++) input_dim_pseudo_chw.d[i] = 1;\n-\n-    // TODO(jie): TRT 3.x only support 4 dimensional input tensor.\n-    //            update the code once TRT 4.0 comes out.\n-    if (op_info.shape().dim_size() != 4) {\n-      string err_str = \"Require 4 dimensional input.\";\n-      StrAppend(&err_str, \" Got \", op_info.shape().dim_size(), \" \",\n-                shape_inference_node_name);\n-      return tensorflow::errors::Unimplemented(err_str);\n-    }\n-\n-    for (int i = 1; i < op_info.shape().dim_size(); i++) {\n-      VLOG(2) << \"dimension: \" << i\n-              << \" , size: \" << op_info.shape().dim(i).size();\n-      input_dim_pseudo_chw.d[i - 1] = op_info.shape().dim(i).size();\n-    }\n-\n-    // TODO(ben,jie): proper way to restore input tensor name?\n-    auto input_tensor_name = node_name;\n-    if (output_idx != 0) {\n-      input_tensor_name = StrCat(node_name, \":\", output_idx);\n-    }\n-    if (added_tensors.count(input_tensor_name)) continue;\n-    added_tensors.insert(input_tensor_name);\n-    input_names->push_back(input_tensor_name);\n-    input_dtypes->push_back(tf_dtype);\n-    nvinfer1::ITensor* input_tensor = converter.network()->addInput(\n-        input_tensor_name.c_str(), dtype, input_dim_pseudo_chw);\n-\n-    if (!input_tensor)\n-      return tensorflow::errors::InvalidArgument(\n-          \"Failed to create Input layer\");\n-    VLOG(2) << \"Input tensor name :\" << input_tensor_name;\n-\n-    if (!converter.insert_input_tensor(input_tensor_name, input_tensor))\n-      return tensorflow::errors::AlreadyExists(\n-          \"Output tensor already exists for op: \" + input_tensor_name);\n-  }\n-\n-  for (const tensorflow::Node* node : *order) {\n-    const tensorflow::NodeDef& node_def = node->def();\n-    VLOG(2) << \"Converting node: \" << node_def.name() << \" , \" << node_def.op();\n-    TF_RETURN_IF_ERROR(converter.convert_node(node_def));\n-  }\n-\n-  VLOG(2) << \"Finished conversion\";\n-\n-  // Gather output metadata\n-  int trt_engine_op_output_idx = 0;\n-  added_tensors.clear();\n-  for (const std::pair<int, int>& output : s.output_inds) {\n-    int node_id = output.first;\n-    int output_idx = output.second;\n-    tensorflow::Node* node = s.graph.FindNodeId(node_id);\n-    string op_name = node->name();\n-    string tensor_name = op_name;\n-\n-    s.output_edge_map->insert(\n-        {trt_engine_op_output_idx == 0\n-             ? engine_name\n-             : StrCat(engine_name, \":\", trt_engine_op_output_idx),\n-         {output_idx, tensor_name}});\n-    trt_engine_op_output_idx++;\n-    if (output_idx != 0)\n-      tensorflow::strings::StrAppend(&tensor_name, \":\", output_idx);\n-    VLOG(2) << \"Output tensor name: \" << tensor_name;\n-    if (added_tensors.count(tensor_name)) continue;\n-    added_tensors.insert(tensor_name);\n-    output_names->push_back(tensor_name);\n-    auto tensor_or_weights = converter.get_tensor(tensor_name);\n+  for (const auto& output : output_tensors) {\n+    auto tensor_or_weights = converter.get_tensor(output.first);\n     if (!tensor_or_weights.is_tensor()) {\n-      return tensorflow::errors::InvalidArgument(\"Output node '\" + tensor_name +\n-                                                 \"' is weights not tensor\");\n+      return tensorflow::errors::InvalidArgument(\n+          \"Output node '\" + output.first + \"' is weights not tensor\");\n     }\n     nvinfer1::ITensor* tensor = tensor_or_weights.tensor();\n+    tensor->setName(output.second.c_str());\n     if (!tensor) {\n       return tensorflow::errors::NotFound(\"Output tensor not found: \" +\n-                                          tensor_name);\n+                                          output.first);\n     }\n-    converter.network()->markOutput(*tensor);\n-    tensorflow::DataType tf_dtype = node->output_type(output_idx);\n-    output_dtypes->push_back(tf_dtype);\n-    nvinfer1::DataType trt_dtype = nvinfer1::DataType::kFLOAT;\n-    TF_RETURN_IF_ERROR(ConvertDType(tf_dtype, &trt_dtype));\n-    tensor->setType(trt_dtype);\n-  }\n-\n-  return tensorflow::Status::OK();\n-}\n-\n-tensorflow::Status InjectCalibrationNode(tensorrt::convert::SubGraphParams& s) {\n-  // Visit nodes in reverse topological order and construct the TRT network.\n-  // Toposort\n-  std::list<tensorflow::Node*> order;\n-  TF_RETURN_IF_ERROR(ReverseTopologicalSort(s, &order));\n-\n-  static int static_id = 0;\n-  string subgraph_name_scope = SubgraphNameScopeGenerator(&order);\n-  // TODO(sami,ben,jie): proper naming!\n-  string calib_op_name =\n-      StrCat(subgraph_name_scope, \"my_trt_calib_op_\", static_id);\n-  string engine_name = StrCat(subgraph_name_scope, \"my_trt_op\", static_id);\n-  static_id++;\n-\n-  auto trt_rmgr = tensorflow::tensorrt::TRTResourceManager::instance();\n-  auto op_rmgr = trt_rmgr->getManager(\"TRTCalibOps\");\n-  auto op_res = new tensorflow::tensorrt::TRTCalibrationResource();\n-  TF_CHECK_OK(op_rmgr->Create(calib_op_name, calib_op_name, op_res));\n-  op_res->logger_ = new tensorflow::tensorrt::Logger();\n-  cudaSetDevice(s.cuda_gpu_id_);\n-  op_res->builder_ = nvinfer1::createInferBuilder(*(op_res->logger_));\n-  op_res->allocator_ = s.allocator_;\n-#if NV_TENSORRT_MAJOR > 3\n-  op_res->builder_->setGpuAllocator(s.allocator_.get());\n-#endif\n-  if (!op_res->builder_) {\n-    return tensorflow::errors::Internal(\n-        \"failed to create TensorRT builder object\");\n-  }\n+    VLOG(1) << \"Marking output tensor \" << output.first << \", as output tensor \"\n+            << output.second;\n \n-  op_res->network_ = op_res->builder_->createNetwork();\n-  if (!op_res->network_) {\n-    return tensorflow::errors::Internal(\n-        \"failed to create TensorRT network object\");\n+    converter.network()->markOutput(*tensor);\n   }\n-\n-  // Build the network\n-  auto weight_rmgr = trt_rmgr->getManager(\"WeightStore\");\n-  auto ws = new tensorflow::tensorrt::TRTWeightStore();\n-  TF_CHECK_OK(weight_rmgr->Create(calib_op_name, calib_op_name, ws));\n-  Converter converter(op_res->network_, ws, s.precision_mode == FP16MODE);\n-\n-  std::vector<string> input_names;\n-  std::vector<tensorflow::DataType> input_dtypes;\n-  std::vector<string> output_names;\n-  std::vector<tensorflow::DataType> output_dtypes;\n-  TF_RETURN_IF_ERROR(ConvertSubgraph(converter, s, &order, &input_names,\n-                                     &input_dtypes, &output_names,\n-                                     &output_dtypes, engine_name));\n-\n-  VLOG(2) << \"Finished processing outputs\";\n-\n-  // Build the engine\n-  op_res->builder_->setMaxBatchSize(s.max_batch_size);\n-  op_res->builder_->setMaxWorkspaceSize(s.max_workspace_size_bytes);\n-  VLOG(0) << \"Max batch size= \" << s.max_batch_size\n-          << \" max workspace size= \" << s.max_workspace_size_bytes;\n-\n-  // Build the TRT op\n-  // TODO(sami,ben,jie): proper naming!\n-  tensorflow::NodeDefBuilder op_builder(calib_op_name, \"TRTCalibOp\");\n-  TF_RETURN_IF_ERROR(SetInputList(s, &op_builder, &input_names, &input_dtypes));\n-\n-  std::vector<string> segment_names;\n-  segment_names.reserve(s.subgraph_node_ids.size());\n-  for (int i : s.subgraph_node_ids) {\n-    auto node = s.graph.FindNodeId(i);\n-    segment_names.push_back(node->name());\n-  }\n-  LOG(INFO) << \"finished op preparation\";\n-\n-  auto status = op_builder.Attr(\"segment_nodes\", segment_names)\n-                    .Attr(\"input_names\", input_names)\n-                    .Attr(\"segment_output_names\", output_names)\n-                    .Attr(\"resource_name\", calib_op_name)\n-                    .Finalize(s.trt_node);\n-\n-  LOG(INFO) << status.ToString();\n-  LOG(INFO) << \"finished op building\";\n-\n+  VLOG(1) << \"Starting engine creation\";\n+  *engine = builder->buildCudaEngine(*converter.network());\n+  VLOG(1) << \"Finished conversion\";\n   return tensorflow::Status::OK();\n }\n+//  Constructs a graphdef from the segment in the given graph. Adds placeholder", "path": "tensorflow/contrib/tensorrt/convert/convert_nodes.cc", "position": null, "original_position": 578, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "565640eae327b092edf43613f77ba5ab0747d20d", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "Please move this comment in the header.", "created_at": "2018-06-12T22:32:22Z", "updated_at": "2018-06-21T22:23:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194908769", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194908769"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194908769"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>Please move this comment in the header.</p>", "body_text": "Please move this comment in the header."}