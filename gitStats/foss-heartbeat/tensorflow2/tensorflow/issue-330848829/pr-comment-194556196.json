{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194556196", "pull_request_review_id": 127713645, "id": 194556196, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDU1NjE5Ng==", "diff_hunk": "@@ -30,85 +37,300 @@ using IRuntime = nvinfer1::IRuntime;\n using Dims = nvinfer1::Dims;\n \n namespace tensorrt {\n+using tensorflow::strings::StrAppend;\n+using tensorflow::strings::StrCat;\n+class AsyncHelper : public tensorflow::core::RefCounted {\n+ public:\n+  AsyncHelper(tensorflow::AsyncOpKernel::DoneCallback done) { done_ = done; }\n+  ~AsyncHelper() override { done_(); }\n \n-TRTEngineOp::TRTEngineOp(OpKernelConstruction* context) : OpKernel(context) {\n+ private:\n+  tensorflow::AsyncOpKernel::DoneCallback done_;\n+};\n+\n+#define TYPECASE(dt, X, Y)                                                \\\n+  case dt: {                                                              \\\n+    return (void*)X->flat<tensorflow::EnumToDataType<dt>::Type>().data(); \\\n+  }\n+\n+void* GetTensorAddress(const Tensor* tensor_ptr) {\n+  auto tensor_type = tensor_ptr->dtype();\n+  switch (tensor_type) {\n+    TYPECASE(tensorflow::DT_FLOAT, tensor_ptr, dest_ptr);\n+    TYPECASE(tensorflow::DT_HALF, tensor_ptr, dest_ptr);\n+    TYPECASE(tensorflow::DT_INT8, tensor_ptr, dest_ptr);\n+    default: {\n+      LOG(FATAL) << \"Unsupported Data type \"\n+                 << tensorflow::DataTypeString(tensor_type);\n+      return nullptr;\n+    }\n+  }\n+}\n+\n+tensorflow::Status TRTEngineOp::ConstructFunctionHandle(OpKernelContext* ctx) {\n+  VLOG(1) << \"Constructing function handle\";\n+  auto lib = ctx->function_library();\n+  if (lib == nullptr) {\n+    return tensorflow::errors::Internal(\"Context function library is null\");\n+  }\n+  auto fdef = lib->GetFunctionLibraryDefinition()->Find(funcdef_name_);\n+  if (fdef == nullptr) {\n+    return tensorflow::errors::Internal(\n+        StrCat(\"Native FunctionDef \", funcdef_name_,\n+               \" can't be found in function library\"));\n+  }\n+  tensorflow::FunctionLibraryRuntime::InstantiateOptions inst_ops;\n+  inst_ops.overlay_lib = nullptr;\n+  inst_ops.state_handle = \"\";\n+  inst_ops.target = ctx->device()->name();\n+  native_func_ = 0;\n+  auto status = lib->Instantiate(funcdef_name_, AttrSlice(&fdef->attr()),\n+                                 inst_ops, &native_func_);\n+  if (!status.ok()) {\n+    LOG(ERROR) << \" Instantiating native function \" << funcdef_name_\n+               << \" failed!\";\n+  }\n+  return status;\n+}\n+\n+TRTEngineOp::TRTEngineOp(OpKernelConstruction* context)\n+    : AsyncOpKernel(context) {\n   // read serialized_engine\n   OP_REQUIRES_OK(context,\n-                 context->GetAttr(\"serialized_engine\", &serialized_engine_));\n+                 context->GetAttr(\"serialized_segment\", &serialized_segment_));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"workspace_size_bytes\", &workspace_size_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"static_engine\", &static_engine));\n+  if (!static_engine) {\n+    if (!segment_graph_.ParseFromString(serialized_segment_)) {\n+      LOG(ERROR) << \"Parsing segment graph failed!\";\n+      context->SetStatus(tensorflow::errors::InvalidArgument(\n+          \"Failed to parse segment graphdef!\"));\n+      return;\n+    }\n+    serialized_segment_.resize(0);\n+  }\n \n-  // register input output node name in trt_sub_graph\n-  OP_REQUIRES_OK(context, context->GetAttr(\"input_nodes\", &input_nodes_));\n-  OP_REQUIRES_OK(context, context->GetAttr(\"output_nodes\", &output_nodes_));\n+  string precision_string;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"precision_mode\", &precision_string));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"calibration_data\", &calibration_data_));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"segment_funcdef_name\", &funcdef_name_));\n+  if (precision_string == \"FP32\") {\n+    precision_mode = tensorflow::tensorrt::convert::FP32MODE;\n+  } else if (precision_string == \"FP16\") {\n+    precision_mode = tensorflow::tensorrt::convert::FP16MODE;\n+  } else if (precision_string == \"INT8\") {\n+    precision_mode = tensorflow::tensorrt::convert::INT8MODE;\n+  }\n+  calibration_mode =\n+      precision_mode == tensorflow::tensorrt::convert::INT8MODE &&\n+      calibration_data_.size() == 0;\n+  if (calibration_data_.size()) {\n+    calibrator_.reset(new TRTInt8Calibrator(calibration_data_));\n+    calibration_data_.resize(0);\n+  }\n+  native_func_ = tensorflow::kInvalidHandle;\n+  OP_REQUIRES_OK(context, context->GetAttr(\"max_cached_engines_count\",\n+                                           &max_cached_engines));\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"fixed_input_size\", &fixed_input_size));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"cached_engine_batches\",\n+                                           &cached_engine_batches));\n+  std::sort(cached_engine_batches.begin(), cached_engine_batches.end());\n+  if (VLOG_IS_ON(1)) {\n+    string s(\"Engine Batches= \");\n+    for (auto i : cached_engine_batches) {\n+      StrAppend(&s, i, \" \");\n+    }\n+    VLOG(1) << s;\n+  }\n }\n \n-void TRTEngineOp::Compute(OpKernelContext* context) {\n-  // TODO(samikama) runtime should be taken from a resourcemanager as well.\n-  // Only engine should be in the op and context and runtime should be taken\n-  // from resourcemanager\n+void TRTEngineOp::ExecuteNativeSegment(tensorflow::OpKernelContext* ctx,\n+                                       AsyncHelper* ah) {", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 143, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "d5aaf3fa4a4851abc6a0e5600474f7674f1adb93", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "nit: use 'helper' as name instead.", "created_at": "2018-06-11T21:37:39Z", "updated_at": "2018-06-21T22:23:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194556196", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194556196"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194556196"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>nit: use 'helper' as name instead.</p>", "body_text": "nit: use 'helper' as name instead."}