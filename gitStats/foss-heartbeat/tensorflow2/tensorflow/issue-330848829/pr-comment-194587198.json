{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194587198", "pull_request_review_id": 127787679, "id": 194587198, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NDU4NzE5OA==", "diff_hunk": "@@ -117,51 +339,238 @@ void TRTEngineOp::Compute(OpKernelContext* context) {\n       std::vector<int> trt_shape(dims.nbDims + 1);\n       trt_shape[0] = num_batch;\n       for (int j = 0; j < dims.nbDims; j++) trt_shape[j + 1] = dims.d[j];\n-      OP_REQUIRES_OK(context,\n-                     TensorShapeUtils::MakeShape(\n-                         trt_shape.data(), trt_shape.size(), &output_shape));\n+      OP_REQUIRES_OK(\n+          ctx, TensorShapeUtils::MakeShape(trt_shape.data(), trt_shape.size(),\n+                                           &output_shape));\n     } else {\n-      LOG(FATAL) << \"output node not found, at \" << output_nodes_[i];\n-      break;\n+      LOG(ERROR) << \"output node not found, at \" << output_name;\n+      ctx->SetStatus(tensorflow::errors::Internal(\"output \" + output_name +\n+                                                  \" but couldn't be found!\"));\n+      return;\n+    }\n+    auto status = ctx->allocate_output(i, output_shape, &output_tensor);\n+    if (!status.ok()) {\n+      LOG(ERROR) << \"Allocating output failed with \" << status;\n+      ctx->SetStatus(status);\n+      return;\n     }\n-\n-    OP_REQUIRES_OK(context,\n-                   context->allocate_output(i, output_shape, &output_tensor));\n     auto dtype = trt_engine_ptr_->getBindingDataType(binding_index);\n     switch (dtype) {\n       case nvinfer1::DataType::kFLOAT:\n         buffers[binding_index] =\n             reinterpret_cast<void*>(output_tensor->flat<float>().data());\n         break;\n       case nvinfer1::DataType::kHALF:\n-        LOG(FATAL) << \"half size is not supported yet!\";\n+        LOG(ERROR) << \"half size is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Half outputs are not supported!\"));\n+        return;\n         break;\n       case nvinfer1::DataType::kINT8:\n-        LOG(FATAL) << \"int8 is not supported yet!\";\n+        LOG(ERROR) << \"int8 is not supported yet!\";\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"INT8 outputs are not supported!\"));\n+        return;\n         break;\n       default:\n-        LOG(FATAL) << \"Unknown data type: \" << int(dtype);\n+        LOG(ERROR) << \"Unknown TRT data type: \" << int(dtype);\n+        ctx->SetStatus(tensorflow::errors::InvalidArgument(\n+            \"Unsupported output data type! \" + int(dtype)));\n+        return;\n         break;\n     }\n   }\n   // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n   const cudaStream_t* stream = CHECK_NOTNULL(\n-      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\n+      reinterpret_cast<const cudaStream_t*>(ctx->op_device_context()\n                                                 ->stream()\n                                                 ->implementation()\n                                                 ->CudaStreamMemberHack()));\n \n   // TODO(jie): trt enqueue does not return error\n-  auto ret = trt_execution_context_ptr_->enqueue(num_batch, &buffers[0],\n-                                                 *stream, nullptr);\n+  auto trt_execution_context_ptr = engine_ctx_pair.second;\n+  auto ret = trt_execution_context_ptr->enqueue(num_batch, &buffers[0], *stream,\n+                                                nullptr);\n   VLOG(2) << \"enqueue returns: \" << ret;\n   // sync should be done by TF.\n-}\n+}  // namespace tensorrt\n TRTEngineOp::~TRTEngineOp() {\n   // Order matters!\n-  trt_execution_context_ptr_.reset();\n-  trt_engine_ptr_.reset();\n-  allocator_.reset();\n+  for (auto eng : engine_map) {\n+    eng.second.first.reset();\n+    eng.second.second.reset();\n+  }\n+  for (auto alloc : allocators_) alloc.second.reset();\n+}\n+// template <typename T>\n+// using destroyed_ptr = std::shared_ptr<T, TRTEngineOp::Destroyer<T>>;\n+TRTEngineOp::EngineCtxPair TRTEngineOp::get_engine(int batch_size,\n+                                                   OpKernelContext* ctx,\n+                                                   bool ignore_dim_change) {\n+  tensorflow::mutex_lock lock(engine_mutex_);\n+  if (static_engine) {\n+    if (engine_map.size()) {\n+      if (engine_map.begin()->first >= batch_size) {\n+        return engine_map.begin()->second;\n+      } else {\n+        return {nullptr, nullptr};\n+      }\n+    } else {\n+      IRuntime* infer = nvinfer1::createInferRuntime(logger);\n+#if NV_TENSORRT_MAJOR > 3\n+      auto device = ctx->device();\n+      auto dev_allocator =\n+          device->GetAllocator(tensorflow::AllocatorAttributes());\n+      if (!dev_allocator) {\n+        LOG(FATAL) << \"Can't find device allocator for gpu device \"\n+                   << device->name();\n+      }\n+      allocator_ = std::make_shared<TRTDeviceAllocator>(dev_allocator);\n+      infer->setGpuAllocator(allocator_.get());\n+#endif\n+      std::shared_ptr<nvinfer1::ICudaEngine> static_engine(\n+          infer->deserializeCudaEngine(serialized_segment_.c_str(),\n+                                       serialized_segment_.size(), nullptr),\n+          Destroyer<nvinfer1::ICudaEngine>());\n+      engine_map.insert({static_engine->getMaxBatchSize(),\n+                         {static_engine,\n+                          {static_engine->createExecutionContext(),\n+                           Destroyer<nvinfer1::IExecutionContext>()}}});\n+      // Runtime is safe to delete after engine creation\n+      infer->destroy();\n+      serialized_segment_.clear();\n+      if (static_engine->getMaxBatchSize() < batch_size) {\n+        return {nullptr, nullptr};\n+      }\n+      return engine_map.at(static_engine->getMaxBatchSize());\n+    }\n+  } else {\n+    auto engine_it = engine_map.find(batch_size);\n+    if (engine_it == engine_map.end() &&\n+        engine_map.size() < (size_t)max_cached_engines) {\n+      auto builder_ = std::shared_ptr<nvinfer1::IBuilder>(\n+          nvinfer1::createInferBuilder(logger),\n+          Destroyer<nvinfer1::IBuilder>());  // reset the builder to ensure\n+                                             // device is correct\n+#if NV_TENSORRT_MAJOR > 3\n+      auto device = context->device();\n+      auto device_name = device->name();\n+      if (allocators_.count(device_name)) {\n+        builder_->setGpuAllocator(allocators_.at(device_name).get());\n+      } else {\n+        std::make_shared<TRTDeviceAllocator> auto dev_allocator =\n+            device->GetAllocator(tensorflow::AllocatorAttributes());\n+        if (!dev_allocator) {\n+          LOG(ERROR) << \"Can't find device allocator for gpu device \"\n+                     << device->name();\n+          ctx->SetStatus(\n+              tensorflow::errors::Internal(\"Can't get device allocator\"));\n+          return nullptr;\n+        }\n+        auto allocator_ = std::make_shared<TRTDeviceAllocator>(dev_allocator);\n+        builder_->setGpuAllocator(allocator_.get());\n+        allocators_.insert({device_name, allocator});\n+      }\n+#endif\n+      VLOG(1) << name() << \" Constructing a new engine with batch size \"\n+              << batch_size;\n+      builder_->setMaxBatchSize(batch_size);\n+      if (precision_mode == tensorflow::tensorrt::convert::FP16MODE) {\n+        builder_->setHalf2Mode(true);\n+      } else if (precision_mode == tensorflow::tensorrt::convert::INT8MODE) {\n+        builder_->setInt8Mode(true);\n+        builder_->setInt8Calibrator(calibrator_.get());\n+      }\n+      builder_->setMaxWorkspaceSize(workspace_size_);\n+      nvinfer1::ICudaEngine* engine = nullptr;\n+      std::vector<tensorflow::PartialTensorShape> shapes;\n+      for (int i = 0; i < ctx->num_inputs(); ++i) {\n+        shapes.emplace_back(ctx->input(i).shape());\n+      }\n+      auto status = tensorflow::tensorrt::convert::ConvertSubgraphToEngine(\n+          segment_graph_, builder_.get(), shapes, &engine, precision_mode);\n+      if (engine) {\n+        engine_map[batch_size] = {\n+            std::shared_ptr<nvinfer1::ICudaEngine>(\n+                engine, Destroyer<nvinfer1::ICudaEngine>()),\n+            std::shared_ptr<nvinfer1::IExecutionContext>(\n+                engine->createExecutionContext(),\n+                Destroyer<nvinfer1::IExecutionContext>())};\n+      } else {\n+        LOG(ERROR) << \"Engine creation for batch size \" << batch_size\n+                   << \" failed\";\n+        ctx->SetStatus(tensorflow::errors::Internal(\"Engine creation failed!\"));\n+        engine_map[batch_size] = {nullptr, nullptr};\n+        return {nullptr, nullptr};\n+      }\n+    }\n+    return engine_map.at(batch_size);\n+  }\n+}\n+\n+tensorflow::Status TRTEngineOp::AllocateCalibrationResources(", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 552, "commit_id": "4631936e61651101932073197c08b600006530a3", "original_commit_id": "d5aaf3fa4a4851abc6a0e5600474f7674f1adb93", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "body": "CalibOp is not there anymore", "created_at": "2018-06-12T00:39:21Z", "updated_at": "2018-06-21T22:23:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194587198", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/194587198"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/19871#discussion_r194587198"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19871"}}, "body_html": "<p>CalibOp is not there anymore</p>", "body_text": "CalibOp is not there anymore", "in_reply_to_id": 194563743}