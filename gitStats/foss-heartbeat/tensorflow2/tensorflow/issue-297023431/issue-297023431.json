{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17006", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17006/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17006/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17006/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17006", "id": 297023431, "node_id": "MDU6SXNzdWUyOTcwMjM0MzE=", "number": 17006, "title": "Batch normalization acting weird?!", "user": {"login": "Fredeli", "id": 9251338, "node_id": "MDQ6VXNlcjkyNTEzMzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/9251338?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Fredeli", "html_url": "https://github.com/Fredeli", "followers_url": "https://api.github.com/users/Fredeli/followers", "following_url": "https://api.github.com/users/Fredeli/following{/other_user}", "gists_url": "https://api.github.com/users/Fredeli/gists{/gist_id}", "starred_url": "https://api.github.com/users/Fredeli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Fredeli/subscriptions", "organizations_url": "https://api.github.com/users/Fredeli/orgs", "repos_url": "https://api.github.com/users/Fredeli/repos", "events_url": "https://api.github.com/users/Fredeli/events{/privacy}", "received_events_url": "https://api.github.com/users/Fredeli/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-02-14T09:18:18Z", "updated_at": "2018-06-17T18:35:24Z", "closed_at": "2018-06-17T18:30:44Z", "author_association": "NONE", "body_html": "<h3>System Information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No custom code</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 7.1</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Exact command to reproduce</strong>: See code below</li>\n<li><strong>Bazel version</strong>: Not building from source</li>\n<li><strong>CUDA/cuDNN version</strong>: Version 6</li>\n<li><strong>GPU model and memory</strong>: Nvidia Quadro k2200 with 4GB</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Hello everyone,</p>\n<p>so theres the method tf.layers.batch_normalization and i am trying to understand what is Happening behind the curtains and in how far it is manually reproducible and in conformance of the paper by Ioffe&amp;Szegedy. After running a small script which can only Train the two Parameters Gamma and beta to minimize the loss it appears that there is not normalization taking place(activation reduces by the mean over all activations of one Batch divided by the square-root of the variance of all activations of one batch). It is more or less always considering the Initial activation and simply multiplies this with Gamma plus beta. And that is especially interesting if the Batch size is reduced to one(not even remotely sure what is Happening then; yet it seems like this does not affect the computation at all)...</p>\n<p>Thank you for your time and consideration!</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\ntest_img <span class=\"pl-k\">=</span> np.array([[[[<span class=\"pl-c1\">50</span>],[<span class=\"pl-c1\">100</span>]],\n                   [[<span class=\"pl-c1\">150</span>],[<span class=\"pl-c1\">2000</span>]]],\n                   [[[<span class=\"pl-c1\">0</span>],[<span class=\"pl-c1\">300</span>]],\n                   [[<span class=\"pl-c1\">140</span>],[<span class=\"pl-c1\">5000</span>]]],\n                   [[[<span class=\"pl-c1\">0</span>],[<span class=\"pl-c1\">300</span>]],\n                   [[<span class=\"pl-c1\">140</span>],[<span class=\"pl-c1\">5500</span>]]],\n                   [[[<span class=\"pl-c1\">0</span>],[<span class=\"pl-c1\">200</span>]],\n                   [[<span class=\"pl-c1\">1400</span>],[<span class=\"pl-c1\">5000</span>]]],\n                   [[[<span class=\"pl-c1\">0</span>],[<span class=\"pl-c1\">300</span>]],\n                   [[<span class=\"pl-c1\">140</span>],[<span class=\"pl-c1\">5000</span>]]]], np.float32)\ngt_img <span class=\"pl-k\">=</span> np.array([[[[<span class=\"pl-c1\">60</span>],[<span class=\"pl-c1\">130</span>]],\n                [[<span class=\"pl-c1\">180</span>],[<span class=\"pl-c1\">225</span>]]],\n                [[[<span class=\"pl-c1\">60</span>],[<span class=\"pl-c1\">130</span>]],\n                [[<span class=\"pl-c1\">180</span>],[<span class=\"pl-c1\">225</span>]]],\n                [[[<span class=\"pl-c1\">600</span>],[<span class=\"pl-c1\">130</span>]],\n                [[<span class=\"pl-c1\">1800</span>],[<span class=\"pl-c1\">225</span>]]],\n                [[[<span class=\"pl-c1\">60</span>],[<span class=\"pl-c1\">100</span>]],\n                [[<span class=\"pl-c1\">180</span>],[<span class=\"pl-c1\">205</span>]]],\n                [[[<span class=\"pl-c1\">60</span>],[<span class=\"pl-c1\">100</span>]],\n                [[<span class=\"pl-c1\">180</span>],[<span class=\"pl-c1\">205</span>]]]], np.float32)\ntest_img_op <span class=\"pl-k\">=</span> tf.convert_to_tensor(test_img, tf.float32)\nnorm_op <span class=\"pl-k\">=</span> tf.layers.batch_normalization(test_img_op, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n\nloss_op <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span> <span class=\"pl-k\">=</span> gt_img,\n                                                             <span class=\"pl-v\">logits</span> <span class=\"pl-k\">=</span> norm_op))\ncount <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\nupdate_ops <span class=\"pl-k\">=</span> tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)\n<span class=\"pl-k\">with</span> tf.control_dependencies(update_ops):\n    optimizer_obj <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.01</span>).minimize(loss_op)\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.group(tf.global_variables_initializer(), \n                      tf.local_variables_initializer()))\n    <span class=\"pl-c1\">print</span>(test_img)\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        count <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        <span class=\"pl-k\">if</span> count <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">100</span>:\n            new_img, op, lossy, trainable <span class=\"pl-k\">=</span> sess.run([norm_op, \n                                                      optimizer_obj, \n                                                      loss_op, \n                                                      tf.trainable_variables()])\n            <span class=\"pl-c1\">print</span>(trainable)\n            <span class=\"pl-c1\">print</span>(new_img)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">break</span></pre></div>", "body_text": "System Information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No custom code\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7.1\nTensorFlow version (use command below): 1.3.0\nPython version: 3.6\nExact command to reproduce: See code below\nBazel version: Not building from source\nCUDA/cuDNN version: Version 6\nGPU model and memory: Nvidia Quadro k2200 with 4GB\n\nDescribe the problem\nHello everyone,\nso theres the method tf.layers.batch_normalization and i am trying to understand what is Happening behind the curtains and in how far it is manually reproducible and in conformance of the paper by Ioffe&Szegedy. After running a small script which can only Train the two Parameters Gamma and beta to minimize the loss it appears that there is not normalization taking place(activation reduces by the mean over all activations of one Batch divided by the square-root of the variance of all activations of one batch). It is more or less always considering the Initial activation and simply multiplies this with Gamma plus beta. And that is especially interesting if the Batch size is reduced to one(not even remotely sure what is Happening then; yet it seems like this does not affect the computation at all)...\nThank you for your time and consideration!\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\n\ntest_img = np.array([[[[50],[100]],\n                   [[150],[2000]]],\n                   [[[0],[300]],\n                   [[140],[5000]]],\n                   [[[0],[300]],\n                   [[140],[5500]]],\n                   [[[0],[200]],\n                   [[1400],[5000]]],\n                   [[[0],[300]],\n                   [[140],[5000]]]], np.float32)\ngt_img = np.array([[[[60],[130]],\n                [[180],[225]]],\n                [[[60],[130]],\n                [[180],[225]]],\n                [[[600],[130]],\n                [[1800],[225]]],\n                [[[60],[100]],\n                [[180],[205]]],\n                [[[60],[100]],\n                [[180],[205]]]], np.float32)\ntest_img_op = tf.convert_to_tensor(test_img, tf.float32)\nnorm_op = tf.layers.batch_normalization(test_img_op, momentum=0)\n\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = gt_img,\n                                                             logits = norm_op))\ncount = 0\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    optimizer_obj = tf.train.AdamOptimizer(0.01).minimize(loss_op)\nwith tf.Session() as sess:\n    sess.run(tf.group(tf.global_variables_initializer(), \n                      tf.local_variables_initializer()))\n    print(test_img)\n    while True:\n        count += 1\n        if count < 100:\n            new_img, op, lossy, trainable = sess.run([norm_op, \n                                                      optimizer_obj, \n                                                      loss_op, \n                                                      tf.trainable_variables()])\n            print(trainable)\n            print(new_img)\n        else:\n            break", "body": "### System Information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7.1\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6\r\n- **Exact command to reproduce**: See code below\r\n- **Bazel version**: Not building from source\r\n- **CUDA/cuDNN version**: Version 6\r\n- **GPU model and memory**: Nvidia Quadro k2200 with 4GB\r\n\r\n\r\n### Describe the problem\r\nHello everyone,\r\n\r\nso theres the method tf.layers.batch_normalization and i am trying to understand what is Happening behind the curtains and in how far it is manually reproducible and in conformance of the paper by Ioffe&Szegedy. After running a small script which can only Train the two Parameters Gamma and beta to minimize the loss it appears that there is not normalization taking place(activation reduces by the mean over all activations of one Batch divided by the square-root of the variance of all activations of one batch). It is more or less always considering the Initial activation and simply multiplies this with Gamma plus beta. And that is especially interesting if the Batch size is reduced to one(not even remotely sure what is Happening then; yet it seems like this does not affect the computation at all)...\r\n\r\nThank you for your time and consideration!\r\n\r\n### Source code / logs\r\n```Python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntest_img = np.array([[[[50],[100]],\r\n                   [[150],[2000]]],\r\n                   [[[0],[300]],\r\n                   [[140],[5000]]],\r\n                   [[[0],[300]],\r\n                   [[140],[5500]]],\r\n                   [[[0],[200]],\r\n                   [[1400],[5000]]],\r\n                   [[[0],[300]],\r\n                   [[140],[5000]]]], np.float32)\r\ngt_img = np.array([[[[60],[130]],\r\n                [[180],[225]]],\r\n                [[[60],[130]],\r\n                [[180],[225]]],\r\n                [[[600],[130]],\r\n                [[1800],[225]]],\r\n                [[[60],[100]],\r\n                [[180],[205]]],\r\n                [[[60],[100]],\r\n                [[180],[205]]]], np.float32)\r\ntest_img_op = tf.convert_to_tensor(test_img, tf.float32)\r\nnorm_op = tf.layers.batch_normalization(test_img_op, momentum=0)\r\n\r\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = gt_img,\r\n                                                             logits = norm_op))\r\ncount = 0\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    optimizer_obj = tf.train.AdamOptimizer(0.01).minimize(loss_op)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.group(tf.global_variables_initializer(), \r\n                      tf.local_variables_initializer()))\r\n    print(test_img)\r\n    while True:\r\n        count += 1\r\n        if count < 100:\r\n            new_img, op, lossy, trainable = sess.run([norm_op, \r\n                                                      optimizer_obj, \r\n                                                      loss_op, \r\n                                                      tf.trainable_variables()])\r\n            print(trainable)\r\n            print(new_img)\r\n        else:\r\n            break\r\n```"}