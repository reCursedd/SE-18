{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18858", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18858/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18858/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18858/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18858", "id": 317555767, "node_id": "MDU6SXNzdWUzMTc1NTU3Njc=", "number": 18858, "title": "How to show the loss curve of training set and validation set at the same time using the customed estimator?", "user": {"login": "zjy8006", "id": 19348442, "node_id": "MDQ6VXNlcjE5MzQ4NDQy", "avatar_url": "https://avatars1.githubusercontent.com/u/19348442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zjy8006", "html_url": "https://github.com/zjy8006", "followers_url": "https://api.github.com/users/zjy8006/followers", "following_url": "https://api.github.com/users/zjy8006/following{/other_user}", "gists_url": "https://api.github.com/users/zjy8006/gists{/gist_id}", "starred_url": "https://api.github.com/users/zjy8006/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zjy8006/subscriptions", "organizations_url": "https://api.github.com/users/zjy8006/orgs", "repos_url": "https://api.github.com/users/zjy8006/repos", "events_url": "https://api.github.com/users/zjy8006/events{/privacy}", "received_events_url": "https://api.github.com/users/zjy8006/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-04-25T09:50:48Z", "updated_at": "2018-11-22T18:56:09Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi, recently I used <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/get_started/regression/custom_regression.py\">custom_estimator.py</a> to build regression model.  In order to clear out the changes of loss value in the training set and validation set. I need to know that how to show the loss curve of training and validation set at the same time. I tried to use train_and_evaluate api of estimator and i got the following picture.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/19348442/39224762-76df9b92-487b-11e8-888d-b64825e99c82.png\"><img src=\"https://user-images.githubusercontent.com/19348442/39224762-76df9b92-487b-11e8-888d-b64825e99c82.png\" alt=\"image\" style=\"max-width:100%;\"></a><br>\nAs it show, the result of evaluation is a point, but i want a line like the loss curve of training set. Just like the picture as shown below.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/19348442/39280146-2d129f22-4930-11e8-81e7-a2379b924eb0.png\"><img src=\"https://user-images.githubusercontent.com/19348442/39280146-2d129f22-4930-11e8-81e7-a2379b924eb0.png\" alt=\"image\" style=\"max-width:100%;\"></a><br>\nHere is my system information:</p>\n<ul>\n<li>\n<p>Have i written custom code: N/A</p>\n</li>\n<li>\n<p>OS: Tested on windows 10 1709.</p>\n</li>\n<li>\n<p>Tensorflow installed from Anaconda 5.1.0 with python 3.6.4</p>\n</li>\n<li>\n<p>Tensorflow version-tested on tensorflow-gpu 1.7.0</p>\n</li>\n<li>\n<p>CUDA/cuDNN version: 9.0 for TF 1.7</p>\n</li>\n<li>\n<p>GPU mode: Nvidia Quadro  K2100M\uff0c 2G of memory</p>\n</li>\n<li>\n<p>Bazel version: N/A</p>\n</li>\n<li>\n<p>Exact command to reproduce: N/A<br>\nHere is the customed estimator:</p>\n</li>\n</ul>\n<pre><code>def my_dnn_regression_fn(features, labels, mode, params):\n    top = tf.feature_column.input_layer(features, params['feature_columns'])\n\n    for units in params.get('hidden_units', [20]):\n        top = tf.layers.dense(inputs=top, units=units, activation=tf.nn.relu)\n\n    output_layer = tf.layers.dense(inputs=top, units=1)\n\n    output_layer = tf.cast(output_layer, tf.float64)\n   \n    predictions = tf.squeeze(output_layer, 1)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        # In 'PREDICT' mode we only need to return predictions.\n        return tf.estimator.EstimatorSpec(\n            mode=mode, predictions={\"predictions\": predictions})\n\n    # calculate the loss using mean squared error\n    average_loss = tf.losses.mean_squared_error(labels, predictions)\n\n    # Pre-made estimators use the total_loss instead of the average,\n    # so report total_loss for compatibility.\n    batch_size = tf.shape(labels)[0]\n    total_loss = tf.to_float(batch_size) * average_loss\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = params.get(\"optimizer\", tf.train.AdamOptimizer)\n        optimizer = optimizer(params.get(\"learning_rate\", None))\n        train_op = optimizer.minimize(\n            loss=average_loss, global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(\n            mode=mode, loss=total_loss, train_op=train_op)\n\n    # In the evaluation mode we will calculate evaluation metrics.\n    assert mode == tf.estimator.ModeKeys.EVAL\n\n    # Calculate root mean squared error\n    rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n\n    # Add the rmse to collection of evaluation metrics.\n    eval_metrics = {\"rmse\": rmse}\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        # Report sum of error for compatibility with pre-made estimators.\n        loss=total_loss,\n        eval_metric_ops=eval_metrics)\n</code></pre>\n<p>And here I used train_and_evaluate api like this:</p>\n<pre><code>    model = tf.estimator.Estimator(\n        model_fn=my_dnn_regression_fn,\n        model_dir=\n        \"./models/temp\",\n        params={\n            'feature_columns': feature_columns,\n            'learning_rate': 0.1,\n            'optimizer': tf.train.AdamOptimizer,\n            'hidden_units': [20, 20, 20, 20]\n        })\n    train_spec = tf.estimator.TrainSpec(input_fn=input_train,max_steps=10000)\n    eval_spec = tf.estimator.EvalSpec(input_fn=input_dev,steps=10000,throttle_secs=60,start_delay_secs=0)\n    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\n</code></pre>\n<p>Did I set the parameter properly? Or, is there other solution for this?</p>", "body_text": "Hi, recently I used custom_estimator.py to build regression model.  In order to clear out the changes of loss value in the training set and validation set. I need to know that how to show the loss curve of training and validation set at the same time. I tried to use train_and_evaluate api of estimator and i got the following picture.\n\nAs it show, the result of evaluation is a point, but i want a line like the loss curve of training set. Just like the picture as shown below.\n\nHere is my system information:\n\n\nHave i written custom code: N/A\n\n\nOS: Tested on windows 10 1709.\n\n\nTensorflow installed from Anaconda 5.1.0 with python 3.6.4\n\n\nTensorflow version-tested on tensorflow-gpu 1.7.0\n\n\nCUDA/cuDNN version: 9.0 for TF 1.7\n\n\nGPU mode: Nvidia Quadro  K2100M\uff0c 2G of memory\n\n\nBazel version: N/A\n\n\nExact command to reproduce: N/A\nHere is the customed estimator:\n\n\ndef my_dnn_regression_fn(features, labels, mode, params):\n    top = tf.feature_column.input_layer(features, params['feature_columns'])\n\n    for units in params.get('hidden_units', [20]):\n        top = tf.layers.dense(inputs=top, units=units, activation=tf.nn.relu)\n\n    output_layer = tf.layers.dense(inputs=top, units=1)\n\n    output_layer = tf.cast(output_layer, tf.float64)\n   \n    predictions = tf.squeeze(output_layer, 1)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        # In 'PREDICT' mode we only need to return predictions.\n        return tf.estimator.EstimatorSpec(\n            mode=mode, predictions={\"predictions\": predictions})\n\n    # calculate the loss using mean squared error\n    average_loss = tf.losses.mean_squared_error(labels, predictions)\n\n    # Pre-made estimators use the total_loss instead of the average,\n    # so report total_loss for compatibility.\n    batch_size = tf.shape(labels)[0]\n    total_loss = tf.to_float(batch_size) * average_loss\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = params.get(\"optimizer\", tf.train.AdamOptimizer)\n        optimizer = optimizer(params.get(\"learning_rate\", None))\n        train_op = optimizer.minimize(\n            loss=average_loss, global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(\n            mode=mode, loss=total_loss, train_op=train_op)\n\n    # In the evaluation mode we will calculate evaluation metrics.\n    assert mode == tf.estimator.ModeKeys.EVAL\n\n    # Calculate root mean squared error\n    rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n\n    # Add the rmse to collection of evaluation metrics.\n    eval_metrics = {\"rmse\": rmse}\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        # Report sum of error for compatibility with pre-made estimators.\n        loss=total_loss,\n        eval_metric_ops=eval_metrics)\n\nAnd here I used train_and_evaluate api like this:\n    model = tf.estimator.Estimator(\n        model_fn=my_dnn_regression_fn,\n        model_dir=\n        \"./models/temp\",\n        params={\n            'feature_columns': feature_columns,\n            'learning_rate': 0.1,\n            'optimizer': tf.train.AdamOptimizer,\n            'hidden_units': [20, 20, 20, 20]\n        })\n    train_spec = tf.estimator.TrainSpec(input_fn=input_train,max_steps=10000)\n    eval_spec = tf.estimator.EvalSpec(input_fn=input_dev,steps=10000,throttle_secs=60,start_delay_secs=0)\n    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\n\nDid I set the parameter properly? Or, is there other solution for this?", "body": "Hi, recently I used [custom_estimator.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/get_started/regression/custom_regression.py) to build regression model.  In order to clear out the changes of loss value in the training set and validation set. I need to know that how to show the loss curve of training and validation set at the same time. I tried to use train_and_evaluate api of estimator and i got the following picture.\r\n![image](https://user-images.githubusercontent.com/19348442/39224762-76df9b92-487b-11e8-888d-b64825e99c82.png)\r\nAs it show, the result of evaluation is a point, but i want a line like the loss curve of training set. Just like the picture as shown below.\r\n![image](https://user-images.githubusercontent.com/19348442/39280146-2d129f22-4930-11e8-81e7-a2379b924eb0.png)\r\nHere is my system information:\r\n\r\n- Have i written custom code: N/A\r\n\r\n- OS: Tested on windows 10 1709.\r\n\r\n- Tensorflow installed from Anaconda 5.1.0 with python 3.6.4\r\n\r\n- Tensorflow version-tested on tensorflow-gpu 1.7.0\r\n\r\n- CUDA/cuDNN version: 9.0 for TF 1.7\r\n\r\n- GPU mode: Nvidia Quadro  K2100M\uff0c 2G of memory\r\n\r\n- Bazel version: N/A\r\n\r\n- Exact command to reproduce: N/A\r\nHere is the customed estimator:\r\n\r\n```\r\ndef my_dnn_regression_fn(features, labels, mode, params):\r\n    top = tf.feature_column.input_layer(features, params['feature_columns'])\r\n\r\n    for units in params.get('hidden_units', [20]):\r\n        top = tf.layers.dense(inputs=top, units=units, activation=tf.nn.relu)\r\n\r\n    output_layer = tf.layers.dense(inputs=top, units=1)\r\n\r\n    output_layer = tf.cast(output_layer, tf.float64)\r\n   \r\n    predictions = tf.squeeze(output_layer, 1)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        # In 'PREDICT' mode we only need to return predictions.\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode, predictions={\"predictions\": predictions})\r\n\r\n    # calculate the loss using mean squared error\r\n    average_loss = tf.losses.mean_squared_error(labels, predictions)\r\n\r\n    # Pre-made estimators use the total_loss instead of the average,\r\n    # so report total_loss for compatibility.\r\n    batch_size = tf.shape(labels)[0]\r\n    total_loss = tf.to_float(batch_size) * average_loss\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = params.get(\"optimizer\", tf.train.AdamOptimizer)\r\n        optimizer = optimizer(params.get(\"learning_rate\", None))\r\n        train_op = optimizer.minimize(\r\n            loss=average_loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode, loss=total_loss, train_op=train_op)\r\n\r\n    # In the evaluation mode we will calculate evaluation metrics.\r\n    assert mode == tf.estimator.ModeKeys.EVAL\r\n\r\n    # Calculate root mean squared error\r\n    rmse = tf.metrics.root_mean_squared_error(labels, predictions)\r\n\r\n    # Add the rmse to collection of evaluation metrics.\r\n    eval_metrics = {\"rmse\": rmse}\r\n\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        # Report sum of error for compatibility with pre-made estimators.\r\n        loss=total_loss,\r\n        eval_metric_ops=eval_metrics)\r\n````\r\n\r\nAnd here I used train_and_evaluate api like this:\r\n```\r\n    model = tf.estimator.Estimator(\r\n        model_fn=my_dnn_regression_fn,\r\n        model_dir=\r\n        \"./models/temp\",\r\n        params={\r\n            'feature_columns': feature_columns,\r\n            'learning_rate': 0.1,\r\n            'optimizer': tf.train.AdamOptimizer,\r\n            'hidden_units': [20, 20, 20, 20]\r\n        })\r\n    train_spec = tf.estimator.TrainSpec(input_fn=input_train,max_steps=10000)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=input_dev,steps=10000,throttle_secs=60,start_delay_secs=0)\r\n    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\r\n```\r\nDid I set the parameter properly? Or, is there other solution for this?\r\n"}