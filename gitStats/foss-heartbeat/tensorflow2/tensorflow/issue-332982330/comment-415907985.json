{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/415907985", "html_url": "https://github.com/tensorflow/tensorflow/issues/20074#issuecomment-415907985", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20074", "id": 415907985, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNTkwNzk4NQ==", "user": {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-24T23:30:43Z", "updated_at": "2018-08-24T23:30:43Z", "author_association": "MEMBER", "body_html": "<p>I poked a little more. I could not reproduce the problem. Meaning I always get lookup and the embedding var to be on ps. I did however observed the difference in the placement of dynamic_seq2seq/encoder/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 op, which moves from worker to ps whenever we have 1 partition. The reason is that the embedding var is on ps, lookup is collocate with the embedding var, and the lookup result is fed to the tensor array op that wants to be collocated with its input (this can be a performance bug on its own). When num_partitions &gt; 1 there is an intermediate node after the lookup (namely ParallelDynamicStitch) that has no collocation constraints and is placed on the worker. So now tensor array ops can be on the worker and lookup on ps.</p>\n<p>Again, I did not witness with tf.device assignments being ignored though internally added collocation constraints can be very intuitive and potentially harmful. To keep things more consistent between having 1 partition or more I would try wrapping embedding lookup with tf.identity. Indeed I am thinking to change embedding_lookup op itself to just always do it on its own.</p>", "body_text": "I poked a little more. I could not reproduce the problem. Meaning I always get lookup and the embedding var to be on ps. I did however observed the difference in the placement of dynamic_seq2seq/encoder/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 op, which moves from worker to ps whenever we have 1 partition. The reason is that the embedding var is on ps, lookup is collocate with the embedding var, and the lookup result is fed to the tensor array op that wants to be collocated with its input (this can be a performance bug on its own). When num_partitions > 1 there is an intermediate node after the lookup (namely ParallelDynamicStitch) that has no collocation constraints and is placed on the worker. So now tensor array ops can be on the worker and lookup on ps.\nAgain, I did not witness with tf.device assignments being ignored though internally added collocation constraints can be very intuitive and potentially harmful. To keep things more consistent between having 1 partition or more I would try wrapping embedding lookup with tf.identity. Indeed I am thinking to change embedding_lookup op itself to just always do it on its own.", "body": "I poked a little more. I could not reproduce the problem. Meaning I always get lookup and the embedding var to be on ps. I did however observed the difference in the placement of dynamic_seq2seq/encoder/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 op, which moves from worker to ps whenever we have 1 partition. The reason is that the embedding var is on ps, lookup is collocate with the embedding var, and the lookup result is fed to the tensor array op that wants to be collocated with its input (this can be a performance bug on its own). When num_partitions > 1 there is an intermediate node after the lookup (namely ParallelDynamicStitch) that has no collocation constraints and is placed on the worker. So now tensor array ops can be on the worker and lookup on ps.\r\n\r\nAgain, I did not witness with tf.device assignments being ignored though internally added collocation constraints can be very intuitive and potentially harmful. To keep things more consistent between having 1 partition or more I would try wrapping embedding lookup with tf.identity. Indeed I am thinking to change embedding_lookup op itself to just always do it on its own."}