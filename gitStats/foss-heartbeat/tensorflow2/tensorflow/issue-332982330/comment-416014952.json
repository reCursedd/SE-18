{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/416014952", "html_url": "https://github.com/tensorflow/tensorflow/issues/20074#issuecomment-416014952", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20074", "id": 416014952, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjAxNDk1Mg==", "user": {"login": "sj6077", "id": 2465713, "node_id": "MDQ6VXNlcjI0NjU3MTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/2465713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sj6077", "html_url": "https://github.com/sj6077", "followers_url": "https://api.github.com/users/sj6077/followers", "following_url": "https://api.github.com/users/sj6077/following{/other_user}", "gists_url": "https://api.github.com/users/sj6077/gists{/gist_id}", "starred_url": "https://api.github.com/users/sj6077/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sj6077/subscriptions", "organizations_url": "https://api.github.com/users/sj6077/orgs", "repos_url": "https://api.github.com/users/sj6077/repos", "events_url": "https://api.github.com/users/sj6077/events{/privacy}", "received_events_url": "https://api.github.com/users/sj6077/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-26T05:23:32Z", "updated_at": "2018-08-26T05:23:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>First of all, thank you for digging the problem. I met the problem when training the below German model.<br>\n<code>python -m nmt.nmt \\ --src=de --tgt=en \\ --ckpt=/path/to/checkpoint/translate.ckpt \\ --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \\ --out_dir=/tmp/deen_gnmt \\ --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \\ --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \\ --inference_output_file=/tmp/deen_gnmt/output_infer \\ --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en</code></p>\n<p>I can wrap embedding lookup with <code>tf.Identity</code> as you suggested but I'm still not sure it's a general solution because collocation constraints and <code>tf.device</code> are always stuck together. In my case, I spent for a long time to find the reason for the failure of the distributed training.<br>\nHowever, if TensorFlow prioritizes internal collocation constraints more than user device placement by <code>tf.device</code> as its policy, maybe I must follow the policy and find another solution.</p>", "body_text": "First of all, thank you for digging the problem. I met the problem when training the below German model.\npython -m nmt.nmt \\ --src=de --tgt=en \\ --ckpt=/path/to/checkpoint/translate.ckpt \\ --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \\ --out_dir=/tmp/deen_gnmt \\ --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \\ --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \\ --inference_output_file=/tmp/deen_gnmt/output_infer \\ --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en\nI can wrap embedding lookup with tf.Identity as you suggested but I'm still not sure it's a general solution because collocation constraints and tf.device are always stuck together. In my case, I spent for a long time to find the reason for the failure of the distributed training.\nHowever, if TensorFlow prioritizes internal collocation constraints more than user device placement by tf.device as its policy, maybe I must follow the policy and find another solution.", "body": "First of all, thank you for digging the problem. I met the problem when training the below German model.\r\n`python -m nmt.nmt \\\r\n    --src=de --tgt=en \\\r\n    --ckpt=/path/to/checkpoint/translate.ckpt \\\r\n    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \\\r\n    --out_dir=/tmp/deen_gnmt \\\r\n    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \\\r\n    --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \\\r\n    --inference_output_file=/tmp/deen_gnmt/output_infer \\\r\n    --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en` \r\n\r\nI can wrap embedding lookup with `tf.Identity` as you suggested but I'm still not sure it's a general solution because collocation constraints and `tf.device` are always stuck together. In my case, I spent for a long time to find the reason for the failure of the distributed training. \r\nHowever, if TensorFlow prioritizes internal collocation constraints more than user device placement by `tf.device` as its policy, maybe I must follow the policy and find another solution."}