{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/419663375", "html_url": "https://github.com/tensorflow/tensorflow/issues/20074#issuecomment-419663375", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20074", "id": 419663375, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTY2MzM3NQ==", "user": {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-08T18:25:38Z", "updated_at": "2018-09-08T18:25:38Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>I can wrap embedding lookup with tf.Identity<br>\nI did that already and the submitted the code . So there should no longer be any asymmetry between 1 and many partitions. I would not be surprised if  your code works now.<br>\nIn my case, I spent for a long time to find the reason for the failure of the distributed training.<br>\nIt does seem a common occurrence. Note The graphdef has all collocation constraints explicitly mentioned which can be useful for debugging.</p>\n</blockquote>\n<blockquote>\n<p>However, if TensorFlow prioritizes internal collocation constraints more<br>\nI do not think we do. If device placement and collocation cant be reconciled the placement should fail.</p>\n</blockquote>", "body_text": "I can wrap embedding lookup with tf.Identity\nI did that already and the submitted the code . So there should no longer be any asymmetry between 1 and many partitions. I would not be surprised if  your code works now.\nIn my case, I spent for a long time to find the reason for the failure of the distributed training.\nIt does seem a common occurrence. Note The graphdef has all collocation constraints explicitly mentioned which can be useful for debugging.\n\n\nHowever, if TensorFlow prioritizes internal collocation constraints more\nI do not think we do. If device placement and collocation cant be reconciled the placement should fail.", "body": "> I can wrap embedding lookup with tf.Identity\r\nI did that already and the submitted the code . So there should no longer be any asymmetry between 1 and many partitions. I would not be surprised if  your code works now. \r\n> In my case, I spent for a long time to find the reason for the failure of the distributed training.\r\nIt does seem a common occurrence. Note The graphdef has all collocation constraints explicitly mentioned which can be useful for debugging.\r\n\r\n>However, if TensorFlow prioritizes internal collocation constraints more \r\nI do not think we do. If device placement and collocation cant be reconciled the placement should fail.\r\n\r\n\r\n\r\n\r\n"}