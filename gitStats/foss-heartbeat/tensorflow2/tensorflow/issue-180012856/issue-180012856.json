{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4651", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4651/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4651/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4651/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4651", "id": 180012856, "node_id": "MDU6SXNzdWUxODAwMTI4NTY=", "number": 4651, "title": "Distributed tensorflow hangs ", "user": {"login": "passerbydj", "id": 20178426, "node_id": "MDQ6VXNlcjIwMTc4NDI2", "avatar_url": "https://avatars1.githubusercontent.com/u/20178426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/passerbydj", "html_url": "https://github.com/passerbydj", "followers_url": "https://api.github.com/users/passerbydj/followers", "following_url": "https://api.github.com/users/passerbydj/following{/other_user}", "gists_url": "https://api.github.com/users/passerbydj/gists{/gist_id}", "starred_url": "https://api.github.com/users/passerbydj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/passerbydj/subscriptions", "organizations_url": "https://api.github.com/users/passerbydj/orgs", "repos_url": "https://api.github.com/users/passerbydj/repos", "events_url": "https://api.github.com/users/passerbydj/events{/privacy}", "received_events_url": "https://api.github.com/users/passerbydj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-09-29T10:36:28Z", "updated_at": "2017-08-31T03:06:16Z", "closed_at": "2016-10-12T23:58:32Z", "author_association": "NONE", "body_html": "<p>I set up a distributed tensorflow according to the inception example(<a href=\"https://github.com/tensorflow/models/tree/master/inception/inception\">https://github.com/tensorflow/models/tree/master/inception/inception</a>), with a very deep network.<br>\nDuring training(minutes or hours), it must run into hanging. No error occurs in ps or workers, but the cpu/gpu usage falls to zero and no any further steps are made.<br>\nIf I replace the network with a much simpler one(much less variables), all things work fine.<br>\nIs there any limit on the amount of vars placed on ps? or any suggestions to deal with the problem?</p>\n<p>Setup: tensorflow r0.10 build on cuda 7.5 and cudnn v5</p>", "body_text": "I set up a distributed tensorflow according to the inception example(https://github.com/tensorflow/models/tree/master/inception/inception), with a very deep network.\nDuring training(minutes or hours), it must run into hanging. No error occurs in ps or workers, but the cpu/gpu usage falls to zero and no any further steps are made.\nIf I replace the network with a much simpler one(much less variables), all things work fine.\nIs there any limit on the amount of vars placed on ps? or any suggestions to deal with the problem?\nSetup: tensorflow r0.10 build on cuda 7.5 and cudnn v5", "body": "I set up a distributed tensorflow according to the inception example(https://github.com/tensorflow/models/tree/master/inception/inception), with a very deep network.\nDuring training(minutes or hours), it must run into hanging. No error occurs in ps or workers, but the cpu/gpu usage falls to zero and no any further steps are made.\nIf I replace the network with a much simpler one(much less variables), all things work fine.\nIs there any limit on the amount of vars placed on ps? or any suggestions to deal with the problem?\n\nSetup: tensorflow r0.10 build on cuda 7.5 and cudnn v5 \n"}