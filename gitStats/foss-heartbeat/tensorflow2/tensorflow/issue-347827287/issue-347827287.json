{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21400", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21400/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21400/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21400/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21400", "id": 347827287, "node_id": "MDU6SXNzdWUzNDc4MjcyODc=", "number": 21400, "title": "contrib.rnn.ConvLSTMCell: zero_state has wrong size if cell has skip connections", "user": {"login": "su-si", "id": 9898285, "node_id": "MDQ6VXNlcjk4OTgyODU=", "avatar_url": "https://avatars0.githubusercontent.com/u/9898285?v=4", "gravatar_id": "", "url": "https://api.github.com/users/su-si", "html_url": "https://github.com/su-si", "followers_url": "https://api.github.com/users/su-si/followers", "following_url": "https://api.github.com/users/su-si/following{/other_user}", "gists_url": "https://api.github.com/users/su-si/gists{/gist_id}", "starred_url": "https://api.github.com/users/su-si/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/su-si/subscriptions", "organizations_url": "https://api.github.com/users/su-si/orgs", "repos_url": "https://api.github.com/users/su-si/repos", "events_url": "https://api.github.com/users/su-si/events{/privacy}", "received_events_url": "https://api.github.com/users/su-si/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-08-06T08:30:45Z", "updated_at": "2018-08-29T09:34:14Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>There seems to be a bug in how ConvLSTMCell handles its states:</p>\n<p>When calculating the output size, ConvLSTMCell has one more channel dimension (in general, as many more channels as there are input channels) than what was given as output_channels.<br>\nThis is only if skip_connection was True at initialization, <strong>and makes sense</strong> (the input is concatenated to the output --&gt; n_channels_out = n_channels_in + original_n_channels_out).</p>\n<p>Also, when creating a cell and applying it to inputs and a state:</p>\n<h3>Minimal reproducible example</h3>\n<pre><code>cell = tf.contrib.rnn.ConvLSTMCell(conv_ndims=2, input_shape=[80,80,1], output_channels=2,\n                                               kernel_shape=[5,5], skip_connection=True)\nstate= cell.zero_state(10, dtype=tf.float32)\noutput, next_state = cell(last_outputs, state)\n</code></pre>\n<p>--&gt; next_state is an LSTMStateTuple with shapes (?, 80, 80, 2) and (?, 80, 80, <strong>3</strong>). Still makes sense since it stores its hidden state as well as outputs and re-uses them in the next timestep, and the outputs gained an additional channel. But cell.zero_state() will have other shapes ( (?, 80, 80, 2) and (?, 80, 80, 2).)</p>\n<p>The zero_state() calculation seems wrong; the code reads (in contrib/rnn/python/ops/rnn_cell.py):</p>\n<pre><code>    self._total_output_channels = output_channels\n    if self._skip_connection:\n      self._total_output_channels += self._input_shape[-1]\n\n    state_size = tensor_shape.TensorShape(\n        self._input_shape[:-1] + [self._output_channels])\n    self._state_size = rnn_cell_impl.LSTMStateTuple(state_size, state_size)\n    self._output_size = tensor_shape.TensorShape(\n        self._input_shape[:-1] + [self._total_output_channels])\n</code></pre>\n<p>Shouldn't it be not<code> rnn_cell_impl.LSTMStateTuple(state_size, state_size)</code>, but <code>rnn_cell_impl.LSTMStateTuple(state_size, self._output_size)</code> ?</p>\n<p>Funnily, it does not disturb the cell to be called with a state of size (80, 80, <strong>2</strong>)   - but for me it raised an error when called again with its next state which has size (80, 80, 3). For completeness, this error was (and this error is <em>not my problem</em>, my problem was inconsistency of shapes of the cell states):</p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \"/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_comm.py\", line 1079, in do_it<br>\nresult = pydevd_vars.evaluate_expression(self.thread_id, self.frame_id, self.expression, self.doExec)<br>\nFile \"/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_vars.py\", line 352, in evaluate_expression<br>\nExec(expression, updated_globals, frame.f_locals)<br>\nFile \"/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_exec2.py\", line 3, in Exec<br>\nexec(exp, global_vars, local_vars)<br>\nFile \"\", line 1, in <br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 232, in <strong>call</strong><br>\nreturn super(RNNCell, self).<strong>call</strong>(inputs, state)<br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 717, in <strong>call</strong><br>\noutputs = self.call(inputs, *args, **kwargs)<br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\", line 2113, in call<br>\n4 * self._output_channels, self._use_bias)<br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\", line 2207, in _conv<br>\n\"kernel\", filter_size + [total_arg_size_depth, num_features], dtype=dtype)<br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable<br>\nconstraint=constraint)<br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable<br>\nconstraint=constraint)<br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 417, in get_variable<br>\nreturn custom_getter(**custom_getter_kwargs)<br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 235, in _rnn_get_variable<br>\nvariable = getter(*args, **kwargs)<br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter<br>\nuse_resource=use_resource, constraint=constraint)<br>\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 738, in _get_single_variable<br>\nfound_var.get_shape()))<br>\nValueError: Trying to share variable conv_lstm_cell/kernel, but specified shape (5, 5, 4, 8) and found shape (5, 5, 3, 8).</p>\n</blockquote>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.8.0</li>\n<li><strong>Python version</strong>:<br>\n3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li></li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li></li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li>(using CPU version)</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li>(using CPU version)</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nSee description above</li>\n<li><strong>Mobile device</strong>:<br>\nN/A</li>\n</ul>", "body_text": "There seems to be a bug in how ConvLSTMCell handles its states:\nWhen calculating the output size, ConvLSTMCell has one more channel dimension (in general, as many more channels as there are input channels) than what was given as output_channels.\nThis is only if skip_connection was True at initialization, and makes sense (the input is concatenated to the output --> n_channels_out = n_channels_in + original_n_channels_out).\nAlso, when creating a cell and applying it to inputs and a state:\nMinimal reproducible example\ncell = tf.contrib.rnn.ConvLSTMCell(conv_ndims=2, input_shape=[80,80,1], output_channels=2,\n                                               kernel_shape=[5,5], skip_connection=True)\nstate= cell.zero_state(10, dtype=tf.float32)\noutput, next_state = cell(last_outputs, state)\n\n--> next_state is an LSTMStateTuple with shapes (?, 80, 80, 2) and (?, 80, 80, 3). Still makes sense since it stores its hidden state as well as outputs and re-uses them in the next timestep, and the outputs gained an additional channel. But cell.zero_state() will have other shapes ( (?, 80, 80, 2) and (?, 80, 80, 2).)\nThe zero_state() calculation seems wrong; the code reads (in contrib/rnn/python/ops/rnn_cell.py):\n    self._total_output_channels = output_channels\n    if self._skip_connection:\n      self._total_output_channels += self._input_shape[-1]\n\n    state_size = tensor_shape.TensorShape(\n        self._input_shape[:-1] + [self._output_channels])\n    self._state_size = rnn_cell_impl.LSTMStateTuple(state_size, state_size)\n    self._output_size = tensor_shape.TensorShape(\n        self._input_shape[:-1] + [self._total_output_channels])\n\nShouldn't it be not rnn_cell_impl.LSTMStateTuple(state_size, state_size), but rnn_cell_impl.LSTMStateTuple(state_size, self._output_size) ?\nFunnily, it does not disturb the cell to be called with a state of size (80, 80, 2)   - but for me it raised an error when called again with its next state which has size (80, 80, 3). For completeness, this error was (and this error is not my problem, my problem was inconsistency of shapes of the cell states):\n\nTraceback (most recent call last):\nFile \"/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_comm.py\", line 1079, in do_it\nresult = pydevd_vars.evaluate_expression(self.thread_id, self.frame_id, self.expression, self.doExec)\nFile \"/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_vars.py\", line 352, in evaluate_expression\nExec(expression, updated_globals, frame.f_locals)\nFile \"/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_exec2.py\", line 3, in Exec\nexec(exp, global_vars, local_vars)\nFile \"\", line 1, in \nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 232, in call\nreturn super(RNNCell, self).call(inputs, state)\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 717, in call\noutputs = self.call(inputs, *args, **kwargs)\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\", line 2113, in call\n4 * self._output_channels, self._use_bias)\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\", line 2207, in _conv\n\"kernel\", filter_size + [total_arg_size_depth, num_features], dtype=dtype)\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\nconstraint=constraint)\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\nconstraint=constraint)\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 417, in get_variable\nreturn custom_getter(**custom_getter_kwargs)\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 235, in _rnn_get_variable\nvariable = getter(*args, **kwargs)\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\nuse_resource=use_resource, constraint=constraint)\nFile \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 738, in _get_single_variable\nfound_var.get_shape()))\nValueError: Trying to share variable conv_lstm_cell/kernel, but specified shape (5, 5, 4, 8) and found shape (5, 5, 3, 8).\n\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 14.04\nTensorFlow installed from (source or binary):\nbinary\nTensorFlow version (use command below):\n1.8.0\nPython version:\n3.6\nBazel version (if compiling from source):\n\nGCC/Compiler version (if compiling from source):\n\nCUDA/cuDNN version:\n(using CPU version)\nGPU model and memory:\n(using CPU version)\nExact command to reproduce:\nSee description above\nMobile device:\nN/A", "body": "There seems to be a bug in how ConvLSTMCell handles its states:\r\n\r\nWhen calculating the output size, ConvLSTMCell has one more channel dimension (in general, as many more channels as there are input channels) than what was given as output_channels. \r\nThis is only if skip_connection was True at initialization, **and makes sense** (the input is concatenated to the output --> n_channels_out = n_channels_in + original_n_channels_out).\r\n\r\nAlso, when creating a cell and applying it to inputs and a state:\r\n### Minimal reproducible example\r\n```\r\ncell = tf.contrib.rnn.ConvLSTMCell(conv_ndims=2, input_shape=[80,80,1], output_channels=2,\r\n                                               kernel_shape=[5,5], skip_connection=True)\r\nstate= cell.zero_state(10, dtype=tf.float32)\r\noutput, next_state = cell(last_outputs, state)\r\n```\r\n\r\n--> next_state is an LSTMStateTuple with shapes (?, 80, 80, 2) and (?, 80, 80, **3**). Still makes sense since it stores its hidden state as well as outputs and re-uses them in the next timestep, and the outputs gained an additional channel. But cell.zero_state() will have other shapes ( (?, 80, 80, 2) and (?, 80, 80, 2).)\r\n\r\n\r\nThe zero_state() calculation seems wrong; the code reads (in contrib/rnn/python/ops/rnn_cell.py):\r\n\r\n```\r\n    self._total_output_channels = output_channels\r\n    if self._skip_connection:\r\n      self._total_output_channels += self._input_shape[-1]\r\n\r\n    state_size = tensor_shape.TensorShape(\r\n        self._input_shape[:-1] + [self._output_channels])\r\n    self._state_size = rnn_cell_impl.LSTMStateTuple(state_size, state_size)\r\n    self._output_size = tensor_shape.TensorShape(\r\n        self._input_shape[:-1] + [self._total_output_channels])\r\n```\r\n\r\nShouldn't it be not` rnn_cell_impl.LSTMStateTuple(state_size, state_size)`, but `rnn_cell_impl.LSTMStateTuple(state_size, self._output_size)` ?\r\n\r\nFunnily, it does not disturb the cell to be called with a state of size (80, 80, **2**)   - but for me it raised an error when called again with its next state which has size (80, 80, 3). For completeness, this error was (and this error is _not my problem_, my problem was inconsistency of shapes of the cell states): \r\n\r\n> Traceback (most recent call last):\r\n  File \"/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_comm.py\", line 1079, in do_it\r\n    result = pydevd_vars.evaluate_expression(self.thread_id, self.frame_id, self.expression, self.doExec)\r\n  File \"/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_vars.py\", line 352, in evaluate_expression\r\n    Exec(expression, updated_globals, frame.f_locals)\r\n  File \"/home/me/programs/pycharm-community-2016.1.1/helpers/pydev/_pydevd_bundle/pydevd_exec2.py\", line 3, in Exec\r\n    exec(exp, global_vars, local_vars)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 232, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 717, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\", line 2113, in call\r\n    4 * self._output_channels, self._use_bias)\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\", line 2207, in _conv\r\n    \"kernel\", filter_size + [total_arg_size_depth, num_features], dtype=dtype)\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\r\n    constraint=constraint)\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\r\n    constraint=constraint)\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 417, in get_variable\r\n    return custom_getter(**custom_getter_kwargs)\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 235, in _rnn_get_variable\r\n    variable = getter(*args, **kwargs)\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"/home/me/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 738, in _get_single_variable\r\n    found_var.get_shape()))\r\nValueError: Trying to share variable conv_lstm_cell/kernel, but specified shape (5, 5, 4, 8) and found shape (5, 5, 3, 8).\r\n\r\n\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: \r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n- **Python version**:\r\n3.6\r\n- **Bazel version (if compiling from source)**: \r\n-\r\n- **GCC/Compiler version (if compiling from source)**:\r\n-\r\n- **CUDA/cuDNN version**:\r\n- (using CPU version)\r\n- **GPU model and memory**:\r\n- (using CPU version)\r\n- **Exact command to reproduce**:\r\nSee description above\r\n- **Mobile device**:\r\nN/A\r\n"}