{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19477", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19477/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19477/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19477/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19477", "id": 325448673, "node_id": "MDU6SXNzdWUzMjU0NDg2NzM=", "number": 19477, "title": "tf.contrib.tensorrt.create_inference_graph fails for ssd_mobilenet_v2_coco network with error", "user": {"login": "x42x64", "id": 25886140, "node_id": "MDQ6VXNlcjI1ODg2MTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/25886140?v=4", "gravatar_id": "", "url": "https://api.github.com/users/x42x64", "html_url": "https://github.com/x42x64", "followers_url": "https://api.github.com/users/x42x64/followers", "following_url": "https://api.github.com/users/x42x64/following{/other_user}", "gists_url": "https://api.github.com/users/x42x64/gists{/gist_id}", "starred_url": "https://api.github.com/users/x42x64/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/x42x64/subscriptions", "organizations_url": "https://api.github.com/users/x42x64/orgs", "repos_url": "https://api.github.com/users/x42x64/repos", "events_url": "https://api.github.com/users/x42x64/events{/privacy}", "received_events_url": "https://api.github.com/users/x42x64/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-22T20:21:52Z", "updated_at": "2018-05-22T20:25:09Z", "closed_at": "2018-05-22T20:25:08Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 17.10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>: 3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0 / 7.1.3</li>\n<li><strong>GPU model and memory</strong>: GeForce 940MX / 2GB</li>\n<li><strong>TensorRT version</strong>: 3.0.4</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.tensorrt as trt\nfrom tensorflow.python.platform import gfile\n\n\ndef get_graph_def_from_pb(file):\n    with gfile.FastGFile(file, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        return graph_def\n\n\ntrt_graph = trt.create_inference_graph(\n                input_graph_def=get_graph_def_from_pb(\"~/Downloads/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\"), # from http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n                outputs=[\"detection_boxes\"],\n                max_batch_size=1,\n                max_workspace_size_bytes=500000000,\n                precision_mode=\"FP32\")\n\n\nwith gfile.FastGFile(\"trt_frozen_graph.pb\", 'wb') as f:\n    f.write(trt_graph.SerializeToString())\n</code></pre>\n<h3>Describe the problem</h3>\n<p>When I try to convert ssd_mobilenet_v2_coco for the use with TensorRT in tensorflow, the convert step fails with below's Traceback. This seems to be a problem within tf.contrib.tensorrt.create_inference_graph <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/python/trt_convert.py#L115\">Line 115</a> which itself is an exception risen if the c++ binary <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/python/trt_convert.py#L102\">trt_convert</a> returns an error. Unfortunately here my bread-crumbs end, as I've not found the line in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/convert/convert_graph.cc\">convert_graph.cc</a>, which might be responsible for this behaviour.</p>\n<p>Using a much simpler network (just a few CNN layers and FCN layers) works - so it does not seem to be my TensorRT installation.</p>\n<h3>Source code / logs</h3>\n<pre><code>Traceback (most recent call last):\n  File \"/home/q0we9saweq/projects/tf_helpers/tensorrt_prep.py\", line 20, in &lt;module&gt;\n    precision_mode=\"FP32\")\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 115, in create_inference_graph\n    int(msg[0]))\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:\n\t [[Node: MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones = Fill[T=DT_INT32](MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\n\t [[Node: MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones = Fill[T=DT_INT32](MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.8.0\nPython version: 3.6.5\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0 / 7.1.3\nGPU model and memory: GeForce 940MX / 2GB\nTensorRT version: 3.0.4\nExact command to reproduce:\n\nimport tensorflow as tf\nimport tensorflow.contrib.tensorrt as trt\nfrom tensorflow.python.platform import gfile\n\n\ndef get_graph_def_from_pb(file):\n    with gfile.FastGFile(file, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        return graph_def\n\n\ntrt_graph = trt.create_inference_graph(\n                input_graph_def=get_graph_def_from_pb(\"~/Downloads/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\"), # from http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n                outputs=[\"detection_boxes\"],\n                max_batch_size=1,\n                max_workspace_size_bytes=500000000,\n                precision_mode=\"FP32\")\n\n\nwith gfile.FastGFile(\"trt_frozen_graph.pb\", 'wb') as f:\n    f.write(trt_graph.SerializeToString())\n\nDescribe the problem\nWhen I try to convert ssd_mobilenet_v2_coco for the use with TensorRT in tensorflow, the convert step fails with below's Traceback. This seems to be a problem within tf.contrib.tensorrt.create_inference_graph Line 115 which itself is an exception risen if the c++ binary trt_convert returns an error. Unfortunately here my bread-crumbs end, as I've not found the line in convert_graph.cc, which might be responsible for this behaviour.\nUsing a much simpler network (just a few CNN layers and FCN layers) works - so it does not seem to be my TensorRT installation.\nSource code / logs\nTraceback (most recent call last):\n  File \"/home/q0we9saweq/projects/tf_helpers/tensorrt_prep.py\", line 20, in <module>\n    precision_mode=\"FP32\")\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 115, in create_inference_graph\n    int(msg[0]))\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:\n\t [[Node: MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones = Fill[T=DT_INT32](MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\n\t [[Node: MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones = Fill[T=DT_INT32](MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7.1.3\r\n- **GPU model and memory**: GeForce 940MX / 2GB \r\n- **TensorRT version**: 3.0.4\r\n- **Exact command to reproduce**:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.tensorrt as trt\r\nfrom tensorflow.python.platform import gfile\r\n\r\n\r\ndef get_graph_def_from_pb(file):\r\n    with gfile.FastGFile(file, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        return graph_def\r\n\r\n\r\ntrt_graph = trt.create_inference_graph(\r\n                input_graph_def=get_graph_def_from_pb(\"~/Downloads/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\"), # from http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\r\n                outputs=[\"detection_boxes\"],\r\n                max_batch_size=1,\r\n                max_workspace_size_bytes=500000000,\r\n                precision_mode=\"FP32\")\r\n\r\n\r\nwith gfile.FastGFile(\"trt_frozen_graph.pb\", 'wb') as f:\r\n    f.write(trt_graph.SerializeToString())\r\n```\r\n\r\n### Describe the problem\r\nWhen I try to convert ssd_mobilenet_v2_coco for the use with TensorRT in tensorflow, the convert step fails with below's Traceback. This seems to be a problem within tf.contrib.tensorrt.create_inference_graph [Line 115](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/python/trt_convert.py#L115) which itself is an exception risen if the c++ binary [trt_convert](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/python/trt_convert.py#L102) returns an error. Unfortunately here my bread-crumbs end, as I've not found the line in [convert_graph.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/convert/convert_graph.cc), which might be responsible for this behaviour.\r\n\r\nUsing a much simpler network (just a few CNN layers and FCN layers) works - so it does not seem to be my TensorRT installation.\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/q0we9saweq/projects/tf_helpers/tensorrt_prep.py\", line 20, in <module>\r\n    precision_mode=\"FP32\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 115, in create_inference_graph\r\n    int(msg[0]))\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:\r\n\t [[Node: MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones = Fill[T=DT_INT32](MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\r\n\t [[Node: MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones = Fill[T=DT_INT32](MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\r\n```\r\n"}