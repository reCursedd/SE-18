{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285892399", "html_url": "https://github.com/tensorflow/tensorflow/issues/8287#issuecomment-285892399", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8287", "id": 285892399, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTg5MjM5OQ==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-11T19:07:58Z", "updated_at": "2017-03-11T19:11:00Z", "author_association": "MEMBER", "body_html": "<p>There's no reason an optimization pass needs to keep the same ops...  In my personal opinion, the cleanest design is for the program to be written using generic ops with well defined semantics, and then any 3rd party wanting to add support for their super-fast library or accelerator supplies two things:</p>\n<ol>\n<li>A set of additional (internal) ops if necessary, with whatever flags/attrs extra inputs/outputs they feel like, and</li>\n<li>A GraphOptimizationPass which transparently, at execution time, rewrites the clean, portable graphdef to use their platform specific ops (the rewritten graph is cached so this is a one-off overhead)</li>\n</ol>\n<p>This way we avoid the need to add extra vendor- and platform-specific options to every op in TensorFlow ;-)</p>\n<p>When we translate parts of a graph into XLA, entire parts of the graph are replaced by a single 'XlaLaunch' op and and conv ops in the fused computation get turned into XLA convolution operators which support arbitrary layout permutations.  The XLA optimizer is free to do what it likes with physical layout (though on GPU we still currently call out to cuDNN).</p>", "body_text": "There's no reason an optimization pass needs to keep the same ops...  In my personal opinion, the cleanest design is for the program to be written using generic ops with well defined semantics, and then any 3rd party wanting to add support for their super-fast library or accelerator supplies two things:\n\nA set of additional (internal) ops if necessary, with whatever flags/attrs extra inputs/outputs they feel like, and\nA GraphOptimizationPass which transparently, at execution time, rewrites the clean, portable graphdef to use their platform specific ops (the rewritten graph is cached so this is a one-off overhead)\n\nThis way we avoid the need to add extra vendor- and platform-specific options to every op in TensorFlow ;-)\nWhen we translate parts of a graph into XLA, entire parts of the graph are replaced by a single 'XlaLaunch' op and and conv ops in the fused computation get turned into XLA convolution operators which support arbitrary layout permutations.  The XLA optimizer is free to do what it likes with physical layout (though on GPU we still currently call out to cuDNN).", "body": "There's no reason an optimization pass needs to keep the same ops...  In my personal opinion, the cleanest design is for the program to be written using generic ops with well defined semantics, and then any 3rd party wanting to add support for their super-fast library or accelerator supplies two things:\r\n1) A set of additional (internal) ops if necessary, with whatever flags/attrs extra inputs/outputs they feel like, and\r\n2) A GraphOptimizationPass which transparently, at execution time, rewrites the clean, portable graphdef to use their platform specific ops (the rewritten graph is cached so this is a one-off overhead)\r\n\r\nThis way we avoid the need to add extra vendor- and platform-specific options to every op in TensorFlow ;-) \r\n\r\nWhen we translate parts of a graph into XLA, entire parts of the graph are replaced by a single 'XlaLaunch' op and and conv ops in the fused computation get turned into XLA convolution operators which support arbitrary layout permutations.  The XLA optimizer is free to do what it likes with physical layout (though on GPU we still currently call out to cuDNN)."}