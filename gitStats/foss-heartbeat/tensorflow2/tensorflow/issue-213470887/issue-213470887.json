{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8287", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8287/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8287/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8287/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8287", "id": 213470887, "node_id": "MDU6SXNzdWUyMTM0NzA4ODc=", "number": 8287, "title": "filter_format for Native Layouts of Convolution Filter Weights", "user": {"login": "cancan101", "id": 51059, "node_id": "MDQ6VXNlcjUxMDU5", "avatar_url": "https://avatars1.githubusercontent.com/u/51059?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cancan101", "html_url": "https://github.com/cancan101", "followers_url": "https://api.github.com/users/cancan101/followers", "following_url": "https://api.github.com/users/cancan101/following{/other_user}", "gists_url": "https://api.github.com/users/cancan101/gists{/gist_id}", "starred_url": "https://api.github.com/users/cancan101/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cancan101/subscriptions", "organizations_url": "https://api.github.com/users/cancan101/orgs", "repos_url": "https://api.github.com/users/cancan101/repos", "events_url": "https://api.github.com/users/cancan101/events{/privacy}", "received_events_url": "https://api.github.com/users/cancan101/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-03-10T22:38:51Z", "updated_at": "2017-06-16T20:04:52Z", "closed_at": "2017-06-16T19:17:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Repost from: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"204598482\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7187\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7187/hovercard?comment_id=285217279&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-285217279\">#7187 (comment)</a>.</p>\n<p>I suggest adding a <code>filter_format</code> to the conv2d op to indicate how the filter weights are stored (similar to <code>data_format</code> for the input / output format).</p>\n<p>The <a href=\"https://www.tensorflow.org/extend/tool_developers/#weight_formats\" rel=\"nofollow\">docs for TF</a> state:</p>\n<blockquote>\n<p>The ordering of convolution weight values is often tricky to deal with when converting between different frameworks. In TensorFlow, the filter weights for the Conv2D operation are stored on the second input, and are expected to be in the order [filter_height, filter_width, input_depth, output_depth], where filter_count increasing by one means moving to an adjacent value in memory.</p>\n</blockquote>\n<p>This will allow various kernel implementers to store weights more efficiently (and mark them as such for down the road conversions).</p>\n<p>In addition to cuDNN, BNNS (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"161807205\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3001\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3001/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3001\">#3001</a>) also uses that same format.</p>\n<p>This needs to be benchmarked, but I assume there is some cost to having to transpose the filter weights during both fprop and bprop (twice). This could even explain some of the performance differences between TF and other frameworks (eg pytorch).</p>\n<p>It looks like TF already <a href=\"https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/stream_executor/dnn.h#L340-L346\">has some concept of these difference</a>. Although as of now all weights are converted from / back to one canonical format.</p>\n<p>While <a href=\"https://github.com/tensorflow/tensorflow/issues/8227\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8227/hovercard\">most users won't tune this</a>, it could be used by xla / optimization passes to choose better layouts.</p>", "body_text": "Repost from: #7187 (comment).\nI suggest adding a filter_format to the conv2d op to indicate how the filter weights are stored (similar to data_format for the input / output format).\nThe docs for TF state:\n\nThe ordering of convolution weight values is often tricky to deal with when converting between different frameworks. In TensorFlow, the filter weights for the Conv2D operation are stored on the second input, and are expected to be in the order [filter_height, filter_width, input_depth, output_depth], where filter_count increasing by one means moving to an adjacent value in memory.\n\nThis will allow various kernel implementers to store weights more efficiently (and mark them as such for down the road conversions).\nIn addition to cuDNN, BNNS (#3001) also uses that same format.\nThis needs to be benchmarked, but I assume there is some cost to having to transpose the filter weights during both fprop and bprop (twice). This could even explain some of the performance differences between TF and other frameworks (eg pytorch).\nIt looks like TF already has some concept of these difference. Although as of now all weights are converted from / back to one canonical format.\nWhile most users won't tune this, it could be used by xla / optimization passes to choose better layouts.", "body": "Repost from: https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-285217279.\r\n\r\nI suggest adding a `filter_format` to the conv2d op to indicate how the filter weights are stored (similar to `data_format` for the input / output format). \r\n\r\nThe [docs for TF](https://www.tensorflow.org/extend/tool_developers/#weight_formats) state: \r\n>The ordering of convolution weight values is often tricky to deal with when converting between different frameworks. In TensorFlow, the filter weights for the Conv2D operation are stored on the second input, and are expected to be in the order [filter_height, filter_width, input_depth, output_depth], where filter_count increasing by one means moving to an adjacent value in memory.\r\n\r\nThis will allow various kernel implementers to store weights more efficiently (and mark them as such for down the road conversions).\r\n\r\nIn addition to cuDNN, BNNS (https://github.com/tensorflow/tensorflow/issues/3001) also uses that same format.\r\n\r\nThis needs to be benchmarked, but I assume there is some cost to having to transpose the filter weights during both fprop and bprop (twice). This could even explain some of the performance differences between TF and other frameworks (eg pytorch).\r\n\r\nIt looks like TF already [has some concept of these difference](https://github.com/tensorflow/tensorflow/blob/067cba5e4b873829f6cdfa61256079d2cfc45d02/tensorflow/stream_executor/dnn.h#L340-L346). Although as of now all weights are converted from / back to one canonical format.\r\n\r\nWhile [most users won't tune this](https://github.com/tensorflow/tensorflow/issues/8227), it could be used by xla / optimization passes to choose better layouts."}