{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/386175250", "html_url": "https://github.com/tensorflow/tensorflow/issues/18994#issuecomment-386175250", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18994", "id": 386175250, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjE3NTI1MA==", "user": {"login": "oscarriddle", "id": 13745902, "node_id": "MDQ6VXNlcjEzNzQ1OTAy", "avatar_url": "https://avatars0.githubusercontent.com/u/13745902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oscarriddle", "html_url": "https://github.com/oscarriddle", "followers_url": "https://api.github.com/users/oscarriddle/followers", "following_url": "https://api.github.com/users/oscarriddle/following{/other_user}", "gists_url": "https://api.github.com/users/oscarriddle/gists{/gist_id}", "starred_url": "https://api.github.com/users/oscarriddle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oscarriddle/subscriptions", "organizations_url": "https://api.github.com/users/oscarriddle/orgs", "repos_url": "https://api.github.com/users/oscarriddle/repos", "events_url": "https://api.github.com/users/oscarriddle/events{/privacy}", "received_events_url": "https://api.github.com/users/oscarriddle/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-03T02:18:05Z", "updated_at": "2018-05-03T02:18:05Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10539540\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samikama\">@samikama</a></p>\n<p>I imported the tensorrt and now the saved_model can be loaded.<br>\nBut when do the sess.run, a CUDA malloc related error popped up.</p>\n<pre><code>import/PermConstNCHWToNHWC-LayoutOptimizer\nimport/PermConstNHWCToNCHW-LayoutOptimizer\nimport/Placeholder\nimport/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer\nimport/InceptionV3/Logits/Conv2d_1c_1x1/BiasAdd-0-0-TransposeNCHWToNHWC-LayoutOptimizer\nimport/InceptionV3/Logits/SpatialSqueeze\nimport/InceptionV3/my_trt_op0\nShape\n2018-05-03 10:08:14.007927: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\nterminate called after throwing an instance of 'nvinfer1::CudaError'\n</code></pre>\n<p>My script:</p>\n<pre><code>    with tf.Session(graph=tf.Graph()) as sess:\n        tf.saved_model.loader.load(sess, ['serve'], './fs2')\n        graph = tf.get_default_graph()\n        for op in tf.get_default_graph().get_operations():\n            print str(op.name)                                                                                                                           \n        inp = graph.get_tensor_by_name(\"import/Placeholder:0\")\n        oup = graph.get_tensor_by_name(\"import/InceptionV3/Logits/SpatialSqueeze:0\")\n        sess.run(oup, {inp: batch_input})\n</code></pre>\n<p>This seems to be a TRT related issue, I'm also trying to figure it out.<br>\nIf you could share some comments, it will be much grateful.</p>\n<p>Thanks,</p>", "body_text": "Hi @samikama\nI imported the tensorrt and now the saved_model can be loaded.\nBut when do the sess.run, a CUDA malloc related error popped up.\nimport/PermConstNCHWToNHWC-LayoutOptimizer\nimport/PermConstNHWCToNCHW-LayoutOptimizer\nimport/Placeholder\nimport/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer\nimport/InceptionV3/Logits/Conv2d_1c_1x1/BiasAdd-0-0-TransposeNCHWToNHWC-LayoutOptimizer\nimport/InceptionV3/Logits/SpatialSqueeze\nimport/InceptionV3/my_trt_op0\nShape\n2018-05-03 10:08:14.007927: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\nterminate called after throwing an instance of 'nvinfer1::CudaError'\n\nMy script:\n    with tf.Session(graph=tf.Graph()) as sess:\n        tf.saved_model.loader.load(sess, ['serve'], './fs2')\n        graph = tf.get_default_graph()\n        for op in tf.get_default_graph().get_operations():\n            print str(op.name)                                                                                                                           \n        inp = graph.get_tensor_by_name(\"import/Placeholder:0\")\n        oup = graph.get_tensor_by_name(\"import/InceptionV3/Logits/SpatialSqueeze:0\")\n        sess.run(oup, {inp: batch_input})\n\nThis seems to be a TRT related issue, I'm also trying to figure it out.\nIf you could share some comments, it will be much grateful.\nThanks,", "body": "Hi @samikama \r\n\r\nI imported the tensorrt and now the saved_model can be loaded. \r\nBut when do the sess.run, a CUDA malloc related error popped up.\r\n```\r\nimport/PermConstNCHWToNHWC-LayoutOptimizer\r\nimport/PermConstNHWCToNCHW-LayoutOptimizer\r\nimport/Placeholder\r\nimport/InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer\r\nimport/InceptionV3/Logits/Conv2d_1c_1x1/BiasAdd-0-0-TransposeNCHWToNHWC-LayoutOptimizer\r\nimport/InceptionV3/Logits/SpatialSqueeze\r\nimport/InceptionV3/my_trt_op0\r\nShape\r\n2018-05-03 10:08:14.007927: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\r\nterminate called after throwing an instance of 'nvinfer1::CudaError'\r\n```\r\n\r\nMy script:\r\n```\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        tf.saved_model.loader.load(sess, ['serve'], './fs2')\r\n        graph = tf.get_default_graph()\r\n        for op in tf.get_default_graph().get_operations():\r\n            print str(op.name)                                                                                                                           \r\n        inp = graph.get_tensor_by_name(\"import/Placeholder:0\")\r\n        oup = graph.get_tensor_by_name(\"import/InceptionV3/Logits/SpatialSqueeze:0\")\r\n        sess.run(oup, {inp: batch_input})\r\n```\r\n\r\nThis seems to be a TRT related issue, I'm also trying to figure it out. \r\nIf you could share some comments, it will be much grateful.\r\n\r\nThanks,"}