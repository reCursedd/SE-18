{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/402601562", "html_url": "https://github.com/tensorflow/tensorflow/issues/18994#issuecomment-402601562", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18994", "id": 402601562, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjYwMTU2Mg==", "user": {"login": "oscarriddle", "id": 13745902, "node_id": "MDQ6VXNlcjEzNzQ1OTAy", "avatar_url": "https://avatars0.githubusercontent.com/u/13745902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oscarriddle", "html_url": "https://github.com/oscarriddle", "followers_url": "https://api.github.com/users/oscarriddle/followers", "following_url": "https://api.github.com/users/oscarriddle/following{/other_user}", "gists_url": "https://api.github.com/users/oscarriddle/gists{/gist_id}", "starred_url": "https://api.github.com/users/oscarriddle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oscarriddle/subscriptions", "organizations_url": "https://api.github.com/users/oscarriddle/orgs", "repos_url": "https://api.github.com/users/oscarriddle/repos", "events_url": "https://api.github.com/users/oscarriddle/events{/privacy}", "received_events_url": "https://api.github.com/users/oscarriddle/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-05T04:11:48Z", "updated_at": "2018-07-05T04:11:48Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=27752913\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/caibobit\">@caibobit</a><br>\nFirst of all, I think you need to clarify the difference between frozen model and the saved_model. They are almost the same but saved_model provided signatureDef that you need to take care.</p>\n<p>When you export a saved model, it requires to set a tag for signaturedef, and later when you load it, you have to specify the tag. From your log, I think you can check the tag of your saved model, by using saved_model_cli:</p>\n<pre><code>2018-07-05 18:57:34.946953: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: fail. Took 113 microseconds.\n2018-07-05 18:57:34.946973: E tensorflow_serving/util/retrier.cc:37] Loading servable: {name: mnist version: 5} failed: Not found: Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\n</code></pre>\n<p>For the second, the c++ tensorflow shall already support loading serialized model, you can check this: <a href=\"url\">https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f</a></p>\n<p>The frozen model can be converted to saved model and vice versa, because they are both serialized ones.</p>\n<p>Thanks,</p>", "body_text": "@caibobit\nFirst of all, I think you need to clarify the difference between frozen model and the saved_model. They are almost the same but saved_model provided signatureDef that you need to take care.\nWhen you export a saved model, it requires to set a tag for signaturedef, and later when you load it, you have to specify the tag. From your log, I think you can check the tag of your saved model, by using saved_model_cli:\n2018-07-05 18:57:34.946953: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: fail. Took 113 microseconds.\n2018-07-05 18:57:34.946973: E tensorflow_serving/util/retrier.cc:37] Loading servable: {name: mnist version: 5} failed: Not found: Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\n\nFor the second, the c++ tensorflow shall already support loading serialized model, you can check this: https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f\nThe frozen model can be converted to saved model and vice versa, because they are both serialized ones.\nThanks,", "body": "@caibobit \r\nFirst of all, I think you need to clarify the difference between frozen model and the saved_model. They are almost the same but saved_model provided signatureDef that you need to take care.\r\n\r\nWhen you export a saved model, it requires to set a tag for signaturedef, and later when you load it, you have to specify the tag. From your log, I think you can check the tag of your saved model, by using saved_model_cli:\r\n```\r\n2018-07-05 18:57:34.946953: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: fail. Took 113 microseconds.\r\n2018-07-05 18:57:34.946973: E tensorflow_serving/util/retrier.cc:37] Loading servable: {name: mnist version: 5} failed: Not found: Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\r\n```\r\nFor the second, the c++ tensorflow shall already support loading serialized model, you can check this: [https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f](url)\r\n\r\nThe frozen model can be converted to saved model and vice versa, because they are both serialized ones.\r\n\r\nThanks,"}