{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/386178772", "html_url": "https://github.com/tensorflow/tensorflow/issues/18994#issuecomment-386178772", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18994", "id": 386178772, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjE3ODc3Mg==", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-03T02:47:22Z", "updated_at": "2018-05-03T02:47:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13745902\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oscarriddle\">@oscarriddle</a> , unfortunately this is a nuisance on our part and we will take care of it when TRT4.0 comes out. It is happening because of memory allocation incompatibility between TRT3 and TF. In order to workaround it you need to tell TF to leave some memory for TRT. You can do this by passing GPUOptions to session configuration like below. But since TF allocation policy defines the amount of allocated memory at first call, you have to make sure that you set the memory fraction at the first session call. I would suggest you to create a dummy session without any graph or anything right after you import tensorflow, before doing anything else. Of course, you have to tune the fraction to accommodate your model and memory size you are passing to TensorRT during the conversion call.</p>\n<div class=\"highlight highlight-source-python\"><pre>  sess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>tf.GPUOptions(<span class=\"pl-v\">per_process_gpu_memory_fraction</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.50</span>)))</pre></div>\n<p>Let me know if this helps.<br>\nCheers,</p>", "body_text": "@oscarriddle , unfortunately this is a nuisance on our part and we will take care of it when TRT4.0 comes out. It is happening because of memory allocation incompatibility between TRT3 and TF. In order to workaround it you need to tell TF to leave some memory for TRT. You can do this by passing GPUOptions to session configuration like below. But since TF allocation policy defines the amount of allocated memory at first call, you have to make sure that you set the memory fraction at the first session call. I would suggest you to create a dummy session without any graph or anything right after you import tensorflow, before doing anything else. Of course, you have to tune the fraction to accommodate your model and memory size you are passing to TensorRT during the conversion call.\n  sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50)))\nLet me know if this helps.\nCheers,", "body": "@oscarriddle , unfortunately this is a nuisance on our part and we will take care of it when TRT4.0 comes out. It is happening because of memory allocation incompatibility between TRT3 and TF. In order to workaround it you need to tell TF to leave some memory for TRT. You can do this by passing GPUOptions to session configuration like below. But since TF allocation policy defines the amount of allocated memory at first call, you have to make sure that you set the memory fraction at the first session call. I would suggest you to create a dummy session without any graph or anything right after you import tensorflow, before doing anything else. Of course, you have to tune the fraction to accommodate your model and memory size you are passing to TensorRT during the conversion call.\r\n\r\n```python\r\n  sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50)))\r\n```\r\nLet me know if this helps.\r\nCheers,\r\n"}