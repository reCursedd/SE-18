{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/367531236", "html_url": "https://github.com/tensorflow/tensorflow/issues/11564#issuecomment-367531236", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11564", "id": 367531236, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzUzMTIzNg==", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-22T00:58:31Z", "updated_at": "2018-02-22T00:58:31Z", "author_association": "MEMBER", "body_html": "<p>Hi, I know this has sat for way too long; I'm sorry about that.</p>\n<p>I just tried this again, and it seems to work.  I'm using the latest version of inception from tensorflow/models, and I hacked it like you did to allow enabling XLA.  (I'll try to push a change upstream to add this flag.)  I ran locally with a P100 and CUDA 8.</p>\n<p>Without XLA:<br>\n<code>step 2950, loss = 12.21 (122.6 examples/sec; 0.261 sec/batch)</code></p>\n<p>With XLA:<br>\n<code>step 2950, loss = 12.51 (122.7 examples/sec; 0.261 sec/batch)</code></p>\n<p>I think the 12.21 vs 12.51 is noise; I let the XLA one run a bit longer and by step 3360 the loss was down to 11.96.</p>\n<p>I'm going to close this because...it seems to be working?  But please don't hesitate to reopen this if it's not working for you.  We have more folks working on this, so I think / hope we'll be able to have a turnaround time significantly shorter than 6 months.  :)</p>\n<p>Thanks again for your patience, and sorry this dragged on so long.</p>", "body_text": "Hi, I know this has sat for way too long; I'm sorry about that.\nI just tried this again, and it seems to work.  I'm using the latest version of inception from tensorflow/models, and I hacked it like you did to allow enabling XLA.  (I'll try to push a change upstream to add this flag.)  I ran locally with a P100 and CUDA 8.\nWithout XLA:\nstep 2950, loss = 12.21 (122.6 examples/sec; 0.261 sec/batch)\nWith XLA:\nstep 2950, loss = 12.51 (122.7 examples/sec; 0.261 sec/batch)\nI think the 12.21 vs 12.51 is noise; I let the XLA one run a bit longer and by step 3360 the loss was down to 11.96.\nI'm going to close this because...it seems to be working?  But please don't hesitate to reopen this if it's not working for you.  We have more folks working on this, so I think / hope we'll be able to have a turnaround time significantly shorter than 6 months.  :)\nThanks again for your patience, and sorry this dragged on so long.", "body": "Hi, I know this has sat for way too long; I'm sorry about that.\r\n\r\nI just tried this again, and it seems to work.  I'm using the latest version of inception from tensorflow/models, and I hacked it like you did to allow enabling XLA.  (I'll try to push a change upstream to add this flag.)  I ran locally with a P100 and CUDA 8.\r\n\r\nWithout XLA:\r\n```step 2950, loss = 12.21 (122.6 examples/sec; 0.261 sec/batch)```\r\n\r\nWith XLA:\r\n```step 2950, loss = 12.51 (122.7 examples/sec; 0.261 sec/batch)```\r\n\r\nI think the 12.21 vs 12.51 is noise; I let the XLA one run a bit longer and by step 3360 the loss was down to 11.96.\r\n\r\nI'm going to close this because...it seems to be working?  But please don't hesitate to reopen this if it's not working for you.  We have more folks working on this, so I think / hope we'll be able to have a turnaround time significantly shorter than 6 months.  :)\r\n\r\nThanks again for your patience, and sorry this dragged on so long."}