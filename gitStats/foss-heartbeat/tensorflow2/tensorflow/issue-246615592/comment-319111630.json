{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/319111630", "html_url": "https://github.com/tensorflow/tensorflow/issues/11894#issuecomment-319111630", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11894", "id": 319111630, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTExMTYzMA==", "user": {"login": "amehrjou", "id": 17348736, "node_id": "MDQ6VXNlcjE3MzQ4NzM2", "avatar_url": "https://avatars2.githubusercontent.com/u/17348736?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amehrjou", "html_url": "https://github.com/amehrjou", "followers_url": "https://api.github.com/users/amehrjou/followers", "following_url": "https://api.github.com/users/amehrjou/following{/other_user}", "gists_url": "https://api.github.com/users/amehrjou/gists{/gist_id}", "starred_url": "https://api.github.com/users/amehrjou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amehrjou/subscriptions", "organizations_url": "https://api.github.com/users/amehrjou/orgs", "repos_url": "https://api.github.com/users/amehrjou/repos", "events_url": "https://api.github.com/users/amehrjou/events{/privacy}", "received_events_url": "https://api.github.com/users/amehrjou/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-31T15:50:40Z", "updated_at": "2017-07-31T15:53:06Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> Exactly. I also realized that the size of the graph becomes very large when <code>tf.gradients</code> is applied for the second time. For example the first <code>tf.gradients</code> added around 100 nodes to a graph with 80 nodes. But the second derivative surprisingly added 1000 nodes to the graph for each dimension of the vector with respect to which derivates are computed. This means that even for an input vector of medium size, the graph does not fit in the memory of any GPU. Thus, do you think if there is any chance to compute Hessian diagonal in a feasible way?</p>", "body_text": "@yaroslavvb Exactly. I also realized that the size of the graph becomes very large when tf.gradients is applied for the second time. For example the first tf.gradients added around 100 nodes to a graph with 80 nodes. But the second derivative surprisingly added 1000 nodes to the graph for each dimension of the vector with respect to which derivates are computed. This means that even for an input vector of medium size, the graph does not fit in the memory of any GPU. Thus, do you think if there is any chance to compute Hessian diagonal in a feasible way?", "body": "@yaroslavvb Exactly. I also realized that the size of the graph becomes very large when `tf.gradients` is applied for the second time. For example the first `tf.gradients` added around 100 nodes to a graph with 80 nodes. But the second derivative surprisingly added 1000 nodes to the graph for each dimension of the vector with respect to which derivates are computed. This means that even for an input vector of medium size, the graph does not fit in the memory of any GPU. Thus, do you think if there is any chance to compute Hessian diagonal in a feasible way? "}