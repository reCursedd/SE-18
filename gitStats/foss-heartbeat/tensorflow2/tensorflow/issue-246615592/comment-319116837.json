{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/319116837", "html_url": "https://github.com/tensorflow/tensorflow/issues/11894#issuecomment-319116837", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11894", "id": 319116837, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTExNjgzNw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-31T16:08:02Z", "updated_at": "2017-07-31T16:08:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Oh you want just the diagonal of Hessian? For piecewise linear neural network (ie, ReLU activations) computing diagonal can be done cheaply (ie, for square loss it's the product of corresponding activation and backprop variances). In a general network, there's no fast way (<a href=\"http://www.cs.utoronto.ca/~ilya/pubs/2012/CurvProp.pdf\" rel=\"nofollow\">http://www.cs.utoronto.ca/~ilya/pubs/2012/CurvProp.pdf</a>)</p>", "body_text": "Oh you want just the diagonal of Hessian? For piecewise linear neural network (ie, ReLU activations) computing diagonal can be done cheaply (ie, for square loss it's the product of corresponding activation and backprop variances). In a general network, there's no fast way (http://www.cs.utoronto.ca/~ilya/pubs/2012/CurvProp.pdf)", "body": "Oh you want just the diagonal of Hessian? For piecewise linear neural network (ie, ReLU activations) computing diagonal can be done cheaply (ie, for square loss it's the product of corresponding activation and backprop variances). In a general network, there's no fast way (http://www.cs.utoronto.ca/~ilya/pubs/2012/CurvProp.pdf)"}