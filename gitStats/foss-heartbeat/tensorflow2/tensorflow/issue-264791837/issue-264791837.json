{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13650", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13650/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13650/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13650/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13650", "id": 264791837, "node_id": "MDU6SXNzdWUyNjQ3OTE4Mzc=", "number": 13650, "title": "What's the difference between function eval() and sess.run()?", "user": {"login": "LaiPiXiong", "id": 15033269, "node_id": "MDQ6VXNlcjE1MDMzMjY5", "avatar_url": "https://avatars2.githubusercontent.com/u/15033269?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LaiPiXiong", "html_url": "https://github.com/LaiPiXiong", "followers_url": "https://api.github.com/users/LaiPiXiong/followers", "following_url": "https://api.github.com/users/LaiPiXiong/following{/other_user}", "gists_url": "https://api.github.com/users/LaiPiXiong/gists{/gist_id}", "starred_url": "https://api.github.com/users/LaiPiXiong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LaiPiXiong/subscriptions", "organizations_url": "https://api.github.com/users/LaiPiXiong/orgs", "repos_url": "https://api.github.com/users/LaiPiXiong/repos", "events_url": "https://api.github.com/users/LaiPiXiong/events{/privacy}", "received_events_url": "https://api.github.com/users/LaiPiXiong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-12T02:08:59Z", "updated_at": "2017-10-12T13:05:36Z", "closed_at": "2017-10-12T13:05:36Z", "author_association": "NONE", "body_html": "<p>Codes are as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> encoding: utf-8</span>\n<span class=\"pl-k\">import</span> load\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> sklearn.metrics <span class=\"pl-k\">import</span> confusion_matrix\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_chunk</span>(<span class=\"pl-smi\">samples</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">chunkSize</span>):\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(samples) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">len</span>(labels):\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">Exception</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Length of samples and labels must equal<span class=\"pl-pds\">'</span></span>)\n    stepStart <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">while</span> stepStart <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">len</span>(samples):\n        stepEnd <span class=\"pl-k\">=</span> stepStart <span class=\"pl-k\">+</span> chunkSize\n        <span class=\"pl-k\">if</span> stepEnd <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">len</span>(samples):\n            <span class=\"pl-k\">yield</span> i, samples[stepStart: stepEnd], labels[stepStart: stepEnd]\n            i <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        stepStart <span class=\"pl-k\">=</span> stepEnd\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">NetWork</span>():\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_hidden</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">num_labels</span>, <span class=\"pl-smi\">image_size</span>, <span class=\"pl-smi\">channel</span>):\n        <span class=\"pl-c1\">self</span>.num_hidden <span class=\"pl-k\">=</span> num_hidden\n        <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">=</span> batch_size\n        <span class=\"pl-c1\">self</span>.num_labels <span class=\"pl-k\">=</span> num_labels\n\n        <span class=\"pl-c1\">self</span>.image_size <span class=\"pl-k\">=</span> image_size\n        <span class=\"pl-c1\">self</span>.num_channel<span class=\"pl-k\">=</span> channel\n\n        <span class=\"pl-c1\">self</span>.tf_train_samples <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n        <span class=\"pl-c1\">self</span>.tf_train_labels  <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n        <span class=\"pl-c1\">self</span>.tf_test_samples  <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n\n        <span class=\"pl-c1\">self</span>.graph <span class=\"pl-k\">=</span> tf.Graph()      \n        <span class=\"pl-c1\">self</span>.define_graph()\n        <span class=\"pl-c1\">self</span>.sess  <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">graph</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.graph)\n        <span class=\"pl-c1\">self</span>.writer<span class=\"pl-k\">=</span> tf.summary.FileWriter(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>./board<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">self</span>.graph)\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">define_graph</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">with</span> <span class=\"pl-c1\">self</span>.graph.as_default():\n            <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>inputs<span class=\"pl-pds\">'</span></span>):\n                <span class=\"pl-c1\">self</span>.tf_train_samples <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span> <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">self</span>.batch_size, <span class=\"pl-c1\">self</span>.image_size, <span class=\"pl-c1\">self</span>.image_size, <span class=\"pl-c1\">self</span>.num_channel), <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_samples<span class=\"pl-pds\">'</span></span>)\n                <span class=\"pl-c1\">self</span>.tf_train_labels   <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span> <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">self</span>.batch_size, <span class=\"pl-c1\">self</span>.num_labels), <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_labels<span class=\"pl-pds\">'</span></span>)\n                <span class=\"pl-c1\">self</span>.tf_test_samples  <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span> <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">self</span>.batch_size, <span class=\"pl-c1\">self</span>.image_size, <span class=\"pl-c1\">self</span>.image_size, <span class=\"pl-c1\">self</span>.num_channel), <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test_labels<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc1<span class=\"pl-pds\">'</span></span>):\n                weight1 <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([<span class=\"pl-c1\">self</span>.image_size <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.image_size, <span class=\"pl-c1\">self</span>.num_hidden], <span class=\"pl-v\">stddev</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>), <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight1<span class=\"pl-pds\">'</span></span>)\n                bias1   <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([<span class=\"pl-c1\">self</span>.num_hidden], <span class=\"pl-v\">stddev</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>), <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias1<span class=\"pl-pds\">'</span></span>)\n                tf.summary.histogram(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight1<span class=\"pl-pds\">'</span></span>, weight1)\n                tf.summary.histogram(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias1<span class=\"pl-pds\">'</span></span>, bias1)\n            <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc2<span class=\"pl-pds\">'</span></span>):\n                weight2 <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([<span class=\"pl-c1\">self</span>.num_hidden, <span class=\"pl-c1\">self</span>.num_labels], <span class=\"pl-v\">stddev</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>), <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight2<span class=\"pl-pds\">'</span></span>)\n                bias2   <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([<span class=\"pl-c1\">self</span>.num_labels], <span class=\"pl-v\">stddev</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>), <span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias2<span class=\"pl-pds\">'</span></span>)\n                tf.summary.histogram(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight2<span class=\"pl-pds\">'</span></span>, weight2)\n                tf.summary.histogram(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias2<span class=\"pl-pds\">'</span></span>, bias2)\n\n            <span class=\"pl-k\">def</span> <span class=\"pl-en\">model</span>(<span class=\"pl-smi\">data</span>):\n                shape <span class=\"pl-k\">=</span> data.get_shape().as_list()\n                reshape <span class=\"pl-k\">=</span> tf.reshape(data, [shape[<span class=\"pl-c1\">0</span>], shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> shape[<span class=\"pl-c1\">3</span>]])\n                <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc1_model<span class=\"pl-pds\">'</span></span>):\n                    hidden  <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(reshape, weight1) <span class=\"pl-k\">+</span> bias1)   \n                <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc2_model<span class=\"pl-pds\">'</span></span>):\n                    <span class=\"pl-k\">return</span> tf.matmul(hidden, weight2) <span class=\"pl-k\">+</span> bias2\n\n            logits <span class=\"pl-k\">=</span> model(<span class=\"pl-c1\">self</span>.tf_train_samples)\n            <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>):\n                <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">logits</span> <span class=\"pl-k\">=</span> logits, <span class=\"pl-v\">labels</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.tf_train_labels))\n                tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">self</span>.loss)\n            <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>optimizer<span class=\"pl-pds\">'</span></span>):\n                <span class=\"pl-c1\">self</span>.optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.01</span>).minimize(<span class=\"pl-c1\">self</span>.loss)\n            <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>predictions<span class=\"pl-pds\">'</span></span>):\n                <span class=\"pl-c1\">self</span>.train_prediction <span class=\"pl-k\">=</span> tf.nn.softmax(logits)\n                <span class=\"pl-c1\">self</span>.test_prediction  <span class=\"pl-k\">=</span> tf.nn.softmax(model(<span class=\"pl-c1\">self</span>.tf_test_samples))\n            <span class=\"pl-c1\">self</span>.merged <span class=\"pl-k\">=</span> tf.summary.merge_all()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">run</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>## train</span>\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">print_confusion_matrix</span>(<span class=\"pl-smi\">confusionMatrix</span>):\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Consusion Matrix: <span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-k\">for</span> i, line <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(confusionMatrix):\n                <span class=\"pl-c1\">print</span> line, line[i]<span class=\"pl-k\">/</span>np.sum(line)\n            \n            a <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n            <span class=\"pl-k\">for</span> i, column <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(np.transpose(confusionMatrix, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>))):\n                a <span class=\"pl-k\">+=</span> (column[i] <span class=\"pl-k\">/</span> np.sum(column)) <span class=\"pl-k\">*</span> (np.sum(column) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">26000</span>)\n                <span class=\"pl-c1\">print</span> column[i] <span class=\"pl-k\">/</span> np.sum(column), \n            <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>, np.sum(confusionMatrix, a)\n\n        <span class=\"pl-k\">with</span> <span class=\"pl-c1\">self</span>.sess <span class=\"pl-k\">as</span> sess:\n            tf.global_variables_initializer().run()\n            <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Start Training<span class=\"pl-pds\">'</span></span>\n\n            train_samples, train_labels <span class=\"pl-k\">=</span> load.loadmat_data(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_32x32.mat<span class=\"pl-pds\">'</span></span>)\n            train_samples, train_labels <span class=\"pl-k\">=</span> load.reformat(train_samples, train_labels)\n            train_samples <span class=\"pl-k\">=</span> load.normalize(train_samples)\n\n            test_samples, test_labels <span class=\"pl-k\">=</span> load.loadmat_data(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test_32x32.mat<span class=\"pl-pds\">'</span></span>)\n            test_samples, test_labels <span class=\"pl-k\">=</span> load.reformat(test_samples, test_labels)\n            test_samples <span class=\"pl-k\">=</span> load.normalize(test_samples)\n\n            <span class=\"pl-k\">for</span> i, samples, labels <span class=\"pl-k\">in</span> get_chunk(train_samples, train_labels, <span class=\"pl-v\">chunkSize</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.batch_size):\n                _, l, predictions, summary <span class=\"pl-k\">=</span> sess.run([<span class=\"pl-c1\">self</span>.optimizer, <span class=\"pl-c1\">self</span>.loss, <span class=\"pl-c1\">self</span>.train_prediction, <span class=\"pl-c1\">self</span>.merged], <span class=\"pl-v\">feed_dict</span> <span class=\"pl-k\">=</span> {<span class=\"pl-c1\">self</span>.tf_train_samples: samples, <span class=\"pl-c1\">self</span>.tf_train_labels: labels})\n                <span class=\"pl-c1\">self</span>.writer.add_summary(summary, i)\n                accuracy, _ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.accuracy(predictions, labels)\n                <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">50</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Minibatch loss at step <span class=\"pl-c1\">%d</span>: loss is <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (i, l)\n                    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Minibatch accuracy: <span class=\"pl-c1\">%.1f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> accuracy\n\n            accuracies <span class=\"pl-k\">=</span> []\n            confusionMatrices <span class=\"pl-k\">=</span> []\n            <span class=\"pl-k\">for</span> i, samples, labels <span class=\"pl-k\">in</span> get_chunk(test_samples, test_labels, <span class=\"pl-v\">chunkSize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.batch_size):\n                result <span class=\"pl-k\">=</span> sess.run(<span class=\"pl-c1\">self</span>.test_prediction, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{<span class=\"pl-c1\">self</span>.tf_test_samples: samples})\n\t\t\t\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})</span>\n                accuracy, cm <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.accuracy(result, labels, <span class=\"pl-v\">need_confusion_matrix</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                accuracies.append(accuracy)\n                confusionMatrices.append(cm)\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Test Accuracy: <span class=\"pl-c1\">%.1f%%</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> accuracy)\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> Average  Accuracy:<span class=\"pl-pds\">'</span></span>, np.average(accuracies))\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Standard Deviation:<span class=\"pl-pds\">'</span></span>, np.std(accuracies))\n            print_confusion_matrix(np.add.reduce(confusionMatrices))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">accuracy</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">predictions</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">need_confusion_matrix</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>):\n        _predictions <span class=\"pl-k\">=</span> np.argmax(predictions, <span class=\"pl-c1\">1</span>)\n        _labels      <span class=\"pl-k\">=</span> np.argmax(labels, <span class=\"pl-c1\">1</span>)\n        cm <span class=\"pl-k\">=</span> confusion_matrix(_labels, _predictions) <span class=\"pl-k\">if</span> need_confusion_matrix <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>\n        accuracy <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">100</span> <span class=\"pl-k\">*</span> np.sum(_predictions <span class=\"pl-k\">==</span> _labels) <span class=\"pl-k\">/</span> predictions.shape[<span class=\"pl-c1\">0</span>])\n        <span class=\"pl-k\">return</span> accuracy, cm\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    net <span class=\"pl-k\">=</span> NetWork(<span class=\"pl-v\">num_hidden</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, <span class=\"pl-v\">batch_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>, <span class=\"pl-v\">num_labels</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">image_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">channel</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>)\n    net.run()</pre></div>\n<p>When I use 'result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})', it works fine, while in 'result = sess.run(self.test_prediction, feed_dict={self.tf_test_samples: samples})', it shows error 'ValueError: operands could not be broadcast together with shapes (10,10) (9,9) '. SO, what's difference between function eval() and sess.run()</p>", "body_text": "Codes are as follows:\n# encoding: utf-8\nimport load\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\ndef get_chunk(samples, labels, chunkSize):\n    if len(samples) != len(labels):\n        raise Exception('Length of samples and labels must equal')\n    stepStart = 0\n    i = 0\n    while stepStart < len(samples):\n        stepEnd = stepStart + chunkSize\n        if stepEnd < len(samples):\n            yield i, samples[stepStart: stepEnd], labels[stepStart: stepEnd]\n            i += 1\n        stepStart = stepEnd\n\nclass NetWork():\n    def __init__(self, num_hidden, batch_size, num_labels, image_size, channel):\n        self.num_hidden = num_hidden\n        self.batch_size = batch_size\n        self.num_labels = num_labels\n\n        self.image_size = image_size\n        self.num_channel= channel\n\n        self.tf_train_samples = None\n        self.tf_train_labels  = None\n        self.tf_test_samples  = None\n\n        self.graph = tf.Graph()      \n        self.define_graph()\n        self.sess  = tf.Session(graph = self.graph)\n        self.writer= tf.summary.FileWriter('./board', self.graph)\n    \n    def define_graph(self):\n        with self.graph.as_default():\n            with tf.name_scope('inputs'):\n                self.tf_train_samples = tf.placeholder(tf.float32, shape = (self.batch_size, self.image_size, self.image_size, self.num_channel), name = 'train_samples')\n                self.tf_train_labels   = tf.placeholder(tf.float32, shape = (self.batch_size, self.num_labels), name = 'train_labels')\n                self.tf_test_samples  = tf.placeholder(tf.float32, shape = (self.batch_size, self.image_size, self.image_size, self.num_channel), name = 'test_labels')\n            with tf.name_scope('fc1'):\n                weight1 = tf.Variable(tf.truncated_normal([self.image_size * self.image_size, self.num_hidden], stddev = 0.1), name = 'weight1')\n                bias1   = tf.Variable(tf.truncated_normal([self.num_hidden], stddev = 0.1), name = 'bias1')\n                tf.summary.histogram('weight1', weight1)\n                tf.summary.histogram('bias1', bias1)\n            with tf.name_scope('fc2'):\n                weight2 = tf.Variable(tf.truncated_normal([self.num_hidden, self.num_labels], stddev = 0.1), name = 'weight2')\n                bias2   = tf.Variable(tf.truncated_normal([self.num_labels], stddev = 0.1), name = 'bias2')\n                tf.summary.histogram('weight2', weight2)\n                tf.summary.histogram('bias2', bias2)\n\n            def model(data):\n                shape = data.get_shape().as_list()\n                reshape = tf.reshape(data, [shape[0], shape[1] * shape[2] * shape[3]])\n                with tf.name_scope('fc1_model'):\n                    hidden  = tf.nn.relu(tf.matmul(reshape, weight1) + bias1)   \n                with tf.name_scope('fc2_model'):\n                    return tf.matmul(hidden, weight2) + bias2\n\n            logits = model(self.tf_train_samples)\n            with tf.name_scope('loss'):\n                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = self.tf_train_labels))\n                tf.summary.scalar('loss', self.loss)\n            with tf.name_scope('optimizer'):\n                self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss)\n            with tf.name_scope('predictions'):\n                self.train_prediction = tf.nn.softmax(logits)\n                self.test_prediction  = tf.nn.softmax(model(self.tf_test_samples))\n            self.merged = tf.summary.merge_all()\n\n    def run(self):\n        ### train\n        def print_confusion_matrix(confusionMatrix):\n            print('Consusion Matrix: ')\n            for i, line in enumerate(confusionMatrix):\n                print line, line[i]/np.sum(line)\n            \n            a = 0\n            for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\n                a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\n                print column[i] / np.sum(column), \n            print '\\n', np.sum(confusionMatrix, a)\n\n        with self.sess as sess:\n            tf.global_variables_initializer().run()\n            print 'Start Training'\n\n            train_samples, train_labels = load.loadmat_data('train_32x32.mat')\n            train_samples, train_labels = load.reformat(train_samples, train_labels)\n            train_samples = load.normalize(train_samples)\n\n            test_samples, test_labels = load.loadmat_data('test_32x32.mat')\n            test_samples, test_labels = load.reformat(test_samples, test_labels)\n            test_samples = load.normalize(test_samples)\n\n            for i, samples, labels in get_chunk(train_samples, train_labels, chunkSize = self.batch_size):\n                _, l, predictions, summary = sess.run([self.optimizer, self.loss, self.train_prediction, self.merged], feed_dict = {self.tf_train_samples: samples, self.tf_train_labels: labels})\n                self.writer.add_summary(summary, i)\n                accuracy, _ = self.accuracy(predictions, labels)\n                if i % 50 == 0:\n                    print 'Minibatch loss at step %d: loss is %f' % (i, l)\n                    print 'Minibatch accuracy: %.1f' % accuracy\n\n            accuracies = []\n            confusionMatrices = []\n            for i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.batch_size):\n                result = sess.run(self.test_prediction, feed_dict={self.tf_test_samples: samples})\n\t\t\t\t# result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\n                accuracies.append(accuracy)\n                confusionMatrices.append(cm)\n                print('Test Accuracy: %.1f%%' % accuracy)\n            print(' Average  Accuracy:', np.average(accuracies))\n            print('Standard Deviation:', np.std(accuracies))\n            print_confusion_matrix(np.add.reduce(confusionMatrices))\n\n    def accuracy(self, predictions, labels, need_confusion_matrix = False):\n        _predictions = np.argmax(predictions, 1)\n        _labels      = np.argmax(labels, 1)\n        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n        accuracy = (100 * np.sum(_predictions == _labels) / predictions.shape[0])\n        return accuracy, cm\n\nif __name__ == '__main__':\n    net = NetWork(num_hidden = 128, batch_size = 100, num_labels = 10, image_size = 32, channel = 1)\n    net.run()\nWhen I use 'result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})', it works fine, while in 'result = sess.run(self.test_prediction, feed_dict={self.tf_test_samples: samples})', it shows error 'ValueError: operands could not be broadcast together with shapes (10,10) (9,9) '. SO, what's difference between function eval() and sess.run()", "body": "Codes are as follows:\r\n```python\r\n# encoding: utf-8\r\nimport load\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom sklearn.metrics import confusion_matrix\r\n\r\ndef get_chunk(samples, labels, chunkSize):\r\n    if len(samples) != len(labels):\r\n        raise Exception('Length of samples and labels must equal')\r\n    stepStart = 0\r\n    i = 0\r\n    while stepStart < len(samples):\r\n        stepEnd = stepStart + chunkSize\r\n        if stepEnd < len(samples):\r\n            yield i, samples[stepStart: stepEnd], labels[stepStart: stepEnd]\r\n            i += 1\r\n        stepStart = stepEnd\r\n\r\nclass NetWork():\r\n    def __init__(self, num_hidden, batch_size, num_labels, image_size, channel):\r\n        self.num_hidden = num_hidden\r\n        self.batch_size = batch_size\r\n        self.num_labels = num_labels\r\n\r\n        self.image_size = image_size\r\n        self.num_channel= channel\r\n\r\n        self.tf_train_samples = None\r\n        self.tf_train_labels  = None\r\n        self.tf_test_samples  = None\r\n\r\n        self.graph = tf.Graph()      \r\n        self.define_graph()\r\n        self.sess  = tf.Session(graph = self.graph)\r\n        self.writer= tf.summary.FileWriter('./board', self.graph)\r\n    \r\n    def define_graph(self):\r\n        with self.graph.as_default():\r\n            with tf.name_scope('inputs'):\r\n                self.tf_train_samples = tf.placeholder(tf.float32, shape = (self.batch_size, self.image_size, self.image_size, self.num_channel), name = 'train_samples')\r\n                self.tf_train_labels   = tf.placeholder(tf.float32, shape = (self.batch_size, self.num_labels), name = 'train_labels')\r\n                self.tf_test_samples  = tf.placeholder(tf.float32, shape = (self.batch_size, self.image_size, self.image_size, self.num_channel), name = 'test_labels')\r\n            with tf.name_scope('fc1'):\r\n                weight1 = tf.Variable(tf.truncated_normal([self.image_size * self.image_size, self.num_hidden], stddev = 0.1), name = 'weight1')\r\n                bias1   = tf.Variable(tf.truncated_normal([self.num_hidden], stddev = 0.1), name = 'bias1')\r\n                tf.summary.histogram('weight1', weight1)\r\n                tf.summary.histogram('bias1', bias1)\r\n            with tf.name_scope('fc2'):\r\n                weight2 = tf.Variable(tf.truncated_normal([self.num_hidden, self.num_labels], stddev = 0.1), name = 'weight2')\r\n                bias2   = tf.Variable(tf.truncated_normal([self.num_labels], stddev = 0.1), name = 'bias2')\r\n                tf.summary.histogram('weight2', weight2)\r\n                tf.summary.histogram('bias2', bias2)\r\n\r\n            def model(data):\r\n                shape = data.get_shape().as_list()\r\n                reshape = tf.reshape(data, [shape[0], shape[1] * shape[2] * shape[3]])\r\n                with tf.name_scope('fc1_model'):\r\n                    hidden  = tf.nn.relu(tf.matmul(reshape, weight1) + bias1)   \r\n                with tf.name_scope('fc2_model'):\r\n                    return tf.matmul(hidden, weight2) + bias2\r\n\r\n            logits = model(self.tf_train_samples)\r\n            with tf.name_scope('loss'):\r\n                self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = self.tf_train_labels))\r\n                tf.summary.scalar('loss', self.loss)\r\n            with tf.name_scope('optimizer'):\r\n                self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss)\r\n            with tf.name_scope('predictions'):\r\n                self.train_prediction = tf.nn.softmax(logits)\r\n                self.test_prediction  = tf.nn.softmax(model(self.tf_test_samples))\r\n            self.merged = tf.summary.merge_all()\r\n\r\n    def run(self):\r\n        ### train\r\n        def print_confusion_matrix(confusionMatrix):\r\n            print('Consusion Matrix: ')\r\n            for i, line in enumerate(confusionMatrix):\r\n                print line, line[i]/np.sum(line)\r\n            \r\n            a = 0\r\n            for i, column in enumerate(np.transpose(confusionMatrix, (1, 0))):\r\n                a += (column[i] / np.sum(column)) * (np.sum(column) / 26000)\r\n                print column[i] / np.sum(column), \r\n            print '\\n', np.sum(confusionMatrix, a)\r\n\r\n        with self.sess as sess:\r\n            tf.global_variables_initializer().run()\r\n            print 'Start Training'\r\n\r\n            train_samples, train_labels = load.loadmat_data('train_32x32.mat')\r\n            train_samples, train_labels = load.reformat(train_samples, train_labels)\r\n            train_samples = load.normalize(train_samples)\r\n\r\n            test_samples, test_labels = load.loadmat_data('test_32x32.mat')\r\n            test_samples, test_labels = load.reformat(test_samples, test_labels)\r\n            test_samples = load.normalize(test_samples)\r\n\r\n            for i, samples, labels in get_chunk(train_samples, train_labels, chunkSize = self.batch_size):\r\n                _, l, predictions, summary = sess.run([self.optimizer, self.loss, self.train_prediction, self.merged], feed_dict = {self.tf_train_samples: samples, self.tf_train_labels: labels})\r\n                self.writer.add_summary(summary, i)\r\n                accuracy, _ = self.accuracy(predictions, labels)\r\n                if i % 50 == 0:\r\n                    print 'Minibatch loss at step %d: loss is %f' % (i, l)\r\n                    print 'Minibatch accuracy: %.1f' % accuracy\r\n\r\n            accuracies = []\r\n            confusionMatrices = []\r\n            for i, samples, labels in get_chunk(test_samples, test_labels, chunkSize=self.batch_size):\r\n                result = sess.run(self.test_prediction, feed_dict={self.tf_test_samples: samples})\r\n\t\t\t\t# result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})\r\n                accuracy, cm = self.accuracy(result, labels, need_confusion_matrix=True)\r\n                accuracies.append(accuracy)\r\n                confusionMatrices.append(cm)\r\n                print('Test Accuracy: %.1f%%' % accuracy)\r\n            print(' Average  Accuracy:', np.average(accuracies))\r\n            print('Standard Deviation:', np.std(accuracies))\r\n            print_confusion_matrix(np.add.reduce(confusionMatrices))\r\n\r\n    def accuracy(self, predictions, labels, need_confusion_matrix = False):\r\n        _predictions = np.argmax(predictions, 1)\r\n        _labels      = np.argmax(labels, 1)\r\n        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\r\n        accuracy = (100 * np.sum(_predictions == _labels) / predictions.shape[0])\r\n        return accuracy, cm\r\n\r\nif __name__ == '__main__':\r\n    net = NetWork(num_hidden = 128, batch_size = 100, num_labels = 10, image_size = 32, channel = 1)\r\n    net.run()\r\n```\r\n\r\nWhen I use 'result = self.test_prediction.eval(feed_dict={self.tf_test_samples: samples})', it works fine, while in 'result = sess.run(self.test_prediction, feed_dict={self.tf_test_samples: samples})', it shows error 'ValueError: operands could not be broadcast together with shapes (10,10) (9,9) '. SO, what's difference between function eval() and sess.run()"}