{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324894574", "html_url": "https://github.com/tensorflow/tensorflow/issues/12215#issuecomment-324894574", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12215", "id": 324894574, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDg5NDU3NA==", "user": {"login": "githubharald", "id": 15148095, "node_id": "MDQ6VXNlcjE1MTQ4MDk1", "avatar_url": "https://avatars2.githubusercontent.com/u/15148095?v=4", "gravatar_id": "", "url": "https://api.github.com/users/githubharald", "html_url": "https://github.com/githubharald", "followers_url": "https://api.github.com/users/githubharald/followers", "following_url": "https://api.github.com/users/githubharald/following{/other_user}", "gists_url": "https://api.github.com/users/githubharald/gists{/gist_id}", "starred_url": "https://api.github.com/users/githubharald/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/githubharald/subscriptions", "organizations_url": "https://api.github.com/users/githubharald/orgs", "repos_url": "https://api.github.com/users/githubharald/repos", "events_url": "https://api.github.com/users/githubharald/events{/privacy}", "received_events_url": "https://api.github.com/users/githubharald/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-25T11:40:01Z", "updated_at": "2017-08-25T11:40:01Z", "author_association": "NONE", "body_html": "<p>Hi Victor, thanks for your answer.</p>\n<p>The warning always appears on those samples on which the condition (see first post) holds. Even after training.<br>\nIn the test program, the input is a random input, but even when repeating the code for multiple random inputs, the warning always occurs.<br>\nThe only way to toggle the warning is by changing the target labelling ... so it appears to me that it has something to to with this target labelling!?</p>\n<p>I further decreased the tensor sizes of the test program to have a closer look at a simple example - code see below.<br>\nThe input has shape T=5 (timesteps) and C=6 (classes).<br>\nThe target labelling is [0, 0, 2]. The probability for a path p which yields the labelling [0,0,2] is not zero.<br>\nE.g. the path 0-&gt;blank-&gt;0-&gt;2-&gt;blank (with blank=C-1=5) yields the target labelling. And so do some more paths.<br>\nTherefore, the probability of seeing the target labelling given the input should not be zero.</p>\n<p><strong>Output of test program:</strong><br>\ninput:<br>\n[[[ 0.59413256  0.23575521  0.95215654  0.46271613  0.08859916  0.43706737]]</p>\n<p>[[ 0.11860785  0.11229196  0.01522826  0.96020464  0.34278502  0.27091574]]</p>\n<p>[[ 0.29061551  0.27465308  0.94130146  0.12166673  0.86577444  0.37988687]]</p>\n<p>[[ 0.77218394  0.07898683  0.85200237  0.4295197   0.76858367  0.72602145]]</p>\n<p>[[ 0.09071068  0.43796645  0.6265344   0.48837476  0.24057374  0.03459447]]]</p>\n<p>label:  SparseTensorValue(indices=[[0, 0], [0, 1], [0, 2]], values=[0, 0, 2], dense_shape=[1, 3])</p>\n<p>loss:  [inf]</p>\n<p><strong>Test program:</strong><br>\nimport tensorflow as tf<br>\nimport numpy as np</p>\n<pre><code>batchSize=1\nnumClasses=6\nnumTimesteps=5\n\ndef createGraph():\n\ttinputs=tf.placeholder(tf.float32, [numTimesteps, batchSize, numClasses])\n\ttlabels=tf.SparseTensor(tf.placeholder(tf.int64, shape=[None,2]) , tf.placeholder(tf.int32,[None]), tf.placeholder(tf.int64,[2])) # labels\n\ttseqLen=tf.placeholder(tf.int32, [None]) # list of sequence length in batch\n\ttloss=tf.reduce_mean(tf.nn.ctc_loss(labels=tlabels, inputs=tinputs, sequence_length=tseqLen, ctc_merge_repeated=True)) # ctc loss\n\treturn (tinputs, tlabels, tseqLen, tloss)\n\ndef getNextBatch(nc): # next batch with given number of chars in label\n\tindices=[[0,i] for i in range(nc)]\n\tvalues=[i%3 for i in range(nc)]\n\tvalues[0]=0\n\tvalues[1]=0 # TODO: (un)comment this to trigger warning\n\tshape=[1, nc]\n\tlabels=tf.SparseTensorValue(indices, values, shape)\n\tseqLen=[nc]\n\tinputs=np.random.rand(numTimesteps, batchSize, numClasses)\n\treturn (labels, inputs, seqLen) \n\n\n(tinputs, tlabels, tseqLen, tloss)=createGraph()\n\nsess=tf.Session()\nsess.run(tf.global_variables_initializer())\n\nnc=3 # number of chars in label\nprint('next batch with 1 element has label len='+str(nc))\n(labels, inputs, seqLen)=getNextBatch(nc)\nresLoss=sess.run([tloss], { tlabels: labels, tinputs:inputs, tseqLen:seqLen } )\nprint('input: ', inputs)\nprint('label: ', labels)\nprint('loss: ', resLoss)\n</code></pre>", "body_text": "Hi Victor, thanks for your answer.\nThe warning always appears on those samples on which the condition (see first post) holds. Even after training.\nIn the test program, the input is a random input, but even when repeating the code for multiple random inputs, the warning always occurs.\nThe only way to toggle the warning is by changing the target labelling ... so it appears to me that it has something to to with this target labelling!?\nI further decreased the tensor sizes of the test program to have a closer look at a simple example - code see below.\nThe input has shape T=5 (timesteps) and C=6 (classes).\nThe target labelling is [0, 0, 2]. The probability for a path p which yields the labelling [0,0,2] is not zero.\nE.g. the path 0->blank->0->2->blank (with blank=C-1=5) yields the target labelling. And so do some more paths.\nTherefore, the probability of seeing the target labelling given the input should not be zero.\nOutput of test program:\ninput:\n[[[ 0.59413256  0.23575521  0.95215654  0.46271613  0.08859916  0.43706737]]\n[[ 0.11860785  0.11229196  0.01522826  0.96020464  0.34278502  0.27091574]]\n[[ 0.29061551  0.27465308  0.94130146  0.12166673  0.86577444  0.37988687]]\n[[ 0.77218394  0.07898683  0.85200237  0.4295197   0.76858367  0.72602145]]\n[[ 0.09071068  0.43796645  0.6265344   0.48837476  0.24057374  0.03459447]]]\nlabel:  SparseTensorValue(indices=[[0, 0], [0, 1], [0, 2]], values=[0, 0, 2], dense_shape=[1, 3])\nloss:  [inf]\nTest program:\nimport tensorflow as tf\nimport numpy as np\nbatchSize=1\nnumClasses=6\nnumTimesteps=5\n\ndef createGraph():\n\ttinputs=tf.placeholder(tf.float32, [numTimesteps, batchSize, numClasses])\n\ttlabels=tf.SparseTensor(tf.placeholder(tf.int64, shape=[None,2]) , tf.placeholder(tf.int32,[None]), tf.placeholder(tf.int64,[2])) # labels\n\ttseqLen=tf.placeholder(tf.int32, [None]) # list of sequence length in batch\n\ttloss=tf.reduce_mean(tf.nn.ctc_loss(labels=tlabels, inputs=tinputs, sequence_length=tseqLen, ctc_merge_repeated=True)) # ctc loss\n\treturn (tinputs, tlabels, tseqLen, tloss)\n\ndef getNextBatch(nc): # next batch with given number of chars in label\n\tindices=[[0,i] for i in range(nc)]\n\tvalues=[i%3 for i in range(nc)]\n\tvalues[0]=0\n\tvalues[1]=0 # TODO: (un)comment this to trigger warning\n\tshape=[1, nc]\n\tlabels=tf.SparseTensorValue(indices, values, shape)\n\tseqLen=[nc]\n\tinputs=np.random.rand(numTimesteps, batchSize, numClasses)\n\treturn (labels, inputs, seqLen) \n\n\n(tinputs, tlabels, tseqLen, tloss)=createGraph()\n\nsess=tf.Session()\nsess.run(tf.global_variables_initializer())\n\nnc=3 # number of chars in label\nprint('next batch with 1 element has label len='+str(nc))\n(labels, inputs, seqLen)=getNextBatch(nc)\nresLoss=sess.run([tloss], { tlabels: labels, tinputs:inputs, tseqLen:seqLen } )\nprint('input: ', inputs)\nprint('label: ', labels)\nprint('loss: ', resLoss)", "body": "Hi Victor, thanks for your answer.\r\n\r\nThe warning always appears on those samples on which the condition (see first post) holds. Even after training.\r\nIn the test program, the input is a random input, but even when repeating the code for multiple random inputs, the warning always occurs.\r\nThe only way to toggle the warning is by changing the target labelling ... so it appears to me that it has something to to with this target labelling!?\r\n\r\nI further decreased the tensor sizes of the test program to have a closer look at a simple example - code see below.\r\nThe input has shape T=5 (timesteps) and C=6 (classes).\r\nThe target labelling is [0, 0, 2]. The probability for a path p which yields the labelling [0,0,2] is not zero.\r\nE.g. the path 0->blank->0->2->blank (with blank=C-1=5) yields the target labelling. And so do some more paths. \r\nTherefore, the probability of seeing the target labelling given the input should not be zero.\r\n\r\n**Output of test program:**\r\ninput:  \r\n[[[ 0.59413256  0.23575521  0.95215654  0.46271613  0.08859916  0.43706737]]\r\n\r\n [[ 0.11860785  0.11229196  0.01522826  0.96020464  0.34278502  0.27091574]]\r\n\r\n [[ 0.29061551  0.27465308  0.94130146  0.12166673  0.86577444  0.37988687]]\r\n\r\n [[ 0.77218394  0.07898683  0.85200237  0.4295197   0.76858367  0.72602145]]\r\n\r\n [[ 0.09071068  0.43796645  0.6265344   0.48837476  0.24057374  0.03459447]]]\r\n \r\nlabel:  SparseTensorValue(indices=[[0, 0], [0, 1], [0, 2]], values=[0, 0, 2], dense_shape=[1, 3])\r\n\r\nloss:  [inf]\r\n\r\n\r\n**Test program:**\r\n\timport tensorflow as tf\r\n\timport numpy as np\r\n\r\n\tbatchSize=1\r\n\tnumClasses=6\r\n\tnumTimesteps=5\r\n\r\n\tdef createGraph():\r\n\t\ttinputs=tf.placeholder(tf.float32, [numTimesteps, batchSize, numClasses])\r\n\t\ttlabels=tf.SparseTensor(tf.placeholder(tf.int64, shape=[None,2]) , tf.placeholder(tf.int32,[None]), tf.placeholder(tf.int64,[2])) # labels\r\n\t\ttseqLen=tf.placeholder(tf.int32, [None]) # list of sequence length in batch\r\n\t\ttloss=tf.reduce_mean(tf.nn.ctc_loss(labels=tlabels, inputs=tinputs, sequence_length=tseqLen, ctc_merge_repeated=True)) # ctc loss\r\n\t\treturn (tinputs, tlabels, tseqLen, tloss)\r\n\r\n\tdef getNextBatch(nc): # next batch with given number of chars in label\r\n\t\tindices=[[0,i] for i in range(nc)]\r\n\t\tvalues=[i%3 for i in range(nc)]\r\n\t\tvalues[0]=0\r\n\t\tvalues[1]=0 # TODO: (un)comment this to trigger warning\r\n\t\tshape=[1, nc]\r\n\t\tlabels=tf.SparseTensorValue(indices, values, shape)\r\n\t\tseqLen=[nc]\r\n\t\tinputs=np.random.rand(numTimesteps, batchSize, numClasses)\r\n\t\treturn (labels, inputs, seqLen) \r\n\r\n\r\n\t(tinputs, tlabels, tseqLen, tloss)=createGraph()\r\n\r\n\tsess=tf.Session()\r\n\tsess.run(tf.global_variables_initializer())\r\n\r\n\tnc=3 # number of chars in label\r\n\tprint('next batch with 1 element has label len='+str(nc))\r\n\t(labels, inputs, seqLen)=getNextBatch(nc)\r\n\tresLoss=sess.run([tloss], { tlabels: labels, tinputs:inputs, tseqLen:seqLen } )\r\n\tprint('input: ', inputs)\r\n\tprint('label: ', labels)\r\n\tprint('loss: ', resLoss)"}