{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323097856", "html_url": "https://github.com/tensorflow/tensorflow/issues/12215#issuecomment-323097856", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12215", "id": 323097856, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzA5Nzg1Ng==", "user": {"login": "vcarbune", "id": 653877, "node_id": "MDQ6VXNlcjY1Mzg3Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/653877?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vcarbune", "html_url": "https://github.com/vcarbune", "followers_url": "https://api.github.com/users/vcarbune/followers", "following_url": "https://api.github.com/users/vcarbune/following{/other_user}", "gists_url": "https://api.github.com/users/vcarbune/gists{/gist_id}", "starred_url": "https://api.github.com/users/vcarbune/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vcarbune/subscriptions", "organizations_url": "https://api.github.com/users/vcarbune/orgs", "repos_url": "https://api.github.com/users/vcarbune/repos", "events_url": "https://api.github.com/users/vcarbune/events{/privacy}", "received_events_url": "https://api.github.com/users/vcarbune/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-17T14:55:32Z", "updated_at": "2017-08-17T14:55:32Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>The warning message you're seeing, as mentioned in the comment in the code, can appear also in cases when ctc_merge_repeated=False.</p>\n<p>It appears whenever it's impossible to determine a valid path for the target labeling based on the network activation through the forward/backward variables (which is where the ctc_merge_repeated is applied - note: the code has some references to the Alex Graves' thesis, which I find useful going through).</p>\n<p>Q1: why is there a warning when repeated chars occur? I thought, as long as the input sequence is not shorter than the target labelling, there is no problem. And when repeated chars are merged in the label, then it gets even shorter, therefore the condition that the input sequence is not shorter still holds.</p>\n<p>Again, this warning changes as training progress (assuming not all the samples in your training set raise this problem), since the network activations change. The warning states that there couldn't be a valid path found based on the forward/backward computation, but not that that the sequence is invalid.</p>\n<p>Note that ctc_merge_repeated is different from collapsing repeated labels, which is done through preprocess_collapse_repeated. The former controls how the CTC loss penalizes internally repeated labels ('a' 'a' 'b' becomes 'blank' 'a' 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation), while the latter actually modifies the ground truth. ('a' 'a' 'b' first becomes 'a' 'b' and then 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation).</p>\n<p>Q2: why does the ctc_loss in its default settings produce this warning? Repeated chars are common in the domains CTCs are used such as handwritten text recognition (HTR)</p>\n<p>This should be the default CTC behavior, where collapsing labels for the loss/gradient computation allows focusing on learning the label transitions instead of where the label appears.</p>\n<p>Q3: what settings should I use when doing HTR? Of course labels can have repeated chars. Therefore ctc_merge_repeated=False would make sense. Any suggestions?</p>\n<p>Having ctc_merge_repeated=True does allow learning repeated characters. Please see the detailed comments in the documentation, specifically about the behavior of the different combination of flags \"Regarding the arguments...\"</p>\n<p>(<a href=\"https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/connectionist_temporal_classification__ctc\" rel=\"nofollow\">https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/connectionist_temporal_classification__ctc</a>_)</p>", "body_text": "Hi,\nThe warning message you're seeing, as mentioned in the comment in the code, can appear also in cases when ctc_merge_repeated=False.\nIt appears whenever it's impossible to determine a valid path for the target labeling based on the network activation through the forward/backward variables (which is where the ctc_merge_repeated is applied - note: the code has some references to the Alex Graves' thesis, which I find useful going through).\nQ1: why is there a warning when repeated chars occur? I thought, as long as the input sequence is not shorter than the target labelling, there is no problem. And when repeated chars are merged in the label, then it gets even shorter, therefore the condition that the input sequence is not shorter still holds.\nAgain, this warning changes as training progress (assuming not all the samples in your training set raise this problem), since the network activations change. The warning states that there couldn't be a valid path found based on the forward/backward computation, but not that that the sequence is invalid.\nNote that ctc_merge_repeated is different from collapsing repeated labels, which is done through preprocess_collapse_repeated. The former controls how the CTC loss penalizes internally repeated labels ('a' 'a' 'b' becomes 'blank' 'a' 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation), while the latter actually modifies the ground truth. ('a' 'a' 'b' first becomes 'a' 'b' and then 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation).\nQ2: why does the ctc_loss in its default settings produce this warning? Repeated chars are common in the domains CTCs are used such as handwritten text recognition (HTR)\nThis should be the default CTC behavior, where collapsing labels for the loss/gradient computation allows focusing on learning the label transitions instead of where the label appears.\nQ3: what settings should I use when doing HTR? Of course labels can have repeated chars. Therefore ctc_merge_repeated=False would make sense. Any suggestions?\nHaving ctc_merge_repeated=True does allow learning repeated characters. Please see the detailed comments in the documentation, specifically about the behavior of the different combination of flags \"Regarding the arguments...\"\n(https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/connectionist_temporal_classification__ctc_)", "body": "Hi,\r\n\r\nThe warning message you're seeing, as mentioned in the comment in the code, can appear also in cases when ctc_merge_repeated=False.\r\n\r\nIt appears whenever it's impossible to determine a valid path for the target labeling based on the network activation through the forward/backward variables (which is where the ctc_merge_repeated is applied - note: the code has some references to the Alex Graves' thesis, which I find useful going through).\r\n\r\nQ1: why is there a warning when repeated chars occur? I thought, as long as the input sequence is not shorter than the target labelling, there is no problem. And when repeated chars are merged in the label, then it gets even shorter, therefore the condition that the input sequence is not shorter still holds.\r\n\r\nAgain, this warning changes as training progress (assuming not all the samples in your training set raise this problem), since the network activations change. The warning states that there couldn't be a valid path found based on the forward/backward computation, but not that that the sequence is invalid.\r\n\r\nNote that ctc_merge_repeated is different from collapsing repeated labels, which is done through preprocess_collapse_repeated. The former controls how the CTC loss penalizes internally repeated labels ('a' 'a' 'b' becomes 'blank' 'a' 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation), while the latter actually modifies the ground truth. ('a' 'a' 'b' first becomes 'a' 'b' and then 'blank' 'a' 'blank' 'b' 'blank' for the forward/backward computation).\r\n\r\nQ2: why does the ctc_loss in its default settings produce this warning? Repeated chars are common in the domains CTCs are used such as handwritten text recognition (HTR)\r\n\r\nThis should be the default CTC behavior, where collapsing labels for the loss/gradient computation allows focusing on learning the label transitions instead of where the label appears.\r\n\r\nQ3: what settings should I use when doing HTR? Of course labels can have repeated chars. Therefore ctc_merge_repeated=False would make sense. Any suggestions?\r\n\r\nHaving ctc_merge_repeated=True does allow learning repeated characters. Please see the detailed comments in the documentation, specifically about the behavior of the different combination of flags \"Regarding the arguments...\" \r\n\r\n(https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/connectionist_temporal_classification__ctc_)"}