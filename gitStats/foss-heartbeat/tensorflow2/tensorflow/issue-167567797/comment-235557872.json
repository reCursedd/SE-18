{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235557872", "html_url": "https://github.com/tensorflow/tensorflow/issues/3504#issuecomment-235557872", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3504", "id": 235557872, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTU1Nzg3Mg==", "user": {"login": "manjunaths", "id": 216373, "node_id": "MDQ6VXNlcjIxNjM3Mw==", "avatar_url": "https://avatars3.githubusercontent.com/u/216373?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manjunaths", "html_url": "https://github.com/manjunaths", "followers_url": "https://api.github.com/users/manjunaths/followers", "following_url": "https://api.github.com/users/manjunaths/following{/other_user}", "gists_url": "https://api.github.com/users/manjunaths/gists{/gist_id}", "starred_url": "https://api.github.com/users/manjunaths/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manjunaths/subscriptions", "organizations_url": "https://api.github.com/users/manjunaths/orgs", "repos_url": "https://api.github.com/users/manjunaths/repos", "events_url": "https://api.github.com/users/manjunaths/events{/privacy}", "received_events_url": "https://api.github.com/users/manjunaths/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-27T11:22:43Z", "updated_at": "2016-07-27T11:22:43Z", "author_association": "NONE", "body_html": "<p>Attached the output of the run and the nvidia-smi that you requested from inside the nvidia-docker container. Note that in the first case, with batch size 32, it is actually running the network successfully (I even verified, using nvidia-smi, that the targetted GPU is being utilized.)</p>\n<p>One funny observation is that the only time I see the bfc_allocator being called is when the run failed with out of memory, the log of which I have attached to my first post that is.</p>\n<p>All the successful runs are with pool_allocator, I don't know if this information is useful, but just an observation.</p>\n<p>Between the runs except batch size and number of gpus, which is specified via the command-line to the python script, I am changing nothing else. I am using the stock scripts.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/385891/slurm-3424.txt\">slurm-3424.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/385892/nvidia-smi-sl270d-m40.txt\">nvidia-smi-sl270d-m40.txt</a></p>", "body_text": "Attached the output of the run and the nvidia-smi that you requested from inside the nvidia-docker container. Note that in the first case, with batch size 32, it is actually running the network successfully (I even verified, using nvidia-smi, that the targetted GPU is being utilized.)\nOne funny observation is that the only time I see the bfc_allocator being called is when the run failed with out of memory, the log of which I have attached to my first post that is.\nAll the successful runs are with pool_allocator, I don't know if this information is useful, but just an observation.\nBetween the runs except batch size and number of gpus, which is specified via the command-line to the python script, I am changing nothing else. I am using the stock scripts.\nslurm-3424.txt\nnvidia-smi-sl270d-m40.txt", "body": "Attached the output of the run and the nvidia-smi that you requested from inside the nvidia-docker container. Note that in the first case, with batch size 32, it is actually running the network successfully (I even verified, using nvidia-smi, that the targetted GPU is being utilized.)\n\nOne funny observation is that the only time I see the bfc_allocator being called is when the run failed with out of memory, the log of which I have attached to my first post that is.\n\nAll the successful runs are with pool_allocator, I don't know if this information is useful, but just an observation. \n\nBetween the runs except batch size and number of gpus, which is specified via the command-line to the python script, I am changing nothing else. I am using the stock scripts.\n\n[slurm-3424.txt](https://github.com/tensorflow/tensorflow/files/385891/slurm-3424.txt)\n[nvidia-smi-sl270d-m40.txt](https://github.com/tensorflow/tensorflow/files/385892/nvidia-smi-sl270d-m40.txt)\n"}