{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3504", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3504/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3504/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3504/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3504", "id": 167567797, "node_id": "MDU6SXNzdWUxNjc1Njc3OTc=", "number": 3504, "title": "Inception v3 model crashes with out of memory with batch size of 128 on a GPU with 12 GB memory..", "user": {"login": "manjunaths", "id": 216373, "node_id": "MDQ6VXNlcjIxNjM3Mw==", "avatar_url": "https://avatars3.githubusercontent.com/u/216373?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manjunaths", "html_url": "https://github.com/manjunaths", "followers_url": "https://api.github.com/users/manjunaths/followers", "following_url": "https://api.github.com/users/manjunaths/following{/other_user}", "gists_url": "https://api.github.com/users/manjunaths/gists{/gist_id}", "starred_url": "https://api.github.com/users/manjunaths/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manjunaths/subscriptions", "organizations_url": "https://api.github.com/users/manjunaths/orgs", "repos_url": "https://api.github.com/users/manjunaths/repos", "events_url": "https://api.github.com/users/manjunaths/events{/privacy}", "received_events_url": "https://api.github.com/users/manjunaths/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2016-07-26T09:54:46Z", "updated_at": "2017-01-23T23:11:57Z", "closed_at": "2017-01-23T23:11:57Z", "author_association": "NONE", "body_html": "<p>When I run the inception v3 network, with a <em>batch size of 32</em> , from <a href=\"https://github.com/tensorflow/models/tree/master/inception\">here</a> on an nVidia GPU with 12 GB memory, I get the below messages during initialization about raising pool_size_limit. The initialization takes a long time. But it eventually manages to run. Is there a way to speed this up ? The pool limit starts from 100 and keeps increasing until it reaches 2997, is there a way to increase the pool limit ? Perhaps, some sort of an environment variable ?</p>\n<p>If I try running with batch_size of 128, it crashes with out of memory. The log of which is attached to this report. Is there a way to fix this ?<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/383534/tensorflow-inception-v3-bs128.txt\">tensorflow-inception-v3-bs128.txt</a></p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nStock gpu-dev Docker image on CentOS 7</p>\n<p>Installed version of CUDA and cuDNN:<br>\nroot@d4239a28fc92:~# ls /usr/local/cuda-7.5/lib64/libcud*<br>\n/usr/local/cuda-7.5/lib64/libcuda.so         /usr/local/cuda-7.5/lib64/libcudart.so<br>\n/usr/local/cuda-7.5/lib64/libcuda.so.1       /usr/local/cuda-7.5/lib64/libcudart.so.7.5<br>\n/usr/local/cuda-7.5/lib64/libcuda.so.352.93  /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18<br>\n/usr/local/cuda-7.5/lib64/libcudadevrt.a     /usr/local/cuda-7.5/lib64/libcudart_static.a</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed.<br>\ntensorflow-0.8.0-cp27-none-linux_x86_64.whl</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.<br>\n0.8.0</li>\n</ol>\n<h3>Steps to reproduce</h3>\n<p>bazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --max_steps=100</p>\n<h3>What have you tried?</h3>\n<ol>\n<li>Running with a smaller batch size of 32 instead of 128 to get the model to run successfully.</li>\n<li>Pass the command -m 240000000000 to nvidia-docker.</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment).<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=2169 evicted_count=1000 eviction_rate=0.461042 and unsatisfied allocation rate=0.722842<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=4230 evicted_count=3000 eviction_rate=0.70922 and unsatisfied allocation rate=0.68447<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 146 to 160<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2019 evicted_count=2000 eviction_rate=0.990589 and unsatisfied allocation rate=0<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2028 evicted_count=2000 eviction_rate=0.986193 and unsatisfied allocation rate=0<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3560 evicted_count=2000 eviction_rate=0.561798 and unsatisfied allocation rate=0.605911<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 449 to 493<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1065 evicted_count=1000 eviction_rate=0.938967 and unsatisfied allocation rate=0<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3333 evicted_count=1000 eviction_rate=0.30003 and unsatisfied allocation rate=0.422349<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 1158 to 1273<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 65569 get requests, put_count=65760 evicted_count=1000 eviction_rate=0.0152068 and unsatisfied allocation rate=0.0161052<br>\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 2725 to 2997<br>\n2016-07-26 13:27:52.039502: step 0, loss = 13.02 (2.4 examples/sec; 13.434 sec/batch)<br>\n2016-07-26 13:28:21.119108: step 10, loss = 14.07 (25.9 examples/sec; 1.234 sec/batch)<br>\n2016-07-26 13:28:33.393687: step 20, loss = 14.80 (26.0 examples/sec; 1.229 sec/batch)<br>\n2016-07-26 13:28:45.642208: step 30, loss = 14.68 (26.3 examples/sec; 1.217 sec/batch)<br>\n2016-07-26 13:28:57.914613: step 40, loss = 13.67 (26.0 examples/sec; 1.229 sec/batch)<br>\n2016-07-26 13:29:10.188971: step 50, loss = 13.41 (26.1 examples/sec; 1.226 sec/batch)<br>\n2016-07-26 13:29:22.507990: step 60, loss = 13.20 (25.9 examples/sec; 1.235 sec/batch)<br>\n2016-07-26 13:29:34.767687: step 70, loss = 13.15 (26.2 examples/sec; 1.223 sec/batch)<br>\n2016-07-26 13:29:47.051145: step 80, loss = 13.08 (26.0 examples/sec; 1.229 sec/batch)<br>\n2016-07-26 13:29:59.302955: step 90, loss = 13.09 (26.1 examples/sec; 1.228 sec/batch)</p>", "body_text": "When I run the inception v3 network, with a batch size of 32 , from here on an nVidia GPU with 12 GB memory, I get the below messages during initialization about raising pool_size_limit. The initialization takes a long time. But it eventually manages to run. Is there a way to speed this up ? The pool limit starts from 100 and keeps increasing until it reaches 2997, is there a way to increase the pool limit ? Perhaps, some sort of an environment variable ?\nIf I try running with batch_size of 128, it crashes with out of memory. The log of which is attached to this report. Is there a way to fix this ?\ntensorflow-inception-v3-bs128.txt\nEnvironment info\nOperating System:\nStock gpu-dev Docker image on CentOS 7\nInstalled version of CUDA and cuDNN:\nroot@d4239a28fc92:~# ls /usr/local/cuda-7.5/lib64/libcud*\n/usr/local/cuda-7.5/lib64/libcuda.so         /usr/local/cuda-7.5/lib64/libcudart.so\n/usr/local/cuda-7.5/lib64/libcuda.so.1       /usr/local/cuda-7.5/lib64/libcudart.so.7.5\n/usr/local/cuda-7.5/lib64/libcuda.so.352.93  /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n/usr/local/cuda-7.5/lib64/libcudadevrt.a     /usr/local/cuda-7.5/lib64/libcudart_static.a\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\ntensorflow-0.8.0-cp27-none-linux_x86_64.whl\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n0.8.0\n\nSteps to reproduce\nbazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --max_steps=100\nWhat have you tried?\n\nRunning with a smaller batch size of 32 instead of 128 to get the model to run successfully.\nPass the command -m 240000000000 to nvidia-docker.\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=2169 evicted_count=1000 eviction_rate=0.461042 and unsatisfied allocation rate=0.722842\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=4230 evicted_count=3000 eviction_rate=0.70922 and unsatisfied allocation rate=0.68447\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 146 to 160\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2019 evicted_count=2000 eviction_rate=0.990589 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2028 evicted_count=2000 eviction_rate=0.986193 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3560 evicted_count=2000 eviction_rate=0.561798 and unsatisfied allocation rate=0.605911\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 449 to 493\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1065 evicted_count=1000 eviction_rate=0.938967 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3333 evicted_count=1000 eviction_rate=0.30003 and unsatisfied allocation rate=0.422349\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 1158 to 1273\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 65569 get requests, put_count=65760 evicted_count=1000 eviction_rate=0.0152068 and unsatisfied allocation rate=0.0161052\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 2725 to 2997\n2016-07-26 13:27:52.039502: step 0, loss = 13.02 (2.4 examples/sec; 13.434 sec/batch)\n2016-07-26 13:28:21.119108: step 10, loss = 14.07 (25.9 examples/sec; 1.234 sec/batch)\n2016-07-26 13:28:33.393687: step 20, loss = 14.80 (26.0 examples/sec; 1.229 sec/batch)\n2016-07-26 13:28:45.642208: step 30, loss = 14.68 (26.3 examples/sec; 1.217 sec/batch)\n2016-07-26 13:28:57.914613: step 40, loss = 13.67 (26.0 examples/sec; 1.229 sec/batch)\n2016-07-26 13:29:10.188971: step 50, loss = 13.41 (26.1 examples/sec; 1.226 sec/batch)\n2016-07-26 13:29:22.507990: step 60, loss = 13.20 (25.9 examples/sec; 1.235 sec/batch)\n2016-07-26 13:29:34.767687: step 70, loss = 13.15 (26.2 examples/sec; 1.223 sec/batch)\n2016-07-26 13:29:47.051145: step 80, loss = 13.08 (26.0 examples/sec; 1.229 sec/batch)\n2016-07-26 13:29:59.302955: step 90, loss = 13.09 (26.1 examples/sec; 1.228 sec/batch)", "body": "When I run the inception v3 network, with a _batch size of 32_ , from [here](https://github.com/tensorflow/models/tree/master/inception) on an nVidia GPU with 12 GB memory, I get the below messages during initialization about raising pool_size_limit. The initialization takes a long time. But it eventually manages to run. Is there a way to speed this up ? The pool limit starts from 100 and keeps increasing until it reaches 2997, is there a way to increase the pool limit ? Perhaps, some sort of an environment variable ?\n\nIf I try running with batch_size of 128, it crashes with out of memory. The log of which is attached to this report. Is there a way to fix this ?\n[tensorflow-inception-v3-bs128.txt](https://github.com/tensorflow/tensorflow/files/383534/tensorflow-inception-v3-bs128.txt)\n### Environment info\n\nOperating System:\nStock gpu-dev Docker image on CentOS 7\n\nInstalled version of CUDA and cuDNN: \nroot@d4239a28fc92:~# ls /usr/local/cuda-7.5/lib64/libcud*\n/usr/local/cuda-7.5/lib64/libcuda.so         /usr/local/cuda-7.5/lib64/libcudart.so\n/usr/local/cuda-7.5/lib64/libcuda.so.1       /usr/local/cuda-7.5/lib64/libcudart.so.7.5\n/usr/local/cuda-7.5/lib64/libcuda.so.352.93  /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n/usr/local/cuda-7.5/lib64/libcudadevrt.a     /usr/local/cuda-7.5/lib64/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.8.0\n### Steps to reproduce\n\nbazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --max_steps=100\n### What have you tried?\n1. Running with a smaller batch size of 32 instead of 128 to get the model to run successfully.\n2. Pass the command -m 240000000000 to nvidia-docker.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=2169 evicted_count=1000 eviction_rate=0.461042 and unsatisfied allocation rate=0.722842\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=4230 evicted_count=3000 eviction_rate=0.70922 and unsatisfied allocation rate=0.68447\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 146 to 160\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2019 evicted_count=2000 eviction_rate=0.990589 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2028 evicted_count=2000 eviction_rate=0.986193 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3560 evicted_count=2000 eviction_rate=0.561798 and unsatisfied allocation rate=0.605911\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 449 to 493\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1065 evicted_count=1000 eviction_rate=0.938967 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3333 evicted_count=1000 eviction_rate=0.30003 and unsatisfied allocation rate=0.422349\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 1158 to 1273\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 65569 get requests, put_count=65760 evicted_count=1000 eviction_rate=0.0152068 and unsatisfied allocation rate=0.0161052\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 2725 to 2997\n2016-07-26 13:27:52.039502: step 0, loss = 13.02 (2.4 examples/sec; 13.434 sec/batch)\n2016-07-26 13:28:21.119108: step 10, loss = 14.07 (25.9 examples/sec; 1.234 sec/batch)\n2016-07-26 13:28:33.393687: step 20, loss = 14.80 (26.0 examples/sec; 1.229 sec/batch)\n2016-07-26 13:28:45.642208: step 30, loss = 14.68 (26.3 examples/sec; 1.217 sec/batch)\n2016-07-26 13:28:57.914613: step 40, loss = 13.67 (26.0 examples/sec; 1.229 sec/batch)\n2016-07-26 13:29:10.188971: step 50, loss = 13.41 (26.1 examples/sec; 1.226 sec/batch)\n2016-07-26 13:29:22.507990: step 60, loss = 13.20 (25.9 examples/sec; 1.235 sec/batch)\n2016-07-26 13:29:34.767687: step 70, loss = 13.15 (26.2 examples/sec; 1.223 sec/batch)\n2016-07-26 13:29:47.051145: step 80, loss = 13.08 (26.0 examples/sec; 1.229 sec/batch)\n2016-07-26 13:29:59.302955: step 90, loss = 13.09 (26.1 examples/sec; 1.228 sec/batch)\n"}