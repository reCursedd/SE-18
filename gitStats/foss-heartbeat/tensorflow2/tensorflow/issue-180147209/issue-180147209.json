{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4661", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4661/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4661/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4661/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4661", "id": 180147209, "node_id": "MDU6SXNzdWUxODAxNDcyMDk=", "number": 4661, "title": "Defun grad_func should accept the outputs of the function being differentiated", "user": {"login": "rizar", "id": 654434, "node_id": "MDQ6VXNlcjY1NDQzNA==", "avatar_url": "https://avatars0.githubusercontent.com/u/654434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rizar", "html_url": "https://github.com/rizar", "followers_url": "https://api.github.com/users/rizar/followers", "following_url": "https://api.github.com/users/rizar/following{/other_user}", "gists_url": "https://api.github.com/users/rizar/gists{/gist_id}", "starred_url": "https://api.github.com/users/rizar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rizar/subscriptions", "organizations_url": "https://api.github.com/users/rizar/orgs", "repos_url": "https://api.github.com/users/rizar/repos", "events_url": "https://api.github.com/users/rizar/events{/privacy}", "received_events_url": "https://api.github.com/users/rizar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zffchen78", "id": 7943790, "node_id": "MDQ6VXNlcjc5NDM3OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7943790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zffchen78", "html_url": "https://github.com/zffchen78", "followers_url": "https://api.github.com/users/zffchen78/followers", "following_url": "https://api.github.com/users/zffchen78/following{/other_user}", "gists_url": "https://api.github.com/users/zffchen78/gists{/gist_id}", "starred_url": "https://api.github.com/users/zffchen78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zffchen78/subscriptions", "organizations_url": "https://api.github.com/users/zffchen78/orgs", "repos_url": "https://api.github.com/users/zffchen78/repos", "events_url": "https://api.github.com/users/zffchen78/events{/privacy}", "received_events_url": "https://api.github.com/users/zffchen78/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zffchen78", "id": 7943790, "node_id": "MDQ6VXNlcjc5NDM3OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7943790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zffchen78", "html_url": "https://github.com/zffchen78", "followers_url": "https://api.github.com/users/zffchen78/followers", "following_url": "https://api.github.com/users/zffchen78/following{/other_user}", "gists_url": "https://api.github.com/users/zffchen78/gists{/gist_id}", "starred_url": "https://api.github.com/users/zffchen78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zffchen78/subscriptions", "organizations_url": "https://api.github.com/users/zffchen78/orgs", "repos_url": "https://api.github.com/users/zffchen78/repos", "events_url": "https://api.github.com/users/zffchen78/events{/privacy}", "received_events_url": "https://api.github.com/users/zffchen78/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-09-29T19:59:43Z", "updated_at": "2016-10-09T01:01:54Z", "closed_at": "2016-10-09T00:58:30Z", "author_association": "NONE", "body_html": "<p>When dealing with discrete stochastic units, one often wants to define custom gradients. It seems like the most legitimate way to it in Tensorflow is to use <code>grad_func</code> keyword argument of <code>function.Defun</code> from <code>tensorflow.python.framework</code>.  However, the gradient function is only given the inputs of the function defined and the gradients, whereas it would be often very convenient to also use the outputs.</p>\n<p>For example, see the code below.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> In current Tensorflow I have to do like this:</span>\n\n<span class=\"pl-en\">@function.Defun</span>(<span class=\"pl-k\">*</span>(<span class=\"pl-c1\">3</span> <span class=\"pl-k\">*</span> [tf.float32]))\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">oneovertwo_unit_bprop</span>(<span class=\"pl-smi\">probs</span>, <span class=\"pl-smi\">noise</span>, <span class=\"pl-smi\">grads</span>):\n  outcome <span class=\"pl-k\">=</span> tf.to_float(tf.less(noise, probs))\n  outcome_probs <span class=\"pl-k\">=</span> outcome <span class=\"pl-k\">*</span> probs <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> outcome) <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> probs)\n  <span class=\"pl-k\">return</span> grads <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span>. <span class=\"pl-k\">/</span> outcome_probs, tf.zeros_like(noise)\n\n\n<span class=\"pl-en\">@function.Defun</span>(<span class=\"pl-k\">*</span>(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> [tf.float32]), <span class=\"pl-v\">grad_func</span><span class=\"pl-k\">=</span>oneovertwo_unit_bprop)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">oneovertwo_unit_fprop</span>(<span class=\"pl-smi\">probs</span>, <span class=\"pl-smi\">noise</span>):\n  <span class=\"pl-k\">return</span> tf.to_float(tf.less(noise, probs))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">oneovertwo_unit</span>(<span class=\"pl-smi\">probs</span>):\n  <span class=\"pl-k\">return</span> oneovertwo_unit_fprop(probs, tf.random_uniform(tf.shape(probs)))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> I would like to do like this</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">oneovertwo_unit_bprop</span>(<span class=\"pl-smi\">probs</span>, <span class=\"pl-smi\">outcome</span>, <span class=\"pl-smi\">grads</span>):\n  outcome_probs <span class=\"pl-k\">=</span> outcome <span class=\"pl-k\">*</span> probs <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> outcome) <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> probs)\n  <span class=\"pl-k\">return</span> grads <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span>. <span class=\"pl-k\">/</span> outcome_probs\n\n\n<span class=\"pl-en\">@function.Defun</span>(<span class=\"pl-k\">*</span>(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> [tf.float32]), <span class=\"pl-v\">grad_func</span><span class=\"pl-k\">=</span>oneovertwo_unit_bprop)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">oneovertwo_unit</span>(<span class=\"pl-smi\">probs</span>):\n  <span class=\"pl-k\">return</span> tf.to_float(tf.less(tf.random_uniform(probs.get_shape()), probs))</pre></div>\n<p>FYI, this an implementation of the gradient estimator from the DARN paper (<a href=\"https://arxiv.org/pdf/1310.8499v2.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1310.8499v2.pdf</a>).</p>", "body_text": "When dealing with discrete stochastic units, one often wants to define custom gradients. It seems like the most legitimate way to it in Tensorflow is to use grad_func keyword argument of function.Defun from tensorflow.python.framework.  However, the gradient function is only given the inputs of the function defined and the gradients, whereas it would be often very convenient to also use the outputs.\nFor example, see the code below.\n# In current Tensorflow I have to do like this:\n\n@function.Defun(*(3 * [tf.float32]))\ndef oneovertwo_unit_bprop(probs, noise, grads):\n  outcome = tf.to_float(tf.less(noise, probs))\n  outcome_probs = outcome * probs + (1 - outcome) * (1 - probs)\n  return grads / 2. / outcome_probs, tf.zeros_like(noise)\n\n\n@function.Defun(*(2 * [tf.float32]), grad_func=oneovertwo_unit_bprop)\ndef oneovertwo_unit_fprop(probs, noise):\n  return tf.to_float(tf.less(noise, probs))\n\n\ndef oneovertwo_unit(probs):\n  return oneovertwo_unit_fprop(probs, tf.random_uniform(tf.shape(probs)))\n\n# I would like to do like this\n\ndef oneovertwo_unit_bprop(probs, outcome, grads):\n  outcome_probs = outcome * probs + (1 - outcome) * (1 - probs)\n  return grads / 2. / outcome_probs\n\n\n@function.Defun(*(2 * [tf.float32]), grad_func=oneovertwo_unit_bprop)\ndef oneovertwo_unit(probs):\n  return tf.to_float(tf.less(tf.random_uniform(probs.get_shape()), probs))\nFYI, this an implementation of the gradient estimator from the DARN paper (https://arxiv.org/pdf/1310.8499v2.pdf).", "body": "When dealing with discrete stochastic units, one often wants to define custom gradients. It seems like the most legitimate way to it in Tensorflow is to use `grad_func` keyword argument of `function.Defun` from `tensorflow.python.framework`.  However, the gradient function is only given the inputs of the function defined and the gradients, whereas it would be often very convenient to also use the outputs. \n\nFor example, see the code below.\n\n``` python\n# In current Tensorflow I have to do like this:\n\n@function.Defun(*(3 * [tf.float32]))\ndef oneovertwo_unit_bprop(probs, noise, grads):\n  outcome = tf.to_float(tf.less(noise, probs))\n  outcome_probs = outcome * probs + (1 - outcome) * (1 - probs)\n  return grads / 2. / outcome_probs, tf.zeros_like(noise)\n\n\n@function.Defun(*(2 * [tf.float32]), grad_func=oneovertwo_unit_bprop)\ndef oneovertwo_unit_fprop(probs, noise):\n  return tf.to_float(tf.less(noise, probs))\n\n\ndef oneovertwo_unit(probs):\n  return oneovertwo_unit_fprop(probs, tf.random_uniform(tf.shape(probs)))\n\n# I would like to do like this\n\ndef oneovertwo_unit_bprop(probs, outcome, grads):\n  outcome_probs = outcome * probs + (1 - outcome) * (1 - probs)\n  return grads / 2. / outcome_probs\n\n\n@function.Defun(*(2 * [tf.float32]), grad_func=oneovertwo_unit_bprop)\ndef oneovertwo_unit(probs):\n  return tf.to_float(tf.less(tf.random_uniform(probs.get_shape()), probs))\n```\n\nFYI, this an implementation of the gradient estimator from the DARN paper (https://arxiv.org/pdf/1310.8499v2.pdf).\n"}