{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/252457118", "html_url": "https://github.com/tensorflow/tensorflow/issues/4661#issuecomment-252457118", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4661", "id": 252457118, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MjQ1NzExOA==", "user": {"login": "zffchen78", "id": 7943790, "node_id": "MDQ6VXNlcjc5NDM3OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7943790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zffchen78", "html_url": "https://github.com/zffchen78", "followers_url": "https://api.github.com/users/zffchen78/followers", "following_url": "https://api.github.com/users/zffchen78/following{/other_user}", "gists_url": "https://api.github.com/users/zffchen78/gists{/gist_id}", "starred_url": "https://api.github.com/users/zffchen78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zffchen78/subscriptions", "organizations_url": "https://api.github.com/users/zffchen78/orgs", "repos_url": "https://api.github.com/users/zffchen78/repos", "events_url": "https://api.github.com/users/zffchen78/events{/privacy}", "received_events_url": "https://api.github.com/users/zffchen78/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-09T00:58:30Z", "updated_at": "2016-10-09T01:01:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It's work as intended. The seemingly duplicated computation done by tf.to_float(tf.less(noise, probs)) can be eliminated by TF runtime. See example in function_test.py. Look for usage of _OptimizationOptions().</p>\n<p>The reasoning behind my design choice is that if the grad_func's signature always gives the implementation the result of the forward function. That typically means the result is needed to be kept around until backprop. It's not always a good choice in face of limited memory.</p>\n<p>Instead, if you always recompute the forward compution inside the backward function, TF can have choice to either eliminate the duplicated computation during the whole graph optimization but likely to use more memory; or redo the computation instead using less memory. E.g., in your case, the way to least memory is to have different random_uniform generator which determisticly generates its output from an input seed(s), that way, only the seed (64-bit) is needed in a very long forward-backward chain and can cheaply regenerate the mask.</p>\n<p>In your case, pulling out the random_uniform as the noise is the right choice. Because random_uniform is a stateful operation which outputs different results on each innovation.</p>\n<p>BTW, I recently improved the Defun syntax so that you no longer needs specify the type in the Defun decorator constructor. So you can remove *(3 * [tf.float32]) and *(2 * [tf.float32]): E.g.,</p>\n<pre><code>@function.Defun()\ndef oneovertwo_unit(probs):\n\n  @function.Defun()\n  def bprop(probs, noise, grads):\n    outcome = tf.to_float(tf.less(noise, probs))\n    outcome_probs = outcome * probs + (1 - outcome) * (1 - probs)\n    return grads / 2. / outcome_probs, tf.zeros_like(noise)\n\n\n  @function.Defun(grad_func=bprop)\n  def fprop(probs, noise):\n    return tf.to_float(tf.less(noise, probs))\n\n  return fprop(probs, tf.random_uniform(tf.shape(probs)))\n\n</code></pre>", "body_text": "It's work as intended. The seemingly duplicated computation done by tf.to_float(tf.less(noise, probs)) can be eliminated by TF runtime. See example in function_test.py. Look for usage of _OptimizationOptions().\nThe reasoning behind my design choice is that if the grad_func's signature always gives the implementation the result of the forward function. That typically means the result is needed to be kept around until backprop. It's not always a good choice in face of limited memory.\nInstead, if you always recompute the forward compution inside the backward function, TF can have choice to either eliminate the duplicated computation during the whole graph optimization but likely to use more memory; or redo the computation instead using less memory. E.g., in your case, the way to least memory is to have different random_uniform generator which determisticly generates its output from an input seed(s), that way, only the seed (64-bit) is needed in a very long forward-backward chain and can cheaply regenerate the mask.\nIn your case, pulling out the random_uniform as the noise is the right choice. Because random_uniform is a stateful operation which outputs different results on each innovation.\nBTW, I recently improved the Defun syntax so that you no longer needs specify the type in the Defun decorator constructor. So you can remove *(3 * [tf.float32]) and *(2 * [tf.float32]): E.g.,\n@function.Defun()\ndef oneovertwo_unit(probs):\n\n  @function.Defun()\n  def bprop(probs, noise, grads):\n    outcome = tf.to_float(tf.less(noise, probs))\n    outcome_probs = outcome * probs + (1 - outcome) * (1 - probs)\n    return grads / 2. / outcome_probs, tf.zeros_like(noise)\n\n\n  @function.Defun(grad_func=bprop)\n  def fprop(probs, noise):\n    return tf.to_float(tf.less(noise, probs))\n\n  return fprop(probs, tf.random_uniform(tf.shape(probs)))", "body": "It's work as intended. The seemingly duplicated computation done by tf.to_float(tf.less(noise, probs)) can be eliminated by TF runtime. See example in function_test.py. Look for usage of _OptimizationOptions().\n\nThe reasoning behind my design choice is that if the grad_func's signature always gives the implementation the result of the forward function. That typically means the result is needed to be kept around until backprop. It's not always a good choice in face of limited memory.\n\nInstead, if you always recompute the forward compution inside the backward function, TF can have choice to either eliminate the duplicated computation during the whole graph optimization but likely to use more memory; or redo the computation instead using less memory. E.g., in your case, the way to least memory is to have different random_uniform generator which determisticly generates its output from an input seed(s), that way, only the seed (64-bit) is needed in a very long forward-backward chain and can cheaply regenerate the mask.\n\nIn your case, pulling out the random_uniform as the noise is the right choice. Because random_uniform is a stateful operation which outputs different results on each innovation.\n\nBTW, I recently improved the Defun syntax so that you no longer needs specify the type in the Defun decorator constructor. So you can remove *(3 \\* [tf.float32]) and *(2 \\* [tf.float32]): E.g.,\n\n```\n@function.Defun()\ndef oneovertwo_unit(probs):\n\n  @function.Defun()\n  def bprop(probs, noise, grads):\n    outcome = tf.to_float(tf.less(noise, probs))\n    outcome_probs = outcome * probs + (1 - outcome) * (1 - probs)\n    return grads / 2. / outcome_probs, tf.zeros_like(noise)\n\n\n  @function.Defun(grad_func=bprop)\n  def fprop(probs, noise):\n    return tf.to_float(tf.less(noise, probs))\n\n  return fprop(probs, tf.random_uniform(tf.shape(probs)))\n\n```\n"}