{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/315565900", "html_url": "https://github.com/tensorflow/tensorflow/issues/8617#issuecomment-315565900", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8617", "id": 315565900, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTU2NTkwMA==", "user": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-15T21:55:54Z", "updated_at": "2017-07-15T21:55:54Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">Keras delegates gradient computation to TF. If you have custom\ndifferentiable ops that don't have weights, a Lambda layer works fine.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On 15 July 2017 at 09:37, Hans Gaiser ***@***.***&gt; wrote:\n If you have parameters to train, you should write a custom layer:\n <a href=\"https://keras.io/layers/writing-your-own-keras-layers/\">https://keras.io/layers/writing-your-own-keras-layers/</a>\n\n Thanks <a class=\"user-mention\" href=\"https://github.com/fchollet\">@fchollet</a> &lt;<a href=\"https://github.com/fchollet\">https://github.com/fchollet</a>&gt;, I think I'm beginning to\n understand the situation. In my specific case I'm tring to use a layer with\n no trainable weights (RoiPooling) but it does have a backward propagation\n function. If I register the op in the C API with REGISTER_OP, wrap it in\n a Keras Lambda layer and register the backward propagation function with\n the same op name (currently only possible in python with the\n @ops.RegisterGradient decorator), then tensorflow (and thus Keras) should\n be able to compute correct gradients. I believe this is correct and I will\n continue to work under that assumption. If it is not correct then I would\n gladly hear about it :)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"215969385\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8617\" href=\"https://github.com/tensorflow/tensorflow/issues/8617#issuecomment-315545819\">#8617 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AArWb_IW-Mm04tVs1YHIzNBJBhZenWc7ks5sOOragaJpZM4Mky9s\">https://github.com/notifications/unsubscribe-auth/AArWb_IW-Mm04tVs1YHIzNBJBhZenWc7ks5sOOragaJpZM4Mky9s</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Keras delegates gradient computation to TF. If you have custom\ndifferentiable ops that don't have weights, a Lambda layer works fine.\n\u2026\nOn 15 July 2017 at 09:37, Hans Gaiser ***@***.***> wrote:\n If you have parameters to train, you should write a custom layer:\n https://keras.io/layers/writing-your-own-keras-layers/\n\n Thanks @fchollet <https://github.com/fchollet>, I think I'm beginning to\n understand the situation. In my specific case I'm tring to use a layer with\n no trainable weights (RoiPooling) but it does have a backward propagation\n function. If I register the op in the C API with REGISTER_OP, wrap it in\n a Keras Lambda layer and register the backward propagation function with\n the same op name (currently only possible in python with the\n @ops.RegisterGradient decorator), then tensorflow (and thus Keras) should\n be able to compute correct gradients. I believe this is correct and I will\n continue to work under that assumption. If it is not correct then I would\n gladly hear about it :)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#8617 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AArWb_IW-Mm04tVs1YHIzNBJBhZenWc7ks5sOOragaJpZM4Mky9s>\n .", "body": "Keras delegates gradient computation to TF. If you have custom\ndifferentiable ops that don't have weights, a Lambda layer works fine.\n\nOn 15 July 2017 at 09:37, Hans Gaiser <notifications@github.com> wrote:\n\n> If you have parameters to train, you should write a custom layer:\n> https://keras.io/layers/writing-your-own-keras-layers/\n>\n> Thanks @fchollet <https://github.com/fchollet>, I think I'm beginning to\n> understand the situation. In my specific case I'm tring to use a layer with\n> no trainable weights (RoiPooling) but it does have a backward propagation\n> function. If I register the op in the C API with REGISTER_OP, wrap it in\n> a Keras Lambda layer and register the backward propagation function with\n> the same op name (currently only possible in python with the\n> @ops.RegisterGradient decorator), then tensorflow (and thus Keras) should\n> be able to compute correct gradients. I believe this is correct and I will\n> continue to work under that assumption. If it is not correct then I would\n> gladly hear about it :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8617#issuecomment-315545819>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb_IW-Mm04tVs1YHIzNBJBhZenWc7ks5sOOragaJpZM4Mky9s>\n> .\n>\n"}