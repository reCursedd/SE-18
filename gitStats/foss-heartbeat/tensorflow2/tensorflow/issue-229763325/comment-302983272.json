{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/302983272", "html_url": "https://github.com/tensorflow/tensorflow/issues/10012#issuecomment-302983272", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10012", "id": 302983272, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjk4MzI3Mg==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-22T02:17:50Z", "updated_at": "2017-05-22T02:17:50Z", "author_association": "MEMBER", "body_html": "<p>TF_Reset is used to clear resource <a href=\"https://www.tensorflow.org/api_docs/python/tf/container\" rel=\"nofollow\">containers</a>. This can be used to clear state in a setup that, for example, runs many graphs and doesn't want to run out of memory. Here is a detailed <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/session.h#L189\">comment</a> that explains the current semantics, and also explains why it is quite confusing currently.</p>\n<p>My understanding is that persistent tensors go a step further that what you suggest. They store the underlying tensors in the session state, such that they live on the underlying device memory. They don't have to be fed and fetched every time. They can be thought of \"in-place\" tensors across steps.</p>", "body_text": "TF_Reset is used to clear resource containers. This can be used to clear state in a setup that, for example, runs many graphs and doesn't want to run out of memory. Here is a detailed comment that explains the current semantics, and also explains why it is quite confusing currently.\nMy understanding is that persistent tensors go a step further that what you suggest. They store the underlying tensors in the session state, such that they live on the underlying device memory. They don't have to be fed and fetched every time. They can be thought of \"in-place\" tensors across steps.", "body": "TF_Reset is used to clear resource [containers](https://www.tensorflow.org/api_docs/python/tf/container). This can be used to clear state in a setup that, for example, runs many graphs and doesn't want to run out of memory. Here is a detailed [comment](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/session.h#L189) that explains the current semantics, and also explains why it is quite confusing currently.\r\n\r\nMy understanding is that persistent tensors go a step further that what you suggest. They store the underlying tensors in the session state, such that they live on the underlying device memory. They don't have to be fed and fetched every time. They can be thought of \"in-place\" tensors across steps.\r\n\r\n"}