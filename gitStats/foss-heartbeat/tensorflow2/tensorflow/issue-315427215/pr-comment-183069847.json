{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/183069847", "pull_request_review_id": 113999448, "id": 183069847, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MzA2OTg0Nw==", "diff_hunk": "@@ -0,0 +1,118 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Ordered bijector.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+\n+from tensorflow.python.framework import tensor_shape\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import check_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops.distributions import bijector\n+\n+\n+__all__ = [\n+    \"Ordered\",\n+]\n+\n+\n+class Ordered(bijector.Bijector):\n+  \"\"\"Bijector which maps a tensor x_k that has increasing elements in the last\n+  dimension to an unconstrained tensor y_k.\n+\n+  The inverse of the bijector applied to a normal random vector `y ~ N(0, 1)`\n+  gives back a sorted random vector with the same distribution `x ~ N(0, 1)`\n+  where `x = sort(y)`\n+\n+  On the last dimension of the tensor, Ordered bijector performs:\n+  `y[0] = x[0]`\n+  `y[1:] = math_ops.log(x[1:] - x[:-1])`\n+\n+  Example Use:\n+\n+  ```python\n+  bijector.Ordered().forward([2, 3, 4])\n+  # Result: [2., 0., 0.]\n+\n+  bijector.Ordered().inverse([0.06428002, -1.07774478, -0.71530371])\n+  # Result: [0.06428002, 0.40464228, 0.8936858]\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               validate_args=False,\n+               name=\"ordered\"):\n+    self._graph_parents = []\n+    self._name = name\n+    super(Ordered, self).__init__(\n+        forward_min_event_ndims=1,\n+        validate_args=validate_args,\n+        name=name)\n+\n+  def _forward_event_shape(self, input_shape):\n+    if input_shape.ndims is None or input_shape[-1] is None:\n+      return input_shape\n+    return tensor_shape.TensorShape([input_shape[-1]])\n+\n+  def _forward_event_shape_tensor(self, input_shape):\n+    return (input_shape[-1])[..., array_ops.newaxis]\n+\n+  def _inverse_event_shape(self, output_shape):\n+    if output_shape.ndims is None or output_shape[-1] is None:\n+      return output_shape\n+    if output_shape[-1] <= 1:\n+      raise ValueError(\"output_shape[-1] = %d <= 1\" % output_shape[-1])\n+    return tensor_shape.TensorShape([output_shape[-1]])\n+\n+  def _inverse_event_shape_tensor(self, output_shape):\n+    if self.validate_args:\n+      is_greater_one = check_ops.assert_greater(\n+          output_shape[-1], 1, message=\"Need last dimension greater than 1.\")\n+      output_shape = control_flow_ops.with_dependencies(\n+          [is_greater_one], output_shape)\n+    return (output_shape[-1])[..., array_ops.newaxis]\n+\n+  def _forward(self, x):\n+    x = self._maybe_assert_valid_x(x)\n+    y0 = array_ops.expand_dims(x[..., 0], -1)\n+    yk = math_ops.log(x[..., 1:] - x[..., :-1])\n+    y = array_ops.concat([y0, yk], axis=-1)\n+    return y\n+\n+  def _inverse(self, y):\n+    x0 = array_ops.expand_dims(y[..., 0], -1)\n+    xk = math_ops.exp(y[..., 1:])\n+    x = array_ops.concat([x0, xk], axis=-1)\n+    return math_ops.cumsum(x, axis=-1)\n+\n+  def _inverse_log_det_jacobian(self, y):", "path": "tensorflow/contrib/distributions/python/ops/bijectors/ordered.py", "position": 102, "original_position": 104, "commit_id": "63f4618fbdd653fd19a3663a64da89c476aeb0cd", "original_commit_id": "5c52028c7337baafd8d92d36a29e0fa088393d06", "user": {"login": "srvasude", "id": 1048839, "node_id": "MDQ6VXNlcjEwNDg4Mzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1048839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srvasude", "html_url": "https://github.com/srvasude", "followers_url": "https://api.github.com/users/srvasude/followers", "following_url": "https://api.github.com/users/srvasude/following{/other_user}", "gists_url": "https://api.github.com/users/srvasude/gists{/gist_id}", "starred_url": "https://api.github.com/users/srvasude/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srvasude/subscriptions", "organizations_url": "https://api.github.com/users/srvasude/orgs", "repos_url": "https://api.github.com/users/srvasude/repos", "events_url": "https://api.github.com/users/srvasude/events{/privacy}", "received_events_url": "https://api.github.com/users/srvasude/received_events", "type": "User", "site_admin": false}, "body": "Can you add as  comments here and below a proof / summary of a proof of these results?\r\n\r\nIt would be helpful for debugging purposes / if someone wants to go back to see the implementation (in this case, the jacobian matrix is lower triangular, which is why you only get a sum of the log of the diagonal entries).", "created_at": "2018-04-20T14:32:55Z", "updated_at": "2018-04-25T17:01:45Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/18647#discussion_r183069847", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18647", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/183069847"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/18647#discussion_r183069847"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18647"}}, "body_html": "<p>Can you add as  comments here and below a proof / summary of a proof of these results?</p>\n<p>It would be helpful for debugging purposes / if someone wants to go back to see the implementation (in this case, the jacobian matrix is lower triangular, which is why you only get a sum of the log of the diagonal entries).</p>", "body_text": "Can you add as  comments here and below a proof / summary of a proof of these results?\nIt would be helpful for debugging purposes / if someone wants to go back to see the implementation (in this case, the jacobian matrix is lower triangular, which is why you only get a sum of the log of the diagonal entries)."}