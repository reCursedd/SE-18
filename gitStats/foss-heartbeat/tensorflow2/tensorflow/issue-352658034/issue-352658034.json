{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21767", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21767/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21767/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21767/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21767", "id": 352658034, "node_id": "MDU6SXNzdWUzNTI2NTgwMzQ=", "number": 21767, "title": "`MirrorStrategy` is not working with `tf.estimator.DNNClassifier`", "user": {"login": "yncxcw", "id": 7780675, "node_id": "MDQ6VXNlcjc3ODA2NzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/7780675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yncxcw", "html_url": "https://github.com/yncxcw", "followers_url": "https://api.github.com/users/yncxcw/followers", "following_url": "https://api.github.com/users/yncxcw/following{/other_user}", "gists_url": "https://api.github.com/users/yncxcw/gists{/gist_id}", "starred_url": "https://api.github.com/users/yncxcw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yncxcw/subscriptions", "organizations_url": "https://api.github.com/users/yncxcw/orgs", "repos_url": "https://api.github.com/users/yncxcw/repos", "events_url": "https://api.github.com/users/yncxcw/events{/privacy}", "received_events_url": "https://api.github.com/users/yncxcw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 996845227, "node_id": "MDU6TGFiZWw5OTY4NDUyMjc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:dist-strat", "name": "comp:dist-strat", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "anj-s", "id": 32556631, "node_id": "MDQ6VXNlcjMyNTU2NjMx", "avatar_url": "https://avatars1.githubusercontent.com/u/32556631?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anj-s", "html_url": "https://github.com/anj-s", "followers_url": "https://api.github.com/users/anj-s/followers", "following_url": "https://api.github.com/users/anj-s/following{/other_user}", "gists_url": "https://api.github.com/users/anj-s/gists{/gist_id}", "starred_url": "https://api.github.com/users/anj-s/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anj-s/subscriptions", "organizations_url": "https://api.github.com/users/anj-s/orgs", "repos_url": "https://api.github.com/users/anj-s/repos", "events_url": "https://api.github.com/users/anj-s/events{/privacy}", "received_events_url": "https://api.github.com/users/anj-s/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "anj-s", "id": 32556631, "node_id": "MDQ6VXNlcjMyNTU2NjMx", "avatar_url": "https://avatars1.githubusercontent.com/u/32556631?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anj-s", "html_url": "https://github.com/anj-s", "followers_url": "https://api.github.com/users/anj-s/followers", "following_url": "https://api.github.com/users/anj-s/following{/other_user}", "gists_url": "https://api.github.com/users/anj-s/gists{/gist_id}", "starred_url": "https://api.github.com/users/anj-s/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anj-s/subscriptions", "organizations_url": "https://api.github.com/users/anj-s/orgs", "repos_url": "https://api.github.com/users/anj-s/repos", "events_url": "https://api.github.com/users/anj-s/events{/privacy}", "received_events_url": "https://api.github.com/users/anj-s/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-08-21T18:28:46Z", "updated_at": "2018-10-05T16:27:42Z", "closed_at": "2018-10-05T16:27:41Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nChanged from <code>models/samples/core/get_started/premade_estimator.py</code></li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.64</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:<br>\nNo</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nCompile from source 1.10</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.10</li>\n<li><strong>Python version</strong>:<br>\n2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.16.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\n5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n9.1</li>\n<li><strong>GPU model and memory</strong>:<br>\n2 x TITAN Xp/12GB</li>\n<li><strong>Exact command to reproduce</strong>:<br>\n<code>python premade_estimator.py</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I try to use <code>MirrorStrategy</code> to parallel <code>premade_estimator.py</code>. The  code is below. I made two changes:<br>\n(1) I make a fake input_fn to build (features, labels) from random values.<br>\n(2) I add two lines to use <code>MirrorStrategy</code> when initializing the <code>conf</code> when building <code>estimator</code>.</p>\n<p>It woks fine if I do not use <code>MirrosStrategy</code>(without (2)), but fails as following logs output.</p>\n<p>BTW: I also tried the original input_fn, reported the same errors.</p>\n<p>Appreciate the help</p>\n<h3>Source code / logs</h3>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport tensorflow as tf\n\nimport iris_data\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--batch_size', default=32, type=int, help='batch size')\nparser.add_argument('--train_steps', default=1000, type=int,\n                    help='number of training steps')\n\nCSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n                    'PetalLength', 'PetalWidth', 'Species']\n\ndef train_input_fn_fake(features_name, batch_size):\n    features_len = len(features_name)\n    data_len = 10000\n    features = tf.random_uniform([data_len, features_len], minval=0, maxval=1, dtype=tf.float32)\n    labels = tf.random_uniform([data_len, 1], minval=0, maxval=2, dtype=tf.int32)\n    def map_dnn_input(features, labels):\n        features = tf.split(features, features_len)\n        features = dict(zip(features_name, features))\n        return features, labels\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels)).map(map_dnn_input)\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n    return dataset\n\n\n\ndef main(argv):\n    args = parser.parse_args(argv[1:])\n\n    # Feature columns describe how to use the input.\n    my_feature_columns = []\n    my_features = []\n    for key in CSV_COLUMN_NAMES:\n        my_features.append(key)\n        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n\n\n    # configure multi gpu\n    distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\n    config = tf.estimator.RunConfig(train_distribute=distribution)\n\n    # Build 2 hidden layer DNN with 10, 10 units respectively.\n    classifier = tf.estimator.DNNClassifier(\n        feature_columns=my_feature_columns,\n        # Two hidden layers of 10 nodes each\n        hidden_units=[100, 100],\n        # The model must choose between 3 classes.\n        n_classes=3,\n        # Config\n        config = config\n        )\n\n    # Train the Model.\n    classifier.train(\n        input_fn=lambda:train_input_fn_fake(my_features,\n                                            args.batch_size),\n        steps=args.train_steps,\n        )\n\n    \nif __name__ == '__main__':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main)\n</code></pre>\n<p>Log output:</p>\n<pre><code>\n2018-08-21 12:27:58.645341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:01:00.0\ntotalMemory: 11.91GiB freeMemory: 10.62GiB\n2018-08-21 12:27:58.874232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:02:00.0\ntotalMemory: 11.91GiB freeMemory: 11.74GiB\n2018-08-21 12:27:58.875049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1\n2018-08-21 12:27:59.303516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-21 12:27:59.303548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 \n2018-08-21 12:27:59.303553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y \n2018-08-21 12:27:59.303557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N \n2018-08-21 12:27:59.303931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:0 with 10261 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-08-21 12:27:59.405388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:1 with 11361 MB memory) -&gt; physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\nINFO:tensorflow:Configured nccl all-reduce.\n2018-08-21 12:27:59.555447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1\n2018-08-21 12:27:59.555515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-21 12:27:59.555524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 \n2018-08-21 12:27:59.555530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y \n2018-08-21 12:27:59.555535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N \n2018-08-21 12:27:59.555693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10261 MB memory) -&gt; physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-08-21 12:27:59.555813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11361 MB memory) -&gt; physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:Partitioned variables are disabled when using DistributionStrategy.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:batch_all_reduce invoked for batches size = 6 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\nINFO:tensorflow:Error reported to Coordinator: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n    yield\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 175, in _call_for_each_tower\n    **merge_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 660, in _distributed_apply\n    self._create_slots(var_list)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.py\", line 73, in _create_slots\n    with ops.colocate_with(v):\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4080, in _colocate_with_for_gradient\n    with self.colocate_with(op, ignore_existing):\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4132, in colocate_with\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1285, in internal_convert_to_tensor_or_indexed_slices\n    value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1124, in internal_convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 445, in _tensor_conversion_mirrored\n    assert not as_ref\nAssertionError\nTraceback (most recent call last):\n  File \"premade_estimator.py\", line 84, in &lt;module&gt;\n    tf.app.run(main)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n    _sys.exit(main(argv))\n  File \"premade_estimator.py\", line 78, in main\n    steps=args.train_steps,\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 343, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1166, in _train_model\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1294, in _train_model_distributed\n    self.config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/distribute.py\", line 857, in call_for_each_tower\n    return self._call_for_each_tower(fn, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 418, in _call_for_each_tower\n    return _call_for_each_tower(self, fn, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 181, in _call_for_each_tower\n    coord.join(threads)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n    yield\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 175, in _call_for_each_tower\n    **merge_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 660, in _distributed_apply\n    self._create_slots(var_list)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.py\", line 73, in _create_slots\n    with ops.colocate_with(v):\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4080, in _colocate_with_for_gradient\n    with self.colocate_with(op, ignore_existing):\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4132, in colocate_with\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1285, in internal_convert_to_tensor_or_indexed_slices\n    value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1124, in internal_convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 445, in _tensor_conversion_mirrored\n    assert not as_ref\nAssertionError\n\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nChanged from models/samples/core/get_started/premade_estimator.py\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.64\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nNo\nTensorFlow installed from (source or binary):\nCompile from source 1.10\nTensorFlow version (use command below):\n1.10\nPython version:\n2.7.12\nBazel version (if compiling from source):\n0.16.1\nGCC/Compiler version (if compiling from source):\n5.4.0\nCUDA/cuDNN version:\n9.1\nGPU model and memory:\n2 x TITAN Xp/12GB\nExact command to reproduce:\npython premade_estimator.py\n\nDescribe the problem\nI try to use MirrorStrategy to parallel premade_estimator.py. The  code is below. I made two changes:\n(1) I make a fake input_fn to build (features, labels) from random values.\n(2) I add two lines to use MirrorStrategy when initializing the conf when building estimator.\nIt woks fine if I do not use MirrosStrategy(without (2)), but fails as following logs output.\nBTW: I also tried the original input_fn, reported the same errors.\nAppreciate the help\nSource code / logs\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport tensorflow as tf\n\nimport iris_data\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--batch_size', default=32, type=int, help='batch size')\nparser.add_argument('--train_steps', default=1000, type=int,\n                    help='number of training steps')\n\nCSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n                    'PetalLength', 'PetalWidth', 'Species']\n\ndef train_input_fn_fake(features_name, batch_size):\n    features_len = len(features_name)\n    data_len = 10000\n    features = tf.random_uniform([data_len, features_len], minval=0, maxval=1, dtype=tf.float32)\n    labels = tf.random_uniform([data_len, 1], minval=0, maxval=2, dtype=tf.int32)\n    def map_dnn_input(features, labels):\n        features = tf.split(features, features_len)\n        features = dict(zip(features_name, features))\n        return features, labels\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels)).map(map_dnn_input)\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n    return dataset\n\n\n\ndef main(argv):\n    args = parser.parse_args(argv[1:])\n\n    # Feature columns describe how to use the input.\n    my_feature_columns = []\n    my_features = []\n    for key in CSV_COLUMN_NAMES:\n        my_features.append(key)\n        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n\n\n    # configure multi gpu\n    distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\n    config = tf.estimator.RunConfig(train_distribute=distribution)\n\n    # Build 2 hidden layer DNN with 10, 10 units respectively.\n    classifier = tf.estimator.DNNClassifier(\n        feature_columns=my_feature_columns,\n        # Two hidden layers of 10 nodes each\n        hidden_units=[100, 100],\n        # The model must choose between 3 classes.\n        n_classes=3,\n        # Config\n        config = config\n        )\n\n    # Train the Model.\n    classifier.train(\n        input_fn=lambda:train_input_fn_fake(my_features,\n                                            args.batch_size),\n        steps=args.train_steps,\n        )\n\n    \nif __name__ == '__main__':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run(main)\n\nLog output:\n\n2018-08-21 12:27:58.645341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:01:00.0\ntotalMemory: 11.91GiB freeMemory: 10.62GiB\n2018-08-21 12:27:58.874232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: \nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:02:00.0\ntotalMemory: 11.91GiB freeMemory: 11.74GiB\n2018-08-21 12:27:58.875049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1\n2018-08-21 12:27:59.303516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-21 12:27:59.303548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 \n2018-08-21 12:27:59.303553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y \n2018-08-21 12:27:59.303557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N \n2018-08-21 12:27:59.303931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:0 with 10261 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-08-21 12:27:59.405388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:1 with 11361 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\nINFO:tensorflow:Configured nccl all-reduce.\n2018-08-21 12:27:59.555447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1\n2018-08-21 12:27:59.555515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-08-21 12:27:59.555524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 \n2018-08-21 12:27:59.555530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y \n2018-08-21 12:27:59.555535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N \n2018-08-21 12:27:59.555693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10261 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-08-21 12:27:59.555813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11361 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:Partitioned variables are disabled when using DistributionStrategy.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:batch_all_reduce invoked for batches size = 6 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\nINFO:tensorflow:Error reported to Coordinator: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n    yield\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 175, in _call_for_each_tower\n    **merge_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 660, in _distributed_apply\n    self._create_slots(var_list)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.py\", line 73, in _create_slots\n    with ops.colocate_with(v):\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4080, in _colocate_with_for_gradient\n    with self.colocate_with(op, ignore_existing):\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4132, in colocate_with\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1285, in internal_convert_to_tensor_or_indexed_slices\n    value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1124, in internal_convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 445, in _tensor_conversion_mirrored\n    assert not as_ref\nAssertionError\nTraceback (most recent call last):\n  File \"premade_estimator.py\", line 84, in <module>\n    tf.app.run(main)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n    _sys.exit(main(argv))\n  File \"premade_estimator.py\", line 78, in main\n    steps=args.train_steps,\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 343, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1166, in _train_model\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1294, in _train_model_distributed\n    self.config)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/distribute.py\", line 857, in call_for_each_tower\n    return self._call_for_each_tower(fn, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 418, in _call_for_each_tower\n    return _call_for_each_tower(self, fn, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 181, in _call_for_each_tower\n    coord.join(threads)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n    yield\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 175, in _call_for_each_tower\n    **merge_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 660, in _distributed_apply\n    self._create_slots(var_list)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.py\", line 73, in _create_slots\n    with ops.colocate_with(v):\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4080, in _colocate_with_for_gradient\n    with self.colocate_with(op, ignore_existing):\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4132, in colocate_with\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1285, in internal_convert_to_tensor_or_indexed_slices\n    value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1124, in internal_convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 445, in _tensor_conversion_mirrored\n    assert not as_ref\nAssertionError", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nChanged from `models/samples/core/get_started/premade_estimator.py`\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.64\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNo\r\n- **TensorFlow installed from (source or binary)**:\r\nCompile from source 1.10\r\n- **TensorFlow version (use command below)**:\r\n1.10\r\n- **Python version**:\r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.16.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\n9.1\r\n- **GPU model and memory**:\r\n2 x TITAN Xp/12GB\r\n- **Exact command to reproduce**:\r\n`python premade_estimator.py`\r\n\r\n### Describe the problem\r\nI try to use `MirrorStrategy` to parallel `premade_estimator.py`. The  code is below. I made two changes:\r\n(1) I make a fake input_fn to build (features, labels) from random values.\r\n(2) I add two lines to use `MirrorStrategy` when initializing the `conf` when building `estimator`.\r\n\r\nIt woks fine if I do not use `MirrosStrategy`(without (2)), but fails as following logs output.\r\n\r\nBTW: I also tried the original input_fn, reported the same errors.\r\n\r\nAppreciate the help\r\n\r\n### Source code / logs\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\nimport iris_data\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--batch_size', default=32, type=int, help='batch size')\r\nparser.add_argument('--train_steps', default=1000, type=int,\r\n                    help='number of training steps')\r\n\r\nCSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\r\n                    'PetalLength', 'PetalWidth', 'Species']\r\n\r\ndef train_input_fn_fake(features_name, batch_size):\r\n    features_len = len(features_name)\r\n    data_len = 10000\r\n    features = tf.random_uniform([data_len, features_len], minval=0, maxval=1, dtype=tf.float32)\r\n    labels = tf.random_uniform([data_len, 1], minval=0, maxval=2, dtype=tf.int32)\r\n    def map_dnn_input(features, labels):\r\n        features = tf.split(features, features_len)\r\n        features = dict(zip(features_name, features))\r\n        return features, labels\r\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels)).map(map_dnn_input)\r\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\r\n    return dataset\r\n\r\n\r\n\r\ndef main(argv):\r\n    args = parser.parse_args(argv[1:])\r\n\r\n    # Feature columns describe how to use the input.\r\n    my_feature_columns = []\r\n    my_features = []\r\n    for key in CSV_COLUMN_NAMES:\r\n        my_features.append(key)\r\n        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\r\n\r\n\r\n    # configure multi gpu\r\n    distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\n    config = tf.estimator.RunConfig(train_distribute=distribution)\r\n\r\n    # Build 2 hidden layer DNN with 10, 10 units respectively.\r\n    classifier = tf.estimator.DNNClassifier(\r\n        feature_columns=my_feature_columns,\r\n        # Two hidden layers of 10 nodes each\r\n        hidden_units=[100, 100],\r\n        # The model must choose between 3 classes.\r\n        n_classes=3,\r\n        # Config\r\n        config = config\r\n        )\r\n\r\n    # Train the Model.\r\n    classifier.train(\r\n        input_fn=lambda:train_input_fn_fake(my_features,\r\n                                            args.batch_size),\r\n        steps=args.train_steps,\r\n        )\r\n\r\n    \r\nif __name__ == '__main__':\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run(main)\r\n```\r\n\r\n\r\n\r\nLog output:\r\n```\r\n\r\n2018-08-21 12:27:58.645341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.91GiB freeMemory: 10.62GiB\r\n2018-08-21 12:27:58.874232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.91GiB freeMemory: 11.74GiB\r\n2018-08-21 12:27:58.875049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1\r\n2018-08-21 12:27:59.303516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-21 12:27:59.303548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 \r\n2018-08-21 12:27:59.303553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y \r\n2018-08-21 12:27:59.303557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N \r\n2018-08-21 12:27:59.303931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:0 with 10261 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-08-21 12:27:59.405388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/device:GPU:1 with 11361 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\n2018-08-21 12:27:59.555447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1\r\n2018-08-21 12:27:59.555515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-21 12:27:59.555524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 \r\n2018-08-21 12:27:59.555530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y \r\n2018-08-21 12:27:59.555535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N \r\n2018-08-21 12:27:59.555693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10261 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-08-21 12:27:59.555813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11361 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Calling model_fn.\r\nWARNING:tensorflow:Partitioned variables are disabled when using DistributionStrategy.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 6 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Error reported to Coordinator: \r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 175, in _call_for_each_tower\r\n    **merge_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 660, in _distributed_apply\r\n    self._create_slots(var_list)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.py\", line 73, in _create_slots\r\n    with ops.colocate_with(v):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4080, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4132, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1285, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1124, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 445, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\nTraceback (most recent call last):\r\n  File \"premade_estimator.py\", line 84, in <module>\r\n    tf.app.run(main)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"premade_estimator.py\", line 78, in main\r\n    steps=args.train_steps,\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 343, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1166, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1294, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/distribute.py\", line 857, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 418, in _call_for_each_tower\r\n    return _call_for_each_tower(self, fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 181, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 175, in _call_for_each_tower\r\n    **merge_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 660, in _distributed_apply\r\n    self._create_slots(var_list)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adagrad.py\", line 73, in _create_slots\r\n    with ops.colocate_with(v):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4080, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4132, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1285, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1124, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 445, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n\r\n```"}