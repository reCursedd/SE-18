{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17347", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17347/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17347/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17347/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17347", "id": 301321689, "node_id": "MDU6SXNzdWUzMDEzMjE2ODk=", "number": 17347, "title": "[suggestion] New option for dynamic_decode function", "user": {"login": "yanghoonkim", "id": 9985986, "node_id": "MDQ6VXNlcjk5ODU5ODY=", "avatar_url": "https://avatars2.githubusercontent.com/u/9985986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yanghoonkim", "html_url": "https://github.com/yanghoonkim", "followers_url": "https://api.github.com/users/yanghoonkim/followers", "following_url": "https://api.github.com/users/yanghoonkim/following{/other_user}", "gists_url": "https://api.github.com/users/yanghoonkim/gists{/gist_id}", "starred_url": "https://api.github.com/users/yanghoonkim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yanghoonkim/subscriptions", "organizations_url": "https://api.github.com/users/yanghoonkim/orgs", "repos_url": "https://api.github.com/users/yanghoonkim/repos", "events_url": "https://api.github.com/users/yanghoonkim/events{/privacy}", "received_events_url": "https://api.github.com/users/yanghoonkim/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": {"login": "andydavis1", "id": 15696327, "node_id": "MDQ6VXNlcjE1Njk2MzI3", "avatar_url": "https://avatars0.githubusercontent.com/u/15696327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andydavis1", "html_url": "https://github.com/andydavis1", "followers_url": "https://api.github.com/users/andydavis1/followers", "following_url": "https://api.github.com/users/andydavis1/following{/other_user}", "gists_url": "https://api.github.com/users/andydavis1/gists{/gist_id}", "starred_url": "https://api.github.com/users/andydavis1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andydavis1/subscriptions", "organizations_url": "https://api.github.com/users/andydavis1/orgs", "repos_url": "https://api.github.com/users/andydavis1/repos", "events_url": "https://api.github.com/users/andydavis1/events{/privacy}", "received_events_url": "https://api.github.com/users/andydavis1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "andydavis1", "id": 15696327, "node_id": "MDQ6VXNlcjE1Njk2MzI3", "avatar_url": "https://avatars0.githubusercontent.com/u/15696327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andydavis1", "html_url": "https://github.com/andydavis1", "followers_url": "https://api.github.com/users/andydavis1/followers", "following_url": "https://api.github.com/users/andydavis1/following{/other_user}", "gists_url": "https://api.github.com/users/andydavis1/gists{/gist_id}", "starred_url": "https://api.github.com/users/andydavis1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andydavis1/subscriptions", "organizations_url": "https://api.github.com/users/andydavis1/orgs", "repos_url": "https://api.github.com/users/andydavis1/repos", "events_url": "https://api.github.com/users/andydavis1/events{/privacy}", "received_events_url": "https://api.github.com/users/andydavis1/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-03-01T08:48:38Z", "updated_at": "2018-11-22T18:54:21Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 8</li>\n<li><strong>GPU model and memory</strong>: 1080ti, 11GB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I found that while using <code>dynamic_decode</code> with option <code>maximum_iterations</code>, The first priority of output length is the maximum length of given decoder input when <strong>maximum length of decoder input &lt; maximum_iteration</strong>. Can you provide a new option that I can just fix the size of decoder output size, not considering the maximum length of given decoder input.</p>\n<h3>example code</h3>\n<pre><code>import tensorflow as tf\n\n\nbatch_size = 4\nhidden_size = 5\n\n# [batch, sequence_length, depth]\n\nmemory = tf.get_variable('memory', [4, 10, 5], dtype = tf.float32)\nlen_enc = tf.Variable([5,6,7,8])\n\n# [batch, sequence_length, depth]\nembd_dec_input = tf.get_variable('dec_input', [4, 8, 5], dtype = tf.float32)\nlen_dec = tf.Variable([3,4,5,6])\n\n\n# Create an attention mechanism\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n        hidden_size, memory,\n        memory_sequence_length=len_enc)\n\n# Build decoder cell\ndecoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n        decoder_cell, attention_mechanism,\n        attention_layer_size=hidden_size)\n\n# Helper for decoder cell\nhelper = tf.contrib.seq2seq.TrainingHelper(\n    embd_dec_input, len_dec\n)\n\n# Decoder initial state\ninitial_state = decoder_cell.zero_state(dtype = tf.float32, batch_size = batch_size)\n\n# Decoder\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    decoder_cell, helper, initial_state,\n    output_layer=None)\n\n# Simply fix the decoder output length as the length of decoder input\nmax_iter = 8\n\noutputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_iter)\n</code></pre>\n<p><strong>Even my maximum length of decoder input length is 6, I want the decoder to output with maximum length 8</strong></p>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4\nPython version: 2.7\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 8\nGPU model and memory: 1080ti, 11GB\nExact command to reproduce:\n\nDescribe the problem\nI found that while using dynamic_decode with option maximum_iterations, The first priority of output length is the maximum length of given decoder input when maximum length of decoder input < maximum_iteration. Can you provide a new option that I can just fix the size of decoder output size, not considering the maximum length of given decoder input.\nexample code\nimport tensorflow as tf\n\n\nbatch_size = 4\nhidden_size = 5\n\n# [batch, sequence_length, depth]\n\nmemory = tf.get_variable('memory', [4, 10, 5], dtype = tf.float32)\nlen_enc = tf.Variable([5,6,7,8])\n\n# [batch, sequence_length, depth]\nembd_dec_input = tf.get_variable('dec_input', [4, 8, 5], dtype = tf.float32)\nlen_dec = tf.Variable([3,4,5,6])\n\n\n# Create an attention mechanism\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\n        hidden_size, memory,\n        memory_sequence_length=len_enc)\n\n# Build decoder cell\ndecoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n        decoder_cell, attention_mechanism,\n        attention_layer_size=hidden_size)\n\n# Helper for decoder cell\nhelper = tf.contrib.seq2seq.TrainingHelper(\n    embd_dec_input, len_dec\n)\n\n# Decoder initial state\ninitial_state = decoder_cell.zero_state(dtype = tf.float32, batch_size = batch_size)\n\n# Decoder\ndecoder = tf.contrib.seq2seq.BasicDecoder(\n    decoder_cell, helper, initial_state,\n    output_layer=None)\n\n# Simply fix the decoder output length as the length of decoder input\nmax_iter = 8\n\noutputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_iter)\n\nEven my maximum length of decoder input length is 6, I want the decoder to output with maximum length 8", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: 1080ti, 11GB\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI found that while using `dynamic_decode` with option `maximum_iterations`, The first priority of output length is the maximum length of given decoder input when **maximum length of decoder input < maximum_iteration**. Can you provide a new option that I can just fix the size of decoder output size, not considering the maximum length of given decoder input.\r\n\r\n### example code\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nbatch_size = 4\r\nhidden_size = 5\r\n\r\n# [batch, sequence_length, depth]\r\n\r\nmemory = tf.get_variable('memory', [4, 10, 5], dtype = tf.float32)\r\nlen_enc = tf.Variable([5,6,7,8])\r\n\r\n# [batch, sequence_length, depth]\r\nembd_dec_input = tf.get_variable('dec_input', [4, 8, 5], dtype = tf.float32)\r\nlen_dec = tf.Variable([3,4,5,6])\r\n\r\n\r\n# Create an attention mechanism\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n        hidden_size, memory,\r\n        memory_sequence_length=len_enc)\r\n\r\n# Build decoder cell\r\ndecoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n        decoder_cell, attention_mechanism,\r\n        attention_layer_size=hidden_size)\r\n\r\n# Helper for decoder cell\r\nhelper = tf.contrib.seq2seq.TrainingHelper(\r\n    embd_dec_input, len_dec\r\n)\r\n\r\n# Decoder initial state\r\ninitial_state = decoder_cell.zero_state(dtype = tf.float32, batch_size = batch_size)\r\n\r\n# Decoder\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(\r\n    decoder_cell, helper, initial_state,\r\n    output_layer=None)\r\n\r\n# Simply fix the decoder output length as the length of decoder input\r\nmax_iter = 8\r\n\r\noutputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_iter)\r\n```\r\n\r\n**Even my maximum length of decoder input length is 6, I want the decoder to output with maximum length 8**"}