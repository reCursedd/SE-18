{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/398349474", "html_url": "https://github.com/tensorflow/tensorflow/issues/1787#issuecomment-398349474", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1787", "id": 398349474, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODM0OTQ3NA==", "user": {"login": "Soltius63", "id": 24676909, "node_id": "MDQ6VXNlcjI0Njc2OTA5", "avatar_url": "https://avatars2.githubusercontent.com/u/24676909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Soltius63", "html_url": "https://github.com/Soltius63", "followers_url": "https://api.github.com/users/Soltius63/followers", "following_url": "https://api.github.com/users/Soltius63/following{/other_user}", "gists_url": "https://api.github.com/users/Soltius63/gists{/gist_id}", "starred_url": "https://api.github.com/users/Soltius63/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Soltius63/subscriptions", "organizations_url": "https://api.github.com/users/Soltius63/orgs", "repos_url": "https://api.github.com/users/Soltius63/repos", "events_url": "https://api.github.com/users/Soltius63/events{/privacy}", "received_events_url": "https://api.github.com/users/Soltius63/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-19T10:16:52Z", "updated_at": "2018-06-19T10:16:52Z", "author_association": "NONE", "body_html": "<p>Just to weigh in on <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4516927\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/philipperemy\">@philipperemy</a>  's answer, CUDNN_STATUS_BAD_PARAM can indeed be sort of unrelated to cuda, and hide a \"simple\" code error. In my case, I was trying to convolute an input with a kernel bigger than the actual input. Code to reproduce :</p>\n<pre><code>class _network(nn.Module):\n    def __init__(self):\n        super(_network, self).__init__()\n\n        self.test_module = nn.Sequential(\n            nn.Conv2d(1, int(64 / 2), 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n    def forward(self, input):\n        output=self.test_module(input)\n        return output\n\ninput=Variable(torch.zeros(64,1,1,1).cuda())\n\nnetwork=_network()\nnetwork=network.cuda()\noutput=network(input)\n</code></pre>\n<p>This gives the CUDNN_STATUS_BAD_PARAM error with Pytorch 0.3.1 and Python 3.6. Removing the \"cuda\" from the network and the input yields a much more informative error on what is actually happening :</p>\n<blockquote>\n<p>RuntimeError: Calculated input size: (3 x 3). Kernel size: (4 x 4). Kernel size can't greater than actual input size at /pytorch/torch/lib/THNN/generic/SpatialConvolutionMM.c:46</p>\n</blockquote>", "body_text": "Just to weigh in on @philipperemy  's answer, CUDNN_STATUS_BAD_PARAM can indeed be sort of unrelated to cuda, and hide a \"simple\" code error. In my case, I was trying to convolute an input with a kernel bigger than the actual input. Code to reproduce :\nclass _network(nn.Module):\n    def __init__(self):\n        super(_network, self).__init__()\n\n        self.test_module = nn.Sequential(\n            nn.Conv2d(1, int(64 / 2), 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n    def forward(self, input):\n        output=self.test_module(input)\n        return output\n\ninput=Variable(torch.zeros(64,1,1,1).cuda())\n\nnetwork=_network()\nnetwork=network.cuda()\noutput=network(input)\n\nThis gives the CUDNN_STATUS_BAD_PARAM error with Pytorch 0.3.1 and Python 3.6. Removing the \"cuda\" from the network and the input yields a much more informative error on what is actually happening :\n\nRuntimeError: Calculated input size: (3 x 3). Kernel size: (4 x 4). Kernel size can't greater than actual input size at /pytorch/torch/lib/THNN/generic/SpatialConvolutionMM.c:46", "body": "Just to weigh in on @philipperemy  's answer, CUDNN_STATUS_BAD_PARAM can indeed be sort of unrelated to cuda, and hide a \"simple\" code error. In my case, I was trying to convolute an input with a kernel bigger than the actual input. Code to reproduce : \r\n\r\n\r\n```\r\nclass _network(nn.Module):\r\n    def __init__(self):\r\n        super(_network, self).__init__()\r\n\r\n        self.test_module = nn.Sequential(\r\n            nn.Conv2d(1, int(64 / 2), 4, 2, 1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n\r\n    def forward(self, input):\r\n        output=self.test_module(input)\r\n        return output\r\n\r\ninput=Variable(torch.zeros(64,1,1,1).cuda())\r\n\r\nnetwork=_network()\r\nnetwork=network.cuda()\r\noutput=network(input)\r\n```\r\n\r\nThis gives the CUDNN_STATUS_BAD_PARAM error with Pytorch 0.3.1 and Python 3.6. Removing the \"cuda\" from the network and the input yields a much more informative error on what is actually happening : \r\n\r\n> RuntimeError: Calculated input size: (3 x 3). Kernel size: (4 x 4). Kernel size can't greater than actual input size at /pytorch/torch/lib/THNN/generic/SpatialConvolutionMM.c:46\r\n"}