{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17851", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17851/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17851/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17851/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17851", "id": 306717552, "node_id": "MDU6SXNzdWUzMDY3MTc1NTI=", "number": 17851, "title": "[Feature Request] Multiple GPU Training using Eager Execution", "user": {"login": "Purpleslz", "id": 15375842, "node_id": "MDQ6VXNlcjE1Mzc1ODQy", "avatar_url": "https://avatars2.githubusercontent.com/u/15375842?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Purpleslz", "html_url": "https://github.com/Purpleslz", "followers_url": "https://api.github.com/users/Purpleslz/followers", "following_url": "https://api.github.com/users/Purpleslz/following{/other_user}", "gists_url": "https://api.github.com/users/Purpleslz/gists{/gist_id}", "starred_url": "https://api.github.com/users/Purpleslz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Purpleslz/subscriptions", "organizations_url": "https://api.github.com/users/Purpleslz/orgs", "repos_url": "https://api.github.com/users/Purpleslz/repos", "events_url": "https://api.github.com/users/Purpleslz/events{/privacy}", "received_events_url": "https://api.github.com/users/Purpleslz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "guptapriya", "id": 14104855, "node_id": "MDQ6VXNlcjE0MTA0ODU1", "avatar_url": "https://avatars1.githubusercontent.com/u/14104855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guptapriya", "html_url": "https://github.com/guptapriya", "followers_url": "https://api.github.com/users/guptapriya/followers", "following_url": "https://api.github.com/users/guptapriya/following{/other_user}", "gists_url": "https://api.github.com/users/guptapriya/gists{/gist_id}", "starred_url": "https://api.github.com/users/guptapriya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guptapriya/subscriptions", "organizations_url": "https://api.github.com/users/guptapriya/orgs", "repos_url": "https://api.github.com/users/guptapriya/repos", "events_url": "https://api.github.com/users/guptapriya/events{/privacy}", "received_events_url": "https://api.github.com/users/guptapriya/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "guptapriya", "id": 14104855, "node_id": "MDQ6VXNlcjE0MTA0ODU1", "avatar_url": "https://avatars1.githubusercontent.com/u/14104855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guptapriya", "html_url": "https://github.com/guptapriya", "followers_url": "https://api.github.com/users/guptapriya/followers", "following_url": "https://api.github.com/users/guptapriya/following{/other_user}", "gists_url": "https://api.github.com/users/guptapriya/gists{/gist_id}", "starred_url": "https://api.github.com/users/guptapriya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guptapriya/subscriptions", "organizations_url": "https://api.github.com/users/guptapriya/orgs", "repos_url": "https://api.github.com/users/guptapriya/repos", "events_url": "https://api.github.com/users/guptapriya/events{/privacy}", "received_events_url": "https://api.github.com/users/guptapriya/received_events", "type": "User", "site_admin": false}, {"login": "josh11b", "id": 15258583, "node_id": "MDQ6VXNlcjE1MjU4NTgz", "avatar_url": "https://avatars0.githubusercontent.com/u/15258583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/josh11b", "html_url": "https://github.com/josh11b", "followers_url": "https://api.github.com/users/josh11b/followers", "following_url": "https://api.github.com/users/josh11b/following{/other_user}", "gists_url": "https://api.github.com/users/josh11b/gists{/gist_id}", "starred_url": "https://api.github.com/users/josh11b/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/josh11b/subscriptions", "organizations_url": "https://api.github.com/users/josh11b/orgs", "repos_url": "https://api.github.com/users/josh11b/repos", "events_url": "https://api.github.com/users/josh11b/events{/privacy}", "received_events_url": "https://api.github.com/users/josh11b/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-03-20T03:36:25Z", "updated_at": "2018-11-14T19:16:06Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm a starter in eager execution and not familiar in DL framework. However, in practice, multiple GPUs training is an important feature. PyTorch has nn.DataParallel and distributed package to support distributed training. Recently, I'm working on training model using eager execution with multiple gpus, and I have noticed that in <a href=\"https://hn.svelte.technology/item/15595123\" rel=\"nofollow\">https://hn.svelte.technology/item/15595123</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> said:</p>\n<blockquote>\n<p>We're still fairly early in the project, so <em>for now threading is the only supported way</em>.</p>\n</blockquote>\n<p>I have two questions about it:</p>\n<ol>\n<li>If there is an example about using threading to train with multiple gpus in eager execution? It will help a lot to starters.</li>\n<li>I have concerns about performance using threading(only one thread can run python at one time in Cython implementation). Could threading speed up the training process? For example, if I have some python operations betweens tf operations(eg. [<em>DenseLayer</em>, <em>some operations using numpy, python list, etc.</em>, <em>DenseLayer</em>]), those tf operations(DenseLayer) in different threads could be parallelized, but those numpy operations in different threads are not going to be parallelizable?</li>\n</ol>\n<p>Sorry for my poor english, please correct me if I'm wrong. Thank you!</p>", "body_text": "I'm a starter in eager execution and not familiar in DL framework. However, in practice, multiple GPUs training is an important feature. PyTorch has nn.DataParallel and distributed package to support distributed training. Recently, I'm working on training model using eager execution with multiple gpus, and I have noticed that in https://hn.svelte.technology/item/15595123 @alextp said:\n\nWe're still fairly early in the project, so for now threading is the only supported way.\n\nI have two questions about it:\n\nIf there is an example about using threading to train with multiple gpus in eager execution? It will help a lot to starters.\nI have concerns about performance using threading(only one thread can run python at one time in Cython implementation). Could threading speed up the training process? For example, if I have some python operations betweens tf operations(eg. [DenseLayer, some operations using numpy, python list, etc., DenseLayer]), those tf operations(DenseLayer) in different threads could be parallelized, but those numpy operations in different threads are not going to be parallelizable?\n\nSorry for my poor english, please correct me if I'm wrong. Thank you!", "body": "I'm a starter in eager execution and not familiar in DL framework. However, in practice, multiple GPUs training is an important feature. PyTorch has nn.DataParallel and distributed package to support distributed training. Recently, I'm working on training model using eager execution with multiple gpus, and I have noticed that in https://hn.svelte.technology/item/15595123 @alextp said:\r\n\r\n> We're still fairly early in the project, so _for now threading is the only supported way_.\r\n\r\nI have two questions about it:\r\n1. If there is an example about using threading to train with multiple gpus in eager execution? It will help a lot to starters.\r\n2. I have concerns about performance using threading(only one thread can run python at one time in Cython implementation). Could threading speed up the training process? For example, if I have some python operations betweens tf operations(eg. [*DenseLayer*, *some operations using numpy, python list, etc.*, *DenseLayer*]), those tf operations(DenseLayer) in different threads could be parallelized, but those numpy operations in different threads are not going to be parallelizable?\r\n\r\nSorry for my poor english, please correct me if I'm wrong. Thank you!"}