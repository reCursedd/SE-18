{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1185", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1185/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1185/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1185/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1185", "id": 134756741, "node_id": "MDU6SXNzdWUxMzQ3NTY3NDE=", "number": 1185, "title": "Logistic Regression(LR) Results looks strange when using bias but good without bias", "user": {"login": "ziyanluo", "id": 11676982, "node_id": "MDQ6VXNlcjExNjc2OTgy", "avatar_url": "https://avatars0.githubusercontent.com/u/11676982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ziyanluo", "html_url": "https://github.com/ziyanluo", "followers_url": "https://api.github.com/users/ziyanluo/followers", "following_url": "https://api.github.com/users/ziyanluo/following{/other_user}", "gists_url": "https://api.github.com/users/ziyanluo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ziyanluo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ziyanluo/subscriptions", "organizations_url": "https://api.github.com/users/ziyanluo/orgs", "repos_url": "https://api.github.com/users/ziyanluo/repos", "events_url": "https://api.github.com/users/ziyanluo/events{/privacy}", "received_events_url": "https://api.github.com/users/ziyanluo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-02-19T02:48:12Z", "updated_at": "2016-02-19T02:49:35Z", "closed_at": "2016-02-19T02:49:35Z", "author_association": "NONE", "body_html": "<p>I tried the LR for sparse dataset using the tensorflow package(0.7.0)<br>\nThe following is part of my procedure:</p>\n<p>weight_values=generateWeight([trainset.feature_num,1],name='weight')<br>\nbias=init_bias([1,1],name='bias')<br>\nsp_shape=tf.placeholder(tf.int64)<br>\nsp_indices=tf.placeholder(tf.int64)<br>\nsp_ids_value=tf.placeholder(tf.int64)<br>\nsp_features_value=tf.placeholder(tf.float32)<br>\nY=tf.placeholder('float',name='Y')</p>\n<p>sp_ids=tf.SparseTensor(sp_indices,sp_ids_value,sp_shape)<br>\nsp_values=tf.SparseTensor(sp_indices,sp_features_value,sp_shape)<br>\n<strong>#Z=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values);<br>\nZ_b=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values)+bias</strong><br>\npredict_op=tf.sigmoid(Z_b,name='result')</p>\n<h1>cost=tf.nn.sigmoid_cross_entropy_with_logits(Z,Y)</h1>\n<p>cost=tf.nn.sigmoid_cross_entropy_with_logits(Z_b,Y)<br>\ntrain_op=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</p>\n<p>...<br>\n<strong>but I find the results is  wired when using the bias,using the bias I find the abs(cost) became larger and larger and even the abs(bias) also became larger and larger.</strong><br>\nIN the early iterations the results like this :<br>\n1. labels[-1], j-sample 1662, i-bias 0.100000 cost 0.761811, z_b 0.045602<br>\n2 labels[1], j-sample 823, i-bias 0.084886 cost 0.653277, z_b 0.081396<br>\n3 labels[-1], j-sample 20802, i-bias 0.089683 cost 0.826316, z_b 0.088132<br>\n4 labels[-1], j-sample 25965, i-bias 0.074462 cost 0.806052, z_b 0.074804<br>\n5 labels[1], j-sample 10322, i-bias 0.059276 cost 0.664358, z_b 0.058433<br>\n6 labels[-1], j-sample 23946, i-bias 0.064129 cost 0.795182, z_b 0.067642<br>\n......<br>\nbut with more iteration the results like this :<br>\n270504 labels[-1], j-sample 446, i-bias -248.318787 cost -250.818787, z_b -250.818787<br>\n270505 labels[1], j-sample 10314, i-bias -248.328781 cost 248.306259, z_b -248.306259<br>\n270506 labels[1], j-sample 3820, i-bias -248.318787 cost 247.367340, z_b -247.367340<br>\n270507 labels[1], j-sample 2922, i-bias -248.308792 cost 248.276184, z_b -248.276184<br>\n270508 labels[-1], j-sample 20797, i-bias -248.298798 cost -255.061432, z_b -255.061432<br>\n270509 labels[-1], j-sample 19755, i-bias -248.308792 cost -251.686646, z <em>b -251.686646<br>\n270510 labels[1], j-sample 9528, i-bias -248.318787 cost 248.405624, z</em> b 248.405624</p>\n<p>However if I donot use the bias the results looks good, what maybe the problems and I wonder the reasons very much.</p>\n<p>Can anynone help me, Thank you!</p>", "body_text": "I tried the LR for sparse dataset using the tensorflow package(0.7.0)\nThe following is part of my procedure:\nweight_values=generateWeight([trainset.feature_num,1],name='weight')\nbias=init_bias([1,1],name='bias')\nsp_shape=tf.placeholder(tf.int64)\nsp_indices=tf.placeholder(tf.int64)\nsp_ids_value=tf.placeholder(tf.int64)\nsp_features_value=tf.placeholder(tf.float32)\nY=tf.placeholder('float',name='Y')\nsp_ids=tf.SparseTensor(sp_indices,sp_ids_value,sp_shape)\nsp_values=tf.SparseTensor(sp_indices,sp_features_value,sp_shape)\n#Z=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values);\nZ_b=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values)+bias\npredict_op=tf.sigmoid(Z_b,name='result')\ncost=tf.nn.sigmoid_cross_entropy_with_logits(Z,Y)\ncost=tf.nn.sigmoid_cross_entropy_with_logits(Z_b,Y)\ntrain_op=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n...\nbut I find the results is  wired when using the bias,using the bias I find the abs(cost) became larger and larger and even the abs(bias) also became larger and larger.\nIN the early iterations the results like this :\n1. labels[-1], j-sample 1662, i-bias 0.100000 cost 0.761811, z_b 0.045602\n2 labels[1], j-sample 823, i-bias 0.084886 cost 0.653277, z_b 0.081396\n3 labels[-1], j-sample 20802, i-bias 0.089683 cost 0.826316, z_b 0.088132\n4 labels[-1], j-sample 25965, i-bias 0.074462 cost 0.806052, z_b 0.074804\n5 labels[1], j-sample 10322, i-bias 0.059276 cost 0.664358, z_b 0.058433\n6 labels[-1], j-sample 23946, i-bias 0.064129 cost 0.795182, z_b 0.067642\n......\nbut with more iteration the results like this :\n270504 labels[-1], j-sample 446, i-bias -248.318787 cost -250.818787, z_b -250.818787\n270505 labels[1], j-sample 10314, i-bias -248.328781 cost 248.306259, z_b -248.306259\n270506 labels[1], j-sample 3820, i-bias -248.318787 cost 247.367340, z_b -247.367340\n270507 labels[1], j-sample 2922, i-bias -248.308792 cost 248.276184, z_b -248.276184\n270508 labels[-1], j-sample 20797, i-bias -248.298798 cost -255.061432, z_b -255.061432\n270509 labels[-1], j-sample 19755, i-bias -248.308792 cost -251.686646, z b -251.686646\n270510 labels[1], j-sample 9528, i-bias -248.318787 cost 248.405624, z b 248.405624\nHowever if I donot use the bias the results looks good, what maybe the problems and I wonder the reasons very much.\nCan anynone help me, Thank you!", "body": "I tried the LR for sparse dataset using the tensorflow package(0.7.0)\nThe following is part of my procedure:\n\nweight_values=generateWeight([trainset.feature_num,1],name='weight')\nbias=init_bias([1,1],name='bias')\nsp_shape=tf.placeholder(tf.int64)\nsp_indices=tf.placeholder(tf.int64)\nsp_ids_value=tf.placeholder(tf.int64)\nsp_features_value=tf.placeholder(tf.float32)\nY=tf.placeholder('float',name='Y')\n\nsp_ids=tf.SparseTensor(sp_indices,sp_ids_value,sp_shape)\nsp_values=tf.SparseTensor(sp_indices,sp_features_value,sp_shape)\n**#Z=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values);\nZ_b=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values)+bias**\npredict_op=tf.sigmoid(Z_b,name='result')\n# cost=tf.nn.sigmoid_cross_entropy_with_logits(Z,Y)\n\ncost=tf.nn.sigmoid_cross_entropy_with_logits(Z_b,Y)\ntrain_op=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n...\n**but I find the results is  wired when using the bias,using the bias I find the abs(cost) became larger and larger and even the abs(bias) also became larger and larger.** \nIN the early iterations the results like this :\n      1. labels[-1], j-sample 1662, i-bias 0.100000 cost 0.761811, z_b 0.045602\n      2 labels[1], j-sample 823, i-bias 0.084886 cost 0.653277, z_b 0.081396\n      3 labels[-1], j-sample 20802, i-bias 0.089683 cost 0.826316, z_b 0.088132\n      4 labels[-1], j-sample 25965, i-bias 0.074462 cost 0.806052, z_b 0.074804\n      5 labels[1], j-sample 10322, i-bias 0.059276 cost 0.664358, z_b 0.058433\n      6 labels[-1], j-sample 23946, i-bias 0.064129 cost 0.795182, z_b 0.067642\n      ......\nbut with more iteration the results like this :\n 270504 labels[-1], j-sample 446, i-bias -248.318787 cost -250.818787, z_b -250.818787\n 270505 labels[1], j-sample 10314, i-bias -248.328781 cost 248.306259, z_b -248.306259\n 270506 labels[1], j-sample 3820, i-bias -248.318787 cost 247.367340, z_b -247.367340\n 270507 labels[1], j-sample 2922, i-bias -248.308792 cost 248.276184, z_b -248.276184\n 270508 labels[-1], j-sample 20797, i-bias -248.298798 cost -255.061432, z_b -255.061432\n 270509 labels[-1], j-sample 19755, i-bias -248.308792 cost -251.686646, z _b -251.686646\n 270510 labels[1], j-sample 9528, i-bias -248.318787 cost 248.405624, z_ b 248.405624\n\nHowever if I donot use the bias the results looks good, what maybe the problems and I wonder the reasons very much.\n\nCan anynone help me, Thank you!\n"}