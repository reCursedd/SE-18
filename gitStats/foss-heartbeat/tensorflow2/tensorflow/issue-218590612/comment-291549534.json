{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/291549534", "html_url": "https://github.com/tensorflow/tensorflow/issues/8879#issuecomment-291549534", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8879", "id": 291549534, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MTU0OTUzNA==", "user": {"login": "zenvendof", "id": 902416, "node_id": "MDQ6VXNlcjkwMjQxNg==", "avatar_url": "https://avatars2.githubusercontent.com/u/902416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zenvendof", "html_url": "https://github.com/zenvendof", "followers_url": "https://api.github.com/users/zenvendof/followers", "following_url": "https://api.github.com/users/zenvendof/following{/other_user}", "gists_url": "https://api.github.com/users/zenvendof/gists{/gist_id}", "starred_url": "https://api.github.com/users/zenvendof/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zenvendof/subscriptions", "organizations_url": "https://api.github.com/users/zenvendof/orgs", "repos_url": "https://api.github.com/users/zenvendof/repos", "events_url": "https://api.github.com/users/zenvendof/events{/privacy}", "received_events_url": "https://api.github.com/users/zenvendof/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-04T16:08:46Z", "updated_at": "2017-04-04T16:08:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13936438\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/crack00ns\">@crack00ns</a> I just encountered this when I tried to free up the GT 750m by taking out the external monitor, which switches the display to Iris Pro.  Running the imagenet tutorial it crashes out like you described:</p>\n<blockquote>\n<p>Total memory: 2.00GiB<br>\nFree memory: 1.72GiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)<br>\nW tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().<br>\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR<br>\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM<br>\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream-&gt;parent()-&gt;GetConvolveAlgorithms(&amp;algorithms)</p>\n</blockquote>\n<p>However, as soon as I enabled Discrete graphics again by using the external monitor it <strong>started to work again</strong>.  So I suggest you <strong>use gfxCardStatus to switch on the Nvidia GPU</strong> before you run the code.</p>\n<p>Given that from 1.1 onwards MAC GPU will be unsupported, I will move away to a Linux desktop with a GTX 1080 Ti soon....</p>", "body_text": "@crack00ns I just encountered this when I tried to free up the GT 750m by taking out the external monitor, which switches the display to Iris Pro.  Running the imagenet tutorial it crashes out like you described:\n\nTotal memory: 2.00GiB\nFree memory: 1.72GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\nW tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\n\nHowever, as soon as I enabled Discrete graphics again by using the external monitor it started to work again.  So I suggest you use gfxCardStatus to switch on the Nvidia GPU before you run the code.\nGiven that from 1.1 onwards MAC GPU will be unsupported, I will move away to a Linux desktop with a GTX 1080 Ti soon....", "body": "@crack00ns I just encountered this when I tried to free up the GT 750m by taking out the external monitor, which switches the display to Iris Pro.  Running the imagenet tutorial it crashes out like you described:\r\n\r\n> Total memory: 2.00GiB\r\n> Free memory: 1.72GiB\r\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\n> W tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\r\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n> F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n\r\nHowever, as soon as I enabled Discrete graphics again by using the external monitor it **started to work again**.  So I suggest you **use gfxCardStatus to switch on the Nvidia GPU** before you run the code. \r\n\r\nGiven that from 1.1 onwards MAC GPU will be unsupported, I will move away to a Linux desktop with a GTX 1080 Ti soon...."}