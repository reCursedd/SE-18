{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15732", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15732/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15732/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15732/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15732", "id": 285202754, "node_id": "MDU6SXNzdWUyODUyMDI3NTQ=", "number": 15732, "title": "Missing OpKernel when using selective registration header", "user": {"login": "xumengwei", "id": 23546158, "node_id": "MDQ6VXNlcjIzNTQ2MTU4", "avatar_url": "https://avatars3.githubusercontent.com/u/23546158?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xumengwei", "html_url": "https://github.com/xumengwei", "followers_url": "https://api.github.com/users/xumengwei/followers", "following_url": "https://api.github.com/users/xumengwei/following{/other_user}", "gists_url": "https://api.github.com/users/xumengwei/gists{/gist_id}", "starred_url": "https://api.github.com/users/xumengwei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xumengwei/subscriptions", "organizations_url": "https://api.github.com/users/xumengwei/orgs", "repos_url": "https://api.github.com/users/xumengwei/repos", "events_url": "https://api.github.com/users/xumengwei/events{/privacy}", "received_events_url": "https://api.github.com/users/xumengwei/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 20, "created_at": "2017-12-30T13:27:02Z", "updated_at": "2018-07-20T01:16:17Z", "closed_at": "2018-07-20T01:12:53Z", "author_association": "NONE", "body_html": "<p>I use the <code>bazel-bin/tensorflow/python/tools/print_selective_registration_header</code> to find out all necessary ops and kernels for my <code>test.pb</code>, and then compile them into a android executable</p>\n<p><code>bazel build test/test:test_run --copt=-DSELECTIVE_REGISTRATION  --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a</code></p>\n<p>The <code>ops_to_register.h</code> is as follows</p>\n<pre><code>// This file was autogenerated by print_selective_registration_header.py\n#ifndef OPS_TO_REGISTER\n#define OPS_TO_REGISTER\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\n  return false\n     || (strcmp(op, \"Add\") == 0)\n     || (strcmp(op, \"AddN\") == 0)\n     || (strcmp(op, \"ApplyGradientDescent\") == 0)\n     || (strcmp(op, \"Assign\") == 0)\n     || (strcmp(op, \"AssignAdd\") == 0)\n     || (strcmp(op, \"BiasAdd\") == 0)\n     || (strcmp(op, \"BiasAddGrad\") == 0)\n     || (strcmp(op, \"BroadcastGradientArgs\") == 0)\n     || (strcmp(op, \"ConcatOffset\") == 0)\n     || (strcmp(op, \"ConcatV2\") == 0)\n     || (strcmp(op, \"Const\") == 0)\n     || (strcmp(op, \"ExpandDims\") == 0)\n     || (strcmp(op, \"Fill\") == 0)\n     || (strcmp(op, \"Floor\") == 0)\n     || (strcmp(op, \"FloorMod\") == 0)\n     || (strcmp(op, \"Gather\") == 0)\n     || (strcmp(op, \"Identity\") == 0)\n     || (strcmp(op, \"L2Loss\") == 0)\n     || (strcmp(op, \"MatMul\") == 0)\n     || (strcmp(op, \"Minimum\") == 0)\n     || (strcmp(op, \"Mul\") == 0)\n     || (strcmp(op, \"Neg\") == 0)\n     || (strcmp(op, \"NoOp\") == 0)\n     || (strcmp(op, \"Pack\") == 0)\n     || (strcmp(op, \"Placeholder\") == 0)\n     || (strcmp(op, \"PlaceholderWithDefault\") == 0)\n     || (strcmp(op, \"PreventGradient\") == 0)\n     || (strcmp(op, \"RandomUniform\") == 0)\n     || (strcmp(op, \"RealDiv\") == 0)\n     || (strcmp(op, \"Reshape\") == 0)\n     || (strcmp(op, \"ScatterSub\") == 0)\n     || (strcmp(op, \"Shape\") == 0)\n     || (strcmp(op, \"Sigmoid\") == 0)\n     || (strcmp(op, \"SigmoidGrad\") == 0)\n     || (strcmp(op, \"Size\") == 0)\n     || (strcmp(op, \"Slice\") == 0)\n     || (strcmp(op, \"Softmax\") == 0)\n     || (strcmp(op, \"SparseSoftmaxCrossEntropyWithLogits\") == 0)\n     || (strcmp(op, \"Split\") == 0)\n     || (strcmp(op, \"SplitV\") == 0)\n     || (strcmp(op, \"Sqrt\") == 0)\n     || (strcmp(op, \"Squeeze\") == 0)\n     || (strcmp(op, \"StridedSlice\") == 0)\n     || (strcmp(op, \"Sub\") == 0)\n     || (strcmp(op, \"Sum\") == 0)\n     || (strcmp(op, \"Tanh\") == 0)\n     || (strcmp(op, \"TanhGrad\") == 0)\n     || (strcmp(op, \"Tile\") == 0)\n     || (strcmp(op, \"TopKV2\") == 0)\n     || (strcmp(op, \"Unpack\") == 0)\n     || (strcmp(op, \"VariableV2\") == 0)\n     || (strcmp(op, \"ZerosLike\") == 0)\n     || (strcmp(op, \"_Recv\") == 0)\n     || (strcmp(op, \"_Send\") == 0)\n  ;\n}\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\n\n\n    namespace {\n      constexpr const char* skip(const char* x) {\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\n      }\n\n      constexpr bool isequal(const char* x, const char* y) {\n        return (*skip(x) &amp;&amp; *skip(y))\n                   ? (*skip(x) == *skip(y) &amp;&amp; isequal(skip(x) + 1, skip(y) + 1))\n                   : (!*skip(x) &amp;&amp; !*skip(y));\n      }\n\n      template&lt;int N&gt;\n      struct find_in {\n        static constexpr bool f(const char* x, const char* const y[N]) {\n          return isequal(x, y[0]) || find_in&lt;N - 1&gt;::f(x, y + 1);\n        }\n      };\n\n      template&lt;&gt;\n      struct find_in&lt;0&gt; {\n        static constexpr bool f(const char* x, const char* const y[]) {\n          return false;\n        }\n      };\n    }  // end namespace\n    constexpr const char* kNecessaryOpKernelClasses[] = {\n\"BinaryOp&lt; CPUDevice, functor::add&lt;float&gt;&gt;\",\n\"AddNOp&lt; CPUDevice, float&gt;\",\n\"ApplyGradientDescentOp&lt;CPUDevice, float&gt;\",\n\"AssignOpT&lt;CPUDevice, ::tensorflow::int64&gt;\",\n\"AssignOpT&lt;CPUDevice, float&gt;\",\n\"DenseUpdateOp&lt;CPUDevice, ::tensorflow::int64, DenseUpdateType::ADD&gt;\",\n\"BiasOp&lt;CPUDevice, float&gt;\",\n\"BiasGradOp&lt;CPUDevice, float&gt;\",\n\"BCastGradArgsOp\",\n\"ConcatOffsetOp\",\n\"ConcatV2Op&lt;CPUDevice, ::tensorflow::int32&gt;\",\n\"ConcatV2Op&lt;CPUDevice, float&gt;\",\n\"ConstantOp\",\n\"ExpandDimsOp\",\n\"FillOp&lt;CPUDevice, float&gt;\",\n\"UnaryOp&lt; CPUDevice, functor::floor&lt;float&gt;&gt;\",\n\"BinaryOp&lt; CPUDevice, functor::safe_floor_mod&lt;int32&gt;&gt;\",\n\"GatherOp&lt;CPUDevice, float, int32&gt;\",\n\"IdentityOp\",\n\"L2LossOp&lt;CPUDevice, float&gt;\",\n\"MatMulOp&lt;CPUDevice, float, false &gt;\",\n\"BinaryOp&lt; CPUDevice, functor::minimum&lt;float&gt;&gt;\",\n\"BinaryOp&lt; CPUDevice, functor::mul&lt;float&gt;&gt;\",\n\"UnaryOp&lt; CPUDevice, functor::neg&lt;float&gt;&gt;\",\n\"NoOp\",\n\"PackOp&lt;CPUDevice, ::tensorflow::int32&gt;\",\n\"PackOp&lt;CPUDevice, float&gt;\",\n\"PlaceholderOp\",\n\"IdentityOp\",\n\"IdentityOp\",\n\"PhiloxRandomOp&lt;CPUDevice, random::UniformDistribution&lt; random::PhiloxRandom, float&gt; &gt;\",\n\"BinaryOp&lt; CPUDevice, functor::div&lt;float&gt;&gt;\",\n\"ReshapeOp\",\n\"ScatterUpdateOp&lt; CPUDevice, float, int32, scatter_op::UpdateOp::SUB&gt;\",\n\"ShapeOp&lt;int32&gt;\",\n\"UnaryOp&lt; CPUDevice, functor::sigmoid&lt;float&gt;&gt;\",\n\"SimpleBinaryOp&lt; CPUDevice, functor::sigmoid_grad&lt;float&gt;&gt;\",\n\"SizeOp&lt;int32&gt;\",\n\"SliceOp&lt;CPUDevice, ::tensorflow::int32&gt;\",\n\"SliceOp&lt;CPUDevice, float&gt;\",\n\"SoftmaxOp&lt;CPUDevice, float&gt;\",\n\"SparseSoftmaxXentWithLogitsOp&lt;CPUDevice, float, int32&gt;\",\n\"SplitOpCPU&lt;float&gt;\",\n\"SplitVOpCPU&lt;float, int32&gt;\",\n\"UnaryOp&lt; CPUDevice, functor::sqrt&lt;float&gt;&gt;\",\n\"SqueezeOp\",\n\"StridedSliceOp&lt;CPUDevice, ::tensorflow::int32&gt;\",\n\"StridedSliceOp&lt;CPUDevice, float&gt;\",\n\"BinaryOp&lt; CPUDevice, functor::sub&lt;float&gt;&gt;\",\n\"ReductionOp&lt;CPUDevice, float, Eigen::internal::SumReducer&lt;float&gt;&gt;\",\n\"UnaryOp&lt; CPUDevice, functor::tanh&lt;float&gt;&gt;\",\n\"SimpleBinaryOp&lt; CPUDevice, functor::tanh_grad&lt;float&gt;&gt;\",\n\"TileOp&lt;CPUDevice&gt;\",\n\"TopK&lt;float&gt;\",\n\"UnpackOp&lt;CPUDevice, float&gt;\",\n\"VariableOp\",\n\"ZerosLikeOp&lt; CPUDevice, float&gt;\",\n\"RecvOp\",\n\"SendOp\",\n};\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in&lt;sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)&gt;::f(clz, kNecessaryOpKernelClasses))\n\n#define SHOULD_REGISTER_OP_GRADIENT false\n#endif\n</code></pre>\n<p>However, when I try to run the executable on my Android device, it says some kernels are missed.</p>\n<pre><code>Error creating graph: Invalid argument: No OpKernel was registered to support Op 'Placeholder' with these attrs.  Registered devices: [CPU], Registered kernels:\n  &lt;no registered kernels&gt;\n\n\t [[Node: OnlineTraining/Model/Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[2000,400]]()]]\n</code></pre>\n<p>How can I generate a complete file for this? If necessary I can also upload the <code>test.pb</code> file for testing.</p>", "body_text": "I use the bazel-bin/tensorflow/python/tools/print_selective_registration_header to find out all necessary ops and kernels for my test.pb, and then compile them into a android executable\nbazel build test/test:test_run --copt=-DSELECTIVE_REGISTRATION  --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\nThe ops_to_register.h is as follows\n// This file was autogenerated by print_selective_registration_header.py\n#ifndef OPS_TO_REGISTER\n#define OPS_TO_REGISTER\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\n  return false\n     || (strcmp(op, \"Add\") == 0)\n     || (strcmp(op, \"AddN\") == 0)\n     || (strcmp(op, \"ApplyGradientDescent\") == 0)\n     || (strcmp(op, \"Assign\") == 0)\n     || (strcmp(op, \"AssignAdd\") == 0)\n     || (strcmp(op, \"BiasAdd\") == 0)\n     || (strcmp(op, \"BiasAddGrad\") == 0)\n     || (strcmp(op, \"BroadcastGradientArgs\") == 0)\n     || (strcmp(op, \"ConcatOffset\") == 0)\n     || (strcmp(op, \"ConcatV2\") == 0)\n     || (strcmp(op, \"Const\") == 0)\n     || (strcmp(op, \"ExpandDims\") == 0)\n     || (strcmp(op, \"Fill\") == 0)\n     || (strcmp(op, \"Floor\") == 0)\n     || (strcmp(op, \"FloorMod\") == 0)\n     || (strcmp(op, \"Gather\") == 0)\n     || (strcmp(op, \"Identity\") == 0)\n     || (strcmp(op, \"L2Loss\") == 0)\n     || (strcmp(op, \"MatMul\") == 0)\n     || (strcmp(op, \"Minimum\") == 0)\n     || (strcmp(op, \"Mul\") == 0)\n     || (strcmp(op, \"Neg\") == 0)\n     || (strcmp(op, \"NoOp\") == 0)\n     || (strcmp(op, \"Pack\") == 0)\n     || (strcmp(op, \"Placeholder\") == 0)\n     || (strcmp(op, \"PlaceholderWithDefault\") == 0)\n     || (strcmp(op, \"PreventGradient\") == 0)\n     || (strcmp(op, \"RandomUniform\") == 0)\n     || (strcmp(op, \"RealDiv\") == 0)\n     || (strcmp(op, \"Reshape\") == 0)\n     || (strcmp(op, \"ScatterSub\") == 0)\n     || (strcmp(op, \"Shape\") == 0)\n     || (strcmp(op, \"Sigmoid\") == 0)\n     || (strcmp(op, \"SigmoidGrad\") == 0)\n     || (strcmp(op, \"Size\") == 0)\n     || (strcmp(op, \"Slice\") == 0)\n     || (strcmp(op, \"Softmax\") == 0)\n     || (strcmp(op, \"SparseSoftmaxCrossEntropyWithLogits\") == 0)\n     || (strcmp(op, \"Split\") == 0)\n     || (strcmp(op, \"SplitV\") == 0)\n     || (strcmp(op, \"Sqrt\") == 0)\n     || (strcmp(op, \"Squeeze\") == 0)\n     || (strcmp(op, \"StridedSlice\") == 0)\n     || (strcmp(op, \"Sub\") == 0)\n     || (strcmp(op, \"Sum\") == 0)\n     || (strcmp(op, \"Tanh\") == 0)\n     || (strcmp(op, \"TanhGrad\") == 0)\n     || (strcmp(op, \"Tile\") == 0)\n     || (strcmp(op, \"TopKV2\") == 0)\n     || (strcmp(op, \"Unpack\") == 0)\n     || (strcmp(op, \"VariableV2\") == 0)\n     || (strcmp(op, \"ZerosLike\") == 0)\n     || (strcmp(op, \"_Recv\") == 0)\n     || (strcmp(op, \"_Send\") == 0)\n  ;\n}\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\n\n\n    namespace {\n      constexpr const char* skip(const char* x) {\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\n      }\n\n      constexpr bool isequal(const char* x, const char* y) {\n        return (*skip(x) && *skip(y))\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\n                   : (!*skip(x) && !*skip(y));\n      }\n\n      template<int N>\n      struct find_in {\n        static constexpr bool f(const char* x, const char* const y[N]) {\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\n        }\n      };\n\n      template<>\n      struct find_in<0> {\n        static constexpr bool f(const char* x, const char* const y[]) {\n          return false;\n        }\n      };\n    }  // end namespace\n    constexpr const char* kNecessaryOpKernelClasses[] = {\n\"BinaryOp< CPUDevice, functor::add<float>>\",\n\"AddNOp< CPUDevice, float>\",\n\"ApplyGradientDescentOp<CPUDevice, float>\",\n\"AssignOpT<CPUDevice, ::tensorflow::int64>\",\n\"AssignOpT<CPUDevice, float>\",\n\"DenseUpdateOp<CPUDevice, ::tensorflow::int64, DenseUpdateType::ADD>\",\n\"BiasOp<CPUDevice, float>\",\n\"BiasGradOp<CPUDevice, float>\",\n\"BCastGradArgsOp\",\n\"ConcatOffsetOp\",\n\"ConcatV2Op<CPUDevice, ::tensorflow::int32>\",\n\"ConcatV2Op<CPUDevice, float>\",\n\"ConstantOp\",\n\"ExpandDimsOp\",\n\"FillOp<CPUDevice, float>\",\n\"UnaryOp< CPUDevice, functor::floor<float>>\",\n\"BinaryOp< CPUDevice, functor::safe_floor_mod<int32>>\",\n\"GatherOp<CPUDevice, float, int32>\",\n\"IdentityOp\",\n\"L2LossOp<CPUDevice, float>\",\n\"MatMulOp<CPUDevice, float, false >\",\n\"BinaryOp< CPUDevice, functor::minimum<float>>\",\n\"BinaryOp< CPUDevice, functor::mul<float>>\",\n\"UnaryOp< CPUDevice, functor::neg<float>>\",\n\"NoOp\",\n\"PackOp<CPUDevice, ::tensorflow::int32>\",\n\"PackOp<CPUDevice, float>\",\n\"PlaceholderOp\",\n\"IdentityOp\",\n\"IdentityOp\",\n\"PhiloxRandomOp<CPUDevice, random::UniformDistribution< random::PhiloxRandom, float> >\",\n\"BinaryOp< CPUDevice, functor::div<float>>\",\n\"ReshapeOp\",\n\"ScatterUpdateOp< CPUDevice, float, int32, scatter_op::UpdateOp::SUB>\",\n\"ShapeOp<int32>\",\n\"UnaryOp< CPUDevice, functor::sigmoid<float>>\",\n\"SimpleBinaryOp< CPUDevice, functor::sigmoid_grad<float>>\",\n\"SizeOp<int32>\",\n\"SliceOp<CPUDevice, ::tensorflow::int32>\",\n\"SliceOp<CPUDevice, float>\",\n\"SoftmaxOp<CPUDevice, float>\",\n\"SparseSoftmaxXentWithLogitsOp<CPUDevice, float, int32>\",\n\"SplitOpCPU<float>\",\n\"SplitVOpCPU<float, int32>\",\n\"UnaryOp< CPUDevice, functor::sqrt<float>>\",\n\"SqueezeOp\",\n\"StridedSliceOp<CPUDevice, ::tensorflow::int32>\",\n\"StridedSliceOp<CPUDevice, float>\",\n\"BinaryOp< CPUDevice, functor::sub<float>>\",\n\"ReductionOp<CPUDevice, float, Eigen::internal::SumReducer<float>>\",\n\"UnaryOp< CPUDevice, functor::tanh<float>>\",\n\"SimpleBinaryOp< CPUDevice, functor::tanh_grad<float>>\",\n\"TileOp<CPUDevice>\",\n\"TopK<float>\",\n\"UnpackOp<CPUDevice, float>\",\n\"VariableOp\",\n\"ZerosLikeOp< CPUDevice, float>\",\n\"RecvOp\",\n\"SendOp\",\n};\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\n\n#define SHOULD_REGISTER_OP_GRADIENT false\n#endif\n\nHowever, when I try to run the executable on my Android device, it says some kernels are missed.\nError creating graph: Invalid argument: No OpKernel was registered to support Op 'Placeholder' with these attrs.  Registered devices: [CPU], Registered kernels:\n  <no registered kernels>\n\n\t [[Node: OnlineTraining/Model/Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[2000,400]]()]]\n\nHow can I generate a complete file for this? If necessary I can also upload the test.pb file for testing.", "body": "I use the `bazel-bin/tensorflow/python/tools/print_selective_registration_header` to find out all necessary ops and kernels for my `test.pb`, and then compile them into a android executable\r\n\r\n`bazel build test/test:test_run --copt=-DSELECTIVE_REGISTRATION  --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a`\r\n\r\nThe `ops_to_register.h` is as follows\r\n\r\n```\r\n// This file was autogenerated by print_selective_registration_header.py\r\n#ifndef OPS_TO_REGISTER\r\n#define OPS_TO_REGISTER\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || (strcmp(op, \"Add\") == 0)\r\n     || (strcmp(op, \"AddN\") == 0)\r\n     || (strcmp(op, \"ApplyGradientDescent\") == 0)\r\n     || (strcmp(op, \"Assign\") == 0)\r\n     || (strcmp(op, \"AssignAdd\") == 0)\r\n     || (strcmp(op, \"BiasAdd\") == 0)\r\n     || (strcmp(op, \"BiasAddGrad\") == 0)\r\n     || (strcmp(op, \"BroadcastGradientArgs\") == 0)\r\n     || (strcmp(op, \"ConcatOffset\") == 0)\r\n     || (strcmp(op, \"ConcatV2\") == 0)\r\n     || (strcmp(op, \"Const\") == 0)\r\n     || (strcmp(op, \"ExpandDims\") == 0)\r\n     || (strcmp(op, \"Fill\") == 0)\r\n     || (strcmp(op, \"Floor\") == 0)\r\n     || (strcmp(op, \"FloorMod\") == 0)\r\n     || (strcmp(op, \"Gather\") == 0)\r\n     || (strcmp(op, \"Identity\") == 0)\r\n     || (strcmp(op, \"L2Loss\") == 0)\r\n     || (strcmp(op, \"MatMul\") == 0)\r\n     || (strcmp(op, \"Minimum\") == 0)\r\n     || (strcmp(op, \"Mul\") == 0)\r\n     || (strcmp(op, \"Neg\") == 0)\r\n     || (strcmp(op, \"NoOp\") == 0)\r\n     || (strcmp(op, \"Pack\") == 0)\r\n     || (strcmp(op, \"Placeholder\") == 0)\r\n     || (strcmp(op, \"PlaceholderWithDefault\") == 0)\r\n     || (strcmp(op, \"PreventGradient\") == 0)\r\n     || (strcmp(op, \"RandomUniform\") == 0)\r\n     || (strcmp(op, \"RealDiv\") == 0)\r\n     || (strcmp(op, \"Reshape\") == 0)\r\n     || (strcmp(op, \"ScatterSub\") == 0)\r\n     || (strcmp(op, \"Shape\") == 0)\r\n     || (strcmp(op, \"Sigmoid\") == 0)\r\n     || (strcmp(op, \"SigmoidGrad\") == 0)\r\n     || (strcmp(op, \"Size\") == 0)\r\n     || (strcmp(op, \"Slice\") == 0)\r\n     || (strcmp(op, \"Softmax\") == 0)\r\n     || (strcmp(op, \"SparseSoftmaxCrossEntropyWithLogits\") == 0)\r\n     || (strcmp(op, \"Split\") == 0)\r\n     || (strcmp(op, \"SplitV\") == 0)\r\n     || (strcmp(op, \"Sqrt\") == 0)\r\n     || (strcmp(op, \"Squeeze\") == 0)\r\n     || (strcmp(op, \"StridedSlice\") == 0)\r\n     || (strcmp(op, \"Sub\") == 0)\r\n     || (strcmp(op, \"Sum\") == 0)\r\n     || (strcmp(op, \"Tanh\") == 0)\r\n     || (strcmp(op, \"TanhGrad\") == 0)\r\n     || (strcmp(op, \"Tile\") == 0)\r\n     || (strcmp(op, \"TopKV2\") == 0)\r\n     || (strcmp(op, \"Unpack\") == 0)\r\n     || (strcmp(op, \"VariableV2\") == 0)\r\n     || (strcmp(op, \"ZerosLike\") == 0)\r\n     || (strcmp(op, \"_Recv\") == 0)\r\n     || (strcmp(op, \"_Send\") == 0)\r\n  ;\r\n}\r\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\r\n\r\n\r\n    namespace {\r\n      constexpr const char* skip(const char* x) {\r\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\r\n      }\r\n\r\n      constexpr bool isequal(const char* x, const char* y) {\r\n        return (*skip(x) && *skip(y))\r\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\r\n                   : (!*skip(x) && !*skip(y));\r\n      }\r\n\r\n      template<int N>\r\n      struct find_in {\r\n        static constexpr bool f(const char* x, const char* const y[N]) {\r\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\r\n        }\r\n      };\r\n\r\n      template<>\r\n      struct find_in<0> {\r\n        static constexpr bool f(const char* x, const char* const y[]) {\r\n          return false;\r\n        }\r\n      };\r\n    }  // end namespace\r\n    constexpr const char* kNecessaryOpKernelClasses[] = {\r\n\"BinaryOp< CPUDevice, functor::add<float>>\",\r\n\"AddNOp< CPUDevice, float>\",\r\n\"ApplyGradientDescentOp<CPUDevice, float>\",\r\n\"AssignOpT<CPUDevice, ::tensorflow::int64>\",\r\n\"AssignOpT<CPUDevice, float>\",\r\n\"DenseUpdateOp<CPUDevice, ::tensorflow::int64, DenseUpdateType::ADD>\",\r\n\"BiasOp<CPUDevice, float>\",\r\n\"BiasGradOp<CPUDevice, float>\",\r\n\"BCastGradArgsOp\",\r\n\"ConcatOffsetOp\",\r\n\"ConcatV2Op<CPUDevice, ::tensorflow::int32>\",\r\n\"ConcatV2Op<CPUDevice, float>\",\r\n\"ConstantOp\",\r\n\"ExpandDimsOp\",\r\n\"FillOp<CPUDevice, float>\",\r\n\"UnaryOp< CPUDevice, functor::floor<float>>\",\r\n\"BinaryOp< CPUDevice, functor::safe_floor_mod<int32>>\",\r\n\"GatherOp<CPUDevice, float, int32>\",\r\n\"IdentityOp\",\r\n\"L2LossOp<CPUDevice, float>\",\r\n\"MatMulOp<CPUDevice, float, false >\",\r\n\"BinaryOp< CPUDevice, functor::minimum<float>>\",\r\n\"BinaryOp< CPUDevice, functor::mul<float>>\",\r\n\"UnaryOp< CPUDevice, functor::neg<float>>\",\r\n\"NoOp\",\r\n\"PackOp<CPUDevice, ::tensorflow::int32>\",\r\n\"PackOp<CPUDevice, float>\",\r\n\"PlaceholderOp\",\r\n\"IdentityOp\",\r\n\"IdentityOp\",\r\n\"PhiloxRandomOp<CPUDevice, random::UniformDistribution< random::PhiloxRandom, float> >\",\r\n\"BinaryOp< CPUDevice, functor::div<float>>\",\r\n\"ReshapeOp\",\r\n\"ScatterUpdateOp< CPUDevice, float, int32, scatter_op::UpdateOp::SUB>\",\r\n\"ShapeOp<int32>\",\r\n\"UnaryOp< CPUDevice, functor::sigmoid<float>>\",\r\n\"SimpleBinaryOp< CPUDevice, functor::sigmoid_grad<float>>\",\r\n\"SizeOp<int32>\",\r\n\"SliceOp<CPUDevice, ::tensorflow::int32>\",\r\n\"SliceOp<CPUDevice, float>\",\r\n\"SoftmaxOp<CPUDevice, float>\",\r\n\"SparseSoftmaxXentWithLogitsOp<CPUDevice, float, int32>\",\r\n\"SplitOpCPU<float>\",\r\n\"SplitVOpCPU<float, int32>\",\r\n\"UnaryOp< CPUDevice, functor::sqrt<float>>\",\r\n\"SqueezeOp\",\r\n\"StridedSliceOp<CPUDevice, ::tensorflow::int32>\",\r\n\"StridedSliceOp<CPUDevice, float>\",\r\n\"BinaryOp< CPUDevice, functor::sub<float>>\",\r\n\"ReductionOp<CPUDevice, float, Eigen::internal::SumReducer<float>>\",\r\n\"UnaryOp< CPUDevice, functor::tanh<float>>\",\r\n\"SimpleBinaryOp< CPUDevice, functor::tanh_grad<float>>\",\r\n\"TileOp<CPUDevice>\",\r\n\"TopK<float>\",\r\n\"UnpackOp<CPUDevice, float>\",\r\n\"VariableOp\",\r\n\"ZerosLikeOp< CPUDevice, float>\",\r\n\"RecvOp\",\r\n\"SendOp\",\r\n};\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\n#define SHOULD_REGISTER_OP_GRADIENT false\r\n#endif\r\n```\r\n\r\nHowever, when I try to run the executable on my Android device, it says some kernels are missed.\r\n\r\n```\r\nError creating graph: Invalid argument: No OpKernel was registered to support Op 'Placeholder' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: OnlineTraining/Model/Placeholder_1 = Placeholder[dtype=DT_FLOAT, shape=[2000,400]]()]]\r\n```\r\n\r\nHow can I generate a complete file for this? If necessary I can also upload the `test.pb` file for testing."}