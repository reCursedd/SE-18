{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316139658", "html_url": "https://github.com/tensorflow/tensorflow/issues/11416#issuecomment-316139658", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11416", "id": 316139658, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjEzOTY1OA==", "user": {"login": "bobzhuyb", "id": 1696340, "node_id": "MDQ6VXNlcjE2OTYzNDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1696340?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bobzhuyb", "html_url": "https://github.com/bobzhuyb", "followers_url": "https://api.github.com/users/bobzhuyb/followers", "following_url": "https://api.github.com/users/bobzhuyb/following{/other_user}", "gists_url": "https://api.github.com/users/bobzhuyb/gists{/gist_id}", "starred_url": "https://api.github.com/users/bobzhuyb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bobzhuyb/subscriptions", "organizations_url": "https://api.github.com/users/bobzhuyb/orgs", "repos_url": "https://api.github.com/users/bobzhuyb/repos", "events_url": "https://api.github.com/users/bobzhuyb/events{/privacy}", "received_events_url": "https://api.github.com/users/bobzhuyb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-18T17:38:10Z", "updated_at": "2017-07-18T17:57:49Z", "author_association": "NONE", "body_html": "<p>Hello <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22274255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shamoya\">@shamoya</a>, I am working with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5404419\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/on-the-run\">@on-the-run</a> on this issue. One important fact we observed is that with only one GPU per server, it will run without a problem. However, with increasing number of GPUs per server, the more likely it will get stuck. With eight GPU, it rarely runs.</p>\n<p>Since our logs show that 1. the (virtual) memory addresses are correctly mapped and exchanged, 2. the NIC reports RDMA WRITE is successful, 3. userspace sees the target buffer remains unchanged (we try initializing rx_message_buffer with 0 or different values, any initial values will remain unchanged), we suspect that the problem is because the mapping from virtual addr to physical addr has been changed.</p>\n<p>We suspect that this is related to CUDA 8 and Pascal GPU, with which Nvidia has a new memory management feature (<a href=\"https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/\" rel=\"nofollow\">https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/</a>)</p>\n<p>This is what may have happened in sequence (our speculation):</p>\n<ol>\n<li>ibv_reg_mr --&gt; NIC remembers the physical addr</li>\n<li>CUDA maps the eight GPUs' memory to virtual addr. Something for some reason changes the userspace virtual addr to physical addr mapping. This thing, OS or CUDA, moves the contents and thinks the tensorflow process won't notice this change.</li>\n<li>RDMA transmission starts, NIC thinks it has written to the physical addr and reports success.</li>\n<li>However, when the tensorflow process checks the memory contents, the process is actually looking at a different physical addr due to step 2.</li>\n</ol>\n<p>Our current workaround is to call mlock() right after <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L561\">malloc()</a>.  It tells the OS to pin the physical memory. We have run our program successfully for a few times since the change. We'll continue checking whether it really solves the problem.</p>\n<p>I am not sure which is the right side to blame -- we don't expect CUDA to change the virtual addr mapping, however it seems that the change does happen, especially when CUDA works aggressively (with eight GPU). We expect ibv_reg_mr to pin the memory, however we searched through the ibv_reg_mr implementation in OFED and didn't see it doing that.</p>", "body_text": "Hello @shamoya, I am working with @on-the-run on this issue. One important fact we observed is that with only one GPU per server, it will run without a problem. However, with increasing number of GPUs per server, the more likely it will get stuck. With eight GPU, it rarely runs.\nSince our logs show that 1. the (virtual) memory addresses are correctly mapped and exchanged, 2. the NIC reports RDMA WRITE is successful, 3. userspace sees the target buffer remains unchanged (we try initializing rx_message_buffer with 0 or different values, any initial values will remain unchanged), we suspect that the problem is because the mapping from virtual addr to physical addr has been changed.\nWe suspect that this is related to CUDA 8 and Pascal GPU, with which Nvidia has a new memory management feature (https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/)\nThis is what may have happened in sequence (our speculation):\n\nibv_reg_mr --> NIC remembers the physical addr\nCUDA maps the eight GPUs' memory to virtual addr. Something for some reason changes the userspace virtual addr to physical addr mapping. This thing, OS or CUDA, moves the contents and thinks the tensorflow process won't notice this change.\nRDMA transmission starts, NIC thinks it has written to the physical addr and reports success.\nHowever, when the tensorflow process checks the memory contents, the process is actually looking at a different physical addr due to step 2.\n\nOur current workaround is to call mlock() right after malloc().  It tells the OS to pin the physical memory. We have run our program successfully for a few times since the change. We'll continue checking whether it really solves the problem.\nI am not sure which is the right side to blame -- we don't expect CUDA to change the virtual addr mapping, however it seems that the change does happen, especially when CUDA works aggressively (with eight GPU). We expect ibv_reg_mr to pin the memory, however we searched through the ibv_reg_mr implementation in OFED and didn't see it doing that.", "body": "Hello @shamoya, I am working with @on-the-run on this issue. One important fact we observed is that with only one GPU per server, it will run without a problem. However, with increasing number of GPUs per server, the more likely it will get stuck. With eight GPU, it rarely runs.\r\n\r\nSince our logs show that 1. the (virtual) memory addresses are correctly mapped and exchanged, 2. the NIC reports RDMA WRITE is successful, 3. userspace sees the target buffer remains unchanged (we try initializing rx_message_buffer with 0 or different values, any initial values will remain unchanged), we suspect that the problem is because the mapping from virtual addr to physical addr has been changed.\r\n\r\nWe suspect that this is related to CUDA 8 and Pascal GPU, with which Nvidia has a new memory management feature (https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/)\r\n\r\nThis is what may have happened in sequence (our speculation):\r\n1. ibv_reg_mr --> NIC remembers the physical addr\r\n2. CUDA maps the eight GPUs' memory to virtual addr. Something for some reason changes the userspace virtual addr to physical addr mapping. This thing, OS or CUDA, moves the contents and thinks the tensorflow process won't notice this change.\r\n3. RDMA transmission starts, NIC thinks it has written to the physical addr and reports success.\r\n4. However, when the tensorflow process checks the memory contents, the process is actually looking at a different physical addr due to step 2.\r\n\r\nOur current workaround is to call mlock() right after [malloc()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L561).  It tells the OS to pin the physical memory. We have run our program successfully for a few times since the change. We'll continue checking whether it really solves the problem.\r\n\r\nI am not sure which is the right side to blame -- we don't expect CUDA to change the virtual addr mapping, however it seems that the change does happen, especially when CUDA works aggressively (with eight GPU). We expect ibv_reg_mr to pin the memory, however we searched through the ibv_reg_mr implementation in OFED and didn't see it doing that.\r\n\r\n\r\n"}