{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316196253", "html_url": "https://github.com/tensorflow/tensorflow/issues/11416#issuecomment-316196253", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11416", "id": 316196253, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjE5NjI1Mw==", "user": {"login": "bobzhuyb", "id": 1696340, "node_id": "MDQ6VXNlcjE2OTYzNDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1696340?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bobzhuyb", "html_url": "https://github.com/bobzhuyb", "followers_url": "https://api.github.com/users/bobzhuyb/followers", "following_url": "https://api.github.com/users/bobzhuyb/following{/other_user}", "gists_url": "https://api.github.com/users/bobzhuyb/gists{/gist_id}", "starred_url": "https://api.github.com/users/bobzhuyb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bobzhuyb/subscriptions", "organizations_url": "https://api.github.com/users/bobzhuyb/orgs", "repos_url": "https://api.github.com/users/bobzhuyb/repos", "events_url": "https://api.github.com/users/bobzhuyb/events{/privacy}", "received_events_url": "https://api.github.com/users/bobzhuyb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-18T21:00:23Z", "updated_at": "2017-07-18T21:00:23Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12075848\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/junshi15\">@junshi15</a> I understand that you only allocate main memory. Nevertheless, CUDA starts after your code, and with Pascal GPUs, it maps GPU memory to the same virtual memory address space as the process (see Unified Memory in <a href=\"https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/\" rel=\"nofollow\">https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/</a>). We do not know what it does with the existing virtual addr and physical addr mapping.</p>\n<p>Otherwise, I don't know how to explain why it works with one GPU, while starts to break with more GPUs..</p>", "body_text": "@junshi15 I understand that you only allocate main memory. Nevertheless, CUDA starts after your code, and with Pascal GPUs, it maps GPU memory to the same virtual memory address space as the process (see Unified Memory in https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/). We do not know what it does with the existing virtual addr and physical addr mapping.\nOtherwise, I don't know how to explain why it works with one GPU, while starts to break with more GPUs..", "body": "@junshi15 I understand that you only allocate main memory. Nevertheless, CUDA starts after your code, and with Pascal GPUs, it maps GPU memory to the same virtual memory address space as the process (see Unified Memory in https://devblogs.nvidia.com/parallelforall/cuda-8-features-revealed/). We do not know what it does with the existing virtual addr and physical addr mapping.\r\n\r\nOtherwise, I don't know how to explain why it works with one GPU, while starts to break with more GPUs.."}