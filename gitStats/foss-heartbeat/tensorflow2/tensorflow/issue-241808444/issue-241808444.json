{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11416", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11416/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11416/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11416/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11416", "id": 241808444, "node_id": "MDU6SXNzdWUyNDE4MDg0NDQ=", "number": 11416, "title": "Distributed training with synchronized SGD using 'grpc+verbs' sometimes hangs indefinitely", "user": {"login": "on-the-run", "id": 5404419, "node_id": "MDQ6VXNlcjU0MDQ0MTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/5404419?v=4", "gravatar_id": "", "url": "https://api.github.com/users/on-the-run", "html_url": "https://github.com/on-the-run", "followers_url": "https://api.github.com/users/on-the-run/followers", "following_url": "https://api.github.com/users/on-the-run/following{/other_user}", "gists_url": "https://api.github.com/users/on-the-run/gists{/gist_id}", "starred_url": "https://api.github.com/users/on-the-run/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/on-the-run/subscriptions", "organizations_url": "https://api.github.com/users/on-the-run/orgs", "repos_url": "https://api.github.com/users/on-the-run/repos", "events_url": "https://api.github.com/users/on-the-run/events{/privacy}", "received_events_url": "https://api.github.com/users/on-the-run/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "on-the-run", "id": 5404419, "node_id": "MDQ6VXNlcjU0MDQ0MTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/5404419?v=4", "gravatar_id": "", "url": "https://api.github.com/users/on-the-run", "html_url": "https://github.com/on-the-run", "followers_url": "https://api.github.com/users/on-the-run/followers", "following_url": "https://api.github.com/users/on-the-run/following{/other_user}", "gists_url": "https://api.github.com/users/on-the-run/gists{/gist_id}", "starred_url": "https://api.github.com/users/on-the-run/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/on-the-run/subscriptions", "organizations_url": "https://api.github.com/users/on-the-run/orgs", "repos_url": "https://api.github.com/users/on-the-run/repos", "events_url": "https://api.github.com/users/on-the-run/events{/privacy}", "received_events_url": "https://api.github.com/users/on-the-run/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "on-the-run", "id": 5404419, "node_id": "MDQ6VXNlcjU0MDQ0MTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/5404419?v=4", "gravatar_id": "", "url": "https://api.github.com/users/on-the-run", "html_url": "https://github.com/on-the-run", "followers_url": "https://api.github.com/users/on-the-run/followers", "following_url": "https://api.github.com/users/on-the-run/following{/other_user}", "gists_url": "https://api.github.com/users/on-the-run/gists{/gist_id}", "starred_url": "https://api.github.com/users/on-the-run/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/on-the-run/subscriptions", "organizations_url": "https://api.github.com/users/on-the-run/orgs", "repos_url": "https://api.github.com/users/on-the-run/repos", "events_url": "https://api.github.com/users/on-the-run/events{/privacy}", "received_events_url": "https://api.github.com/users/on-the-run/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 68, "created_at": "2017-07-10T18:44:52Z", "updated_at": "2017-12-20T19:30:08Z", "closed_at": "2017-12-20T19:29:20Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04.2 LTS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.2.0-1755-gee4259a 1.2.1</li>\n<li><strong>Python version</strong>: Python 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.1</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/5.1</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1080 Ti</li>\n<li><strong>Exact command to reproduce</strong>: Sorry, the command is not available since this is custom code</li>\n<li><strong>RDMA driver version</strong>: libibverbs-dev (1.2.1mlnx1-OFED.4.0.1.5.3.40200)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am training a seq2seq model with synchronized SGD (using <code>tf.train.SyncReplicasOptimizer</code> and <code>tf.train.Supervisor</code>), over RDMA (using <code>grpc+verbs</code> protocol), on 2 workers and 1 parameter server. Each worker has 8 GPUs, and the model on each GPU is the same.</p>\n<p>I can train this model fine with the default <code>grpc</code> protocol using the same setting. When I switched to <code>grpc+verbs</code>, the behavior becomes unpredictable. Most of the times both workers hang (for at least 12 hours, so not because I didn't wait long enough). Sometimes the chief worker (worker 0) starts training, but worker 1 hangs. (This should not happen since I am programming with synchronized SGD. Worker 0 should be blocked if worker 1 is not ready.) In rare case it can go through and start training. I believe the RDMA driver is correctly installed.</p>\n<p>When worker hangs, the CPU utilization stays low but not zero, and the GPU utilization is 0.</p>\n<h3>Source code / logs</h3>\n<p>I attached below the log from worker 1 <a href=\"https://github.com/tensorflow/tensorflow/files/1136607/worker1.txt\">worker1.txt</a>. In this case, worker 1 hangs, and worker 0 starts training. (There are some custom logs printed.) In the log you can find that worker 1 sent a <code>RDMA_MESSAGE_TENSOR_REQUEST</code> to the parameter server, but never got an ACK, and I have checked the log from the parameter server, which showed that it never received a <code>RDMA_MESSAGE_TENSOR_REQUEST</code> from worker 1.</p>\n<p>I also attached the full trace <a href=\"https://github.com/tensorflow/tensorflow/files/1136672/gdb_worker1.txt\">gdb_worker1.txt</a> collected on worker 1 in gdb with <code>thread apply all bt</code>, during worker 1 hanging.</p>\n<p>Thanks.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.2 LTS\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): v1.2.0-1755-gee4259a 1.2.1\nPython version: Python 3.5.2\nBazel version (if compiling from source): 0.5.1\nCUDA/cuDNN version: 8.0/5.1\nGPU model and memory: GeForce GTX 1080 Ti\nExact command to reproduce: Sorry, the command is not available since this is custom code\nRDMA driver version: libibverbs-dev (1.2.1mlnx1-OFED.4.0.1.5.3.40200)\n\nDescribe the problem\nI am training a seq2seq model with synchronized SGD (using tf.train.SyncReplicasOptimizer and tf.train.Supervisor), over RDMA (using grpc+verbs protocol), on 2 workers and 1 parameter server. Each worker has 8 GPUs, and the model on each GPU is the same.\nI can train this model fine with the default grpc protocol using the same setting. When I switched to grpc+verbs, the behavior becomes unpredictable. Most of the times both workers hang (for at least 12 hours, so not because I didn't wait long enough). Sometimes the chief worker (worker 0) starts training, but worker 1 hangs. (This should not happen since I am programming with synchronized SGD. Worker 0 should be blocked if worker 1 is not ready.) In rare case it can go through and start training. I believe the RDMA driver is correctly installed.\nWhen worker hangs, the CPU utilization stays low but not zero, and the GPU utilization is 0.\nSource code / logs\nI attached below the log from worker 1 worker1.txt. In this case, worker 1 hangs, and worker 0 starts training. (There are some custom logs printed.) In the log you can find that worker 1 sent a RDMA_MESSAGE_TENSOR_REQUEST to the parameter server, but never got an ACK, and I have checked the log from the parameter server, which showed that it never received a RDMA_MESSAGE_TENSOR_REQUEST from worker 1.\nI also attached the full trace gdb_worker1.txt collected on worker 1 in gdb with thread apply all bt, during worker 1 hanging.\nThanks.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.2.0-1755-gee4259a 1.2.1\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GeForce GTX 1080 Ti\r\n- **Exact command to reproduce**: Sorry, the command is not available since this is custom code\r\n- **RDMA driver version**: libibverbs-dev (1.2.1mlnx1-OFED.4.0.1.5.3.40200)\r\n\r\n### Describe the problem\r\nI am training a seq2seq model with synchronized SGD (using `tf.train.SyncReplicasOptimizer` and `tf.train.Supervisor`), over RDMA (using `grpc+verbs` protocol), on 2 workers and 1 parameter server. Each worker has 8 GPUs, and the model on each GPU is the same.\r\n\r\nI can train this model fine with the default `grpc` protocol using the same setting. When I switched to `grpc+verbs`, the behavior becomes unpredictable. Most of the times both workers hang (for at least 12 hours, so not because I didn't wait long enough). Sometimes the chief worker (worker 0) starts training, but worker 1 hangs. (This should not happen since I am programming with synchronized SGD. Worker 0 should be blocked if worker 1 is not ready.) In rare case it can go through and start training. I believe the RDMA driver is correctly installed.\r\n\r\nWhen worker hangs, the CPU utilization stays low but not zero, and the GPU utilization is 0.\r\n\r\n### Source code / logs\r\nI attached below the log from worker 1 [worker1.txt](https://github.com/tensorflow/tensorflow/files/1136607/worker1.txt). In this case, worker 1 hangs, and worker 0 starts training. (There are some custom logs printed.) In the log you can find that worker 1 sent a `RDMA_MESSAGE_TENSOR_REQUEST` to the parameter server, but never got an ACK, and I have checked the log from the parameter server, which showed that it never received a `RDMA_MESSAGE_TENSOR_REQUEST` from worker 1.\r\n\r\nI also attached the full trace [gdb_worker1.txt](https://github.com/tensorflow/tensorflow/files/1136672/gdb_worker1.txt) collected on worker 1 in gdb with `thread apply all bt`, during worker 1 hanging.\r\n\r\nThanks.\r\n\r\n\r\n\r\n"}