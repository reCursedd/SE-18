{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6976", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6976/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6976/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6976/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6976", "id": 202142607, "node_id": "MDU6SXNzdWUyMDIxNDI2MDc=", "number": 6976, "title": "Unexpected behavior in tensorflow's distributed training", "user": {"login": "infwinston", "id": 1206058, "node_id": "MDQ6VXNlcjEyMDYwNTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1206058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/infwinston", "html_url": "https://github.com/infwinston", "followers_url": "https://api.github.com/users/infwinston/followers", "following_url": "https://api.github.com/users/infwinston/following{/other_user}", "gists_url": "https://api.github.com/users/infwinston/gists{/gist_id}", "starred_url": "https://api.github.com/users/infwinston/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/infwinston/subscriptions", "organizations_url": "https://api.github.com/users/infwinston/orgs", "repos_url": "https://api.github.com/users/infwinston/repos", "events_url": "https://api.github.com/users/infwinston/events{/privacy}", "received_events_url": "https://api.github.com/users/infwinston/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-01-20T13:40:31Z", "updated_at": "2017-02-02T16:15:19Z", "closed_at": "2017-01-20T18:04:28Z", "author_association": "NONE", "body_html": "<p>Hi Tensorflowers,</p>\n<p>I was doing some distributed training experiments on tensorflow v0.11.0.<br>\nI modified the ResNet code from the official model zoos <a href=\"https://github.com/tensorflow/models/tree/master/resnet\">here</a> to be the one can do distributed training.<br>\nIn the experiments, ResNet56 is used for training on CIFAR10 data set and the settings follow the original <a href=\"https://arxiv.org/abs/1512.03385\" rel=\"nofollow\">paper</a>.</p>\n<p>When I set the number of parameter server = 1, worker = 1, I expected the behavior would be the same as the single gpu one (which is the original code in the <a href=\"https://github.com/tensorflow/models/tree/master/resnet\">repo</a>).<br>\nIt turns out that there is a huge performance gap. Please see the following figure.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/1206058/22150624/fd6332d2-df54-11e6-8091-7aeeb4770c5b.png\"><img src=\"https://cloud.githubusercontent.com/assets/1206058/22150624/fd6332d2-df54-11e6-8091-7aeeb4770c5b.png\" alt=\"resnet_cifar10_1ps1worker_tf\" style=\"max-width:100%;\"></a><br>\n<strong>x-axis</strong>: time in second, <strong>y-axis</strong>: testing error<br>\n<strong>tf-0</strong>: single gpu version, <strong>tf-1</strong>: distributed version with # ps=1, # worker=1<br>\nBoth code were run by 160 epochs and with the same parameter/learning rate schedule.</p>\n<p>The single gpu version can achieve 7% error rate (which is consistent with the original paper), but the distributed one is stalled at 12% error.<br>\nI think there might be something wrong as the performance should be similar for both cases.<br>\nCould you check if that is the case? (or maybe I used the wrong way to do distributed training)<br>\nPlease feel free to ask if you have any problem with the setting. Thanks.</p>\n<p>The single gpu version code can be found <a href=\"https://github.com/tensorflow/models/blob/d93ffd0b69253ee3c3a40a19a4c86ac6975bd570/resnet/resnet_main.py\">here</a> (I used the earlier 0.11 compatible version)<br>\nThe code for distributed version can be found <a href=\"https://gist.github.com/infwinston/8a7c86a75d3177bac4737903cc4c4fe3\">here</a>.</p>\n<p>Some detailed settings:</p>\n<pre><code>batch size=128, num_residual_units=9, relu_leakiness=0\n</code></pre>\n<p>The command I launched:</p>\n<pre><code>ps0\n&gt; export CUDA_VISIBLE_DEVICES=\"\"; python resnet_dist.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"ps\" --task_id=\"0\" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train\nworker0\n&gt; export CUDA_VISIBLE_DEVICES=\"0\"; python resnet_dist.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"worker\" --task_id=\"0\" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train\n</code></pre>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nUbuntu 14.04</p>\n<p>Installed version of CUDA and cuDNN:<br>\nCUDA 7.5, cuDNN 5.1</p>\n<pre><code>&gt; ls -l /usr/local/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189170 Oct 25 22:51 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so -&gt; libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5 -&gt; libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Oct 25 22:51 /usr/local/cuda/lib/libcudart_static.a\n</code></pre>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/282823b877f173e6a33bbc9d4b9ad7dd8413ada6/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/282823b877f173e6a33bbc9d4b9ad7dd8413ada6\"><tt>282823b</tt></a></li>\n<li>The output of <code>bazel version</code></li>\n</ol>\n<pre><code>Build label: 0.4.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\nBuild timestamp: 1481136431\nBuild timestamp as int: 1481136431\n</code></pre>\n<p>I built tensorflow v0.11.0 by myself.</p>", "body_text": "Hi Tensorflowers,\nI was doing some distributed training experiments on tensorflow v0.11.0.\nI modified the ResNet code from the official model zoos here to be the one can do distributed training.\nIn the experiments, ResNet56 is used for training on CIFAR10 data set and the settings follow the original paper.\nWhen I set the number of parameter server = 1, worker = 1, I expected the behavior would be the same as the single gpu one (which is the original code in the repo).\nIt turns out that there is a huge performance gap. Please see the following figure.\n\nx-axis: time in second, y-axis: testing error\ntf-0: single gpu version, tf-1: distributed version with # ps=1, # worker=1\nBoth code were run by 160 epochs and with the same parameter/learning rate schedule.\nThe single gpu version can achieve 7% error rate (which is consistent with the original paper), but the distributed one is stalled at 12% error.\nI think there might be something wrong as the performance should be similar for both cases.\nCould you check if that is the case? (or maybe I used the wrong way to do distributed training)\nPlease feel free to ask if you have any problem with the setting. Thanks.\nThe single gpu version code can be found here (I used the earlier 0.11 compatible version)\nThe code for distributed version can be found here.\nSome detailed settings:\nbatch size=128, num_residual_units=9, relu_leakiness=0\n\nThe command I launched:\nps0\n> export CUDA_VISIBLE_DEVICES=\"\"; python resnet_dist.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"ps\" --task_id=\"0\" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train\nworker0\n> export CUDA_VISIBLE_DEVICES=\"0\"; python resnet_dist.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"worker\" --task_id=\"0\" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train\n\nEnvironment info\nOperating System:\nUbuntu 14.04\nInstalled version of CUDA and cuDNN:\nCUDA 7.5, cuDNN 5.1\n> ls -l /usr/local/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189170 Oct 25 22:51 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Oct 25 22:51 /usr/local/cuda/lib/libcudart_static.a\n\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD)\n282823b\nThe output of bazel version\n\nBuild label: 0.4.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\nBuild timestamp: 1481136431\nBuild timestamp as int: 1481136431\n\nI built tensorflow v0.11.0 by myself.", "body": "Hi Tensorflowers,\r\n\r\nI was doing some distributed training experiments on tensorflow v0.11.0.\r\nI modified the ResNet code from the official model zoos [here](https://github.com/tensorflow/models/tree/master/resnet) to be the one can do distributed training.\r\nIn the experiments, ResNet56 is used for training on CIFAR10 data set and the settings follow the original [paper](https://arxiv.org/abs/1512.03385).\r\n\r\nWhen I set the number of parameter server = 1, worker = 1, I expected the behavior would be the same as the single gpu one (which is the original code in the [repo](https://github.com/tensorflow/models/tree/master/resnet)).\r\nIt turns out that there is a huge performance gap. Please see the following figure.\r\n![resnet_cifar10_1ps1worker_tf](https://cloud.githubusercontent.com/assets/1206058/22150624/fd6332d2-df54-11e6-8091-7aeeb4770c5b.png)\r\n**x-axis**: time in second, **y-axis**: testing error\r\n**tf-0**: single gpu version, **tf-1**: distributed version with # ps=1, # worker=1\r\nBoth code were run by 160 epochs and with the same parameter/learning rate schedule.\r\n\r\nThe single gpu version can achieve 7% error rate (which is consistent with the original paper), but the distributed one is stalled at 12% error.\r\nI think there might be something wrong as the performance should be similar for both cases.\r\nCould you check if that is the case? (or maybe I used the wrong way to do distributed training)\r\nPlease feel free to ask if you have any problem with the setting. Thanks.\r\n\r\nThe single gpu version code can be found [here](https://github.com/tensorflow/models/blob/d93ffd0b69253ee3c3a40a19a4c86ac6975bd570/resnet/resnet_main.py) (I used the earlier 0.11 compatible version)\r\nThe code for distributed version can be found [here](https://gist.github.com/infwinston/8a7c86a75d3177bac4737903cc4c4fe3).\r\n\r\nSome detailed settings:\r\n```\r\nbatch size=128, num_residual_units=9, relu_leakiness=0\r\n```\r\n\r\nThe command I launched:\r\n```\r\nps0\r\n> export CUDA_VISIBLE_DEVICES=\"\"; python resnet_dist.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"ps\" --task_id=\"0\" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train\r\nworker0\r\n> export CUDA_VISIBLE_DEVICES=\"0\"; python resnet_dist.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"worker\" --task_id=\"0\" --batch_size=128 --dataset='cifar10' --train_data_path=cifar10/data_batch* --log_root=./tmp-log-root/ --num_gpus=1 --mode train\r\n```\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN:\r\nCUDA 7.5, cuDNN 5.1\r\n```\r\n> ls -l /usr/local/cuda/lib/libcud*\r\n-rw-r--r-- 1 root root 189170 Oct 25 22:51 /usr/local/cuda/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 311596 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 558020 Oct 25 22:51 /usr/local/cuda/lib/libcudart_static.a\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n282823b877f173e6a33bbc9d4b9ad7dd8413ada6\r\n2. The output of `bazel version`\r\n```\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\r\nBuild timestamp: 1481136431\r\nBuild timestamp as int: 1481136431\r\n```\r\nI built tensorflow v0.11.0 by myself.\r\n"}