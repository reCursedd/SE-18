{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/274251345", "html_url": "https://github.com/tensorflow/tensorflow/issues/6976#issuecomment-274251345", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6976", "id": 274251345, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDI1MTM0NQ==", "user": {"login": "infwinston", "id": 1206058, "node_id": "MDQ6VXNlcjEyMDYwNTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1206058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/infwinston", "html_url": "https://github.com/infwinston", "followers_url": "https://api.github.com/users/infwinston/followers", "following_url": "https://api.github.com/users/infwinston/following{/other_user}", "gists_url": "https://api.github.com/users/infwinston/gists{/gist_id}", "starred_url": "https://api.github.com/users/infwinston/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/infwinston/subscriptions", "organizations_url": "https://api.github.com/users/infwinston/orgs", "repos_url": "https://api.github.com/users/infwinston/repos", "events_url": "https://api.github.com/users/infwinston/events{/privacy}", "received_events_url": "https://api.github.com/users/infwinston/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-21T09:46:43Z", "updated_at": "2017-01-21T09:46:43Z", "author_association": "NONE", "body_html": "<p>Thanks for your response.<br>\nBut actually I don't think it's the case that one just got lucky.<br>\nThere might be some randomness but the performance gap should not be such huge I think.<br>\nThe jump of curve is because the learning rate decreased and indeed it is reproducible.<br>\nI have done several experiments on it and found that is the case.<br>\nAlso, this kind of curve is consistent with the one in the original <a href=\"https://arxiv.org/pdf/1512.03385v1.pdf\" rel=\"nofollow\">paper</a> (See Fig. 6) and many people can reproduce it by using the same settings (e.g., <a href=\"http://torch.ch/blog/2016/02/04/resnets.html\" rel=\"nofollow\">[1]</a>, <a href=\"https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet\">[2]</a>, or more examples <a href=\"https://github.com/KaimingHe/deep-residual-networks#third-party-re-implementations\">here</a>)<br>\nBut you're right, I should report the one without using SyncReplicas first. I am already working on this.</p>\n<p>Anyway, I did not have a direct proof to something wrong in tensorflow but if you think this is not the right place to discuss then I can move to stackoverflow.<br>\nThanks.</p>", "body_text": "Thanks for your response.\nBut actually I don't think it's the case that one just got lucky.\nThere might be some randomness but the performance gap should not be such huge I think.\nThe jump of curve is because the learning rate decreased and indeed it is reproducible.\nI have done several experiments on it and found that is the case.\nAlso, this kind of curve is consistent with the one in the original paper (See Fig. 6) and many people can reproduce it by using the same settings (e.g., [1], [2], or more examples here)\nBut you're right, I should report the one without using SyncReplicas first. I am already working on this.\nAnyway, I did not have a direct proof to something wrong in tensorflow but if you think this is not the right place to discuss then I can move to stackoverflow.\nThanks.", "body": "Thanks for your response.\r\nBut actually I don't think it's the case that one just got lucky.\r\nThere might be some randomness but the performance gap should not be such huge I think.\r\nThe jump of curve is because the learning rate decreased and indeed it is reproducible.\r\nI have done several experiments on it and found that is the case.\r\nAlso, this kind of curve is consistent with the one in the original [paper](https://arxiv.org/pdf/1512.03385v1.pdf) (See Fig. 6) and many people can reproduce it by using the same settings (e.g., [[1]](http://torch.ch/blog/2016/02/04/resnets.html), [[2]](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet), or more examples [here](https://github.com/KaimingHe/deep-residual-networks#third-party-re-implementations))\r\nBut you're right, I should report the one without using SyncReplicas first. I am already working on this.\r\n\r\nAnyway, I did not have a direct proof to something wrong in tensorflow but if you think this is not the right place to discuss then I can move to stackoverflow.\r\nThanks.\r\n"}