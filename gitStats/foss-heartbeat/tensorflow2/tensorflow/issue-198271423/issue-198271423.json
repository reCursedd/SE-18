{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6592", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6592/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6592/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6592/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6592", "id": 198271423, "node_id": "MDU6SXNzdWUxOTgyNzE0MjM=", "number": 6592, "title": "About the implementation of recently added attentional seq2seq decoder function", "user": {"login": "jihunchoi", "id": 1898501, "node_id": "MDQ6VXNlcjE4OTg1MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1898501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jihunchoi", "html_url": "https://github.com/jihunchoi", "followers_url": "https://api.github.com/users/jihunchoi/followers", "following_url": "https://api.github.com/users/jihunchoi/following{/other_user}", "gists_url": "https://api.github.com/users/jihunchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jihunchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jihunchoi/subscriptions", "organizations_url": "https://api.github.com/users/jihunchoi/orgs", "repos_url": "https://api.github.com/users/jihunchoi/repos", "events_url": "https://api.github.com/users/jihunchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/jihunchoi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-01-01T15:41:22Z", "updated_at": "2017-01-01T17:30:06Z", "closed_at": "2017-01-01T17:30:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hello, I heard a good news that the attentional seq2seq decoder function is recently added in the master branch.<br>\nIt is a very pleasing news to me, since nowadays I have been struggling to implement the feature.</p>\n<p>However, while reviewing the detailed implementation, I found some unclear points.</p>\n<p><del>Firstly, in line 395-397 of attention_decoder_fn.py,<br>\n<code>attention_query</code> and <code>context</code> are concatenated and passed to a fully connected layer to be projected to a vector whose size is <code>num_units</code>.<br>\nAs far as I know, the current implementations of RNN cells already do this procedure.<br>\nFor example, the current implementation of <code>LSTMCell</code> (line 337-338 in core_rnn_cell_impl.py) concatenates <code>inputs</code> and <code>m_prev</code>, and multiplies the concatenated vector by <code>(input_dim + num_units, 4 * num_units)</code> weight matrix.<br>\nThus, to avoid multiplying <code>inputs</code> with weight matrices two times, I think line 396-397 should be removed.</del><br>\nReading codes again, I found I understood them wrongly.<br>\nAnd, I also think <code>concat_v2</code> function should be used to concatenate two vectors.</p>\n<p><del>Secondly, in line 122-123 and 288-289, I see that <code>cell_output</code> is used as <code>attention_query</code> parameter of <code>attention_construct_fn</code>.<br>\nThis is correct if we choose 'Luong-style' alignment calculation, where the computation path is \"h_t -&gt; a_t -&gt; c_t -&gt; tilde h_t\", however this seems to be problematic if we choose 'Bahdanau-style' alignment calculation, where the computation path is \"h_(t-1) -&gt; a_t -&gt; c_t -&gt; h_t\".<br>\nFor now I can't think of a simple way to correct this behavior since the style of attention mechanism is unknown in decoder_fn's scope, but I think it should be dealt with anyway.</del></p>\n<p>Please tell me if I read the codes wrongly! :)</p>\n<p>I am a bit confused with the exact meaning of luong and bahdanau method. The both options use h_(t-1), unlike the referencing paper.<br>\nI close this, and will make a new issue after organizing my brain!</p>", "body_text": "Hello, I heard a good news that the attentional seq2seq decoder function is recently added in the master branch.\nIt is a very pleasing news to me, since nowadays I have been struggling to implement the feature.\nHowever, while reviewing the detailed implementation, I found some unclear points.\nFirstly, in line 395-397 of attention_decoder_fn.py,\nattention_query and context are concatenated and passed to a fully connected layer to be projected to a vector whose size is num_units.\nAs far as I know, the current implementations of RNN cells already do this procedure.\nFor example, the current implementation of LSTMCell (line 337-338 in core_rnn_cell_impl.py) concatenates inputs and m_prev, and multiplies the concatenated vector by (input_dim + num_units, 4 * num_units) weight matrix.\nThus, to avoid multiplying inputs with weight matrices two times, I think line 396-397 should be removed.\nReading codes again, I found I understood them wrongly.\nAnd, I also think concat_v2 function should be used to concatenate two vectors.\nSecondly, in line 122-123 and 288-289, I see that cell_output is used as attention_query parameter of attention_construct_fn.\nThis is correct if we choose 'Luong-style' alignment calculation, where the computation path is \"h_t -> a_t -> c_t -> tilde h_t\", however this seems to be problematic if we choose 'Bahdanau-style' alignment calculation, where the computation path is \"h_(t-1) -> a_t -> c_t -> h_t\".\nFor now I can't think of a simple way to correct this behavior since the style of attention mechanism is unknown in decoder_fn's scope, but I think it should be dealt with anyway.\nPlease tell me if I read the codes wrongly! :)\nI am a bit confused with the exact meaning of luong and bahdanau method. The both options use h_(t-1), unlike the referencing paper.\nI close this, and will make a new issue after organizing my brain!", "body": "Hello, I heard a good news that the attentional seq2seq decoder function is recently added in the master branch.\r\nIt is a very pleasing news to me, since nowadays I have been struggling to implement the feature.\r\n\r\nHowever, while reviewing the detailed implementation, I found some unclear points.\r\n\r\n~~Firstly, in line 395-397 of attention_decoder_fn.py,\r\n`attention_query` and `context` are concatenated and passed to a fully connected layer to be projected to a vector whose size is `num_units`.\r\nAs far as I know, the current implementations of RNN cells already do this procedure.\r\nFor example, the current implementation of `LSTMCell` (line 337-338 in core_rnn_cell_impl.py) concatenates `inputs` and `m_prev`, and multiplies the concatenated vector by `(input_dim + num_units, 4 * num_units)` weight matrix.\r\nThus, to avoid multiplying `inputs` with weight matrices two times, I think line 396-397 should be removed.~~\r\nReading codes again, I found I understood them wrongly.\r\nAnd, I also think `concat_v2` function should be used to concatenate two vectors.\r\n\r\n~~Secondly, in line 122-123 and 288-289, I see that `cell_output` is used as `attention_query` parameter of `attention_construct_fn`.\r\nThis is correct if we choose 'Luong-style' alignment calculation, where the computation path is \"h_t -> a_t -> c_t -> tilde h_t\", however this seems to be problematic if we choose 'Bahdanau-style' alignment calculation, where the computation path is \"h_(t-1) -> a_t -> c_t -> h_t\".\r\nFor now I can't think of a simple way to correct this behavior since the style of attention mechanism is unknown in decoder_fn's scope, but I think it should be dealt with anyway.~~\r\n\r\nPlease tell me if I read the codes wrongly! :)\r\n\r\nI am a bit confused with the exact meaning of luong and bahdanau method. The both options use h_(t-1), unlike the referencing paper.\r\nI close this, and will make a new issue after organizing my brain! "}