{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/244444302", "html_url": "https://github.com/tensorflow/tensorflow/issues/4132#issuecomment-244444302", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4132", "id": 244444302, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NDQ0NDMwMg==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-02T17:53:39Z", "updated_at": "2016-09-02T17:53:39Z", "author_association": "MEMBER", "body_html": "<p>I'm going to look at this a bit more, but first, in case you're willing to do some more experiments, let me give my thoughts.  I'm suspicious of the interaction between python and the backend graph execution environment.  I'm much more familiar with that backend environment and how it actually executes large tensor Ops, and there's nothing I know of there that seems like a plausible cause for the problem.  Due to the hybrid python/compiled nature of the binary, my usual profiling tools are not much good, so it's difficult to identify where the time is going, but I'm doubtful it's actually the tensor Op execution.  I wonder whether we might sometimes be seeing a slow data transfer from python to feed the placeholder.  Grasping at straws, maybe data alignment might be an issue.  If you see reproducible differences between python versions (and/or SWIG?) that would be interesting.</p>", "body_text": "I'm going to look at this a bit more, but first, in case you're willing to do some more experiments, let me give my thoughts.  I'm suspicious of the interaction between python and the backend graph execution environment.  I'm much more familiar with that backend environment and how it actually executes large tensor Ops, and there's nothing I know of there that seems like a plausible cause for the problem.  Due to the hybrid python/compiled nature of the binary, my usual profiling tools are not much good, so it's difficult to identify where the time is going, but I'm doubtful it's actually the tensor Op execution.  I wonder whether we might sometimes be seeing a slow data transfer from python to feed the placeholder.  Grasping at straws, maybe data alignment might be an issue.  If you see reproducible differences between python versions (and/or SWIG?) that would be interesting.", "body": "I'm going to look at this a bit more, but first, in case you're willing to do some more experiments, let me give my thoughts.  I'm suspicious of the interaction between python and the backend graph execution environment.  I'm much more familiar with that backend environment and how it actually executes large tensor Ops, and there's nothing I know of there that seems like a plausible cause for the problem.  Due to the hybrid python/compiled nature of the binary, my usual profiling tools are not much good, so it's difficult to identify where the time is going, but I'm doubtful it's actually the tensor Op execution.  I wonder whether we might sometimes be seeing a slow data transfer from python to feed the placeholder.  Grasping at straws, maybe data alignment might be an issue.  If you see reproducible differences between python versions (and/or SWIG?) that would be interesting.\n"}