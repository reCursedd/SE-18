{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/181883327", "pull_request_review_id": 112586277, "id": 181883327, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MTg4MzMyNw==", "diff_hunk": "@@ -2399,16 +2403,156 @@ port::Status CUDABlas::DoBlasGemmBatchedInternal(\n                       \"failed BLAS call, see log for details\");\n }\n \n+// Supports tensor ops, use with half and float only\n+template <typename T>\n+port::Status CUDABlas::DoBlasGemmBatchedInternalTensorOp(\n+    Stream *stream, blas::Transpose transa, blas::Transpose transb, uint64 m,\n+    uint64 n, uint64 k, float alpha,\n+    const port::ArraySlice<DeviceMemory<T> *> &a_ptrs_to_wrappers, int lda,\n+    const port::ArraySlice<DeviceMemory<T> *> &b_ptrs_to_wrappers, int ldb,\n+    float beta, const port::ArraySlice<DeviceMemory<T> *> &c_ptrs_to_wrappers,\n+    int ldc, int batch_count, ScratchAllocator *scratch_allocator) {\n+  std::vector<T *> a_raw_ptrs, b_raw_ptrs, c_raw_ptrs;\n+  for (int i = 0; i < batch_count; ++i) {\n+    a_raw_ptrs.push_back(static_cast<T *>(a_ptrs_to_wrappers[i]->opaque()));\n+    b_raw_ptrs.push_back(static_cast<T *>(b_ptrs_to_wrappers[i]->opaque()));\n+    c_raw_ptrs.push_back(static_cast<T *>(c_ptrs_to_wrappers[i]->opaque()));\n+  }\n+\n+  typedef void CUDA_T;\n+\n+  const size_t size = batch_count * sizeof(CUDA_T *);\n+\n+  // Device-side copy of pointers to matrices.\n+  DeviceMemory<CUDA_T *> a;\n+  DeviceMemory<CUDA_T *> b;\n+  DeviceMemory<CUDA_T *> c;\n+\n+  // If temporary space is allocated for device-side copies of pointers to\n+  // matrices, that temporary space should not be freed until this function\n+  // returns. Although the values for these unique_ptrs are not set here, they\n+  // are declared at this scope so they will be destroyed when the function\n+  // returns.\n+  //\n+  // If a scratch allocator is provided, these pointers will not be used at all.\n+  std::unique_ptr<TemporaryDeviceMemory<CUDA_T *>> a_temporary;\n+  std::unique_ptr<TemporaryDeviceMemory<CUDA_T *>> b_temporary;\n+  std::unique_ptr<TemporaryDeviceMemory<CUDA_T *>> c_temporary;\n+\n+  // Decide how to allocate device-side copy of pointers to matrices based on\n+  // whether a scratch allocator was passed.\n+  if (scratch_allocator != nullptr) {\n+    SE_ASSIGN_OR_RETURN(DeviceMemory<uint8> a_bytes,\n+                        scratch_allocator->AllocateBytes(stream, size));\n+    SE_ASSIGN_OR_RETURN(DeviceMemory<uint8> b_bytes,\n+                        scratch_allocator->AllocateBytes(stream, size));\n+    SE_ASSIGN_OR_RETURN(DeviceMemory<uint8> c_bytes,\n+                        scratch_allocator->AllocateBytes(stream, size));\n+    a = DeviceMemory<CUDA_T *>(a_bytes);\n+    b = DeviceMemory<CUDA_T *>(b_bytes);\n+    c = DeviceMemory<CUDA_T *>(c_bytes);\n+  } else {\n+    SE_ASSIGN_OR_RETURN(a_temporary,\n+                        stream->AllocateTemporaryArray<CUDA_T *>(batch_count));\n+    SE_ASSIGN_OR_RETURN(b_temporary,\n+                        stream->AllocateTemporaryArray<CUDA_T *>(batch_count));\n+    SE_ASSIGN_OR_RETURN(c_temporary,\n+                        stream->AllocateTemporaryArray<CUDA_T *>(batch_count));\n+    a = DeviceMemory<CUDA_T *>(*a_temporary->mutable_device_memory());\n+    b = DeviceMemory<CUDA_T *>(*b_temporary->mutable_device_memory());\n+    c = DeviceMemory<CUDA_T *>(*c_temporary->mutable_device_memory());\n+  }\n+\n+  if (!stream->ThenMemcpy(&a, a_raw_ptrs.data(), size).ok() ||\n+      !stream->ThenMemcpy(&b, b_raw_ptrs.data(), size).ok() ||\n+      !stream->ThenMemcpy(&c, c_raw_ptrs.data(), size).ok()) {\n+    return port::Status(port::error::INTERNAL,\n+                        \"failed to copy memory from host to device in \"\n+                        \"CUDABlas::DoBlasGemmBatched\");\n+  }\n+\n+  cudaDataType_t data_type = CUDADataType<T>::type;\n+  cudaDataType_t compute_type = CUDA_R_32F;\n+\n+#if CUDA_VERSION >= 9010", "path": "tensorflow/stream_executor/cuda/cuda_blas.cc", "position": null, "original_position": 86, "commit_id": "bcea39316f9bfffd9c40da63e34faa557ebf849e", "original_commit_id": "bf2a21af1bb186d424a12a817416a55e96ca635b", "user": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "body": "Is there any reason we cannot put the following part into the already existing function: CUDABlas::DoBlasGemmBatchedInternal()? In that way, cublasGemmBatchedEx() can be used for all data types (half, float, and double).", "created_at": "2018-04-16T20:59:55Z", "updated_at": "2018-05-08T17:20:34Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/18436#discussion_r181883327", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18436", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/181883327"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/18436#discussion_r181883327"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18436"}}, "body_html": "<p>Is there any reason we cannot put the following part into the already existing function: CUDABlas::DoBlasGemmBatchedInternal()? In that way, cublasGemmBatchedEx() can be used for all data types (half, float, and double).</p>", "body_text": "Is there any reason we cannot put the following part into the already existing function: CUDABlas::DoBlasGemmBatchedInternal()? In that way, cublasGemmBatchedEx() can be used for all data types (half, float, and double)."}