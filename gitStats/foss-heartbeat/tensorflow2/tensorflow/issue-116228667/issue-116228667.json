{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/107", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/107/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/107/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/107/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/107", "id": 116228667, "node_id": "MDU6SXNzdWUxMTYyMjg2Njc=", "number": 107, "title": "Truncated backprop docs are confusing", "user": {"login": "allentran", "id": 6768556, "node_id": "MDQ6VXNlcjY3Njg1NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/6768556?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allentran", "html_url": "https://github.com/allentran", "followers_url": "https://api.github.com/users/allentran/followers", "following_url": "https://api.github.com/users/allentran/following{/other_user}", "gists_url": "https://api.github.com/users/allentran/gists{/gist_id}", "starred_url": "https://api.github.com/users/allentran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allentran/subscriptions", "organizations_url": "https://api.github.com/users/allentran/orgs", "repos_url": "https://api.github.com/users/allentran/repos", "events_url": "https://api.github.com/users/allentran/events{/privacy}", "received_events_url": "https://api.github.com/users/allentran/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2015-11-10T23:45:39Z", "updated_at": "2015-11-11T04:18:20Z", "closed_at": "2015-11-11T04:18:20Z", "author_association": "NONE", "body_html": "<p>The docs (<a href=\"http://tensorflow.org/tutorials/recurrent/index.md\" rel=\"nofollow\">http://tensorflow.org/tutorials/recurrent/index.md</a>) imply that to truncate backprop, you feed blocks of fixed (time) length (<code>num_steps</code>) to <code>RNNCell</code>.</p>\n<p>Does this mean that if I have sentences of length 100 and I want to truncate by 20 time steps, I would send tensors of shape <code>batch_size x 20</code>?</p>\n<p>If I'm right and you are supposed to feed blocks of fixed length inputs in an inner loop, don't I need to initialize the initial state from the previous block for the forward pass to be right.  But if I do use the final hidden state of the previous block as the next initial state, won't the autograd backpropagate all the way through.  In other words, looping over blocks of intervals and then within each in block while connecting the previous block's hidden state to the next block's initial state seems equivalent to just looping over time in the first place.</p>\n<p>Am I missing something?</p>", "body_text": "The docs (http://tensorflow.org/tutorials/recurrent/index.md) imply that to truncate backprop, you feed blocks of fixed (time) length (num_steps) to RNNCell.\nDoes this mean that if I have sentences of length 100 and I want to truncate by 20 time steps, I would send tensors of shape batch_size x 20?\nIf I'm right and you are supposed to feed blocks of fixed length inputs in an inner loop, don't I need to initialize the initial state from the previous block for the forward pass to be right.  But if I do use the final hidden state of the previous block as the next initial state, won't the autograd backpropagate all the way through.  In other words, looping over blocks of intervals and then within each in block while connecting the previous block's hidden state to the next block's initial state seems equivalent to just looping over time in the first place.\nAm I missing something?", "body": "The docs (http://tensorflow.org/tutorials/recurrent/index.md) imply that to truncate backprop, you feed blocks of fixed (time) length (`num_steps`) to `RNNCell`.  \n\nDoes this mean that if I have sentences of length 100 and I want to truncate by 20 time steps, I would send tensors of shape `batch_size x 20`?\n\nIf I'm right and you are supposed to feed blocks of fixed length inputs in an inner loop, don't I need to initialize the initial state from the previous block for the forward pass to be right.  But if I do use the final hidden state of the previous block as the next initial state, won't the autograd backpropagate all the way through.  In other words, looping over blocks of intervals and then within each in block while connecting the previous block's hidden state to the next block's initial state seems equivalent to just looping over time in the first place.\n\nAm I missing something?\n"}