{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/395970757", "html_url": "https://github.com/tensorflow/tensorflow/issues/3115#issuecomment-395970757", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3115", "id": 395970757, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NTk3MDc1Nw==", "user": {"login": "gauravbansal98", "id": 34395347, "node_id": "MDQ6VXNlcjM0Mzk1MzQ3", "avatar_url": "https://avatars1.githubusercontent.com/u/34395347?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gauravbansal98", "html_url": "https://github.com/gauravbansal98", "followers_url": "https://api.github.com/users/gauravbansal98/followers", "following_url": "https://api.github.com/users/gauravbansal98/following{/other_user}", "gists_url": "https://api.github.com/users/gauravbansal98/gists{/gist_id}", "starred_url": "https://api.github.com/users/gauravbansal98/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gauravbansal98/subscriptions", "organizations_url": "https://api.github.com/users/gauravbansal98/orgs", "repos_url": "https://api.github.com/users/gauravbansal98/repos", "events_url": "https://api.github.com/users/gauravbansal98/events{/privacy}", "received_events_url": "https://api.github.com/users/gauravbansal98/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-09T13:47:13Z", "updated_at": "2018-06-09T13:47:13Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=172688\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/davek44\">@davek44</a> I am trying to initialize the cell with a constant initializer but it is giving en error. The code which I'm using is -<br>\nf = [np.random.normal(size = [15, 40]), np.random.normal(size = [40,])]<br>\ninit = tf.constant_initializer(f, verify_shape = True, dtype = tf.float32)</p>\n<p>cell = tf.contrib.rnn.LSTMCell(lstm_units, initializer = init)<br>\ninitial_state = cell.zero_state(2, dtype = tf.float32)<br>\nunused_encoder_outputs, encoder_state =tf.nn.dynamic_rnn(cell ,source_seq_embedded,sequence_length=source_seq_len,dtype = tf.float32, initial_state = initial_state)</p>\n<h2>It is giving error -</h2>\n<p>TypeError                                 Traceback (most recent call last)<br>\n in ()<br>\n29 initial_state = cell.zero_state(2, dtype = tf.float32)<br>\n30 unused_encoder_outputs, encoder_state =tf.nn.dynamic_rnn(cell ,source_seq_embedded,sequence_length=source_seq_len,<br>\n---&gt; 31                                                         dtype = tf.float32, initial_state = initial_state)<br>\n32<br>\n33 sampling_prob = tf.Variable(0.0, dtype=tf.float32)</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)<br>\n625         swap_memory=swap_memory,<br>\n626         sequence_length=sequence_length,<br>\n--&gt; 627         dtype=dtype)<br>\n628<br>\n629     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)<br>\n822       parallel_iterations=parallel_iterations,<br>\n823       maximum_iterations=time_steps,<br>\n--&gt; 824       swap_memory=swap_memory)<br>\n825<br>\n826   # Unpack final output if not using output tuples.</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)<br>\n3222     if loop_context.outer_context is None:<br>\n3223       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)<br>\n-&gt; 3224     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)<br>\n3225     if maximum_iterations is not None:<br>\n3226       return result[1]</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)<br>\n2954       with ops.get_default_graph()._lock:  # pylint: disable=protected-access<br>\n2955         original_body_result, exit_vars = self._BuildLoop(<br>\n-&gt; 2956             pred, body, original_loop_vars, loop_vars, shape_invariants)<br>\n2957     finally:<br>\n2958       self.Exit()</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)<br>\n2891         flat_sequence=vars_for_body_with_tensor_arrays)<br>\n2892     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access<br>\n-&gt; 2893     body_result = body(*packed_vars_for_body)<br>\n2894     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access<br>\n2895     if not nest.is_sequence(body_result):</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in (i, lv)<br>\n3192         cond = lambda i, lv: (  # pylint: disable=g-long-lambda<br>\n3193             math_ops.logical_and(i &lt; maximum_iterations, orig_cond(*lv)))<br>\n-&gt; 3194         body = lambda i, lv: (i + 1, orig_body(*lv))<br>\n3195<br>\n3196     if context.executing_eagerly():</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in _time_step(time, output_ta_t, state)<br>\n791           call_cell=call_cell,<br>\n792           state_size=state_size,<br>\n--&gt; 793           skip_conditionals=True)<br>\n794     else:<br>\n795       (output, new_state) = call_cell()</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in _rnn_step(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)<br>\n246     # steps.  This is faster when max_seq_len is equal to the number of unrolls<br>\n247     # (which is typical for dynamic_rnn).<br>\n--&gt; 248     new_output, new_state = call_cell()<br>\n249     nest.assert_same_structure(state, new_state)<br>\n250     new_state = nest.flatten(new_state)</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in ()<br>\n779<br>\n780     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)<br>\n--&gt; 781     call_cell = lambda: cell(input_t, state)<br>\n782<br>\n783     if sequence_length is not None:</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in <strong>call</strong>(self, inputs, state, scope, *args, **kwargs)<br>\n337     # method.  See the class docstring for more details.<br>\n338     return base_layer.Layer.<strong>call</strong>(self, inputs, state, scope=scope,<br>\n--&gt; 339                                      *args, **kwargs)<br>\n340<br>\n341</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in <strong>call</strong>(self, inputs, *args, **kwargs)<br>\n697           if all(hasattr(x, 'get_shape') for x in input_list):<br>\n698             input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs)<br>\n--&gt; 699           self.build(input_shapes)<br>\n700         try:<br>\n701           # Note: not all sub-classes of Layer call Layer.<strong>init</strong> (especially</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in build(self, inputs_shape)<br>\n765         shape=[input_depth + h_depth, 4 * self._num_units],<br>\n766         initializer=self._initializer,<br>\n--&gt; 767         partitioner=maybe_partitioner)<br>\n768     self._bias = self.add_variable(<br>\n769         _BIAS_VARIABLE_NAME,</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in add_variable(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)<br>\n544             constraint=constraint,<br>\n545             trainable=trainable and self.trainable,<br>\n--&gt; 546             partitioner=partitioner)<br>\n547<br>\n548         if init_graph is not None:  # pylint: disable=protected-access</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)<br>\n434     new_variable = getter(<br>\n435         name=name, shape=shape, dtype=dtype, initializer=initializer,<br>\n--&gt; 436         **kwargs_for_getter)<br>\n437<br>\n438     # If we set an initializer and the variable processed it, tracking will not</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)<br>\n1315       partitioner=partitioner, validate_shape=validate_shape,<br>\n1316       use_resource=use_resource, custom_getter=custom_getter,<br>\n-&gt; 1317       constraint=constraint)<br>\n1318 get_variable_or_local_docstring = (<br>\n1319     \"\"\"%s</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)<br>\n1077           partitioner=partitioner, validate_shape=validate_shape,<br>\n1078           use_resource=use_resource, custom_getter=custom_getter,<br>\n-&gt; 1079           constraint=constraint)<br>\n1080<br>\n1081   def _get_partitioned_variable(self,</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)<br>\n423           caching_device=caching_device, partitioner=partitioner,<br>\n424           validate_shape=validate_shape, use_resource=use_resource,<br>\n--&gt; 425           constraint=constraint)<br>\n426<br>\n427   def _get_partitioned_variable(</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)<br>\n392           trainable=trainable, collections=collections,<br>\n393           caching_device=caching_device, validate_shape=validate_shape,<br>\n--&gt; 394           use_resource=use_resource, constraint=constraint)<br>\n395<br>\n396     if custom_getter is not None:</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)<br>\n784         validate_shape=validate_shape,<br>\n785         constraint=constraint,<br>\n--&gt; 786         use_resource=use_resource)<br>\n787     if not context.executing_eagerly() or self._store_eager_variables:<br>\n788       # In eager mode we do not want to keep default references to Variable</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource)<br>\n2218                          name=name, dtype=dtype,<br>\n2219                          constraint=constraint,<br>\n-&gt; 2220                          use_resource=use_resource)<br>\n2221<br>\n2222</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in (**kwargs)<br>\n2208              constraint=None,<br>\n2209              use_resource=None):<br>\n-&gt; 2210   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)<br>\n2211   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access<br>\n2212     previous_getter = _make_getter(getter, previous_getter)</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)<br>\n2191         collections=collections, validate_shape=validate_shape,<br>\n2192         caching_device=caching_device, name=name, dtype=dtype,<br>\n-&gt; 2193         constraint=constraint)<br>\n2194<br>\n2195</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in <strong>init</strong>(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)<br>\n233           dtype=dtype,<br>\n234           expected_shape=expected_shape,<br>\n--&gt; 235           constraint=constraint)<br>\n236<br>\n237   def <strong>repr</strong>(self):</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)<br>\n341             with ops.name_scope(\"Initializer\"), ops.device(None):<br>\n342               self._initial_value = ops.convert_to_tensor(<br>\n--&gt; 343                   initial_value(), name=\"initial_value\", dtype=dtype)<br>\n344               shape = (self._initial_value.get_shape()<br>\n345                        if validate_shape else tensor_shape.unknown_shape())</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in ()<br>\n768           initializer = initializer(dtype=dtype)<br>\n769         init_val = lambda: initializer(  # pylint: disable=g-long-lambda<br>\n--&gt; 770             shape.as_list(), dtype=dtype, partition_info=partition_info)<br>\n771         variable_dtype = dtype.base_dtype<br>\n772</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py in <strong>call</strong>(self, shape, dtype, partition_info, verify_shape)<br>\n215       verify_shape = self._verify_shape<br>\n216     return constant_op.constant(<br>\n--&gt; 217         self.value, dtype=dtype, shape=shape, verify_shape=verify_shape)<br>\n218<br>\n219   def get_config(self):</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)<br>\n212   tensor_value.tensor.CopyFrom(<br>\n213       tensor_util.make_tensor_proto(<br>\n--&gt; 214           value, dtype=dtype, shape=shape, verify_shape=verify_shape))<br>\n215   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)<br>\n216   const_tensor = g.create_op(</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)<br>\n430       nparray = np.empty(shape, dtype=np_dt)<br>\n431     else:<br>\n--&gt; 432       _AssertCompatible(values, dtype)<br>\n433       nparray = np.array(values, dtype=np_dt)<br>\n434       # check to them.</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)<br>\n341     else:<br>\n342       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %<br>\n--&gt; 343                       (dtype.name, repr(mismatch), type(mismatch).<strong>name</strong>))<br>\n344<br>\n345</p>\n<p>TypeError: Expected float32, got array([[ 1.04484751e+00,  4.59392878e-02,  8.02706034e-01,<br>\n-1.11174105e+00,  8.89477561e-01, -1.20486729e+00,<br>\n8.86266342e-01, -1.64510319e+00, -9.89266584e-01,<br>\n1.31369015e+00,  4.88107334e-01,  1.49676477e-01,<br>\n-6.81036506e-01, -9.64134982e-02, -1.39690939e+00,<br>\n7.25600555e-02, -9.32924292e-02, -7.95291205e-01,<br>\n2.18235671e-01,  1.01727717e+00,  9.90535181e-01,<br>\n-4.35980568e-01,  7.94711686e-01, -3.13013630e-02,<br>\n-7.20243003e-01, -1.37384000e+00, -9.62151398e-01,<br>\n1.07646743e+00,  4.69513293e-01, -4.47742163e-02,<br>\n-4.95195536e-01,  6.76953787e-01,  2.39035230e-01,<br>\n-1.11104812e-01,  2.05507744e+00,  9.33967217e-01,<br>\n-8.96051458e-01, -8.77908566e-02,  1.43848201e+00,<br>\n-1.04624360e+00],<br>\n[-3.41335190e-01,  1.40035909e+00, -1.60449447e-01,<br>\n-9.41774525e-01,  1.81853256e+00, -2.20882493e-01,<br>\n3.01935183e+00, -6.22724692e-01,  6.57625596e-01,<br>\n3.85760589e-01,  1.25143595e+00,  3.95787835e-01,<br>\n-1.15711379e-01,  1.04277452e+00,  1.18388115e-01,<br>\n-2.67029800e+00,  4.53791661e-01,  1.21855412e+00,<br>\n1.35663694e+00,  1.11392313e+00, -6.14336793e-01,<br>\n2.90739971e-01,  1.37860208e+00,  2.52123588e+00,<br>\n-8.47994930e-01, -1.57757604e-01, -3.22334965e-01,<br>\n-5.11127934e-01,  1.17326724e+00, -1.33463413e-01,<br>\n1.53402616e+00,  9.27541940e-01, -1.65259834e+00,<br>\n1.52193955e+00, -7.02834497e-01,  1.30055130e+00,<br>\n-1.27054858e+00,  1.93102289e+00,  4.90642813e-01,<br>\n1.05364636e+00],<br>\n[-2.82434425e-01,  4.84068350e-01, -1.54210849e+00,<br>\n2.17935455e+00, -1.33159023e+00,  9.24332614e-01,<br>\n-5.04315970e-01,  9.85161964e-01,  8.72890896e-01,<br>\n9.33685162e-01,  2.29173759e-01,  1.80474211e+00,<br>\n7.72485270e-01,  1.11224300e-01, -7.35284649e-01,<br>\n1.22797144e-01,  2.49131972e-02, -8.82123778e-01,<br>\n1.05901313e+00, -3.91695444e-01,  1.52364293e+00,<br>\n-2.02547241e+00, -1.11905764e+00, -1.39702563e-01,<br>\n6.00853245e-01,  1.06457879e+00,  7.23094038e-01,<br>\n2.30281455e-01,  2.85937688e-02,  1.88589155e-02,<br>\n6.45507430e-01, -5.51370987e-02, -1.68287628e+00,<br>\n-3.59386792e-01,  2.64980652e-01, -4.73594067e-01,<br>\n-1.16968518e+00,  6.28055865e-01,  2.84448999e+00,<br>\n-8.13157852e-01],<br>\n[-1.31717291e-01,  1.02786130e+00,  1.53418718e-01,<br>\n-1.97088279e-01,  1.25098576e+00, -3.64066728e-01,<br>\n-1.31257463e-01,  1.64076530e+00, -2.15908745e+00,<br>\n-6.22189979e-01, -2.53163951e-02,  1.44229587e+00,<br>\n1.42744437e+00,  1.17886987e+00, -1.37219218e+00,<br>\n1.95677402e+00,  1.25885663e-01, -1.74788095e-01,<br>\n1.49759607e+00,  1.69086673e+00,  1.70750645e+00,<br>\n8.23124033e-03, -6.57806464e-01, -6.86276358e-01,<br>\n2.76553853e-01, -5.89633972e-01,  4.83433320e-01,<br>\n-4.78963168e-01,  1.57649031e+00, -8.97853620e-01,<br>\n1.12044567e-01, -6.14123732e-01, -2.61661823e-01,<br>\n1.08654068e+00, -2.13519684e+00, -2.20165083e+00,<br>\n-1.79124609e+00,  1.66096352e+00,  1.01991370e+00,<br>\n-6.31529272e-01],<br>\n[-9.41644716e-01, -8.22052119e-02, -7.82717117e-01,<br>\n4.66941465e-01, -3.41506828e-01,  2.85859511e-01,<br>\n-5.39347861e-01,  4.83069925e-01, -1.96077011e+00,<br>\n-1.79893071e+00,  4.02349515e-01,  2.19665512e-01,<br>\n4.81469059e-01, -9.67396064e-01,  1.28371488e+00,<br>\n1.74737441e-01, -1.42200749e+00, -1.02551183e+00,<br>\n9.19474529e-01,  1.10227219e+00,  2.39780104e-01,<br>\n1.54564488e+00,  5.48480031e-01,  2.53774766e-01,<br>\n-1.05690541e+00,  2.04109323e+00, -8.91820616e-01,<br>\n-9.67841805e-01,  6.84840673e-02, -2.64095272e+00,<br>\n-9.63133723e-01,  1.27685637e+00,  6.88337275e-01,<br>\n2.05600684e+00, -1.62778234e+00, -3.23933044e-01,<br>\n-3.16615539e-01, -6.67369617e-01, -1.15297191e+00,<br>\n-5.91319319e-01],<br>\n[ 1.91052422e+00,  6.33272998e-01,  8.26497424e-01,<br>\n6.03023598e-01,  3.92400071e-01,  9.29020785e-01,<br>\n-6.88585491e-01,  9.77940308e-01, -1.26848693e+00,<br>\n9.94840201e-02,  7.52848243e-01, -3.04239973e-01,<br>\n3.67167359e-01,  5.69559156e-01, -6.36591114e-01,<br>\n1.11083130e+00, -8.39891783e-01,  4.10057372e-01,<br>\n-1.38307188e-01,  3.90518293e-01, -5.50275734e-01,<br>\n-8.79075363e-01, -1.66958109e-01,  6.56225679e-01,<br>\n1.95612624e+00, -4.45663548e-01, -1.09293024e-01,<br>\n-2.01618327e+00,  4.58819223e-01,  9.22194083e-01,<br>\n9.95847584e-01,  5.23072609e-02,  1.24794740e+00,<br>\n-4.53764894e-02,  2.95947625e-01, -7.41021448e-01,<br>\n2.50512089e+00,  9.61431201e-01,  1.69506349e+00,<br>\n6.95733555e-01],<br>\n[ 1.16595131e+00, -9.16556527e-01,  1.58695646e+00,<br>\n1.53449732e-01,  1.17051377e-01, -4.14580879e-01,<br>\n-5.97632083e-01, -9.09788623e-02,  2.06036713e-01,<br>\n-1.68144113e+00, -1.44447690e+00,  6.33471999e-01,<br>\n1.67047115e+00,  1.32731338e+00, -1.04672470e+00,<br>\n4.54657832e-01, -2.23668935e-01,  6.40420094e-01,<br>\n-2.76215451e-01,  1.29460409e+00, -1.38940318e-01,<br>\n-1.04813960e+00, -1.47631494e+00,  4.58803440e-01,<br>\n5.98103656e-01, -6.04906161e-01, -5.70246924e-02,<br>\n1.42080757e+00, -1.97360377e+00, -7.95265695e-01,<br>\n2.94820707e-01, -4.83390366e-01,  4.50911220e-01,<br>\n8.74186348e-01, -5.99966992e-01, -1.21949591e-01,<br>\n-2.52309447e-01,  1.53064407e+00,  3.72279673e-01,<br>\n1.07315179e+00],<br>\n[ 8.86889539e-01,  1.16549545e-01,  1.38816289e+00,<br>\n1.01371089e+00, -1.44051110e+00, -3.53644010e-01,<br>\n-3.65782677e-01,  6.13860886e-02, -3.14833634e-01,<br>\n2.59271600e+00,  8.16096390e-01,  1.50172499e-01,<br>\n5.30068573e-01, -7.30386914e-01, -2.03084764e-01,<br>\n-6.74878994e-01,  9.66930750e-02,  7.21899634e-01,<br>\n-1.99889298e-01,  1.64892187e-02, -1.19705017e+00,<br>\n-1.59441455e+00,  6.20422424e-01, -2.18686254e-01,<br>\n1.12343324e+00,  1.86378524e+00, -1.83527953e-01,<br>\n-1.00652180e+00, -1.65150954e-01,  4.72737655e-02,<br>\n4.04227537e-01,  7.75649967e-01, -1.67503021e+00,<br>\n1.58444824e+00, -5.54640787e-01, -9.06882661e-01,<br>\n-8.85061481e-01, -2.57646814e-01, -1.58392420e-01,<br>\n-8.72515024e-01],<br>\n[ 2.30347997e-01,  2.93106723e+00,  2.32079761e+00,<br>\n1.05747984e-02,  6.43344453e-01,  1.48450845e+00,<br>\n-1.32288375e+00, -3.35875203e-02, -1.53430836e-01,<br>\n1.13107152e+00,  2.71825618e-01, -1.70896982e+00,<br>\n6.73988768e-01, -1.52623124e+00,  8.51444615e-01,<br>\n9.58495328e-01,  1.13117333e+00,  1.57066734e-01,<br>\n-2.49850210e-01, -3.57635309e-01,  6.84622356e-01,<br>\n2.02666474e-01, -7.12002046e-01,  1.24946477e-02,<br>\n5.75374743e-01,  1.93356031e-01, -1.29900584e+00,<br>\n5.04674883e-02,  1.22158010e+00,  1.63139677e+00,<br>\n-2.49405615e-01,  1.82925921e+00,  3.54699037e-01,<br>\n9.97620566e-01,  9.85580452e-02,  1.11274364e-01,<br>\n-6.00244704e-01, -1.95023124e-01,  2.00273727e-01,<br>\n1.83794424e+00],<br>\n[ 7.09088942e-01, -1.04879043e+00, -5.74103670e-01,<br>\n7.43992315e-01,  1.52200161e+00, -2.03207326e+00,<br>\n2.11875292e-01,  1.04272441e+00, -1.26958291e+00,<br>\n2.70366571e-01, -2.19008125e-01,  1.40483006e+00,<br>\n-2.09482711e-01, -9.20875207e-01,  1.72865711e+00,<br>\n-2.64627344e-01,  3.47781018e-01,  4.81586537e-01,<br>\n1.07322305e+00, -8.17717513e-01,  1.28391605e+00,<br>\n1.97842517e-01,  1.72612806e+00,  2.13807753e-01,<br>\n1.03357557e+00,  5.84119555e-01, -8.61174988e-01,<br>\n-1.17678694e+00,  1.92928507e+00,  5.60831434e-01,<br>\n3.31615685e-01,  6.15759349e-02, -9.18765403e-02,<br>\n-1.25591199e+00, -1.69617993e+00, -1.04056449e-03,<br>\n-1.02922137e-01, -6.70639273e-01,  1.16609151e-01,<br>\n1.10320101e+00],<br>\n[ 3.14600442e-01, -4.65363105e-01,  1.67887551e+00,<br>\n-6.92450808e-01,  3.14357667e-01,  1.32540623e-01,<br>\n9.15669217e-01, -1.39204004e+00,  7.20252339e-01,<br>\n5.94095417e-01, -8.18707441e-02,  5.91751351e-01,<br>\n-6.68691321e-01,  4.96172554e-01,  5.29582332e-01,<br>\n-1.55275307e-01,  2.46515467e-02,  6.03194472e-01,<br>\n1.31759695e+00,  5.45525211e-01, -2.30398220e-02,<br>\n8.12616540e-01, -2.61076466e-01,  1.66934365e+00,<br>\n-8.31025772e-01, -2.00446090e-01,  2.91231454e-01,<br>\n7.98223470e-01, -9.60227710e-01,  1.90107485e+00,<br>\n-7.44250114e-01, -7.20353857e-01,  1.32092989e-01,<br>\n8.44143593e-01,  5.01753704e-01, -2.05186337e-02,<br>\n1.13485011e+00, -1.06831952e-01,  2.44283342e-01,<br>\n-2.09994058e+00],<br>\n[-6.29958586e-01,  5.63142982e-01, -3.90122789e-01,<br>\n1.63745415e+00,  6.94110430e-01,  8.22926404e-02,<br>\n1.95863036e-01, -1.38925172e-01,  2.25575648e-01,<br>\n8.12663637e-01,  2.17542039e-01, -4.50958688e-01,<br>\n-1.01270050e+00,  3.51468797e-01, -1.67982586e+00,<br>\n1.07461121e+00,  6.99860539e-01,  4.94039591e-01,<br>\n-5.90147661e-01, -1.83109452e+00,  1.03962324e+00,<br>\n1.15843905e+00, -3.49911595e-01,  7.62128043e-01,<br>\n-1.67721454e+00,  3.76573218e-01, -7.48380931e-01,<br>\n-1.00878241e+00,  1.25593116e+00,  1.18679312e+00,<br>\n-1.05711288e+00, -4.00083667e-01, -1.10232880e+00,<br>\n1.21308371e+00, -4.47723244e-01, -2.68600107e-01,<br>\n1.26868008e+00,  8.23863001e-01,  5.47930794e-01,<br>\n2.77146607e-01],<br>\n[ 8.58208487e-02,  1.56905645e+00,  1.82461216e+00,<br>\n-1.95056238e+00,  8.84093358e-01, -2.30546309e-01,<br>\n1.68715760e+00,  1.51402049e+00,  1.97083952e+00,<br>\n3.47050464e-01,  2.45418091e+00, -1.28676786e+00,<br>\n-3.13972072e-02, -9.15640304e-02, -6.08760602e-01,<br>\n6.27315932e-01,  2.99423764e-01,  1.47387084e+00,<br>\n-2.22367289e-01,  1.36963199e+00, -1.03270696e+00,<br>\n5.07323436e-01,  1.03998597e+00,  7.48829007e-01,<br>\n5.56559176e-01,  8.65196651e-01, -9.69346627e-01,<br>\n-3.61670544e-02,  9.58567659e-01, -3.05810054e-01,<br>\n1.57696086e+00,  6.16235833e-01, -1.05029472e+00,<br>\n4.44189207e-01, -2.39608656e+00,  5.52516728e-01,<br>\n1.41880306e+00,  8.11277537e-01,  4.57157626e-01,<br>\n-3.63861533e-01],<br>\n[-5.50649441e-01,  6.17569991e-01, -6.07970309e-01,<br>\n4.38174300e-01, -1.35517086e+00,  2.25004668e-02,<br>\n-3.37577140e-02,  1.06195204e-01,  1.28345880e+00,<br>\n-3.46079223e-01, -1.53264926e-01,  1.22802991e+00,<br>\n1.22813763e-01,  2.24233889e+00, -5.45865371e-01,<br>\n-6.42548620e-01, -5.81109641e-01, -9.70743156e-01,<br>\n-1.80179925e+00, -1.11872377e+00, -2.22970191e+00,<br>\n4.94857288e-01, -5.86297124e-01, -6.56848913e-01,<br>\n-1.17092937e-03, -7.39394009e-01,  1.19986436e+00,<br>\n3.96717450e-01, -1.24115664e-01,  1.00324288e+00,<br>\n-1.14458800e-01, -2.25008521e+00,  5.45973540e-01,<br>\n2.37025750e-01, -1.14356885e+00, -4.88471919e-01,<br>\n-1.13206828e+00, -1.62136190e+00,  1.23749918e-01,<br>\n3.08363831e-01],<br>\n[-2.36785516e+00,  1.93758109e-01,  9.52158319e-01,<br>\n1.74236299e-01, -1.37006190e+00, -3.20627648e-01,<br>\n2.16886312e+00, -8.07024449e-01, -9.12120118e-01,<br>\n-2.45296765e+00,  1.09160450e+00, -1.00823007e+00,<br>\n-1.08279055e-01, -3.37003362e-01,  8.89784314e-01,<br>\n4.28913333e-01, -1.31532876e+00,  1.99979263e-01,<br>\n-2.95699114e-01, -1.07325068e+00, -7.65415033e-01,<br>\n-7.57991272e-01,  1.58699077e+00, -6.27470343e-01,<br>\n4.39592393e-01,  1.19306894e+00, -1.41762289e+00,<br>\n-1.92094209e-01,  4.98275997e-01,  4.75333804e-01,<br>\n2.42416125e-01,  3.97092492e-02,  2.18313764e-01,<br>\n9.20630155e-01,  1.32786770e-01,  1.07811778e+00,<br>\n-1.56297503e-01,  1.84818124e-01,  6.83802272e-01,<br>\n-1.76218224e+00]]) of type 'ndarray' instead.</p>\n<p>Can anybody tell why this error is coming, the size of numpy arrays are correct.</p>", "body_text": "@davek44 I am trying to initialize the cell with a constant initializer but it is giving en error. The code which I'm using is -\nf = [np.random.normal(size = [15, 40]), np.random.normal(size = [40,])]\ninit = tf.constant_initializer(f, verify_shape = True, dtype = tf.float32)\ncell = tf.contrib.rnn.LSTMCell(lstm_units, initializer = init)\ninitial_state = cell.zero_state(2, dtype = tf.float32)\nunused_encoder_outputs, encoder_state =tf.nn.dynamic_rnn(cell ,source_seq_embedded,sequence_length=source_seq_len,dtype = tf.float32, initial_state = initial_state)\nIt is giving error -\nTypeError                                 Traceback (most recent call last)\n in ()\n29 initial_state = cell.zero_state(2, dtype = tf.float32)\n30 unused_encoder_outputs, encoder_state =tf.nn.dynamic_rnn(cell ,source_seq_embedded,sequence_length=source_seq_len,\n---> 31                                                         dtype = tf.float32, initial_state = initial_state)\n32\n33 sampling_prob = tf.Variable(0.0, dtype=tf.float32)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\n625         swap_memory=swap_memory,\n626         sequence_length=sequence_length,\n--> 627         dtype=dtype)\n628\n629     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\n822       parallel_iterations=parallel_iterations,\n823       maximum_iterations=time_steps,\n--> 824       swap_memory=swap_memory)\n825\n826   # Unpack final output if not using output tuples.\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\n3222     if loop_context.outer_context is None:\n3223       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\n-> 3224     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n3225     if maximum_iterations is not None:\n3226       return result[1]\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\n2954       with ops.get_default_graph()._lock:  # pylint: disable=protected-access\n2955         original_body_result, exit_vars = self._BuildLoop(\n-> 2956             pred, body, original_loop_vars, loop_vars, shape_invariants)\n2957     finally:\n2958       self.Exit()\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n2891         flat_sequence=vars_for_body_with_tensor_arrays)\n2892     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\n-> 2893     body_result = body(*packed_vars_for_body)\n2894     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\n2895     if not nest.is_sequence(body_result):\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in (i, lv)\n3192         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n3193             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n-> 3194         body = lambda i, lv: (i + 1, orig_body(*lv))\n3195\n3196     if context.executing_eagerly():\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in _time_step(time, output_ta_t, state)\n791           call_cell=call_cell,\n792           state_size=state_size,\n--> 793           skip_conditionals=True)\n794     else:\n795       (output, new_state) = call_cell()\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in _rnn_step(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\n246     # steps.  This is faster when max_seq_len is equal to the number of unrolls\n247     # (which is typical for dynamic_rnn).\n--> 248     new_output, new_state = call_cell()\n249     nest.assert_same_structure(state, new_state)\n250     new_state = nest.flatten(new_state)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in ()\n779\n780     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n--> 781     call_cell = lambda: cell(input_t, state)\n782\n783     if sequence_length is not None:\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state, scope, *args, **kwargs)\n337     # method.  See the class docstring for more details.\n338     return base_layer.Layer.call(self, inputs, state, scope=scope,\n--> 339                                      *args, **kwargs)\n340\n341\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in call(self, inputs, *args, **kwargs)\n697           if all(hasattr(x, 'get_shape') for x in input_list):\n698             input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs)\n--> 699           self.build(input_shapes)\n700         try:\n701           # Note: not all sub-classes of Layer call Layer.init (especially\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in build(self, inputs_shape)\n765         shape=[input_depth + h_depth, 4 * self._num_units],\n766         initializer=self._initializer,\n--> 767         partitioner=maybe_partitioner)\n768     self._bias = self.add_variable(\n769         _BIAS_VARIABLE_NAME,\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in add_variable(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\n544             constraint=constraint,\n545             trainable=trainable and self.trainable,\n--> 546             partitioner=partitioner)\n547\n548         if init_graph is not None:  # pylint: disable=protected-access\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\n434     new_variable = getter(\n435         name=name, shape=shape, dtype=dtype, initializer=initializer,\n--> 436         **kwargs_for_getter)\n437\n438     # If we set an initializer and the variable processed it, tracking will not\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n1315       partitioner=partitioner, validate_shape=validate_shape,\n1316       use_resource=use_resource, custom_getter=custom_getter,\n-> 1317       constraint=constraint)\n1318 get_variable_or_local_docstring = (\n1319     \"\"\"%s\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n1077           partitioner=partitioner, validate_shape=validate_shape,\n1078           use_resource=use_resource, custom_getter=custom_getter,\n-> 1079           constraint=constraint)\n1080\n1081   def _get_partitioned_variable(self,\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n423           caching_device=caching_device, partitioner=partitioner,\n424           validate_shape=validate_shape, use_resource=use_resource,\n--> 425           constraint=constraint)\n426\n427   def _get_partitioned_variable(\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\n392           trainable=trainable, collections=collections,\n393           caching_device=caching_device, validate_shape=validate_shape,\n--> 394           use_resource=use_resource, constraint=constraint)\n395\n396     if custom_getter is not None:\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\n784         validate_shape=validate_shape,\n785         constraint=constraint,\n--> 786         use_resource=use_resource)\n787     if not context.executing_eagerly() or self._store_eager_variables:\n788       # In eager mode we do not want to keep default references to Variable\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource)\n2218                          name=name, dtype=dtype,\n2219                          constraint=constraint,\n-> 2220                          use_resource=use_resource)\n2221\n2222\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in (**kwargs)\n2208              constraint=None,\n2209              use_resource=None):\n-> 2210   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n2211   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\n2212     previous_getter = _make_getter(getter, previous_getter)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\n2191         collections=collections, validate_shape=validate_shape,\n2192         caching_device=caching_device, name=name, dtype=dtype,\n-> 2193         constraint=constraint)\n2194\n2195\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in init(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\n233           dtype=dtype,\n234           expected_shape=expected_shape,\n--> 235           constraint=constraint)\n236\n237   def repr(self):\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\n341             with ops.name_scope(\"Initializer\"), ops.device(None):\n342               self._initial_value = ops.convert_to_tensor(\n--> 343                   initial_value(), name=\"initial_value\", dtype=dtype)\n344               shape = (self._initial_value.get_shape()\n345                        if validate_shape else tensor_shape.unknown_shape())\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in ()\n768           initializer = initializer(dtype=dtype)\n769         init_val = lambda: initializer(  # pylint: disable=g-long-lambda\n--> 770             shape.as_list(), dtype=dtype, partition_info=partition_info)\n771         variable_dtype = dtype.base_dtype\n772\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py in call(self, shape, dtype, partition_info, verify_shape)\n215       verify_shape = self._verify_shape\n216     return constant_op.constant(\n--> 217         self.value, dtype=dtype, shape=shape, verify_shape=verify_shape)\n218\n219   def get_config(self):\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\n212   tensor_value.tensor.CopyFrom(\n213       tensor_util.make_tensor_proto(\n--> 214           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n215   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\n216   const_tensor = g.create_op(\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\n430       nparray = np.empty(shape, dtype=np_dt)\n431     else:\n--> 432       _AssertCompatible(values, dtype)\n433       nparray = np.array(values, dtype=np_dt)\n434       # check to them.\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)\n341     else:\n342       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n--> 343                       (dtype.name, repr(mismatch), type(mismatch).name))\n344\n345\nTypeError: Expected float32, got array([[ 1.04484751e+00,  4.59392878e-02,  8.02706034e-01,\n-1.11174105e+00,  8.89477561e-01, -1.20486729e+00,\n8.86266342e-01, -1.64510319e+00, -9.89266584e-01,\n1.31369015e+00,  4.88107334e-01,  1.49676477e-01,\n-6.81036506e-01, -9.64134982e-02, -1.39690939e+00,\n7.25600555e-02, -9.32924292e-02, -7.95291205e-01,\n2.18235671e-01,  1.01727717e+00,  9.90535181e-01,\n-4.35980568e-01,  7.94711686e-01, -3.13013630e-02,\n-7.20243003e-01, -1.37384000e+00, -9.62151398e-01,\n1.07646743e+00,  4.69513293e-01, -4.47742163e-02,\n-4.95195536e-01,  6.76953787e-01,  2.39035230e-01,\n-1.11104812e-01,  2.05507744e+00,  9.33967217e-01,\n-8.96051458e-01, -8.77908566e-02,  1.43848201e+00,\n-1.04624360e+00],\n[-3.41335190e-01,  1.40035909e+00, -1.60449447e-01,\n-9.41774525e-01,  1.81853256e+00, -2.20882493e-01,\n3.01935183e+00, -6.22724692e-01,  6.57625596e-01,\n3.85760589e-01,  1.25143595e+00,  3.95787835e-01,\n-1.15711379e-01,  1.04277452e+00,  1.18388115e-01,\n-2.67029800e+00,  4.53791661e-01,  1.21855412e+00,\n1.35663694e+00,  1.11392313e+00, -6.14336793e-01,\n2.90739971e-01,  1.37860208e+00,  2.52123588e+00,\n-8.47994930e-01, -1.57757604e-01, -3.22334965e-01,\n-5.11127934e-01,  1.17326724e+00, -1.33463413e-01,\n1.53402616e+00,  9.27541940e-01, -1.65259834e+00,\n1.52193955e+00, -7.02834497e-01,  1.30055130e+00,\n-1.27054858e+00,  1.93102289e+00,  4.90642813e-01,\n1.05364636e+00],\n[-2.82434425e-01,  4.84068350e-01, -1.54210849e+00,\n2.17935455e+00, -1.33159023e+00,  9.24332614e-01,\n-5.04315970e-01,  9.85161964e-01,  8.72890896e-01,\n9.33685162e-01,  2.29173759e-01,  1.80474211e+00,\n7.72485270e-01,  1.11224300e-01, -7.35284649e-01,\n1.22797144e-01,  2.49131972e-02, -8.82123778e-01,\n1.05901313e+00, -3.91695444e-01,  1.52364293e+00,\n-2.02547241e+00, -1.11905764e+00, -1.39702563e-01,\n6.00853245e-01,  1.06457879e+00,  7.23094038e-01,\n2.30281455e-01,  2.85937688e-02,  1.88589155e-02,\n6.45507430e-01, -5.51370987e-02, -1.68287628e+00,\n-3.59386792e-01,  2.64980652e-01, -4.73594067e-01,\n-1.16968518e+00,  6.28055865e-01,  2.84448999e+00,\n-8.13157852e-01],\n[-1.31717291e-01,  1.02786130e+00,  1.53418718e-01,\n-1.97088279e-01,  1.25098576e+00, -3.64066728e-01,\n-1.31257463e-01,  1.64076530e+00, -2.15908745e+00,\n-6.22189979e-01, -2.53163951e-02,  1.44229587e+00,\n1.42744437e+00,  1.17886987e+00, -1.37219218e+00,\n1.95677402e+00,  1.25885663e-01, -1.74788095e-01,\n1.49759607e+00,  1.69086673e+00,  1.70750645e+00,\n8.23124033e-03, -6.57806464e-01, -6.86276358e-01,\n2.76553853e-01, -5.89633972e-01,  4.83433320e-01,\n-4.78963168e-01,  1.57649031e+00, -8.97853620e-01,\n1.12044567e-01, -6.14123732e-01, -2.61661823e-01,\n1.08654068e+00, -2.13519684e+00, -2.20165083e+00,\n-1.79124609e+00,  1.66096352e+00,  1.01991370e+00,\n-6.31529272e-01],\n[-9.41644716e-01, -8.22052119e-02, -7.82717117e-01,\n4.66941465e-01, -3.41506828e-01,  2.85859511e-01,\n-5.39347861e-01,  4.83069925e-01, -1.96077011e+00,\n-1.79893071e+00,  4.02349515e-01,  2.19665512e-01,\n4.81469059e-01, -9.67396064e-01,  1.28371488e+00,\n1.74737441e-01, -1.42200749e+00, -1.02551183e+00,\n9.19474529e-01,  1.10227219e+00,  2.39780104e-01,\n1.54564488e+00,  5.48480031e-01,  2.53774766e-01,\n-1.05690541e+00,  2.04109323e+00, -8.91820616e-01,\n-9.67841805e-01,  6.84840673e-02, -2.64095272e+00,\n-9.63133723e-01,  1.27685637e+00,  6.88337275e-01,\n2.05600684e+00, -1.62778234e+00, -3.23933044e-01,\n-3.16615539e-01, -6.67369617e-01, -1.15297191e+00,\n-5.91319319e-01],\n[ 1.91052422e+00,  6.33272998e-01,  8.26497424e-01,\n6.03023598e-01,  3.92400071e-01,  9.29020785e-01,\n-6.88585491e-01,  9.77940308e-01, -1.26848693e+00,\n9.94840201e-02,  7.52848243e-01, -3.04239973e-01,\n3.67167359e-01,  5.69559156e-01, -6.36591114e-01,\n1.11083130e+00, -8.39891783e-01,  4.10057372e-01,\n-1.38307188e-01,  3.90518293e-01, -5.50275734e-01,\n-8.79075363e-01, -1.66958109e-01,  6.56225679e-01,\n1.95612624e+00, -4.45663548e-01, -1.09293024e-01,\n-2.01618327e+00,  4.58819223e-01,  9.22194083e-01,\n9.95847584e-01,  5.23072609e-02,  1.24794740e+00,\n-4.53764894e-02,  2.95947625e-01, -7.41021448e-01,\n2.50512089e+00,  9.61431201e-01,  1.69506349e+00,\n6.95733555e-01],\n[ 1.16595131e+00, -9.16556527e-01,  1.58695646e+00,\n1.53449732e-01,  1.17051377e-01, -4.14580879e-01,\n-5.97632083e-01, -9.09788623e-02,  2.06036713e-01,\n-1.68144113e+00, -1.44447690e+00,  6.33471999e-01,\n1.67047115e+00,  1.32731338e+00, -1.04672470e+00,\n4.54657832e-01, -2.23668935e-01,  6.40420094e-01,\n-2.76215451e-01,  1.29460409e+00, -1.38940318e-01,\n-1.04813960e+00, -1.47631494e+00,  4.58803440e-01,\n5.98103656e-01, -6.04906161e-01, -5.70246924e-02,\n1.42080757e+00, -1.97360377e+00, -7.95265695e-01,\n2.94820707e-01, -4.83390366e-01,  4.50911220e-01,\n8.74186348e-01, -5.99966992e-01, -1.21949591e-01,\n-2.52309447e-01,  1.53064407e+00,  3.72279673e-01,\n1.07315179e+00],\n[ 8.86889539e-01,  1.16549545e-01,  1.38816289e+00,\n1.01371089e+00, -1.44051110e+00, -3.53644010e-01,\n-3.65782677e-01,  6.13860886e-02, -3.14833634e-01,\n2.59271600e+00,  8.16096390e-01,  1.50172499e-01,\n5.30068573e-01, -7.30386914e-01, -2.03084764e-01,\n-6.74878994e-01,  9.66930750e-02,  7.21899634e-01,\n-1.99889298e-01,  1.64892187e-02, -1.19705017e+00,\n-1.59441455e+00,  6.20422424e-01, -2.18686254e-01,\n1.12343324e+00,  1.86378524e+00, -1.83527953e-01,\n-1.00652180e+00, -1.65150954e-01,  4.72737655e-02,\n4.04227537e-01,  7.75649967e-01, -1.67503021e+00,\n1.58444824e+00, -5.54640787e-01, -9.06882661e-01,\n-8.85061481e-01, -2.57646814e-01, -1.58392420e-01,\n-8.72515024e-01],\n[ 2.30347997e-01,  2.93106723e+00,  2.32079761e+00,\n1.05747984e-02,  6.43344453e-01,  1.48450845e+00,\n-1.32288375e+00, -3.35875203e-02, -1.53430836e-01,\n1.13107152e+00,  2.71825618e-01, -1.70896982e+00,\n6.73988768e-01, -1.52623124e+00,  8.51444615e-01,\n9.58495328e-01,  1.13117333e+00,  1.57066734e-01,\n-2.49850210e-01, -3.57635309e-01,  6.84622356e-01,\n2.02666474e-01, -7.12002046e-01,  1.24946477e-02,\n5.75374743e-01,  1.93356031e-01, -1.29900584e+00,\n5.04674883e-02,  1.22158010e+00,  1.63139677e+00,\n-2.49405615e-01,  1.82925921e+00,  3.54699037e-01,\n9.97620566e-01,  9.85580452e-02,  1.11274364e-01,\n-6.00244704e-01, -1.95023124e-01,  2.00273727e-01,\n1.83794424e+00],\n[ 7.09088942e-01, -1.04879043e+00, -5.74103670e-01,\n7.43992315e-01,  1.52200161e+00, -2.03207326e+00,\n2.11875292e-01,  1.04272441e+00, -1.26958291e+00,\n2.70366571e-01, -2.19008125e-01,  1.40483006e+00,\n-2.09482711e-01, -9.20875207e-01,  1.72865711e+00,\n-2.64627344e-01,  3.47781018e-01,  4.81586537e-01,\n1.07322305e+00, -8.17717513e-01,  1.28391605e+00,\n1.97842517e-01,  1.72612806e+00,  2.13807753e-01,\n1.03357557e+00,  5.84119555e-01, -8.61174988e-01,\n-1.17678694e+00,  1.92928507e+00,  5.60831434e-01,\n3.31615685e-01,  6.15759349e-02, -9.18765403e-02,\n-1.25591199e+00, -1.69617993e+00, -1.04056449e-03,\n-1.02922137e-01, -6.70639273e-01,  1.16609151e-01,\n1.10320101e+00],\n[ 3.14600442e-01, -4.65363105e-01,  1.67887551e+00,\n-6.92450808e-01,  3.14357667e-01,  1.32540623e-01,\n9.15669217e-01, -1.39204004e+00,  7.20252339e-01,\n5.94095417e-01, -8.18707441e-02,  5.91751351e-01,\n-6.68691321e-01,  4.96172554e-01,  5.29582332e-01,\n-1.55275307e-01,  2.46515467e-02,  6.03194472e-01,\n1.31759695e+00,  5.45525211e-01, -2.30398220e-02,\n8.12616540e-01, -2.61076466e-01,  1.66934365e+00,\n-8.31025772e-01, -2.00446090e-01,  2.91231454e-01,\n7.98223470e-01, -9.60227710e-01,  1.90107485e+00,\n-7.44250114e-01, -7.20353857e-01,  1.32092989e-01,\n8.44143593e-01,  5.01753704e-01, -2.05186337e-02,\n1.13485011e+00, -1.06831952e-01,  2.44283342e-01,\n-2.09994058e+00],\n[-6.29958586e-01,  5.63142982e-01, -3.90122789e-01,\n1.63745415e+00,  6.94110430e-01,  8.22926404e-02,\n1.95863036e-01, -1.38925172e-01,  2.25575648e-01,\n8.12663637e-01,  2.17542039e-01, -4.50958688e-01,\n-1.01270050e+00,  3.51468797e-01, -1.67982586e+00,\n1.07461121e+00,  6.99860539e-01,  4.94039591e-01,\n-5.90147661e-01, -1.83109452e+00,  1.03962324e+00,\n1.15843905e+00, -3.49911595e-01,  7.62128043e-01,\n-1.67721454e+00,  3.76573218e-01, -7.48380931e-01,\n-1.00878241e+00,  1.25593116e+00,  1.18679312e+00,\n-1.05711288e+00, -4.00083667e-01, -1.10232880e+00,\n1.21308371e+00, -4.47723244e-01, -2.68600107e-01,\n1.26868008e+00,  8.23863001e-01,  5.47930794e-01,\n2.77146607e-01],\n[ 8.58208487e-02,  1.56905645e+00,  1.82461216e+00,\n-1.95056238e+00,  8.84093358e-01, -2.30546309e-01,\n1.68715760e+00,  1.51402049e+00,  1.97083952e+00,\n3.47050464e-01,  2.45418091e+00, -1.28676786e+00,\n-3.13972072e-02, -9.15640304e-02, -6.08760602e-01,\n6.27315932e-01,  2.99423764e-01,  1.47387084e+00,\n-2.22367289e-01,  1.36963199e+00, -1.03270696e+00,\n5.07323436e-01,  1.03998597e+00,  7.48829007e-01,\n5.56559176e-01,  8.65196651e-01, -9.69346627e-01,\n-3.61670544e-02,  9.58567659e-01, -3.05810054e-01,\n1.57696086e+00,  6.16235833e-01, -1.05029472e+00,\n4.44189207e-01, -2.39608656e+00,  5.52516728e-01,\n1.41880306e+00,  8.11277537e-01,  4.57157626e-01,\n-3.63861533e-01],\n[-5.50649441e-01,  6.17569991e-01, -6.07970309e-01,\n4.38174300e-01, -1.35517086e+00,  2.25004668e-02,\n-3.37577140e-02,  1.06195204e-01,  1.28345880e+00,\n-3.46079223e-01, -1.53264926e-01,  1.22802991e+00,\n1.22813763e-01,  2.24233889e+00, -5.45865371e-01,\n-6.42548620e-01, -5.81109641e-01, -9.70743156e-01,\n-1.80179925e+00, -1.11872377e+00, -2.22970191e+00,\n4.94857288e-01, -5.86297124e-01, -6.56848913e-01,\n-1.17092937e-03, -7.39394009e-01,  1.19986436e+00,\n3.96717450e-01, -1.24115664e-01,  1.00324288e+00,\n-1.14458800e-01, -2.25008521e+00,  5.45973540e-01,\n2.37025750e-01, -1.14356885e+00, -4.88471919e-01,\n-1.13206828e+00, -1.62136190e+00,  1.23749918e-01,\n3.08363831e-01],\n[-2.36785516e+00,  1.93758109e-01,  9.52158319e-01,\n1.74236299e-01, -1.37006190e+00, -3.20627648e-01,\n2.16886312e+00, -8.07024449e-01, -9.12120118e-01,\n-2.45296765e+00,  1.09160450e+00, -1.00823007e+00,\n-1.08279055e-01, -3.37003362e-01,  8.89784314e-01,\n4.28913333e-01, -1.31532876e+00,  1.99979263e-01,\n-2.95699114e-01, -1.07325068e+00, -7.65415033e-01,\n-7.57991272e-01,  1.58699077e+00, -6.27470343e-01,\n4.39592393e-01,  1.19306894e+00, -1.41762289e+00,\n-1.92094209e-01,  4.98275997e-01,  4.75333804e-01,\n2.42416125e-01,  3.97092492e-02,  2.18313764e-01,\n9.20630155e-01,  1.32786770e-01,  1.07811778e+00,\n-1.56297503e-01,  1.84818124e-01,  6.83802272e-01,\n-1.76218224e+00]]) of type 'ndarray' instead.\nCan anybody tell why this error is coming, the size of numpy arrays are correct.", "body": "@davek44 I am trying to initialize the cell with a constant initializer but it is giving en error. The code which I'm using is -  \r\nf = [np.random.normal(size = [15, 40]), np.random.normal(size = [40,])]\r\ninit = tf.constant_initializer(f, verify_shape = True, dtype = tf.float32)\r\n\r\ncell = tf.contrib.rnn.LSTMCell(lstm_units, initializer = init)\r\ninitial_state = cell.zero_state(2, dtype = tf.float32)\r\nunused_encoder_outputs, encoder_state =tf.nn.dynamic_rnn(cell ,source_seq_embedded,sequence_length=source_seq_len,dtype = tf.float32, initial_state = initial_state)\r\n\r\nIt is giving error - \r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-445bac49bd02> in <module>()\r\n     29 initial_state = cell.zero_state(2, dtype = tf.float32)\r\n     30 unused_encoder_outputs, encoder_state =tf.nn.dynamic_rnn(cell ,source_seq_embedded,sequence_length=source_seq_len,\r\n---> 31                                                         dtype = tf.float32, initial_state = initial_state)\r\n     32 \r\n     33 sampling_prob = tf.Variable(0.0, dtype=tf.float32)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    625         swap_memory=swap_memory,\r\n    626         sequence_length=sequence_length,\r\n--> 627         dtype=dtype)\r\n    628 \r\n    629     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\r\n    822       parallel_iterations=parallel_iterations,\r\n    823       maximum_iterations=time_steps,\r\n--> 824       swap_memory=swap_memory)\r\n    825 \r\n    826   # Unpack final output if not using output tuples.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\r\n   3222     if loop_context.outer_context is None:\r\n   3223       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n-> 3224     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   3225     if maximum_iterations is not None:\r\n   3226       return result[1]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2954       with ops.get_default_graph()._lock:  # pylint: disable=protected-access\r\n   2955         original_body_result, exit_vars = self._BuildLoop(\r\n-> 2956             pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2957     finally:\r\n   2958       self.Exit()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2891         flat_sequence=vars_for_body_with_tensor_arrays)\r\n   2892     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n-> 2893     body_result = body(*packed_vars_for_body)\r\n   2894     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n   2895     if not nest.is_sequence(body_result):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in <lambda>(i, lv)\r\n   3192         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\r\n   3193             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\r\n-> 3194         body = lambda i, lv: (i + 1, orig_body(*lv))\r\n   3195 \r\n   3196     if context.executing_eagerly():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in _time_step(time, output_ta_t, state)\r\n    791           call_cell=call_cell,\r\n    792           state_size=state_size,\r\n--> 793           skip_conditionals=True)\r\n    794     else:\r\n    795       (output, new_state) = call_cell()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in _rnn_step(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\r\n    246     # steps.  This is faster when max_seq_len is equal to the number of unrolls\r\n    247     # (which is typical for dynamic_rnn).\r\n--> 248     new_output, new_state = call_cell()\r\n    249     nest.assert_same_structure(state, new_state)\r\n    250     new_state = nest.flatten(new_state)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py in <lambda>()\r\n    779 \r\n    780     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\r\n--> 781     call_cell = lambda: cell(input_t, state)\r\n    782 \r\n    783     if sequence_length is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope, *args, **kwargs)\r\n    337     # method.  See the class docstring for more details.\r\n    338     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\r\n--> 339                                      *args, **kwargs)\r\n    340 \r\n    341 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    697           if all(hasattr(x, 'get_shape') for x in input_list):\r\n    698             input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs)\r\n--> 699           self.build(input_shapes)\r\n    700         try:\r\n    701           # Note: not all sub-classes of Layer call Layer.__init__ (especially\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in build(self, inputs_shape)\r\n    765         shape=[input_depth + h_depth, 4 * self._num_units],\r\n    766         initializer=self._initializer,\r\n--> 767         partitioner=maybe_partitioner)\r\n    768     self._bias = self.add_variable(\r\n    769         _BIAS_VARIABLE_NAME,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in add_variable(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\r\n    544             constraint=constraint,\r\n    545             trainable=trainable and self.trainable,\r\n--> 546             partitioner=partitioner)\r\n    547 \r\n    548         if init_graph is not None:  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n    434     new_variable = getter(\r\n    435         name=name, shape=shape, dtype=dtype, initializer=initializer,\r\n--> 436         **kwargs_for_getter)\r\n    437 \r\n    438     # If we set an initializer and the variable processed it, tracking will not\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1315       partitioner=partitioner, validate_shape=validate_shape,\r\n   1316       use_resource=use_resource, custom_getter=custom_getter,\r\n-> 1317       constraint=constraint)\r\n   1318 get_variable_or_local_docstring = (\r\n   1319     \"\"\"%s\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1077           partitioner=partitioner, validate_shape=validate_shape,\r\n   1078           use_resource=use_resource, custom_getter=custom_getter,\r\n-> 1079           constraint=constraint)\r\n   1080 \r\n   1081   def _get_partitioned_variable(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n    423           caching_device=caching_device, partitioner=partitioner,\r\n    424           validate_shape=validate_shape, use_resource=use_resource,\r\n--> 425           constraint=constraint)\r\n    426 \r\n    427   def _get_partitioned_variable(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\r\n    392           trainable=trainable, collections=collections,\r\n    393           caching_device=caching_device, validate_shape=validate_shape,\r\n--> 394           use_resource=use_resource, constraint=constraint)\r\n    395 \r\n    396     if custom_getter is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\r\n    784         validate_shape=validate_shape,\r\n    785         constraint=constraint,\r\n--> 786         use_resource=use_resource)\r\n    787     if not context.executing_eagerly() or self._store_eager_variables:\r\n    788       # In eager mode we do not want to keep default references to Variable\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource)\r\n   2218                          name=name, dtype=dtype,\r\n   2219                          constraint=constraint,\r\n-> 2220                          use_resource=use_resource)\r\n   2221 \r\n   2222 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>(**kwargs)\r\n   2208              constraint=None,\r\n   2209              use_resource=None):\r\n-> 2210   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n   2211   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n   2212     previous_getter = _make_getter(getter, previous_getter)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\r\n   2191         collections=collections, validate_shape=validate_shape,\r\n   2192         caching_device=caching_device, name=name, dtype=dtype,\r\n-> 2193         constraint=constraint)\r\n   2194 \r\n   2195 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\r\n    233           dtype=dtype,\r\n    234           expected_shape=expected_shape,\r\n--> 235           constraint=constraint)\r\n    236 \r\n    237   def __repr__(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\r\n    341             with ops.name_scope(\"Initializer\"), ops.device(None):\r\n    342               self._initial_value = ops.convert_to_tensor(\r\n--> 343                   initial_value(), name=\"initial_value\", dtype=dtype)\r\n    344               shape = (self._initial_value.get_shape()\r\n    345                        if validate_shape else tensor_shape.unknown_shape())\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>()\r\n    768           initializer = initializer(dtype=dtype)\r\n    769         init_val = lambda: initializer(  # pylint: disable=g-long-lambda\r\n--> 770             shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n    771         variable_dtype = dtype.base_dtype\r\n    772 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py in __call__(self, shape, dtype, partition_info, verify_shape)\r\n    215       verify_shape = self._verify_shape\r\n    216     return constant_op.constant(\r\n--> 217         self.value, dtype=dtype, shape=shape, verify_shape=verify_shape)\r\n    218 \r\n    219   def get_config(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    212   tensor_value.tensor.CopyFrom(\r\n    213       tensor_util.make_tensor_proto(\r\n--> 214           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    215   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    216   const_tensor = g.create_op(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    430       nparray = np.empty(shape, dtype=np_dt)\r\n    431     else:\r\n--> 432       _AssertCompatible(values, dtype)\r\n    433       nparray = np.array(values, dtype=np_dt)\r\n    434       # check to them.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)\r\n    341     else:\r\n    342       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\r\n--> 343                       (dtype.name, repr(mismatch), type(mismatch).__name__))\r\n    344 \r\n    345 \r\n\r\nTypeError: Expected float32, got array([[ 1.04484751e+00,  4.59392878e-02,  8.02706034e-01,\r\n        -1.11174105e+00,  8.89477561e-01, -1.20486729e+00,\r\n         8.86266342e-01, -1.64510319e+00, -9.89266584e-01,\r\n         1.31369015e+00,  4.88107334e-01,  1.49676477e-01,\r\n        -6.81036506e-01, -9.64134982e-02, -1.39690939e+00,\r\n         7.25600555e-02, -9.32924292e-02, -7.95291205e-01,\r\n         2.18235671e-01,  1.01727717e+00,  9.90535181e-01,\r\n        -4.35980568e-01,  7.94711686e-01, -3.13013630e-02,\r\n        -7.20243003e-01, -1.37384000e+00, -9.62151398e-01,\r\n         1.07646743e+00,  4.69513293e-01, -4.47742163e-02,\r\n        -4.95195536e-01,  6.76953787e-01,  2.39035230e-01,\r\n        -1.11104812e-01,  2.05507744e+00,  9.33967217e-01,\r\n        -8.96051458e-01, -8.77908566e-02,  1.43848201e+00,\r\n        -1.04624360e+00],\r\n       [-3.41335190e-01,  1.40035909e+00, -1.60449447e-01,\r\n        -9.41774525e-01,  1.81853256e+00, -2.20882493e-01,\r\n         3.01935183e+00, -6.22724692e-01,  6.57625596e-01,\r\n         3.85760589e-01,  1.25143595e+00,  3.95787835e-01,\r\n        -1.15711379e-01,  1.04277452e+00,  1.18388115e-01,\r\n        -2.67029800e+00,  4.53791661e-01,  1.21855412e+00,\r\n         1.35663694e+00,  1.11392313e+00, -6.14336793e-01,\r\n         2.90739971e-01,  1.37860208e+00,  2.52123588e+00,\r\n        -8.47994930e-01, -1.57757604e-01, -3.22334965e-01,\r\n        -5.11127934e-01,  1.17326724e+00, -1.33463413e-01,\r\n         1.53402616e+00,  9.27541940e-01, -1.65259834e+00,\r\n         1.52193955e+00, -7.02834497e-01,  1.30055130e+00,\r\n        -1.27054858e+00,  1.93102289e+00,  4.90642813e-01,\r\n         1.05364636e+00],\r\n       [-2.82434425e-01,  4.84068350e-01, -1.54210849e+00,\r\n         2.17935455e+00, -1.33159023e+00,  9.24332614e-01,\r\n        -5.04315970e-01,  9.85161964e-01,  8.72890896e-01,\r\n         9.33685162e-01,  2.29173759e-01,  1.80474211e+00,\r\n         7.72485270e-01,  1.11224300e-01, -7.35284649e-01,\r\n         1.22797144e-01,  2.49131972e-02, -8.82123778e-01,\r\n         1.05901313e+00, -3.91695444e-01,  1.52364293e+00,\r\n        -2.02547241e+00, -1.11905764e+00, -1.39702563e-01,\r\n         6.00853245e-01,  1.06457879e+00,  7.23094038e-01,\r\n         2.30281455e-01,  2.85937688e-02,  1.88589155e-02,\r\n         6.45507430e-01, -5.51370987e-02, -1.68287628e+00,\r\n        -3.59386792e-01,  2.64980652e-01, -4.73594067e-01,\r\n        -1.16968518e+00,  6.28055865e-01,  2.84448999e+00,\r\n        -8.13157852e-01],\r\n       [-1.31717291e-01,  1.02786130e+00,  1.53418718e-01,\r\n        -1.97088279e-01,  1.25098576e+00, -3.64066728e-01,\r\n        -1.31257463e-01,  1.64076530e+00, -2.15908745e+00,\r\n        -6.22189979e-01, -2.53163951e-02,  1.44229587e+00,\r\n         1.42744437e+00,  1.17886987e+00, -1.37219218e+00,\r\n         1.95677402e+00,  1.25885663e-01, -1.74788095e-01,\r\n         1.49759607e+00,  1.69086673e+00,  1.70750645e+00,\r\n         8.23124033e-03, -6.57806464e-01, -6.86276358e-01,\r\n         2.76553853e-01, -5.89633972e-01,  4.83433320e-01,\r\n        -4.78963168e-01,  1.57649031e+00, -8.97853620e-01,\r\n         1.12044567e-01, -6.14123732e-01, -2.61661823e-01,\r\n         1.08654068e+00, -2.13519684e+00, -2.20165083e+00,\r\n        -1.79124609e+00,  1.66096352e+00,  1.01991370e+00,\r\n        -6.31529272e-01],\r\n       [-9.41644716e-01, -8.22052119e-02, -7.82717117e-01,\r\n         4.66941465e-01, -3.41506828e-01,  2.85859511e-01,\r\n        -5.39347861e-01,  4.83069925e-01, -1.96077011e+00,\r\n        -1.79893071e+00,  4.02349515e-01,  2.19665512e-01,\r\n         4.81469059e-01, -9.67396064e-01,  1.28371488e+00,\r\n         1.74737441e-01, -1.42200749e+00, -1.02551183e+00,\r\n         9.19474529e-01,  1.10227219e+00,  2.39780104e-01,\r\n         1.54564488e+00,  5.48480031e-01,  2.53774766e-01,\r\n        -1.05690541e+00,  2.04109323e+00, -8.91820616e-01,\r\n        -9.67841805e-01,  6.84840673e-02, -2.64095272e+00,\r\n        -9.63133723e-01,  1.27685637e+00,  6.88337275e-01,\r\n         2.05600684e+00, -1.62778234e+00, -3.23933044e-01,\r\n        -3.16615539e-01, -6.67369617e-01, -1.15297191e+00,\r\n        -5.91319319e-01],\r\n       [ 1.91052422e+00,  6.33272998e-01,  8.26497424e-01,\r\n         6.03023598e-01,  3.92400071e-01,  9.29020785e-01,\r\n        -6.88585491e-01,  9.77940308e-01, -1.26848693e+00,\r\n         9.94840201e-02,  7.52848243e-01, -3.04239973e-01,\r\n         3.67167359e-01,  5.69559156e-01, -6.36591114e-01,\r\n         1.11083130e+00, -8.39891783e-01,  4.10057372e-01,\r\n        -1.38307188e-01,  3.90518293e-01, -5.50275734e-01,\r\n        -8.79075363e-01, -1.66958109e-01,  6.56225679e-01,\r\n         1.95612624e+00, -4.45663548e-01, -1.09293024e-01,\r\n        -2.01618327e+00,  4.58819223e-01,  9.22194083e-01,\r\n         9.95847584e-01,  5.23072609e-02,  1.24794740e+00,\r\n        -4.53764894e-02,  2.95947625e-01, -7.41021448e-01,\r\n         2.50512089e+00,  9.61431201e-01,  1.69506349e+00,\r\n         6.95733555e-01],\r\n       [ 1.16595131e+00, -9.16556527e-01,  1.58695646e+00,\r\n         1.53449732e-01,  1.17051377e-01, -4.14580879e-01,\r\n        -5.97632083e-01, -9.09788623e-02,  2.06036713e-01,\r\n        -1.68144113e+00, -1.44447690e+00,  6.33471999e-01,\r\n         1.67047115e+00,  1.32731338e+00, -1.04672470e+00,\r\n         4.54657832e-01, -2.23668935e-01,  6.40420094e-01,\r\n        -2.76215451e-01,  1.29460409e+00, -1.38940318e-01,\r\n        -1.04813960e+00, -1.47631494e+00,  4.58803440e-01,\r\n         5.98103656e-01, -6.04906161e-01, -5.70246924e-02,\r\n         1.42080757e+00, -1.97360377e+00, -7.95265695e-01,\r\n         2.94820707e-01, -4.83390366e-01,  4.50911220e-01,\r\n         8.74186348e-01, -5.99966992e-01, -1.21949591e-01,\r\n        -2.52309447e-01,  1.53064407e+00,  3.72279673e-01,\r\n         1.07315179e+00],\r\n       [ 8.86889539e-01,  1.16549545e-01,  1.38816289e+00,\r\n         1.01371089e+00, -1.44051110e+00, -3.53644010e-01,\r\n        -3.65782677e-01,  6.13860886e-02, -3.14833634e-01,\r\n         2.59271600e+00,  8.16096390e-01,  1.50172499e-01,\r\n         5.30068573e-01, -7.30386914e-01, -2.03084764e-01,\r\n        -6.74878994e-01,  9.66930750e-02,  7.21899634e-01,\r\n        -1.99889298e-01,  1.64892187e-02, -1.19705017e+00,\r\n        -1.59441455e+00,  6.20422424e-01, -2.18686254e-01,\r\n         1.12343324e+00,  1.86378524e+00, -1.83527953e-01,\r\n        -1.00652180e+00, -1.65150954e-01,  4.72737655e-02,\r\n         4.04227537e-01,  7.75649967e-01, -1.67503021e+00,\r\n         1.58444824e+00, -5.54640787e-01, -9.06882661e-01,\r\n        -8.85061481e-01, -2.57646814e-01, -1.58392420e-01,\r\n        -8.72515024e-01],\r\n       [ 2.30347997e-01,  2.93106723e+00,  2.32079761e+00,\r\n         1.05747984e-02,  6.43344453e-01,  1.48450845e+00,\r\n        -1.32288375e+00, -3.35875203e-02, -1.53430836e-01,\r\n         1.13107152e+00,  2.71825618e-01, -1.70896982e+00,\r\n         6.73988768e-01, -1.52623124e+00,  8.51444615e-01,\r\n         9.58495328e-01,  1.13117333e+00,  1.57066734e-01,\r\n        -2.49850210e-01, -3.57635309e-01,  6.84622356e-01,\r\n         2.02666474e-01, -7.12002046e-01,  1.24946477e-02,\r\n         5.75374743e-01,  1.93356031e-01, -1.29900584e+00,\r\n         5.04674883e-02,  1.22158010e+00,  1.63139677e+00,\r\n        -2.49405615e-01,  1.82925921e+00,  3.54699037e-01,\r\n         9.97620566e-01,  9.85580452e-02,  1.11274364e-01,\r\n        -6.00244704e-01, -1.95023124e-01,  2.00273727e-01,\r\n         1.83794424e+00],\r\n       [ 7.09088942e-01, -1.04879043e+00, -5.74103670e-01,\r\n         7.43992315e-01,  1.52200161e+00, -2.03207326e+00,\r\n         2.11875292e-01,  1.04272441e+00, -1.26958291e+00,\r\n         2.70366571e-01, -2.19008125e-01,  1.40483006e+00,\r\n        -2.09482711e-01, -9.20875207e-01,  1.72865711e+00,\r\n        -2.64627344e-01,  3.47781018e-01,  4.81586537e-01,\r\n         1.07322305e+00, -8.17717513e-01,  1.28391605e+00,\r\n         1.97842517e-01,  1.72612806e+00,  2.13807753e-01,\r\n         1.03357557e+00,  5.84119555e-01, -8.61174988e-01,\r\n        -1.17678694e+00,  1.92928507e+00,  5.60831434e-01,\r\n         3.31615685e-01,  6.15759349e-02, -9.18765403e-02,\r\n        -1.25591199e+00, -1.69617993e+00, -1.04056449e-03,\r\n        -1.02922137e-01, -6.70639273e-01,  1.16609151e-01,\r\n         1.10320101e+00],\r\n       [ 3.14600442e-01, -4.65363105e-01,  1.67887551e+00,\r\n        -6.92450808e-01,  3.14357667e-01,  1.32540623e-01,\r\n         9.15669217e-01, -1.39204004e+00,  7.20252339e-01,\r\n         5.94095417e-01, -8.18707441e-02,  5.91751351e-01,\r\n        -6.68691321e-01,  4.96172554e-01,  5.29582332e-01,\r\n        -1.55275307e-01,  2.46515467e-02,  6.03194472e-01,\r\n         1.31759695e+00,  5.45525211e-01, -2.30398220e-02,\r\n         8.12616540e-01, -2.61076466e-01,  1.66934365e+00,\r\n        -8.31025772e-01, -2.00446090e-01,  2.91231454e-01,\r\n         7.98223470e-01, -9.60227710e-01,  1.90107485e+00,\r\n        -7.44250114e-01, -7.20353857e-01,  1.32092989e-01,\r\n         8.44143593e-01,  5.01753704e-01, -2.05186337e-02,\r\n         1.13485011e+00, -1.06831952e-01,  2.44283342e-01,\r\n        -2.09994058e+00],\r\n       [-6.29958586e-01,  5.63142982e-01, -3.90122789e-01,\r\n         1.63745415e+00,  6.94110430e-01,  8.22926404e-02,\r\n         1.95863036e-01, -1.38925172e-01,  2.25575648e-01,\r\n         8.12663637e-01,  2.17542039e-01, -4.50958688e-01,\r\n        -1.01270050e+00,  3.51468797e-01, -1.67982586e+00,\r\n         1.07461121e+00,  6.99860539e-01,  4.94039591e-01,\r\n        -5.90147661e-01, -1.83109452e+00,  1.03962324e+00,\r\n         1.15843905e+00, -3.49911595e-01,  7.62128043e-01,\r\n        -1.67721454e+00,  3.76573218e-01, -7.48380931e-01,\r\n        -1.00878241e+00,  1.25593116e+00,  1.18679312e+00,\r\n        -1.05711288e+00, -4.00083667e-01, -1.10232880e+00,\r\n         1.21308371e+00, -4.47723244e-01, -2.68600107e-01,\r\n         1.26868008e+00,  8.23863001e-01,  5.47930794e-01,\r\n         2.77146607e-01],\r\n       [ 8.58208487e-02,  1.56905645e+00,  1.82461216e+00,\r\n        -1.95056238e+00,  8.84093358e-01, -2.30546309e-01,\r\n         1.68715760e+00,  1.51402049e+00,  1.97083952e+00,\r\n         3.47050464e-01,  2.45418091e+00, -1.28676786e+00,\r\n        -3.13972072e-02, -9.15640304e-02, -6.08760602e-01,\r\n         6.27315932e-01,  2.99423764e-01,  1.47387084e+00,\r\n        -2.22367289e-01,  1.36963199e+00, -1.03270696e+00,\r\n         5.07323436e-01,  1.03998597e+00,  7.48829007e-01,\r\n         5.56559176e-01,  8.65196651e-01, -9.69346627e-01,\r\n        -3.61670544e-02,  9.58567659e-01, -3.05810054e-01,\r\n         1.57696086e+00,  6.16235833e-01, -1.05029472e+00,\r\n         4.44189207e-01, -2.39608656e+00,  5.52516728e-01,\r\n         1.41880306e+00,  8.11277537e-01,  4.57157626e-01,\r\n        -3.63861533e-01],\r\n       [-5.50649441e-01,  6.17569991e-01, -6.07970309e-01,\r\n         4.38174300e-01, -1.35517086e+00,  2.25004668e-02,\r\n        -3.37577140e-02,  1.06195204e-01,  1.28345880e+00,\r\n        -3.46079223e-01, -1.53264926e-01,  1.22802991e+00,\r\n         1.22813763e-01,  2.24233889e+00, -5.45865371e-01,\r\n        -6.42548620e-01, -5.81109641e-01, -9.70743156e-01,\r\n        -1.80179925e+00, -1.11872377e+00, -2.22970191e+00,\r\n         4.94857288e-01, -5.86297124e-01, -6.56848913e-01,\r\n        -1.17092937e-03, -7.39394009e-01,  1.19986436e+00,\r\n         3.96717450e-01, -1.24115664e-01,  1.00324288e+00,\r\n        -1.14458800e-01, -2.25008521e+00,  5.45973540e-01,\r\n         2.37025750e-01, -1.14356885e+00, -4.88471919e-01,\r\n        -1.13206828e+00, -1.62136190e+00,  1.23749918e-01,\r\n         3.08363831e-01],\r\n       [-2.36785516e+00,  1.93758109e-01,  9.52158319e-01,\r\n         1.74236299e-01, -1.37006190e+00, -3.20627648e-01,\r\n         2.16886312e+00, -8.07024449e-01, -9.12120118e-01,\r\n        -2.45296765e+00,  1.09160450e+00, -1.00823007e+00,\r\n        -1.08279055e-01, -3.37003362e-01,  8.89784314e-01,\r\n         4.28913333e-01, -1.31532876e+00,  1.99979263e-01,\r\n        -2.95699114e-01, -1.07325068e+00, -7.65415033e-01,\r\n        -7.57991272e-01,  1.58699077e+00, -6.27470343e-01,\r\n         4.39592393e-01,  1.19306894e+00, -1.41762289e+00,\r\n        -1.92094209e-01,  4.98275997e-01,  4.75333804e-01,\r\n         2.42416125e-01,  3.97092492e-02,  2.18313764e-01,\r\n         9.20630155e-01,  1.32786770e-01,  1.07811778e+00,\r\n        -1.56297503e-01,  1.84818124e-01,  6.83802272e-01,\r\n        -1.76218224e+00]]) of type 'ndarray' instead.\r\n\r\nCan anybody tell why this error is coming, the size of numpy arrays are correct."}