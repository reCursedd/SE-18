{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3508", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3508/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3508/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3508/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3508", "id": 167632257, "node_id": "MDU6SXNzdWUxNjc2MzIyNTc=", "number": 3508, "title": "Training on a GTX 1080 does not work, produces random labels", "user": {"login": "akors", "id": 3023492, "node_id": "MDQ6VXNlcjMwMjM0OTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3023492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akors", "html_url": "https://github.com/akors", "followers_url": "https://api.github.com/users/akors/followers", "following_url": "https://api.github.com/users/akors/following{/other_user}", "gists_url": "https://api.github.com/users/akors/gists{/gist_id}", "starred_url": "https://api.github.com/users/akors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akors/subscriptions", "organizations_url": "https://api.github.com/users/akors/orgs", "repos_url": "https://api.github.com/users/akors/repos", "events_url": "https://api.github.com/users/akors/events{/privacy}", "received_events_url": "https://api.github.com/users/akors/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-07-26T15:10:38Z", "updated_at": "2016-07-26T15:11:05Z", "closed_at": "2016-07-26T15:11:05Z", "author_association": "NONE", "body_html": "<p>Hi!<br>\nI have come across a very very strange issue. Namely, training on an NVIDIA GTX 1080 does not work at all, and judging from the error rate, the predicted labels are completely random.</p>\n<p>I have 2 almost identical systems (see below), and while on the System with the GTX 960 the training runs perfectly fine, on the system with the GTX 1080, the training simply doesn't work.</p>\n<p>To test this, I ran<br>\nthe following code:</p>\n<p><code>python -m tensorflow.models.image.mnist.convolutional</code></p>\n<p>On the System 1 (GTX 960), i get to an error rate below 4% within 2-3 batches, and at the end, it's below 1%:</p>\n<pre><code>Step 0 (epoch 0.00), 7.6 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 13.0 ms\nMinibatch loss: 3.296, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 7.3%\nStep 200 (epoch 0.23), 13.1 ms\nMinibatch loss: 3.459, learning rate: 0.010000\nMinibatch error: 12.5%\nValidation error: 3.9%\n...\nStep 8500 (epoch 9.89), 13.0 ms\nMinibatch loss: 1.604, learning rate: 0.006302\nMinibatch error: 1.6%\nValidation error: 0.9%\nTest error: 0.8%\n</code></pre>\n<p>On the GTX 1080 system, the performance simply <em>never</em> improves! Error rate is steady at around 90%.</p>\n<pre><code>Step 8400 (epoch 9.77), 5.5 ms\nMinibatch loss: 3.881, learning rate: 0.006302\nMinibatch error: 85.9%\nValidation error: 88.7%\nStep 8500 (epoch 9.89), 5.5 ms\nMinibatch loss: 3.877, learning rate: 0.006302\nMinibatch error: 87.5%\nValidation error: 88.7%\nTest error: 89.7%\n</code></pre>\n<p>I tested this with TensorFlow 0.9, from the release PIP package for Python 3.5 with GPU support.<br>\nI also tested this with TensorFlow master from a week ago (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/fc9162975e52978d3af38549b570cc3cc5f0ab66/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/fc9162975e52978d3af38549b570cc3cc5f0ab66\"><tt>fc91629</tt></a>), compiled to a PIP package on one machine, installed on both machines.</p>\n<p>Here are the full system Specs, but the difference between them is only the GPU (1080 vs 960) and the Driver (367.35 vs 361.42)</p>\n<p>System 1</p>\n<ul>\n<li>OS: Ubuntu 16.04.1 LTS</li>\n<li>Kernel: 4.4.0-31-generic</li>\n<li>NVIDIA Driver Version: 361.42</li>\n<li>CPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz</li>\n<li>RAM: 12 GB Ram</li>\n<li>GPU: NVIDIA GTX 960, 4GB VRAM (edition: MSI GTX 960 Gaming 4G)</li>\n</ul>\n<p>System 2:</p>\n<ul>\n<li>OS: Ubuntu 16.04.1 LTS</li>\n<li>Kernel: 4.4.0-31-generic</li>\n<li>NVIDIA Driver Version: 367.35</li>\n<li>CPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz</li>\n<li>RAM: 12 GB Ram</li>\n<li>GPU: NVIDIA GTX 1080, 8GB VRAM</li>\n</ul>\n<p>My LD_LIBRARY_PATH on both machines is:<br>\n<code>:/usr/local/cuda/lib64:/usr/local/cuda-7.5/extras/CUPTI/lib64/</code></p>\n<p>My Cuda version is 7.5, cudnn is 4.0.7 on BOTH machines.</p>\n<p>Output of <code>ls -l /usr/local/cuda/lib64</code> on the Machine with the GTX 960 and GTX 1080<br>\n<a href=\"https://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx960-txt\">https://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx960-txt</a><br>\n<a href=\"https://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx1080-txt\">https://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx1080-txt</a></p>\n<p>Does anyone know what could cause this and how to fix this?</p>", "body_text": "Hi!\nI have come across a very very strange issue. Namely, training on an NVIDIA GTX 1080 does not work at all, and judging from the error rate, the predicted labels are completely random.\nI have 2 almost identical systems (see below), and while on the System with the GTX 960 the training runs perfectly fine, on the system with the GTX 1080, the training simply doesn't work.\nTo test this, I ran\nthe following code:\npython -m tensorflow.models.image.mnist.convolutional\nOn the System 1 (GTX 960), i get to an error rate below 4% within 2-3 batches, and at the end, it's below 1%:\nStep 0 (epoch 0.00), 7.6 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 13.0 ms\nMinibatch loss: 3.296, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 7.3%\nStep 200 (epoch 0.23), 13.1 ms\nMinibatch loss: 3.459, learning rate: 0.010000\nMinibatch error: 12.5%\nValidation error: 3.9%\n...\nStep 8500 (epoch 9.89), 13.0 ms\nMinibatch loss: 1.604, learning rate: 0.006302\nMinibatch error: 1.6%\nValidation error: 0.9%\nTest error: 0.8%\n\nOn the GTX 1080 system, the performance simply never improves! Error rate is steady at around 90%.\nStep 8400 (epoch 9.77), 5.5 ms\nMinibatch loss: 3.881, learning rate: 0.006302\nMinibatch error: 85.9%\nValidation error: 88.7%\nStep 8500 (epoch 9.89), 5.5 ms\nMinibatch loss: 3.877, learning rate: 0.006302\nMinibatch error: 87.5%\nValidation error: 88.7%\nTest error: 89.7%\n\nI tested this with TensorFlow 0.9, from the release PIP package for Python 3.5 with GPU support.\nI also tested this with TensorFlow master from a week ago (fc91629), compiled to a PIP package on one machine, installed on both machines.\nHere are the full system Specs, but the difference between them is only the GPU (1080 vs 960) and the Driver (367.35 vs 361.42)\nSystem 1\n\nOS: Ubuntu 16.04.1 LTS\nKernel: 4.4.0-31-generic\nNVIDIA Driver Version: 361.42\nCPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz\nRAM: 12 GB Ram\nGPU: NVIDIA GTX 960, 4GB VRAM (edition: MSI GTX 960 Gaming 4G)\n\nSystem 2:\n\nOS: Ubuntu 16.04.1 LTS\nKernel: 4.4.0-31-generic\nNVIDIA Driver Version: 367.35\nCPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz\nRAM: 12 GB Ram\nGPU: NVIDIA GTX 1080, 8GB VRAM\n\nMy LD_LIBRARY_PATH on both machines is:\n:/usr/local/cuda/lib64:/usr/local/cuda-7.5/extras/CUPTI/lib64/\nMy Cuda version is 7.5, cudnn is 4.0.7 on BOTH machines.\nOutput of ls -l /usr/local/cuda/lib64 on the Machine with the GTX 960 and GTX 1080\nhttps://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx960-txt\nhttps://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx1080-txt\nDoes anyone know what could cause this and how to fix this?", "body": "Hi! \nI have come across a very very strange issue. Namely, training on an NVIDIA GTX 1080 does not work at all, and judging from the error rate, the predicted labels are completely random.\n\nI have 2 almost identical systems (see below), and while on the System with the GTX 960 the training runs perfectly fine, on the system with the GTX 1080, the training simply doesn't work.\n\nTo test this, I ran\nthe following code: \n\n`python -m tensorflow.models.image.mnist.convolutional`\n\nOn the System 1 (GTX 960), i get to an error rate below 4% within 2-3 batches, and at the end, it's below 1%:\n\n```\nStep 0 (epoch 0.00), 7.6 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 13.0 ms\nMinibatch loss: 3.296, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 7.3%\nStep 200 (epoch 0.23), 13.1 ms\nMinibatch loss: 3.459, learning rate: 0.010000\nMinibatch error: 12.5%\nValidation error: 3.9%\n...\nStep 8500 (epoch 9.89), 13.0 ms\nMinibatch loss: 1.604, learning rate: 0.006302\nMinibatch error: 1.6%\nValidation error: 0.9%\nTest error: 0.8%\n```\n\nOn the GTX 1080 system, the performance simply _never_ improves! Error rate is steady at around 90%.\n\n```\nStep 8400 (epoch 9.77), 5.5 ms\nMinibatch loss: 3.881, learning rate: 0.006302\nMinibatch error: 85.9%\nValidation error: 88.7%\nStep 8500 (epoch 9.89), 5.5 ms\nMinibatch loss: 3.877, learning rate: 0.006302\nMinibatch error: 87.5%\nValidation error: 88.7%\nTest error: 89.7%\n```\n\nI tested this with TensorFlow 0.9, from the release PIP package for Python 3.5 with GPU support.\nI also tested this with TensorFlow master from a week ago (fc9162975e52978d3af38549b570cc3cc5f0ab66), compiled to a PIP package on one machine, installed on both machines.\n\nHere are the full system Specs, but the difference between them is only the GPU (1080 vs 960) and the Driver (367.35 vs 361.42)\n\nSystem 1\n- OS: Ubuntu 16.04.1 LTS\n- Kernel: 4.4.0-31-generic\n- NVIDIA Driver Version: 361.42 \n- CPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz\n- RAM: 12 GB Ram\n- GPU: NVIDIA GTX 960, 4GB VRAM (edition: MSI GTX 960 Gaming 4G)\n\nSystem 2:\n- OS: Ubuntu 16.04.1 LTS\n- Kernel: 4.4.0-31-generic\n- NVIDIA Driver Version: 367.35\n- CPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz\n- RAM: 12 GB Ram\n- GPU: NVIDIA GTX 1080, 8GB VRAM\n\nMy LD_LIBRARY_PATH on both machines is:\n`:/usr/local/cuda/lib64:/usr/local/cuda-7.5/extras/CUPTI/lib64/`\n\nMy Cuda version is 7.5, cudnn is 4.0.7 on BOTH machines. \n\nOutput of `ls -l /usr/local/cuda/lib64` on the Machine with the GTX 960 and GTX 1080\nhttps://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx960-txt\nhttps://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx1080-txt\n\nDoes anyone know what could cause this and how to fix this?\n"}