{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3870", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3870/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3870/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3870/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3870", "id": 171568793, "node_id": "MDU6SXNzdWUxNzE1Njg3OTM=", "number": 3870, "title": "DNNLinearCombinedClassifier in distributed system on cpu. Worker1 costs 1000% of CPU and other workers cost 30%~50%.", "user": {"login": "coderXiangLi", "id": 8342490, "node_id": "MDQ6VXNlcjgzNDI0OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8342490?v=4", "gravatar_id": "", "url": "https://api.github.com/users/coderXiangLi", "html_url": "https://github.com/coderXiangLi", "followers_url": "https://api.github.com/users/coderXiangLi/followers", "following_url": "https://api.github.com/users/coderXiangLi/following{/other_user}", "gists_url": "https://api.github.com/users/coderXiangLi/gists{/gist_id}", "starred_url": "https://api.github.com/users/coderXiangLi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/coderXiangLi/subscriptions", "organizations_url": "https://api.github.com/users/coderXiangLi/orgs", "repos_url": "https://api.github.com/users/coderXiangLi/repos", "events_url": "https://api.github.com/users/coderXiangLi/events{/privacy}", "received_events_url": "https://api.github.com/users/coderXiangLi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ilblackdragon", "id": 175486, "node_id": "MDQ6VXNlcjE3NTQ4Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/175486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ilblackdragon", "html_url": "https://github.com/ilblackdragon", "followers_url": "https://api.github.com/users/ilblackdragon/followers", "following_url": "https://api.github.com/users/ilblackdragon/following{/other_user}", "gists_url": "https://api.github.com/users/ilblackdragon/gists{/gist_id}", "starred_url": "https://api.github.com/users/ilblackdragon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ilblackdragon/subscriptions", "organizations_url": "https://api.github.com/users/ilblackdragon/orgs", "repos_url": "https://api.github.com/users/ilblackdragon/repos", "events_url": "https://api.github.com/users/ilblackdragon/events{/privacy}", "received_events_url": "https://api.github.com/users/ilblackdragon/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ilblackdragon", "id": 175486, "node_id": "MDQ6VXNlcjE3NTQ4Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/175486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ilblackdragon", "html_url": "https://github.com/ilblackdragon", "followers_url": "https://api.github.com/users/ilblackdragon/followers", "following_url": "https://api.github.com/users/ilblackdragon/following{/other_user}", "gists_url": "https://api.github.com/users/ilblackdragon/gists{/gist_id}", "starred_url": "https://api.github.com/users/ilblackdragon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ilblackdragon/subscriptions", "organizations_url": "https://api.github.com/users/ilblackdragon/orgs", "repos_url": "https://api.github.com/users/ilblackdragon/repos", "events_url": "https://api.github.com/users/ilblackdragon/events{/privacy}", "received_events_url": "https://api.github.com/users/ilblackdragon/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-08-17T03:51:31Z", "updated_at": "2017-02-09T22:37:00Z", "closed_at": "2017-01-24T22:07:20Z", "author_association": "NONE", "body_html": "<h4>Installed from source, r0.10 branch.</h4>\n<h3>Steps to reproduce</h3>\n<p>I use multi-ps &amp; multi-worker to train. Model is DNNLinearCombinedClassifier under <code>contrib.learn</code>.</p>\n<h4>Distributed code:</h4>\n<p>ps &amp; worker entrance:</p>\n<pre><code>  server = tf.train.Server(\n    {'ps': ps_hosts,\n     'worker': worker_hosts},\n    job_name=FLAGS.job_name,\n    task_index=task_id)\n\n  if FLAGS.job_name == 'ps':\n    # `ps` jobs wait for incoming connections from the workers.\n    server.join()\n  else:      \n      # `worker` jobs will actually do the work.\n      self.worker_do(server.targer, cluster_spec, task_id)\n</code></pre>\n<p>self.worker_do:</p>\n<pre><code>num_workers = len(cluster_spec.as_dict()['worker'])\nnum_parameter_servers = len(cluster_spec.as_dict()['ps']) \nrun_config = RunConfig(master=target, task=task_id, num_ps_replicas=num_parameter_servers,\n                     num_cores=8)\nclassifier = DNNLinearCombinedClassifier(dnn_hidden_units=[1024, 512, 256], config=run_config, dnn_feature_columns=deep, model_dir=FLAGS.train_dir)\nclassifier.fit(input_fn=_input, steps=FLAGS.max_steps)\n</code></pre>\n<p>Start ps1...N, then worker1...N</p>\n<p><code>When training begin, I found worker1 cost 1000% CPU and other workers cost less on 100%.</code></p>\n<h3>What have you tried?</h3>\n<p>code in BaseEstimator when <code>__init__</code>:</p>\n<pre><code> # Set device function depending if there are replicas or not.\nif self._config.num_ps_replicas &gt; 0:\n  ps_ops = ['Variable', 'AutoReloadVariable']\n  self._device_fn = device_setter.replica_device_setter(\n      ps_tasks=self._config.num_ps_replicas,\n      merge_devices=False, ps_ops=ps_ops)\nelse:\n  self._device_fn = None\n</code></pre>\n<p>I tried to add a param to <code>device_setter.replica_device_setter</code></p>\n<pre><code>worker_device='/job:worker/task:%d' % self._config.task \n</code></pre>\n<p>And it works. Cpu balances.</p>\n<h3>Suggesting</h3>\n<p>Could you consider adding an API to place <code>_device_fn</code> to BaseEstimator. Then, tf users can customize their own <code>_device_fn</code>.</p>\n<p>Thanks.</p>", "body_text": "Installed from source, r0.10 branch.\nSteps to reproduce\nI use multi-ps & multi-worker to train. Model is DNNLinearCombinedClassifier under contrib.learn.\nDistributed code:\nps & worker entrance:\n  server = tf.train.Server(\n    {'ps': ps_hosts,\n     'worker': worker_hosts},\n    job_name=FLAGS.job_name,\n    task_index=task_id)\n\n  if FLAGS.job_name == 'ps':\n    # `ps` jobs wait for incoming connections from the workers.\n    server.join()\n  else:      \n      # `worker` jobs will actually do the work.\n      self.worker_do(server.targer, cluster_spec, task_id)\n\nself.worker_do:\nnum_workers = len(cluster_spec.as_dict()['worker'])\nnum_parameter_servers = len(cluster_spec.as_dict()['ps']) \nrun_config = RunConfig(master=target, task=task_id, num_ps_replicas=num_parameter_servers,\n                     num_cores=8)\nclassifier = DNNLinearCombinedClassifier(dnn_hidden_units=[1024, 512, 256], config=run_config, dnn_feature_columns=deep, model_dir=FLAGS.train_dir)\nclassifier.fit(input_fn=_input, steps=FLAGS.max_steps)\n\nStart ps1...N, then worker1...N\nWhen training begin, I found worker1 cost 1000% CPU and other workers cost less on 100%.\nWhat have you tried?\ncode in BaseEstimator when __init__:\n # Set device function depending if there are replicas or not.\nif self._config.num_ps_replicas > 0:\n  ps_ops = ['Variable', 'AutoReloadVariable']\n  self._device_fn = device_setter.replica_device_setter(\n      ps_tasks=self._config.num_ps_replicas,\n      merge_devices=False, ps_ops=ps_ops)\nelse:\n  self._device_fn = None\n\nI tried to add a param to device_setter.replica_device_setter\nworker_device='/job:worker/task:%d' % self._config.task \n\nAnd it works. Cpu balances.\nSuggesting\nCould you consider adding an API to place _device_fn to BaseEstimator. Then, tf users can customize their own _device_fn.\nThanks.", "body": "#### Installed from source, r0.10 branch.\n### Steps to reproduce\n\nI use multi-ps & multi-worker to train. Model is DNNLinearCombinedClassifier under `contrib.learn`.\n#### Distributed code:\n\n ps & worker entrance:\n\n```\n  server = tf.train.Server(\n    {'ps': ps_hosts,\n     'worker': worker_hosts},\n    job_name=FLAGS.job_name,\n    task_index=task_id)\n\n  if FLAGS.job_name == 'ps':\n    # `ps` jobs wait for incoming connections from the workers.\n    server.join()\n  else:      \n      # `worker` jobs will actually do the work.\n      self.worker_do(server.targer, cluster_spec, task_id)\n```\n\nself.worker_do:\n\n```\nnum_workers = len(cluster_spec.as_dict()['worker'])\nnum_parameter_servers = len(cluster_spec.as_dict()['ps']) \nrun_config = RunConfig(master=target, task=task_id, num_ps_replicas=num_parameter_servers,\n                     num_cores=8)\nclassifier = DNNLinearCombinedClassifier(dnn_hidden_units=[1024, 512, 256], config=run_config, dnn_feature_columns=deep, model_dir=FLAGS.train_dir)\nclassifier.fit(input_fn=_input, steps=FLAGS.max_steps)\n```\n\nStart ps1...N, then worker1...N\n\n`When training begin, I found worker1 cost 1000% CPU and other workers cost less on 100%.`\n### What have you tried?\n\ncode in BaseEstimator when `__init__`:\n\n```\n # Set device function depending if there are replicas or not.\nif self._config.num_ps_replicas > 0:\n  ps_ops = ['Variable', 'AutoReloadVariable']\n  self._device_fn = device_setter.replica_device_setter(\n      ps_tasks=self._config.num_ps_replicas,\n      merge_devices=False, ps_ops=ps_ops)\nelse:\n  self._device_fn = None\n```\n\nI tried to add a param to `device_setter.replica_device_setter`\n\n```\nworker_device='/job:worker/task:%d' % self._config.task \n```\n\nAnd it works. Cpu balances.\n### Suggesting\n\nCould you consider adding an API to place `_device_fn` to BaseEstimator. Then, tf users can customize their own `_device_fn`.\n\nThanks.\n"}