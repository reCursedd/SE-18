{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/227060283", "pull_request_review_id": 167016321, "id": 227060283, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzA2MDI4Mw==", "diff_hunk": "@@ -0,0 +1,2058 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <regex>\n+#include <string>\n+#include <vector>\n+#include <algorithm>\n+#include <iterator>\n+\n+#include <avro.h>\n+\n+#include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/framework/common_shape_fns.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/framework/partial_tensor_shape.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/strings/numbers.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+\n+using ::tensorflow::shape_inference::ShapeHandle;\n+\n+// As boiler plate for the class I used\n+// tensorflow/core/util/example_proto_helper.h and therein\n+// \"ParseSingleExampleAttrs\".\n+\n+// Checks for valid type for the avro attributes; currently we support bool,\n+// int, long, float, double, string.\n+//\n+// 'dtype' The data type.\n+//\n+// returns OK if any of the supported types; otherwise false.\n+//\n+tensorflow::Status CheckValidType(const tensorflow::DataType& dtype);\n+\n+// Check that all dense shapes are defined. Here, 'defined' means that:\n+// * All shapes have at least one dimension.\n+// * A shape can have an undefined dimension -1, as first dimension.\n+//\n+// 'dense_shape' The dense shapes.\n+//\n+// returns OK if the shapes are defined; otherwise false.\n+//\n+tensorflow::Status CheckDenseShapeToBeDefined(\n+    const std::vector<tensorflow::PartialTensorShape>& dense_shapes);\n+\n+// Struct that holds information about dense tensors that is used during\n+// parsing.\n+struct DenseInformation {\n+  tensorflow::DataType type;             // Type\n+  tensorflow::PartialTensorShape shape;  // Shape\n+  bool variable_length;  // This dense tensor has a variable length in the 2nd\n+                         // dimension\n+  std::size_t elements_per_stride;  // Number of elements per stride\n+};\n+\n+// This class holds the attributes passed into the parse avro record function.\n+// In addition, it builds up information about the 'elements per stride',\n+// 'variable length' for dense tensors, and\n+// 'dense shape' information.\n+class ParseAvroAttrs {\n+ public:\n+  // Initializes the attribute information\n+  template <typename ContextType>\n+  tensorflow::Status Init(ContextType* ctx) {\n+    std::vector<tensorflow::DataType> dense_types;\n+    std::vector<tensorflow::PartialTensorShape> dense_shapes;\n+\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"Nsparse\", &num_sparse));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"Ndense\", &num_dense));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"sparse_types\", &sparse_types));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"Tdense\", &dense_types));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"dense_shapes\", &dense_shapes));\n+\n+    // Check that all dense shapes are defined\n+    TF_RETURN_IF_ERROR(CheckDenseShapeToBeDefined(dense_shapes));\n+\n+    for (int i_dense = 0; i_dense < dense_shapes.size(); ++i_dense) {\n+      DenseInformation dense_info;\n+      tensorflow::TensorShape dense_shape;\n+      // This is the case where we have a fixed len sequence feature, and the\n+      // 1st dimension is undefined.\n+      if (dense_shapes[i_dense].dims() > 0 &&\n+          dense_shapes[i_dense].dim_size(0) == -1) {\n+        dense_info.variable_length = true;\n+        for (int d = 1; d < dense_shapes[i_dense].dims(); ++d) {\n+          dense_shape.AddDim(dense_shapes[i_dense].dim_size(d));\n+        }\n+        // This is the case where all dimensions are defined.\n+      } else {\n+        dense_info.variable_length = false;\n+        dense_shapes[i_dense].AsTensorShape(&dense_shape);\n+      }\n+      // Fill in the remaining information into the dense info and add it to to\n+      // the vector\n+      dense_info.elements_per_stride = dense_shape.num_elements();\n+      dense_info.shape = dense_shapes[i_dense];\n+      dense_info.type = dense_types[i_dense];\n+      dense_infos.push_back(dense_info);\n+    }\n+    return FinishInit();\n+  }\n+\n+  // All these attributes are publicly accessible, hence we did not suffix them\n+  // with '_'.\n+  tensorflow::int64 num_sparse;  // Number of sparse features\n+  tensorflow::int64 num_dense;   // Number of dense features (fixed and variable\n+                                 // length)\n+  std::vector<tensorflow::DataType> sparse_types;  // Types for sparse features\n+  std::vector<DenseInformation> dense_infos;  // Information about each dense\n+                                              // tensor\n+ private:\n+  tensorflow::Status FinishInit();  // for context-independent parts of Init.\n+};\n+\n+// As boiler plate I used tensorflow/core/util/example_proto_helper.cc and\n+// therein \"ParseSingleExampleAttrs\" and\n+Status CheckValidType(const DataType& dtype) {\n+  switch (dtype) {\n+    case DT_BOOL:\n+    case DT_INT32:\n+    case DT_INT64:\n+    case DT_FLOAT:\n+    case DT_DOUBLE:\n+    case DT_STRING:\n+      return Status::OK();\n+    default:\n+      return errors::InvalidArgument(\"Received input dtype: \",\n+                                     DataTypeString(dtype));\n+  }\n+}\n+\n+Status CheckDenseShapeToBeDefined(\n+    const std::vector<PartialTensorShape>& dense_shapes) {\n+  for (int i = 0; i < dense_shapes.size(); ++i) {\n+    const PartialTensorShape& dense_shape = dense_shapes[i];\n+    bool shape_ok = true;\n+    if (dense_shape.dims() == -1) {\n+      shape_ok = false;\n+    } else {\n+      for (int d = 1; d < dense_shape.dims() && shape_ok; ++d) {\n+        if (dense_shape.dim_size(d) == -1) {\n+          shape_ok = false;\n+        }\n+      }\n+    }\n+    if (!shape_ok) {\n+      return errors::InvalidArgument(\n+          \"dense_shapes[\", i,\n+          \"] has unknown rank or unknown inner dimensions: \",\n+          dense_shape.DebugString());\n+    }\n+  }\n+  return Status::OK();\n+}\n+\n+// Finishes the initialization for the attributes, which essentially checks that\n+// the attributes have the correct values.\n+//\n+// returns OK if all attributes are valid; otherwise false.\n+Status ParseAvroAttrs::FinishInit() {\n+  if (static_cast<size_t>(num_sparse) != sparse_types.size()) {\n+    return errors::InvalidArgument(\"len(sparse_keys) != len(sparse_types)\");\n+  }\n+  if (static_cast<size_t>(num_dense) != dense_infos.size()) {\n+    return errors::InvalidArgument(\"len(dense_keys) != len(dense_infos)\");\n+  }\n+  if (num_dense > std::numeric_limits<int32>::max()) {\n+    return errors::InvalidArgument(\"num_dense_ too large\");\n+  }\n+  for (const DenseInformation& dense_info : dense_infos) {\n+    TF_RETURN_IF_ERROR(CheckValidType(dense_info.type));\n+  }\n+  for (const DataType& type : sparse_types) {\n+    TF_RETURN_IF_ERROR(CheckValidType(type));\n+  }\n+  return Status::OK();\n+}\n+\n+// Register the parse function when building the shared library\n+// For the op I used as boiler plate: tensorflow/core/ops/parsing_ops.cc and\n+// there 'ParseExample'\n+// For the op kernel I used as boiler plate:\n+// tensorflow/core/kernels/example_parsing_ops.cc and there 'ExampleParserOp'\n+// For the compute method I used as boiler plate:\n+// tensorflow/core/util/example_proto_fast_parsing.cc and there the\n+//   method 'FastParseExample'\n+\n+REGISTER_OP(\"ParseAvroRecord\")\n+    .Input(\"serialized: string\")\n+    .Input(\"sparse_keys: Nsparse * string\")\n+    .Input(\"dense_keys: Ndense * string\")\n+    .Input(\"dense_defaults: Tdense\")\n+    .Output(\"sparse_indices: Nsparse * int64\")\n+    .Output(\"sparse_values: sparse_types\")\n+    .Output(\"sparse_shapes: Nsparse * int64\")\n+    .Output(\"dense_values: Tdense\")\n+    .Attr(\"Nsparse: int >= 0\")  // Inferred from sparse_keys\n+    .Attr(\"Ndense: int >= 0\")   // Inferred from dense_keys\n+    .Attr(\"sparse_types: list({float,double,int64,int32,string,bool}) >= 0\")\n+    .Attr(\"Tdense: list({float,double,int64,int32,string,bool}) >= 0\")\n+    .Attr(\"dense_shapes: list(shape) >= 0\")\n+    .Attr(\"schema: string\")\n+    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n+\n+       ParseAvroAttrs attrs;\n+       TF_RETURN_IF_ERROR(attrs.Init(c));\n+\n+       // Get the batch size and load it into input\n+       ShapeHandle input;\n+       TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &input));\n+\n+       // Get the schema, parse it, and log it\n+       string schema;\n+       TF_RETURN_IF_ERROR(c->GetAttr(\"schema\", &schema));\n+\n+       std::unique_ptr<avro_schema_t, std::function<void(avro_schema_t*)>>\n+           p_reader_schema(new avro_schema_t, [](avro_schema_t* ptr) {\n+             avro_schema_decref(*ptr);\n+           });\n+       if (avro_schema_from_json_length(schema.c_str(), schema.length(),\n+                                        &(*p_reader_schema)) != 0) {\n+         return errors::InvalidArgument(\"The provided json schema is invalid. \",\n+                                        avro_strerror());\n+       }\n+       LOG(INFO) << \"Avro parser with schema\\n\" << schema;\n+\n+       int output_idx = 0;\n+\n+       // Output sparse_indices, sparse_values, sparse_shapes\n+       for (int i_sparse = 0; i_sparse < attrs.num_sparse; ++i_sparse) {\n+         c->set_output(output_idx++, c->Matrix(c->UnknownDim(), 2));\n+       }\n+       for (int i_sparse = 0; i_sparse < attrs.num_sparse; ++i_sparse) {\n+         c->set_output(output_idx++, c->Vector(c->UnknownDim()));\n+       }\n+       for (int i_sparse = 0; i_sparse < attrs.num_sparse; ++i_sparse) {\n+         c->set_output(output_idx++, c->Vector(2));\n+       }\n+\n+       // Output dense_values\n+       for (int i_dense = 0; i_dense < attrs.num_dense; ++i_dense) {\n+         ShapeHandle dense;\n+         TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(\n+             attrs.dense_infos[i_dense].shape, &dense));\n+         TF_RETURN_IF_ERROR(c->Concatenate(input, dense, &dense));\n+         c->set_output(output_idx++, dense);\n+       }\n+\n+       return Status::OK();\n+     })\n+    .Doc(R\"doc(\n+      Parses a serialized avro record that follows the supplied schema into typed tensors.\n+      serialized: A vector containing a batch of binary serialized avro records.\n+      dense_keys: A list of Ndense string Tensors.\n+        The keys expected are associated with dense values.\n+      dense_defaults: A list of Ndense Tensors (some may be empty).\n+        These defaults can either be fully defined for all values in the tensor or they\n+        can be defined as padding element that is used whenever the original input data\n+        misses values to fill out the full tensor.\n+      dense_shapes: A list of Ndense shapes; the shapes of the dense tensors.\n+        The number of elements corresponding to dense_key[j]\n+        must always equal dense_shapes[j].NumEntries().\n+        If dense_shapes[j] == (D0, D1, ..., DN) then the shape of output\n+        Tensor dense_values[j] will be (|serialized|, D0, D1, ..., DN):\n+        The dense outputs are just the inputs row-stacked by batch.\n+        This works for dense_shapes[j] = (-1, D1, ..., DN).  In this case\n+        the shape of the output Tensor dense_values[j] will be\n+        (|serialized|, M, D1, .., DN), where M is the maximum number of blocks\n+        of elements of length D1 * .... * DN, across all minibatch entries\n+        in the input.  Any minibatch entry with less than M blocks of elements of\n+        length D1 * ... * DN will be padded with the corresponding default_value\n+        scalar element along the second dimension.\n+      dense_types: A list of Ndense types; the type of data in each Feature given in dense_keys.\n+      sparse_keys: A list of Ndense string Tensors (scalars).\n+        The keys expected are associated with sparse values.\n+      sparse_types: A list of Nsparse types; the data types of data in each Feature\n+        given in sparse_keys.\n+      schema: A string that describes the avro schema of the underlying serialized avro string.\n+        Currently the parse function supports the primitive types DT_STRING, DT_DOUBLE, DT_FLOAT,\n+        DT_INT64, DT_INT32, and DT_BOOL.\n+        The supported avro version depends on the compiled library avro library linked against\n+        TensorFlow during build. More instructions on avro:\n+        https://avro.apache.org/docs/1.8.1/spec.html\n+    )doc\");\n+\n+template <typename T>\n+using SmallVector = gtl::InlinedVector<T, 4>;  // Up to 4 items are stored\n+                                               // without allocating heap memory\n+\n+// Parses the str into an positive integer number. Does not support '-'.\n+//\n+// 'str' is the string that represents a positive integer, e.g. '32482'.\n+//\n+// returns True if the string can be parsed into a positive integer, otherwise\n+// false.\n+//\n+bool IsNonNegativeInt(const string& str) {\n+  return !str.empty() && std::find_if(str.begin(), str.end(), [](char c) {\n+                           return !std::isdigit(c);\n+                         }) == str.end();\n+}\n+\n+// We use this Avro field representation when parsing user-defined strings to\n+// access types fields, maps, arrays in avro.\n+// An Avro field can be a name, index, key, or asterisk which is used as\n+// wildcard when parsing arrays.\n+// The base class AvroField is an abstract class that defines the avro field\n+// types and general methods.\n+class AvroField {\n+ public:\n+  enum Type {\n+    name,\n+    index,\n+    key,\n+    mapAsterisk,\n+    arrayAsterisk,\n+    arrayFilter\n+  };  // Types for an avro field\n+\n+  // Get the type for an Avro field.\n+  //\n+  // returns The avro type for this field.\n+  //\n+  virtual Type GetType() const = 0;\n+\n+  // Get a human-readable string representation of this Avro field.\n+  //\n+  // returns The string for this field.\n+  //\n+  virtual string ToString() const = 0;\n+\n+  virtual ~AvroField() {}\n+};\n+\n+// Represents a field name which is used to access records' fields.\n+class AvroFieldName : public AvroField {\n+ public:\n+  // Create an Avro field name.\n+  //\n+  // 'name' The name for this Avro field.\n+  //\n+  // returns An instance of 'AvroFieldName'.\n+  //\n+  AvroFieldName(const string& name) : name(name) {}\n+\n+  // Get the name for the Avro field.\n+  //\n+  // returns The name.\n+  //\n+  inline string GetName() const { return name; }\n+\n+  // Get the type, see super-class.\n+  AvroField::Type GetType() const { return AvroField::Type::name; }\n+\n+  // Get the string representation, see super-class\n+  string ToString() const { return GetName(); }\n+\n+ private:\n+  string name;  // A string to hold the name\n+};\n+\n+// Represents an index which is used to access elements in arrays.\n+class AvroFieldIndex : public AvroField {\n+ public:\n+  // Create an Avro index.\n+  //\n+  // 'index' The index for this Avro field.\n+  //\n+  // returns An instance of 'AvroFieldIndex'.\n+  //\n+  AvroFieldIndex(int index) : index(index) {}\n+\n+  // Get the index of this Avro field.\n+  //\n+  // returns The index.\n+  //\n+  inline int GetIndex() const { return index; }\n+\n+  // Get the type, see super-class.\n+  AvroField::Type GetType() const { return AvroField::Type::index; }\n+\n+  // Get the string representation, see super-class.\n+  string ToString() const { return std::to_string(GetIndex()); }\n+\n+ private:\n+  int index;  // index to hold\n+};\n+\n+// Represents a key of a map. Side note: We could have used the same type for a\n+// name field in a record and a key in\n+// a map since avro's low level API treats these cases the same way. However, we\n+// choose this cleaner design.\n+class AvroFieldKey : public AvroField {\n+ public:\n+  // Create an Avro field key.\n+  //\n+  // 'key' The key for this Avro field.\n+  //\n+  // returns An instance of 'AvroFieldKey'.\n+  AvroFieldKey(const string& key) : key(key) {}\n+\n+  // Get the key string for this Avro field.\n+  //\n+  // returns The key.\n+  inline string GetKey() const { return key; }\n+\n+  // Get the type, see super-class.\n+  AvroField::Type GetType() const { return AvroField::Type::key; }\n+\n+  // Get the string representation, see super-class.\n+  string ToString() const { return GetKey(); }\n+\n+ private:\n+  string key;  // the key string\n+};\n+\n+// Used to select all values in a map.\n+class AvroFieldMapAsterisk : public AvroField {\n+ public:\n+  // Creates the Avro field map asterisk.\n+  //\n+  // returns An instance of 'AvroFieldMapAsterisk'.\n+  AvroFieldMapAsterisk() {}\n+\n+  // Get the type, see super-class.\n+  AvroField::Type GetType() const { return AvroField::Type::mapAsterisk; }\n+\n+  // Get the string representation, see super-class.\n+  string ToString() const { return \"'*'\"; }\n+};\n+\n+// Used to select all items in an array.\n+class AvroFieldArrayAsterisk : public AvroField {\n+ public:\n+  // Creates the Avro field asterisk.\n+  //\n+  // returns An instance of 'AvroFieldAsterisk'.\n+  AvroFieldArrayAsterisk() {}\n+\n+  // Get the type, see super-class.\n+  AvroField::Type GetType() const { return AvroField::Type::arrayAsterisk; }\n+\n+  // Get the string representation, see super-class.\n+  string ToString() const { return \"*\"; }\n+};\n+\n+// Filters are represented through a key and value pair. The key indicates which\n+// field the\n+// filter will be applied and the value is the criterion for the filter. Only\n+// supported for arrays.\n+class AvroFieldArrayFilter : public AvroField {\n+ public:\n+  // Creates the Avro field filter.\n+  //\n+  // returns An instance of 'AvroFieldFilter'.\n+  AvroFieldArrayFilter(const string& key, const string& value)\n+      : key(key), value(value) {}\n+\n+  // Get the type, see super-class.\n+  AvroField::Type GetType() const { return AvroField::Type::arrayFilter; }\n+\n+  // Get the key of this filter.\n+  inline string GetKey() const { return key; }\n+\n+  // Get the value of this filter.\n+  inline string GetValue() const { return value; }\n+\n+  // Get the string representation, see super-class.\n+  string ToString() const { return GetKey() + \"=\" + GetValue(); }\n+\n+ private:\n+  string key;    // the key of this filter\n+  string value;  // the value of this filter\n+};\n+\n+// The sparse buffer holds a list with primitive data types. This is used when\n+// parsing all tensors.\n+struct SparseBuffer {\n+  // Only the list that corresponds to the data type of the tensor is used\n+  SmallVector<string> string_list;\n+  SmallVector<double> double_list;\n+  SmallVector<float> float_list;\n+  SmallVector<int64> int64_list;\n+  SmallVector<int32> int32_list;\n+  SmallVector<bool> bool_list;      // TODO(fraudies): Change to\n+                                    // util::bitmap::InlinedBitVector<NBITS>\n+  std::vector<size_t> end_indices;  // End indices per row in the batch\n+  size_t n_elements;  // The total number of elements in the batch; required by\n+                      // 'SetValues' to accumulate.\n+};\n+\n+// Template specializations for 'GetListFromBuffer' for the supported types.\n+template <typename T>\n+const SmallVector<T>& GetListFromBuffer(const SparseBuffer& buffer);\n+\n+template <>\n+const SmallVector<int64>& GetListFromBuffer<int64>(const SparseBuffer& buffer) {\n+  return buffer.int64_list;\n+}\n+template <>\n+const SmallVector<int32>& GetListFromBuffer<int32>(const SparseBuffer& buffer) {\n+  return buffer.int32_list;\n+}\n+template <>\n+const SmallVector<float>& GetListFromBuffer<float>(const SparseBuffer& buffer) {\n+  return buffer.float_list;\n+}\n+template <>\n+const SmallVector<double>& GetListFromBuffer<double>(\n+    const SparseBuffer& buffer) {\n+  return buffer.double_list;\n+}\n+template <>\n+const SmallVector<bool>& GetListFromBuffer<bool>(const SparseBuffer& buffer) {\n+  return buffer.bool_list;\n+}\n+template <>\n+const SmallVector<string>& GetListFromBuffer<string>(\n+    const SparseBuffer& buffer) {\n+  return buffer.string_list;\n+}\n+\n+// Template specialization for 'CopyOrMoveBlock'; Note: 'string' values are\n+// moved, others are copied.\n+template <typename InputIterT, typename OutputIterT>\n+void CopyOrMoveBlock(const InputIterT b, const InputIterT e, OutputIterT t) {\n+  std::copy(b, e, t);\n+}\n+\n+template <>\n+void CopyOrMoveBlock(const string* b, const string* e, string* t) {\n+  std::move(b, e, t);\n+}\n+\n+// Checks that default values are available if required for filling in of\n+// values.\n+//\n+// This method is used to check that fixed len sequence features have the\n+// correct default. If any of the rows has less\n+// than 'n_elements_per_batch' values, we check that these can be filled in from\n+// the 'default_value' tensor.\n+//\n+// 'key' Name of the element that is parsed.\n+//\n+// 'n_elements_per_batch'  The number of elements in a batch.\n+//\n+// 'end_indices'  The end indices of the dense tensor as it is.\n+//\n+// 'default_value'  Tensor with default values. If we need to fill in this\n+// tensor must have at least\n+//                  'n_elements_per_batch' many elements.\n+//\n+// returns OK if we can fill in elements or no elements need to be filled in;\n+// otherwise false.\n+//\n+Status CheckDefaultsAvailable(const string& key,\n+                              const size_t n_elements_per_batch,\n+                              const std::vector<size_t>& end_indices,\n+                              const Tensor& default_value) {\n+  const size_t n_batches = end_indices.size();\n+  size_t n_total_elements_is = 0;\n+  const size_t n_default_elements = default_value.NumElements();\n+  const size_t n_elems_be = n_elements_per_batch;  // per row\n+  const bool not_enough_defaults = n_default_elements < n_elements_per_batch;\n+  for (size_t i_batches = 0; i_batches < n_batches; ++i_batches) {\n+    const size_t n_elems_is =\n+        end_indices[i_batches] - n_total_elements_is;  // per row\n+    n_total_elements_is = end_indices[i_batches];\n+    const size_t n_fill = n_elems_be - n_elems_is;\n+    if (n_fill > 0 && not_enough_defaults) {\n+      return errors::InvalidArgument(\"For key '\", key, \"' in batch \", i_batches,\n+                                     \" found \", n_elems_is, \" elements \",\n+                                     \"but for fixed length need \", n_elems_be,\n+                                     \" elements but default provides only \",\n+                                     n_default_elements, \" elements.\");\n+    }\n+  }\n+  return Status::OK();\n+}\n+\n+// Checks that a default value is supplied.\n+//\n+// This method is used by the fixed len sequence feature.\n+//\n+// 'key' Name of the element that is parsed.\n+//\n+// 'default_value'  Tensor with default value. If we need to fill in this tensor\n+// must have at least 1 element.\n+//\n+// returns OK we have at least one element; otherwise false.\n+//\n+Status CheckDefaultAvailable(const string& key, const Tensor& default_value) {\n+  const bool no_default = default_value.NumElements() <= 0;\n+  if (no_default) {\n+    return errors::InvalidArgument(\"For key '\", key, \"' no default is set in \",\n+                                   default_value.DebugString());\n+  }\n+  return Status::OK();\n+}\n+\n+// Fills in defaults from values in the 'default_tensor'. Note, if there is\n+// nothing to fill in the method leaves\n+// 'values' unchanged.\n+//\n+// 'n_elements' The total number of elements that shall be.\n+//\n+// 'n_elements_per_batch' The number of elements per batch that shall be.\n+//\n+// 'end_indices'  The end indices per batch that are.\n+//\n+// 'values' The result tensor.\n+//\n+template <typename T>\n+void FillInFromValues(const size_t n_elements,\n+                      const size_t n_elements_per_batch,\n+                      const Tensor& default_value,\n+                      const std::vector<size_t>& end_indices, Tensor* values) {\n+  auto tensor_data_ptr = values->flat<T>().data();\n+  const size_t n_batches = end_indices.size();\n+  auto list_ptr = default_value.flat<T>().data();\n+  size_t n_total_elements = 0;\n+  for (size_t i_batches = 0; i_batches < n_batches; ++i_batches) {\n+    const size_t n_elems = end_indices[i_batches] - n_total_elements;\n+    CopyOrMoveBlock(list_ptr + n_elems, list_ptr + n_elements_per_batch,\n+                    tensor_data_ptr + n_elems);\n+    tensor_data_ptr += n_elements_per_batch;\n+    n_total_elements = end_indices[i_batches];\n+  }\n+}\n+\n+// Copy a variable length dense tensor from the 'buffer' into 'values' using the\n+// 'end_indices' to identify the blocks\n+// that shall be copied per batch.\n+//\n+// 'n_elements' The overall number of elements.\n+//\n+// 'n_elements_per_batch' The number of elements in a batch.  Note, that the\n+// 'buffer' may not have that many elements\n+//                        per batch and we have separate methods to fill these\n+// with defaults.\n+//\n+// 'buffer' The buffer that contains the 'end_indices' and a flattened list of\n+// all values for one batch.\n+//\n+// 'values' The result tensor.\n+//\n+template <typename T>\n+void CopyVarLen(const size_t n_elements, const size_t n_elements_per_batch,\n+                const SparseBuffer& buffer, Tensor* values) {\n+\n+  // Data is [batch_size, max_num_elements, data_stride_size]\n+  //   and num_elements_per_minibatch = max_num_elements * data_stride_size\n+  auto tensor_data_ptr = values->flat<T>().data();\n+\n+  // Number of examples being stored in this buffer\n+  const auto& end_indices = buffer.end_indices;\n+  const size_t n_batches = end_indices.size();\n+\n+  const auto& list = GetListFromBuffer<T>(buffer);\n+  auto list_ptr = list.begin();\n+\n+#ifdef DEBUG_LOG_ENABLED\n+  auto list_ptr_ = list.begin();\n+  for (size_t i_elem = 0; i_elem < buffer.n_elements; ++i_elem) {\n+    LOG(INFO) << *list_ptr_ << \", \";\n+    list_ptr_++;\n+  }\n+#endif\n+\n+  size_t n_total_elements = 0;\n+  // Iterate through all the examples stored in this buffer.\n+  for (size_t i_batches = 0; i_batches < n_batches; ++i_batches) {\n+    // Number of elements stored for this example.\n+    const size_t n_elems = end_indices[i_batches] - n_total_elements;\n+    CopyOrMoveBlock(list_ptr, list_ptr + n_elems, tensor_data_ptr);\n+    // Move forward this many elements in the varlen buffer.\n+    list_ptr += n_elems;\n+    // Move forward to the next batch entry in the values output.\n+    tensor_data_ptr += n_elements_per_batch;\n+    n_total_elements = end_indices[i_batches];\n+  }\n+\n+#ifdef DEBUG_LOG_ENABLED\n+  for (size_t i_batches = 0; i_batches < n_batches; ++i_batches) {\n+    LOG(INFO) << \"End [\" << i_batches << \"] = \" << end_indices[i_batches];\n+  }\n+  LOG(INFO) << \"Total \" << n_total_elements << \" elements; counted \"\n+            << list.size() << \" elements.\";\n+  LOG(INFO) << \"Number of \" << n_elements_per_batch << \" per batch.\";\n+#endif\n+\n+  DCHECK(n_total_elements == list.size());\n+}\n+\n+// Fills in from a scalar in 'default_value' and copies from 'buffer' into\n+// 'values'.\n+//\n+// Pre-fills all values with 'default_value' and then copies in the supplied\n+// values from the 'buffer'.\n+// This code is mostly borrowed from\n+// 'tensorflow/core/util/example_proto_fast_parsing.cc' and therein the method\n+// 'FillInFixedLen'.\n+//\n+// 'n_elements' The total number of elements.\n+//\n+// 'n_elements_per_batch' The number of elements in a batch.\n+//\n+// 'buffer' The buffer with the parsed elements.\n+//\n+// 'default_value' The tensor with the scalar default value, which we assume\n+// exists here.\n+//\n+// 'values' The return tensors.\n+//\n+template <typename T>\n+void FillFromScalarAndCopy(const size_t n_elements,\n+                           const size_t n_elements_per_batch,\n+                           const SparseBuffer& buffer,\n+                           const Tensor& default_value, Tensor* values) {\n+\n+  // Fill tensor with default to create padding\n+  std::fill(values->flat<T>().data(), values->flat<T>().data() + n_elements,\n+            default_value.flat<T>()(0));\n+\n+  CopyVarLen<T>(n_elements, n_elements_per_batch, buffer, values);\n+}\n+\n+// Defines a TensorFlow operator that parses an Avro string into TensorFlow\n+// native tensors.\n+// It uses avro c and proto (because of TensorFlow's design).\n+//\n+// When developing this parse function I took inspiration from:\n+// tensorflow/core/util/example_proto_fast_parsing.cc\n+// TODO(fraudies): Run valgrind on this to check for memory leaks\n+class ParseAvroRecordOp : public OpKernel {\n+ public:\n+  // Constructs the parse op. This function does not include the parsing of the\n+  // strings into AvroTypes because these\n+  // strings are considered inputs and might change with each call of the\n+  // 'Compute' function. So, no caching is possible\n+  // for these. This follows the design in\n+  // tensorflow/core/util/example_proto_fast_parsing.cc.\n+  //\n+  // 'ctx' The context to the TensorFlow environment that helps to create\n+  // tensors and error messages.\n+  //\n+  // returns An instance of 'ParseAvroRecordOp'.\n+  //\n+  explicit ParseAvroRecordOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    string schema;\n+\n+    // Clear error message for avro\n+    avro_set_error(\"\");\n+\n+    // Get the schema supplied by the user as string and parse it\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"schema\", &schema));\n+    std::unique_ptr<avro_schema_t, std::function<void(avro_schema_t*)>>\n+        p_reader_schema(new avro_schema_t,\n+                        [](avro_schema_t* ptr) { avro_schema_decref(*ptr); });\n+\n+    OP_REQUIRES(ctx,\n+                avro_schema_from_json_length(schema.c_str(), schema.length(),\n+                                             &(*p_reader_schema)) == 0,\n+                errors::InvalidArgument(\"The provided json schema is invalid. \",\n+                                        avro_strerror()));\n+\n+    // Get a generic Avro class and instance of that class\n+    p_iface_ = avro_generic_class_from_schema((*p_reader_schema));\n+    OP_REQUIRES(ctx, p_iface_ != nullptr,\n+                errors::InvalidArgument(\n+                    \"Unable to create class for user-supplied schema. \",\n+                    avro_strerror()));\n+\n+    // Get attributes\n+    OP_REQUIRES_OK(ctx, attrs_.Init(ctx));\n+  }\n+\n+  // Destructor used for clean-up of avro structures.\n+  virtual ~ParseAvroRecordOp() { avro_value_iface_decref(p_iface_); }\n+\n+  // The compute function parses the user-provided strings to pull data from the\n+  // avro serialized string.\n+  //\n+  // 'ctx' The context for this operator which gives access to the inputs and\n+  // outputs.\n+  //\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* serialized;\n+    OpInputList dense_keys;\n+    OpInputList sparse_keys;\n+    OpInputList dense_defaults;\n+\n+    // Get input arguments\n+    OP_REQUIRES_OK(ctx, ctx->input(\"serialized\", &serialized));\n+    OP_REQUIRES_OK(ctx, ctx->input_list(\"dense_keys\", &dense_keys));\n+    OP_REQUIRES_OK(ctx, ctx->input_list(\"sparse_keys\", &sparse_keys));\n+    OP_REQUIRES_OK(ctx, ctx->input_list(\"dense_defaults\", &dense_defaults));\n+\n+    CHECK_EQ(dense_keys.size(), attrs_.num_dense);\n+    CHECK_EQ(sparse_keys.size(), attrs_.num_sparse);\n+    int64 n_serialized = serialized->NumElements();\n+\n+    // Create and initialize buffers\n+    std::vector<SparseBuffer> sparse_buffers(attrs_.num_sparse);\n+    // TODO(fraudies): Optimize by using a fixed allocation for dense tensors\n+    // for fixed len features\n+    std::vector<SparseBuffer> dense_buffers(attrs_.num_dense);\n+    for (int64 i_sparse = 0; i_sparse < attrs_.num_sparse; ++i_sparse) {\n+      sparse_buffers[i_sparse].n_elements = 0;\n+    }\n+    for (int64 i_dense = 0; i_dense < attrs_.num_dense; ++i_dense) {\n+      dense_buffers[i_dense].n_elements = 0;\n+    }\n+\n+    // Get outputs\n+    OpOutputList dense_values;\n+    OpOutputList sparse_indices;\n+    OpOutputList sparse_values;\n+    OpOutputList sparse_shapes;\n+    OP_REQUIRES_OK(ctx, ctx->output_list(\"dense_values\", &dense_values));\n+    OP_REQUIRES_OK(ctx, ctx->output_list(\"sparse_indices\", &sparse_indices));\n+    OP_REQUIRES_OK(ctx, ctx->output_list(\"sparse_values\", &sparse_values));\n+    OP_REQUIRES_OK(ctx, ctx->output_list(\"sparse_shapes\", &sparse_shapes));\n+\n+    // Can we know the sizes of these tensors in advance?\n+    // std::vector<Tensor> dense_values(attrs_.num_dense);\n+    // std::vector<Tensor> sparse_values(attrs_.num_sparse);\n+    // std::vector<Tensor> sparse_indices(attrs_.num_sparse);\n+    // std::vector<Tensor> sparse_shapes(attrs_.num_sparse);\n+\n+    // Ensure serialized is a vector\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(serialized->shape()),\n+                errors::InvalidArgument(\n+                    \"Expected serialized to be a vector, got shape: \",\n+                    serialized->shape().DebugString()));\n+    // Ensure the number of defaults matches\n+    OP_REQUIRES(ctx, dense_defaults.size() == attrs_.num_dense,\n+                errors::InvalidArgument(\n+                    \"Expected len(dense_defaults) == len(dense_keys) but got: \",\n+                    dense_defaults.size(), \" vs. \", attrs_.num_dense));\n+\n+    // Check that the defaults are set correctly\n+    for (int i_dense = 0; i_dense < static_cast<int>(attrs_.num_dense);\n+         ++i_dense) {\n+\n+      // Get the default value\n+      const Tensor& def_value = dense_defaults[i_dense];\n+\n+      // Get the information about the dense tensor\n+      const DenseInformation& dense_info = attrs_.dense_infos[i_dense];\n+\n+      if (dense_info.variable_length) {\n+        OP_REQUIRES(\n+            ctx, def_value.NumElements() == 1,\n+            errors::InvalidArgument(\"dense_shape[\", i_dense,\n+                                    \"] is a variable length shape: \",\n+                                    dense_info.shape.DebugString(),\n+                                    \", therefore \"\n+                                    \"def_value[\",\n+                                    i_dense,\n+                                    \"] must contain a single element (\"\n+                                    \"the padding element).  But its shape is: \",\n+                                    def_value.shape().DebugString()));\n+      } else if (def_value.NumElements() > 0) {\n+        OP_REQUIRES(ctx, dense_info.shape.IsCompatibleWith(def_value.shape()),\n+                    errors::InvalidArgument(\n+                        \"def_value[\", i_dense, \"].shape() == \",\n+                        def_value.shape().DebugString(),\n+                        \" is not compatible with dense_shapes_[\", i_dense,\n+                        \"] == \", dense_info.shape.DebugString()));\n+      }\n+      OP_REQUIRES(ctx, def_value.dtype() == dense_info.type,\n+                  errors::InvalidArgument(\n+                      \"dense_defaults[\", i_dense, \"].dtype() == \",\n+                      DataTypeString(def_value.dtype()), \" != dense_types_[\",\n+                      i_dense, \"] == \", DataTypeString(dense_info.type)));\n+    }\n+\n+    // Parse values into memory\n+    OP_REQUIRES_OK(\n+        ctx, ParseAndSetValues(&sparse_buffers, &dense_buffers, *serialized,\n+                               sparse_keys, dense_keys));\n+\n+    // Convert sparse into tensors\n+    OP_REQUIRES_OK(ctx, ConvertSparseBufferIntoSparseValuesIndicesShapes(\n+                            &sparse_values, &sparse_indices, &sparse_shapes,\n+                            sparse_keys, sparse_buffers, n_serialized));\n+\n+#ifdef DEBUG_LOG_ENABLED\n+    for (int i_dense = 0; i_dense < static_cast<int>(attrs_.num_dense);\n+         ++i_dense) {\n+      LOG(INFO) << \"Dense shape for key '\"\n+                << dense_keys[i_dense].scalar<string>()()\n+                << \"' is variable length? \"\n+                << (attrs_.dense_infos[i_dense].variable_length ? \"yes\" : \"no\");\n+    }\n+#endif\n+\n+    // Convert var len dense into tensors\n+    OP_REQUIRES_OK(\n+        ctx, ConvertVarLenIntoDense(&dense_values, dense_keys, dense_buffers,\n+                                    dense_defaults, n_serialized));\n+\n+    // Convert fixed len dense values into tensors\n+    OP_REQUIRES_OK(\n+        ctx, ConvertFixedLenIntoDense(&dense_values, dense_keys, dense_buffers,\n+                                      dense_defaults, n_serialized));\n+  }\n+\n+  // Get a human readable string representation of the Avro type.\n+  // Avro does not have a function like this.\n+  //\n+  // 'avro_type' The Avro type.\n+  //\n+  // returns a string representation of the Avro type.\n+  //\n+  static string AvroTypeToString(avro_type_t avro_type) {\n+    switch (avro_type) {\n+      case AVRO_BOOLEAN:\n+        return \"bool\";\n+      case AVRO_BYTES:\n+        return \"byte\";\n+      case AVRO_DOUBLE:\n+        return \"double\";\n+      case AVRO_FLOAT:\n+        return \"float\";\n+      case AVRO_INT64:\n+        return \"long\";\n+      case AVRO_INT32:\n+        return \"int\";\n+      case AVRO_NULL:\n+        return \"null\";\n+      case AVRO_STRING:\n+        return \"string\";\n+      case AVRO_ARRAY:\n+        return \"array\";\n+      case AVRO_ENUM:\n+        return \"enum\";\n+      case AVRO_FIXED:\n+        return \"fixed\";\n+      case AVRO_MAP:\n+        return \"map\";\n+      case AVRO_RECORD:\n+        return \"record\";\n+      case AVRO_UNION:\n+        return \"union\";\n+      case AVRO_LINK:\n+        return \"link\";\n+      default:\n+        return \"unknown\";\n+    }\n+  }\n+\n+ private:\n+  // Parses values out of the 'serialized' strings for the 'sparse_keys' and\n+  // 'dense_keys'. Next, it adds these values to\n+  // the 'sparse_buffers' and 'dense_buffers' in the same order as they are\n+  // defined in the keys, respectively.\n+  Status ParseAndSetValues(std::vector<SparseBuffer>* sparse_buffers,\n+                           std::vector<SparseBuffer>* dense_buffers,\n+                           const Tensor& serialized,\n+                           const OpInputList& sparse_keys,\n+                           const OpInputList& dense_keys) {\n+\n+    // Allocate space for and parse sparse and dense features\n+    std::vector<std::vector<AvroField*>> sparse_features(\n+        attrs_.num_sparse);  // List of list of sparse Avro fields\n+    std::vector<std::vector<AvroField*>> dense_features(\n+        attrs_.num_dense);  // List of list of sparse Avro fields\n+\n+    for (int i_sparse = 0; i_sparse < static_cast<int>(attrs_.num_sparse);\n+         ++i_sparse) {\n+      TF_RETURN_IF_ERROR(StringToAvroField(sparse_keys[i_sparse].scalar<string>()(),\n+                                           &sparse_features[i_sparse]));\n+    }\n+    for (int i_dense = 0; i_dense < static_cast<int>(attrs_.num_dense);\n+         ++i_dense) {\n+      TF_RETURN_IF_ERROR(StringToAvroField(dense_keys[i_dense].scalar<string>()(),\n+                                           &dense_features[i_dense]));\n+    }\n+\n+    auto serialized_strings = serialized.flat<string>();\n+    int64 n_serialized = serialized_strings.size();\n+    for (int i_serialized = 0; i_serialized < static_cast<int>(n_serialized);\n+         ++i_serialized) {\n+      avro_value_t value_;\n+\n+      // Create instance for reader class\n+      if (avro_generic_value_new(p_iface_, &value_)) {\n+        return errors::InvalidArgument(\n+            \"Unable to value for user-supplied schema. \", avro_strerror());\n+      }\n+\n+      // Wrap the string with the avro reader\n+      avro_reader_t reader_ =\n+          avro_reader_memory(serialized_strings(i_serialized).data(),\n+                             serialized_strings(i_serialized).size());\n+\n+      // Read value from string using the avro reader\n+      avro_value_read(reader_, &value_);\n+\n+      // Parse sparse features\n+      for (int i_sparse = 0; i_sparse < static_cast<int>(attrs_.num_sparse);\n+           ++i_sparse) {\n+        TF_RETURN_IF_ERROR(SetValues(&(*sparse_buffers)[i_sparse],\n+                                     sparse_features[i_sparse], value_,\n+                                     attrs_.sparse_types[i_sparse]));\n+      }\n+\n+      // Parse dense features fixed and variable length\n+      for (int i_dense = 0; i_dense < static_cast<int>(attrs_.num_dense);\n+           ++i_dense) {\n+        TF_RETURN_IF_ERROR(SetValues(&(*dense_buffers)[i_dense],\n+                                     dense_features[i_dense], value_,\n+                                     attrs_.dense_infos[i_dense].type));\n+      }\n+      // Free up the reader and value\n+      avro_reader_free(reader_);\n+      avro_value_decref(&value_);\n+    }\n+\n+    // Clean-up avro field instances for sparse features\n+    for (int64 i_sparse = 0; i_sparse < sparse_features.size(); ++i_sparse) {\n+      ClearAvroFields(sparse_features[i_sparse]);\n+    }\n+    sparse_features.clear();\n+\n+    // Clean-up avro field instances for dense features\n+    for (int64 i_dense = 0; i_dense < dense_features.size(); ++i_dense) {\n+      ClearAvroFields(dense_features[i_dense]);\n+    }\n+    dense_features.clear();\n+\n+    return Status::OK();\n+  }\n+\n+  // Converts sparse buffer data into sparse values, indices, and shapes.\n+  Status ConvertSparseBufferIntoSparseValuesIndicesShapes(\n+      OpOutputList* sparse_values, OpOutputList* sparse_indices,\n+      OpOutputList* sparse_shapes, const OpInputList& sparse_keys,\n+      const std::vector<SparseBuffer>& sparse_buffers,\n+      const int64 n_serialized) {\n+\n+#ifdef DEBUG_LOG_ENABLED\n+    LOG(INFO) << \"Converting \" << attrs_.num_sparse << \" sparse tensors.\";\n+#endif\n+\n+    for (int i_sparse = 0; i_sparse < static_cast<int>(attrs_.num_sparse);\n+         ++i_sparse) {\n+      const int64 n_elements =\n+          static_cast<int64>(sparse_buffers[i_sparse].n_elements);\n+\n+#ifdef DEBUG_LOG_ENABLED\n+      LOG(INFO) << \"For key '\" << sparse_keys[i_sparse].scalar<string>()()\n+                << \"' found the following end indices.\";\n+      const std::vector<size_t> end_indices =\n+          sparse_buffers[i_sparse].end_indices;\n+      const size_t n_batches = end_indices.size();\n+      for (size_t i_batches = 0; i_batches < n_batches; ++i_batches) {\n+        LOG(INFO) << \"End [\" << i_batches << \"] = \" << end_indices[i_batches];\n+      }\n+      LOG(INFO) << \"With \" << n_elements << \" elements.\";\n+#endif\n+\n+      // Sparse shapes\n+      Tensor* sparse_shapes_ptr;\n+      sparse_shapes->allocate(i_sparse, TensorShape({2}), &sparse_shapes_ptr);\n+      Tensor& shape = *sparse_shapes_ptr;\n+      // Why isn't this a reference type? Is it inferred as one?\n+      auto shape_data = shape.vec<int64>();\n+      shape_data(0) = n_serialized;\n+      shape_data(1) = n_elements;\n+\n+      // Sparse indices\n+      Tensor* indices_ptr;\n+      sparse_indices->allocate(i_sparse, TensorShape({n_elements, 2}),\n+                               &indices_ptr);\n+      Tensor& indices = *indices_ptr;\n+\n+      int64* index = &indices.matrix<int64>()(0, 0);\n+      size_t row_index = 0;\n+\n+      // The user cannot parse a scalar into a variable length tensor -- it does\n+      // not make sense\n+      if (sparse_buffers[i_sparse].end_indices.empty()) {\n+        return errors::InvalidArgument(\n+            \"Tried to load non-array into VarLenFeature. \",\n+            \"Use FixedLenFeature instead for key: '\",\n+            sparse_keys[i_sparse].scalar<string>()(), \"'.\");\n+      }\n+\n+      // Build row (for batch) / column index, the row index increases by +1\n+      // with each column\n+      size_t n_total_elements = 0;\n+      for (size_t end_index : sparse_buffers[i_sparse].end_indices) {\n+        const size_t n_elems = end_index - n_total_elements;\n+        for (size_t col_index = 0; col_index < n_elems; ++col_index) {\n+          *index = row_index;\n+          index++;\n+          *index = col_index;\n+          index++;\n+        }\n+        row_index++;\n+        n_total_elements = end_index;\n+      }\n+\n+#ifdef DEBUG_LOG_ENABLED\n+      LOG(INFO) << \"Created index \" << indices.DebugString();\n+#endif\n+\n+      // Sparse values\n+      TensorShape values_shape({static_cast<int64>(n_elements)});\n+      Tensor* values_ptr;\n+      sparse_values->allocate(i_sparse, values_shape, &values_ptr);\n+      Tensor& values = *values_ptr;\n+\n+#ifdef DEBUG_LOG_ENABLED\n+      LOG(INFO) << \"Created values \" << values.DebugString();\n+#endif\n+\n+      // Based on the type we copy the values\n+      switch (attrs_.sparse_types[i_sparse]) {\n+        case DT_STRING: {\n+          std::move(sparse_buffers[i_sparse].string_list.begin(),\n+                    sparse_buffers[i_sparse].string_list.end(),\n+                    values.flat<string>().data());\n+          break;\n+        }\n+        case DT_DOUBLE: {\n+          std::copy(sparse_buffers[i_sparse].double_list.begin(),\n+                    sparse_buffers[i_sparse].double_list.end(),\n+                    values.flat<double>().data());\n+          break;\n+        }\n+        case DT_FLOAT: {\n+          std::copy(sparse_buffers[i_sparse].float_list.begin(),\n+                    sparse_buffers[i_sparse].float_list.end(),\n+                    values.flat<float>().data());\n+          break;\n+        }\n+        case DT_INT64: {\n+          std::copy(sparse_buffers[i_sparse].int64_list.begin(),\n+                    sparse_buffers[i_sparse].int64_list.end(),\n+                    values.flat<int64>().data());\n+          break;\n+        }\n+        case DT_INT32: {\n+          std::copy(sparse_buffers[i_sparse].int32_list.begin(),\n+                    sparse_buffers[i_sparse].int32_list.end(),\n+                    values.flat<int32>().data());\n+          break;\n+        }\n+        case DT_BOOL: {\n+          std::copy(sparse_buffers[i_sparse].bool_list.begin(),\n+                    sparse_buffers[i_sparse].bool_list.end(),\n+                    values.flat<bool>().data());\n+          break;\n+        }\n+        default:\n+          CHECK(false) << \"Should not happen.\";  /// Error when avro type does\n+                                                 /// not match with tf type\n+      }\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // Converts variable length dense buffers into dense tensors filling in from\n+  // defaults if necessary.\n+  Status ConvertVarLenIntoDense(OpOutputList* dense_values,\n+                                const OpInputList& dense_keys,\n+                                const std::vector<SparseBuffer>& dense_buffers,\n+                                const OpInputList& dense_defaults,\n+                                const int64 n_serialized) {\n+\n+    for (int i_dense = 0; i_dense < static_cast<int>(attrs_.num_dense);\n+         ++i_dense) {\n+\n+      // Get the information about the dense tensor\n+      const DenseInformation& dense_info = attrs_.dense_infos[i_dense];\n+\n+      // If this is not a variable length dense tensor skip\n+      if (!dense_info.variable_length) {\n+        continue;\n+      }\n+\n+#ifdef DEBUG_LOG_ENABLED\n+      LOG(INFO) << \"Working on key '\" << dense_keys[i_dense].scalar<string>()()\n+                << \"'.\";\n+#endif\n+\n+      // Find the maximum number of features and elements in the batch\n+      size_t n_max_features = 0;\n+      const std::vector<size_t>& end_indices =\n+          dense_buffers[i_dense].end_indices;\n+      n_max_features = std::max(n_max_features, end_indices[0]);\n+      for (size_t i_serialized = 1; i_serialized < end_indices.size();\n+           ++i_serialized) {\n+        n_max_features =\n+            std::max(n_max_features,\n+                     end_indices[i_serialized] - end_indices[i_serialized - 1]);\n+      }\n+      size_t n_max_elements = n_max_features / dense_info.elements_per_stride;\n+\n+#ifdef DEBUG_LOG_ENABLED\n+      LOG(INFO) << \"Found maximum number of \" << n_max_elements << \" elements.\";\n+#endif\n+\n+      // Define the dense tensor shape\n+      TensorShape out_shape;\n+      out_shape.AddDim(\n+          n_serialized);  // This is the number of elements in the batch\n+      out_shape.AddDim(n_max_elements);  // Add the variable length dimension\n+                                         // with 'n_max_elements'\n+      for (int i_dim = 1; i_dim < dense_info.shape.dims(); ++i_dim) {\n+        out_shape.AddDim(dense_info.shape.dim_size(i_dim));\n+      }\n+\n+#ifdef DEBUG_LOG_ENABLED\n+      LOG(INFO) << \"Dense shape for key '\"\n+                << dense_keys[i_dense].scalar<string>()() << \"' is \"\n+                << out_shape.DebugString();\n+#endif\n+\n+      // Create the dense values for the shape and compute the number of\n+      // elements per batch\n+      Tensor* values_ptr;\n+      dense_values->allocate(i_dense, out_shape, &values_ptr);\n+      Tensor& values = *values_ptr;\n+      const size_t n_elements = values.NumElements();\n+      const size_t n_elements_per_batch =\n+          n_max_elements * dense_info.elements_per_stride;\n+\n+#ifdef DEBUG_LOG_ENABLED\n+      LOG(INFO) << \"Maximally we have \" << n_max_elements << \" elements.\";\n+      LOG(INFO) << \"The dense tensor with batches has \" << n_elements\n+                << \" elements.\";\n+#endif\n+\n+      // Get the default value tensor\n+      const Tensor& default_value = dense_defaults[i_dense];\n+\n+      // Check that we have the correct number of values that need to be filled\n+      // in\n+      TF_RETURN_IF_ERROR(CheckDefaultAvailable(\n+          dense_keys[i_dense].scalar<string>()(), default_value));\n+\n+      // Copy the data into the output values\n+      switch (dense_info.type) {\n+        case DT_STRING: {\n+          FillFromScalarAndCopy<string>(n_elements, n_elements_per_batch,\n+                                        dense_buffers[i_dense],\n+                                        dense_defaults[i_dense], &values);\n+          break;\n+        }\n+        case DT_DOUBLE: {\n+          FillFromScalarAndCopy<double>(n_elements, n_elements_per_batch,\n+                                        dense_buffers[i_dense],\n+                                        dense_defaults[i_dense], &values);\n+          break;\n+        }\n+        case DT_FLOAT: {\n+          FillFromScalarAndCopy<float>(n_elements, n_elements_per_batch,\n+                                       dense_buffers[i_dense],\n+                                       dense_defaults[i_dense], &values);\n+          break;\n+        }\n+        case DT_INT64: {\n+          FillFromScalarAndCopy<int64>(n_elements, n_elements_per_batch,\n+                                       dense_buffers[i_dense],\n+                                       dense_defaults[i_dense], &values);\n+          break;\n+        }\n+        case DT_INT32: {\n+          FillFromScalarAndCopy<int32>(n_elements, n_elements_per_batch,\n+                                       dense_buffers[i_dense],\n+                                       dense_defaults[i_dense], &values);\n+          break;\n+        }\n+        case DT_BOOL: {\n+          FillFromScalarAndCopy<bool>(n_elements, n_elements_per_batch,\n+                                      dense_buffers[i_dense],\n+                                      dense_defaults[i_dense], &values);\n+          break;\n+        }\n+        default:\n+          CHECK(false) << \"Should not happen.\";  // Error when avro type does\n+                                                 // not match with tf type\n+      }\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Convert fixed length dense buffers into dense tensors filling in from\n+  // defaults if necessary.\n+  Status ConvertFixedLenIntoDense(\n+      OpOutputList* dense_values, const OpInputList& dense_keys,\n+      const std::vector<SparseBuffer>& dense_buffers,\n+      const OpInputList& dense_defaults, const int64 n_serialized) {\n+\n+    for (int i_dense = 0; i_dense < static_cast<int>(attrs_.num_dense);\n+         ++i_dense) {\n+\n+      // Get the information about the dense tensor\n+      const DenseInformation& dense_info = attrs_.dense_infos[i_dense];\n+\n+      // Skip any variable length dense tensors\n+      if (dense_info.variable_length) {\n+        continue;\n+      }\n+\n+      // Define the output shape\n+      TensorShape out_shape;\n+      out_shape.AddDim(n_serialized);\n+      for (const int64 dim : dense_info.shape.dim_sizes()) {\n+        out_shape.AddDim(dim);\n+      }\n+\n+      // Create the output tensor\n+      Tensor* values_ptr;\n+      dense_values->allocate(i_dense, out_shape, &values_ptr);\n+      Tensor& values = *values_ptr;\n+\n+      // Get the number of values overall and per batch\n+      size_t n_elements = values.NumElements();\n+      size_t n_elements_per_batch = dense_info.elements_per_stride;\n+\n+#ifdef DEBUG_LOG_ENABLED\n+      LOG(INFO) << \"Fixed length dense tensor has \" << n_elements\n+                << \" elements.\";\n+      LOG(INFO) << \"The stride size is \" << n_elements_per_batch\n+                << \" elements.\";\n+#endif\n+\n+      const Tensor& default_value = dense_defaults[i_dense];\n+\n+      // Check that we have the correct number of values that need to be filled\n+      // in\n+      TF_RETURN_IF_ERROR(CheckDefaultsAvailable(\n+          dense_keys[i_dense].scalar<string>()(), n_elements_per_batch,\n+          dense_buffers[i_dense].end_indices, default_value));\n+\n+      // Copy values and fill in missing values from the defaults\n+      switch (dense_info.type) {\n+        case DT_STRING: {\n+          CopyVarLen<string>(n_elements, n_elements_per_batch,\n+                             dense_buffers[i_dense], &values);\n+          FillInFromValues<string>(n_elements, n_elements_per_batch,\n+                                   default_value,\n+                                   dense_buffers[i_dense].end_indices, &values);\n+          break;\n+        }\n+        case DT_DOUBLE: {\n+          CopyVarLen<double>(n_elements, n_elements_per_batch,\n+                             dense_buffers[i_dense], &values);\n+          FillInFromValues<double>(n_elements, n_elements_per_batch,\n+                                   default_value,\n+                                   dense_buffers[i_dense].end_indices, &values);\n+          break;\n+        }\n+        case DT_FLOAT: {\n+          CopyVarLen<float>(n_elements, n_elements_per_batch,\n+                            dense_buffers[i_dense], &values);\n+          FillInFromValues<float>(n_elements, n_elements_per_batch,\n+                                  default_value,\n+                                  dense_buffers[i_dense].end_indices, &values);\n+          break;\n+        }\n+        case DT_INT64: {\n+          CopyVarLen<int64>(n_elements, n_elements_per_batch,\n+                            dense_buffers[i_dense], &values);\n+          FillInFromValues<int64>(n_elements, n_elements_per_batch,\n+                                  default_value,\n+                                  dense_buffers[i_dense].end_indices, &values);\n+          break;\n+        }\n+        case DT_INT32: {\n+          CopyVarLen<int32>(n_elements, n_elements_per_batch,\n+                            dense_buffers[i_dense], &values);\n+          FillInFromValues<int32>(n_elements, n_elements_per_batch,\n+                                  default_value,\n+                                  dense_buffers[i_dense].end_indices, &values);\n+          break;\n+        }\n+        case DT_BOOL: {\n+          CopyVarLen<bool>(n_elements, n_elements_per_batch,\n+                           dense_buffers[i_dense], &values);\n+          FillInFromValues<bool>(n_elements, n_elements_per_batch,\n+                                 default_value,\n+                                 dense_buffers[i_dense].end_indices, &values);\n+          break;\n+        }\n+        default:\n+          CHECK(false) << \"Should not happen.\";  /// Error when avro type does\n+                                                 /// not match with tf type\n+      }\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // Parses a string into avro fields. ASSUMES that the vector for 'avro_fields'\n+  // is empty!\n+  //\n+  // 'avro_fields' An empty vector where we add the avro_fields.\n+  //\n+  // 'str' Parse this string into Avro types.\n+  //\n+  static Status StringToAvroField(const string& str,\n+                                  std::vector<AvroField*>* avro_fields_ptr) {\n+    // Split into tokens using the separator\n+    std::vector<AvroField*>& avro_fields = *avro_fields_ptr;\n+\n+    std::vector<string> incomplete_tokens = str_util::Split(str, \".\");", "path": "tensorflow/contrib/avro/ops/parse_avro_record.cc", "position": 1391, "original_position": 1434, "commit_id": "bdf790d5c888e59d593230286edaaf5314daaee5", "original_commit_id": "d1be9e544357e72eb37f6adb626670f745e79a9b", "user": {"login": "fraudies", "id": 1770877, "node_id": "MDQ6VXNlcjE3NzA4Nzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1770877?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fraudies", "html_url": "https://github.com/fraudies", "followers_url": "https://api.github.com/users/fraudies/followers", "following_url": "https://api.github.com/users/fraudies/following{/other_user}", "gists_url": "https://api.github.com/users/fraudies/gists{/gist_id}", "starred_url": "https://api.github.com/users/fraudies/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fraudies/subscriptions", "organizations_url": "https://api.github.com/users/fraudies/orgs", "repos_url": "https://api.github.com/users/fraudies/repos", "events_url": "https://api.github.com/users/fraudies/events{/privacy}", "received_events_url": "https://api.github.com/users/fraudies/received_events", "type": "User", "site_admin": false}, "body": "It's true that you can put any string as the key in a map. \r\nFilters in maps are not supported.\r\nKey access in maps is supported. \r\nThe syntax for that is myMap[myKey] and myKey should be anything -- I'll change that.\r\n\r\nFilter in arrays are supported to a limited extent (only on attributes within the same record that is in the array).\r\nThe syntax is myArray[attributeName=filterValue].anotherAttributeName to get all another attributes where the attribute name is equal to filter value.\r\n\r\nIn the latter case attributeName and filterValue shall adhere to the spec here https://avro.apache.org/docs/1.7.7/spec.html#Names to allow for nesting in later implementations of filters.", "created_at": "2018-10-22T17:18:18Z", "updated_at": "2018-10-22T21:28:38Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/18224#discussion_r227060283", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18224", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/227060283"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/18224#discussion_r227060283"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18224"}}, "body_html": "<p>It's true that you can put any string as the key in a map.<br>\nFilters in maps are not supported.<br>\nKey access in maps is supported.<br>\nThe syntax for that is myMap[myKey] and myKey should be anything -- I'll change that.</p>\n<p>Filter in arrays are supported to a limited extent (only on attributes within the same record that is in the array).<br>\nThe syntax is myArray[attributeName=filterValue].anotherAttributeName to get all another attributes where the attribute name is equal to filter value.</p>\n<p>In the latter case attributeName and filterValue shall adhere to the spec here <a href=\"https://avro.apache.org/docs/1.7.7/spec.html#Names\" rel=\"nofollow\">https://avro.apache.org/docs/1.7.7/spec.html#Names</a> to allow for nesting in later implementations of filters.</p>", "body_text": "It's true that you can put any string as the key in a map.\nFilters in maps are not supported.\nKey access in maps is supported.\nThe syntax for that is myMap[myKey] and myKey should be anything -- I'll change that.\nFilter in arrays are supported to a limited extent (only on attributes within the same record that is in the array).\nThe syntax is myArray[attributeName=filterValue].anotherAttributeName to get all another attributes where the attribute name is equal to filter value.\nIn the latter case attributeName and filterValue shall adhere to the spec here https://avro.apache.org/docs/1.7.7/spec.html#Names to allow for nesting in later implementations of filters.", "in_reply_to_id": 210127615}