{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209789186", "pull_request_review_id": 145835935, "id": 209789186, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTc4OTE4Ng==", "diff_hunk": "@@ -0,0 +1,2058 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <regex>\n+#include <string>\n+#include <vector>\n+#include <algorithm>\n+#include <iterator>\n+\n+#include <avro.h>\n+\n+#include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/framework/common_shape_fns.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/framework/partial_tensor_shape.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/strings/numbers.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+\n+using ::tensorflow::shape_inference::ShapeHandle;\n+\n+// As boiler plate for the class I used\n+// tensorflow/core/util/example_proto_helper.h and therein\n+// \"ParseSingleExampleAttrs\".\n+\n+// Checks for valid type for the avro attributes; currently we support bool,\n+// int, long, float, double, string.\n+//\n+// 'dtype' The data type.\n+//\n+// returns OK if any of the supported types; otherwise false.\n+//\n+tensorflow::Status CheckValidType(const tensorflow::DataType& dtype);\n+\n+// Check that all dense shapes are defined. Here, 'defined' means that:\n+// * All shapes have at least one dimension.\n+// * A shape can have an undefined dimension -1, as first dimension.\n+//\n+// 'dense_shape' The dense shapes.\n+//\n+// returns OK if the shapes are defined; otherwise false.\n+//\n+tensorflow::Status CheckDenseShapeToBeDefined(\n+    const std::vector<tensorflow::PartialTensorShape>& dense_shapes);\n+\n+// Struct that holds information about dense tensors that is used during\n+// parsing.\n+struct DenseInformation {\n+  tensorflow::DataType type;             // Type\n+  tensorflow::PartialTensorShape shape;  // Shape\n+  bool variable_length;  // This dense tensor has a variable length in the 2nd\n+                         // dimension\n+  std::size_t elements_per_stride;  // Number of elements per stride\n+};\n+\n+// This class holds the attributes passed into the parse avro record function.\n+// In addition, it builds up information about the 'elements per stride',\n+// 'variable length' for dense tensors, and\n+// 'dense shape' information.\n+class ParseAvroAttrs {\n+ public:\n+  // Initializes the attribute information\n+  template <typename ContextType>\n+  tensorflow::Status Init(ContextType* ctx) {\n+    std::vector<tensorflow::DataType> dense_types;\n+    std::vector<tensorflow::PartialTensorShape> dense_shapes;\n+\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"Nsparse\", &num_sparse));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"Ndense\", &num_dense));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"sparse_types\", &sparse_types));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"Tdense\", &dense_types));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"dense_shapes\", &dense_shapes));\n+\n+    // Check that all dense shapes are defined\n+    TF_RETURN_IF_ERROR(CheckDenseShapeToBeDefined(dense_shapes));\n+\n+    for (int i_dense = 0; i_dense < dense_shapes.size(); ++i_dense) {\n+      DenseInformation dense_info;\n+      tensorflow::TensorShape dense_shape;\n+      // This is the case where we have a fixed len sequence feature, and the\n+      // 1st dimension is undefined.\n+      if (dense_shapes[i_dense].dims() > 0 &&\n+          dense_shapes[i_dense].dim_size(0) == -1) {\n+        dense_info.variable_length = true;\n+        for (int d = 1; d < dense_shapes[i_dense].dims(); ++d) {\n+          dense_shape.AddDim(dense_shapes[i_dense].dim_size(d));\n+        }\n+        // This is the case where all dimensions are defined.\n+      } else {\n+        dense_info.variable_length = false;\n+        dense_shapes[i_dense].AsTensorShape(&dense_shape);\n+      }\n+      // Fill in the remaining information into the dense info and add it to to\n+      // the vector\n+      dense_info.elements_per_stride = dense_shape.num_elements();\n+      dense_info.shape = dense_shapes[i_dense];\n+      dense_info.type = dense_types[i_dense];\n+      dense_infos.push_back(dense_info);\n+    }\n+    return FinishInit();\n+  }\n+\n+  // All these attributes are publicly accessible, hence we did not suffix them\n+  // with '_'.\n+  tensorflow::int64 num_sparse;  // Number of sparse features\n+  tensorflow::int64 num_dense;   // Number of dense features (fixed and variable\n+                                 // length)\n+  std::vector<tensorflow::DataType> sparse_types;  // Types for sparse features\n+  std::vector<DenseInformation> dense_infos;  // Information about each dense\n+                                              // tensor\n+ private:\n+  tensorflow::Status FinishInit();  // for context-independent parts of Init.\n+};\n+\n+// As boiler plate I used tensorflow/core/util/example_proto_helper.cc and\n+// therein \"ParseSingleExampleAttrs\" and\n+Status CheckValidType(const DataType& dtype) {\n+  switch (dtype) {\n+    case DT_BOOL:\n+    case DT_INT32:\n+    case DT_INT64:\n+    case DT_FLOAT:\n+    case DT_DOUBLE:\n+    case DT_STRING:\n+      return Status::OK();\n+    default:\n+      return errors::InvalidArgument(\"Received input dtype: \",\n+                                     DataTypeString(dtype));\n+  }\n+}\n+\n+Status CheckDenseShapeToBeDefined(\n+    const std::vector<PartialTensorShape>& dense_shapes) {\n+  for (int i = 0; i < dense_shapes.size(); ++i) {\n+    const PartialTensorShape& dense_shape = dense_shapes[i];\n+    bool shape_ok = true;\n+    if (dense_shape.dims() == -1) {\n+      shape_ok = false;\n+    } else {\n+      for (int d = 1; d < dense_shape.dims() && shape_ok; ++d) {\n+        if (dense_shape.dim_size(d) == -1) {\n+          shape_ok = false;\n+        }\n+      }\n+    }\n+    if (!shape_ok) {\n+      return errors::InvalidArgument(\n+          \"dense_shapes[\", i,\n+          \"] has unknown rank or unknown inner dimensions: \",\n+          dense_shape.DebugString());\n+    }\n+  }\n+  return Status::OK();\n+}\n+\n+// Finishes the initialization for the attributes, which essentially checks that\n+// the attributes have the correct values.\n+//\n+// returns OK if all attributes are valid; otherwise false.\n+Status ParseAvroAttrs::FinishInit() {\n+  if (static_cast<size_t>(num_sparse) != sparse_types.size()) {\n+    return errors::InvalidArgument(\"len(sparse_keys) != len(sparse_types)\");\n+  }\n+  if (static_cast<size_t>(num_dense) != dense_infos.size()) {\n+    return errors::InvalidArgument(\"len(dense_keys) != len(dense_infos)\");\n+  }\n+  if (num_dense > std::numeric_limits<int32>::max()) {\n+    return errors::InvalidArgument(\"num_dense_ too large\");\n+  }\n+  for (const DenseInformation& dense_info : dense_infos) {\n+    TF_RETURN_IF_ERROR(CheckValidType(dense_info.type));\n+  }\n+  for (const DataType& type : sparse_types) {\n+    TF_RETURN_IF_ERROR(CheckValidType(type));\n+  }\n+  return Status::OK();\n+}\n+\n+// Register the parse function when building the shared library\n+// For the op I used as boiler plate: tensorflow/core/ops/parsing_ops.cc and\n+// there 'ParseExample'\n+// For the op kernel I used as boiler plate:\n+// tensorflow/core/kernels/example_parsing_ops.cc and there 'ExampleParserOp'\n+// For the compute method I used as boiler plate:\n+// tensorflow/core/util/example_proto_fast_parsing.cc and there the\n+//   method 'FastParseExample'\n+\n+REGISTER_OP(\"ParseAvroRecord\")\n+    .Input(\"serialized: string\")\n+    .Input(\"sparse_keys: Nsparse * string\")\n+    .Input(\"dense_keys: Ndense * string\")\n+    .Input(\"dense_defaults: Tdense\")\n+    .Output(\"sparse_indices: Nsparse * int64\")\n+    .Output(\"sparse_values: sparse_types\")\n+    .Output(\"sparse_shapes: Nsparse * int64\")\n+    .Output(\"dense_values: Tdense\")\n+    .Attr(\"Nsparse: int >= 0\")  // Inferred from sparse_keys\n+    .Attr(\"Ndense: int >= 0\")   // Inferred from dense_keys\n+    .Attr(\"sparse_types: list({float,double,int64,int32,string,bool}) >= 0\")\n+    .Attr(\"Tdense: list({float,double,int64,int32,string,bool}) >= 0\")\n+    .Attr(\"dense_shapes: list(shape) >= 0\")\n+    .Attr(\"schema: string\")\n+    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n+\n+       ParseAvroAttrs attrs;\n+       TF_RETURN_IF_ERROR(attrs.Init(c));\n+\n+       // Get the batch size and load it into input\n+       ShapeHandle input;\n+       TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &input));\n+\n+       // Get the schema, parse it, and log it\n+       string schema;\n+       TF_RETURN_IF_ERROR(c->GetAttr(\"schema\", &schema));\n+\n+       std::unique_ptr<avro_schema_t, std::function<void(avro_schema_t*)>>\n+           p_reader_schema(new avro_schema_t, [](avro_schema_t* ptr) {\n+             avro_schema_decref(*ptr);\n+           });\n+       if (avro_schema_from_json_length(schema.c_str(), schema.length(),\n+                                        &(*p_reader_schema)) != 0) {\n+         return errors::InvalidArgument(\"The provided json schema is invalid. \",\n+                                        avro_strerror());\n+       }\n+       LOG(INFO) << \"Avro parser with schema\\n\" << schema;\n+\n+       int output_idx = 0;\n+\n+       // Output sparse_indices, sparse_values, sparse_shapes\n+       for (int i_sparse = 0; i_sparse < attrs.num_sparse; ++i_sparse) {\n+         c->set_output(output_idx++, c->Matrix(c->UnknownDim(), 2));\n+       }\n+       for (int i_sparse = 0; i_sparse < attrs.num_sparse; ++i_sparse) {\n+         c->set_output(output_idx++, c->Vector(c->UnknownDim()));\n+       }\n+       for (int i_sparse = 0; i_sparse < attrs.num_sparse; ++i_sparse) {\n+         c->set_output(output_idx++, c->Vector(2));\n+       }\n+\n+       // Output dense_values\n+       for (int i_dense = 0; i_dense < attrs.num_dense; ++i_dense) {\n+         ShapeHandle dense;\n+         TF_RETURN_IF_ERROR(c->MakeShapeFromPartialTensorShape(\n+             attrs.dense_infos[i_dense].shape, &dense));\n+         TF_RETURN_IF_ERROR(c->Concatenate(input, dense, &dense));\n+         c->set_output(output_idx++, dense);\n+       }\n+\n+       return Status::OK();\n+     })\n+    .Doc(R\"doc(\n+      Parses a serialized avro record that follows the supplied schema into typed tensors.\n+      serialized: A vector containing a batch of binary serialized avro records.\n+      dense_keys: A list of Ndense string Tensors.\n+        The keys expected are associated with dense values.\n+      dense_defaults: A list of Ndense Tensors (some may be empty).\n+        These defaults can either be fully defined for all values in the tensor or they\n+        can be defined as padding element that is used whenever the original input data\n+        misses values to fill out the full tensor.\n+      dense_shapes: A list of Ndense shapes; the shapes of the dense tensors.\n+        The number of elements corresponding to dense_key[j]\n+        must always equal dense_shapes[j].NumEntries().\n+        If dense_shapes[j] == (D0, D1, ..., DN) then the shape of output\n+        Tensor dense_values[j] will be (|serialized|, D0, D1, ..., DN):\n+        The dense outputs are just the inputs row-stacked by batch.\n+        This works for dense_shapes[j] = (-1, D1, ..., DN).  In this case\n+        the shape of the output Tensor dense_values[j] will be\n+        (|serialized|, M, D1, .., DN), where M is the maximum number of blocks\n+        of elements of length D1 * .... * DN, across all minibatch entries\n+        in the input.  Any minibatch entry with less than M blocks of elements of\n+        length D1 * ... * DN will be padded with the corresponding default_value\n+        scalar element along the second dimension.\n+      dense_types: A list of Ndense types; the type of data in each Feature given in dense_keys.\n+      sparse_keys: A list of Ndense string Tensors (scalars).\n+        The keys expected are associated with sparse values.\n+      sparse_types: A list of Nsparse types; the data types of data in each Feature\n+        given in sparse_keys.\n+      schema: A string that describes the avro schema of the underlying serialized avro string.\n+        Currently the parse function supports the primitive types DT_STRING, DT_DOUBLE, DT_FLOAT,\n+        DT_INT64, DT_INT32, and DT_BOOL.\n+        The supported avro version depends on the compiled library avro library linked against\n+        TensorFlow during build. More instructions on avro:\n+        https://avro.apache.org/docs/1.8.1/spec.html\n+    )doc\");", "path": "tensorflow/contrib/avro/ops/parse_avro_record.cc", "position": 301, "original_position": 304, "commit_id": "bdf790d5c888e59d593230286edaaf5314daaee5", "original_commit_id": "d1be9e544357e72eb37f6adb626670f745e79a9b", "user": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "body": "Put the REGISTER_OP at the top of the file, since it's good documentation for what's implemented in it.", "created_at": "2018-08-13T23:23:42Z", "updated_at": "2018-10-22T21:28:38Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/18224#discussion_r209789186", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18224", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209789186"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/18224#discussion_r209789186"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18224"}}, "body_html": "<p>Put the REGISTER_OP at the top of the file, since it's good documentation for what's implemented in it.</p>", "body_text": "Put the REGISTER_OP at the top of the file, since it's good documentation for what's implemented in it."}