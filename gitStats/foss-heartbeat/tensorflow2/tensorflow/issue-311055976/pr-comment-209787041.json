{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209787041", "pull_request_review_id": 145835935, "id": 209787041, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTc4NzA0MQ==", "diff_hunk": "@@ -0,0 +1,419 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <avro.h>\n+\n+#include \"tensorflow/core/framework/dataset.h\"\n+#include \"tensorflow/core/framework/common_shape_fns.h\"\n+#include \"tensorflow/core/framework/partial_tensor_shape.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/io/buffered_inputstream.h\"\n+#include \"tensorflow/core/lib/io/inputbuffer.h\"\n+\n+// As boiler plate I used\n+// https://github.com/tensorflow/tensorflow/core/kernels/reader_dataset_ops.cc\n+// https://github.com/tensorflow/tensorflow/blob/v1.4.1/tensorflow/core/ops/dataset_ops.cc\n+// (register op)\n+\n+namespace tensorflow {\n+\n+// Register the avro record dataset operator\n+REGISTER_OP(\"AvroRecordDataset\")\n+    .Input(\"filenames: string\")\n+    .Input(\"schema: string\")\n+    .Input(\"buffer_size: int64\")\n+    .Output(\"handle: variant\")\n+    .SetIsStateful()\n+    .SetShapeFn(shape_inference::ScalarShape)\n+    .Doc(R\"doc(\n+Creates a dataset that emits the avro records from one or more files.\n+filenames: A scalar or vector containing the name(s) of the file(s) to be\n+  read.\n+schema: A string used that is used for schema resolution.\n+)doc\");\n+\n+// This class represents the avro reader options\n+class AvroReaderOptions {\n+ public:\n+  // Creates avro reader options with the given schema and buffer size.\n+  //\n+  static AvroReaderOptions CreateAvroReaderOptions(const string& schema,\n+                                                   int64 buffer_size) {\n+    AvroReaderOptions options;\n+    options.schema = schema;\n+    options.buffer_size = buffer_size;\n+    return options;\n+  }\n+  string schema;\n+  int64 buffer_size =\n+      256 * 1024;  // 256 kB as default but this can be overwritten by the user\n+};\n+\n+void AvroFileReaderDestructor(avro_file_reader_t reader) {\n+  // I don't think we need the CHECK_NOTNULL\n+  CHECK_GE(avro_file_reader_close(reader), 0);\n+}\n+\n+void AvroSchemaDestructor(avro_schema_t schema) {\n+  // Confusingly, it appears that the avro_file_reader_t creates its\n+  // own reference to this schema, so the schema is not really\n+  // \"uniquely\" owned...\n+  CHECK_GE(avro_schema_decref(schema), 0);\n+};\n+\n+void AvroValueInterfaceDestructor(avro_value_iface_t * iface)  {\n+  avro_value_iface_decref(iface);\n+}\n+\n+\n+// This reader is not thread safe\n+class SequentialAvroRecordReader {\n+ public:\n+  // Construct a sequential avro record reader\n+  //\n+  // 'file' is the random access file\n+  //\n+  // 'file_size' is the size of the file\n+  //\n+  // 'filename' is the name of the file\n+  //\n+  // 'options' are avro reader options\n+  //\n+  SequentialAvroRecordReader(RandomAccessFile* file, const uint64 file_size,\n+                             const string& filename,\n+                             const AvroReaderOptions& options =\n+                                 AvroReaderOptions())\n+    : initialized_(false),\n+      filename_(filename),\n+      file_buffer_(file_size, '\\0'),\n+      input_buffer_size_(options.buffer_size),\n+      input_buffer_(new io::InputBuffer(file, options.buffer_size)),\n+      reader_schema_str_(options.schema),\n+      file_reader_(nullptr, AvroFileReaderDestructor),\n+      reader_schema_(nullptr, AvroSchemaDestructor),\n+      writer_schema_(nullptr, AvroSchemaDestructor),\n+      p_reader_iface_(nullptr, AvroValueInterfaceDestructor),\n+      p_writer_iface_(nullptr, AvroValueInterfaceDestructor) { }\n+  virtual ~SequentialAvroRecordReader() {\n+    // Guard against clean-up of non-initialized instances\n+    if (initialized_) {\n+      avro_value_decref(&reader_value_);\n+      avro_value_decref(&writer_value_);\n+    }\n+  }\n+  // Reads the next record into the string record\n+  //\n+  // 'record' pointer to the string where to load the record in\n+  //\n+  // returns Status about this operation\n+  //\n+  Status ReadRecord(string* record) {\n+    bool at_end =\n+      avro_file_reader_read_value(file_reader_.get(), &writer_value_) != 0;\n+    // Are writer_value_ and reader_value_ aliases to the same thing???\n+    size_t len;\n+    if (avro_value_sizeof(&reader_value_, &len)) {\n+      return Status(errors::InvalidArgument(\"Could not find size of value, \",\n+                                            avro_strerror()));\n+    }\n+    record->resize(len);\n+    avro_writer_t mem_writer = avro_writer_memory(record->data(), len);\n+    if (avro_value_write(mem_writer, &reader_value_)) {\n+      avro_writer_free(mem_writer);\n+      return Status(errors::InvalidArgument(\"Unable to write value to memory.\"));\n+    }\n+    avro_writer_free(mem_writer);\n+    return at_end ? errors::OutOfRange(\"eof\") : Status::OK();\n+  }\n+  // Call for startup of work after construction. Loads data into memory and\n+  // sets up the avro file reader\n+  //\n+  // returns Status about this operation\n+  //\n+  Status OnWorkStartup() {\n+    // Clear the error message, so we won't get a wrong message\n+    avro_set_error(\"\");\n+    Status status;\n+\n+    // Read the file into memory via the gfile API so we can accept\n+    // files on S3, HDFS, etc.\n+    TF_RETURN_IF_ERROR(CreateAndLoadFileIntoBuffer(input_buffer_size_));\n+    FILE* fp = fmemopen(static_cast<void*>(const_cast<char*>(file_buffer_.data())),\n+                        file_buffer_.size(), \"r\");\n+    if (fp == nullptr) {\n+      return Status(errors::InvalidArgument(\"Unable to open file \", filename_,\n+                                            \" on memory in avro reader.\"));\n+    }\n+\n+    // Get an avro file reader for that file handle, the 1 indicates to close\n+    // the file handle when done\n+    avro_file_reader_t file_reader_tmp;\n+    if (avro_file_reader_fp(fp, filename_.c_str(), 1, &file_reader_tmp) != 0) {\n+      return Status(errors::InvalidArgument(\"Unable to open file \", filename_,\n+                                            \" in avro reader. \", avro_strerror()));\n+    }\n+    file_reader_.reset(file_reader_tmp);\n+\n+    writer_schema_.reset(avro_file_reader_get_writer_schema(file_reader_.get()));\n+\n+    // The user provided a schema for the reader, check if we need to do schema\n+    // resolution\n+    bool do_resolution = false;\n+    if (reader_schema_str_.length() > 0) {\n+\n+      avro_schema_t reader_schema_tmp;\n+      // Create value to read into using the provided schema\n+      if (avro_schema_from_json_length(reader_schema_str_.data(),\n+                                       reader_schema_str_.length(),\n+                                       &reader_schema_tmp) != 0) {\n+        return Status(errors::InvalidArgument(\n+            \"The provided json schema is invalid. \", avro_strerror()));\n+      }\n+      reader_schema_.reset(reader_schema_tmp);\n+      do_resolution = !avro_schema_equal(writer_schema_.get(), reader_schema_.get());\n+      // We need to do a schema resolution, if the schemas are not the same\n+    }\n+\n+    if (do_resolution) {\n+      // Create reader class\n+      p_reader_iface_.reset(avro_generic_class_from_schema(reader_schema_.get()));\n+      // Create instance for reader class\n+      if (avro_generic_value_new(p_reader_iface_.get(), &reader_value_) != 0) {\n+        return Status(errors::InvalidArgument(\n+            \"Unable to value for user-supplied schema. \", avro_strerror()));\n+      }\n+      // Create resolved writer class\n+      p_writer_iface_.reset(avro_resolved_writer_new(writer_schema_.get(), reader_schema_.get()));\n+      if (p_writer_iface_.get() == nullptr) {\n+        // Cleanup\n+        avro_value_decref(&reader_value_);\n+        return Status(errors::InvalidArgument(\"Schemas are incompatible. \",\n+                                              avro_strerror()));\n+      }\n+      // Create instance for resolved writer class\n+      if (avro_resolved_writer_new_value(p_writer_iface_.get(), &writer_value_) !=\n+          0) {\n+        // Cleanup\n+        avro_value_decref(&reader_value_);\n+        return Status(\n+            errors::InvalidArgument(\"Unable to create resolved writer.\"));\n+      }\n+      avro_resolved_writer_set_dest(&writer_value_, &reader_value_);\n+    } else {\n+      p_writer_iface_.reset(avro_generic_class_from_schema(writer_schema_.get()));\n+      if (avro_generic_value_new(p_writer_iface_.get(), &writer_value_) != 0) {\n+        return Status(errors::InvalidArgument(\n+            \"Unable to create instance for generic class.\"));\n+      }\n+      // The reader_value_ is the same as the writer_value_ in the case we do\n+      // not need to resolve the schema\n+      avro_value_copy_ref(&reader_value_, &writer_value_);\n+    }\n+\n+    // We initialized this avro record reader\n+    initialized_ = true;\n+\n+    return Status::OK();\n+  }\n+\n+ private:\n+  // Loads file contents into file_buffer_\n+  //\n+  // 'read_buffer_size' buffer size when reading file contents\n+  //\n+  Status CreateAndLoadFileIntoBuffer(int64 read_buffer_size) {\n+    int64 total_bytes_read = 0;\n+    Status status;\n+\n+    // While we still need to read data\n+    char* buffer = const_cast<char*>(file_buffer_.data());\n+    while (total_bytes_read < file_buffer_.size()) {\n+      size_t bytes_read;\n+      status = input_buffer_->ReadNBytes(read_buffer_size, buffer,\n+                                         &bytes_read);\n+      total_bytes_read += bytes_read;\n+      buffer += bytes_read;\n+      // If we are at the end of the file\n+      if (errors::IsOutOfRange(status)) {\n+        break;\n+      } else if (!status.ok()) {\n+        return status;\n+      }\n+    }\n+\n+    CHECK_EQ(total_bytes_read, file_buffer_.size());\n+    return Status::OK();\n+  }\n+\n+  bool initialized_;                               // Has been initialized\n+  const string filename_;                                // Name of the file\n+  std::string file_buffer_;                        // The data buffer\n+  const size_t input_buffer_size_;\n+  std::unique_ptr<io::InputBuffer> input_buffer_;  // input buffer used to load\n+                                                   // from random access file\n+  const string reader_schema_str_;  // User supplied string to read this avro\n+                                    // file\n+\n+  using AvroFileReaderUPtr = std::unique_ptr<struct avro_file_reader_t_,\n+                                             void(*)(avro_file_reader_t)>;\n+  AvroFileReaderUPtr file_reader_;  // Avro file reader\n+  // TODO: Use std::remove_pointer\n+  using AvroSchemaUPtr = std::unique_ptr<struct avro_obj_t,\n+                                         void(*)(avro_schema_t)>;\n+  AvroSchemaUPtr reader_schema_;  // Schema to read, set only when doing schema\n+                                  // resolution\n+  AvroSchemaUPtr writer_schema_; // Schema that the file was written with\n+  using AvroValueInterfacePtr = std::unique_ptr<avro_value_iface_t,\n+                                                void(*)(avro_value_iface_t*)>;\n+  AvroValueInterfacePtr p_reader_iface_;  // Reader class info to create instances\n+  AvroValueInterfacePtr p_writer_iface_;  // Writer class info to create instances\n+  avro_value_t reader_value_;  // Reader value, unequal from writer value when\n+                               // doing schema resolution\n+  avro_value_t writer_value_;  // Writer value\n+};\n+\n+class AvroRecordDatasetOp : public DatasetOpKernel {\n+ public:\n+  using DatasetOpKernel::DatasetOpKernel;\n+\n+  void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n+    const Tensor* filenames_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"filenames\", &filenames_tensor));\n+    OP_REQUIRES(\n+        ctx, filenames_tensor->dims() <= 1,\n+        errors::InvalidArgument(\"`filenames` must be a scalar or a vector.\"));\n+\n+    std::vector<string> filenames;\n+    filenames.reserve(filenames_tensor->NumElements());\n+    for (int i = 0; i < filenames_tensor->NumElements(); ++i) {\n+      filenames.push_back(filenames_tensor->flat<string>()(i));\n+    }\n+\n+    string schema;\n+    OP_REQUIRES_OK(ctx, ParseScalarArgument<string>(ctx, \"schema\", &schema));\n+\n+    int64 buffer_size = -1;\n+    OP_REQUIRES_OK(\n+        ctx, ParseScalarArgument<int64>(ctx, \"buffer_size\", &buffer_size));\n+    OP_REQUIRES(ctx, buffer_size >= 256,\n+                errors::InvalidArgument(\"`buffer_size` must be >= 256 B\"));\n+\n+    *output = new Dataset(std::move(filenames), schema, buffer_size);\n+  }\n+\n+ private:\n+  class Dataset : public DatasetBase {\n+   public:\n+    explicit Dataset(std::vector<string> filenames, const string& schema,\n+                     int64 buffer_size)\n+        : filenames_(std::move(filenames)),\n+          options_(AvroReaderOptions::CreateAvroReaderOptions(schema,\n+                                                              buffer_size)) {}\n+\n+    std::unique_ptr<IteratorBase> MakeIteratorInternal(const string& prefix) const\n+        override {\n+      return std::unique_ptr<IteratorBase>(\n+          new Iterator({this, strings::StrCat(prefix, \"::AvroRecord\")}));\n+    }\n+\n+    const DataTypeVector& output_dtypes() const override {\n+      static DataTypeVector* dtypes = new DataTypeVector({DT_STRING});\n+      return *dtypes;\n+    }\n+\n+    const std::vector<PartialTensorShape>& output_shapes() const override {\n+      static std::vector<PartialTensorShape>* shapes =\n+          new std::vector<PartialTensorShape>({{}});\n+      return *shapes;\n+    }\n+\n+    string DebugString() const override { return \"AvroRecordDatasetOp::Dataset\"; }\n+\n+   private:\n+    class Iterator : public DatasetIterator<Dataset> {\n+     public:\n+      explicit Iterator(const Params& params)\n+          : DatasetIterator<Dataset>(params) {}\n+\n+      Status GetNextInternal(IteratorContext* ctx,\n+                             std::vector<Tensor>* out_tensors,\n+                             bool* end_of_sequence) override {\n+        mutex_lock l(mu_);\n+        do { // What is the point of this loop???\n+          // We are currently processing a file, so try to read the next record.\n+          if (reader_) {\n+            Tensor result_tensor(cpu_allocator(), DT_STRING, {});\n+            Status s = reader_->ReadRecord(&result_tensor.scalar<string>()());\n+            if (s.ok()) {\n+              out_tensors->emplace_back(std::move(result_tensor));\n+              *end_of_sequence = false;\n+              return Status::OK();\n+            } else if (!errors::IsOutOfRange(s)) {\n+              return s;\n+            } else {\n+              CHECK(errors::IsOutOfRange(s));\n+              // We have reached the end of the current file, so maybe\n+              // move on to next file.\n+              reader_.reset();\n+              file_.reset();\n+              ++current_file_index_;\n+            }\n+          }\n+\n+          // Iteration ends when there are no more files to process.\n+          if (current_file_index_ == dataset()->filenames_.size()) {\n+            *end_of_sequence = true;\n+            return Status::OK();\n+          }\n+\n+          // Actually move on to next file.\n+          // Looks like this cannot request multiple files in parallel. Hmm.", "path": "tensorflow/contrib/avro/ops/avro_record_dataset.cc", "position": 343, "original_position": 384, "commit_id": "bdf790d5c888e59d593230286edaaf5314daaee5", "original_commit_id": "d1be9e544357e72eb37f6adb626670f745e79a9b", "user": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "body": "Turn this comment into a TODO?", "created_at": "2018-08-13T23:10:32Z", "updated_at": "2018-10-22T21:28:38Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/18224#discussion_r209787041", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18224", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209787041"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/18224#discussion_r209787041"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18224"}}, "body_html": "<p>Turn this comment into a TODO?</p>", "body_text": "Turn this comment into a TODO?"}