{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/226532484", "pull_request_review_id": 166377597, "id": 226532484, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNjUzMjQ4NA==", "diff_hunk": "@@ -0,0 +1,2058 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <regex>\n+#include <string>\n+#include <vector>\n+#include <algorithm>\n+#include <iterator>\n+\n+#include <avro.h>\n+\n+#include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n+#include \"tensorflow/core/framework/common_shape_fns.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/framework/partial_tensor_shape.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/strings/numbers.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+\n+namespace tensorflow {\n+\n+using ::tensorflow::shape_inference::ShapeHandle;\n+\n+// As boiler plate for the class I used\n+// tensorflow/core/util/example_proto_helper.h and therein\n+// \"ParseSingleExampleAttrs\".\n+\n+// Checks for valid type for the avro attributes; currently we support bool,\n+// int, long, float, double, string.\n+//\n+// 'dtype' The data type.\n+//\n+// returns OK if any of the supported types; otherwise false.\n+//\n+tensorflow::Status CheckValidType(const tensorflow::DataType& dtype);\n+\n+// Check that all dense shapes are defined. Here, 'defined' means that:\n+// * All shapes have at least one dimension.\n+// * A shape can have an undefined dimension -1, as first dimension.\n+//\n+// 'dense_shape' The dense shapes.\n+//\n+// returns OK if the shapes are defined; otherwise false.\n+//\n+tensorflow::Status CheckDenseShapeToBeDefined(\n+    const std::vector<tensorflow::PartialTensorShape>& dense_shapes);\n+\n+// Struct that holds information about dense tensors that is used during\n+// parsing.\n+struct DenseInformation {\n+  tensorflow::DataType type;             // Type\n+  tensorflow::PartialTensorShape shape;  // Shape\n+  bool variable_length;  // This dense tensor has a variable length in the 2nd\n+                         // dimension\n+  std::size_t elements_per_stride;  // Number of elements per stride\n+};\n+\n+// This class holds the attributes passed into the parse avro record function.\n+// In addition, it builds up information about the 'elements per stride',\n+// 'variable length' for dense tensors, and\n+// 'dense shape' information.\n+class ParseAvroAttrs {\n+ public:\n+  // Initializes the attribute information\n+  template <typename ContextType>\n+  tensorflow::Status Init(ContextType* ctx) {\n+    std::vector<tensorflow::DataType> dense_types;\n+    std::vector<tensorflow::PartialTensorShape> dense_shapes;\n+\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"Nsparse\", &num_sparse));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"Ndense\", &num_dense));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"sparse_types\", &sparse_types));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"Tdense\", &dense_types));\n+    TF_RETURN_IF_ERROR(ctx->GetAttr(\"dense_shapes\", &dense_shapes));\n+\n+    // Check that all dense shapes are defined\n+    TF_RETURN_IF_ERROR(CheckDenseShapeToBeDefined(dense_shapes));\n+\n+    for (int i_dense = 0; i_dense < dense_shapes.size(); ++i_dense) {\n+      DenseInformation dense_info;\n+      tensorflow::TensorShape dense_shape;\n+      // This is the case where we have a fixed len sequence feature, and the\n+      // 1st dimension is undefined.\n+      if (dense_shapes[i_dense].dims() > 0 &&\n+          dense_shapes[i_dense].dim_size(0) == -1) {\n+        dense_info.variable_length = true;\n+        for (int d = 1; d < dense_shapes[i_dense].dims(); ++d) {\n+          dense_shape.AddDim(dense_shapes[i_dense].dim_size(d));\n+        }\n+        // This is the case where all dimensions are defined.\n+      } else {\n+        dense_info.variable_length = false;\n+        dense_shapes[i_dense].AsTensorShape(&dense_shape);\n+      }\n+      // Fill in the remaining information into the dense info and add it to to\n+      // the vector\n+      dense_info.elements_per_stride = dense_shape.num_elements();\n+      dense_info.shape = dense_shapes[i_dense];\n+      dense_info.type = dense_types[i_dense];\n+      dense_infos.push_back(dense_info);\n+    }\n+    return FinishInit();\n+  }\n+\n+  // All these attributes are publicly accessible, hence we did not suffix them\n+  // with '_'.\n+  tensorflow::int64 num_sparse;  // Number of sparse features\n+  tensorflow::int64 num_dense;   // Number of dense features (fixed and variable\n+                                 // length)\n+  std::vector<tensorflow::DataType> sparse_types;  // Types for sparse features\n+  std::vector<DenseInformation> dense_infos;  // Information about each dense\n+                                              // tensor\n+ private:\n+  tensorflow::Status FinishInit();  // for context-independent parts of Init.\n+};\n+\n+// As boiler plate I used tensorflow/core/util/example_proto_helper.cc and\n+// therein \"ParseSingleExampleAttrs\" and\n+Status CheckValidType(const DataType& dtype) {\n+  switch (dtype) {\n+    case DT_BOOL:\n+    case DT_INT32:\n+    case DT_INT64:\n+    case DT_FLOAT:\n+    case DT_DOUBLE:\n+    case DT_STRING:\n+      return Status::OK();\n+    default:\n+      return errors::InvalidArgument(\"Received input dtype: \",\n+                                     DataTypeString(dtype));\n+  }\n+}\n+\n+Status CheckDenseShapeToBeDefined(\n+    const std::vector<PartialTensorShape>& dense_shapes) {\n+  for (int i = 0; i < dense_shapes.size(); ++i) {\n+    const PartialTensorShape& dense_shape = dense_shapes[i];\n+    bool shape_ok = true;\n+    if (dense_shape.dims() == -1) {\n+      shape_ok = false;\n+    } else {\n+      for (int d = 1; d < dense_shape.dims() && shape_ok; ++d) {\n+        if (dense_shape.dim_size(d) == -1) {\n+          shape_ok = false;\n+        }\n+      }\n+    }\n+    if (!shape_ok) {\n+      return errors::InvalidArgument(\n+          \"dense_shapes[\", i,\n+          \"] has unknown rank or unknown inner dimensions: \",\n+          dense_shape.DebugString());\n+    }\n+  }\n+  return Status::OK();\n+}\n+\n+// Finishes the initialization for the attributes, which essentially checks that\n+// the attributes have the correct values.\n+//\n+// returns OK if all attributes are valid; otherwise false.\n+Status ParseAvroAttrs::FinishInit() {\n+  if (static_cast<size_t>(num_sparse) != sparse_types.size()) {\n+    return errors::InvalidArgument(\"len(sparse_keys) != len(sparse_types)\");\n+  }\n+  if (static_cast<size_t>(num_dense) != dense_infos.size()) {\n+    return errors::InvalidArgument(\"len(dense_keys) != len(dense_infos)\");\n+  }\n+  if (num_dense > std::numeric_limits<int32>::max()) {\n+    return errors::InvalidArgument(\"num_dense_ too large\");\n+  }\n+  for (const DenseInformation& dense_info : dense_infos) {\n+    TF_RETURN_IF_ERROR(CheckValidType(dense_info.type));\n+  }\n+  for (const DataType& type : sparse_types) {\n+    TF_RETURN_IF_ERROR(CheckValidType(type));\n+  }\n+  return Status::OK();\n+}\n+\n+// Register the parse function when building the shared library\n+// For the op I used as boiler plate: tensorflow/core/ops/parsing_ops.cc and\n+// there 'ParseExample'\n+// For the op kernel I used as boiler plate:\n+// tensorflow/core/kernels/example_parsing_ops.cc and there 'ExampleParserOp'\n+// For the compute method I used as boiler plate:\n+// tensorflow/core/util/example_proto_fast_parsing.cc and there the\n+//   method 'FastParseExample'\n+\n+REGISTER_OP(\"ParseAvroRecord\")", "path": "tensorflow/contrib/avro/ops/parse_avro_record.cc", "position": 205, "original_position": 208, "commit_id": "bdf790d5c888e59d593230286edaaf5314daaee5", "original_commit_id": "d1be9e544357e72eb37f6adb626670f745e79a9b", "user": {"login": "erwa", "id": 544734, "node_id": "MDQ6VXNlcjU0NDczNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/544734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erwa", "html_url": "https://github.com/erwa", "followers_url": "https://api.github.com/users/erwa/followers", "following_url": "https://api.github.com/users/erwa/following{/other_user}", "gists_url": "https://api.github.com/users/erwa/gists{/gist_id}", "starred_url": "https://api.github.com/users/erwa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erwa/subscriptions", "organizations_url": "https://api.github.com/users/erwa/orgs", "repos_url": "https://api.github.com/users/erwa/repos", "events_url": "https://api.github.com/users/erwa/events{/privacy}", "received_events_url": "https://api.github.com/users/erwa/received_events", "type": "User", "site_admin": false}, "body": "I agree with @skye. Currently, it seems `AvroRecordDataset` will return string tensors (each row is a string tensor). Then, `parse_avro_record` applies the filters and returns typed tensors for each of the fields in the records. Seems like `AvroRecordDataset` could be initialized with the filters and just directly return the typed tensors.\r\n\r\nMaking this change would require significant refactoring, so I'm fine doing this as a follow-up if performance becomes an issue.", "created_at": "2018-10-19T04:38:41Z", "updated_at": "2018-10-22T21:28:38Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/18224#discussion_r226532484", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18224", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/226532484"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/18224#discussion_r226532484"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/18224"}}, "body_html": "<p>I agree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=88808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/skye\">@skye</a>. Currently, it seems <code>AvroRecordDataset</code> will return string tensors (each row is a string tensor). Then, <code>parse_avro_record</code> applies the filters and returns typed tensors for each of the fields in the records. Seems like <code>AvroRecordDataset</code> could be initialized with the filters and just directly return the typed tensors.</p>\n<p>Making this change would require significant refactoring, so I'm fine doing this as a follow-up if performance becomes an issue.</p>", "body_text": "I agree with @skye. Currently, it seems AvroRecordDataset will return string tensors (each row is a string tensor). Then, parse_avro_record applies the filters and returns typed tensors for each of the fields in the records. Seems like AvroRecordDataset could be initialized with the filters and just directly return the typed tensors.\nMaking this change would require significant refactoring, so I'm fine doing this as a follow-up if performance becomes an issue.", "in_reply_to_id": 209788552}