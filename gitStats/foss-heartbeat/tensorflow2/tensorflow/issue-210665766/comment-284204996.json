{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284204996", "html_url": "https://github.com/tensorflow/tensorflow/issues/7936#issuecomment-284204996", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7936", "id": 284204996, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDIwNDk5Ng==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-05T04:37:47Z", "updated_at": "2017-03-05T04:37:47Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> 's example works because he turns on JIT scope for the specific ops, which is needed if you are not using a GPU and want XLA for CPU to work via JIT.</p>\n<p><code>with jit_scope(compile_ops=True):</code></p>\n<p>I left a note in the document to warn people.</p>\n<blockquote>\n<p>Note: Turning on JIT at the session level will not result in operations being compiled for the CPU. JIT compilation for CPU operations must be done via the manual method documented below. This decision was made due to the CPU backend being single-threaded.</p>\n</blockquote>\n<p>And at the start of the tutorial.</p>\n<blockquote>\n<p>Currently JIT at the session level, which is what is used for the tutorial, only supports GPU.</p>\n</blockquote>\n<p>Do not interpret this as hostile.  Let me know if you think there is a way to make it more clear.  Thank you for trying the tutorial, I am excited people are giving it a try.    Closing issue, please reopen if necessary.</p>", "body_text": "@yaroslavvb 's example works because he turns on JIT scope for the specific ops, which is needed if you are not using a GPU and want XLA for CPU to work via JIT.\nwith jit_scope(compile_ops=True):\nI left a note in the document to warn people.\n\nNote: Turning on JIT at the session level will not result in operations being compiled for the CPU. JIT compilation for CPU operations must be done via the manual method documented below. This decision was made due to the CPU backend being single-threaded.\n\nAnd at the start of the tutorial.\n\nCurrently JIT at the session level, which is what is used for the tutorial, only supports GPU.\n\nDo not interpret this as hostile.  Let me know if you think there is a way to make it more clear.  Thank you for trying the tutorial, I am excited people are giving it a try.    Closing issue, please reopen if necessary.", "body": "@yaroslavvb 's example works because he turns on JIT scope for the specific ops, which is needed if you are not using a GPU and want XLA for CPU to work via JIT.  \r\n\r\n`with jit_scope(compile_ops=True):`\r\n\r\nI left a note in the document to warn people.  \r\n\r\n> Note: Turning on JIT at the session level will not result in operations being compiled for the CPU. JIT compilation for CPU operations must be done via the manual method documented below. This decision was made due to the CPU backend being single-threaded.\r\n\r\nAnd at the start of the tutorial.  \r\n\r\n> Currently JIT at the session level, which is what is used for the tutorial, only supports GPU.\r\n\r\nDo not interpret this as hostile.  Let me know if you think there is a way to make it more clear.  Thank you for trying the tutorial, I am excited people are giving it a try.    Closing issue, please reopen if necessary."}