{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356836719", "html_url": "https://github.com/tensorflow/tensorflow/issues/16008#issuecomment-356836719", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16008", "id": 356836719, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjgzNjcxOQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-11T06:11:54Z", "updated_at": "2018-01-11T07:36:18Z", "author_association": "MEMBER", "body_html": "<p>This question is better asked on  <a href=\"http://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">StackOverflow</a> since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!</p>\n<p>If you post to StackOverflow, you might want to share more information on how you're measuring time. A couple of things to think about:</p>\n<ul>\n<li>If you're measuring the time for the very first call to <code>inferenceInterface.run</code> in your process, then that includes some overhead of GPU initialization by the TensorFlow runtime. So, you might want to discard that.</li>\n<li>The exact numbers will depend on the specifications of the GPU. From a cursory look, the GeForce 940MX <a href=\"https://www.techpowerup.com/gpudb/2797/geforce-940mx\" rel=\"nofollow\">can do 950GFLOPS</a>, while the TitanX that was used to report the numbers in the <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\">Object Detection model zoo</a> can do about 7x that (<a href=\"https://www.techpowerup.com/gpudb/2632/geforce-gtx-titan-x\" rel=\"nofollow\">~6700GFLOPS</a>). So, you could expect almost 7x more time than reported there.</li>\n</ul>\n<p>Long story short, it is possible that with the GPU you're using, you won't see much of a speedup.<br>\nAlso, would be good to make sure that your measurement isn't biased by the first run.</p>\n<p>For example, I tried the following (showing just the relevant snippet) on a 416x416 image:</p>\n<div class=\"highlight highlight-source-java\"><pre><span class=\"pl-k\">try</span> (<span class=\"pl-k\">Tensor&lt;<span class=\"pl-smi\">UInt8</span>&gt;</span> input <span class=\"pl-k\">=</span> makeImageTensor(filename)) {\n  <span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>; i <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">10</span>; <span class=\"pl-k\">++</span>i) {\n    <span class=\"pl-k\">final</span> <span class=\"pl-k\">long</span> start <span class=\"pl-k\">=</span> <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>currentTimeMillis();\n    <span class=\"pl-k\">List&lt;<span class=\"pl-k\">Tensor&lt;?&gt;</span>&gt;</span> outputs <span class=\"pl-k\">=</span>\n            .session\n            .runner()\n            .feed(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>image_tensor<span class=\"pl-pds\">\"</span></span>, input)\n            .fetch(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>detection_scores<span class=\"pl-pds\">\"</span></span>)\n            .fetch(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>detection_classes<span class=\"pl-pds\">\"</span></span>)\n            .fetch(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>detection_boxes<span class=\"pl-pds\">\"</span></span>)\n            .run();\n    <span class=\"pl-k\">final</span> <span class=\"pl-k\">long</span> end <span class=\"pl-k\">=</span> <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>currentTimeMillis();\n    <span class=\"pl-k\">for</span> (<span class=\"pl-k\">Tensor&lt;?&gt;</span> t <span class=\"pl-k\">:</span> outputs) {\n      t<span class=\"pl-k\">.</span>close();\n    }\n    <span class=\"pl-smi\">System</span><span class=\"pl-k\">.</span>out<span class=\"pl-k\">.</span>printf(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>%3d - %d ms<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>, i, (end <span class=\"pl-k\">-</span> start));\n  }\n}  </pre></div>\n<p>Which is only measuring the time it takes to execute the graph, not any time spent in creating the input or other things. With that I see about 1/2 the time taken with GPU than CPU (50ms vs. 100ms). I'm using an Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz CPU and an NVIDIA TitanX GPU.</p>\n<p>Hope that helps.</p>", "body_text": "This question is better asked on  StackOverflow since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\nIf you post to StackOverflow, you might want to share more information on how you're measuring time. A couple of things to think about:\n\nIf you're measuring the time for the very first call to inferenceInterface.run in your process, then that includes some overhead of GPU initialization by the TensorFlow runtime. So, you might want to discard that.\nThe exact numbers will depend on the specifications of the GPU. From a cursory look, the GeForce 940MX can do 950GFLOPS, while the TitanX that was used to report the numbers in the Object Detection model zoo can do about 7x that (~6700GFLOPS). So, you could expect almost 7x more time than reported there.\n\nLong story short, it is possible that with the GPU you're using, you won't see much of a speedup.\nAlso, would be good to make sure that your measurement isn't biased by the first run.\nFor example, I tried the following (showing just the relevant snippet) on a 416x416 image:\ntry (Tensor<UInt8> input = makeImageTensor(filename)) {\n  for (int i = 0; i < 10; ++i) {\n    final long start = System.currentTimeMillis();\n    List<Tensor<?>> outputs =\n            .session\n            .runner()\n            .feed(\"image_tensor\", input)\n            .fetch(\"detection_scores\")\n            .fetch(\"detection_classes\")\n            .fetch(\"detection_boxes\")\n            .run();\n    final long end = System.currentTimeMillis();\n    for (Tensor<?> t : outputs) {\n      t.close();\n    }\n    System.out.printf(\"%3d - %d ms\\n\", i, (end - start));\n  }\n}  \nWhich is only measuring the time it takes to execute the graph, not any time spent in creating the input or other things. With that I see about 1/2 the time taken with GPU than CPU (50ms vs. 100ms). I'm using an Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz CPU and an NVIDIA TitanX GPU.\nHope that helps.", "body": "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nIf you post to StackOverflow, you might want to share more information on how you're measuring time. A couple of things to think about:\r\n\r\n- If you're measuring the time for the very first call to `inferenceInterface.run` in your process, then that includes some overhead of GPU initialization by the TensorFlow runtime. So, you might want to discard that.\r\n- The exact numbers will depend on the specifications of the GPU. From a cursory look, the GeForce 940MX [can do 950GFLOPS](https://www.techpowerup.com/gpudb/2797/geforce-940mx), while the TitanX that was used to report the numbers in the [Object Detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) can do about 7x that ([~6700GFLOPS](https://www.techpowerup.com/gpudb/2632/geforce-gtx-titan-x)). So, you could expect almost 7x more time than reported there.\r\n\r\nLong story short, it is possible that with the GPU you're using, you won't see much of a speedup.\r\nAlso, would be good to make sure that your measurement isn't biased by the first run.\r\n\r\nFor example, I tried the following (showing just the relevant snippet) on a 416x416 image:\r\n\r\n```java\r\ntry (Tensor<UInt8> input = makeImageTensor(filename)) {\r\n  for (int i = 0; i < 10; ++i) {\r\n    final long start = System.currentTimeMillis();\r\n    List<Tensor<?>> outputs =\r\n            .session\r\n            .runner()\r\n            .feed(\"image_tensor\", input)\r\n            .fetch(\"detection_scores\")\r\n            .fetch(\"detection_classes\")\r\n            .fetch(\"detection_boxes\")\r\n            .run();\r\n    final long end = System.currentTimeMillis();\r\n    for (Tensor<?> t : outputs) {\r\n      t.close();\r\n    }\r\n    System.out.printf(\"%3d - %d ms\\n\", i, (end - start));\r\n  }\r\n}  \r\n```\r\n\r\nWhich is only measuring the time it takes to execute the graph, not any time spent in creating the input or other things. With that I see about 1/2 the time taken with GPU than CPU (50ms vs. 100ms). I'm using an Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz CPU and an NVIDIA TitanX GPU.\r\n\r\nHope that helps."}