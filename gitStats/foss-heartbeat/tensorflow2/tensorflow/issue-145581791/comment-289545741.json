{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289545741", "html_url": "https://github.com/tensorflow/tensorflow/issues/1763#issuecomment-289545741", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1763", "id": 289545741, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTU0NTc0MQ==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-27T18:40:28Z", "updated_at": "2017-03-27T18:40:28Z", "author_association": "NONE", "body_html": "<p>That sounds much better than the present situation, especially if the new parameter is prominently mentioned in the documentation.</p>\n<p>Arguably, changing the dtype during rescaling (as well as in the composite op <code>tf.image.crop_and_resize()</code>) is the most aggravating problem with the status quo, because short of casting back to uint8_t again there's no way to avoid float for the parts of the pipeline that, strictly speaking, do not need float. For performance reasons, resize is typically done right after decode.</p>\n<p>At a high level, I don't want to use any float32 until I <em>have</em> to be in float32, i.e. in our case until after we do resize, crop, and flip (and, when implemented, rotate and shear as well -- these need not convert to float either). It certainly would be great if I could maintain compact uint8_t representation for as long as I can.</p>\n<p>IMO this problem is more important than it might appear. I can tell you that for us inability to do data augmentation quickly enough nearly killed TF as a viable option. Our more demanding training pipelines currently use PyTorch for data augmentation, a solution which, as an engineer, makes me cringe, but hey, it runs at nearly twice the speed doing the same thing.</p>\n<p>There are a number of open issues in the issue tracker to that end, and being more frugal with cycles could help speed things up.</p>", "body_text": "That sounds much better than the present situation, especially if the new parameter is prominently mentioned in the documentation.\nArguably, changing the dtype during rescaling (as well as in the composite op tf.image.crop_and_resize()) is the most aggravating problem with the status quo, because short of casting back to uint8_t again there's no way to avoid float for the parts of the pipeline that, strictly speaking, do not need float. For performance reasons, resize is typically done right after decode.\nAt a high level, I don't want to use any float32 until I have to be in float32, i.e. in our case until after we do resize, crop, and flip (and, when implemented, rotate and shear as well -- these need not convert to float either). It certainly would be great if I could maintain compact uint8_t representation for as long as I can.\nIMO this problem is more important than it might appear. I can tell you that for us inability to do data augmentation quickly enough nearly killed TF as a viable option. Our more demanding training pipelines currently use PyTorch for data augmentation, a solution which, as an engineer, makes me cringe, but hey, it runs at nearly twice the speed doing the same thing.\nThere are a number of open issues in the issue tracker to that end, and being more frugal with cycles could help speed things up.", "body": "That sounds much better than the present situation, especially if the new parameter is prominently mentioned in the documentation. \r\n\r\nArguably, changing the dtype during rescaling (as well as in the composite op `tf.image.crop_and_resize()`) is the most aggravating problem with the status quo, because short of casting back to uint8_t again there's no way to avoid float for the parts of the pipeline that, strictly speaking, do not need float. For performance reasons, resize is typically done right after decode. \r\n\r\nAt a high level, I don't want to use any float32 until I _have_ to be in float32, i.e. in our case until after we do resize, crop, and flip (and, when implemented, rotate and shear as well -- these need not convert to float either). It certainly would be great if I could maintain compact uint8_t representation for as long as I can.\r\n\r\nIMO this problem is more important than it might appear. I can tell you that for us inability to do data augmentation quickly enough nearly killed TF as a viable option. Our more demanding training pipelines currently use PyTorch for data augmentation, a solution which, as an engineer, makes me cringe, but hey, it runs at nearly twice the speed doing the same thing.\r\n\r\nThere are a number of open issues in the issue tracker to that end, and being more frugal with cycles could help speed things up."}