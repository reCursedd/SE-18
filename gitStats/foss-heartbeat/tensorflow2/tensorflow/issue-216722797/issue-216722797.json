{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8687", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8687/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8687/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8687/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8687", "id": 216722797, "node_id": "MDU6SXNzdWUyMTY3MjI3OTc=", "number": 8687, "title": "[HDFS]OutOfRangeError cause java runtime error", "user": {"login": "DjangoPeng", "id": 16943353, "node_id": "MDQ6VXNlcjE2OTQzMzUz", "avatar_url": "https://avatars3.githubusercontent.com/u/16943353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DjangoPeng", "html_url": "https://github.com/DjangoPeng", "followers_url": "https://api.github.com/users/DjangoPeng/followers", "following_url": "https://api.github.com/users/DjangoPeng/following{/other_user}", "gists_url": "https://api.github.com/users/DjangoPeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/DjangoPeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DjangoPeng/subscriptions", "organizations_url": "https://api.github.com/users/DjangoPeng/orgs", "repos_url": "https://api.github.com/users/DjangoPeng/repos", "events_url": "https://api.github.com/users/DjangoPeng/events{/privacy}", "received_events_url": "https://api.github.com/users/DjangoPeng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jhseu", "id": 170179, "node_id": "MDQ6VXNlcjE3MDE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/170179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhseu", "html_url": "https://github.com/jhseu", "followers_url": "https://api.github.com/users/jhseu/followers", "following_url": "https://api.github.com/users/jhseu/following{/other_user}", "gists_url": "https://api.github.com/users/jhseu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhseu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhseu/subscriptions", "organizations_url": "https://api.github.com/users/jhseu/orgs", "repos_url": "https://api.github.com/users/jhseu/repos", "events_url": "https://api.github.com/users/jhseu/events{/privacy}", "received_events_url": "https://api.github.com/users/jhseu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jhseu", "id": 170179, "node_id": "MDQ6VXNlcjE3MDE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/170179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhseu", "html_url": "https://github.com/jhseu", "followers_url": "https://api.github.com/users/jhseu/followers", "following_url": "https://api.github.com/users/jhseu/following{/other_user}", "gists_url": "https://api.github.com/users/jhseu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhseu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhseu/subscriptions", "organizations_url": "https://api.github.com/users/jhseu/orgs", "repos_url": "https://api.github.com/users/jhseu/repos", "events_url": "https://api.github.com/users/jhseu/events{/privacy}", "received_events_url": "https://api.github.com/users/jhseu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2017-03-24T09:52:51Z", "updated_at": "2017-06-28T00:21:42Z", "closed_at": "2017-06-28T00:21:42Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Environment info</h3>\n<p>TensorFlow: v1.0.0 and v1.0.1<br>\nJDK: 1.8.0_111<br>\nHadoop: 2.6.0<br>\nGCC: 4.8.2</p>\n<h3>OutOfRangeError</h3>\n<p>I think the <code>tf.errors.OutOfRangeError</code> is a bug, at least, if you want to read data(tfrecords, csv, etc) from hdfs with readers and queues.</p>\n<h3>How to reproduce</h3>\n<p>I followed the guide of <a href=\"https://www.tensorflow.org/programmers_guide/reading_data\" rel=\"nofollow\">reading data</a>.<br>\nI used the recommended mnist example <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data\">here</a>.</p>\n<p>Firstly, convert MNIST dataset by this script: <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py\">convert_to_records.py</a></p>\n<p>Secondly, put the tfrecords of MNIST dataset into your hdfs. For example, assume the url is <code>hdfs://host:port/tfrecords/mnist-data/</code>.</p>\n<p>Then, train the MNIST network by this script: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py\">fully_connected_reader.py</a>. And feed the <code>train_dir</code> argument with <code>hdfs://host:port/tfrecords/mnist-data/</code>.</p>\n<p>Finally, you can get the train log and error log as below:</p>\n<h3>Train log</h3>\n<div class=\"highlight highlight-source-shell\"><pre>Step 0: loss = 2.29 (1.795 sec)\nStep 100: loss = 1.99 (0.045 sec)\nStep 200: loss = 1.62 (0.045 sec)\nStep 300: loss = 1.39 (0.043 sec)\nStep 400: loss = 1.01 (0.048 sec)\nStep 500: loss = 0.77 (0.043 sec)\nStep 600: loss = 0.65 (0.045 sec)\nStep 700: loss = 0.60 (0.043 sec)\nStep 800: loss = 0.65 (0.043 sec)\nStep 900: loss = 0.46 (0.043 sec)</pre></div>\n<h3>Error log</h3>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> A fatal error has been detected by the Java Runtime Environment:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>  SIGSEGV (0xb) at pc=0x00007f16a2a39660, pid=43645, tid=0x00007f16a2eff740</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Problematic frame:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> C  [libpython2.7.so.1.0+0x134660]  visit_decref+0x0</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> An error report file with more information is saved as:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> /home/mind/projects/tf_hdfs/hs_err_pid43645.log</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> If you would like to submit a bug report, please visit:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>   http://bugreport.java.com/bugreport/crash.jsp</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The crash happened outside the Java Virtual Machine in native code.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> See problematic frame for where to report the bug.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\nAborted (core dumped)</pre></div>\n<h3>Ask for a better way</h3>\n<p>I know it's hard to avoid the <code>OutOfRangeError</code> because the multithreads of readers. But can we have a better way to read? Especially when you want to read data from hdfs, it's inconvenient and difficult to catch the exception.</p>", "body_text": "Environment info\nTensorFlow: v1.0.0 and v1.0.1\nJDK: 1.8.0_111\nHadoop: 2.6.0\nGCC: 4.8.2\nOutOfRangeError\nI think the tf.errors.OutOfRangeError is a bug, at least, if you want to read data(tfrecords, csv, etc) from hdfs with readers and queues.\nHow to reproduce\nI followed the guide of reading data.\nI used the recommended mnist example here.\nFirstly, convert MNIST dataset by this script: convert_to_records.py\nSecondly, put the tfrecords of MNIST dataset into your hdfs. For example, assume the url is hdfs://host:port/tfrecords/mnist-data/.\nThen, train the MNIST network by this script: fully_connected_reader.py. And feed the train_dir argument with hdfs://host:port/tfrecords/mnist-data/.\nFinally, you can get the train log and error log as below:\nTrain log\nStep 0: loss = 2.29 (1.795 sec)\nStep 100: loss = 1.99 (0.045 sec)\nStep 200: loss = 1.62 (0.045 sec)\nStep 300: loss = 1.39 (0.043 sec)\nStep 400: loss = 1.01 (0.048 sec)\nStep 500: loss = 0.77 (0.043 sec)\nStep 600: loss = 0.65 (0.045 sec)\nStep 700: loss = 0.60 (0.043 sec)\nStep 800: loss = 0.65 (0.043 sec)\nStep 900: loss = 0.46 (0.043 sec)\nError log\n#\n# A fatal error has been detected by the Java Runtime Environment:\n#\n#  SIGSEGV (0xb) at pc=0x00007f16a2a39660, pid=43645, tid=0x00007f16a2eff740\n#\n# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)\n# Problematic frame:\n# C  [libpython2.7.so.1.0+0x134660]  visit_decref+0x0\n#\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\n#\n# An error report file with more information is saved as:\n# /home/mind/projects/tf_hdfs/hs_err_pid43645.log\n#\n# If you would like to submit a bug report, please visit:\n#   http://bugreport.java.com/bugreport/crash.jsp\n# The crash happened outside the Java Virtual Machine in native code.\n# See problematic frame for where to report the bug.\n#\nAborted (core dumped)\nAsk for a better way\nI know it's hard to avoid the OutOfRangeError because the multithreads of readers. But can we have a better way to read? Especially when you want to read data from hdfs, it's inconvenient and difficult to catch the exception.", "body": "### Environment info\r\nTensorFlow: v1.0.0 and v1.0.1\r\nJDK: 1.8.0_111\r\nHadoop: 2.6.0\r\nGCC: 4.8.2\r\n\r\n### OutOfRangeError\r\nI think the `tf.errors.OutOfRangeError` is a bug, at least, if you want to read data(tfrecords, csv, etc) from hdfs with readers and queues. \r\n\r\n### How to reproduce \r\nI followed the guide of [reading data](https://www.tensorflow.org/programmers_guide/reading_data).\r\nI used the recommended mnist example [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data).  \r\n\r\nFirstly, convert MNIST dataset by this script: [convert_to_records.py](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py)\r\n\r\nSecondly, put the tfrecords of MNIST dataset into your hdfs. For example, assume the url is `hdfs://host:port/tfrecords/mnist-data/`.\r\n\r\nThen, train the MNIST network by this script: [fully_connected_reader.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py). And feed the `train_dir` argument with `hdfs://host:port/tfrecords/mnist-data/`.\r\n\r\nFinally, you can get the train log and error log as below:\r\n### Train log\r\n```shell\r\nStep 0: loss = 2.29 (1.795 sec)\r\nStep 100: loss = 1.99 (0.045 sec)\r\nStep 200: loss = 1.62 (0.045 sec)\r\nStep 300: loss = 1.39 (0.043 sec)\r\nStep 400: loss = 1.01 (0.048 sec)\r\nStep 500: loss = 0.77 (0.043 sec)\r\nStep 600: loss = 0.65 (0.045 sec)\r\nStep 700: loss = 0.60 (0.043 sec)\r\nStep 800: loss = 0.65 (0.043 sec)\r\nStep 900: loss = 0.46 (0.043 sec)\r\n```\r\n\r\n### Error log\r\n```shell\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x00007f16a2a39660, pid=43645, tid=0x00007f16a2eff740\r\n#\r\n# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)\r\n# Problematic frame:\r\n# C  [libpython2.7.so.1.0+0x134660]  visit_decref+0x0\r\n#\r\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\r\n#\r\n# An error report file with more information is saved as:\r\n# /home/mind/projects/tf_hdfs/hs_err_pid43645.log\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   http://bugreport.java.com/bugreport/crash.jsp\r\n# The crash happened outside the Java Virtual Machine in native code.\r\n# See problematic frame for where to report the bug.\r\n#\r\nAborted (core dumped)\r\n```\r\n\r\n### Ask for a better way\r\nI know it's hard to avoid the `OutOfRangeError` because the multithreads of readers. But can we have a better way to read? Especially when you want to read data from hdfs, it's inconvenient and difficult to catch the exception. "}