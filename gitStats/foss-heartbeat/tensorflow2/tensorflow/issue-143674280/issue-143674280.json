{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1652", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1652/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1652/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1652/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1652", "id": 143674280, "node_id": "MDU6SXNzdWUxNDM2NzQyODA=", "number": 1652, "title": "RNN Cell: different device placement for variables and ops", "user": {"login": "seominjoon", "id": 1738325, "node_id": "MDQ6VXNlcjE3MzgzMjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1738325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seominjoon", "html_url": "https://github.com/seominjoon", "followers_url": "https://api.github.com/users/seominjoon/followers", "following_url": "https://api.github.com/users/seominjoon/following{/other_user}", "gists_url": "https://api.github.com/users/seominjoon/gists{/gist_id}", "starred_url": "https://api.github.com/users/seominjoon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seominjoon/subscriptions", "organizations_url": "https://api.github.com/users/seominjoon/orgs", "repos_url": "https://api.github.com/users/seominjoon/repos", "events_url": "https://api.github.com/users/seominjoon/events{/privacy}", "received_events_url": "https://api.github.com/users/seominjoon/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-03-26T07:33:40Z", "updated_at": "2018-04-29T13:39:02Z", "closed_at": "2016-03-28T18:32:05Z", "author_association": "NONE", "body_html": "<p>I want to use RNN cell on multiple GPUs. For maximum performance, I want to place RNN cell variables on CPU and operations (matmul, tanh, etc) on GPU, just like in cifar10 multi gpu example. However, the current implementation of RNN cell seems to only allow same device placement for both variables and ops. Could you enable different device placement?</p>", "body_text": "I want to use RNN cell on multiple GPUs. For maximum performance, I want to place RNN cell variables on CPU and operations (matmul, tanh, etc) on GPU, just like in cifar10 multi gpu example. However, the current implementation of RNN cell seems to only allow same device placement for both variables and ops. Could you enable different device placement?", "body": "I want to use RNN cell on multiple GPUs. For maximum performance, I want to place RNN cell variables on CPU and operations (matmul, tanh, etc) on GPU, just like in cifar10 multi gpu example. However, the current implementation of RNN cell seems to only allow same device placement for both variables and ops. Could you enable different device placement?\n"}