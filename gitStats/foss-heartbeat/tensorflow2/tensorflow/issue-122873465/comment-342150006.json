{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/342150006", "html_url": "https://github.com/tensorflow/tensorflow/issues/543#issuecomment-342150006", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/543", "id": 342150006, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjE1MDAwNg==", "user": {"login": "TillLindemann", "id": 22681439, "node_id": "MDQ6VXNlcjIyNjgxNDM5", "avatar_url": "https://avatars3.githubusercontent.com/u/22681439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TillLindemann", "html_url": "https://github.com/TillLindemann", "followers_url": "https://api.github.com/users/TillLindemann/followers", "following_url": "https://api.github.com/users/TillLindemann/following{/other_user}", "gists_url": "https://api.github.com/users/TillLindemann/gists{/gist_id}", "starred_url": "https://api.github.com/users/TillLindemann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TillLindemann/subscriptions", "organizations_url": "https://api.github.com/users/TillLindemann/orgs", "repos_url": "https://api.github.com/users/TillLindemann/repos", "events_url": "https://api.github.com/users/TillLindemann/events{/privacy}", "received_events_url": "https://api.github.com/users/TillLindemann/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-06T13:35:15Z", "updated_at": "2017-11-06T13:36:09Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5636133\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lglhuada\">@lglhuada</a>  Hi, recently I have the same issue, I  tried  three different types of  neural network :<br>\n1.the simple feedforward network ,2.the pixelcnn 3.GAN.<br>\nAnd weird things happened,<br>\n1 uses very low usage of the gpu, and that project has very large training data.<br>\n2 uses almost 100% of gpu .<br>\n3 the usage of gpu is preodic , sometimes up to 50% sometimes down to 0%.<br>\nAfter exiperiment, I finally found the reason, it's the code is to blame, actually the code was written by someone who has no gpu on his laptop,so it's totally unfriendly with gpu,most operations do run on the gpu,but there still some operations are run on the cpu,and when the code running on the cpu,the gpu has to wait,that causes the gap. there are some solutions for it , change the type of variables and constants to tf.float32. but that would not change a lot , so if the code is not friendly with gpu ,you should use tensorflow-cpu version,that might be faster than gpu-version. So  In conclusion, if your gpu does running on some codes and not running on some codes,it probably the code's problem.</p>", "body_text": "@lglhuada  Hi, recently I have the same issue, I  tried  three different types of  neural network :\n1.the simple feedforward network ,2.the pixelcnn 3.GAN.\nAnd weird things happened,\n1 uses very low usage of the gpu, and that project has very large training data.\n2 uses almost 100% of gpu .\n3 the usage of gpu is preodic , sometimes up to 50% sometimes down to 0%.\nAfter exiperiment, I finally found the reason, it's the code is to blame, actually the code was written by someone who has no gpu on his laptop,so it's totally unfriendly with gpu,most operations do run on the gpu,but there still some operations are run on the cpu,and when the code running on the cpu,the gpu has to wait,that causes the gap. there are some solutions for it , change the type of variables and constants to tf.float32. but that would not change a lot , so if the code is not friendly with gpu ,you should use tensorflow-cpu version,that might be faster than gpu-version. So  In conclusion, if your gpu does running on some codes and not running on some codes,it probably the code's problem.", "body": "@lglhuada  Hi, recently I have the same issue, I  tried  three different types of  neural network : \r\n1.the simple feedforward network ,2.the pixelcnn 3.GAN. \r\nAnd weird things happened, \r\n1 uses very low usage of the gpu, and that project has very large training data.\r\n2 uses almost 100% of gpu .\r\n3 the usage of gpu is preodic , sometimes up to 50% sometimes down to 0%. \r\nAfter exiperiment, I finally found the reason, it's the code is to blame, actually the code was written by someone who has no gpu on his laptop,so it's totally unfriendly with gpu,most operations do run on the gpu,but there still some operations are run on the cpu,and when the code running on the cpu,the gpu has to wait,that causes the gap. there are some solutions for it , change the type of variables and constants to tf.float32. but that would not change a lot , so if the code is not friendly with gpu ,you should use tensorflow-cpu version,that might be faster than gpu-version. So  In conclusion, if your gpu does running on some codes and not running on some codes,it probably the code's problem."}