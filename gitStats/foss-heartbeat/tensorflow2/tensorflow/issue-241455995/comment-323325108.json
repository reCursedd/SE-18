{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323325108", "html_url": "https://github.com/tensorflow/tensorflow/pull/11377#issuecomment-323325108", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11377", "id": 323325108, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzMyNTEwOA==", "user": {"login": "dguerra", "id": 5977844, "node_id": "MDQ6VXNlcjU5Nzc4NDQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/5977844?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dguerra", "html_url": "https://github.com/dguerra", "followers_url": "https://api.github.com/users/dguerra/followers", "following_url": "https://api.github.com/users/dguerra/following{/other_user}", "gists_url": "https://api.github.com/users/dguerra/gists{/gist_id}", "starred_url": "https://api.github.com/users/dguerra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dguerra/subscriptions", "organizations_url": "https://api.github.com/users/dguerra/orgs", "repos_url": "https://api.github.com/users/dguerra/repos", "events_url": "https://api.github.com/users/dguerra/events{/privacy}", "received_events_url": "https://api.github.com/users/dguerra/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-18T11:02:44Z", "updated_at": "2017-08-18T11:02:44Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3902382\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/theflofly\">@theflofly</a> yes sure, I've used variables as inputs to the ops.</p>\n<p>Shall we start a new thread or platform for conversation about general usage and development of Tensorflow C++ API ?</p>\n<pre><code>#include \"tensorflow/cc/client/client_session.h\"\n#include \"tensorflow/cc/ops/standard_ops.h\"\n#include \"tensorflow/cc/framework/gradients.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n\n\nint main()\n{\n  using namespace tensorflow;\n  using namespace tensorflow::ops;\n  Scope root = Scope::NewRootScope();\n  \n  auto W = Variable(root.WithOpName(\"W\"), {3,1}, DT_DOUBLE);\n\n  auto x = Placeholder(root.WithOpName(\"x\"), DT_DOUBLE);\n  auto y = Placeholder(root.WithOpName(\"y\"), DT_DOUBLE);\n\n  auto d = Subtract(root, y, MatMul(root, x, W));\n  \n  // Compute gradients\n  auto dd = MatMul(root, d, d, MatMul::TransposeA(true));\n  // auto half = Const(root, {0.5});\n  auto loss = MatMul(root, dd, {{0.5}}); \n\n  double learning_rate = 0.1;\n \n\n  std::vector&lt;Output&gt; grad_outputs;\n  TF_CHECK_OK(AddSymbolicGradients(root, {loss}, {W}, &amp;grad_outputs));\n\n  //Explicit gradient formula:\n  //auto grad_W = Subtract(root, MatMul(root, MatMul(root, x, x, MatMul::TransposeA(true)), W), MatMul(root, x, y,  MatMul::TransposeA(true)));\n  \n  // apply either: the ouput from AddSymbolicGradients or explicit gradient formula\n  auto apply_grad_W = ApplyGradientDescent(root, W, learning_rate,  grad_outputs[0]);\n\n  //Initialize variables\n  auto init_W = Assign(root, W, {{1.0},{1.0},{1.0}});\n\n  std::vector&lt;Tensor&gt; outputs;\n  ClientSession session(root);\n\n  //Run variable initializers\n  session.Run({init_W}, &amp;outputs);\n\n  for(unsigned int i=0;i&lt;200;++i)\n  {\n    //y = 3.0 * x1 + 4.0 * x2 + 5.0\n    TF_CHECK_OK(session.Run( { {x,{{1.0,-1.0,3.0}, {1.0,2.0,1.0}, {1.0,-2.0,-2.0}, {1.0,0.0,2.0}}}, {y,{{14.0}, {15.0}, {-9.0}, {13.0}}} } , {loss, apply_grad_W}, &amp;outputs));\n    std::cout &lt;&lt; std::string(\"loss: \") &lt;&lt; outputs[0].scalar&lt;double&gt;() &lt;&lt; std::endl &lt;&lt; std::string(\"weights: \")&lt;&lt; outputs[1].matrix&lt;double&gt;() &lt;&lt; std::endl;\n  }\n  return 0;\n}\n\n</code></pre>", "body_text": "@theflofly yes sure, I've used variables as inputs to the ops.\nShall we start a new thread or platform for conversation about general usage and development of Tensorflow C++ API ?\n#include \"tensorflow/cc/client/client_session.h\"\n#include \"tensorflow/cc/ops/standard_ops.h\"\n#include \"tensorflow/cc/framework/gradients.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n\n\nint main()\n{\n  using namespace tensorflow;\n  using namespace tensorflow::ops;\n  Scope root = Scope::NewRootScope();\n  \n  auto W = Variable(root.WithOpName(\"W\"), {3,1}, DT_DOUBLE);\n\n  auto x = Placeholder(root.WithOpName(\"x\"), DT_DOUBLE);\n  auto y = Placeholder(root.WithOpName(\"y\"), DT_DOUBLE);\n\n  auto d = Subtract(root, y, MatMul(root, x, W));\n  \n  // Compute gradients\n  auto dd = MatMul(root, d, d, MatMul::TransposeA(true));\n  // auto half = Const(root, {0.5});\n  auto loss = MatMul(root, dd, {{0.5}}); \n\n  double learning_rate = 0.1;\n \n\n  std::vector<Output> grad_outputs;\n  TF_CHECK_OK(AddSymbolicGradients(root, {loss}, {W}, &grad_outputs));\n\n  //Explicit gradient formula:\n  //auto grad_W = Subtract(root, MatMul(root, MatMul(root, x, x, MatMul::TransposeA(true)), W), MatMul(root, x, y,  MatMul::TransposeA(true)));\n  \n  // apply either: the ouput from AddSymbolicGradients or explicit gradient formula\n  auto apply_grad_W = ApplyGradientDescent(root, W, learning_rate,  grad_outputs[0]);\n\n  //Initialize variables\n  auto init_W = Assign(root, W, {{1.0},{1.0},{1.0}});\n\n  std::vector<Tensor> outputs;\n  ClientSession session(root);\n\n  //Run variable initializers\n  session.Run({init_W}, &outputs);\n\n  for(unsigned int i=0;i<200;++i)\n  {\n    //y = 3.0 * x1 + 4.0 * x2 + 5.0\n    TF_CHECK_OK(session.Run( { {x,{{1.0,-1.0,3.0}, {1.0,2.0,1.0}, {1.0,-2.0,-2.0}, {1.0,0.0,2.0}}}, {y,{{14.0}, {15.0}, {-9.0}, {13.0}}} } , {loss, apply_grad_W}, &outputs));\n    std::cout << std::string(\"loss: \") << outputs[0].scalar<double>() << std::endl << std::string(\"weights: \")<< outputs[1].matrix<double>() << std::endl;\n  }\n  return 0;\n}", "body": "@theflofly yes sure, I've used variables as inputs to the ops.\r\n\r\nShall we start a new thread or platform for conversation about general usage and development of Tensorflow C++ API ?\r\n\r\n\r\n```\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/cc/framework/gradients.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n\r\n\r\nint main()\r\n{\r\n  using namespace tensorflow;\r\n  using namespace tensorflow::ops;\r\n  Scope root = Scope::NewRootScope();\r\n  \r\n  auto W = Variable(root.WithOpName(\"W\"), {3,1}, DT_DOUBLE);\r\n\r\n  auto x = Placeholder(root.WithOpName(\"x\"), DT_DOUBLE);\r\n  auto y = Placeholder(root.WithOpName(\"y\"), DT_DOUBLE);\r\n\r\n  auto d = Subtract(root, y, MatMul(root, x, W));\r\n  \r\n  // Compute gradients\r\n  auto dd = MatMul(root, d, d, MatMul::TransposeA(true));\r\n  // auto half = Const(root, {0.5});\r\n  auto loss = MatMul(root, dd, {{0.5}}); \r\n\r\n  double learning_rate = 0.1;\r\n \r\n\r\n  std::vector<Output> grad_outputs;\r\n  TF_CHECK_OK(AddSymbolicGradients(root, {loss}, {W}, &grad_outputs));\r\n\r\n  //Explicit gradient formula:\r\n  //auto grad_W = Subtract(root, MatMul(root, MatMul(root, x, x, MatMul::TransposeA(true)), W), MatMul(root, x, y,  MatMul::TransposeA(true)));\r\n  \r\n  // apply either: the ouput from AddSymbolicGradients or explicit gradient formula\r\n  auto apply_grad_W = ApplyGradientDescent(root, W, learning_rate,  grad_outputs[0]);\r\n\r\n  //Initialize variables\r\n  auto init_W = Assign(root, W, {{1.0},{1.0},{1.0}});\r\n\r\n  std::vector<Tensor> outputs;\r\n  ClientSession session(root);\r\n\r\n  //Run variable initializers\r\n  session.Run({init_W}, &outputs);\r\n\r\n  for(unsigned int i=0;i<200;++i)\r\n  {\r\n    //y = 3.0 * x1 + 4.0 * x2 + 5.0\r\n    TF_CHECK_OK(session.Run( { {x,{{1.0,-1.0,3.0}, {1.0,2.0,1.0}, {1.0,-2.0,-2.0}, {1.0,0.0,2.0}}}, {y,{{14.0}, {15.0}, {-9.0}, {13.0}}} } , {loss, apply_grad_W}, &outputs));\r\n    std::cout << std::string(\"loss: \") << outputs[0].scalar<double>() << std::endl << std::string(\"weights: \")<< outputs[1].matrix<double>() << std::endl;\r\n  }\r\n  return 0;\r\n}\r\n\r\n```\r\n"}