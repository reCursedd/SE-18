{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323301651", "html_url": "https://github.com/tensorflow/tensorflow/pull/11377#issuecomment-323301651", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11377", "id": 323301651, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzMwMTY1MQ==", "user": {"login": "theflofly", "id": 3902382, "node_id": "MDQ6VXNlcjM5MDIzODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3902382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/theflofly", "html_url": "https://github.com/theflofly", "followers_url": "https://api.github.com/users/theflofly/followers", "following_url": "https://api.github.com/users/theflofly/following{/other_user}", "gists_url": "https://api.github.com/users/theflofly/gists{/gist_id}", "starred_url": "https://api.github.com/users/theflofly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/theflofly/subscriptions", "organizations_url": "https://api.github.com/users/theflofly/orgs", "repos_url": "https://api.github.com/users/theflofly/repos", "events_url": "https://api.github.com/users/theflofly/events{/privacy}", "received_events_url": "https://api.github.com/users/theflofly/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-18T09:08:18Z", "updated_at": "2017-08-18T11:52:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p><strong>[UNRELATED TO THE PR]</strong></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5977844\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dguerra\">@dguerra</a>: Really ?<br>\nDo you give a Assign node or a Var node as input to an op ?<br>\nIf you have:</p>\n<pre><code>auto w1 = Variable(scope, {3, 3}, DT_FLOAT);\nauto assign_w1 = Assign(scope, w1, Const(scope, ...));\n</code></pre>\n<p>then are you doing this: <code>Tanh(scope, w1);</code>(1) or <code>Tanh(scope, assign_w1);</code>(2) ? Because if you are using assign_ node as in (2) everywhere, your variables will never change, as at each step the const value is reassigned. If you are using var as in (1) the gradient graph will not be correct because of an error that I solved.<br>\nI am replacing the tests from Const to Var and I will do a PR.<br>\nIf you are able to train a network (add ApplyGradientDescent nodes to you graph) and it works please share your code because I may be wrong but I did not succeed to train without editing gradients.cc.<br>\nAlso we should stop polluting this PR I guess.</p>", "body_text": "[UNRELATED TO THE PR]\n@dguerra: Really ?\nDo you give a Assign node or a Var node as input to an op ?\nIf you have:\nauto w1 = Variable(scope, {3, 3}, DT_FLOAT);\nauto assign_w1 = Assign(scope, w1, Const(scope, ...));\n\nthen are you doing this: Tanh(scope, w1);(1) or Tanh(scope, assign_w1);(2) ? Because if you are using assign_ node as in (2) everywhere, your variables will never change, as at each step the const value is reassigned. If you are using var as in (1) the gradient graph will not be correct because of an error that I solved.\nI am replacing the tests from Const to Var and I will do a PR.\nIf you are able to train a network (add ApplyGradientDescent nodes to you graph) and it works please share your code because I may be wrong but I did not succeed to train without editing gradients.cc.\nAlso we should stop polluting this PR I guess.", "body": "**[UNRELATED TO THE PR]**\r\n\r\n@dguerra: Really ?\r\nDo you give a Assign node or a Var node as input to an op ?\r\nIf you have:\r\n```\r\nauto w1 = Variable(scope, {3, 3}, DT_FLOAT);\r\nauto assign_w1 = Assign(scope, w1, Const(scope, ...));\r\n```\r\nthen are you doing this: `Tanh(scope, w1);`(1) or `Tanh(scope, assign_w1);`(2) ? Because if you are using assign_ node as in (2) everywhere, your variables will never change, as at each step the const value is reassigned. If you are using var as in (1) the gradient graph will not be correct because of an error that I solved.\r\nI am replacing the tests from Const to Var and I will do a PR.\r\nIf you are able to train a network (add ApplyGradientDescent nodes to you graph) and it works please share your code because I may be wrong but I did not succeed to train without editing gradients.cc.\r\nAlso we should stop polluting this PR I guess."}