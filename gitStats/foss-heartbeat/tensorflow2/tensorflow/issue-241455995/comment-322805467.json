{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/322805467", "html_url": "https://github.com/tensorflow/tensorflow/pull/11377#issuecomment-322805467", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11377", "id": 322805467, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjgwNTQ2Nw==", "user": {"login": "theflofly", "id": 3902382, "node_id": "MDQ6VXNlcjM5MDIzODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3902382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/theflofly", "html_url": "https://github.com/theflofly", "followers_url": "https://api.github.com/users/theflofly/followers", "following_url": "https://api.github.com/users/theflofly/following{/other_user}", "gists_url": "https://api.github.com/users/theflofly/gists{/gist_id}", "starred_url": "https://api.github.com/users/theflofly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/theflofly/subscriptions", "organizations_url": "https://api.github.com/users/theflofly/orgs", "repos_url": "https://api.github.com/users/theflofly/repos", "events_url": "https://api.github.com/users/theflofly/events{/privacy}", "received_events_url": "https://api.github.com/users/theflofly/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-16T15:16:24Z", "updated_at": "2017-08-18T11:52:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p><strong>[UNRELATED TO THE PR]</strong></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5977844\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dguerra\">@dguerra</a> Someone beat me at it. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16907534\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rmlarsen\">@rmlarsen</a> merged 19 hours ago. You now have:</p>\n<ul>\n<li>AddGrad</li>\n<li>SubGrad</li>\n<li>MulGrad</li>\n<li>DivGrad</li>\n<li>RealDivGrad</li>\n<li>SquaredDifferenceGrad</li>\n</ul>\n<p>in <code>tensorflow/cc/gradients/math_grad.cc</code>. You code should now run smoothly.</p>\n<p>The thing that bother me is that for instance, in core we have:</p>\n<pre><code>Status AddGrad(const AttrSlice&amp; attrs, FunctionDef* g) {\n  // clang-format off\n  return GradForBinaryCwise(g, {\n      {{\"gx\"}, \"Identity\", {\"dz\"}},\n      {{\"gy\"}, \"Identity\", {\"dz\"}},\n  });\n  // clang-format on\n}\nREGISTER_OP_GRADIENT(\"Add\", AddGrad);\n</code></pre>\n<p>and in the cc/gradient we have:</p>\n<pre><code>Status AddGrad(const Scope&amp; scope, const Operation&amp; op,\n                const std::vector&lt;Output&gt;&amp; grad_inputs,\n                std::vector&lt;Output&gt;* grad_outputs) {\n   // y = x_1 + x_2\n   // dy/dx_1 = dy/dx_2 = 1\n   auto gx_1 = Identity(scope, grad_inputs[0]);\n   auto gx_2 = Identity(scope, grad_inputs[0]);\n   return BinaryGradCommon(scope, op, grad_outputs, gx_1, gx_2);\n }\n REGISTER_GRADIENT_OP(\"Add\", AddGrad);\n</code></pre>\n<p>So I am wondering if there is a way to use the kernel code instead of redefining it in the gradient API.</p>", "body_text": "[UNRELATED TO THE PR]\n@dguerra Someone beat me at it. @rmlarsen merged 19 hours ago. You now have:\n\nAddGrad\nSubGrad\nMulGrad\nDivGrad\nRealDivGrad\nSquaredDifferenceGrad\n\nin tensorflow/cc/gradients/math_grad.cc. You code should now run smoothly.\nThe thing that bother me is that for instance, in core we have:\nStatus AddGrad(const AttrSlice& attrs, FunctionDef* g) {\n  // clang-format off\n  return GradForBinaryCwise(g, {\n      {{\"gx\"}, \"Identity\", {\"dz\"}},\n      {{\"gy\"}, \"Identity\", {\"dz\"}},\n  });\n  // clang-format on\n}\nREGISTER_OP_GRADIENT(\"Add\", AddGrad);\n\nand in the cc/gradient we have:\nStatus AddGrad(const Scope& scope, const Operation& op,\n                const std::vector<Output>& grad_inputs,\n                std::vector<Output>* grad_outputs) {\n   // y = x_1 + x_2\n   // dy/dx_1 = dy/dx_2 = 1\n   auto gx_1 = Identity(scope, grad_inputs[0]);\n   auto gx_2 = Identity(scope, grad_inputs[0]);\n   return BinaryGradCommon(scope, op, grad_outputs, gx_1, gx_2);\n }\n REGISTER_GRADIENT_OP(\"Add\", AddGrad);\n\nSo I am wondering if there is a way to use the kernel code instead of redefining it in the gradient API.", "body": "**[UNRELATED TO THE PR]**\r\n\r\n@dguerra Someone beat me at it. @rmlarsen merged 19 hours ago. You now have:\r\n\r\n* AddGrad\r\n* SubGrad\r\n* MulGrad\r\n* DivGrad\r\n* RealDivGrad\r\n* SquaredDifferenceGrad\r\n\r\nin `tensorflow/cc/gradients/math_grad.cc`. You code should now run smoothly.\r\n\r\nThe thing that bother me is that for instance, in core we have:\r\n```\r\nStatus AddGrad(const AttrSlice& attrs, FunctionDef* g) {\r\n  // clang-format off\r\n  return GradForBinaryCwise(g, {\r\n      {{\"gx\"}, \"Identity\", {\"dz\"}},\r\n      {{\"gy\"}, \"Identity\", {\"dz\"}},\r\n  });\r\n  // clang-format on\r\n}\r\nREGISTER_OP_GRADIENT(\"Add\", AddGrad);\r\n```\r\nand in the cc/gradient we have:\r\n```\r\nStatus AddGrad(const Scope& scope, const Operation& op,\r\n                const std::vector<Output>& grad_inputs,\r\n                std::vector<Output>* grad_outputs) {\r\n   // y = x_1 + x_2\r\n   // dy/dx_1 = dy/dx_2 = 1\r\n   auto gx_1 = Identity(scope, grad_inputs[0]);\r\n   auto gx_2 = Identity(scope, grad_inputs[0]);\r\n   return BinaryGradCommon(scope, op, grad_outputs, gx_1, gx_2);\r\n }\r\n REGISTER_GRADIENT_OP(\"Add\", AddGrad);\r\n```\r\nSo I am wondering if there is a way to use the kernel code instead of redefining it in the gradient API."}