{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324546254", "html_url": "https://github.com/tensorflow/tensorflow/pull/11377#issuecomment-324546254", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11377", "id": 324546254, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDU0NjI1NA==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-24T06:32:15Z", "updated_at": "2017-08-24T06:32:15Z", "author_association": "MEMBER", "body_html": "<p>Catching up on this now that I'm back. Seems like there has been a bunch of discussion unrelated to this specific PR :). Perhaps, as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5977844\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dguerra\">@dguerra</a> suggested, this discussion can be moved off this PR into a separate issue?</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3902382\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/theflofly\">@theflofly</a> : Responses to your <a href=\"https://github.com/tensorflow/tensorflow/pull/11377#issuecomment-321402783\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/11377/hovercard\">comments above</a>:</p>\n<ul>\n<li>\n<p>The document would outline a plan for making optimizers available in other languages. Most likely this would consist of at least two parts: (1) Detailed design of the C++ API (such as handling of sparse gradients, <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/training/optimizer.py#L235\">slots</a>), perhaps even an introduction of a <code>Variable</code> class in the API and (2) Design of a C API for optimizers (which will likely wrap over the C++ API), sample outlines of the implementation in one or two language to provide confidence that the API is sufficient, discussion of if/how users in other languages can add new optimizers and how they can be made available across languages (or does doing so require a C++ implementation) etc. I think it might be best for someone on the TensorFlow team who has experience with other language bindings to start on such a draft and share it here for comments/suggestions/improvements/discussion. I will try to get this going on our end, but do not have a timeline yet.</p>\n</li>\n<li>\n<p>There is some confusion about the gradients in <code>core/</code> vs <code>cc/</code>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1450614\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/suharshs\">@suharshs</a> knows the details, but long story short, the gradients in <code>cc/</code> <em>are</em> accessible in other languages. They are the ones picked up by <code>TF_AddGradients</code>. So, that mechanism is already setup (the unfortunate directory structure confusion notwithstanding).</p>\n</li>\n<li>\n<p>I agree that ultimately Optimizers should be part of the API. However, a separate plugin/repository for now was a suggestion to allow for rapid progress that doesn't block on us (TensorFlow maintainers) from being to have the bandwidth to provide support for it.</p>\n</li>\n</ul>\n<p>A technical detail regarding variables: We should be using resource variables (<a href=\"https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/resource_variable_ops.py#L38\"><code>class ResourceVariable</code> in Python</a>) instead of reference variables, as they have more clearly defined semantics. We're aiming to switch <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/variable_scope.py#L254\">Python to that as well</a> in upcoming releases. For example, this would mean using <a href=\"https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/resource-apply-gradient-descent\" rel=\"nofollow\"><code>ResourceApplyGradientDescent</code></a> instead of the <a href=\"https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/apply-gradient-descent\" rel=\"nofollow\"><code>ApplyGradientDescent</code></a> operation. I apologize that this distinction isn't well documented (it has been mostly an implementation detail, but now that we're seeing contributions for other languages, it is an implementation detail that more people will be interested in :)</p>\n<p>Regarding this PR itself: Even if we were to ignore the C API and other languages and think only of a C++ Optimizer, there are some broader design considerations that need to be thought through - sparse gradients, slots, reference variables.</p>\n<p>A starting point might be a write-up that looks at the features required to implement the various optimizers that exist in Python today and their handling of dense and sparse variables.</p>\n<p>As you've noted, any PRs to help improve coverage of the C++ gradient registry are also greatly appreciated.</p>", "body_text": "Catching up on this now that I'm back. Seems like there has been a bunch of discussion unrelated to this specific PR :). Perhaps, as @dguerra suggested, this discussion can be moved off this PR into a separate issue?\n@theflofly : Responses to your comments above:\n\n\nThe document would outline a plan for making optimizers available in other languages. Most likely this would consist of at least two parts: (1) Detailed design of the C++ API (such as handling of sparse gradients, slots), perhaps even an introduction of a Variable class in the API and (2) Design of a C API for optimizers (which will likely wrap over the C++ API), sample outlines of the implementation in one or two language to provide confidence that the API is sufficient, discussion of if/how users in other languages can add new optimizers and how they can be made available across languages (or does doing so require a C++ implementation) etc. I think it might be best for someone on the TensorFlow team who has experience with other language bindings to start on such a draft and share it here for comments/suggestions/improvements/discussion. I will try to get this going on our end, but do not have a timeline yet.\n\n\nThere is some confusion about the gradients in core/ vs cc/, @suharshs knows the details, but long story short, the gradients in cc/ are accessible in other languages. They are the ones picked up by TF_AddGradients. So, that mechanism is already setup (the unfortunate directory structure confusion notwithstanding).\n\n\nI agree that ultimately Optimizers should be part of the API. However, a separate plugin/repository for now was a suggestion to allow for rapid progress that doesn't block on us (TensorFlow maintainers) from being to have the bandwidth to provide support for it.\n\n\nA technical detail regarding variables: We should be using resource variables (class ResourceVariable in Python) instead of reference variables, as they have more clearly defined semantics. We're aiming to switch Python to that as well in upcoming releases. For example, this would mean using ResourceApplyGradientDescent instead of the ApplyGradientDescent operation. I apologize that this distinction isn't well documented (it has been mostly an implementation detail, but now that we're seeing contributions for other languages, it is an implementation detail that more people will be interested in :)\nRegarding this PR itself: Even if we were to ignore the C API and other languages and think only of a C++ Optimizer, there are some broader design considerations that need to be thought through - sparse gradients, slots, reference variables.\nA starting point might be a write-up that looks at the features required to implement the various optimizers that exist in Python today and their handling of dense and sparse variables.\nAs you've noted, any PRs to help improve coverage of the C++ gradient registry are also greatly appreciated.", "body": "Catching up on this now that I'm back. Seems like there has been a bunch of discussion unrelated to this specific PR :). Perhaps, as @dguerra suggested, this discussion can be moved off this PR into a separate issue? \r\n\r\n@theflofly : Responses to your [comments above](https://github.com/tensorflow/tensorflow/pull/11377#issuecomment-321402783):\r\n\r\n- The document would outline a plan for making optimizers available in other languages. Most likely this would consist of at least two parts: (1) Detailed design of the C++ API (such as handling of sparse gradients, [slots](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/training/optimizer.py#L235)), perhaps even an introduction of a `Variable` class in the API and (2) Design of a C API for optimizers (which will likely wrap over the C++ API), sample outlines of the implementation in one or two language to provide confidence that the API is sufficient, discussion of if/how users in other languages can add new optimizers and how they can be made available across languages (or does doing so require a C++ implementation) etc. I think it might be best for someone on the TensorFlow team who has experience with other language bindings to start on such a draft and share it here for comments/suggestions/improvements/discussion. I will try to get this going on our end, but do not have a timeline yet.\r\n\r\n- There is some confusion about the gradients in `core/` vs `cc/`, @suharshs knows the details, but long story short, the gradients in `cc/` *are* accessible in other languages. They are the ones picked up by `TF_AddGradients`. So, that mechanism is already setup (the unfortunate directory structure confusion notwithstanding).\r\n\r\n- I agree that ultimately Optimizers should be part of the API. However, a separate plugin/repository for now was a suggestion to allow for rapid progress that doesn't block on us (TensorFlow maintainers) from being to have the bandwidth to provide support for it.\r\n\r\nA technical detail regarding variables: We should be using resource variables ([`class ResourceVariable` in Python](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/resource_variable_ops.py#L38)) instead of reference variables, as they have more clearly defined semantics. We're aiming to switch [Python to that as well](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/variable_scope.py#L254) in upcoming releases. For example, this would mean using [`ResourceApplyGradientDescent`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/resource-apply-gradient-descent) instead of the [`ApplyGradientDescent`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/apply-gradient-descent) operation. I apologize that this distinction isn't well documented (it has been mostly an implementation detail, but now that we're seeing contributions for other languages, it is an implementation detail that more people will be interested in :)\r\n\r\nRegarding this PR itself: Even if we were to ignore the C API and other languages and think only of a C++ Optimizer, there are some broader design considerations that need to be thought through - sparse gradients, slots, reference variables. \r\n\r\nA starting point might be a write-up that looks at the features required to implement the various optimizers that exist in Python today and their handling of dense and sparse variables. \r\n\r\nAs you've noted, any PRs to help improve coverage of the C++ gradient registry are also greatly appreciated."}