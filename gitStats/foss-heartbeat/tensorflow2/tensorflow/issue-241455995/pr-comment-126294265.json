{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/126294265", "pull_request_review_id": 48767739, "id": 126294265, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNjI5NDI2NQ==", "diff_hunk": "@@ -0,0 +1,154 @@\n+/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/cc/training/optimizer.h\"\n+#include \"tensorflow/cc/training/gradient_descent_optimizer.h\"\n+#include \"tensorflow/core/framework/tensor_testutil.h\"\n+#include \"tensorflow/core/lib/core/status_test_util.h\"\n+#include \"tensorflow/core/util/equal_graph_def.h\"\n+#include \"tensorflow/cc/client/client_session.h\"\n+\n+namespace tensorflow {\n+using namespace ops;  // NOLINT(build/namespaces)\n+\n+namespace {\n+\n+class OptimizerTest : public ::testing::Test {\n+ protected:\n+  OptimizerTest()\n+      : scope_expected_(Scope::NewRootScope()),\n+        scope_test_(Scope::NewRootScope()) {}\n+\n+  void CompareTestAndExpectedGraphs() {\n+    GraphDef gdef_test;\n+    TF_ASSERT_OK(scope_test_.ToGraphDef(&gdef_test));\n+    GraphDef gdef_exp;\n+    TF_ASSERT_OK(scope_expected_.ToGraphDef(&gdef_exp));\n+    TF_EXPECT_GRAPH_EQ(gdef_exp, gdef_test);\n+  }\n+\n+  Scope scope_expected_;\n+  Scope scope_test_;\n+};\n+\n+class OptimizerValueTest : public ::testing::Test {\n+ protected:\n+  OptimizerValueTest()\n+      : scope_(Scope::NewRootScope()) {}\n+\n+  Scope scope_;\n+};\n+\n+// test that the graph is correctly built\n+TEST_F(OptimizerTest, OneMatMul) {\n+\n+  for (const bool expected : {false, true}) {\n+\n+    const Scope& scope = expected ? scope_expected_ : scope_test_;\n+\n+    // the forward node should be the same for the test and expected scope\n+    // TODO(theflofly): merge Const and Assign using one constructor as in python\n+    auto x = Variable(scope.WithOpName(\"x\"), {2, 2}, DT_FLOAT);\n+    auto assign_x = Assign(scope.WithOpName(\"Assign_x\"), x, Const(scope, {{1.0f, 2.0f}, {3.0f, 4.0f}}));\n+\n+    auto y = Variable(scope.WithOpName(\"y\"), {2, 2}, DT_FLOAT);\n+    auto assign_y = Assign(scope.WithOpName(\"Assign_y\"), y, Const(scope, {{1.0f, 0.0f}, {0.0f, 1.0f}}));\n+\n+    // the assign node is only used once, it should not be used in the graph\n+    auto z = MatMul(scope.WithOpName(\"z\"), x,  y);\n+\n+    TF_ASSERT_OK(scope.status());\n+    CHECK_NOTNULL(z.node());\n+\n+    if (expected) {\n+\n+      // we manually add the gradient node to the expected scope\n+      Scope scope_gradient = scope.NewSubScope(\"Gradients\");\n+      Scope scope_optimizer = scope.NewSubScope(\"GradientDescent\");\n+\n+      // gradients\n+      auto dz = ops::OnesLike(scope_gradient, z);\n+      auto dx = MatMul(scope_gradient, dz, y, MatMul::TransposeB(true));\n+      auto dy = MatMul(scope_gradient, x, dz, MatMul::TransposeA(true));\n+\n+      // update\n+      ApplyGradientDescent(scope_optimizer.NewSubScope(\"update\"), \n+                           {x}, \n+                           Cast(scope_optimizer.NewSubScope(\"learning_rate\"), 0.01f, static_cast<DataType>(Output{x}.type() - 100)), \n+                           {dx});\n+\n+      ApplyGradientDescent(scope_optimizer.NewSubScope(\"update\"), \n+                           {y}, \n+                           Cast(scope_optimizer.NewSubScope(\"learning_rate\"), 0.01f, static_cast<DataType>(Output{y}.type() - 100)), \n+                           {dy});\n+\n+    } else {\n+      \n+      // the gradient nodes and update nodes are added to the graph\n+      auto train = GradientDescentOptimizer(0.01).Minimize(scope, {z});\n+\n+      TF_ASSERT_OK(scope.status());\n+\n+      ClientSession session(scope);\n+\n+      // TODO(theflofly): a global initializer would be nice\n+      TF_CHECK_OK(session.Run({assign_x, assign_y}, nullptr));\n+\n+    }\n+  }\n+\n+  CompareTestAndExpectedGraphs();\n+\n+}\n+\n+// test that the value produced by the optimizer are correct\n+TEST_F(OptimizerValueTest, NeuralNetworkValues) {\n+  \n+  auto x = Const(scope_, {{1.0f, 2.0f}, {3.0f, 4.0f}, {5.0f, 6.0f}});\n+  auto y = Const(scope_, {{1.0f}, {2.0f}, {3.0f}});\n+\n+  auto w1 = Variable(scope_, {2, 1}, DT_FLOAT);\n+  auto assign_w1 = Assign(scope_, w1, Const(scope_, {{0.1f}, {0.2f}}));\n+\n+  auto layer_1 = Tanh(scope_, MatMul(scope_, x, w1));\n+\n+  //TODO(theflofly): add the gradients and do\n+  //auto b1 = Variable(scope_, {1, 1}, DT_FLOAT);\n+  //auto assign_b1 = Assign(scope_, b1, Const(scope_, {{0.45f}}));\n+  //auto layer_1 = Tanh(scope_, Add(scope_, MatMul(scope_, x, w1), b1));\n+  //auto loss = Mean(scope_, Square(scope_, Subtract(scope_, layer_1, y)), 1);\n+  //finally compare the decreasing loss\n+\n+  auto train = GradientDescentOptimizer(0.01).Minimize(scope_, {layer_1});\n+\n+  TF_ASSERT_OK(scope_.status());\n+\n+  ClientSession session(scope_);\n+  \n+  // TODO(theflofly): a global initializer would be nice\n+  TF_CHECK_OK(session.Run({assign_w1}, nullptr));\n+\n+  for (int i = 0; i < 10; i++) {\n+    TF_CHECK_OK(session.Run({train}, nullptr));\n+  }\n+  std::vector<Tensor> outputs;\n+  TF_CHECK_OK(session.Run({layer_1}, &outputs));\n+\n+  test::ExpectTensorEqual<float>(outputs[0], test::AsTensor<float>({-0.66430414, -0.95039594, -0.99360687}, {3, 1}));\n+\n+}\n+\n+}  // namespace\n+}  // namespace tensorflow", "path": "tensorflow/cc/training/optimizer_test.cc", "position": null, "original_position": 154, "commit_id": "194c40cb3a959831339d75042dd7228fe191ff6a", "original_commit_id": "53b68f0932b1765d38f13bfce57b0438ad471bce", "user": {"login": "caisq", "id": 16824702, "node_id": "MDQ6VXNlcjE2ODI0NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16824702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caisq", "html_url": "https://github.com/caisq", "followers_url": "https://api.github.com/users/caisq/followers", "following_url": "https://api.github.com/users/caisq/following{/other_user}", "gists_url": "https://api.github.com/users/caisq/gists{/gist_id}", "starred_url": "https://api.github.com/users/caisq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caisq/subscriptions", "organizations_url": "https://api.github.com/users/caisq/orgs", "repos_url": "https://api.github.com/users/caisq/repos", "events_url": "https://api.github.com/users/caisq/events{/privacy}", "received_events_url": "https://api.github.com/users/caisq/received_events", "type": "User", "site_admin": false}, "body": "C++ style issue. Every file needs a line break at the end.", "created_at": "2017-07-09T02:17:08Z", "updated_at": "2017-08-18T17:11:32Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11377#discussion_r126294265", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11377", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/126294265"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11377#discussion_r126294265"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11377"}}, "body_html": "<p>C++ style issue. Every file needs a line break at the end.</p>", "body_text": "C++ style issue. Every file needs a line break at the end."}