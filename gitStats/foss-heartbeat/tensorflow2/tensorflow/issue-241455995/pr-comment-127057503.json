{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/127057503", "pull_request_review_id": 49603693, "id": 127057503, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNzA1NzUwMw==", "diff_hunk": "@@ -0,0 +1,177 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/cc/training/optimizer.h\"\n+#include \"tensorflow/cc/framework/gradients.h\"\n+#include \"tensorflow/core/grappler/op_types.h\"\n+\n+namespace tensorflow {\n+namespace {\n+\n+const char kGradientsName[] = \"Gradients\";\n+\n+// processor abstract class\n+class OptimizableVariable {\n+ public:\n+    virtual ~OptimizableVariable() {}\n+    virtual Output UpdateOp(const Scope& scope,\n+                            const Optimizer& optimizer,\n+                            const Output& grad) = 0;\n+    explicit OptimizableVariable(const Output& v) : variable_(v) {}\n+    Output variable() { return variable_; }\n+ protected:\n+    Output variable_;\n+};\n+\n+// processor for a Variable\n+class RefVariableProcessor: public OptimizableVariable {\n+ public:\n+    ~RefVariableProcessor() {}\n+    explicit RefVariableProcessor(const Output& v) : OptimizableVariable(v) {}\n+    Output UpdateOp(const Scope& scope,\n+                    const Optimizer& optimizer,\n+                    const Output& grad) override {\n+        return optimizer.ApplyDense(scope, grad, variable_);\n+    }\n+};\n+\n+// return the processor regarding the type of the variable\n+OptimizableVariable* GetProcessor(const Scope& scope, const Output& variable) {\n+    if (::tensorflow::grappler::IsVariable(variable.node()->def())) {\n+        return new RefVariableProcessor(variable);\n+    } else {\n+        scope.UpdateStatus(Status(::tensorflow::error::Code::INVALID_ARGUMENT,\n+            \"No processor yet for this kind of variable: \"\n+            + variable.node()->def().op()));\n+    }\n+    return nullptr;\n+}\n+\n+}  // namespace\n+\n+void Optimizer::ComputeGradients(const Scope& scope,\n+                                 const std::vector<Output>& loss,\n+                                 const std::vector<Output>& var_list,\n+                                 const std::vector<Output>& grad_loss,\n+                                 GradAndVar* grads_and_vars) {\n+    // if a ComputeGradients overload has been called,\n+    // the scope is already kGradientsName\n+    Scope scope_gradient = scope;\n+    if (scope.Name() != kGradientsName) {\n+        scope_gradient = scope.NewSubScope(kGradientsName);\n+    }\n+\n+    // add the gradients node to the graph\n+    std::vector<Output> grad_outputs;\n+    TF_CHECK_OK(AddSymbolicGradients(scope_gradient,\n+                                     loss,\n+                                     var_list,\n+                                     grad_loss,\n+                                     &grad_outputs));\n+\n+    // create a vector of pair (grad, var)\n+    for (int i = 0; i < var_list.size(); ++i) {\n+        if (::tensorflow::grappler::IsVariable(var_list[i].node()->def())) {\n+            grads_and_vars->push_back(std::make_tuple(grad_outputs[i],\n+                                                      var_list[i]));\n+        }\n+    }\n+\n+    // if we don't have the same number of pair (grad, var)\n+    // as the number of vars it means that at least\n+    // one var from var_list is not a variable\n+    if (var_list.size() != grads_and_vars->size()) {\n+        scope.UpdateStatus(Status(::tensorflow::error::Code::INVALID_ARGUMENT,\n+            \"You are trying to compute the gradients of non Variable Output.\"));\n+    }\n+}\n+\n+std::vector<Output> Optimizer::ApplyGradients(const Scope& scope,\n+                                              const GradAndVar& grads_and_vars) {\n+    // we'll return a list of update ops\n+    // TODO(theflofly): make a Group op and return one output\n+    std::vector<Output> update_ops;\n+\n+    if (grads_and_vars.empty()) {\n+        scope.UpdateStatus(Status(::tensorflow::error::Code::INVALID_ARGUMENT,\n+                                  \"No variable to update provided.\"));\n+    }\n+\n+    Scope scope_optimizer = scope.NewSubScope(this->name_);\n+\n+    // each instance can prepare itself using their own logic\n+    this->Prepare(scope_optimizer);\n+\n+    // retrieve the processor for each (gradient, var) tuple\n+    for (int i = 0; i < grads_and_vars.size(); i++) {\n+        Output grad = std::get<0>(grads_and_vars[i]);\n+        Output var = std::get<1>(grads_and_vars[i]);\n+        OptimizableVariable *processor = GetProcessor(scope_optimizer, var);\n+\n+        // no processor for this kind of variable, the scope.status()\n+        // contains more details\n+        if (processor == nullptr) {\n+            return update_ops;\n+        }\n+\n+        update_ops.push_back(processor->UpdateOp(scope_optimizer, *this, grad));\n+        delete processor;\n+    }\n+\n+    return update_ops;\n+}\n+\n+void Optimizer::ComputeGradients(const Scope& scope,\n+                                 const std::vector<Output>& loss,\n+                                 const std::vector<Output>& var_list,\n+                                 GradAndVar* grads_and_vars) {\n+    Scope scope_gradient = scope.NewSubScope(kGradientsName);\n+\n+    // fill grad_loss with 'OnesLike' for all shapes in 'loss'\n+    std::vector<Output> grad_loss;\n+    grad_loss.reserve(loss.size());\n+\n+    for (const Output& loss_output : loss) {\n+        grad_loss.emplace_back(ops::OnesLike(scope_gradient, loss_output));\n+    }\n+\n+    ComputeGradients(scope_gradient, loss, var_list, grad_loss, grads_and_vars);\n+}\n+\n+std::vector<Output> Optimizer::Minimize(const Scope& scope,\n+                                        const std::vector<Output>& loss,\n+                                        const std::vector<Output>& var_list) {\n+    GradAndVar grad_and_var;\n+    ComputeGradients(scope, loss, var_list, &grad_and_var);\n+    return ApplyGradients(scope, grad_and_var);\n+}\n+\n+std::vector<Output> Optimizer::Minimize(const Scope& scope,\n+                                        const std::vector<Output>& loss) {\n+    // retrieve all the variables from the graph\n+    std::vector<Output> var_list;\n+\n+    for (Node* node : scope.graph()->nodes()) {\n+        if (::tensorflow::grappler::IsVariable(node->def())) {", "path": "tensorflow/cc/training/optimizer.cc", "position": null, "original_position": 167, "commit_id": "194c40cb3a959831339d75042dd7228fe191ff6a", "original_commit_id": "53b68f0932b1765d38f13bfce57b0438ad471bce", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "body": "I don't think the c_api has support for trainable variables (not sure what the plan is). I think for now we can add a TODO for this. @asimshankar may know more.", "created_at": "2017-07-12T20:06:09Z", "updated_at": "2017-08-18T17:11:32Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11377#discussion_r127057503", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11377", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/127057503"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11377#discussion_r127057503"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11377"}}, "body_html": "<p>I don't think the c_api has support for trainable variables (not sure what the plan is). I think for now we can add a TODO for this. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> may know more.</p>", "body_text": "I don't think the c_api has support for trainable variables (not sure what the plan is). I think for now we can add a TODO for this. @asimshankar may know more.", "in_reply_to_id": 126294285}