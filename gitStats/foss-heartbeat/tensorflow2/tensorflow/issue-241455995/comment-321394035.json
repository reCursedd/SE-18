{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321394035", "html_url": "https://github.com/tensorflow/tensorflow/pull/11377#issuecomment-321394035", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11377", "id": 321394035, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTM5NDAzNQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-09T22:02:54Z", "updated_at": "2017-08-09T22:02:54Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3902382\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/theflofly\">@theflofly</a> : Firstly, I'd like to thank you for the PR and for pursuing this. I think there is some misunderstanding, so let me try to clarify my postings a bit. There were two points I wanted to make:</p>\n<ul>\n<li>\n<p>I didn't mean to suggest that optimizers be <em>implemented</em> in C, but they <em>be accessible</em> from the C API. I meant to suggest that we think through the path to making these features accessible to other language APIs (even if we don't get to implementing it all immediately). Perhaps the right solution still involves your C++ Optimizer class made accessible via C API calls (just like we do for other runtime constructs in the C API implementation), but let's think it through. I do apologize though, this is something I should probably have pointed out earlier in the discussion in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"227968926\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9837\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9837/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/9837\">#9837</a>.</p>\n</li>\n<li>\n<p>On an unrelated note: The ecosystem would benefit from the ability to have extensions to the TensorFlow APIs built, owned and hosted by contributors so that ideas (particularly in early stages) are not slowed down by unavailable bandwidth from the TensorFlow maintainers. It appeared to me (but perhaps I'm wrong) that this particular feature would have been a great way to demonstrate that as it is purely additive over existing C++ APIs (though, maybe some of the changes you require could be split out into their own PR). But, I do admit that this idea is somewhat orthogonal to the content of the PR itself.</p>\n</li>\n</ul>\n<p>Regarding other points you have raised since: I would argue that the C++ gradients should not be removed precisely because they are providing a path to constructing the gradient graph in other languages (<code>TF_AddGradients</code>). And in fact, internally we have even discussed ways in which gradient functions defined in one language can be made accessible to others. So while the implementation isn't quite there yet (and isn't on the top of the priority list yet), I feel comfortable with the thought put into the plan for gradient functions.</p>\n<p>Regarding this PR, it would be helpful to chart out the full plan in a document before worrying about specifically the C++ code for a gradient descent optimizer. Again, it's okay if we don't get to implementing everything right away, but it would be helpful to have a design and plan charted out and agreed upon. That would also allow for others in the community interested in this area to productively contribute.</p>\n<p>Does that sound reasonable to you? If so, I'm happy to engage on charting a path out in a document. If not, I'd be glad to hear your concerns.</p>", "body_text": "@theflofly : Firstly, I'd like to thank you for the PR and for pursuing this. I think there is some misunderstanding, so let me try to clarify my postings a bit. There were two points I wanted to make:\n\n\nI didn't mean to suggest that optimizers be implemented in C, but they be accessible from the C API. I meant to suggest that we think through the path to making these features accessible to other language APIs (even if we don't get to implementing it all immediately). Perhaps the right solution still involves your C++ Optimizer class made accessible via C API calls (just like we do for other runtime constructs in the C API implementation), but let's think it through. I do apologize though, this is something I should probably have pointed out earlier in the discussion in #9837.\n\n\nOn an unrelated note: The ecosystem would benefit from the ability to have extensions to the TensorFlow APIs built, owned and hosted by contributors so that ideas (particularly in early stages) are not slowed down by unavailable bandwidth from the TensorFlow maintainers. It appeared to me (but perhaps I'm wrong) that this particular feature would have been a great way to demonstrate that as it is purely additive over existing C++ APIs (though, maybe some of the changes you require could be split out into their own PR). But, I do admit that this idea is somewhat orthogonal to the content of the PR itself.\n\n\nRegarding other points you have raised since: I would argue that the C++ gradients should not be removed precisely because they are providing a path to constructing the gradient graph in other languages (TF_AddGradients). And in fact, internally we have even discussed ways in which gradient functions defined in one language can be made accessible to others. So while the implementation isn't quite there yet (and isn't on the top of the priority list yet), I feel comfortable with the thought put into the plan for gradient functions.\nRegarding this PR, it would be helpful to chart out the full plan in a document before worrying about specifically the C++ code for a gradient descent optimizer. Again, it's okay if we don't get to implementing everything right away, but it would be helpful to have a design and plan charted out and agreed upon. That would also allow for others in the community interested in this area to productively contribute.\nDoes that sound reasonable to you? If so, I'm happy to engage on charting a path out in a document. If not, I'd be glad to hear your concerns.", "body": "@theflofly : Firstly, I'd like to thank you for the PR and for pursuing this. I think there is some misunderstanding, so let me try to clarify my postings a bit. There were two points I wanted to make:\r\n\r\n- I didn't mean to suggest that optimizers be _implemented_ in C, but they _be accessible_ from the C API. I meant to suggest that we think through the path to making these features accessible to other language APIs (even if we don't get to implementing it all immediately). Perhaps the right solution still involves your C++ Optimizer class made accessible via C API calls (just like we do for other runtime constructs in the C API implementation), but let's think it through. I do apologize though, this is something I should probably have pointed out earlier in the discussion in #9837. \r\n\r\n- On an unrelated note: The ecosystem would benefit from the ability to have extensions to the TensorFlow APIs built, owned and hosted by contributors so that ideas (particularly in early stages) are not slowed down by unavailable bandwidth from the TensorFlow maintainers. It appeared to me (but perhaps I'm wrong) that this particular feature would have been a great way to demonstrate that as it is purely additive over existing C++ APIs (though, maybe some of the changes you require could be split out into their own PR). But, I do admit that this idea is somewhat orthogonal to the content of the PR itself. \r\n\r\nRegarding other points you have raised since: I would argue that the C++ gradients should not be removed precisely because they are providing a path to constructing the gradient graph in other languages (`TF_AddGradients`). And in fact, internally we have even discussed ways in which gradient functions defined in one language can be made accessible to others. So while the implementation isn't quite there yet (and isn't on the top of the priority list yet), I feel comfortable with the thought put into the plan for gradient functions.\r\n\r\nRegarding this PR, it would be helpful to chart out the full plan in a document before worrying about specifically the C++ code for a gradient descent optimizer. Again, it's okay if we don't get to implementing everything right away, but it would be helpful to have a design and plan charted out and agreed upon. That would also allow for others in the community interested in this area to productively contribute. \r\n\r\nDoes that sound reasonable to you? If so, I'm happy to engage on charting a path out in a document. If not, I'd be glad to hear your concerns.\r\n"}