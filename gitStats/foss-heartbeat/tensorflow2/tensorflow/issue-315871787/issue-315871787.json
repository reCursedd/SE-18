{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18691", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18691/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18691/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18691/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18691", "id": 315871787, "node_id": "MDU6SXNzdWUzMTU4NzE3ODc=", "number": 18691, "title": "Periodic overhead when using tensorflow dataset for model training on GPU", "user": {"login": "nicaogr", "id": 23408564, "node_id": "MDQ6VXNlcjIzNDA4NTY0", "avatar_url": "https://avatars3.githubusercontent.com/u/23408564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nicaogr", "html_url": "https://github.com/nicaogr", "followers_url": "https://api.github.com/users/nicaogr/followers", "following_url": "https://api.github.com/users/nicaogr/following{/other_user}", "gists_url": "https://api.github.com/users/nicaogr/gists{/gist_id}", "starred_url": "https://api.github.com/users/nicaogr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nicaogr/subscriptions", "organizations_url": "https://api.github.com/users/nicaogr/orgs", "repos_url": "https://api.github.com/users/nicaogr/repos", "events_url": "https://api.github.com/users/nicaogr/events{/privacy}", "received_events_url": "https://api.github.com/users/nicaogr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2018-04-19T13:13:00Z", "updated_at": "2018-06-25T19:24:38Z", "closed_at": "2018-06-25T19:00:44Z", "author_association": "NONE", "body_html": "<p>As you can see it in the following code, I am trying to train a simple model on Tensorflow with a Tensorflow Dataset. The dataset is pretty huge (2000 exemples of each 300*2000 elements). I shuffle, repeat and batch the dataset in order to do a stochastic gradient descent for training my model.</p>\n<p>But I can observe a period overhead of the optimisation step (it is sess.run(train) in my code).</p>\n<p>As you can see it here, every 5 steps, it needs 3s instead of 0.5 to do the optimisation.</p>\n<p>Step 105 duration : 3.5233473777770996</p>\n<p>Step 106 duration : 0.5653283596038818</p>\n<p>Step 107 duration : 0.5391891002655029</p>\n<p>Step 108 duration : 0.5480048656463623</p>\n<p>Step 109 duration : 0.0415492057800293</p>\n<p>Step 110 duration : 3.032115936279297</p>\n<p>Step 111 duration : 0.5407207012176514</p>\n<p>Step 112 duration : 0.5276811122894287</p>\n<p>Step 113 duration : 0.5448746681213379</p>\n<p>Step 114 duration : 0.04253268241882324</p>\n<p>Step 115 duration : 3.1273345947265625</p>\n<p>Moreover my GPU is almost all the time at 0% utilisation with around 90% of the memory used.</p>\n<p>It seems that this overhead arrived when the Iterator finish to see all the dataset.</p>\n<p>Do you have any idea how I can speed up my training ?<br>\nIt is due to the fact that my model is really simple ?</p>\n<p>Best,</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li>**Have I written custom code **:  named Test_TimeDataset.py</li>\n<li>**OS Platform and Distribution **:  Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li>**TensorFlow version **: 1.4</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 8.0 and cuDNN 6.0.21</li>\n<li><strong>GPU model and memory</strong>: GeForce 1080 with 11Go</li>\n<li><strong>Exact command to reproduce</strong>: python Test_TimeDataset.py</li>\n</ul>\n<p>I also reproduce it with</p>\n<ul>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li>**TensorFlow version **: 1.8</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.0 and cuDNN 7.1.3.16<br>\nOn the same machine (I have both versions on the same Ubuntu session).</li>\n</ul>\n<p>But I am not sure that Tensorflow use the cuDNN library.</p>\n<p>You can also found the code <a href=\"https://github.com/nicaogr/Git_Issues/blob/master/Test_TimeDataset.py\">here</a></p>\n<h3>Source code</h3>\n<p>``<br>\nimport tensorflow as tf<br>\nimport numpy as np<br>\nimport os, time, multiprocessing<br>\nimport matplotlib.pyplot as plt</p>\n<p>def _floats_feature(value):<br>\nreturn tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))</p>\n<p>def parser(record):<br>\nnum_features = 2000<br>\nsize_group = 300<br>\nnum_classes= 10<br>\nclass_indice = 0<br>\nkeys_to_features={<br>\n'X': tf.FixedLenFeature([size_group*num_features],tf.float32),<br>\n'label' : tf.FixedLenFeature([num_classes],tf.float32)}<br>\nparsed = tf.parse_single_example(record, keys_to_features)</p>\n<pre><code>label = parsed['label']\nlabel = tf.slice(label,[class_indice],[1])\nlabel = tf.squeeze(label) # To get a vector one dimension\nX = parsed['X']\nX= tf.reshape(X, [size_group,num_features])\nreturn X, label\n</code></pre>\n<p>def test_train_w_dataset():<br>\nnum_features = 2000<br>\nnum_ex = 2000<br>\nsize_group = 300<br>\nnum_classes = 10<br>\nbatch_size= 480<br>\nmax_iters = 300<br>\nbuffer_size = 10000</p>\n<pre><code># Creation of the Dataset \nfilename_tfrecords = 'tmp.tfrecords'\nif not(os.path.isfile(filename_tfrecords)): # If the file doesn't exist we will create it\n    print(\"Start creating the Dataset\")\n    writer = tf.python_io.TFRecordWriter(filename_tfrecords)\n    \n    for i in range(num_ex):\n        if i % 1000 == 0: print(\"Step :\",i)\n        X = np.random.normal(size=(size_group,num_features))\n        vectors =  2*np.random.randint(0,2,(num_classes,1))-1\n        features=tf.train.Features(feature={\n                    'X': _floats_feature(X),\n                    'label' : _floats_feature(vectors)})\n        example = tf.train.Example(features=features)       \n        writer.write(example.SerializeToString())\n    writer.close()\nelse:\n    print(\"The dataset tfrecords already exist\")\n \ntrain_dataset = tf.data.TFRecordDataset(filename_tfrecords)\nnum_proc = multiprocessing.cpu_count()\ntrain_dataset = train_dataset.map(parser,\n                                    num_parallel_calls=num_proc)\ndataset_shuffle = train_dataset.shuffle(buffer_size=buffer_size,\n                                             reshuffle_each_iteration=True) \ndataset_shuffle = dataset_shuffle.batch(batch_size)\ndataset_shuffle = dataset_shuffle.repeat() \ndataset_shuffle = dataset_shuffle.prefetch(batch_size) \nshuffle_iterator = dataset_shuffle.make_initializable_iterator()\nX_, y_ = shuffle_iterator.get_next()\n\nW=tf.Variable(tf.random_normal([num_features], stddev=1.),name=\"weights\")\nW=tf.reshape(W,(1,1,num_features))\nProd=tf.reduce_sum(tf.multiply(W,X_),axis=2)\nMax=tf.reduce_max(Prod,axis=1)\nTan= tf.reduce_sum(tf.multiply(tf.tanh(Max),y_))\nloss= tf.add(Tan,tf.reduce_sum(tf.multiply(W,W)))\n\nLR = 0.01\nrestarts = 1\noptimizer = tf.train.GradientDescentOptimizer(LR) \nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\ntrain = optimizer.minimize(loss)  \nprint(\"The graph is defined\")\nsess = tf.Session(config=config)\n    \ndurationTab = []\n\nfor essai in range(restarts+1):\n    t0 = time.time()\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    sess.run(shuffle_iterator.initializer)\n    t1 = time.time()\n    duration = t1 - t0\n    print('Duration of initialization : ',duration)\n    for step in range(max_iters):\n        t0 = time.time()\n        sess.run(train)\n        t1 = time.time()\n        duration = t1 - t0\n        print(\"Step \",str(step),' duration : ',duration)\n        durationTab += [duration]\n        \n\nplt.plot(durationTab)\nplt.ylabel('Duration')\nplt.xlabel('Iteration')\nplt.show()\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\ntest_train_w_dataset()</p>\n<p>``</p>", "body_text": "As you can see it in the following code, I am trying to train a simple model on Tensorflow with a Tensorflow Dataset. The dataset is pretty huge (2000 exemples of each 300*2000 elements). I shuffle, repeat and batch the dataset in order to do a stochastic gradient descent for training my model.\nBut I can observe a period overhead of the optimisation step (it is sess.run(train) in my code).\nAs you can see it here, every 5 steps, it needs 3s instead of 0.5 to do the optimisation.\nStep 105 duration : 3.5233473777770996\nStep 106 duration : 0.5653283596038818\nStep 107 duration : 0.5391891002655029\nStep 108 duration : 0.5480048656463623\nStep 109 duration : 0.0415492057800293\nStep 110 duration : 3.032115936279297\nStep 111 duration : 0.5407207012176514\nStep 112 duration : 0.5276811122894287\nStep 113 duration : 0.5448746681213379\nStep 114 duration : 0.04253268241882324\nStep 115 duration : 3.1273345947265625\nMoreover my GPU is almost all the time at 0% utilisation with around 90% of the memory used.\nIt seems that this overhead arrived when the Iterator finish to see all the dataset.\nDo you have any idea how I can speed up my training ?\nIt is due to the fact that my model is really simple ?\nBest,\n\nSystem information\n\n**Have I written custom code **:  named Test_TimeDataset.py\n**OS Platform and Distribution **:  Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): Binary\n**TensorFlow version **: 1.4\nPython version: 3.6\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: CUDA 8.0 and cuDNN 6.0.21\nGPU model and memory: GeForce 1080 with 11Go\nExact command to reproduce: python Test_TimeDataset.py\n\nI also reproduce it with\n\nTensorFlow installed from (source or binary): Binary\n**TensorFlow version **: 1.8\nPython version: 3.6\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: CUDA 9.0 and cuDNN 7.1.3.16\nOn the same machine (I have both versions on the same Ubuntu session).\n\nBut I am not sure that Tensorflow use the cuDNN library.\nYou can also found the code here\nSource code\n``\nimport tensorflow as tf\nimport numpy as np\nimport os, time, multiprocessing\nimport matplotlib.pyplot as plt\ndef _floats_feature(value):\nreturn tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))\ndef parser(record):\nnum_features = 2000\nsize_group = 300\nnum_classes= 10\nclass_indice = 0\nkeys_to_features={\n'X': tf.FixedLenFeature([size_group*num_features],tf.float32),\n'label' : tf.FixedLenFeature([num_classes],tf.float32)}\nparsed = tf.parse_single_example(record, keys_to_features)\nlabel = parsed['label']\nlabel = tf.slice(label,[class_indice],[1])\nlabel = tf.squeeze(label) # To get a vector one dimension\nX = parsed['X']\nX= tf.reshape(X, [size_group,num_features])\nreturn X, label\n\ndef test_train_w_dataset():\nnum_features = 2000\nnum_ex = 2000\nsize_group = 300\nnum_classes = 10\nbatch_size= 480\nmax_iters = 300\nbuffer_size = 10000\n# Creation of the Dataset \nfilename_tfrecords = 'tmp.tfrecords'\nif not(os.path.isfile(filename_tfrecords)): # If the file doesn't exist we will create it\n    print(\"Start creating the Dataset\")\n    writer = tf.python_io.TFRecordWriter(filename_tfrecords)\n    \n    for i in range(num_ex):\n        if i % 1000 == 0: print(\"Step :\",i)\n        X = np.random.normal(size=(size_group,num_features))\n        vectors =  2*np.random.randint(0,2,(num_classes,1))-1\n        features=tf.train.Features(feature={\n                    'X': _floats_feature(X),\n                    'label' : _floats_feature(vectors)})\n        example = tf.train.Example(features=features)       \n        writer.write(example.SerializeToString())\n    writer.close()\nelse:\n    print(\"The dataset tfrecords already exist\")\n \ntrain_dataset = tf.data.TFRecordDataset(filename_tfrecords)\nnum_proc = multiprocessing.cpu_count()\ntrain_dataset = train_dataset.map(parser,\n                                    num_parallel_calls=num_proc)\ndataset_shuffle = train_dataset.shuffle(buffer_size=buffer_size,\n                                             reshuffle_each_iteration=True) \ndataset_shuffle = dataset_shuffle.batch(batch_size)\ndataset_shuffle = dataset_shuffle.repeat() \ndataset_shuffle = dataset_shuffle.prefetch(batch_size) \nshuffle_iterator = dataset_shuffle.make_initializable_iterator()\nX_, y_ = shuffle_iterator.get_next()\n\nW=tf.Variable(tf.random_normal([num_features], stddev=1.),name=\"weights\")\nW=tf.reshape(W,(1,1,num_features))\nProd=tf.reduce_sum(tf.multiply(W,X_),axis=2)\nMax=tf.reduce_max(Prod,axis=1)\nTan= tf.reduce_sum(tf.multiply(tf.tanh(Max),y_))\nloss= tf.add(Tan,tf.reduce_sum(tf.multiply(W,W)))\n\nLR = 0.01\nrestarts = 1\noptimizer = tf.train.GradientDescentOptimizer(LR) \nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\ntrain = optimizer.minimize(loss)  \nprint(\"The graph is defined\")\nsess = tf.Session(config=config)\n    \ndurationTab = []\n\nfor essai in range(restarts+1):\n    t0 = time.time()\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    sess.run(shuffle_iterator.initializer)\n    t1 = time.time()\n    duration = t1 - t0\n    print('Duration of initialization : ',duration)\n    for step in range(max_iters):\n        t0 = time.time()\n        sess.run(train)\n        t1 = time.time()\n        duration = t1 - t0\n        print(\"Step \",str(step),' duration : ',duration)\n        durationTab += [duration]\n        \n\nplt.plot(durationTab)\nplt.ylabel('Duration')\nplt.xlabel('Iteration')\nplt.show()\n\nif name == 'main':\ntest_train_w_dataset()\n``", "body": "As you can see it in the following code, I am trying to train a simple model on Tensorflow with a Tensorflow Dataset. The dataset is pretty huge (2000 exemples of each 300*2000 elements). I shuffle, repeat and batch the dataset in order to do a stochastic gradient descent for training my model.\r\n\r\nBut I can observe a period overhead of the optimisation step (it is sess.run(train) in my code).\r\n\r\nAs you can see it here, every 5 steps, it needs 3s instead of 0.5 to do the optimisation.\r\n\r\nStep 105 duration : 3.5233473777770996\r\n\r\nStep 106 duration : 0.5653283596038818\r\n\r\nStep 107 duration : 0.5391891002655029\r\n\r\nStep 108 duration : 0.5480048656463623\r\n\r\nStep 109 duration : 0.0415492057800293\r\n\r\nStep 110 duration : 3.032115936279297\r\n\r\nStep 111 duration : 0.5407207012176514\r\n\r\nStep 112 duration : 0.5276811122894287\r\n\r\nStep 113 duration : 0.5448746681213379\r\n\r\nStep 114 duration : 0.04253268241882324\r\n\r\nStep 115 duration : 3.1273345947265625\r\n\r\nMoreover my GPU is almost all the time at 0% utilisation with around 90% of the memory used.\r\n\r\nIt seems that this overhead arrived when the Iterator finish to see all the dataset.\r\n\r\n\r\nDo you have any idea how I can speed up my training ?\r\nIt is due to the fact that my model is really simple ? \r\n\r\nBest,\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code **:  named Test_TimeDataset.py\r\n- **OS Platform and Distribution **:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version **: 1.4\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0 and cuDNN 6.0.21\r\n- **GPU model and memory**: GeForce 1080 with 11Go \r\n- **Exact command to reproduce**: python Test_TimeDataset.py\r\n\r\nI also reproduce it with \r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version **: 1.8 \r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0 and cuDNN 7.1.3.16\r\nOn the same machine (I have both versions on the same Ubuntu session).\r\n\r\nBut I am not sure that Tensorflow use the cuDNN library.\r\n\r\nYou can also found the code [here](https://github.com/nicaogr/Git_Issues/blob/master/Test_TimeDataset.py)\r\n\r\n### Source code\r\n``\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os, time, multiprocessing\r\nimport matplotlib.pyplot as plt\r\n\r\ndef _floats_feature(value):\r\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))\r\n\r\n\r\ndef parser(record):\r\n    num_features = 2000\r\n    size_group = 300\r\n    num_classes= 10\r\n    class_indice = 0\r\n    keys_to_features={\r\n                'X': tf.FixedLenFeature([size_group*num_features],tf.float32),\r\n                'label' : tf.FixedLenFeature([num_classes],tf.float32)}\r\n    parsed = tf.parse_single_example(record, keys_to_features)\r\n    \r\n    label = parsed['label']\r\n    label = tf.slice(label,[class_indice],[1])\r\n    label = tf.squeeze(label) # To get a vector one dimension\r\n    X = parsed['X']\r\n    X= tf.reshape(X, [size_group,num_features])\r\n    return X, label\r\n\r\n\r\ndef test_train_w_dataset(): \r\n    num_features = 2000\r\n    num_ex = 2000\r\n    size_group = 300\r\n    num_classes = 10\r\n    batch_size= 480\r\n    max_iters = 300\r\n    buffer_size = 10000\r\n    \r\n    # Creation of the Dataset \r\n    filename_tfrecords = 'tmp.tfrecords'\r\n    if not(os.path.isfile(filename_tfrecords)): # If the file doesn't exist we will create it\r\n        print(\"Start creating the Dataset\")\r\n        writer = tf.python_io.TFRecordWriter(filename_tfrecords)\r\n        \r\n        for i in range(num_ex):\r\n            if i % 1000 == 0: print(\"Step :\",i)\r\n            X = np.random.normal(size=(size_group,num_features))\r\n            vectors =  2*np.random.randint(0,2,(num_classes,1))-1\r\n            features=tf.train.Features(feature={\r\n                        'X': _floats_feature(X),\r\n                        'label' : _floats_feature(vectors)})\r\n            example = tf.train.Example(features=features)       \r\n            writer.write(example.SerializeToString())\r\n        writer.close()\r\n    else:\r\n        print(\"The dataset tfrecords already exist\")\r\n     \r\n    train_dataset = tf.data.TFRecordDataset(filename_tfrecords)\r\n    num_proc = multiprocessing.cpu_count()\r\n    train_dataset = train_dataset.map(parser,\r\n                                        num_parallel_calls=num_proc)\r\n    dataset_shuffle = train_dataset.shuffle(buffer_size=buffer_size,\r\n                                                 reshuffle_each_iteration=True) \r\n    dataset_shuffle = dataset_shuffle.batch(batch_size)\r\n    dataset_shuffle = dataset_shuffle.repeat() \r\n    dataset_shuffle = dataset_shuffle.prefetch(batch_size) \r\n    shuffle_iterator = dataset_shuffle.make_initializable_iterator()\r\n    X_, y_ = shuffle_iterator.get_next()\r\n\r\n    W=tf.Variable(tf.random_normal([num_features], stddev=1.),name=\"weights\")\r\n    W=tf.reshape(W,(1,1,num_features))\r\n    Prod=tf.reduce_sum(tf.multiply(W,X_),axis=2)\r\n    Max=tf.reduce_max(Prod,axis=1)\r\n    Tan= tf.reduce_sum(tf.multiply(tf.tanh(Max),y_))\r\n    loss= tf.add(Tan,tf.reduce_sum(tf.multiply(W,W)))\r\n\r\n    LR = 0.01\r\n    restarts = 1\r\n    optimizer = tf.train.GradientDescentOptimizer(LR) \r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    train = optimizer.minimize(loss)  \r\n    print(\"The graph is defined\")\r\n    sess = tf.Session(config=config)\r\n        \r\n    durationTab = []\r\n    \r\n    for essai in range(restarts+1):\r\n        t0 = time.time()\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n        sess.run(shuffle_iterator.initializer)\r\n        t1 = time.time()\r\n        duration = t1 - t0\r\n        print('Duration of initialization : ',duration)\r\n        for step in range(max_iters):\r\n            t0 = time.time()\r\n            sess.run(train)\r\n            t1 = time.time()\r\n            duration = t1 - t0\r\n            print(\"Step \",str(step),' duration : ',duration)\r\n            durationTab += [duration]\r\n            \r\n\r\n    plt.plot(durationTab)\r\n    plt.ylabel('Duration')\r\n    plt.xlabel('Iteration')\r\n    plt.show()\r\n\r\nif __name__ == '__main__':\r\n    test_train_w_dataset()\r\n\r\n``"}