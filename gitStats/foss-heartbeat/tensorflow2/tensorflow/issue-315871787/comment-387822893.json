{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/387822893", "html_url": "https://github.com/tensorflow/tensorflow/issues/18691#issuecomment-387822893", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18691", "id": 387822893, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzgyMjg5Mw==", "user": {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T17:56:56Z", "updated_at": "2018-05-09T17:56:56Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23408564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nicaogr\">@nicaogr</a> can you grab timeline results that show the difference between the 3s and the 0.5s? I agree that something is happening every 5 timesteps that's pretty slow. It's not clear to me why you think the problem comes from datasets--is there some evidence that you have to point the finger there, or is it just the intuition about the size of your training set?</p>\n<p>I wonder whether you're running into an unintended consequence of your dataset (1.2G elements, right?) being able to fit into memory. It might be that the datasets API is designed to work in circumstances that are disk-bound, and that it's not optimized for the in-memory case.</p>", "body_text": "@nicaogr can you grab timeline results that show the difference between the 3s and the 0.5s? I agree that something is happening every 5 timesteps that's pretty slow. It's not clear to me why you think the problem comes from datasets--is there some evidence that you have to point the finger there, or is it just the intuition about the size of your training set?\nI wonder whether you're running into an unintended consequence of your dataset (1.2G elements, right?) being able to fit into memory. It might be that the datasets API is designed to work in circumstances that are disk-bound, and that it's not optimized for the in-memory case.", "body": "@nicaogr can you grab timeline results that show the difference between the 3s and the 0.5s? I agree that something is happening every 5 timesteps that's pretty slow. It's not clear to me why you think the problem comes from datasets--is there some evidence that you have to point the finger there, or is it just the intuition about the size of your training set?\r\n\r\nI wonder whether you're running into an unintended consequence of your dataset (1.2G elements, right?) being able to fit into memory. It might be that the datasets API is designed to work in circumstances that are disk-bound, and that it's not optimized for the in-memory case."}