{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14173", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14173/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14173/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14173/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14173", "id": 270539986, "node_id": "MDU6SXNzdWUyNzA1Mzk5ODY=", "number": 14173, "title": "using batchnorm in conv2d discard the bias", "user": {"login": "fstrub95", "id": 13366148, "node_id": "MDQ6VXNlcjEzMzY2MTQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/13366148?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fstrub95", "html_url": "https://github.com/fstrub95", "followers_url": "https://api.github.com/users/fstrub95/followers", "following_url": "https://api.github.com/users/fstrub95/following{/other_user}", "gists_url": "https://api.github.com/users/fstrub95/gists{/gist_id}", "starred_url": "https://api.github.com/users/fstrub95/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fstrub95/subscriptions", "organizations_url": "https://api.github.com/users/fstrub95/orgs", "repos_url": "https://api.github.com/users/fstrub95/repos", "events_url": "https://api.github.com/users/fstrub95/events{/privacy}", "received_events_url": "https://api.github.com/users/fstrub95/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-02T05:58:16Z", "updated_at": "2017-12-06T23:25:58Z", "closed_at": "2017-11-02T06:02:33Z", "author_association": "NONE", "body_html": "<p>Hi!<br>\nI observed that whenever I applied batch normalization to conv2d, the bias variable are not created!<br>\nversion : Tensorflow 1.3</p>\n<pre><code>import tensorflow\n\nimage = tf.placeholder(tf.float32, [None, 14, 14, 1024])\n\ntf.contrib.layers.conv2d(image,\n                                 num_outputs=128,\n                                 kernel_size=[3,3],\n                                 normalizer_fn=tf.layers.batch_normalization,\n                                 normalizer_params={\"training\": False, \"reuse\": False},\n                                 activation_fn=tf.nn.relu,\n                                 reuse=False,\n                                 scope=\"conv1\")\n                                 \ntf.contrib.layers.conv2d(image,\n                                 num_outputs=128,\n                                 kernel_size=[3,3],\n                                 #normalizer_fn=tf.layers.batch_normalization,\n                                 #normalizer_params={\"training\": False, \"reuse\": False},\n                                 #activation_fn=tf.nn.relu,\n                                 reuse=False,\n                                 scope=\"conv2\")\n                                 \nfor v in tf.trainable_variables():\n\tprint(v)\n</code></pre>\n<p>output:</p>\n<p>&lt;tf.Variable 'conv1/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref&gt;<br>\n&lt;tf.Variable 'conv1/batch_normalization/beta:0' shape=(128,) dtype=float32_ref&gt;<br>\n&lt;tf.Variable 'conv1/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref&gt;<br>\n&lt;tf.Variable 'conv2/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref&gt;<br>\n<strong>&lt;tf.Variable 'conv2/biases:0' shape=(128,) dtype=float32_ref&gt;</strong></p>\n<p>no bias for conv1 !</p>\n<p>Question: Is it a bug or a feature :)</p>\n<p>[edit] I also observed the same behavior with mlp.</p>\n<p>Thank you very much!</p>\n<p>Florian</p>", "body_text": "Hi!\nI observed that whenever I applied batch normalization to conv2d, the bias variable are not created!\nversion : Tensorflow 1.3\nimport tensorflow\n\nimage = tf.placeholder(tf.float32, [None, 14, 14, 1024])\n\ntf.contrib.layers.conv2d(image,\n                                 num_outputs=128,\n                                 kernel_size=[3,3],\n                                 normalizer_fn=tf.layers.batch_normalization,\n                                 normalizer_params={\"training\": False, \"reuse\": False},\n                                 activation_fn=tf.nn.relu,\n                                 reuse=False,\n                                 scope=\"conv1\")\n                                 \ntf.contrib.layers.conv2d(image,\n                                 num_outputs=128,\n                                 kernel_size=[3,3],\n                                 #normalizer_fn=tf.layers.batch_normalization,\n                                 #normalizer_params={\"training\": False, \"reuse\": False},\n                                 #activation_fn=tf.nn.relu,\n                                 reuse=False,\n                                 scope=\"conv2\")\n                                 \nfor v in tf.trainable_variables():\n\tprint(v)\n\noutput:\n<tf.Variable 'conv1/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref>\n<tf.Variable 'conv1/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>\n<tf.Variable 'conv1/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>\n<tf.Variable 'conv2/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref>\n<tf.Variable 'conv2/biases:0' shape=(128,) dtype=float32_ref>\nno bias for conv1 !\nQuestion: Is it a bug or a feature :)\n[edit] I also observed the same behavior with mlp.\nThank you very much!\nFlorian", "body": "Hi!\r\nI observed that whenever I applied batch normalization to conv2d, the bias variable are not created!\r\nversion : Tensorflow 1.3\r\n\r\n```\r\nimport tensorflow\r\n\r\nimage = tf.placeholder(tf.float32, [None, 14, 14, 1024])\r\n\r\ntf.contrib.layers.conv2d(image,\r\n                                 num_outputs=128,\r\n                                 kernel_size=[3,3],\r\n                                 normalizer_fn=tf.layers.batch_normalization,\r\n                                 normalizer_params={\"training\": False, \"reuse\": False},\r\n                                 activation_fn=tf.nn.relu,\r\n                                 reuse=False,\r\n                                 scope=\"conv1\")\r\n                                 \r\ntf.contrib.layers.conv2d(image,\r\n                                 num_outputs=128,\r\n                                 kernel_size=[3,3],\r\n                                 #normalizer_fn=tf.layers.batch_normalization,\r\n                                 #normalizer_params={\"training\": False, \"reuse\": False},\r\n                                 #activation_fn=tf.nn.relu,\r\n                                 reuse=False,\r\n                                 scope=\"conv2\")\r\n                                 \r\nfor v in tf.trainable_variables():\r\n\tprint(v)\r\n```\r\noutput:\r\n\r\n<tf.Variable 'conv1/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref>\r\n<tf.Variable 'conv1/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>\r\n<tf.Variable 'conv1/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>\r\n<tf.Variable 'conv2/weights:0' shape=(3, 3, 1024, 128) dtype=float32_ref>\r\n**<tf.Variable 'conv2/biases:0' shape=(128,) dtype=float32_ref>**\r\n\r\nno bias for conv1 !\r\n\r\nQuestion: Is it a bug or a feature :)\r\n\r\n[edit] I also observed the same behavior with mlp.\r\n\r\nThank you very much!\r\n\r\nFlorian\r\n\r\n"}