{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/193074750", "html_url": "https://github.com/tensorflow/tensorflow/issues/1397#issuecomment-193074750", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1397", "id": 193074750, "node_id": "MDEyOklzc3VlQ29tbWVudDE5MzA3NDc1MA==", "user": {"login": "thinxer", "id": 66073, "node_id": "MDQ6VXNlcjY2MDcz", "avatar_url": "https://avatars3.githubusercontent.com/u/66073?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thinxer", "html_url": "https://github.com/thinxer", "followers_url": "https://api.github.com/users/thinxer/followers", "following_url": "https://api.github.com/users/thinxer/following{/other_user}", "gists_url": "https://api.github.com/users/thinxer/gists{/gist_id}", "starred_url": "https://api.github.com/users/thinxer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thinxer/subscriptions", "organizations_url": "https://api.github.com/users/thinxer/orgs", "repos_url": "https://api.github.com/users/thinxer/repos", "events_url": "https://api.github.com/users/thinxer/events{/privacy}", "received_events_url": "https://api.github.com/users/thinxer/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-07T03:33:18Z", "updated_at": "2016-03-08T02:53:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm not sure if you are adapting from the cifar 10 example. If yes, you should try place variables on GPU. The cifar10 example uses variables on CPU by default. You may achieve about 1.5x speed up compared to a single GPU setup with 2 GPUs.</p>\n<p>I've got about 2x speed up with 4 GPUs, and 3x with 8 GPUs. It is far from linear acceleration. I guess <code>tf.reduce_mean</code> has a noticeable overhead here, when averaging grads from different GPUs.</p>\n<p>Even when only one GPU is used, adding the <code>tf.reduce_mean</code> operation can cause a 50% penalty on speed.</p>\n<p>The above results are collected with Titan X GPUs and a VGG-16 network.</p>", "body_text": "I'm not sure if you are adapting from the cifar 10 example. If yes, you should try place variables on GPU. The cifar10 example uses variables on CPU by default. You may achieve about 1.5x speed up compared to a single GPU setup with 2 GPUs.\nI've got about 2x speed up with 4 GPUs, and 3x with 8 GPUs. It is far from linear acceleration. I guess tf.reduce_mean has a noticeable overhead here, when averaging grads from different GPUs.\nEven when only one GPU is used, adding the tf.reduce_mean operation can cause a 50% penalty on speed.\nThe above results are collected with Titan X GPUs and a VGG-16 network.", "body": "I'm not sure if you are adapting from the cifar 10 example. If yes, you should try place variables on GPU. The cifar10 example uses variables on CPU by default. You may achieve about 1.5x speed up compared to a single GPU setup with 2 GPUs.\n\nI've got about 2x speed up with 4 GPUs, and 3x with 8 GPUs. It is far from linear acceleration. I guess `tf.reduce_mean` has a noticeable overhead here, when averaging grads from different GPUs.\n\nEven when only one GPU is used, adding the `tf.reduce_mean` operation can cause a 50% penalty on speed.\n\nThe above results are collected with Titan X GPUs and a VGG-16 network.\n"}