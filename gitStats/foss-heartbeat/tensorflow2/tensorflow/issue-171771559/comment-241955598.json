{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241955598", "html_url": "https://github.com/tensorflow/tensorflow/issues/3886#issuecomment-241955598", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3886", "id": 241955598, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTk1NTU5OA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-24T04:40:32Z", "updated_at": "2016-08-24T04:40:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is intended behavior: variable reads are cached by default, and the implicitly created caching operator (a <code>tf.identity()</code>) is not ordered (by any control or data dependency) with the assignment operator. As a result, the caching operator can cache the value <code>0.</code> or <code>1.</code> depending on whether it runs before or after the <code>tf.assign()</code> operator.</p>\n<p>There are several ways to avoid this:</p>\n<ol>\n<li>\n<p>As you noticed, you can use the <code>Variable.ref()</code> method to perform a non-caching read of the variable. This will ensure that it's read after the assignment, and you will always get the result <code>3</code>.</p>\n</li>\n<li>\n<p>You can read the <em>result</em> of the assignment, which is itself a ref-tensor (l-value) representing the value of the tensor after the assignment. The following program always prints <code>3</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0</span>.)\nx_op <span class=\"pl-k\">=</span> tf.assign(x, [<span class=\"pl-c1\">1</span>.], <span class=\"pl-v\">validate_shape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>) \nplus_op <span class=\"pl-k\">=</span> x_op <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>\n\ntf.initialize_variables([x]).run()\n<span class=\"pl-c1\">print</span> plus_op.eval()</pre></div>\n</li>\n<li>\n<p>(Perhaps least intuitively.) You would always get the result <code>3</code> if you didn't change the shape of the variable in the assignment. The following program also always prints <code>3</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0</span>.)\nx_op <span class=\"pl-k\">=</span> tf.assign(x, <span class=\"pl-c1\">1</span>.) \n<span class=\"pl-k\">with</span> tf.control_dependencies([x_op]) \nplus_op <span class=\"pl-k\">=</span> x <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>\n\ntf.initialize_variables([x]).run()\n<span class=\"pl-c1\">print</span> plus_op.eval()</pre></div>\n<p>...however this is an implementation detail (because the same buffer is used for <code>x</code> before and after the assignment, whereas when you change the shape a new buffer is allocated).</p>\n</li>\n</ol>\n<p>This is, admittedly, not the most intuitive semantics, but the caching tends to give the most efficient execution plan when running in a distributed setting with variables on a shared parameter server.</p>", "body_text": "This is intended behavior: variable reads are cached by default, and the implicitly created caching operator (a tf.identity()) is not ordered (by any control or data dependency) with the assignment operator. As a result, the caching operator can cache the value 0. or 1. depending on whether it runs before or after the tf.assign() operator.\nThere are several ways to avoid this:\n\n\nAs you noticed, you can use the Variable.ref() method to perform a non-caching read of the variable. This will ensure that it's read after the assignment, and you will always get the result 3.\n\n\nYou can read the result of the assignment, which is itself a ref-tensor (l-value) representing the value of the tensor after the assignment. The following program always prints 3:\nx = tf.Variable(0.)\nx_op = tf.assign(x, [1.], validate_shape=False) \nplus_op = x_op + 2\n\ntf.initialize_variables([x]).run()\nprint plus_op.eval()\n\n\n(Perhaps least intuitively.) You would always get the result 3 if you didn't change the shape of the variable in the assignment. The following program also always prints 3:\nx = tf.Variable(0.)\nx_op = tf.assign(x, 1.) \nwith tf.control_dependencies([x_op]) \nplus_op = x + 2\n\ntf.initialize_variables([x]).run()\nprint plus_op.eval()\n...however this is an implementation detail (because the same buffer is used for x before and after the assignment, whereas when you change the shape a new buffer is allocated).\n\n\nThis is, admittedly, not the most intuitive semantics, but the caching tends to give the most efficient execution plan when running in a distributed setting with variables on a shared parameter server.", "body": "This is intended behavior: variable reads are cached by default, and the implicitly created caching operator (a `tf.identity()`) is not ordered (by any control or data dependency) with the assignment operator. As a result, the caching operator can cache the value `0.` or `1.` depending on whether it runs before or after the `tf.assign()` operator.\n\nThere are several ways to avoid this:\n1. As you noticed, you can use the `Variable.ref()` method to perform a non-caching read of the variable. This will ensure that it's read after the assignment, and you will always get the result `3`.\n2. You can read the _result_ of the assignment, which is itself a ref-tensor (l-value) representing the value of the tensor after the assignment. The following program always prints `3`:\n   \n   ``` python\n   x = tf.Variable(0.)\n   x_op = tf.assign(x, [1.], validate_shape=False) \n   plus_op = x_op + 2\n   \n   tf.initialize_variables([x]).run()\n   print plus_op.eval()\n   ```\n3. (Perhaps least intuitively.) You would always get the result `3` if you didn't change the shape of the variable in the assignment. The following program also always prints `3`:\n   \n   ``` python\n   x = tf.Variable(0.)\n   x_op = tf.assign(x, 1.) \n   with tf.control_dependencies([x_op]) \n   plus_op = x + 2\n   \n   tf.initialize_variables([x]).run()\n   print plus_op.eval()\n   ```\n   \n   ...however this is an implementation detail (because the same buffer is used for `x` before and after the assignment, whereas when you change the shape a new buffer is allocated).\n\nThis is, admittedly, not the most intuitive semantics, but the caching tends to give the most efficient execution plan when running in a distributed setting with variables on a shared parameter server. \n"}