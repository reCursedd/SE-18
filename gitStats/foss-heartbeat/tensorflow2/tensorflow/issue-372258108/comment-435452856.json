{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435452856", "html_url": "https://github.com/tensorflow/tensorflow/issues/23133#issuecomment-435452856", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23133", "id": 435452856, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTQ1Mjg1Ng==", "user": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-02T17:30:06Z", "updated_at": "2018-11-02T17:30:06Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10554727\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/imranparuk\">@imranparuk</a> I modified your Keras Siamese code and tf.estimator.train() works as expected. Here are the accuracy results:</p>\n<pre><code>* Accuracy on training set: 95.86%\n* Accuracy on test set: 95.71%\n</code></pre>\n<p>Code to produce the above results:</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\n\nimport random\nimport tensorflow as tf\n\nfrom tensorflow.python import keras\nfrom keras.models import Sequential  \nfrom tensorflow.python.keras.datasets import mnist\n\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import Input, Flatten, Dense, Dropout, Lambda\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint, Callback\nfrom tensorflow.python.keras.utils import HDF5Matrix, to_categorical\nfrom tensorflow.python.keras.losses import categorical_crossentropy\nfrom tensorflow.python.keras.optimizers import RMSprop\nfrom tensorflow.python.keras import backend as K\n\nnum_classes = 10\nepochs = 20\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef euclidean_distance(vects):\n    x, y = vects\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\n\n\ndef contrastive_loss(y_true, y_pred):\n    '''Contrastive loss from Hadsell-et-al.'06\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    '''\n    margin = 1\n    sqaure_pred = K.square(y_pred)\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n\n\ndef create_pairs(x, digit_indices):\n    '''Positive and negative pair creation.\n    Alternates between positive and negative pairs.\n    '''\n    pairs = []\n    labels = []\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n    for d in range(num_classes):\n        for i in range(n):\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n            pairs += [[x[z1], x[z2]]]\n            inc = random.randrange(1, num_classes)\n            dn = (d + inc) % num_classes\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n            pairs += [[x[z1], x[z2]]]\n            labels += [1, 0]\n    return np.array(pairs), np.array(labels)\n\n\ndef create_base_network(input_shape):\n    '''Base network to be shared (eq. to feature extraction).\n    '''\n\n    input = Input(shape=input_shape)\n    x = Flatten()(input)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    return Model(input, x)\n\n\ndef compute_accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    pred = y_pred.ravel() &lt; 0.5\n    return np.mean(pred == y_true)\n\n\ndef accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return K.mean(K.equal(y_true, K.cast(y_pred &lt; 0.5, y_true.dtype)))\n\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\ninput_shape = x_train.shape[1:]\n\n# create training+test positive and negative pairs\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\n\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\nte_pairs, te_y = create_pairs(x_test, digit_indices)\n\n\n# network definition\nbase_network = create_base_network(input_shape)\n\ninput_a = Input(shape=input_shape, name='input_a')\ninput_b = Input(shape=input_shape, name='input_b')\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a = base_network(input_a)\nprocessed_b = base_network(input_b)\n\ndistance = Lambda(euclidean_distance,\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel = Model([input_a, input_b], distance)\n\n# train\nrms = RMSprop()\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n\nmodel.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n           batch_size=128,\n           epochs=1,\n           validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n\nestimata = tf.keras.estimator.model_to_estimator(keras_model=model)\n\n\ninput_name1 = model.input_names[0]\ninput_name2 = model.input_names[1]\n\n\n\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={input_name1: tr_pairs[:,0],\n           input_name2: tr_pairs[:,1]},\n        y=tr_y,\n        batch_size=128,\n        num_epochs=1,\n        shuffle=True)\n#\nestimata.train(\n    input_fn=train_input_fn,\n    steps=None)  # ,\n\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={input_name1: te_pairs[:, 0],\n       input_name2: te_pairs[:, 1]},\n    y=te_y,\n    num_epochs=1,\n    shuffle=False)\n\neval_results = estimata.evaluate(input_fn=eval_input_fn)\n\n# compute final accuracy on training and test sets\ny_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\ntr_acc = compute_accuracy(tr_y, y_pred)\n\ny_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\nte_acc = compute_accuracy(te_y, y_pred)\n\nprint('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\nprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n</code></pre>", "body_text": "@imranparuk I modified your Keras Siamese code and tf.estimator.train() works as expected. Here are the accuracy results:\n* Accuracy on training set: 95.86%\n* Accuracy on test set: 95.71%\n\nCode to produce the above results:\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\n\nimport random\nimport tensorflow as tf\n\nfrom tensorflow.python import keras\nfrom keras.models import Sequential  \nfrom tensorflow.python.keras.datasets import mnist\n\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import Input, Flatten, Dense, Dropout, Lambda\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint, Callback\nfrom tensorflow.python.keras.utils import HDF5Matrix, to_categorical\nfrom tensorflow.python.keras.losses import categorical_crossentropy\nfrom tensorflow.python.keras.optimizers import RMSprop\nfrom tensorflow.python.keras import backend as K\n\nnum_classes = 10\nepochs = 20\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef euclidean_distance(vects):\n    x, y = vects\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\n\n\ndef contrastive_loss(y_true, y_pred):\n    '''Contrastive loss from Hadsell-et-al.'06\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    '''\n    margin = 1\n    sqaure_pred = K.square(y_pred)\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n\n\ndef create_pairs(x, digit_indices):\n    '''Positive and negative pair creation.\n    Alternates between positive and negative pairs.\n    '''\n    pairs = []\n    labels = []\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n    for d in range(num_classes):\n        for i in range(n):\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n            pairs += [[x[z1], x[z2]]]\n            inc = random.randrange(1, num_classes)\n            dn = (d + inc) % num_classes\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n            pairs += [[x[z1], x[z2]]]\n            labels += [1, 0]\n    return np.array(pairs), np.array(labels)\n\n\ndef create_base_network(input_shape):\n    '''Base network to be shared (eq. to feature extraction).\n    '''\n\n    input = Input(shape=input_shape)\n    x = Flatten()(input)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    return Model(input, x)\n\n\ndef compute_accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    pred = y_pred.ravel() < 0.5\n    return np.mean(pred == y_true)\n\n\ndef accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\ninput_shape = x_train.shape[1:]\n\n# create training+test positive and negative pairs\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\n\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\nte_pairs, te_y = create_pairs(x_test, digit_indices)\n\n\n# network definition\nbase_network = create_base_network(input_shape)\n\ninput_a = Input(shape=input_shape, name='input_a')\ninput_b = Input(shape=input_shape, name='input_b')\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a = base_network(input_a)\nprocessed_b = base_network(input_b)\n\ndistance = Lambda(euclidean_distance,\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel = Model([input_a, input_b], distance)\n\n# train\nrms = RMSprop()\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n\nmodel.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n           batch_size=128,\n           epochs=1,\n           validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n\nestimata = tf.keras.estimator.model_to_estimator(keras_model=model)\n\n\ninput_name1 = model.input_names[0]\ninput_name2 = model.input_names[1]\n\n\n\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={input_name1: tr_pairs[:,0],\n           input_name2: tr_pairs[:,1]},\n        y=tr_y,\n        batch_size=128,\n        num_epochs=1,\n        shuffle=True)\n#\nestimata.train(\n    input_fn=train_input_fn,\n    steps=None)  # ,\n\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={input_name1: te_pairs[:, 0],\n       input_name2: te_pairs[:, 1]},\n    y=te_y,\n    num_epochs=1,\n    shuffle=False)\n\neval_results = estimata.evaluate(input_fn=eval_input_fn)\n\n# compute final accuracy on training and test sets\ny_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\ntr_acc = compute_accuracy(tr_y, y_pred)\n\ny_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\nte_acc = compute_accuracy(te_y, y_pred)\n\nprint('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\nprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))", "body": "@imranparuk I modified your Keras Siamese code and tf.estimator.train() works as expected. Here are the accuracy results:\r\n```\r\n* Accuracy on training set: 95.86%\r\n* Accuracy on test set: 95.71%\r\n```\r\n\r\nCode to produce the above results:\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import print_function\r\nimport numpy as np\r\n\r\nimport random\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python import keras\r\nfrom keras.models import Sequential  \r\nfrom tensorflow.python.keras.datasets import mnist\r\n\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import Input, Flatten, Dense, Dropout, Lambda\r\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint, Callback\r\nfrom tensorflow.python.keras.utils import HDF5Matrix, to_categorical\r\nfrom tensorflow.python.keras.losses import categorical_crossentropy\r\nfrom tensorflow.python.keras.optimizers import RMSprop\r\nfrom tensorflow.python.keras import backend as K\r\n\r\nnum_classes = 10\r\nepochs = 20\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n\r\ndef euclidean_distance(vects):\r\n    x, y = vects\r\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\r\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\r\n\r\n\r\ndef eucl_dist_output_shape(shapes):\r\n    shape1, shape2 = shapes\r\n    return (shape1[0], 1)\r\n\r\n\r\ndef contrastive_loss(y_true, y_pred):\r\n    '''Contrastive loss from Hadsell-et-al.'06\r\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\r\n    '''\r\n    margin = 1\r\n    sqaure_pred = K.square(y_pred)\r\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\r\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\r\n\r\n\r\ndef create_pairs(x, digit_indices):\r\n    '''Positive and negative pair creation.\r\n    Alternates between positive and negative pairs.\r\n    '''\r\n    pairs = []\r\n    labels = []\r\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\r\n    for d in range(num_classes):\r\n        for i in range(n):\r\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\r\n            pairs += [[x[z1], x[z2]]]\r\n            inc = random.randrange(1, num_classes)\r\n            dn = (d + inc) % num_classes\r\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\r\n            pairs += [[x[z1], x[z2]]]\r\n            labels += [1, 0]\r\n    return np.array(pairs), np.array(labels)\r\n\r\n\r\ndef create_base_network(input_shape):\r\n    '''Base network to be shared (eq. to feature extraction).\r\n    '''\r\n\r\n    input = Input(shape=input_shape)\r\n    x = Flatten()(input)\r\n    x = Dense(128, activation='relu')(x)\r\n    x = Dropout(0.1)(x)\r\n    x = Dense(128, activation='relu')(x)\r\n    x = Dropout(0.1)(x)\r\n    x = Dense(128, activation='relu')(x)\r\n    return Model(input, x)\r\n\r\n\r\ndef compute_accuracy(y_true, y_pred):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    pred = y_pred.ravel() < 0.5\r\n    return np.mean(pred == y_true)\r\n\r\n\r\ndef accuracy(y_true, y_pred):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\r\n\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\ninput_shape = x_train.shape[1:]\r\n\r\n# create training+test positive and negative pairs\r\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\r\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\r\n\r\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\r\nte_pairs, te_y = create_pairs(x_test, digit_indices)\r\n\r\n\r\n# network definition\r\nbase_network = create_base_network(input_shape)\r\n\r\ninput_a = Input(shape=input_shape, name='input_a')\r\ninput_b = Input(shape=input_shape, name='input_b')\r\n\r\n# because we re-use the same instance `base_network`,\r\n# the weights of the network\r\n# will be shared across the two branches\r\nprocessed_a = base_network(input_a)\r\nprocessed_b = base_network(input_b)\r\n\r\ndistance = Lambda(euclidean_distance,\r\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\r\n\r\nmodel = Model([input_a, input_b], distance)\r\n\r\n# train\r\nrms = RMSprop()\r\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\r\n\r\nmodel.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\r\n           batch_size=128,\r\n           epochs=1,\r\n           validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\r\n\r\nestimata = tf.keras.estimator.model_to_estimator(keras_model=model)\r\n\r\n\r\ninput_name1 = model.input_names[0]\r\ninput_name2 = model.input_names[1]\r\n\r\n\r\n\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={input_name1: tr_pairs[:,0],\r\n           input_name2: tr_pairs[:,1]},\r\n        y=tr_y,\r\n        batch_size=128,\r\n        num_epochs=1,\r\n        shuffle=True)\r\n#\r\nestimata.train(\r\n    input_fn=train_input_fn,\r\n    steps=None)  # ,\r\n\r\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={input_name1: te_pairs[:, 0],\r\n       input_name2: te_pairs[:, 1]},\r\n    y=te_y,\r\n    num_epochs=1,\r\n    shuffle=False)\r\n\r\neval_results = estimata.evaluate(input_fn=eval_input_fn)\r\n\r\n# compute final accuracy on training and test sets\r\ny_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\r\ntr_acc = compute_accuracy(tr_y, y_pred)\r\n\r\ny_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\r\nte_acc = compute_accuracy(te_y, y_pred)\r\n\r\nprint('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\r\nprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\r\n```"}