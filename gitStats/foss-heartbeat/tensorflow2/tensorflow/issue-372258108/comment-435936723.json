{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435936723", "html_url": "https://github.com/tensorflow/tensorflow/issues/23133#issuecomment-435936723", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23133", "id": 435936723, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTkzNjcyMw==", "user": {"login": "imranparuk", "id": 10554727, "node_id": "MDQ6VXNlcjEwNTU0NzI3", "avatar_url": "https://avatars3.githubusercontent.com/u/10554727?v=4", "gravatar_id": "", "url": "https://api.github.com/users/imranparuk", "html_url": "https://github.com/imranparuk", "followers_url": "https://api.github.com/users/imranparuk/followers", "following_url": "https://api.github.com/users/imranparuk/following{/other_user}", "gists_url": "https://api.github.com/users/imranparuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/imranparuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/imranparuk/subscriptions", "organizations_url": "https://api.github.com/users/imranparuk/orgs", "repos_url": "https://api.github.com/users/imranparuk/repos", "events_url": "https://api.github.com/users/imranparuk/events{/privacy}", "received_events_url": "https://api.github.com/users/imranparuk/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-05T16:21:42Z", "updated_at": "2018-11-05T16:21:42Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42785337\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wt-huang\">@wt-huang</a> thank you for your reply and for pointing out that error. I have re-run the code and I have noticed some strange errors that might be a good clue.</p>\n<pre><code>INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/tmpqco1ytuk/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\nINFO:tensorflow:Warm-starting from: ('/tmp/tmpqco1ytuk/keras/keras_model.ckpt',)\nINFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: RMSprop/lr; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: RMSprop/rho; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: RMSprop/decay; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: RMSprop/iterations; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_1; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_2; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_3; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_4; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_5; prev_var_name: Unchanged\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\n</code></pre>\n<p>I have updated the code as I noticed in my original script, the code is being trained in keras, trained again as an estimator and then evaluated. In my current code, its just being trained as an estimator. It results in the original poor accuracy.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">import</span> random\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">from</span> tensorflow.python <span class=\"pl-k\">import</span> keras\n<span class=\"pl-k\">from</span> keras.models <span class=\"pl-k\">import</span> Sequential\n<span class=\"pl-k\">from</span> tensorflow.python.keras.datasets <span class=\"pl-k\">import</span> mnist\n\n<span class=\"pl-k\">from</span> tensorflow.python.keras.models <span class=\"pl-k\">import</span> Model\n<span class=\"pl-k\">from</span> tensorflow.python.keras.layers <span class=\"pl-k\">import</span> Input, Flatten, Dense, Dropout, Lambda\n\n<span class=\"pl-k\">from</span> tensorflow.python.keras.optimizers <span class=\"pl-k\">import</span> RMSprop\n<span class=\"pl-k\">from</span> tensorflow.python.keras <span class=\"pl-k\">import</span> backend <span class=\"pl-k\">as</span> K\n\nnum_classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nepochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\ntf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">euclidean_distance</span>(<span class=\"pl-smi\">vects</span>):\n    x, y <span class=\"pl-k\">=</span> vects\n    sum_square <span class=\"pl-k\">=</span> K.sum(K.square(x <span class=\"pl-k\">-</span> y), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">keepdims</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">return</span> K.sqrt(K.maximum(sum_square, K.epsilon()))\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">eucl_dist_output_shape</span>(<span class=\"pl-smi\">shapes</span>):\n    shape1, shape2 <span class=\"pl-k\">=</span> shapes\n    <span class=\"pl-k\">return</span> (shape1[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">contrastive_loss</span>(<span class=\"pl-smi\">y_true</span>, <span class=\"pl-smi\">y_pred</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Contrastive loss from Hadsell-et-al.'06</span>\n<span class=\"pl-s\">    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    margin <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n    sqaure_pred <span class=\"pl-k\">=</span> K.square(y_pred)\n    margin_square <span class=\"pl-k\">=</span> K.square(K.maximum(margin <span class=\"pl-k\">-</span> y_pred, <span class=\"pl-c1\">0</span>))\n    <span class=\"pl-k\">return</span> K.mean(y_true <span class=\"pl-k\">*</span> sqaure_pred <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> y_true) <span class=\"pl-k\">*</span> margin_square)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_pairs</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">digit_indices</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Positive and negative pair creation.</span>\n<span class=\"pl-s\">    Alternates between positive and negative pairs.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    pairs <span class=\"pl-k\">=</span> []\n    labels <span class=\"pl-k\">=</span> []\n    n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">min</span>([<span class=\"pl-c1\">len</span>(digit_indices[d]) <span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_classes)]) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_classes):\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n):\n            z1, z2 <span class=\"pl-k\">=</span> digit_indices[d][i], digit_indices[d][i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>]\n            pairs <span class=\"pl-k\">+=</span> [[x[z1], x[z2]]]\n            inc <span class=\"pl-k\">=</span> random.randrange(<span class=\"pl-c1\">1</span>, num_classes)\n            dn <span class=\"pl-k\">=</span> (d <span class=\"pl-k\">+</span> inc) <span class=\"pl-k\">%</span> num_classes\n            z1, z2 <span class=\"pl-k\">=</span> digit_indices[d][i], digit_indices[dn][i]\n            pairs <span class=\"pl-k\">+=</span> [[x[z1], x[z2]]]\n            labels <span class=\"pl-k\">+=</span> [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>]\n    <span class=\"pl-k\">return</span> np.array(pairs), np.array(labels)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_base_network</span>(<span class=\"pl-smi\">input_shape</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Base network to be shared (eq. to feature extraction).</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n\n    <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>input_shape)\n    x <span class=\"pl-k\">=</span> Flatten()(<span class=\"pl-c1\">input</span>)\n    x <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">128</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\n    x <span class=\"pl-k\">=</span> Dropout(<span class=\"pl-c1\">0.1</span>)(x)\n    x <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">128</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\n    x <span class=\"pl-k\">=</span> Dropout(<span class=\"pl-c1\">0.1</span>)(x)\n    x <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">128</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\n    <span class=\"pl-k\">return</span> Model(<span class=\"pl-c1\">input</span>, x)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">compute_accuracy</span>(<span class=\"pl-smi\">y_true</span>, <span class=\"pl-smi\">y_pred</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Compute classification accuracy with a fixed threshold on distances.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    pred <span class=\"pl-k\">=</span> y_pred.ravel() <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0.5</span>\n    <span class=\"pl-k\">return</span> np.mean(pred <span class=\"pl-k\">==</span> y_true)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">accuracy</span>(<span class=\"pl-smi\">y_true</span>, <span class=\"pl-smi\">y_pred</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Compute classification accuracy with a fixed threshold on distances.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    <span class=\"pl-k\">return</span> K.mean(K.equal(y_true, K.cast(y_pred <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0.5</span>, y_true.dtype)))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the data, split between train and test sets</span>\n(x_train, y_train), (x_test, y_test) <span class=\"pl-k\">=</span> mnist.load_data()\nx_train <span class=\"pl-k\">=</span> x_train.astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>)\nx_test <span class=\"pl-k\">=</span> x_test.astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>)\nx_train <span class=\"pl-k\">/=</span> <span class=\"pl-c1\">255</span>\nx_test <span class=\"pl-k\">/=</span> <span class=\"pl-c1\">255</span>\ninput_shape <span class=\"pl-k\">=</span> x_train.shape[<span class=\"pl-c1\">1</span>:]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> create training+test positive and negative pairs</span>\ndigit_indices <span class=\"pl-k\">=</span> [np.where(y_train <span class=\"pl-k\">==</span> i)[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_classes)]\ntr_pairs, tr_y <span class=\"pl-k\">=</span> create_pairs(x_train, digit_indices)\n\ndigit_indices <span class=\"pl-k\">=</span> [np.where(y_test <span class=\"pl-k\">==</span> i)[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_classes)]\nte_pairs, te_y <span class=\"pl-k\">=</span> create_pairs(x_test, digit_indices)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> network definition</span>\nbase_network <span class=\"pl-k\">=</span> create_base_network(input_shape)\n\ninput_a <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>input_shape, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_a<span class=\"pl-pds\">'</span></span>)\ninput_b <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>input_shape, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_b<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> because we re-use the same instance `base_network`,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the weights of the network</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> will be shared across the two branches</span>\nprocessed_a <span class=\"pl-k\">=</span> base_network(input_a)\nprocessed_b <span class=\"pl-k\">=</span> base_network(input_b)\n\ndistance <span class=\"pl-k\">=</span> Lambda(euclidean_distance,\n                  <span class=\"pl-v\">output_shape</span><span class=\"pl-k\">=</span>eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel <span class=\"pl-k\">=</span> Model([input_a, input_b], distance)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> train</span>\nrms <span class=\"pl-k\">=</span> RMSprop()\nmodel.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>contrastive_loss, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>rms, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[accuracy])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>            batch_size=128,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>            epochs=1)</span>\n\nestimata <span class=\"pl-k\">=</span> tf.keras.estimator.model_to_estimator(<span class=\"pl-v\">keras_model</span><span class=\"pl-k\">=</span>model)\n\ninput_name1 <span class=\"pl-k\">=</span> model.input_names[<span class=\"pl-c1\">0</span>]\ninput_name2 <span class=\"pl-k\">=</span> model.input_names[<span class=\"pl-c1\">1</span>]\n\ntrain_input_fn <span class=\"pl-k\">=</span> tf.estimator.inputs.numpy_input_fn(\n        <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{input_name1: tr_pairs[:,<span class=\"pl-c1\">0</span>],\n           input_name2: tr_pairs[:,<span class=\"pl-c1\">1</span>]},\n        <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>tr_y,\n        <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>,\n        <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n        <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\nestimata.train(\n    <span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>train_input_fn,\n    <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ,</span>\n\neval_input_fn <span class=\"pl-k\">=</span> tf.estimator.inputs.numpy_input_fn(\n    <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{input_name1: te_pairs[:, <span class=\"pl-c1\">0</span>],\n       input_name2: te_pairs[:, <span class=\"pl-c1\">1</span>]},\n    <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>te_y,\n    <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n    <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\neval_results <span class=\"pl-k\">=</span> estimata.evaluate(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>eval_input_fn)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> compute final accuracy on training and test sets</span>\ny_pred <span class=\"pl-k\">=</span> model.predict([tr_pairs[:, <span class=\"pl-c1\">0</span>], tr_pairs[:, <span class=\"pl-c1\">1</span>]])\ntr_acc <span class=\"pl-k\">=</span> compute_accuracy(tr_y, y_pred)\n\ny_pred <span class=\"pl-k\">=</span> model.predict([te_pairs[:, <span class=\"pl-c1\">0</span>], te_pairs[:, <span class=\"pl-c1\">1</span>]])\nte_acc <span class=\"pl-k\">=</span> compute_accuracy(te_y, y_pred)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>* Accuracy on training set: <span class=\"pl-c1\">%0.2f%%</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (<span class=\"pl-c1\">100</span> <span class=\"pl-k\">*</span> tr_acc))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>* Accuracy on test set: <span class=\"pl-c1\">%0.2f%%</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (<span class=\"pl-c1\">100</span> <span class=\"pl-k\">*</span> te_acc))</pre></div>\n<p>run this code and see if it works your side?</p>", "body_text": "@wt-huang thank you for your reply and for pointing out that error. I have re-run the code and I have noticed some strange errors that might be a good clue.\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/tmpqco1ytuk/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\nINFO:tensorflow:Warm-starting from: ('/tmp/tmpqco1ytuk/keras/keras_model.ckpt',)\nINFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: RMSprop/lr; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: RMSprop/rho; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: RMSprop/decay; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: RMSprop/iterations; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_1; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_2; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_3; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_4; prev_var_name: Unchanged\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_5; prev_var_name: Unchanged\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\n\nI have updated the code as I noticed in my original script, the code is being trained in keras, trained again as an estimator and then evaluated. In my current code, its just being trained as an estimator. It results in the original poor accuracy.\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\n\nimport random\nimport tensorflow as tf\n\nfrom tensorflow.python import keras\nfrom keras.models import Sequential\nfrom tensorflow.python.keras.datasets import mnist\n\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import Input, Flatten, Dense, Dropout, Lambda\n\nfrom tensorflow.python.keras.optimizers import RMSprop\nfrom tensorflow.python.keras import backend as K\n\nnum_classes = 10\nepochs = 20\ntf.logging.set_verbosity(tf.logging.INFO)\n\ndef euclidean_distance(vects):\n    x, y = vects\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\n\ndef contrastive_loss(y_true, y_pred):\n    '''Contrastive loss from Hadsell-et-al.'06\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    '''\n    margin = 1\n    sqaure_pred = K.square(y_pred)\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n\ndef create_pairs(x, digit_indices):\n    '''Positive and negative pair creation.\n    Alternates between positive and negative pairs.\n    '''\n    pairs = []\n    labels = []\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n    for d in range(num_classes):\n        for i in range(n):\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n            pairs += [[x[z1], x[z2]]]\n            inc = random.randrange(1, num_classes)\n            dn = (d + inc) % num_classes\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n            pairs += [[x[z1], x[z2]]]\n            labels += [1, 0]\n    return np.array(pairs), np.array(labels)\n\ndef create_base_network(input_shape):\n    '''Base network to be shared (eq. to feature extraction).\n    '''\n\n    input = Input(shape=input_shape)\n    x = Flatten()(input)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    return Model(input, x)\n\ndef compute_accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    pred = y_pred.ravel() < 0.5\n    return np.mean(pred == y_true)\n\n\ndef accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\ninput_shape = x_train.shape[1:]\n\n# create training+test positive and negative pairs\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\n\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\nte_pairs, te_y = create_pairs(x_test, digit_indices)\n\n# network definition\nbase_network = create_base_network(input_shape)\n\ninput_a = Input(shape=input_shape, name='input_a')\ninput_b = Input(shape=input_shape, name='input_b')\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a = base_network(input_a)\nprocessed_b = base_network(input_b)\n\ndistance = Lambda(euclidean_distance,\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel = Model([input_a, input_b], distance)\n\n# train\nrms = RMSprop()\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n\n# model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n#            batch_size=128,\n#            epochs=1)\n\nestimata = tf.keras.estimator.model_to_estimator(keras_model=model)\n\ninput_name1 = model.input_names[0]\ninput_name2 = model.input_names[1]\n\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={input_name1: tr_pairs[:,0],\n           input_name2: tr_pairs[:,1]},\n        y=tr_y,\n        batch_size=128,\n        num_epochs=1,\n        shuffle=True)\n#\nestimata.train(\n    input_fn=train_input_fn,\n    steps=None)  # ,\n\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={input_name1: te_pairs[:, 0],\n       input_name2: te_pairs[:, 1]},\n    y=te_y,\n    num_epochs=1,\n    shuffle=False)\n\neval_results = estimata.evaluate(input_fn=eval_input_fn)\n\n# compute final accuracy on training and test sets\ny_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\ntr_acc = compute_accuracy(tr_y, y_pred)\n\ny_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\nte_acc = compute_accuracy(te_y, y_pred)\n\nprint('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\nprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\nrun this code and see if it works your side?", "body": "@wt-huang thank you for your reply and for pointing out that error. I have re-run the code and I have noticed some strange errors that might be a good clue.\r\n\r\n```\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/tmpqco1ytuk/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\nINFO:tensorflow:Warm-starting from: ('/tmp/tmpqco1ytuk/keras/keras_model.ckpt',)\r\nINFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: RMSprop/lr; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: RMSprop/rho; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: RMSprop/decay; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: RMSprop/iterations; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_2; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_3; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_4; prev_var_name: Unchanged\r\nINFO:tensorflow:Warm-starting variable: training/RMSprop/Variable_5; prev_var_name: Unchanged\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n```\r\n\r\nI have updated the code as I noticed in my original script, the code is being trained in keras, trained again as an estimator and then evaluated. In my current code, its just being trained as an estimator. It results in the original poor accuracy. \r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import print_function\r\nimport numpy as np\r\n\r\nimport random\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python import keras\r\nfrom keras.models import Sequential\r\nfrom tensorflow.python.keras.datasets import mnist\r\n\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import Input, Flatten, Dense, Dropout, Lambda\r\n\r\nfrom tensorflow.python.keras.optimizers import RMSprop\r\nfrom tensorflow.python.keras import backend as K\r\n\r\nnum_classes = 10\r\nepochs = 20\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ndef euclidean_distance(vects):\r\n    x, y = vects\r\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\r\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\r\n\r\ndef eucl_dist_output_shape(shapes):\r\n    shape1, shape2 = shapes\r\n    return (shape1[0], 1)\r\n\r\ndef contrastive_loss(y_true, y_pred):\r\n    '''Contrastive loss from Hadsell-et-al.'06\r\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\r\n    '''\r\n    margin = 1\r\n    sqaure_pred = K.square(y_pred)\r\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\r\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\r\n\r\ndef create_pairs(x, digit_indices):\r\n    '''Positive and negative pair creation.\r\n    Alternates between positive and negative pairs.\r\n    '''\r\n    pairs = []\r\n    labels = []\r\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\r\n    for d in range(num_classes):\r\n        for i in range(n):\r\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\r\n            pairs += [[x[z1], x[z2]]]\r\n            inc = random.randrange(1, num_classes)\r\n            dn = (d + inc) % num_classes\r\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\r\n            pairs += [[x[z1], x[z2]]]\r\n            labels += [1, 0]\r\n    return np.array(pairs), np.array(labels)\r\n\r\ndef create_base_network(input_shape):\r\n    '''Base network to be shared (eq. to feature extraction).\r\n    '''\r\n\r\n    input = Input(shape=input_shape)\r\n    x = Flatten()(input)\r\n    x = Dense(128, activation='relu')(x)\r\n    x = Dropout(0.1)(x)\r\n    x = Dense(128, activation='relu')(x)\r\n    x = Dropout(0.1)(x)\r\n    x = Dense(128, activation='relu')(x)\r\n    return Model(input, x)\r\n\r\ndef compute_accuracy(y_true, y_pred):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    pred = y_pred.ravel() < 0.5\r\n    return np.mean(pred == y_true)\r\n\r\n\r\ndef accuracy(y_true, y_pred):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\ninput_shape = x_train.shape[1:]\r\n\r\n# create training+test positive and negative pairs\r\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\r\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\r\n\r\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\r\nte_pairs, te_y = create_pairs(x_test, digit_indices)\r\n\r\n# network definition\r\nbase_network = create_base_network(input_shape)\r\n\r\ninput_a = Input(shape=input_shape, name='input_a')\r\ninput_b = Input(shape=input_shape, name='input_b')\r\n\r\n# because we re-use the same instance `base_network`,\r\n# the weights of the network\r\n# will be shared across the two branches\r\nprocessed_a = base_network(input_a)\r\nprocessed_b = base_network(input_b)\r\n\r\ndistance = Lambda(euclidean_distance,\r\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\r\n\r\nmodel = Model([input_a, input_b], distance)\r\n\r\n# train\r\nrms = RMSprop()\r\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\r\n\r\n# model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\r\n#            batch_size=128,\r\n#            epochs=1)\r\n\r\nestimata = tf.keras.estimator.model_to_estimator(keras_model=model)\r\n\r\ninput_name1 = model.input_names[0]\r\ninput_name2 = model.input_names[1]\r\n\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={input_name1: tr_pairs[:,0],\r\n           input_name2: tr_pairs[:,1]},\r\n        y=tr_y,\r\n        batch_size=128,\r\n        num_epochs=1,\r\n        shuffle=True)\r\n#\r\nestimata.train(\r\n    input_fn=train_input_fn,\r\n    steps=None)  # ,\r\n\r\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={input_name1: te_pairs[:, 0],\r\n       input_name2: te_pairs[:, 1]},\r\n    y=te_y,\r\n    num_epochs=1,\r\n    shuffle=False)\r\n\r\neval_results = estimata.evaluate(input_fn=eval_input_fn)\r\n\r\n# compute final accuracy on training and test sets\r\ny_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\r\ntr_acc = compute_accuracy(tr_y, y_pred)\r\n\r\ny_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\r\nte_acc = compute_accuracy(te_y, y_pred)\r\n\r\nprint('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\r\nprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\r\n```\r\n\r\nrun this code and see if it works your side?"}