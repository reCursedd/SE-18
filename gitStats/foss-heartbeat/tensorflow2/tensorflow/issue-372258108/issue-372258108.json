{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23133", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23133/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23133/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23133/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23133", "id": 372258108, "node_id": "MDU6SXNzdWUzNzIyNTgxMDg=", "number": 23133, "title": "Keras to Estimators Siamese Issue", "user": {"login": "imranparuk", "id": 10554727, "node_id": "MDQ6VXNlcjEwNTU0NzI3", "avatar_url": "https://avatars3.githubusercontent.com/u/10554727?v=4", "gravatar_id": "", "url": "https://api.github.com/users/imranparuk", "html_url": "https://github.com/imranparuk", "followers_url": "https://api.github.com/users/imranparuk/followers", "following_url": "https://api.github.com/users/imranparuk/following{/other_user}", "gists_url": "https://api.github.com/users/imranparuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/imranparuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/imranparuk/subscriptions", "organizations_url": "https://api.github.com/users/imranparuk/orgs", "repos_url": "https://api.github.com/users/imranparuk/repos", "events_url": "https://api.github.com/users/imranparuk/events{/privacy}", "received_events_url": "https://api.github.com/users/imranparuk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097546578, "node_id": "MDU6TGFiZWwxMDk3NTQ2NTc4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:keras", "name": "comp:keras", "color": "0052cc", "default": false}, {"id": 1105108936, "node_id": "MDU6TGFiZWwxMTA1MTA4OTM2", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:model", "name": "comp:model", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-10-20T22:18:46Z", "updated_at": "2018-11-19T07:48:48Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi there, I wounder if anyone could assist me. Forgive me if this isn't the correct place for this or it is not in the perfect format...</p>\n<p>I am trying to convert the Keras Siamese example into an estimator, however it does not seem to be working...</p>\n<ul>\n<li>System information<br>\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04<br>\nTensorFlow installed from (source or binary):<br>\npip3 install --upgrade tensorflow-gpu==1.12.0-rc1<br>\nTensorFlow version: 1.12.0-rc1<br>\nPython version: Python 3.6.5<br>\nInstalled using virtualenv? pip? conda?: pip3<br>\nCUDA/cuDNN version: 9.0 / 7.0.5<br>\nGPU model and memory: nvidia gtx 1050 (Lenovo Laptop)</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Trains a Siamese MLP on pairs of digits from the MNIST dataset.</span>\n<span class=\"pl-s\">It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the</span>\n<span class=\"pl-s\">output of the shared network and by optimizing the contrastive loss (see paper</span>\n<span class=\"pl-s\">for mode details).</span>\n<span class=\"pl-s\"># References</span>\n<span class=\"pl-s\">- Dimensionality Reduction by Learning an Invariant Mapping</span>\n<span class=\"pl-s\">    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf</span>\n<span class=\"pl-s\">Gets to 97.2% test accuracy after 20 epochs.</span>\n<span class=\"pl-s\">2 seconds per epoch on a Titan X Maxwell GPU</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">import</span> random\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>from tf.keras.models import Sequential  # This does not work!</span>\n<span class=\"pl-k\">from</span> tensorflow.python.keras.datasets <span class=\"pl-k\">import</span> mnist\n\n<span class=\"pl-k\">from</span> tensorflow.python.keras.models <span class=\"pl-k\">import</span> Model\n<span class=\"pl-k\">from</span> tensorflow.python.keras.layers <span class=\"pl-k\">import</span> Input, Flatten, Dense, Dropout, Lambda\n<span class=\"pl-k\">from</span> tensorflow.python.keras.callbacks <span class=\"pl-k\">import</span> ModelCheckpoint, Callback\n<span class=\"pl-k\">from</span> tensorflow.python.keras.utils <span class=\"pl-k\">import</span> HDF5Matrix, to_categorical\n<span class=\"pl-k\">from</span> tensorflow.python.keras.losses <span class=\"pl-k\">import</span> categorical_crossentropy\n<span class=\"pl-k\">from</span> tensorflow.python.keras.optimizers <span class=\"pl-k\">import</span> RMSprop\n<span class=\"pl-k\">from</span> tensorflow.python.keras <span class=\"pl-k\">import</span> backend <span class=\"pl-k\">as</span> K\n\nnum_classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nepochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\ntf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">euclidean_distance</span>(<span class=\"pl-smi\">vects</span>):\n    x, y <span class=\"pl-k\">=</span> vects\n    sum_square <span class=\"pl-k\">=</span> K.sum(K.square(x <span class=\"pl-k\">-</span> y), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">keepdims</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">return</span> K.sqrt(K.maximum(sum_square, K.epsilon()))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">eucl_dist_output_shape</span>(<span class=\"pl-smi\">shapes</span>):\n    shape1, shape2 <span class=\"pl-k\">=</span> shapes\n    <span class=\"pl-k\">return</span> (shape1[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">1</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">contrastive_loss</span>(<span class=\"pl-smi\">y_true</span>, <span class=\"pl-smi\">y_pred</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Contrastive loss from Hadsell-et-al.'06</span>\n<span class=\"pl-s\">    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    margin <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n    sqaure_pred <span class=\"pl-k\">=</span> K.square(y_pred)\n    margin_square <span class=\"pl-k\">=</span> K.square(K.maximum(margin <span class=\"pl-k\">-</span> y_pred, <span class=\"pl-c1\">0</span>))\n    <span class=\"pl-k\">return</span> K.mean(y_true <span class=\"pl-k\">*</span> sqaure_pred <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> y_true) <span class=\"pl-k\">*</span> margin_square)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_pairs</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">digit_indices</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Positive and negative pair creation.</span>\n<span class=\"pl-s\">    Alternates between positive and negative pairs.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    pairs <span class=\"pl-k\">=</span> []\n    labels <span class=\"pl-k\">=</span> []\n    n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">min</span>([<span class=\"pl-c1\">len</span>(digit_indices[d]) <span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_classes)]) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_classes):\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n):\n            z1, z2 <span class=\"pl-k\">=</span> digit_indices[d][i], digit_indices[d][i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>]\n            pairs <span class=\"pl-k\">+=</span> [[x[z1], x[z2]]]\n            inc <span class=\"pl-k\">=</span> random.randrange(<span class=\"pl-c1\">1</span>, num_classes)\n            dn <span class=\"pl-k\">=</span> (d <span class=\"pl-k\">+</span> inc) <span class=\"pl-k\">%</span> num_classes\n            z1, z2 <span class=\"pl-k\">=</span> digit_indices[d][i], digit_indices[dn][i]\n            pairs <span class=\"pl-k\">+=</span> [[x[z1], x[z2]]]\n            labels <span class=\"pl-k\">+=</span> [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>]\n    <span class=\"pl-k\">return</span> np.array(pairs), np.array(labels)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_base_network</span>(<span class=\"pl-smi\">input_shape</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Base network to be shared (eq. to feature extraction).</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n\n    <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>input_shape)\n    x <span class=\"pl-k\">=</span> Flatten()(<span class=\"pl-c1\">input</span>)\n    x <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">128</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\n    x <span class=\"pl-k\">=</span> Dropout(<span class=\"pl-c1\">0.1</span>)(x)\n    x <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">128</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\n    x <span class=\"pl-k\">=</span> Dropout(<span class=\"pl-c1\">0.1</span>)(x)\n    x <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">128</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\n    <span class=\"pl-k\">return</span> Model(<span class=\"pl-c1\">input</span>, x)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">compute_accuracy</span>(<span class=\"pl-smi\">y_true</span>, <span class=\"pl-smi\">y_pred</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Compute classification accuracy with a fixed threshold on distances.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    pred <span class=\"pl-k\">=</span> y_pred.ravel() <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0.5</span>\n    <span class=\"pl-k\">return</span> np.mean(pred <span class=\"pl-k\">==</span> y_true)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">accuracy</span>(<span class=\"pl-smi\">y_true</span>, <span class=\"pl-smi\">y_pred</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Compute classification accuracy with a fixed threshold on distances.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    <span class=\"pl-k\">return</span> K.mean(K.equal(y_true, K.cast(y_pred <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0.5</span>, y_true.dtype)))\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the data, split between train and test sets</span>\n(x_train, y_train), (x_test, y_test) <span class=\"pl-k\">=</span> mnist.load_data()\nx_train <span class=\"pl-k\">=</span> x_train.astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>)\nx_test <span class=\"pl-k\">=</span> x_test.astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>)\nx_train <span class=\"pl-k\">/=</span> <span class=\"pl-c1\">255</span>\nx_test <span class=\"pl-k\">/=</span> <span class=\"pl-c1\">255</span>\ninput_shape <span class=\"pl-k\">=</span> x_train.shape[<span class=\"pl-c1\">1</span>:]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> create training+test positive and negative pairs</span>\ndigit_indices <span class=\"pl-k\">=</span> [np.where(y_train <span class=\"pl-k\">==</span> i)[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_classes)]\ntr_pairs, tr_y <span class=\"pl-k\">=</span> create_pairs(x_train, digit_indices)\n\ndigit_indices <span class=\"pl-k\">=</span> [np.where(y_test <span class=\"pl-k\">==</span> i)[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_classes)]\nte_pairs, te_y <span class=\"pl-k\">=</span> create_pairs(x_test, digit_indices)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> network definition</span>\nbase_network <span class=\"pl-k\">=</span> create_base_network(input_shape)\n\ninput_a <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>input_shape, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_a<span class=\"pl-pds\">'</span></span>)\ninput_b <span class=\"pl-k\">=</span> Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>input_shape, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_b<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> because we re-use the same instance `base_network`,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the weights of the network</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> will be shared across the two branches</span>\nprocessed_a <span class=\"pl-k\">=</span> base_network(input_a)\nprocessed_b <span class=\"pl-k\">=</span> base_network(input_b)\n\ndistance <span class=\"pl-k\">=</span> Lambda(euclidean_distance,\n                  <span class=\"pl-v\">output_shape</span><span class=\"pl-k\">=</span>eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel <span class=\"pl-k\">=</span> Model([input_a, input_b], distance)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> train</span>\nrms <span class=\"pl-k\">=</span> RMSprop()\nmodel.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>contrastive_loss, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>rms, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[accuracy])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>           batch_size=128,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>           epochs=1,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>           validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))</span>\n\nestimata <span class=\"pl-k\">=</span> tf.keras.estimator.model_to_estimator(<span class=\"pl-v\">keras_model</span><span class=\"pl-k\">=</span>model)\n\n\ninput_name1 <span class=\"pl-k\">=</span> model.input_names[<span class=\"pl-c1\">0</span>]\ninput_name2 <span class=\"pl-k\">=</span> model.input_names[<span class=\"pl-c1\">1</span>]\n\n\n\ntrain_input_fn <span class=\"pl-k\">=</span> tf.estimator.inputs.numpy_input_fn(\n        <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{input_name1: tr_pairs[:,<span class=\"pl-c1\">0</span>],\n           input_name2: tr_pairs[:,<span class=\"pl-c1\">1</span>]},\n        <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>tr_y,\n        <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>,\n        <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n        <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\nestimata.train(\n    <span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>train_input_fn,\n    <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ,</span>\n\neval_input_fn <span class=\"pl-k\">=</span> tf.estimator.inputs.numpy_input_fn(\n    <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>{input_name1: te_pairs[:, <span class=\"pl-c1\">0</span>],\n       input_name2: te_pairs[:, <span class=\"pl-c1\">1</span>]},\n    <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span>te_y,\n    <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n    <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\neval_results <span class=\"pl-k\">=</span> estimata.evaluate(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>eval_input_fn)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> compute final accuracy on training and test sets</span>\ny_pred <span class=\"pl-k\">=</span> model.predict([tr_pairs[:, <span class=\"pl-c1\">0</span>], tr_pairs[:, <span class=\"pl-c1\">1</span>]])\ntr_acc <span class=\"pl-k\">=</span> compute_accuracy(tr_y, y_pred)\n\ny_pred <span class=\"pl-k\">=</span> model.predict([te_pairs[:, <span class=\"pl-c1\">0</span>], te_pairs[:, <span class=\"pl-c1\">1</span>]])\nte_acc <span class=\"pl-k\">=</span> compute_accuracy(te_y, y_pred)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>* Accuracy on training set: <span class=\"pl-c1\">%0.2f%%</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (<span class=\"pl-c1\">100</span> <span class=\"pl-k\">*</span> tr_acc))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>* Accuracy on test set: <span class=\"pl-c1\">%0.2f%%</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (<span class=\"pl-c1\">100</span> <span class=\"pl-k\">*</span> te_acc))</pre></div>\n<p>With Keras model.fit(), this yields:</p>\n<pre><code>Accuracy on training set: 95.70%\nAccuracy on test set: 95.33%\n</code></pre>\n<p>However With Estimators tf.estimator.train() the results are much worse:</p>\n<pre><code>Accuracy on training set: 53.11%\nAccuracy on test set: 53.80%\n</code></pre>", "body_text": "Hi there, I wounder if anyone could assist me. Forgive me if this isn't the correct place for this or it is not in the perfect format...\nI am trying to convert the Keras Siamese example into an estimator, however it does not seem to be working...\n\nSystem information\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\nTensorFlow installed from (source or binary):\npip3 install --upgrade tensorflow-gpu==1.12.0-rc1\nTensorFlow version: 1.12.0-rc1\nPython version: Python 3.6.5\nInstalled using virtualenv? pip? conda?: pip3\nCUDA/cuDNN version: 9.0 / 7.0.5\nGPU model and memory: nvidia gtx 1050 (Lenovo Laptop)\n\n'''Trains a Siamese MLP on pairs of digits from the MNIST dataset.\nIt follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\noutput of the shared network and by optimizing the contrastive loss (see paper\nfor mode details).\n# References\n- Dimensionality Reduction by Learning an Invariant Mapping\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\nGets to 97.2% test accuracy after 20 epochs.\n2 seconds per epoch on a Titan X Maxwell GPU\n'''\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\n\nimport random\nimport tensorflow as tf\n#from tf.keras.models import Sequential  # This does not work!\nfrom tensorflow.python.keras.datasets import mnist\n\nfrom tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.layers import Input, Flatten, Dense, Dropout, Lambda\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint, Callback\nfrom tensorflow.python.keras.utils import HDF5Matrix, to_categorical\nfrom tensorflow.python.keras.losses import categorical_crossentropy\nfrom tensorflow.python.keras.optimizers import RMSprop\nfrom tensorflow.python.keras import backend as K\n\nnum_classes = 10\nepochs = 20\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef euclidean_distance(vects):\n    x, y = vects\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\n\n\ndef contrastive_loss(y_true, y_pred):\n    '''Contrastive loss from Hadsell-et-al.'06\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    '''\n    margin = 1\n    sqaure_pred = K.square(y_pred)\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n\n\ndef create_pairs(x, digit_indices):\n    '''Positive and negative pair creation.\n    Alternates between positive and negative pairs.\n    '''\n    pairs = []\n    labels = []\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n    for d in range(num_classes):\n        for i in range(n):\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n            pairs += [[x[z1], x[z2]]]\n            inc = random.randrange(1, num_classes)\n            dn = (d + inc) % num_classes\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n            pairs += [[x[z1], x[z2]]]\n            labels += [1, 0]\n    return np.array(pairs), np.array(labels)\n\n\ndef create_base_network(input_shape):\n    '''Base network to be shared (eq. to feature extraction).\n    '''\n\n    input = Input(shape=input_shape)\n    x = Flatten()(input)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    return Model(input, x)\n\n\ndef compute_accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    pred = y_pred.ravel() < 0.5\n    return np.mean(pred == y_true)\n\n\ndef accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\ninput_shape = x_train.shape[1:]\n\n# create training+test positive and negative pairs\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\n\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\nte_pairs, te_y = create_pairs(x_test, digit_indices)\n\n\n# network definition\nbase_network = create_base_network(input_shape)\n\ninput_a = Input(shape=input_shape, name='input_a')\ninput_b = Input(shape=input_shape, name='input_b')\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a = base_network(input_a)\nprocessed_b = base_network(input_b)\n\ndistance = Lambda(euclidean_distance,\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel = Model([input_a, input_b], distance)\n\n# train\nrms = RMSprop()\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n\n# model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n#           batch_size=128,\n#           epochs=1,\n#           validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n\nestimata = tf.keras.estimator.model_to_estimator(keras_model=model)\n\n\ninput_name1 = model.input_names[0]\ninput_name2 = model.input_names[1]\n\n\n\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={input_name1: tr_pairs[:,0],\n           input_name2: tr_pairs[:,1]},\n        y=tr_y,\n        batch_size=128,\n        num_epochs=1,\n        shuffle=True)\n#\nestimata.train(\n    input_fn=train_input_fn,\n    steps=None)  # ,\n\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={input_name1: te_pairs[:, 0],\n       input_name2: te_pairs[:, 1]},\n    y=te_y,\n    num_epochs=1,\n    shuffle=False)\n\neval_results = estimata.evaluate(input_fn=eval_input_fn)\n\n# compute final accuracy on training and test sets\ny_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\ntr_acc = compute_accuracy(tr_y, y_pred)\n\ny_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\nte_acc = compute_accuracy(te_y, y_pred)\n\nprint('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\nprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\nWith Keras model.fit(), this yields:\nAccuracy on training set: 95.70%\nAccuracy on test set: 95.33%\n\nHowever With Estimators tf.estimator.train() the results are much worse:\nAccuracy on training set: 53.11%\nAccuracy on test set: 53.80%", "body": "Hi there, I wounder if anyone could assist me. Forgive me if this isn't the correct place for this or it is not in the perfect format...\r\n\r\nI am trying to convert the Keras Siamese example into an estimator, however it does not seem to be working...\r\n\r\n* System information\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n    TensorFlow installed from (source or binary):\r\n    pip3 install --upgrade tensorflow-gpu==1.12.0-rc1\r\n    TensorFlow version: 1.12.0-rc1\r\n    Python version: Python 3.6.5 \r\n    Installed using virtualenv? pip? conda?: pip3\r\n    CUDA/cuDNN version: 9.0 / 7.0.5\r\n    GPU model and memory: nvidia gtx 1050 (Lenovo Laptop)\r\n\r\n```python\r\n'''Trains a Siamese MLP on pairs of digits from the MNIST dataset.\r\nIt follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\r\noutput of the shared network and by optimizing the contrastive loss (see paper\r\nfor mode details).\r\n# References\r\n- Dimensionality Reduction by Learning an Invariant Mapping\r\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\r\nGets to 97.2% test accuracy after 20 epochs.\r\n2 seconds per epoch on a Titan X Maxwell GPU\r\n'''\r\nfrom __future__ import absolute_import\r\nfrom __future__ import print_function\r\nimport numpy as np\r\n\r\nimport random\r\nimport tensorflow as tf\r\n#from tf.keras.models import Sequential  # This does not work!\r\nfrom tensorflow.python.keras.datasets import mnist\r\n\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import Input, Flatten, Dense, Dropout, Lambda\r\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint, Callback\r\nfrom tensorflow.python.keras.utils import HDF5Matrix, to_categorical\r\nfrom tensorflow.python.keras.losses import categorical_crossentropy\r\nfrom tensorflow.python.keras.optimizers import RMSprop\r\nfrom tensorflow.python.keras import backend as K\r\n\r\nnum_classes = 10\r\nepochs = 20\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n\r\ndef euclidean_distance(vects):\r\n    x, y = vects\r\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\r\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\r\n\r\n\r\ndef eucl_dist_output_shape(shapes):\r\n    shape1, shape2 = shapes\r\n    return (shape1[0], 1)\r\n\r\n\r\ndef contrastive_loss(y_true, y_pred):\r\n    '''Contrastive loss from Hadsell-et-al.'06\r\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\r\n    '''\r\n    margin = 1\r\n    sqaure_pred = K.square(y_pred)\r\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\r\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\r\n\r\n\r\ndef create_pairs(x, digit_indices):\r\n    '''Positive and negative pair creation.\r\n    Alternates between positive and negative pairs.\r\n    '''\r\n    pairs = []\r\n    labels = []\r\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\r\n    for d in range(num_classes):\r\n        for i in range(n):\r\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\r\n            pairs += [[x[z1], x[z2]]]\r\n            inc = random.randrange(1, num_classes)\r\n            dn = (d + inc) % num_classes\r\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\r\n            pairs += [[x[z1], x[z2]]]\r\n            labels += [1, 0]\r\n    return np.array(pairs), np.array(labels)\r\n\r\n\r\ndef create_base_network(input_shape):\r\n    '''Base network to be shared (eq. to feature extraction).\r\n    '''\r\n\r\n    input = Input(shape=input_shape)\r\n    x = Flatten()(input)\r\n    x = Dense(128, activation='relu')(x)\r\n    x = Dropout(0.1)(x)\r\n    x = Dense(128, activation='relu')(x)\r\n    x = Dropout(0.1)(x)\r\n    x = Dense(128, activation='relu')(x)\r\n    return Model(input, x)\r\n\r\n\r\ndef compute_accuracy(y_true, y_pred):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    pred = y_pred.ravel() < 0.5\r\n    return np.mean(pred == y_true)\r\n\r\n\r\ndef accuracy(y_true, y_pred):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\r\n\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\ninput_shape = x_train.shape[1:]\r\n\r\n# create training+test positive and negative pairs\r\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\r\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\r\n\r\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\r\nte_pairs, te_y = create_pairs(x_test, digit_indices)\r\n\r\n\r\n# network definition\r\nbase_network = create_base_network(input_shape)\r\n\r\ninput_a = Input(shape=input_shape, name='input_a')\r\ninput_b = Input(shape=input_shape, name='input_b')\r\n\r\n# because we re-use the same instance `base_network`,\r\n# the weights of the network\r\n# will be shared across the two branches\r\nprocessed_a = base_network(input_a)\r\nprocessed_b = base_network(input_b)\r\n\r\ndistance = Lambda(euclidean_distance,\r\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\r\n\r\nmodel = Model([input_a, input_b], distance)\r\n\r\n# train\r\nrms = RMSprop()\r\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\r\n\r\n# model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\r\n#           batch_size=128,\r\n#           epochs=1,\r\n#           validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\r\n\r\nestimata = tf.keras.estimator.model_to_estimator(keras_model=model)\r\n\r\n\r\ninput_name1 = model.input_names[0]\r\ninput_name2 = model.input_names[1]\r\n\r\n\r\n\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={input_name1: tr_pairs[:,0],\r\n           input_name2: tr_pairs[:,1]},\r\n        y=tr_y,\r\n        batch_size=128,\r\n        num_epochs=1,\r\n        shuffle=True)\r\n#\r\nestimata.train(\r\n    input_fn=train_input_fn,\r\n    steps=None)  # ,\r\n\r\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={input_name1: te_pairs[:, 0],\r\n       input_name2: te_pairs[:, 1]},\r\n    y=te_y,\r\n    num_epochs=1,\r\n    shuffle=False)\r\n\r\neval_results = estimata.evaluate(input_fn=eval_input_fn)\r\n\r\n# compute final accuracy on training and test sets\r\ny_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\r\ntr_acc = compute_accuracy(tr_y, y_pred)\r\n\r\ny_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\r\nte_acc = compute_accuracy(te_y, y_pred)\r\n\r\nprint('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\r\nprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\r\n```\r\n\r\nWith Keras model.fit(), this yields:\r\n```\r\nAccuracy on training set: 95.70%\r\nAccuracy on test set: 95.33%\r\n```\r\n\r\nHowever With Estimators tf.estimator.train() the results are much worse:\r\n```\r\nAccuracy on training set: 53.11%\r\nAccuracy on test set: 53.80%\r\n```"}