{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273647478", "html_url": "https://github.com/tensorflow/tensorflow/issues/6845#issuecomment-273647478", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6845", "id": 273647478, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzY0NzQ3OA==", "user": {"login": "erik-mileiq", "id": 16887670, "node_id": "MDQ6VXNlcjE2ODg3Njcw", "avatar_url": "https://avatars3.githubusercontent.com/u/16887670?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erik-mileiq", "html_url": "https://github.com/erik-mileiq", "followers_url": "https://api.github.com/users/erik-mileiq/followers", "following_url": "https://api.github.com/users/erik-mileiq/following{/other_user}", "gists_url": "https://api.github.com/users/erik-mileiq/gists{/gist_id}", "starred_url": "https://api.github.com/users/erik-mileiq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erik-mileiq/subscriptions", "organizations_url": "https://api.github.com/users/erik-mileiq/orgs", "repos_url": "https://api.github.com/users/erik-mileiq/repos", "events_url": "https://api.github.com/users/erik-mileiq/events{/privacy}", "received_events_url": "https://api.github.com/users/erik-mileiq/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-19T00:38:48Z", "updated_at": "2017-01-19T00:52:48Z", "author_association": "NONE", "body_html": "<p>Sorry to comment on a closed thread. I believe I'm running into a similar issue since I have a relatively simple feed-forward network with lots of examples that I want to feed through it. I've generated a trace file which shows my process dominated by QueueDequeueMany, and the GPUs oscillate between 0% and 30-60% usage. I have tried different <code>time.sleep</code> settings to give the queue time to fill, increased the number of preprocess threads, and increased the batch size, but none have had a noticeable effect.</p>\n<p>I am trying the <code>tf.train.batch(..., enqueue_many=True)</code> approach outlined above, but I get errors when I start using multiple GPUs.</p>\n<pre><code>\nE tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server\nE tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\nE tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server\nE tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n\n</code></pre>\n<p>The process continues then exits before completing any steps with the following error originating from where I called <code>tf.train.batch(..., enqueue_many=True)</code>:</p>\n<pre><code>  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 692, in batch\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 458, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1099, in _queue_dequeue_many\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): FIFOQueue '_182_tower_0/batch/fifo_queue' is closed and has insufficient elements (requested 512, current size 0)\n\t [[Node: tower_0/batch = QueueDequeueMany[_class=[\"loc:@tower_0/batch/fifo_queue\"], component_types=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tower_0/batch/fifo_queue, tower_0/batch/n/_507)]]\n\t [[Node: tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape/_378 = _HostSend[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_519_tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape\", _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape)]]\n</code></pre>\n<p>In different runs it references different tensor names, here <code>.../embedding/end_location_shared_embedding...</code>, but in previous runs it has referenced other embedding variables. These are created with <code>tf.contrib.layers.[shared_]embedding_column[s](...)</code>. Perhaps that is causing a problem on multiple GPUs.</p>\n<p>I do not get this error if I set <code>num_gpus=1</code>. I need multiple GPUs because speed is an issue due to the quantity of data I am feeding through it.</p>\n<p>Is there something I need to do differently to use <code>enqueue_many=True</code> with multiple GPUs?</p>", "body_text": "Sorry to comment on a closed thread. I believe I'm running into a similar issue since I have a relatively simple feed-forward network with lots of examples that I want to feed through it. I've generated a trace file which shows my process dominated by QueueDequeueMany, and the GPUs oscillate between 0% and 30-60% usage. I have tried different time.sleep settings to give the queue time to fill, increased the number of preprocess threads, and increased the batch size, but none have had a noticeable effect.\nI am trying the tf.train.batch(..., enqueue_many=True) approach outlined above, but I get errors when I start using multiple GPUs.\n\nE tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server\nE tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\nE tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server\nE tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n\n\nThe process continues then exits before completing any steps with the following error originating from where I called tf.train.batch(..., enqueue_many=True):\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 692, in batch\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 458, in dequeue_many\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1099, in _queue_dequeue_many\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): FIFOQueue '_182_tower_0/batch/fifo_queue' is closed and has insufficient elements (requested 512, current size 0)\n\t [[Node: tower_0/batch = QueueDequeueMany[_class=[\"loc:@tower_0/batch/fifo_queue\"], component_types=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tower_0/batch/fifo_queue, tower_0/batch/n/_507)]]\n\t [[Node: tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape/_378 = _HostSend[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_519_tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape\", _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape)]]\n\nIn different runs it references different tensor names, here .../embedding/end_location_shared_embedding..., but in previous runs it has referenced other embedding variables. These are created with tf.contrib.layers.[shared_]embedding_column[s](...). Perhaps that is causing a problem on multiple GPUs.\nI do not get this error if I set num_gpus=1. I need multiple GPUs because speed is an issue due to the quantity of data I am feeding through it.\nIs there something I need to do differently to use enqueue_many=True with multiple GPUs?", "body": "Sorry to comment on a closed thread. I believe I'm running into a similar issue since I have a relatively simple feed-forward network with lots of examples that I want to feed through it. I've generated a trace file which shows my process dominated by QueueDequeueMany, and the GPUs oscillate between 0% and 30-60% usage. I have tried different `time.sleep` settings to give the queue time to fill, increased the number of preprocess threads, and increased the batch size, but none have had a noticeable effect.\r\n\r\nI am trying the `tf.train.batch(..., enqueue_many=True)` approach outlined above, but I get errors when I start using multiple GPUs.\r\n```\r\n\r\nE tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server\r\nE tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\r\nE tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server\r\nE tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\r\n\r\n```\r\n\r\nThe process continues then exits before completing any steps with the following error originating from where I called `tf.train.batch(..., enqueue_many=True)`:\r\n\r\n```\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 692, in batch\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 458, in dequeue_many\r\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1099, in _queue_dequeue_many\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/ubuntu/.virtualenvs/test-six3/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): FIFOQueue '_182_tower_0/batch/fifo_queue' is closed and has insufficient elements (requested 512, current size 0)\r\n\t [[Node: tower_0/batch = QueueDequeueMany[_class=[\"loc:@tower_0/batch/fifo_queue\"], component_types=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tower_0/batch/fifo_queue, tower_0/batch/n/_507)]]\r\n\t [[Node: tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape/_378 = _HostSend[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_519_tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape\", _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/embedding/end_location_shared_embedding/end_location_shared_embeddingweights/Reshape_1/shape)]]\r\n```\r\nIn different runs it references different tensor names, here `.../embedding/end_location_shared_embedding...`, but in previous runs it has referenced other embedding variables. These are created with `tf.contrib.layers.[shared_]embedding_column[s](...)`. Perhaps that is causing a problem on multiple GPUs.\r\n\r\nI do not get this error if I set `num_gpus=1`. I need multiple GPUs because speed is an issue due to the quantity of data I am feeding through it.\r\n\r\nIs there something I need to do differently to use `enqueue_many=True` with multiple GPUs?"}