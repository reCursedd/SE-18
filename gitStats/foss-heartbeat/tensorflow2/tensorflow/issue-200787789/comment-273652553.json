{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273652553", "html_url": "https://github.com/tensorflow/tensorflow/issues/6845#issuecomment-273652553", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6845", "id": 273652553, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzY1MjU1Mw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-19T01:10:02Z", "updated_at": "2017-01-19T01:10:02Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">A quick google search shows \"Resource Exhausted\" on writing event file can\nbe caused by running out of memory\n<a href=\"http://stackoverflow.com/questions/38340070/after-500-steps-tensorflow-fail-to-write-summaries\">http://stackoverflow.com/questions/38340070/after-500-steps-tensorflow-fail-to-write-summaries</a>\n</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Jan 18, 2017 at 4:38 PM, Erik @ MileIQ ***@***.***&gt; wrote:\n Sorry to comment on a closed thread. I believe I'm running into a similar\n issue since I have a relatively simple feed-forward network with lots of\n examples that I want to feed through it. I've generated a trace file which\n shows my process dominated by QueueDequeueMany, and the GPUs oscillate\n between 0% and 30-60% usage. I have tried different time.sleep settings\n to give the queue time to fill, increased the number of preprocess threads,\n and increased the batch size, but none have had a noticeable effect.\n\n I am trying the tf.train.batch(..., enqueue_many=True) approach outlined\n above, but I get errors when I start using multiple GPUs.\n\n\n E tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server\n E tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n E tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server\n E tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n\n\n I do not get this error if I set num_gpus=1. I need multiple GPUs because\n speed is an issue due to the quantity of data I am feeding through it.\n\n Is there something I need to do differently to use enqueue_many=True with\n multiple GPUs?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"200787789\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6845\" href=\"https://github.com/tensorflow/tensorflow/issues/6845#issuecomment-273647478\">#6845 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AABaHDegEBsiCB9GBEySZEzo5vmC3LdMks5rTrCbgaJpZM4Ljje7\">https://github.com/notifications/unsubscribe-auth/AABaHDegEBsiCB9GBEySZEzo5vmC3LdMks5rTrCbgaJpZM4Ljje7</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "A quick google search shows \"Resource Exhausted\" on writing event file can\nbe caused by running out of memory\nhttp://stackoverflow.com/questions/38340070/after-500-steps-tensorflow-fail-to-write-summaries\n\n\u2026\nOn Wed, Jan 18, 2017 at 4:38 PM, Erik @ MileIQ ***@***.***> wrote:\n Sorry to comment on a closed thread. I believe I'm running into a similar\n issue since I have a relatively simple feed-forward network with lots of\n examples that I want to feed through it. I've generated a trace file which\n shows my process dominated by QueueDequeueMany, and the GPUs oscillate\n between 0% and 30-60% usage. I have tried different time.sleep settings\n to give the queue time to fill, increased the number of preprocess threads,\n and increased the batch size, but none have had a noticeable effect.\n\n I am trying the tf.train.batch(..., enqueue_many=True) approach outlined\n above, but I get errors when I start using multiple GPUs.\n\n\n E tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server\n E tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n E tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server\n E tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n\n\n I do not get this error if I set num_gpus=1. I need multiple GPUs because\n speed is an issue due to the quantity of data I am feeding through it.\n\n Is there something I need to do differently to use enqueue_many=True with\n multiple GPUs?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#6845 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AABaHDegEBsiCB9GBEySZEzo5vmC3LdMks5rTrCbgaJpZM4Ljje7>\n .", "body": "A quick google search shows \"Resource Exhausted\" on writing event file can\nbe caused by running out of memory\nhttp://stackoverflow.com/questions/38340070/after-500-steps-tensorflow-fail-to-write-summaries\n\nOn Wed, Jan 18, 2017 at 4:38 PM, Erik @ MileIQ <notifications@github.com>\nwrote:\n\n> Sorry to comment on a closed thread. I believe I'm running into a similar\n> issue since I have a relatively simple feed-forward network with lots of\n> examples that I want to feed through it. I've generated a trace file which\n> shows my process dominated by QueueDequeueMany, and the GPUs oscillate\n> between 0% and 30-60% usage. I have tried different time.sleep settings\n> to give the queue time to fill, increased the number of preprocess threads,\n> and increased the batch size, but none have had a noticeable effect.\n>\n> I am trying the tf.train.batch(..., enqueue_many=True) approach outlined\n> above, but I get errors when I start using multiple GPUs.\n>\n>\n> E tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785002.server\n> E tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n> E tensorflow/core/util/events_writer.cc:62] Could not open events file: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server: Resource exhausted: /data_full/output/full/checkpoint-events/train/events.out.tfevents.1484785004.server\n> E tensorflow/core/util/events_writer.cc:95] Write failed because file could not be opened.\n>\n>\n> I do not get this error if I set num_gpus=1. I need multiple GPUs because\n> speed is an issue due to the quantity of data I am feeding through it.\n>\n> Is there something I need to do differently to use enqueue_many=True with\n> multiple GPUs?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6845#issuecomment-273647478>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHDegEBsiCB9GBEySZEzo5vmC3LdMks5rTrCbgaJpZM4Ljje7>\n> .\n>\n"}