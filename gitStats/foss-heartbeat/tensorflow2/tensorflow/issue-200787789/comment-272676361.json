{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/272676361", "html_url": "https://github.com/tensorflow/tensorflow/issues/6845#issuecomment-272676361", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6845", "id": 272676361, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MjY3NjM2MQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-15T06:14:24Z", "updated_at": "2017-01-15T06:14:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I suppose this is a \"bug\" in how Python works. What happens when main compute thread session.run call is too fast (ie, &lt;2ms), then Python does not switch to the other reading read and will keep running your compute thread until it runs out of data to consume. At which point it'll switch to \"read one example-run one train step-read another example\", which is much less efficient than reading many examples before switching threads to do computation on them.</p>\n<p>This is not a common scenario because usually: train_op is slow/compute-intensive, and the pipeline is bottlenecked by compute part rather than IO part.</p>\n<p>If adding <code>time.sleep</code> helps at first, and then training slows down again, suggests that enqueing part of the pipeline has lower throughput than your dequeing half.</p>\n<p>It could mean that your \"dequeing\" (ie training part) is fundamentally easy, and decoding can't keep up. It could mean that decoding examples is inefficient and could be sped up. So you could try benchmarking just the decoding part (ie, start queue runners, and see how fast they can add things to queue), and try to estimate if the speed at which this happens is close to maximum attainable</p>", "body_text": "I suppose this is a \"bug\" in how Python works. What happens when main compute thread session.run call is too fast (ie, <2ms), then Python does not switch to the other reading read and will keep running your compute thread until it runs out of data to consume. At which point it'll switch to \"read one example-run one train step-read another example\", which is much less efficient than reading many examples before switching threads to do computation on them.\nThis is not a common scenario because usually: train_op is slow/compute-intensive, and the pipeline is bottlenecked by compute part rather than IO part.\nIf adding time.sleep helps at first, and then training slows down again, suggests that enqueing part of the pipeline has lower throughput than your dequeing half.\nIt could mean that your \"dequeing\" (ie training part) is fundamentally easy, and decoding can't keep up. It could mean that decoding examples is inefficient and could be sped up. So you could try benchmarking just the decoding part (ie, start queue runners, and see how fast they can add things to queue), and try to estimate if the speed at which this happens is close to maximum attainable", "body": "I suppose this is a \"bug\" in how Python works. What happens when main compute thread session.run call is too fast (ie, <2ms), then Python does not switch to the other reading read and will keep running your compute thread until it runs out of data to consume. At which point it'll switch to \"read one example-run one train step-read another example\", which is much less efficient than reading many examples before switching threads to do computation on them.\r\n\r\nThis is not a common scenario because usually: train_op is slow/compute-intensive, and the pipeline is bottlenecked by compute part rather than IO part.\r\n\r\nIf adding `time.sleep` helps at first, and then training slows down again, suggests that enqueing part of the pipeline has lower throughput than your dequeing half.\r\n\r\nIt could mean that your \"dequeing\" (ie training part) is fundamentally easy, and decoding can't keep up. It could mean that decoding examples is inefficient and could be sped up. So you could try benchmarking just the decoding part (ie, start queue runners, and see how fast they can add things to queue), and try to estimate if the speed at which this happens is close to maximum attainable"}