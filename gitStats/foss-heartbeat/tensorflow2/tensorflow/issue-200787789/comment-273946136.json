{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273946136", "html_url": "https://github.com/tensorflow/tensorflow/issues/6845#issuecomment-273946136", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6845", "id": 273946136, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Mzk0NjEzNg==", "user": {"login": "erik-mileiq", "id": 16887670, "node_id": "MDQ6VXNlcjE2ODg3Njcw", "avatar_url": "https://avatars3.githubusercontent.com/u/16887670?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erik-mileiq", "html_url": "https://github.com/erik-mileiq", "followers_url": "https://api.github.com/users/erik-mileiq/followers", "following_url": "https://api.github.com/users/erik-mileiq/following{/other_user}", "gists_url": "https://api.github.com/users/erik-mileiq/gists{/gist_id}", "starred_url": "https://api.github.com/users/erik-mileiq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erik-mileiq/subscriptions", "organizations_url": "https://api.github.com/users/erik-mileiq/orgs", "repos_url": "https://api.github.com/users/erik-mileiq/repos", "events_url": "https://api.github.com/users/erik-mileiq/events{/privacy}", "received_events_url": "https://api.github.com/users/erik-mileiq/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-20T00:55:18Z", "updated_at": "2017-01-20T01:00:16Z", "author_association": "NONE", "body_html": "<p>Thanks for the quick response <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>.</p>\n<p>I just now discovered <a href=\"https://www.tensorflow.org/api_docs/python/io_ops/readers#ReaderBase.read_up_to\" rel=\"nofollow\">tf.ReaderBase.read_up_to</a>, which seems to solve the problem without the extra code of <code>queue_batch = [] ... queue_batch.append(serialized) ... tf.train.batch(..., enqueue_many=True)</code>. Now reading is able to keep up with the training and the queue grows over time instead of quickly depleting.</p>\n<pre><code>_, serialized = reader.read_up_to(filename_queue, batch_size)\nbatch = tf.train.batch([serialized], batch_size=batch_size, ..., enqueue_many=True)\n</code></pre>\n<hr>\n<p>For posterity:<br>\nI believe the Resource Exhausted error was a result of a race condition, since it was resolved by adding a sleep command before creating a tf.summary.FileWriter object.</p>\n<p>Stepping back, the reason why I believe I have the same issue as described here: when I increase the batch size and time for the queue to fill up, I get much faster speeds when the queue is full (~35k examples/gpu/sec). Once the queue depletes, it falls back to (~5k examples/gpu/sec), which is about what I get with much smaller batch sizes. I also get smaller queue sizes when I increase the number of threads. This leads me to believe that the input is the bottleneck for training.</p>\n<p>Here is the trace at step 1 when the queue is not depleted. UnsortedSegmentSum on the CPU dominates, I think due to the summing of the variables across GPUs.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/16887670/22130086/e4b96b94-de5e-11e6-994b-d202d288b1ab.png\"><img src=\"https://cloud.githubusercontent.com/assets/16887670/22130086/e4b96b94-de5e-11e6-994b-d202d288b1ab.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>By step 200, the queue has been depleted and now the process is dominated by QueueDequeueMany:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/16887670/22130127/21d63084-de5f-11e6-9db8-122a7339ebb4.png\"><img src=\"https://cloud.githubusercontent.com/assets/16887670/22130127/21d63084-de5f-11e6-9db8-122a7339ebb4.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>", "body_text": "Thanks for the quick response @yaroslavvb.\nI just now discovered tf.ReaderBase.read_up_to, which seems to solve the problem without the extra code of queue_batch = [] ... queue_batch.append(serialized) ... tf.train.batch(..., enqueue_many=True). Now reading is able to keep up with the training and the queue grows over time instead of quickly depleting.\n_, serialized = reader.read_up_to(filename_queue, batch_size)\nbatch = tf.train.batch([serialized], batch_size=batch_size, ..., enqueue_many=True)\n\n\nFor posterity:\nI believe the Resource Exhausted error was a result of a race condition, since it was resolved by adding a sleep command before creating a tf.summary.FileWriter object.\nStepping back, the reason why I believe I have the same issue as described here: when I increase the batch size and time for the queue to fill up, I get much faster speeds when the queue is full (~35k examples/gpu/sec). Once the queue depletes, it falls back to (~5k examples/gpu/sec), which is about what I get with much smaller batch sizes. I also get smaller queue sizes when I increase the number of threads. This leads me to believe that the input is the bottleneck for training.\nHere is the trace at step 1 when the queue is not depleted. UnsortedSegmentSum on the CPU dominates, I think due to the summing of the variables across GPUs.\n\nBy step 200, the queue has been depleted and now the process is dominated by QueueDequeueMany:", "body": "Thanks for the quick response @yaroslavvb. \r\n\r\nI just now discovered [tf.ReaderBase.read_up_to](https://www.tensorflow.org/api_docs/python/io_ops/readers#ReaderBase.read_up_to), which seems to solve the problem without the extra code of `queue_batch = [] ... queue_batch.append(serialized) ... tf.train.batch(..., enqueue_many=True)`. Now reading is able to keep up with the training and the queue grows over time instead of quickly depleting.\r\n\r\n```\r\n_, serialized = reader.read_up_to(filename_queue, batch_size)\r\nbatch = tf.train.batch([serialized], batch_size=batch_size, ..., enqueue_many=True)\r\n```\r\n\r\n----\r\nFor posterity:\r\nI believe the Resource Exhausted error was a result of a race condition, since it was resolved by adding a sleep command before creating a tf.summary.FileWriter object.\r\n\r\nStepping back, the reason why I believe I have the same issue as described here: when I increase the batch size and time for the queue to fill up, I get much faster speeds when the queue is full (~35k examples/gpu/sec). Once the queue depletes, it falls back to (~5k examples/gpu/sec), which is about what I get with much smaller batch sizes. I also get smaller queue sizes when I increase the number of threads. This leads me to believe that the input is the bottleneck for training.\r\n\r\nHere is the trace at step 1 when the queue is not depleted. UnsortedSegmentSum on the CPU dominates, I think due to the summing of the variables across GPUs.\r\n![image](https://cloud.githubusercontent.com/assets/16887670/22130086/e4b96b94-de5e-11e6-994b-d202d288b1ab.png)\r\n\r\nBy step 200, the queue has been depleted and now the process is dominated by QueueDequeueMany:\r\n![image](https://cloud.githubusercontent.com/assets/16887670/22130127/21d63084-de5f-11e6-9db8-122a7339ebb4.png)"}