{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6116", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6116/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6116/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6116/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6116", "id": 193706605, "node_id": "MDU6SXNzdWUxOTM3MDY2MDU=", "number": 6116, "title": "grpc RecvTensor is slow", "user": {"login": "llhe", "id": 192829, "node_id": "MDQ6VXNlcjE5MjgyOQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/192829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/llhe", "html_url": "https://github.com/llhe", "followers_url": "https://api.github.com/users/llhe/followers", "following_url": "https://api.github.com/users/llhe/following{/other_user}", "gists_url": "https://api.github.com/users/llhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/llhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/llhe/subscriptions", "organizations_url": "https://api.github.com/users/llhe/orgs", "repos_url": "https://api.github.com/users/llhe/repos", "events_url": "https://api.github.com/users/llhe/events{/privacy}", "received_events_url": "https://api.github.com/users/llhe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jhseu", "id": 170179, "node_id": "MDQ6VXNlcjE3MDE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/170179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhseu", "html_url": "https://github.com/jhseu", "followers_url": "https://api.github.com/users/jhseu/followers", "following_url": "https://api.github.com/users/jhseu/following{/other_user}", "gists_url": "https://api.github.com/users/jhseu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhseu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhseu/subscriptions", "organizations_url": "https://api.github.com/users/jhseu/orgs", "repos_url": "https://api.github.com/users/jhseu/repos", "events_url": "https://api.github.com/users/jhseu/events{/privacy}", "received_events_url": "https://api.github.com/users/jhseu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jhseu", "id": 170179, "node_id": "MDQ6VXNlcjE3MDE3OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/170179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhseu", "html_url": "https://github.com/jhseu", "followers_url": "https://api.github.com/users/jhseu/followers", "following_url": "https://api.github.com/users/jhseu/following{/other_user}", "gists_url": "https://api.github.com/users/jhseu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhseu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhseu/subscriptions", "organizations_url": "https://api.github.com/users/jhseu/orgs", "repos_url": "https://api.github.com/users/jhseu/repos", "events_url": "https://api.github.com/users/jhseu/events{/privacy}", "received_events_url": "https://api.github.com/users/jhseu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 42, "created_at": "2016-12-06T08:03:00Z", "updated_at": "2018-03-08T17:23:46Z", "closed_at": "2017-08-09T16:41:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I made benchmark tests for distributed setup with loopback network, profiling it and found there is excessive memory copying in the client side of RecvTensor call, which is actually one of the bottleneck.</p>\n<p>Here is the code, which mainly stolen from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> <a href=\"https://gist.github.com/yaroslavvb/1124bb02a9fd4abce3d86caf2f950cb2\">here</a>,</p>\n<div class=\"highlight highlight-source-python\"><pre>  <span class=\"pl-k\">with</span> tf.device(device1):                                                      \n    params <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>params<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[params_size], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype,        \n                             <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.zeros_initializer)                  \n  <span class=\"pl-k\">with</span> tf.device(device2):                                                      \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> constant node gets placed on device1 because of simple_placer             </span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>    update = tf.constant(1, shape=[params_size], dtype=dtype)              </span>\n    update <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>update<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[params_size], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype,        \n                             <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.ones_initializer())                 \n    add_op <span class=\"pl-k\">=</span> params.assign(update)</pre></div>\n<p>Here is the profiling result (google perftools) with tensor size 100MB (one fact is, the throughput will degrade with the increasing of tensor size):</p>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/files/633183/device1-prof.pdf\">device1 worker profiling report</a></li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/files/633184/device2-prof.pdf\">device2 worker profiling report</a></li>\n</ul>\n<p>From the result, the sending side (device2) look fine, but the receiving side (device1, the grpc client) consumes too many CPU cycles for the data transfer.</p>\n<p>By the way, I made rough stats for this <a href=\"https://github.com/grpc/grpc/blob/d7ff4ff40071d2b486a052183e3e9f9382afb745/src/core/lib/support/slice_buffer.c#L278\">memmove</a> call. For one round of 100MB tensor assignment, there are roughly 2GB data moved (actually, including the copy inside memmove, it should be 4GB copied with a naive memmove), which is 20+ times RAM bandwidth amplification (the result is an average of 100 round run, which may not precise but the scale should be ok).</p>", "body_text": "I made benchmark tests for distributed setup with loopback network, profiling it and found there is excessive memory copying in the client side of RecvTensor call, which is actually one of the bottleneck.\nHere is the code, which mainly stolen from @yaroslavvb here,\n  with tf.device(device1):                                                      \n    params = tf.get_variable(\"params\", shape=[params_size], dtype=dtype,        \n                             initializer=tf.zeros_initializer)                  \n  with tf.device(device2):                                                      \n    # constant node gets placed on device1 because of simple_placer             \n    #    update = tf.constant(1, shape=[params_size], dtype=dtype)              \n    update = tf.get_variable(\"update\", shape=[params_size], dtype=dtype,        \n                             initializer=tf.ones_initializer())                 \n    add_op = params.assign(update)\nHere is the profiling result (google perftools) with tensor size 100MB (one fact is, the throughput will degrade with the increasing of tensor size):\n\ndevice1 worker profiling report\ndevice2 worker profiling report\n\nFrom the result, the sending side (device2) look fine, but the receiving side (device1, the grpc client) consumes too many CPU cycles for the data transfer.\nBy the way, I made rough stats for this memmove call. For one round of 100MB tensor assignment, there are roughly 2GB data moved (actually, including the copy inside memmove, it should be 4GB copied with a naive memmove), which is 20+ times RAM bandwidth amplification (the result is an average of 100 round run, which may not precise but the scale should be ok).", "body": "I made benchmark tests for distributed setup with loopback network, profiling it and found there is excessive memory copying in the client side of RecvTensor call, which is actually one of the bottleneck.\r\n\r\nHere is the code, which mainly stolen from @yaroslavvb [here](https://gist.github.com/yaroslavvb/1124bb02a9fd4abce3d86caf2f950cb2),\r\n```python\r\n  with tf.device(device1):                                                      \r\n    params = tf.get_variable(\"params\", shape=[params_size], dtype=dtype,        \r\n                             initializer=tf.zeros_initializer)                  \r\n  with tf.device(device2):                                                      \r\n    # constant node gets placed on device1 because of simple_placer             \r\n    #    update = tf.constant(1, shape=[params_size], dtype=dtype)              \r\n    update = tf.get_variable(\"update\", shape=[params_size], dtype=dtype,        \r\n                             initializer=tf.ones_initializer())                 \r\n    add_op = params.assign(update)\r\n```\r\n\r\nHere is the profiling result (google perftools) with tensor size 100MB (one fact is, the throughput will degrade with the increasing of tensor size):\r\n* [device1 worker profiling report](https://github.com/tensorflow/tensorflow/files/633183/device1-prof.pdf)\r\n* [device2 worker profiling report](https://github.com/tensorflow/tensorflow/files/633184/device2-prof.pdf)\r\n\r\nFrom the result, the sending side (device2) look fine, but the receiving side (device1, the grpc client) consumes too many CPU cycles for the data transfer.\r\n\r\nBy the way, I made rough stats for this [memmove](https://github.com/grpc/grpc/blob/d7ff4ff40071d2b486a052183e3e9f9382afb745/src/core/lib/support/slice_buffer.c#L278) call. For one round of 100MB tensor assignment, there are roughly 2GB data moved (actually, including the copy inside memmove, it should be 4GB copied with a naive memmove), which is 20+ times RAM bandwidth amplification (the result is an average of 100 round run, which may not precise but the scale should be ok)."}