{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/410110382", "html_url": "https://github.com/tensorflow/tensorflow/issues/20998#issuecomment-410110382", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20998", "id": 410110382, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMDExMDM4Mg==", "user": {"login": "ahundt", "id": 55744, "node_id": "MDQ6VXNlcjU1NzQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/55744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahundt", "html_url": "https://github.com/ahundt", "followers_url": "https://api.github.com/users/ahundt/followers", "following_url": "https://api.github.com/users/ahundt/following{/other_user}", "gists_url": "https://api.github.com/users/ahundt/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahundt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahundt/subscriptions", "organizations_url": "https://api.github.com/users/ahundt/orgs", "repos_url": "https://api.github.com/users/ahundt/repos", "events_url": "https://api.github.com/users/ahundt/events{/privacy}", "received_events_url": "https://api.github.com/users/ahundt/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-03T00:48:04Z", "updated_at": "2018-08-03T00:48:59Z", "author_association": "NONE", "body_html": "<p>Ah well disabling that printout may help me immensely! I've never seen the logging print quickly, as far as I have seen, catching the error takes an extreme quantity of time.</p>\n<p>This can sometimes happen right at my program's startup when I have good amounts of system memory free, perhaps using 8GB / 48GB and thus 40GB free. For example, if I tried to extend a NASNetLarge model with 8 consecutive Conv layers with a 3x3 filter size at the bottom, each with 4096 filters, and a batch size of 32, GPU memory (8GB-12GB depending on the card) will be completely exhausted and this error will occur.</p>\n<p>Since I'm doing a random search of those parameters (# layers, # filters, etc), it isn't easy to completely avoid this issue unless there is a way to predict future memory usage.</p>", "body_text": "Ah well disabling that printout may help me immensely! I've never seen the logging print quickly, as far as I have seen, catching the error takes an extreme quantity of time.\nThis can sometimes happen right at my program's startup when I have good amounts of system memory free, perhaps using 8GB / 48GB and thus 40GB free. For example, if I tried to extend a NASNetLarge model with 8 consecutive Conv layers with a 3x3 filter size at the bottom, each with 4096 filters, and a batch size of 32, GPU memory (8GB-12GB depending on the card) will be completely exhausted and this error will occur.\nSince I'm doing a random search of those parameters (# layers, # filters, etc), it isn't easy to completely avoid this issue unless there is a way to predict future memory usage.", "body": "Ah well disabling that printout may help me immensely! I've never seen the logging print quickly, as far as I have seen, catching the error takes an extreme quantity of time.\r\n\r\nThis can sometimes happen right at my program's startup when I have good amounts of system memory free, perhaps using 8GB / 48GB and thus 40GB free. For example, if I tried to extend a NASNetLarge model with 8 consecutive Conv layers with a 3x3 filter size at the bottom, each with 4096 filters, and a batch size of 32, GPU memory (8GB-12GB depending on the card) will be completely exhausted and this error will occur. \r\n\r\nSince I'm doing a random search of those parameters (# layers, # filters, etc), it isn't easy to completely avoid this issue unless there is a way to predict future memory usage. "}