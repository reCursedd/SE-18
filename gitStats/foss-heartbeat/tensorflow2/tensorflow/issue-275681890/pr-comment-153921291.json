{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/153921291", "pull_request_review_id": 80004687, "id": 153921291, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzkyMTI5MQ==", "diff_hunk": "@@ -45,11 +45,9 @@\n VALIDATION_FILE = 'validation.tfrecords'\n \n \n-def read_and_decode(filename_queue):\n-  reader = tf.TFRecordReader()\n-  _, serialized_example = reader.read(filename_queue)\n-  features = tf.parse_single_example(\n-      serialized_example,\n+def decode(serialized_example):\n+  features = tf.parse_example(", "path": "tensorflow/examples/how_tos/reading_data/fully_connected_reader.py", "position": null, "original_position": 10, "commit_id": "e018f3093dc1f94fa64c3ca928fca685f43aae50", "original_commit_id": "980f2eb047f6cddec3aa64d4461f6ff6613b89ed", "user": {"login": "FirefoxMetzger", "id": 4402489, "node_id": "MDQ6VXNlcjQ0MDI0ODk=", "avatar_url": "https://avatars1.githubusercontent.com/u/4402489?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FirefoxMetzger", "html_url": "https://github.com/FirefoxMetzger", "followers_url": "https://api.github.com/users/FirefoxMetzger/followers", "following_url": "https://api.github.com/users/FirefoxMetzger/following{/other_user}", "gists_url": "https://api.github.com/users/FirefoxMetzger/gists{/gist_id}", "starred_url": "https://api.github.com/users/FirefoxMetzger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FirefoxMetzger/subscriptions", "organizations_url": "https://api.github.com/users/FirefoxMetzger/orgs", "repos_url": "https://api.github.com/users/FirefoxMetzger/repos", "events_url": "https://api.github.com/users/FirefoxMetzger/events{/privacy}", "received_events_url": "https://api.github.com/users/FirefoxMetzger/received_events", "type": "User", "site_admin": false}, "body": "Okay, I've just moved batching and shuffling up to try decoding examples as batches rather than single examples. That does make a huge difference in speed and removes both reshape ops. I will still revert the changes and update the PR.\r\n\r\n(Just FYI here are the times on my super recent GTX750Ti)\r\n\r\nDecoding `parse_single_example`:\r\n\r\n    Step 0: loss = 2.31 (0.609 sec)\r\n    Step 100: loss = 2.15 (0.031 sec)\r\n    Step 200: loss = 1.96 (0.030 sec)\r\n    Step 300: loss = 1.60 (0.029 sec)\r\n    Step 400: loss = 1.35 (0.029 sec)\r\n    Step 500: loss = 0.90 (0.031 sec)\r\n    Step 600: loss = 0.88 (0.030 sec)\r\n    Step 700: loss = 0.72 (0.031 sec)\r\n    Step 800: loss = 0.62 (0.028 sec)\r\n    Step 900: loss = 0.60 (0.031 sec)\r\n    Step 1000: loss = 0.56 (0.027 sec)\r\n    Done training for 2 epochs, 1100 steps.\r\n\r\n\r\nDecoding `parse_example` (one example at a time):\r\n\r\n    Step 0: loss = 2.30 (0.585 sec)\r\n    Step 100: loss = 2.18 (0.031 sec)\r\n    Step 200: loss = 1.99 (0.031 sec)\r\n    Step 300: loss = 1.70 (0.031 sec)\r\n    Step 400: loss = 1.39 (0.030 sec)\r\n    Step 500: loss = 1.14 (0.029 sec)\r\n    Step 600: loss = 0.73 (0.031 sec)\r\n    Step 700: loss = 0.73 (0.028 sec)\r\n    Step 800: loss = 0.75 (0.030 sec)\r\n    Step 900: loss = 0.59 (0.028 sec)\r\n    Step 1000: loss = 0.80 (0.030 sec)\r\n    Done training for 2 epochs, 1100 steps.\r\n\r\nDecoding `parse_example` batched:\r\n\r\n    Step 0: loss = 2.35 (0.237 sec)\r\n    Step 100: loss = 2.10 (0.004 sec)\r\n    Step 200: loss = 1.79 (0.004 sec)\r\n    Step 300: loss = 1.42 (0.004 sec)\r\n    Step 400: loss = 1.14 (0.005 sec)\r\n    Step 500: loss = 0.87 (0.003 sec)\r\n    Step 600: loss = 0.64 (0.004 sec)\r\n    Step 700: loss = 0.69 (0.004 sec)\r\n    Step 800: loss = 0.75 (0.005 sec)\r\n    Step 900: loss = 0.45 (0.004 sec)\r\n    Step 1000: loss = 0.57 (0.005 sec)\r\n    Done training for 2 epochs, 1100 steps.", "created_at": "2017-11-29T21:33:28Z", "updated_at": "2017-12-04T14:43:25Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14751#discussion_r153921291", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14751", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/153921291"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14751#discussion_r153921291"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14751"}}, "body_html": "<p>Okay, I've just moved batching and shuffling up to try decoding examples as batches rather than single examples. That does make a huge difference in speed and removes both reshape ops. I will still revert the changes and update the PR.</p>\n<p>(Just FYI here are the times on my super recent GTX750Ti)</p>\n<p>Decoding <code>parse_single_example</code>:</p>\n<pre><code>Step 0: loss = 2.31 (0.609 sec)\nStep 100: loss = 2.15 (0.031 sec)\nStep 200: loss = 1.96 (0.030 sec)\nStep 300: loss = 1.60 (0.029 sec)\nStep 400: loss = 1.35 (0.029 sec)\nStep 500: loss = 0.90 (0.031 sec)\nStep 600: loss = 0.88 (0.030 sec)\nStep 700: loss = 0.72 (0.031 sec)\nStep 800: loss = 0.62 (0.028 sec)\nStep 900: loss = 0.60 (0.031 sec)\nStep 1000: loss = 0.56 (0.027 sec)\nDone training for 2 epochs, 1100 steps.\n</code></pre>\n<p>Decoding <code>parse_example</code> (one example at a time):</p>\n<pre><code>Step 0: loss = 2.30 (0.585 sec)\nStep 100: loss = 2.18 (0.031 sec)\nStep 200: loss = 1.99 (0.031 sec)\nStep 300: loss = 1.70 (0.031 sec)\nStep 400: loss = 1.39 (0.030 sec)\nStep 500: loss = 1.14 (0.029 sec)\nStep 600: loss = 0.73 (0.031 sec)\nStep 700: loss = 0.73 (0.028 sec)\nStep 800: loss = 0.75 (0.030 sec)\nStep 900: loss = 0.59 (0.028 sec)\nStep 1000: loss = 0.80 (0.030 sec)\nDone training for 2 epochs, 1100 steps.\n</code></pre>\n<p>Decoding <code>parse_example</code> batched:</p>\n<pre><code>Step 0: loss = 2.35 (0.237 sec)\nStep 100: loss = 2.10 (0.004 sec)\nStep 200: loss = 1.79 (0.004 sec)\nStep 300: loss = 1.42 (0.004 sec)\nStep 400: loss = 1.14 (0.005 sec)\nStep 500: loss = 0.87 (0.003 sec)\nStep 600: loss = 0.64 (0.004 sec)\nStep 700: loss = 0.69 (0.004 sec)\nStep 800: loss = 0.75 (0.005 sec)\nStep 900: loss = 0.45 (0.004 sec)\nStep 1000: loss = 0.57 (0.005 sec)\nDone training for 2 epochs, 1100 steps.\n</code></pre>", "body_text": "Okay, I've just moved batching and shuffling up to try decoding examples as batches rather than single examples. That does make a huge difference in speed and removes both reshape ops. I will still revert the changes and update the PR.\n(Just FYI here are the times on my super recent GTX750Ti)\nDecoding parse_single_example:\nStep 0: loss = 2.31 (0.609 sec)\nStep 100: loss = 2.15 (0.031 sec)\nStep 200: loss = 1.96 (0.030 sec)\nStep 300: loss = 1.60 (0.029 sec)\nStep 400: loss = 1.35 (0.029 sec)\nStep 500: loss = 0.90 (0.031 sec)\nStep 600: loss = 0.88 (0.030 sec)\nStep 700: loss = 0.72 (0.031 sec)\nStep 800: loss = 0.62 (0.028 sec)\nStep 900: loss = 0.60 (0.031 sec)\nStep 1000: loss = 0.56 (0.027 sec)\nDone training for 2 epochs, 1100 steps.\n\nDecoding parse_example (one example at a time):\nStep 0: loss = 2.30 (0.585 sec)\nStep 100: loss = 2.18 (0.031 sec)\nStep 200: loss = 1.99 (0.031 sec)\nStep 300: loss = 1.70 (0.031 sec)\nStep 400: loss = 1.39 (0.030 sec)\nStep 500: loss = 1.14 (0.029 sec)\nStep 600: loss = 0.73 (0.031 sec)\nStep 700: loss = 0.73 (0.028 sec)\nStep 800: loss = 0.75 (0.030 sec)\nStep 900: loss = 0.59 (0.028 sec)\nStep 1000: loss = 0.80 (0.030 sec)\nDone training for 2 epochs, 1100 steps.\n\nDecoding parse_example batched:\nStep 0: loss = 2.35 (0.237 sec)\nStep 100: loss = 2.10 (0.004 sec)\nStep 200: loss = 1.79 (0.004 sec)\nStep 300: loss = 1.42 (0.004 sec)\nStep 400: loss = 1.14 (0.005 sec)\nStep 500: loss = 0.87 (0.003 sec)\nStep 600: loss = 0.64 (0.004 sec)\nStep 700: loss = 0.69 (0.004 sec)\nStep 800: loss = 0.75 (0.005 sec)\nStep 900: loss = 0.45 (0.004 sec)\nStep 1000: loss = 0.57 (0.005 sec)\nDone training for 2 epochs, 1100 steps.", "in_reply_to_id": 153911453}