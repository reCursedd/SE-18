{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22464", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22464/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22464/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22464/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22464", "id": 362857966, "node_id": "MDU6SXNzdWUzNjI4NTc5NjY=", "number": 22464, "title": "memory leak cased by function tf.dynamic_partition", "user": {"login": "mzhaoshuai", "id": 22476764, "node_id": "MDQ6VXNlcjIyNDc2NzY0", "avatar_url": "https://avatars1.githubusercontent.com/u/22476764?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mzhaoshuai", "html_url": "https://github.com/mzhaoshuai", "followers_url": "https://api.github.com/users/mzhaoshuai/followers", "following_url": "https://api.github.com/users/mzhaoshuai/following{/other_user}", "gists_url": "https://api.github.com/users/mzhaoshuai/gists{/gist_id}", "starred_url": "https://api.github.com/users/mzhaoshuai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mzhaoshuai/subscriptions", "organizations_url": "https://api.github.com/users/mzhaoshuai/orgs", "repos_url": "https://api.github.com/users/mzhaoshuai/repos", "events_url": "https://api.github.com/users/mzhaoshuai/events{/privacy}", "received_events_url": "https://api.github.com/users/mzhaoshuai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-09-22T13:39:33Z", "updated_at": "2018-11-22T18:58:48Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<em>no</em></li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  <em>Linux Ubuntu 16.04.4 LTS</em>\u3002</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:<em>no</em></li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<em>binary</em></li>\n<li><strong>TensorFlow version (use command below)</strong>:<em>1.8.0</em></li>\n<li><strong>Python version</strong>:<em>Python 3.5.2</em></li>\n<li><strong>Bazel version (if compiling from source)</strong>:<em>None</em></li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<em>None</em></li>\n<li><strong>CUDA/cuDNN version</strong>:<em>CUDA-9.0/cuDNN-7.0.5</em></li>\n<li><strong>GPU model and memory</strong>: <em>GTX 1080Ti 11GB, 2 same cards</em></li>\n<li><strong>Exact command to reproduce</strong>:<em>below</em></li>\n</ul>\n<h3>Supplementary material</h3>\n<p>I just pulled the tensorflow's offical docker image and RUN some <code>pip install ...</code> instructions.<br>\nThe tag of the iamge is <code>1.8.0-devel-gpu-py3</code>.</p>\n<h3>Describe the problem</h3>\n<p>I want to calculate a focal loss.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/22476764/45917573-bb403600-beac-11e8-8262-56be63acd1c3.png\"><img src=\"https://user-images.githubusercontent.com/22476764/45917573-bb403600-beac-11e8-8262-56be63acd1c3.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>It just added weight factor to the original cross entropy loss.</p>\n<h4><code>tf_leak</code> code (shown in Source code / logs)</h4>\n<p>When I used the <code>tf_leak</code> code as shown below, there will be memory leak.<br>\nI used <code>top</code> command to see the process's memory usage.</p>\n<ul>\n<li>\n<p><strong>pic01 occupy about 3G memory after about 1min</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/22476764/45917603-212cbd80-bead-11e8-8b2b-e784196405b2.png\"><img src=\"https://user-images.githubusercontent.com/22476764/45917603-212cbd80-bead-11e8-8b2b-e784196405b2.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p><strong>pic02 occupy about 11G memory after about 33min</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/22476764/45917609-3ace0500-bead-11e8-9d6e-b58d748f6fbc.png\"><img src=\"https://user-images.githubusercontent.com/22476764/45917609-3ace0500-bead-11e8-9d6e-b58d748f6fbc.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p><strong>pic03 occupy about 14G memory after about 191min</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/22476764/45917619-45889a00-bead-11e8-8370-082b341c4907.png\"><img src=\"https://user-images.githubusercontent.com/22476764/45917619-45889a00-bead-11e8-8370-082b341c4907.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n</li>\n</ul>\n<p><strong>I use the <code>tf.estimator</code> and the graph is finazied. So I do not add ops to the graph when the seesion is running.</strong></p>\n<ul>\n<li><strong>pic04 graph is finazied</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/22476764/45917706-16bef380-beae-11e8-8af9-a0a8fd46c52a.png\"><img src=\"https://user-images.githubusercontent.com/22476764/45917706-16bef380-beae-11e8-8af9-a0a8fd46c52a.png\" alt=\"image\" style=\"max-width:100%;\"></a></li>\n</ul>\n<h4><code>tf_no_leak</code> code (shown in Source code / logs)</h4>\n<p>I replace the <code> tf.dynamic_partition</code> function with <code>tf.boolean_mask</code> and the memory leak disappered.</p>\n<ul>\n<li>\n<p><strong>pic05 occupy about 3G memory after about 9min</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/22476764/45917758-ec216a80-beae-11e8-9d49-115f2598fa50.png\"><img src=\"https://user-images.githubusercontent.com/22476764/45917758-ec216a80-beae-11e8-9d49-115f2598fa50.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p><strong>pic06 occupy about 3G memory after about 133min</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/22476764/45917782-2be85200-beaf-11e8-9427-021f2f5fcd51.png\"><img src=\"https://user-images.githubusercontent.com/22476764/45917782-2be85200-beaf-11e8-9427-021f2f5fcd51.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n</li>\n</ul>\n<h4>more information</h4>\n<p>I tested the <code>tf_leak</code> and <code>tf_no_leak</code> code with a small dataset, when I use a bigger dataset, the process will occupy a large memory and the system will kill the process. This is why I found the problem.</p>\n<h3>Source code / logs</h3>\n<pre><code>params:\n\t\tvalid_labels\t:\tthe labels, a Tensor with shape [N * w * h, ]\n\t\tvalid_logits\t:\tthe logits, a Tensor with shape [N * w * h, num_classes]\n\t\tnum_classes \t: \tthe number of classes, a scalar\n                ohem_prob_g  :       a hyperparameter, a scalar, default=1.5\n</code></pre>\n<ol>\n<li><code>tf_leak</code> code</li>\n</ol>\n<pre><code>valid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \\\n\t\t\t\t\tclip_value_min=1e-6, clip_value_max=1.0\nvalid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)\nweight_matrix = ohem_prob_g - tf.dynamic_partition(valid_probs, valid_one_hot, num_partitions=2)[1]\n## calculate the final cross-entropy\ncross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, \n\t\t\t\t\t\tlogits=valid_logits, weights=weight_matrix)\nreturn cross_entropy\n</code></pre>\n<ol start=\"2\">\n<li><code>tf_no_leak</code> code</li>\n</ol>\n<p>same as the <code>tf_leak</code> code except I replace the <code>tf.dynamic_partition</code> with <code>tf.boolean_mask</code>.</p>\n<pre><code>valid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \\\n\t\t\t\t\tclip_value_min=1e-6, clip_value_max=1.0)\nvalid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)\n## calculate the final cross-entropy\nweight_matrix = ohem_prob_g - tf.boolean_mask(valid_probs, valid_one_hot)\ncross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, \n\t\t\t\t\t\tlogits=valid_logits, weights=weight_matrix)\nreturn cross_entropy\n</code></pre>\n<ol start=\"3\">\n<li>input code used for the <code>tf.estimator</code></li>\n</ol>\n<pre><code>def input_fn(is_training, data_dir, batch_size, num_epochs=1):\n  \"\"\"Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\n  Args:\n    is_training: A boolean denoting whether the input is for training.\n    data_dir: The directory containing the input data.\n    batch_size: The number of samples per batch.\n    num_epochs: The number of epochs to repeat the dataset.\n  Returns:\n    A tuple of images and labels.\n  \"\"\"\n  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(is_training, data_dir))\n  dataset = dataset.flat_map(tf.data.TFRecordDataset)\n  if is_training:\n    dataset = dataset.shuffle(buffer_size=_NUM_IMAGES['train'])\n\n  dataset = dataset.map(parse_record)\n  dataset = dataset.map(\n      lambda image, label, idx: preprocess_image(image, label, idx, is_training),\n      num_parallel_calls=1)\n\n  # We call repeat after shuffling, rather than before, to prevent separate\n  # epochs from blending together.\n  dataset = dataset.repeat(num_epochs)\n  dataset = dataset.batch(batch_size)\n  dataset = dataset.prefetch(buffer_size=FLAGS.prefetch_buffer_size)\n\n  iterator = dataset.make_one_shot_iterator()\n  images, labels, idx = iterator.get_next()\n\n  return images, labels\n\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04.4 LTS\u3002\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):1.8.0\nPython version:Python 3.5.2\nBazel version (if compiling from source):None\nGCC/Compiler version (if compiling from source):None\nCUDA/cuDNN version:CUDA-9.0/cuDNN-7.0.5\nGPU model and memory: GTX 1080Ti 11GB, 2 same cards\nExact command to reproduce:below\n\nSupplementary material\nI just pulled the tensorflow's offical docker image and RUN some pip install ... instructions.\nThe tag of the iamge is 1.8.0-devel-gpu-py3.\nDescribe the problem\nI want to calculate a focal loss.\n\nIt just added weight factor to the original cross entropy loss.\ntf_leak code (shown in Source code / logs)\nWhen I used the tf_leak code as shown below, there will be memory leak.\nI used top command to see the process's memory usage.\n\n\npic01 occupy about 3G memory after about 1min\n\n\n\npic02 occupy about 11G memory after about 33min\n\n\n\npic03 occupy about 14G memory after about 191min\n\n\n\nI use the tf.estimator and the graph is finazied. So I do not add ops to the graph when the seesion is running.\n\npic04 graph is finazied\n\n\ntf_no_leak code (shown in Source code / logs)\nI replace the  tf.dynamic_partition function with tf.boolean_mask and the memory leak disappered.\n\n\npic05 occupy about 3G memory after about 9min\n\n\n\npic06 occupy about 3G memory after about 133min\n\n\n\nmore information\nI tested the tf_leak and tf_no_leak code with a small dataset, when I use a bigger dataset, the process will occupy a large memory and the system will kill the process. This is why I found the problem.\nSource code / logs\nparams:\n\t\tvalid_labels\t:\tthe labels, a Tensor with shape [N * w * h, ]\n\t\tvalid_logits\t:\tthe logits, a Tensor with shape [N * w * h, num_classes]\n\t\tnum_classes \t: \tthe number of classes, a scalar\n                ohem_prob_g  :       a hyperparameter, a scalar, default=1.5\n\n\ntf_leak code\n\nvalid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \\\n\t\t\t\t\tclip_value_min=1e-6, clip_value_max=1.0\nvalid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)\nweight_matrix = ohem_prob_g - tf.dynamic_partition(valid_probs, valid_one_hot, num_partitions=2)[1]\n## calculate the final cross-entropy\ncross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, \n\t\t\t\t\t\tlogits=valid_logits, weights=weight_matrix)\nreturn cross_entropy\n\n\ntf_no_leak code\n\nsame as the tf_leak code except I replace the tf.dynamic_partition with tf.boolean_mask.\nvalid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \\\n\t\t\t\t\tclip_value_min=1e-6, clip_value_max=1.0)\nvalid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)\n## calculate the final cross-entropy\nweight_matrix = ohem_prob_g - tf.boolean_mask(valid_probs, valid_one_hot)\ncross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, \n\t\t\t\t\t\tlogits=valid_logits, weights=weight_matrix)\nreturn cross_entropy\n\n\ninput code used for the tf.estimator\n\ndef input_fn(is_training, data_dir, batch_size, num_epochs=1):\n  \"\"\"Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\n  Args:\n    is_training: A boolean denoting whether the input is for training.\n    data_dir: The directory containing the input data.\n    batch_size: The number of samples per batch.\n    num_epochs: The number of epochs to repeat the dataset.\n  Returns:\n    A tuple of images and labels.\n  \"\"\"\n  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(is_training, data_dir))\n  dataset = dataset.flat_map(tf.data.TFRecordDataset)\n  if is_training:\n    dataset = dataset.shuffle(buffer_size=_NUM_IMAGES['train'])\n\n  dataset = dataset.map(parse_record)\n  dataset = dataset.map(\n      lambda image, label, idx: preprocess_image(image, label, idx, is_training),\n      num_parallel_calls=1)\n\n  # We call repeat after shuffling, rather than before, to prevent separate\n  # epochs from blending together.\n  dataset = dataset.repeat(num_epochs)\n  dataset = dataset.batch(batch_size)\n  dataset = dataset.prefetch(buffer_size=FLAGS.prefetch_buffer_size)\n\n  iterator = dataset.make_one_shot_iterator()\n  images, labels, idx = iterator.get_next()\n\n  return images, labels", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:_no_\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  _Linux Ubuntu 16.04.4 LTS_\u3002\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:_no_\r\n- **TensorFlow installed from (source or binary)**:_binary_\r\n- **TensorFlow version (use command below)**:_1.8.0_\r\n- **Python version**:_Python 3.5.2_\r\n- **Bazel version (if compiling from source)**:_None_\r\n- **GCC/Compiler version (if compiling from source)**:_None_\r\n- **CUDA/cuDNN version**:_CUDA-9.0/cuDNN-7.0.5_\r\n- **GPU model and memory**: _GTX 1080Ti 11GB, 2 same cards_\r\n- **Exact command to reproduce**:_below_\r\n\r\n### Supplementary material\r\nI just pulled the tensorflow's offical docker image and RUN some `pip install ...` instructions.\r\nThe tag of the iamge is `1.8.0-devel-gpu-py3`.\r\n\r\n### Describe the problem\r\nI want to calculate a focal loss. \r\n![image](https://user-images.githubusercontent.com/22476764/45917573-bb403600-beac-11e8-8262-56be63acd1c3.png)\r\n\r\nIt just added weight factor to the original cross entropy loss.\r\n\r\n#### `tf_leak` code (shown in Source code / logs) \r\n\r\nWhen I used the `tf_leak` code as shown below, there will be memory leak.\r\nI used `top` command to see the process's memory usage.\r\n\r\n- **pic01 occupy about 3G memory after about 1min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917603-212cbd80-bead-11e8-8b2b-e784196405b2.png)\r\n\r\n- **pic02 occupy about 11G memory after about 33min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917609-3ace0500-bead-11e8-9d6e-b58d748f6fbc.png)\r\n\r\n- **pic03 occupy about 14G memory after about 191min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917619-45889a00-bead-11e8-8370-082b341c4907.png)\r\n\r\n**I use the `tf.estimator` and the graph is finazied. So I do not add ops to the graph when the seesion is running.**\r\n\r\n- **pic04 graph is finazied**\r\n![image](https://user-images.githubusercontent.com/22476764/45917706-16bef380-beae-11e8-8af9-a0a8fd46c52a.png)\r\n\r\n\r\n#### `tf_no_leak` code (shown in Source code / logs) \r\n \r\nI replace the ` tf.dynamic_partition` function with `tf.boolean_mask` and the memory leak disappered.\r\n\r\n- **pic05 occupy about 3G memory after about 9min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917758-ec216a80-beae-11e8-9d49-115f2598fa50.png)\r\n\r\n- **pic06 occupy about 3G memory after about 133min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917782-2be85200-beaf-11e8-9427-021f2f5fcd51.png)\r\n\r\n#### more information\r\n\r\nI tested the `tf_leak` and `tf_no_leak` code with a small dataset, when I use a bigger dataset, the process will occupy a large memory and the system will kill the process. This is why I found the problem.\r\n\r\n\r\n### Source code / logs\r\n```\r\nparams:\r\n\t\tvalid_labels\t:\tthe labels, a Tensor with shape [N * w * h, ]\r\n\t\tvalid_logits\t:\tthe logits, a Tensor with shape [N * w * h, num_classes]\r\n\t\tnum_classes \t: \tthe number of classes, a scalar\r\n                ohem_prob_g  :       a hyperparameter, a scalar, default=1.5\r\n```\r\n\r\n1. `tf_leak` code\r\n\r\n```\t\r\nvalid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \\\r\n\t\t\t\t\tclip_value_min=1e-6, clip_value_max=1.0\r\nvalid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)\r\nweight_matrix = ohem_prob_g - tf.dynamic_partition(valid_probs, valid_one_hot, num_partitions=2)[1]\r\n## calculate the final cross-entropy\r\ncross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, \r\n\t\t\t\t\t\tlogits=valid_logits, weights=weight_matrix)\r\nreturn cross_entropy\r\n```\r\n\r\n2. `tf_no_leak` code\r\n\r\nsame as the `tf_leak` code except I replace the `tf.dynamic_partition` with `tf.boolean_mask`.\r\n\r\n```\r\nvalid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \\\r\n\t\t\t\t\tclip_value_min=1e-6, clip_value_max=1.0)\r\nvalid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)\r\n## calculate the final cross-entropy\r\nweight_matrix = ohem_prob_g - tf.boolean_mask(valid_probs, valid_one_hot)\r\ncross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, \r\n\t\t\t\t\t\tlogits=valid_logits, weights=weight_matrix)\r\nreturn cross_entropy\r\n```\r\n\r\n3. input code used for the `tf.estimator`\r\n\r\n```\r\ndef input_fn(is_training, data_dir, batch_size, num_epochs=1):\r\n  \"\"\"Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\r\n  Args:\r\n    is_training: A boolean denoting whether the input is for training.\r\n    data_dir: The directory containing the input data.\r\n    batch_size: The number of samples per batch.\r\n    num_epochs: The number of epochs to repeat the dataset.\r\n  Returns:\r\n    A tuple of images and labels.\r\n  \"\"\"\r\n  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(is_training, data_dir))\r\n  dataset = dataset.flat_map(tf.data.TFRecordDataset)\r\n  if is_training:\r\n    dataset = dataset.shuffle(buffer_size=_NUM_IMAGES['train'])\r\n\r\n  dataset = dataset.map(parse_record)\r\n  dataset = dataset.map(\r\n      lambda image, label, idx: preprocess_image(image, label, idx, is_training),\r\n      num_parallel_calls=1)\r\n\r\n  # We call repeat after shuffling, rather than before, to prevent separate\r\n  # epochs from blending together.\r\n  dataset = dataset.repeat(num_epochs)\r\n  dataset = dataset.batch(batch_size)\r\n  dataset = dataset.prefetch(buffer_size=FLAGS.prefetch_buffer_size)\r\n\r\n  iterator = dataset.make_one_shot_iterator()\r\n  images, labels, idx = iterator.get_next()\r\n\r\n  return images, labels\r\n\r\n```\r\n"}