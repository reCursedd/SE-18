{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5829", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5829/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5829/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5829/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5829", "id": 191487472, "node_id": "MDU6SXNzdWUxOTE0ODc0NzI=", "number": 5829, "title": "sampled_softmax souldn't be linear with respect to vocabulary size, but actually is", "user": {"login": "pltrdy", "id": 6375843, "node_id": "MDQ6VXNlcjYzNzU4NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6375843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pltrdy", "html_url": "https://github.com/pltrdy", "followers_url": "https://api.github.com/users/pltrdy/followers", "following_url": "https://api.github.com/users/pltrdy/following{/other_user}", "gists_url": "https://api.github.com/users/pltrdy/gists{/gist_id}", "starred_url": "https://api.github.com/users/pltrdy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pltrdy/subscriptions", "organizations_url": "https://api.github.com/users/pltrdy/orgs", "repos_url": "https://api.github.com/users/pltrdy/repos", "events_url": "https://api.github.com/users/pltrdy/events{/privacy}", "received_events_url": "https://api.github.com/users/pltrdy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2016-11-24T10:43:57Z", "updated_at": "2016-11-25T08:14:56Z", "closed_at": "2016-11-25T08:14:56Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard1/tf.nn.sampled_softmax_loss.md\">tf.nn.sampled_softmax_loss API doc page</a> tells us to get into Section 3 of <a href=\"https://arxiv.org/abs/1412.2007\" rel=\"nofollow\">Jean et al., 2014</a> (<a href=\"https://arxiv.org/pdf/1412.2007v2.pdf\" rel=\"nofollow\">pdf</a>) for more information about it.</p>\n<p>And actually, they quite start sec. 3.1saying:</p>\n<blockquote>\n<p>\"With  the  proposed  approach,  the  computational complexity of training becomes constant with respect to the size of the target vocabulary\"</p>\n</blockquote>\n<p>Which, kept my attention.<br>\nI ran a benchmark using my <a href=\"https://github.com/pltrdy/tf_rnnlm\">custom RNN LM script</a> (derived from <a href=\"https://github.com/tensorflow/tensorflow/blob/282823b877f173e6a33bbc9d4b9ad7dd8413ada6/tensorflow/models/rnn/ptb/ptb_word_lm.py\">tensorflow 0.11 ptb_word_lm.py</a>.</p>\n<p><a href=\"https://github.com/pltrdy/tf_rnnlm/blob/master/benchmark.md\">Results shows</a> that, only changing vocab_size increases computation time linearly. (the benchmark is using <a href=\"https://github.com/tensorflow/tensorflow/blob/282823b877f173e6a33bbc9d4b9ad7dd8413ada6/tensorflow/models/rnn/ptb/ptb_word_lm.py#L189\">'SmallConfig'</a> with <code>vocab_size</code> of either 10k or 150k.</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04.</p>\n<p>Installed version of CUDA and cuDNN:</p>\n<pre><code>$ ls -l /usr/local/cuda-8.0/lib64/libcud*\n-rw-r--r-- 1 root root 546K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root   16 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so -&gt; libcudart.so.8.0*\nlrwxrwxrwx 1 root root   19 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44*\n-rwxr-xr-x 1 root root 406K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44*\n-rw-r--r-- 1 root root 757K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart_static.a\n</code></pre>\n<h4>TensorFlow Version:</h4>\n<p>0.11RC2</p>\n<p>Thx for your reading &amp; feebacks<br>\n<code>pltrdy</code></p>", "body_text": "Hi,\ntf.nn.sampled_softmax_loss API doc page tells us to get into Section 3 of Jean et al., 2014 (pdf) for more information about it.\nAnd actually, they quite start sec. 3.1saying:\n\n\"With  the  proposed  approach,  the  computational complexity of training becomes constant with respect to the size of the target vocabulary\"\n\nWhich, kept my attention.\nI ran a benchmark using my custom RNN LM script (derived from tensorflow 0.11 ptb_word_lm.py.\nResults shows that, only changing vocab_size increases computation time linearly. (the benchmark is using 'SmallConfig' with vocab_size of either 10k or 150k.\nEnvironment info\nOperating System: Ubuntu 14.04.\nInstalled version of CUDA and cuDNN:\n$ ls -l /usr/local/cuda-8.0/lib64/libcud*\n-rw-r--r-- 1 root root 546K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root   16 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0*\nlrwxrwxrwx 1 root root   19 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44*\n-rwxr-xr-x 1 root root 406K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44*\n-rw-r--r-- 1 root root 757K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart_static.a\n\nTensorFlow Version:\n0.11RC2\nThx for your reading & feebacks\npltrdy", "body": "Hi,\r\n\r\n[tf.nn.sampled_softmax_loss API doc page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard1/tf.nn.sampled_softmax_loss.md) tells us to get into Section 3 of [Jean et al., 2014](https://arxiv.org/abs/1412.2007) ([pdf](https://arxiv.org/pdf/1412.2007v2.pdf)) for more information about it.\r\n\r\nAnd actually, they quite start sec. 3.1saying:\r\n> \"With  the  proposed  approach,  the  computational complexity of training becomes constant with respect to the size of the target vocabulary\"\r\n\r\nWhich, kept my attention.   \r\nI ran a benchmark using my [custom RNN LM script](https://github.com/pltrdy/tf_rnnlm) (derived from [tensorflow 0.11 ptb_word_lm.py](https://github.com/tensorflow/tensorflow/blob/282823b877f173e6a33bbc9d4b9ad7dd8413ada6/tensorflow/models/rnn/ptb/ptb_word_lm.py).    \r\n\r\n[Results shows](https://github.com/pltrdy/tf_rnnlm/blob/master/benchmark.md) that, only changing vocab_size increases computation time linearly. (the benchmark is using ['SmallConfig'](https://github.com/tensorflow/tensorflow/blob/282823b877f173e6a33bbc9d4b9ad7dd8413ada6/tensorflow/models/rnn/ptb/ptb_word_lm.py#L189) with `vocab_size` of either 10k or 150k.\r\n\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n$ ls -l /usr/local/cuda-8.0/lib64/libcud*\r\n-rw-r--r-- 1 root root 546K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root   16 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0*\r\nlrwxrwxrwx 1 root root   19 oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44*\r\n-rwxr-xr-x 1 root root 406K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44*\r\n-rw-r--r-- 1 root root 757K oct.  21 10:20 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\n```\r\n\r\n\r\n#### TensorFlow Version:\r\n0.11RC2\r\n\r\n\r\n\r\nThx for your reading & feebacks\r\n`pltrdy`\r\n"}