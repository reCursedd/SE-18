{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/159452470", "html_url": "https://github.com/tensorflow/tensorflow/issues/333#issuecomment-159452470", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/333", "id": 159452470, "node_id": "MDEyOklzc3VlQ29tbWVudDE1OTQ1MjQ3MA==", "user": {"login": "touts", "id": 15698149, "node_id": "MDQ6VXNlcjE1Njk4MTQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15698149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/touts", "html_url": "https://github.com/touts", "followers_url": "https://api.github.com/users/touts/followers", "following_url": "https://api.github.com/users/touts/following{/other_user}", "gists_url": "https://api.github.com/users/touts/gists{/gist_id}", "starred_url": "https://api.github.com/users/touts/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/touts/subscriptions", "organizations_url": "https://api.github.com/users/touts/orgs", "repos_url": "https://api.github.com/users/touts/repos", "events_url": "https://api.github.com/users/touts/events{/privacy}", "received_events_url": "https://api.github.com/users/touts/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-25T01:02:53Z", "updated_at": "2015-11-25T01:02:53Z", "author_association": "NONE", "body_html": "<p>The error indicates that the optimizer could not find any gradients for the variables to optimize.</p>\n<p>This can be caused by several problems:<br>\n1- The optimizer could not find any variable to optimize.<br>\n2- The loss does not depend on any variable.</p>\n<p>For 1-: Optimizers look  for trainable variables in the default graph. They use <code>tf.trainable_variables()</code> as the list of variables to optimize.  So try calling that function and print its output before calling <code>.minimize()</code>.  If you get an empty list then the optimizer won't find variables either.</p>\n<p>Why can't it find variables?</p>\n<ul>\n<li>The variables are looked for in the <em>default graph,</em> which is a thread-local  variable.  You mention \"outside the main thread\" which hints that your program is using multiple python threads.  If this is the case you need to pass the  default graph from the main thread to the other thread and install it as the default graph.  See <code>tf.get_default_graph()</code> and <code>Graph.as_default()</code> in <a href=\"http://tensorflow.org/api_docs/python/framework.md#Graph\" rel=\"nofollow\">http://tensorflow.org/api_docs/python/framework.md#Graph</a> .</li>\n<li>Maybe you have not created any variables?  Or you only create variables with  <code>trainable=False</code>?</li>\n</ul>\n<p>For 2-: The optimizer computes gradients of the loss with respect to variables.   If there are  no variables there are no gradients and nothing to optimize.  For example, if you pass a constant to <code>minimize()</code> nothing can be done.  Now, the fact that you say it works when called from the same file seems to rule out that possibility, but check anyway.</p>\n<p>Finally: The error message clearly needs to be improved.</p>", "body_text": "The error indicates that the optimizer could not find any gradients for the variables to optimize.\nThis can be caused by several problems:\n1- The optimizer could not find any variable to optimize.\n2- The loss does not depend on any variable.\nFor 1-: Optimizers look  for trainable variables in the default graph. They use tf.trainable_variables() as the list of variables to optimize.  So try calling that function and print its output before calling .minimize().  If you get an empty list then the optimizer won't find variables either.\nWhy can't it find variables?\n\nThe variables are looked for in the default graph, which is a thread-local  variable.  You mention \"outside the main thread\" which hints that your program is using multiple python threads.  If this is the case you need to pass the  default graph from the main thread to the other thread and install it as the default graph.  See tf.get_default_graph() and Graph.as_default() in http://tensorflow.org/api_docs/python/framework.md#Graph .\nMaybe you have not created any variables?  Or you only create variables with  trainable=False?\n\nFor 2-: The optimizer computes gradients of the loss with respect to variables.   If there are  no variables there are no gradients and nothing to optimize.  For example, if you pass a constant to minimize() nothing can be done.  Now, the fact that you say it works when called from the same file seems to rule out that possibility, but check anyway.\nFinally: The error message clearly needs to be improved.", "body": "The error indicates that the optimizer could not find any gradients for the variables to optimize.\n\nThis can be caused by several problems:\n 1- The optimizer could not find any variable to optimize.\n 2- The loss does not depend on any variable.\n\nFor 1-: Optimizers look  for trainable variables in the default graph. They use `tf.trainable_variables()` as the list of variables to optimize.  So try calling that function and print its output before calling `.minimize()`.  If you get an empty list then the optimizer won't find variables either.\n\nWhy can't it find variables?\n-  The variables are looked for in the _default graph,_ which is a thread-local  variable.  You mention \"outside the main thread\" which hints that your program is using multiple python threads.  If this is the case you need to pass the  default graph from the main thread to the other thread and install it as the default graph.  See `tf.get_default_graph()` and `Graph.as_default()` in http://tensorflow.org/api_docs/python/framework.md#Graph .\n-  Maybe you have not created any variables?  Or you only create variables with  `trainable=False`?\n\nFor 2-: The optimizer computes gradients of the loss with respect to variables.   If there are  no variables there are no gradients and nothing to optimize.  For example, if you pass a constant to `minimize()` nothing can be done.  Now, the fact that you say it works when called from the same file seems to rule out that possibility, but check anyway.\n\nFinally: The error message clearly needs to be improved.\n"}