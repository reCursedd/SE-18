{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/322347265", "html_url": "https://github.com/tensorflow/tensorflow/issues/12268#issuecomment-322347265", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12268", "id": 322347265, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjM0NzI2NQ==", "user": {"login": "zakizhou", "id": 19201532, "node_id": "MDQ6VXNlcjE5MjAxNTMy", "avatar_url": "https://avatars0.githubusercontent.com/u/19201532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zakizhou", "html_url": "https://github.com/zakizhou", "followers_url": "https://api.github.com/users/zakizhou/followers", "following_url": "https://api.github.com/users/zakizhou/following{/other_user}", "gists_url": "https://api.github.com/users/zakizhou/gists{/gist_id}", "starred_url": "https://api.github.com/users/zakizhou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zakizhou/subscriptions", "organizations_url": "https://api.github.com/users/zakizhou/orgs", "repos_url": "https://api.github.com/users/zakizhou/repos", "events_url": "https://api.github.com/users/zakizhou/events{/privacy}", "received_events_url": "https://api.github.com/users/zakizhou/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-15T00:42:59Z", "updated_at": "2017-08-15T00:42:59Z", "author_association": "NONE", "body_html": "<p>Thanks! <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6510203\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reedwm\">@reedwm</a></p>\n<p>This is the source code of definition of vgg16 in slim:</p>\n<pre><code>def vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope='vgg_16'):\n  \"\"\"Oxford Net VGG 16-Layers version D Example.\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  \"\"\"\n  with variable_scope.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n    end_points_collection = sc.original_name_scope + '_end_points'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with arg_scope(\n        [layers.conv2d, layers_lib.fully_connected, layers_lib.max_pool2d],\n        outputs_collections=end_points_collection):\n      net = layers_lib.repeat(\n          inputs, 2, layers.conv2d, 64, [3, 3], scope='conv1')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool1')\n      net = layers_lib.repeat(net, 2, layers.conv2d, 128, [3, 3], scope='conv2')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool2')\n      net = layers_lib.repeat(net, 3, layers.conv2d, 256, [3, 3], scope='conv3')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool3')\n      net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv4')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool4')\n      net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv5')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool5')\n      # Use conv2d instead of fully_connected layers.\n      net = layers.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n      net = layers_lib.dropout(\n          net, dropout_keep_prob, is_training=is_training, scope='dropout6')\n      net = layers.conv2d(net, 4096, [1, 1], scope='fc7')\n      net = layers_lib.dropout(\n          net, dropout_keep_prob, is_training=is_training, scope='dropout7')\n      net = layers.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          scope='fc8')\n      # Convert end_points_collection into a end_point dict.\n      end_points = utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = array_ops.squeeze(net, [1, 2], name='fc8/squeezed')\n        end_points[sc.name + '/fc8'] = net\n      return net, end_points\n</code></pre>\n<p>As you can see,  both functions(conv2d and maxpooling) use default padding argument which is \"same\" for conv2d and \"valid\" for maxpooling, for an input image with shape <code>[None, 224, 224, 3]</code> where 224 is a multiple of 32, \"valid\" padding of maxpooling has the same effect with \"same\" padding, but if the shape is <code>[None, 300, 300, 3]</code>(which is the input shape of ssd), output shape of \"valid\" padding is different from \"same\", so I want an argument in the definition of <code>vgg_16</code> to determine whether the padding for maxpooling is \"same\" or \"valid\".</p>", "body_text": "Thanks! @reedwm\nThis is the source code of definition of vgg16 in slim:\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope='vgg_16'):\n  \"\"\"Oxford Net VGG 16-Layers version D Example.\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  \"\"\"\n  with variable_scope.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n    end_points_collection = sc.original_name_scope + '_end_points'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with arg_scope(\n        [layers.conv2d, layers_lib.fully_connected, layers_lib.max_pool2d],\n        outputs_collections=end_points_collection):\n      net = layers_lib.repeat(\n          inputs, 2, layers.conv2d, 64, [3, 3], scope='conv1')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool1')\n      net = layers_lib.repeat(net, 2, layers.conv2d, 128, [3, 3], scope='conv2')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool2')\n      net = layers_lib.repeat(net, 3, layers.conv2d, 256, [3, 3], scope='conv3')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool3')\n      net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv4')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool4')\n      net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv5')\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool5')\n      # Use conv2d instead of fully_connected layers.\n      net = layers.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n      net = layers_lib.dropout(\n          net, dropout_keep_prob, is_training=is_training, scope='dropout6')\n      net = layers.conv2d(net, 4096, [1, 1], scope='fc7')\n      net = layers_lib.dropout(\n          net, dropout_keep_prob, is_training=is_training, scope='dropout7')\n      net = layers.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          scope='fc8')\n      # Convert end_points_collection into a end_point dict.\n      end_points = utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = array_ops.squeeze(net, [1, 2], name='fc8/squeezed')\n        end_points[sc.name + '/fc8'] = net\n      return net, end_points\n\nAs you can see,  both functions(conv2d and maxpooling) use default padding argument which is \"same\" for conv2d and \"valid\" for maxpooling, for an input image with shape [None, 224, 224, 3] where 224 is a multiple of 32, \"valid\" padding of maxpooling has the same effect with \"same\" padding, but if the shape is [None, 300, 300, 3](which is the input shape of ssd), output shape of \"valid\" padding is different from \"same\", so I want an argument in the definition of vgg_16 to determine whether the padding for maxpooling is \"same\" or \"valid\".", "body": "Thanks! @reedwm \r\n\r\nThis is the source code of definition of vgg16 in slim:\r\n```\r\ndef vgg_16(inputs,\r\n           num_classes=1000,\r\n           is_training=True,\r\n           dropout_keep_prob=0.5,\r\n           spatial_squeeze=True,\r\n           scope='vgg_16'):\r\n  \"\"\"Oxford Net VGG 16-Layers version D Example.\r\n  Note: All the fully_connected layers have been transformed to conv2d layers.\r\n        To use in classification mode, resize input to 224x224.\r\n  Args:\r\n    inputs: a tensor of size [batch_size, height, width, channels].\r\n    num_classes: number of predicted classes.\r\n    is_training: whether or not the model is being trained.\r\n    dropout_keep_prob: the probability that activations are kept in the dropout\r\n      layers during training.\r\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\r\n      outputs. Useful to remove unnecessary dimensions for classification.\r\n    scope: Optional scope for the variables.\r\n  Returns:\r\n    the last op containing the log predictions and end_points dict.\r\n  \"\"\"\r\n  with variable_scope.variable_scope(scope, 'vgg_16', [inputs]) as sc:\r\n    end_points_collection = sc.original_name_scope + '_end_points'\r\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\r\n    with arg_scope(\r\n        [layers.conv2d, layers_lib.fully_connected, layers_lib.max_pool2d],\r\n        outputs_collections=end_points_collection):\r\n      net = layers_lib.repeat(\r\n          inputs, 2, layers.conv2d, 64, [3, 3], scope='conv1')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool1')\r\n      net = layers_lib.repeat(net, 2, layers.conv2d, 128, [3, 3], scope='conv2')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool2')\r\n      net = layers_lib.repeat(net, 3, layers.conv2d, 256, [3, 3], scope='conv3')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool3')\r\n      net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv4')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool4')\r\n      net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv5')\r\n      net = layers_lib.max_pool2d(net, [2, 2], scope='pool5')\r\n      # Use conv2d instead of fully_connected layers.\r\n      net = layers.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\r\n      net = layers_lib.dropout(\r\n          net, dropout_keep_prob, is_training=is_training, scope='dropout6')\r\n      net = layers.conv2d(net, 4096, [1, 1], scope='fc7')\r\n      net = layers_lib.dropout(\r\n          net, dropout_keep_prob, is_training=is_training, scope='dropout7')\r\n      net = layers.conv2d(\r\n          net,\r\n          num_classes, [1, 1],\r\n          activation_fn=None,\r\n          normalizer_fn=None,\r\n          scope='fc8')\r\n      # Convert end_points_collection into a end_point dict.\r\n      end_points = utils.convert_collection_to_dict(end_points_collection)\r\n      if spatial_squeeze:\r\n        net = array_ops.squeeze(net, [1, 2], name='fc8/squeezed')\r\n        end_points[sc.name + '/fc8'] = net\r\n      return net, end_points\r\n```\r\nAs you can see,  both functions(conv2d and maxpooling) use default padding argument which is \"same\" for conv2d and \"valid\" for maxpooling, for an input image with shape `[None, 224, 224, 3]` where 224 is a multiple of 32, \"valid\" padding of maxpooling has the same effect with \"same\" padding, but if the shape is `[None, 300, 300, 3]`(which is the input shape of ssd), output shape of \"valid\" padding is different from \"same\", so I want an argument in the definition of `vgg_16` to determine whether the padding for maxpooling is \"same\" or \"valid\"."}