{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/425061441", "html_url": "https://github.com/tensorflow/tensorflow/pull/22562#issuecomment-425061441", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22562", "id": 425061441, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTA2MTQ0MQ==", "user": {"login": "superbobry", "id": 185856, "node_id": "MDQ6VXNlcjE4NTg1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/185856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/superbobry", "html_url": "https://github.com/superbobry", "followers_url": "https://api.github.com/users/superbobry/followers", "following_url": "https://api.github.com/users/superbobry/following{/other_user}", "gists_url": "https://api.github.com/users/superbobry/gists{/gist_id}", "starred_url": "https://api.github.com/users/superbobry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/superbobry/subscriptions", "organizations_url": "https://api.github.com/users/superbobry/orgs", "repos_url": "https://api.github.com/users/superbobry/repos", "events_url": "https://api.github.com/users/superbobry/events{/privacy}", "received_events_url": "https://api.github.com/users/superbobry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-27T11:51:52Z", "updated_at": "2018-09-27T11:51:52Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7946809\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gunan\">@gunan</a> a few questions to drop the [WIP] status:</p>\n<ul>\n<li>Are you OK with having the same copy-pasted shim in both cuda_driver.cc and gpu_cudamalloc_allocator.cc? As an alternative, I can make gpu_cudamalloc_allocator use  cuda_driver, or extract the wrappers into a separate file.</li>\n<li>What would be the best way to eliminate the remaining dependency on cudart? The usages are scattered through multiple subsystems, so the local shim trick does not apply. I could create a wrapper in a separate file, e.g. (common_runtime/gpu/gpu_cudart.*) and change the usages to call the wrapped functions.</li>\n<li>Ideally, I'd like to ensure that if CUDA is not available, one cannot register a GPU device. Could you point me to the part of the code responsible for that?</li>\n</ul>", "body_text": "@gunan a few questions to drop the [WIP] status:\n\nAre you OK with having the same copy-pasted shim in both cuda_driver.cc and gpu_cudamalloc_allocator.cc? As an alternative, I can make gpu_cudamalloc_allocator use  cuda_driver, or extract the wrappers into a separate file.\nWhat would be the best way to eliminate the remaining dependency on cudart? The usages are scattered through multiple subsystems, so the local shim trick does not apply. I could create a wrapper in a separate file, e.g. (common_runtime/gpu/gpu_cudart.*) and change the usages to call the wrapped functions.\nIdeally, I'd like to ensure that if CUDA is not available, one cannot register a GPU device. Could you point me to the part of the code responsible for that?", "body": "@gunan a few questions to drop the [WIP] status:\r\n\r\n* Are you OK with having the same copy-pasted shim in both cuda_driver.cc and gpu_cudamalloc_allocator.cc? As an alternative, I can make gpu_cudamalloc_allocator use  cuda_driver, or extract the wrappers into a separate file.\r\n* What would be the best way to eliminate the remaining dependency on cudart? The usages are scattered through multiple subsystems, so the local shim trick does not apply. I could create a wrapper in a separate file, e.g. (common_runtime/gpu/gpu_cudart.*) and change the usages to call the wrapped functions.\r\n* Ideally, I'd like to ensure that if CUDA is not available, one cannot register a GPU device. Could you point me to the part of the code responsible for that?"}