{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11337", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11337/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11337/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11337/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11337", "id": 241098893, "node_id": "MDU6SXNzdWUyNDEwOTg4OTM=", "number": 11337, "title": "get error in tensorflow 1.2 but the code works on tensorflow 1.0", "user": {"login": "songhuiming", "id": 1667488, "node_id": "MDQ6VXNlcjE2Njc0ODg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1667488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/songhuiming", "html_url": "https://github.com/songhuiming", "followers_url": "https://api.github.com/users/songhuiming/followers", "following_url": "https://api.github.com/users/songhuiming/following{/other_user}", "gists_url": "https://api.github.com/users/songhuiming/gists{/gist_id}", "starred_url": "https://api.github.com/users/songhuiming/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/songhuiming/subscriptions", "organizations_url": "https://api.github.com/users/songhuiming/orgs", "repos_url": "https://api.github.com/users/songhuiming/repos", "events_url": "https://api.github.com/users/songhuiming/events{/privacy}", "received_events_url": "https://api.github.com/users/songhuiming/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2017-07-06T22:00:47Z", "updated_at": "2018-03-27T18:11:30Z", "closed_at": "2018-03-27T18:11:30Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I have the following code. I can run it successfully in tf v1.0 but it failed in tf v1.2. The error is from the (inputs, state) in the GRUCell. Can you help me understand why the errors come from?</p>\n<p>Thank you.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span>  tensorflow.contrib.learn.python.learn.estimators.dnn  <span class=\"pl-k\">import</span> DNNClassifier\n<span class=\"pl-k\">from</span> tensorflow.contrib.layers <span class=\"pl-k\">import</span> real_valued_column\n<span class=\"pl-k\">from</span> tensorflow.contrib.layers.python.layers.initializers <span class=\"pl-k\">import</span> xavier_initializer\n\ndropout<span class=\"pl-k\">=</span><span class=\"pl-c1\">0.2</span>\nhidden_1_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\nhidden_2_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">250</span>\n<span class=\"pl-c1\">NUM_EPOCHS</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>\n<span class=\"pl-c1\">BATCH_SIZE</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">50</span>\nlr<span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0001</span>\n\nnum_features <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2328</span>\n<span class=\"pl-c1\">RNN_HIDDEN_SIZE</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>\n<span class=\"pl-c1\">FIRST_LAYER_SIZE</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>\n<span class=\"pl-c1\">SECOND_LAYER_SIZE</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">250</span>\n<span class=\"pl-c1\">NUM_LAYERS</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>\n<span class=\"pl-c1\">BATCH_SIZE</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">50</span>\n<span class=\"pl-c1\">NUM_EPOCHS</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>\nlr<span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0003</span>\n<span class=\"pl-c1\">ATTN_LENGTH</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">30</span>\nbeta<span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">RNNModel</span>():\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        global_step <span class=\"pl-k\">=</span> tf.contrib.framework.get_or_create_global_step()\n        <span class=\"pl-c1\">self</span>.input_data <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">BATCH_SIZE</span>,num_features])\n        <span class=\"pl-c1\">self</span>.target_data <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32,<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">BATCH_SIZE</span>])\n        <span class=\"pl-c1\">self</span>.dropout_prob <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[])\n        \n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">makeGRUCells</span>():\n            base_cell <span class=\"pl-k\">=</span> tf.contrib.rnn.GRUCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">RNN_HIDDEN_SIZE</span>,) \n            layered_cell <span class=\"pl-k\">=</span> tf.contrib.rnn.MultiRNNCell([base_cell] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">NUM_LAYERS</span>,<span class=\"pl-v\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>) \n            attn_cell <span class=\"pl-k\">=</span>tf.contrib.rnn.AttentionCellWrapper(<span class=\"pl-v\">cell</span><span class=\"pl-k\">=</span>layered_cell,<span class=\"pl-v\">attn_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">ATTN_LENGTH</span>,<span class=\"pl-v\">state_is_tuple</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n            <span class=\"pl-k\">return</span> attn_cell\n        \n        <span class=\"pl-c1\">self</span>.gru_cell <span class=\"pl-k\">=</span> makeGRUCells()\n        <span class=\"pl-c1\">self</span>.zero_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.gru_cell.zero_state(<span class=\"pl-c1\">1</span>, tf.float32)\n        \n        <span class=\"pl-c1\">self</span>.start_state <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">self</span>.gru_cell.state_size])\n        <span class=\"pl-c1\">print</span>((<span class=\"pl-c1\">self</span>.start_state))\n        \n        \n\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fff<span class=\"pl-pds\">\"</span></span>,<span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>xavier_initializer(<span class=\"pl-v\">uniform</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">reuse</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>):\n            droped_input <span class=\"pl-k\">=</span> tf.nn.dropout(<span class=\"pl-c1\">self</span>.input_data,<span class=\"pl-v\">keep_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dropout_prob)\n            \n            layer_1 <span class=\"pl-k\">=</span> tf.contrib.layers.fully_connected(\n                <span class=\"pl-v\">num_outputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FIRST_LAYER_SIZE</span>,\n                <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>droped_input,\n            )\n            layer_2 <span class=\"pl-k\">=</span> tf.contrib.layers.fully_connected(\n                <span class=\"pl-v\">num_outputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">RNN_HIDDEN_SIZE</span>,\n                <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>layer_1,\n            )\n            \n        \n        split_inputs <span class=\"pl-k\">=</span> tf.reshape(droped_input,<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">BATCH_SIZE</span>,num_features],<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>reshape_l1<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Each item in the batch is a time step, iterate through them</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(split_inputs.shape)</span>\n        split_inputs <span class=\"pl-k\">=</span> tf.unstack(split_inputs,<span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>unpack_l1<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(\"lentgh is \" + str(len(split_inputs)))</span>\n        states <span class=\"pl-k\">=</span>[]\n        outputs <span class=\"pl-k\">=</span>[]\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rnn<span class=\"pl-pds\">\"</span></span>,<span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>xavier_initializer(<span class=\"pl-v\">uniform</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>), <span class=\"pl-v\">reuse</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>) <span class=\"pl-k\">as</span> scope:\n            state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.start_state\n            <span class=\"pl-k\">for</span> i, inp <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(split_inputs):\n                <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span><span class=\"pl-c1\">0</span>:\n                    scope.reuse_variables()\n                <span class=\"pl-c1\">print</span>((state))\n                <span class=\"pl-c1\">print</span>((inp))\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>this is for <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(i))\n                output, state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.gru_cell(<span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> inp, <span class=\"pl-v\">state</span> <span class=\"pl-k\">=</span> state)\n                states.append(state)\n                outputs.append(output)\n        <span class=\"pl-c1\">self</span>.end_state <span class=\"pl-k\">=</span> states[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n        outputs <span class=\"pl-k\">=</span> tf.stack(outputs,<span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Pack them back into a single tensor</span>\n        outputs <span class=\"pl-k\">=</span> tf.reshape(outputs,<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">BATCH_SIZE</span>,<span class=\"pl-c1\">RNN_HIDDEN_SIZE</span>])\n        <span class=\"pl-c1\">self</span>.logits <span class=\"pl-k\">=</span> tf.contrib.layers.fully_connected(\n            <span class=\"pl-v\">num_outputs</span><span class=\"pl-k\">=</span>num_classes,\n            <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>outputs,\n            <span class=\"pl-v\">activation_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>\n        )\n\n            \n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loss<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">reuse</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>):\n            <span class=\"pl-c1\">self</span>.penalties <span class=\"pl-k\">=</span>    tf.reduce_sum([beta<span class=\"pl-k\">*</span>tf.nn.l2_loss(var) <span class=\"pl-k\">for</span> var <span class=\"pl-k\">in</span> tf.trainable_variables()])\n\n            \n            <span class=\"pl-c1\">self</span>.losses <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(<span class=\"pl-v\">logits</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.logits,<span class=\"pl-v\">labels</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.target_data)\n            <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> tf.reduce_sum(<span class=\"pl-c1\">self</span>.losses <span class=\"pl-k\">+</span> beta<span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.penalties)\n        \n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train_step<span class=\"pl-pds\">\"</span></span>):\n            opt <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(lr)\n            gvs <span class=\"pl-k\">=</span> opt.compute_gradients(<span class=\"pl-c1\">self</span>.loss)\n            <span class=\"pl-c1\">self</span>.train_op <span class=\"pl-k\">=</span> opt.apply_gradients(gvs, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step)\n        \n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>predictions<span class=\"pl-pds\">\"</span></span>):\n            probs <span class=\"pl-k\">=</span> tf.nn.softmax(<span class=\"pl-c1\">self</span>.logits)\n            <span class=\"pl-c1\">self</span>.predictions <span class=\"pl-k\">=</span> tf.argmax(probs, <span class=\"pl-c1\">1</span>)\n            correct_pred <span class=\"pl-k\">=</span> tf.cast(tf.equal(<span class=\"pl-c1\">self</span>.predictions, tf.cast(<span class=\"pl-c1\">self</span>.target_data,tf.int64)),tf.float64)\n            <span class=\"pl-c1\">self</span>.accuracy <span class=\"pl-k\">=</span> tf.reduce_mean(correct_pred)\n\n         \n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n    model <span class=\"pl-k\">=</span> RNNModel()</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">WARNING</span>:tensorflow:<span class=\"pl-k\">&lt;</span>tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper <span class=\"pl-c1\">object</span> at <span class=\"pl-c1\"><span class=\"pl-k\">0x</span>0000000038EB4CC0</span><span class=\"pl-k\">&gt;</span>: Using a concatenated state <span class=\"pl-k\">is</span> slower <span class=\"pl-k\">and</span> will soon be deprecated.  Use state_is_tuple<span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>.\nTensor(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Placeholder_3:0<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3300</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>float32)\nTensor(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Placeholder_3:0<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3300</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>float32)\nTensor(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>unpack_l1:0<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2328</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>float32)\nthis <span class=\"pl-k\">is</span> <span class=\"pl-k\">for</span> <span class=\"pl-c1\">0</span>\n\n\n\n\n<span class=\"pl-ii\">--------------------------------------------------------------------------</span><span class=\"pl-k\">-</span>\n<span class=\"pl-c1\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">20</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">2249079aecbe</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n      <span class=\"pl-c1\">1</span> <span class=\"pl-k\">with</span> tf.Graph().as_default():\n<span class=\"pl-ii\">----</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">2</span>     model <span class=\"pl-k\">=</span> RNNModel()\n\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">19</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">58646adfd4d3</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-c1\">self</span>)\n     <span class=\"pl-c1\">48</span>                 <span class=\"pl-c1\">print</span>((inp))\n     <span class=\"pl-c1\">49</span>                 <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>this is for <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(i))\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">50</span>                 output, state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.gru_cell(<span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> inp, <span class=\"pl-v\">state</span> <span class=\"pl-k\">=</span> state)\n     <span class=\"pl-c1\">51</span>                 states.append(state)\n     <span class=\"pl-c1\">52</span>                 outputs.append(output)\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)</span>\n    <span class=\"pl-c1\">178</span>       <span class=\"pl-k\">with</span> vs.variable_scope(vs.get_variable_scope(),\n    <span class=\"pl-c1\">179</span>                              <span class=\"pl-v\">custom_getter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._rnn_get_variable):\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">180</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">super</span>(RNNCell, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__call__</span>(inputs, state)\n    <span class=\"pl-c1\">181</span> \n    <span class=\"pl-c1\">182</span>   <span class=\"pl-k\">def</span> <span class=\"pl-en\">_rnn_get_variable</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">getter</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)</span>\n    <span class=\"pl-c1\">439</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> Check input assumptions set after layer building, e.g. input shape.</span>\n    <span class=\"pl-c1\">440</span>         <span class=\"pl-c1\">self</span>._assert_input_compatibility(inputs)\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">441</span>         outputs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.call(inputs, <span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">442</span> \n    <span class=\"pl-c1\">443</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> Apply activity regularization.</span>\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn_cell.py in call(self, inputs, state)</span>\n   <span class=\"pl-c1\">1111</span>       input_size <span class=\"pl-k\">=</span> inputs.get_shape().as_list()[<span class=\"pl-c1\">1</span>]\n   <span class=\"pl-c1\">1112</span>     inputs <span class=\"pl-k\">=</span> _linear([inputs, attns], input_size, <span class=\"pl-c1\">True</span>)\n<span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">1113</span>     lstm_output, new_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._cell(inputs, state)\n   <span class=\"pl-c1\">1114</span>     <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._state_is_tuple:\n   <span class=\"pl-c1\">1115</span>       new_state_cat <span class=\"pl-k\">=</span> array_ops.concat(nest.flatten(new_state), <span class=\"pl-c1\">1</span>)\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)</span>\n    <span class=\"pl-c1\">178</span>       <span class=\"pl-k\">with</span> vs.variable_scope(vs.get_variable_scope(),\n    <span class=\"pl-c1\">179</span>                              <span class=\"pl-v\">custom_getter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._rnn_get_variable):\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">180</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">super</span>(RNNCell, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__call__</span>(inputs, state)\n    <span class=\"pl-c1\">181</span> \n    <span class=\"pl-c1\">182</span>   <span class=\"pl-k\">def</span> <span class=\"pl-en\">_rnn_get_variable</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">getter</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)</span>\n    <span class=\"pl-c1\">439</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> Check input assumptions set after layer building, e.g. input shape.</span>\n    <span class=\"pl-c1\">440</span>         <span class=\"pl-c1\">self</span>._assert_input_compatibility(inputs)\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">441</span>         outputs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.call(inputs, <span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">442</span> \n    <span class=\"pl-c1\">443</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> Apply activity regularization.</span>\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)</span>\n    <span class=\"pl-c1\">914</span>                                       [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, cell.state_size])\n    <span class=\"pl-c1\">915</span>           cur_state_pos <span class=\"pl-k\">+=</span> cell.state_size\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">916</span>         cur_inp, new_state <span class=\"pl-k\">=</span> cell(cur_inp, cur_state)\n    <span class=\"pl-c1\">917</span>         new_states.append(new_state)\n    <span class=\"pl-c1\">918</span> \n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)</span>\n    <span class=\"pl-c1\">178</span>       <span class=\"pl-k\">with</span> vs.variable_scope(vs.get_variable_scope(),\n    <span class=\"pl-c1\">179</span>                              <span class=\"pl-v\">custom_getter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._rnn_get_variable):\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">180</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">super</span>(RNNCell, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__call__</span>(inputs, state)\n    <span class=\"pl-c1\">181</span> \n    <span class=\"pl-c1\">182</span>   <span class=\"pl-k\">def</span> <span class=\"pl-en\">_rnn_get_variable</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">getter</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)</span>\n    <span class=\"pl-c1\">439</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> Check input assumptions set after layer building, e.g. input shape.</span>\n    <span class=\"pl-c1\">440</span>         <span class=\"pl-c1\">self</span>._assert_input_compatibility(inputs)\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">441</span>         outputs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.call(inputs, <span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">442</span> \n    <span class=\"pl-c1\">443</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> Apply activity regularization.</span>\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)</span>\n    <span class=\"pl-c1\">293</span>       value <span class=\"pl-k\">=</span> math_ops.sigmoid(\n    <span class=\"pl-c1\">294</span>           _linear([inputs, state], <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>._num_units, <span class=\"pl-c1\">True</span>, bias_ones,\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">295</span>                   <span class=\"pl-c1\">self</span>._kernel_initializer))\n    <span class=\"pl-c1\">296</span>       r, u <span class=\"pl-k\">=</span> array_ops.split(<span class=\"pl-v\">value</span><span class=\"pl-k\">=</span>value, <span class=\"pl-v\">num_or_size_splits</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c1\">297</span>     <span class=\"pl-k\">with</span> vs.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candidate<span class=\"pl-pds\">\"</span></span>):\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _linear(args, output_size, bias, bias_initializer, kernel_initializer)</span>\n   <span class=\"pl-c1\">1015</span>         <span class=\"pl-c1\">_WEIGHTS_VARIABLE_NAME</span>, [total_arg_size, output_size],\n   <span class=\"pl-c1\">1016</span>         dtype<span class=\"pl-k\">=</span>dtype,\n<span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">1017</span>         initializer<span class=\"pl-k\">=</span>kernel_initializer)\n   <span class=\"pl-c1\">1018</span>     <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(args) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>:\n   <span class=\"pl-c1\">1019</span>       res <span class=\"pl-k\">=</span> math_ops.matmul(args[<span class=\"pl-c1\">0</span>], weights)\n\nC:\\<span class=\"pl-ii\">Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)</span>\n   <span class=\"pl-c1\">1063</span>       collections<span class=\"pl-k\">=</span>collections, caching_device<span class=\"pl-k\">=</span>caching_device,\n   <span class=\"pl-c1\">1064</span>       partitioner<span class=\"pl-k\">=</span>partitioner, validate_shape<span class=\"pl-k\">=</span>validate_shape,\n<span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">1065</span>       use_resource<span class=\"pl-k\">=</span>use_resource, custom_getter<span class=\"pl-k\">=</span>custom_getter)\n   <span class=\"pl-c1\">1066</span> get_variable_or_local_docstring <span class=\"pl-k\">=</span> (\n   <span class=\"pl-c1\">1067</span>     <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span><span class=\"pl-c1\">%s</span></span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">C:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages<span class=\"pl-cce\">\\t</span>ensorflow\\python\\ops<span class=\"pl-cce\">\\v</span>ariable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)</span>\n<span class=\"pl-s\">    960           collections=collections, caching_device=caching_device,</span>\n<span class=\"pl-s\">    961           partitioner=partitioner, validate_shape=validate_shape,</span>\n<span class=\"pl-s\">--&gt; 962           use_resource=use_resource, custom_getter=custom_getter)</span>\n<span class=\"pl-s\">    963 </span>\n<span class=\"pl-s\">    964   def _get_partitioned_variable(self,</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">C:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages<span class=\"pl-cce\">\\t</span>ensorflow\\python\\ops<span class=\"pl-cce\">\\v</span>ariable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)</span>\n<span class=\"pl-s\">    358           reuse=reuse, trainable=trainable, collections=collections,</span>\n<span class=\"pl-s\">    359           caching_device=caching_device, partitioner=partitioner,</span>\n<span class=\"pl-s\">--&gt; 360           validate_shape=validate_shape, use_resource=use_resource)</span>\n<span class=\"pl-s\">    361     else:</span>\n<span class=\"pl-s\">    362       return _true_getter(</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">C:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages<span class=\"pl-cce\">\\t</span>ensorflow\\python\\ops<span class=\"pl-cce\">\\v</span>ariable_scope.py in wrapped_custom_getter(getter, *args, **kwargs)</span>\n<span class=\"pl-s\">   1403     return custom_getter(</span>\n<span class=\"pl-s\">   1404         functools.partial(old_getter, getter),</span>\n<span class=\"pl-s\">-&gt; 1405         *args, **kwargs)</span>\n<span class=\"pl-s\">   1406   return wrapped_custom_getter</span>\n<span class=\"pl-s\">   1407 </span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">C:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages<span class=\"pl-cce\">\\t</span>ensorflow\\python\\ops<span class=\"pl-cce\">\\r</span>nn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)</span>\n<span class=\"pl-s\">    181 </span>\n<span class=\"pl-s\">    182   def _rnn_get_variable(self, getter, *args, **kwargs):</span>\n<span class=\"pl-s\">--&gt; 183     variable = getter(*args, **kwargs)</span>\n<span class=\"pl-s\">    184     trainable = (variable in tf_variables.trainable_variables() or</span>\n<span class=\"pl-s\">    185                  (isinstance(variable, tf_variables.PartitionedVariable) and</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">C:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages<span class=\"pl-cce\">\\t</span>ensorflow\\python\\ops<span class=\"pl-cce\">\\v</span>ariable_scope.py in wrapped_custom_getter(getter, *args, **kwargs)</span>\n<span class=\"pl-s\">   1403     return custom_getter(</span>\n<span class=\"pl-s\">   1404         functools.partial(old_getter, getter),</span>\n<span class=\"pl-s\">-&gt; 1405         *args, **kwargs)</span>\n<span class=\"pl-s\">   1406   return wrapped_custom_getter</span>\n<span class=\"pl-s\">   1407 </span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">C:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages<span class=\"pl-cce\">\\t</span>ensorflow\\python\\ops<span class=\"pl-cce\">\\r</span>nn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)</span>\n<span class=\"pl-s\">    181 </span>\n<span class=\"pl-s\">    182   def _rnn_get_variable(self, getter, *args, **kwargs):</span>\n<span class=\"pl-s\">--&gt; 183     variable = getter(*args, **kwargs)</span>\n<span class=\"pl-s\">    184     trainable = (variable in tf_variables.trainable_variables() or</span>\n<span class=\"pl-s\">    185                  (isinstance(variable, tf_variables.PartitionedVariable) and</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">C:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages<span class=\"pl-cce\">\\t</span>ensorflow\\python\\ops<span class=\"pl-cce\">\\r</span>nn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)</span>\n<span class=\"pl-s\">    181 </span>\n<span class=\"pl-s\">    182   def _rnn_get_variable(self, getter, *args, **kwargs):</span>\n<span class=\"pl-s\">--&gt; 183     variable = getter(*args, **kwargs)</span>\n<span class=\"pl-s\">    184     trainable = (variable in tf_variables.trainable_variables() or</span>\n<span class=\"pl-s\">    185                  (isinstance(variable, tf_variables.PartitionedVariable) and</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">C:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages<span class=\"pl-cce\">\\t</span>ensorflow\\python\\ops<span class=\"pl-cce\">\\v</span>ariable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)</span>\n<span class=\"pl-s\">    350           trainable=trainable, collections=collections,</span>\n<span class=\"pl-s\">    351           caching_device=caching_device, validate_shape=validate_shape,</span>\n<span class=\"pl-s\">--&gt; 352           use_resource=use_resource)</span>\n<span class=\"pl-s\">    353 </span>\n<span class=\"pl-s\">    354     if custom_getter is not None:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">C:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages<span class=\"pl-cce\">\\t</span>ensorflow\\python\\ops<span class=\"pl-cce\">\\v</span>ariable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)</span>\n<span class=\"pl-s\">    667         raise ValueError(\"Trying to share variable <span class=\"pl-c1\">%s</span>, but specified shape <span class=\"pl-c1\">%s</span>\"</span>\n<span class=\"pl-s\">    668                          \" and found shape <span class=\"pl-c1\">%s</span>.\" % (name, shape,</span>\n<span class=\"pl-s\">--&gt; 669                                                    found_var.get_shape()))</span>\n<span class=\"pl-s\">    670       if not dtype.is_compatible_with(found_var.dtype):</span>\n<span class=\"pl-s\">    671         dtype_str = dtype.name</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">ValueError: Trying to share variable rnn/attention_cell_wrapper/multi_rnn_cell/cell_0/gru_cell/gates/kernel, but specified shape (200, 200) and found shape (2428, 200).</span></pre></div>", "body_text": "Hi,\nI have the following code. I can run it successfully in tf v1.0 but it failed in tf v1.2. The error is from the (inputs, state) in the GRUCell. Can you help me understand why the errors come from?\nThank you.\nimport tensorflow as tf\nfrom  tensorflow.contrib.learn.python.learn.estimators.dnn  import DNNClassifier\nfrom tensorflow.contrib.layers import real_valued_column\nfrom tensorflow.contrib.layers.python.layers.initializers import xavier_initializer\n\ndropout=0.2\nhidden_1_size = 1000\nhidden_2_size = 250\nNUM_EPOCHS=100\nBATCH_SIZE=50\nlr=0.0001\n\nnum_features = 2328\nRNN_HIDDEN_SIZE=100\nFIRST_LAYER_SIZE=1000\nSECOND_LAYER_SIZE=250\nNUM_LAYERS=2\nBATCH_SIZE=50\nNUM_EPOCHS=200\nlr=0.0003\nATTN_LENGTH=30\nbeta=0\n\n\nclass RNNModel():\n    def __init__(self):\n        global_step = tf.contrib.framework.get_or_create_global_step()\n        self.input_data = tf.placeholder(dtype=tf.float32,shape=[BATCH_SIZE,num_features])\n        self.target_data = tf.placeholder(dtype=tf.int32,shape=[BATCH_SIZE])\n        self.dropout_prob = tf.placeholder(dtype=tf.float32,shape=[])\n        \n        def makeGRUCells():\n            base_cell = tf.contrib.rnn.GRUCell(num_units=RNN_HIDDEN_SIZE,) \n            layered_cell = tf.contrib.rnn.MultiRNNCell([base_cell] * NUM_LAYERS,state_is_tuple=False) \n            attn_cell =tf.contrib.rnn.AttentionCellWrapper(cell=layered_cell,attn_length=ATTN_LENGTH,state_is_tuple=False)\n            return attn_cell\n        \n        self.gru_cell = makeGRUCells()\n        self.zero_state = self.gru_cell.zero_state(1, tf.float32)\n        \n        self.start_state = tf.placeholder(dtype=tf.float32,shape=[1,self.gru_cell.state_size])\n        print((self.start_state))\n        \n        \n\n        with tf.variable_scope(\"fff\",initializer=xavier_initializer(uniform=False), reuse = None):\n            droped_input = tf.nn.dropout(self.input_data,keep_prob=self.dropout_prob)\n            \n            layer_1 = tf.contrib.layers.fully_connected(\n                num_outputs=FIRST_LAYER_SIZE,\n                inputs=droped_input,\n            )\n            layer_2 = tf.contrib.layers.fully_connected(\n                num_outputs=RNN_HIDDEN_SIZE,\n                inputs=layer_1,\n            )\n            \n        \n        split_inputs = tf.reshape(droped_input,shape=[1,BATCH_SIZE,num_features],name=\"reshape_l1\") # Each item in the batch is a time step, iterate through them\n        #print(split_inputs.shape)\n        split_inputs = tf.unstack(split_inputs,axis=1,name=\"unpack_l1\")\n        #print(\"lentgh is \" + str(len(split_inputs)))\n        states =[]\n        outputs =[]\n        with tf.variable_scope(\"rnn\",initializer=xavier_initializer(uniform=False), reuse = None) as scope:\n            state = self.start_state\n            for i, inp in enumerate(split_inputs):\n                if i >0:\n                    scope.reuse_variables()\n                print((state))\n                print((inp))\n                print(\"this is for \" + str(i))\n                output, state = self.gru_cell(inputs = inp, state = state)\n                states.append(state)\n                outputs.append(output)\n        self.end_state = states[-1]\n        outputs = tf.stack(outputs,axis=1) # Pack them back into a single tensor\n        outputs = tf.reshape(outputs,shape=[BATCH_SIZE,RNN_HIDDEN_SIZE])\n        self.logits = tf.contrib.layers.fully_connected(\n            num_outputs=num_classes,\n            inputs=outputs,\n            activation_fn=None\n        )\n\n            \n        with tf.variable_scope(\"loss\", reuse = None):\n            self.penalties =    tf.reduce_sum([beta*tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n\n            \n            self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits,labels = self.target_data)\n            self.loss = tf.reduce_sum(self.losses + beta*self.penalties)\n        \n        with tf.name_scope(\"train_step\"):\n            opt = tf.train.AdamOptimizer(lr)\n            gvs = opt.compute_gradients(self.loss)\n            self.train_op = opt.apply_gradients(gvs, global_step=global_step)\n        \n        with tf.name_scope(\"predictions\"):\n            probs = tf.nn.softmax(self.logits)\n            self.predictions = tf.argmax(probs, 1)\n            correct_pred = tf.cast(tf.equal(self.predictions, tf.cast(self.target_data,tf.int64)),tf.float64)\n            self.accuracy = tf.reduce_mean(correct_pred)\n\n         \nwith tf.Graph().as_default():\n    model = RNNModel()\nWARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x0000000038EB4CC0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nTensor(\"Placeholder_3:0\", shape=(1, 3300), dtype=float32)\nTensor(\"Placeholder_3:0\", shape=(1, 3300), dtype=float32)\nTensor(\"unpack_l1:0\", shape=(1, 2328), dtype=float32)\nthis is for 0\n\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-20-2249079aecbe> in <module>()\n      1 with tf.Graph().as_default():\n----> 2     model = RNNModel()\n\n<ipython-input-19-58646adfd4d3> in __init__(self)\n     48                 print((inp))\n     49                 print(\"this is for \" + str(i))\n---> 50                 output, state = self.gru_cell(inputs = inp, state = state)\n     51                 states.append(state)\n     52                 outputs.append(output)\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\n    178       with vs.variable_scope(vs.get_variable_scope(),\n    179                              custom_getter=self._rnn_get_variable):\n--> 180         return super(RNNCell, self).__call__(inputs, state)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\n    439         # Check input assumptions set after layer building, e.g. input shape.\n    440         self._assert_input_compatibility(inputs)\n--> 441         outputs = self.call(inputs, *args, **kwargs)\n    442 \n    443         # Apply activity regularization.\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn_cell.py in call(self, inputs, state)\n   1111       input_size = inputs.get_shape().as_list()[1]\n   1112     inputs = _linear([inputs, attns], input_size, True)\n-> 1113     lstm_output, new_state = self._cell(inputs, state)\n   1114     if self._state_is_tuple:\n   1115       new_state_cat = array_ops.concat(nest.flatten(new_state), 1)\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\n    178       with vs.variable_scope(vs.get_variable_scope(),\n    179                              custom_getter=self._rnn_get_variable):\n--> 180         return super(RNNCell, self).__call__(inputs, state)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\n    439         # Check input assumptions set after layer building, e.g. input shape.\n    440         self._assert_input_compatibility(inputs)\n--> 441         outputs = self.call(inputs, *args, **kwargs)\n    442 \n    443         # Apply activity regularization.\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)\n    914                                       [-1, cell.state_size])\n    915           cur_state_pos += cell.state_size\n--> 916         cur_inp, new_state = cell(cur_inp, cur_state)\n    917         new_states.append(new_state)\n    918 \n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\n    178       with vs.variable_scope(vs.get_variable_scope(),\n    179                              custom_getter=self._rnn_get_variable):\n--> 180         return super(RNNCell, self).__call__(inputs, state)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\n    439         # Check input assumptions set after layer building, e.g. input shape.\n    440         self._assert_input_compatibility(inputs)\n--> 441         outputs = self.call(inputs, *args, **kwargs)\n    442 \n    443         # Apply activity regularization.\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)\n    293       value = math_ops.sigmoid(\n    294           _linear([inputs, state], 2 * self._num_units, True, bias_ones,\n--> 295                   self._kernel_initializer))\n    296       r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n    297     with vs.variable_scope(\"candidate\"):\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _linear(args, output_size, bias, bias_initializer, kernel_initializer)\n   1015         _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n   1016         dtype=dtype,\n-> 1017         initializer=kernel_initializer)\n   1018     if len(args) == 1:\n   1019       res = math_ops.matmul(args[0], weights)\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n   1063       collections=collections, caching_device=caching_device,\n   1064       partitioner=partitioner, validate_shape=validate_shape,\n-> 1065       use_resource=use_resource, custom_getter=custom_getter)\n   1066 get_variable_or_local_docstring = (\n   1067     \"\"\"%s\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n    960           collections=collections, caching_device=caching_device,\n    961           partitioner=partitioner, validate_shape=validate_shape,\n--> 962           use_resource=use_resource, custom_getter=custom_getter)\n    963 \n    964   def _get_partitioned_variable(self,\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n    358           reuse=reuse, trainable=trainable, collections=collections,\n    359           caching_device=caching_device, partitioner=partitioner,\n--> 360           validate_shape=validate_shape, use_resource=use_resource)\n    361     else:\n    362       return _true_getter(\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in wrapped_custom_getter(getter, *args, **kwargs)\n   1403     return custom_getter(\n   1404         functools.partial(old_getter, getter),\n-> 1405         *args, **kwargs)\n   1406   return wrapped_custom_getter\n   1407 \n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n--> 183     variable = getter(*args, **kwargs)\n    184     trainable = (variable in tf_variables.trainable_variables() or\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in wrapped_custom_getter(getter, *args, **kwargs)\n   1403     return custom_getter(\n   1404         functools.partial(old_getter, getter),\n-> 1405         *args, **kwargs)\n   1406   return wrapped_custom_getter\n   1407 \n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n--> 183     variable = getter(*args, **kwargs)\n    184     trainable = (variable in tf_variables.trainable_variables() or\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n--> 183     variable = getter(*args, **kwargs)\n    184     trainable = (variable in tf_variables.trainable_variables() or\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\n    350           trainable=trainable, collections=collections,\n    351           caching_device=caching_device, validate_shape=validate_shape,\n--> 352           use_resource=use_resource)\n    353 \n    354     if custom_getter is not None:\n\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\n    667         raise ValueError(\"Trying to share variable %s, but specified shape %s\"\n    668                          \" and found shape %s.\" % (name, shape,\n--> 669                                                    found_var.get_shape()))\n    670       if not dtype.is_compatible_with(found_var.dtype):\n    671         dtype_str = dtype.name\n\nValueError: Trying to share variable rnn/attention_cell_wrapper/multi_rnn_cell/cell_0/gru_cell/gates/kernel, but specified shape (200, 200) and found shape (2428, 200).", "body": "Hi,\r\n\r\nI have the following code. I can run it successfully in tf v1.0 but it failed in tf v1.2. The error is from the (inputs, state) in the GRUCell. Can you help me understand why the errors come from?\r\n\r\nThank you.\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom  tensorflow.contrib.learn.python.learn.estimators.dnn  import DNNClassifier\r\nfrom tensorflow.contrib.layers import real_valued_column\r\nfrom tensorflow.contrib.layers.python.layers.initializers import xavier_initializer\r\n\r\ndropout=0.2\r\nhidden_1_size = 1000\r\nhidden_2_size = 250\r\nNUM_EPOCHS=100\r\nBATCH_SIZE=50\r\nlr=0.0001\r\n\r\nnum_features = 2328\r\nRNN_HIDDEN_SIZE=100\r\nFIRST_LAYER_SIZE=1000\r\nSECOND_LAYER_SIZE=250\r\nNUM_LAYERS=2\r\nBATCH_SIZE=50\r\nNUM_EPOCHS=200\r\nlr=0.0003\r\nATTN_LENGTH=30\r\nbeta=0\r\n\r\n\r\nclass RNNModel():\r\n    def __init__(self):\r\n        global_step = tf.contrib.framework.get_or_create_global_step()\r\n        self.input_data = tf.placeholder(dtype=tf.float32,shape=[BATCH_SIZE,num_features])\r\n        self.target_data = tf.placeholder(dtype=tf.int32,shape=[BATCH_SIZE])\r\n        self.dropout_prob = tf.placeholder(dtype=tf.float32,shape=[])\r\n        \r\n        def makeGRUCells():\r\n            base_cell = tf.contrib.rnn.GRUCell(num_units=RNN_HIDDEN_SIZE,) \r\n            layered_cell = tf.contrib.rnn.MultiRNNCell([base_cell] * NUM_LAYERS,state_is_tuple=False) \r\n            attn_cell =tf.contrib.rnn.AttentionCellWrapper(cell=layered_cell,attn_length=ATTN_LENGTH,state_is_tuple=False)\r\n            return attn_cell\r\n        \r\n        self.gru_cell = makeGRUCells()\r\n        self.zero_state = self.gru_cell.zero_state(1, tf.float32)\r\n        \r\n        self.start_state = tf.placeholder(dtype=tf.float32,shape=[1,self.gru_cell.state_size])\r\n        print((self.start_state))\r\n        \r\n        \r\n\r\n        with tf.variable_scope(\"fff\",initializer=xavier_initializer(uniform=False), reuse = None):\r\n            droped_input = tf.nn.dropout(self.input_data,keep_prob=self.dropout_prob)\r\n            \r\n            layer_1 = tf.contrib.layers.fully_connected(\r\n                num_outputs=FIRST_LAYER_SIZE,\r\n                inputs=droped_input,\r\n            )\r\n            layer_2 = tf.contrib.layers.fully_connected(\r\n                num_outputs=RNN_HIDDEN_SIZE,\r\n                inputs=layer_1,\r\n            )\r\n            \r\n        \r\n        split_inputs = tf.reshape(droped_input,shape=[1,BATCH_SIZE,num_features],name=\"reshape_l1\") # Each item in the batch is a time step, iterate through them\r\n        #print(split_inputs.shape)\r\n        split_inputs = tf.unstack(split_inputs,axis=1,name=\"unpack_l1\")\r\n        #print(\"lentgh is \" + str(len(split_inputs)))\r\n        states =[]\r\n        outputs =[]\r\n        with tf.variable_scope(\"rnn\",initializer=xavier_initializer(uniform=False), reuse = None) as scope:\r\n            state = self.start_state\r\n            for i, inp in enumerate(split_inputs):\r\n                if i >0:\r\n                    scope.reuse_variables()\r\n                print((state))\r\n                print((inp))\r\n                print(\"this is for \" + str(i))\r\n                output, state = self.gru_cell(inputs = inp, state = state)\r\n                states.append(state)\r\n                outputs.append(output)\r\n        self.end_state = states[-1]\r\n        outputs = tf.stack(outputs,axis=1) # Pack them back into a single tensor\r\n        outputs = tf.reshape(outputs,shape=[BATCH_SIZE,RNN_HIDDEN_SIZE])\r\n        self.logits = tf.contrib.layers.fully_connected(\r\n            num_outputs=num_classes,\r\n            inputs=outputs,\r\n            activation_fn=None\r\n        )\r\n\r\n            \r\n        with tf.variable_scope(\"loss\", reuse = None):\r\n            self.penalties =    tf.reduce_sum([beta*tf.nn.l2_loss(var) for var in tf.trainable_variables()])\r\n\r\n            \r\n            self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits,labels = self.target_data)\r\n            self.loss = tf.reduce_sum(self.losses + beta*self.penalties)\r\n        \r\n        with tf.name_scope(\"train_step\"):\r\n            opt = tf.train.AdamOptimizer(lr)\r\n            gvs = opt.compute_gradients(self.loss)\r\n            self.train_op = opt.apply_gradients(gvs, global_step=global_step)\r\n        \r\n        with tf.name_scope(\"predictions\"):\r\n            probs = tf.nn.softmax(self.logits)\r\n            self.predictions = tf.argmax(probs, 1)\r\n            correct_pred = tf.cast(tf.equal(self.predictions, tf.cast(self.target_data,tf.int64)),tf.float64)\r\n            self.accuracy = tf.reduce_mean(correct_pred)\r\n\r\n         \r\nwith tf.Graph().as_default():\r\n    model = RNNModel()\r\n```\r\n\r\n\r\n```python\r\n\r\nWARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x0000000038EB4CC0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\r\nTensor(\"Placeholder_3:0\", shape=(1, 3300), dtype=float32)\r\nTensor(\"Placeholder_3:0\", shape=(1, 3300), dtype=float32)\r\nTensor(\"unpack_l1:0\", shape=(1, 2328), dtype=float32)\r\nthis is for 0\r\n\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-20-2249079aecbe> in <module>()\r\n      1 with tf.Graph().as_default():\r\n----> 2     model = RNNModel()\r\n\r\n<ipython-input-19-58646adfd4d3> in __init__(self)\r\n     48                 print((inp))\r\n     49                 print(\"this is for \" + str(i))\r\n---> 50                 output, state = self.gru_cell(inputs = inp, state = state)\r\n     51                 states.append(state)\r\n     52                 outputs.append(output)\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    178       with vs.variable_scope(vs.get_variable_scope(),\r\n    179                              custom_getter=self._rnn_get_variable):\r\n--> 180         return super(RNNCell, self).__call__(inputs, state)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    439         # Check input assumptions set after layer building, e.g. input shape.\r\n    440         self._assert_input_compatibility(inputs)\r\n--> 441         outputs = self.call(inputs, *args, **kwargs)\r\n    442 \r\n    443         # Apply activity regularization.\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn_cell.py in call(self, inputs, state)\r\n   1111       input_size = inputs.get_shape().as_list()[1]\r\n   1112     inputs = _linear([inputs, attns], input_size, True)\r\n-> 1113     lstm_output, new_state = self._cell(inputs, state)\r\n   1114     if self._state_is_tuple:\r\n   1115       new_state_cat = array_ops.concat(nest.flatten(new_state), 1)\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    178       with vs.variable_scope(vs.get_variable_scope(),\r\n    179                              custom_getter=self._rnn_get_variable):\r\n--> 180         return super(RNNCell, self).__call__(inputs, state)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    439         # Check input assumptions set after layer building, e.g. input shape.\r\n    440         self._assert_input_compatibility(inputs)\r\n--> 441         outputs = self.call(inputs, *args, **kwargs)\r\n    442 \r\n    443         # Apply activity regularization.\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)\r\n    914                                       [-1, cell.state_size])\r\n    915           cur_state_pos += cell.state_size\r\n--> 916         cur_inp, new_state = cell(cur_inp, cur_state)\r\n    917         new_states.append(new_state)\r\n    918 \r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    178       with vs.variable_scope(vs.get_variable_scope(),\r\n    179                              custom_getter=self._rnn_get_variable):\r\n--> 180         return super(RNNCell, self).__call__(inputs, state)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    439         # Check input assumptions set after layer building, e.g. input shape.\r\n    440         self._assert_input_compatibility(inputs)\r\n--> 441         outputs = self.call(inputs, *args, **kwargs)\r\n    442 \r\n    443         # Apply activity regularization.\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)\r\n    293       value = math_ops.sigmoid(\r\n    294           _linear([inputs, state], 2 * self._num_units, True, bias_ones,\r\n--> 295                   self._kernel_initializer))\r\n    296       r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\r\n    297     with vs.variable_scope(\"candidate\"):\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _linear(args, output_size, bias, bias_initializer, kernel_initializer)\r\n   1015         _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\r\n   1016         dtype=dtype,\r\n-> 1017         initializer=kernel_initializer)\r\n   1018     if len(args) == 1:\r\n   1019       res = math_ops.matmul(args[0], weights)\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n   1063       collections=collections, caching_device=caching_device,\r\n   1064       partitioner=partitioner, validate_shape=validate_shape,\r\n-> 1065       use_resource=use_resource, custom_getter=custom_getter)\r\n   1066 get_variable_or_local_docstring = (\r\n   1067     \"\"\"%s\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    960           collections=collections, caching_device=caching_device,\r\n    961           partitioner=partitioner, validate_shape=validate_shape,\r\n--> 962           use_resource=use_resource, custom_getter=custom_getter)\r\n    963 \r\n    964   def _get_partitioned_variable(self,\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    358           reuse=reuse, trainable=trainable, collections=collections,\r\n    359           caching_device=caching_device, partitioner=partitioner,\r\n--> 360           validate_shape=validate_shape, use_resource=use_resource)\r\n    361     else:\r\n    362       return _true_getter(\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in wrapped_custom_getter(getter, *args, **kwargs)\r\n   1403     return custom_getter(\r\n   1404         functools.partial(old_getter, getter),\r\n-> 1405         *args, **kwargs)\r\n   1406   return wrapped_custom_getter\r\n   1407 \r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n--> 183     variable = getter(*args, **kwargs)\r\n    184     trainable = (variable in tf_variables.trainable_variables() or\r\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in wrapped_custom_getter(getter, *args, **kwargs)\r\n   1403     return custom_getter(\r\n   1404         functools.partial(old_getter, getter),\r\n-> 1405         *args, **kwargs)\r\n   1406   return wrapped_custom_getter\r\n   1407 \r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n--> 183     variable = getter(*args, **kwargs)\r\n    184     trainable = (variable in tf_variables.trainable_variables() or\r\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n--> 183     variable = getter(*args, **kwargs)\r\n    184     trainable = (variable in tf_variables.trainable_variables() or\r\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\r\n    350           trainable=trainable, collections=collections,\r\n    351           caching_device=caching_device, validate_shape=validate_shape,\r\n--> 352           use_resource=use_resource)\r\n    353 \r\n    354     if custom_getter is not None:\r\n\r\nC:\\Users\\hsong01\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\r\n    667         raise ValueError(\"Trying to share variable %s, but specified shape %s\"\r\n    668                          \" and found shape %s.\" % (name, shape,\r\n--> 669                                                    found_var.get_shape()))\r\n    670       if not dtype.is_compatible_with(found_var.dtype):\r\n    671         dtype_str = dtype.name\r\n\r\nValueError: Trying to share variable rnn/attention_cell_wrapper/multi_rnn_cell/cell_0/gru_cell/gates/kernel, but specified shape (200, 200) and found shape (2428, 200).\r\n```\r\n"}