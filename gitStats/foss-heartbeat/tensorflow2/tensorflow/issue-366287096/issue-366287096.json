{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22695", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22695/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22695/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22695/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22695", "id": 366287096, "node_id": "MDU6SXNzdWUzNjYyODcwOTY=", "number": 22695, "title": "Tensorflow C API: SessionRun() latency", "user": {"login": "EnricoGiordano1992", "id": 3462634, "node_id": "MDQ6VXNlcjM0NjI2MzQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/3462634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EnricoGiordano1992", "html_url": "https://github.com/EnricoGiordano1992", "followers_url": "https://api.github.com/users/EnricoGiordano1992/followers", "following_url": "https://api.github.com/users/EnricoGiordano1992/following{/other_user}", "gists_url": "https://api.github.com/users/EnricoGiordano1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/EnricoGiordano1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EnricoGiordano1992/subscriptions", "organizations_url": "https://api.github.com/users/EnricoGiordano1992/orgs", "repos_url": "https://api.github.com/users/EnricoGiordano1992/repos", "events_url": "https://api.github.com/users/EnricoGiordano1992/events{/privacy}", "received_events_url": "https://api.github.com/users/EnricoGiordano1992/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097545817, "node_id": "MDU6TGFiZWwxMDk3NTQ1ODE3", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:apis", "name": "comp:apis", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-10-03T11:12:06Z", "updated_at": "2018-10-24T20:23:23Z", "closed_at": "2018-10-24T20:23:23Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 18.04 lts</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: no</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.16</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 7.3.0</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.2 / 7.3</li>\n<li><strong>GPU model and memory</strong>: Nvidia GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683</li>\n<li><strong>Exact command to reproduce</strong>: compile and execute my code</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Scenario:<br>\nVGG16 from Tensorflow C API. We would like to inference a (potentially) different number of frames for each execution of<br>\nSessionRun(), To do so we are allocating a tensor that holds data for the frames to execute (dimensions n_frames x 50 x 50 x 3).</p>\n<p>Observed behavior and questions:</p>\n<ol>\n<li>for a given n_frames, e.g. n_frames = 3, executing SessionRun() n times gives a time profiling like this:<br>\n~141ms<br>\n~3.8ms<br>\n~3.8ms<br>\n....<br>\n//n times</li>\n</ol>\n<p>We are assuming the 1st  SessionRun() takes longer to upload the graph on GPU. Is that correct?</p>\n<ol start=\"2\">\n<li>if we run a pattern like this:<br>\nn_frames = 3 --&gt; call SessionRun() n times<br>\nn_frames = 3 --&gt; call SessionRun() n times<br>\nn_frames = 1 --&gt; call SessionRun() n times</li>\n</ol>\n<p>the time profiling is like this:<br>\n~141ms<br>\n~3.8ms<br>\n~3.8ms<br>\n....<br>\n//n times<br>\n~3.8ms<br>\n~3.8ms<br>\n~3.8ms<br>\n....<br>\n//n times<br>\n<strong>~70ms</strong><br>\n~3.8ms<br>\n~3.8ms<br>\n....<br>\n//n times</p>\n<p>It looks like every time the input tensor size changes, something happens and the 1st execution takes longer.<br>\nQuestion:<br>\nAssuming that my application scenario is n_frames = 32 is the largest size for the input tensor is there a way<br>\nto \"reserve\" GPU resources to get the same inference time (~3.8ms in the example) even though the actual tensor parameter passed to SessionRun() is smaller (i.e. n_frames = 1, n_frames= 2, etc...)</p>\n<h3>Source code / logs</h3>\n<pre><code>#include \"tensorflow/c/c_api.h\"\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;memory.h&gt;\n#include &lt;string.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;vector&gt;\n#include &lt;algorithm&gt;\n#include &lt;iterator&gt;\n#include &lt;iostream&gt;\n\n\nTF_Buffer* read_file(const char* file);\n\nvoid free_buffer(void* data, size_t length) {\n}\n\nstatic void Deallocator(void* data, size_t length, void* arg) {\n}\n\nint main() {\n  TF_Buffer* graph_def = read_file(\"data/graph.pb\");\n  TF_Graph* graph = TF_NewGraph();\n\n  TF_Status* status = TF_NewStatus();\n  TF_ImportGraphDefOptions* graph_opts = TF_NewImportGraphDefOptions();\n  TF_GraphImportGraphDef(graph, graph_def, graph_opts, status);\n  if (TF_GetCode(status) != TF_OK) {\n          fprintf(stderr, \"ERROR: Unable to import graph %s\", TF_Message(status));\n          return 1;\n  }\n  else {\n          fprintf(stdout, \"Successfully imported graph\\n\");\n  }\n  const int num_bytes_in = 1 * 50 * 50 * 3 * sizeof(float);\n  const int num_bytes_out = 1 * 2 * sizeof(float);\n\n  int64_t in_dims[] = {3, 50, 50, 3};\n  int64_t out_dims[] = {3, 2};\n\n  int64_t in_dims2[] = {2, 50, 50, 3};\n  int64_t out_dims2[] = {2, 2};\n\n  float values[3 * 50 * 50 * 3] = {0xff};\n  float values2[2 * 50 * 50 * 3] = {0xff};\n\n  std::vector&lt;TF_Output&gt; inputs;\n  std::vector&lt;TF_Tensor*&gt; input_values;\n  std::vector&lt;TF_Tensor*&gt; input_values2;\n\n  inputs.push_back({TF_GraphOperationByName(graph, \"input_1\"), 0});\n  input_values.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values, num_bytes_in, &amp;Deallocator, 0));\n  input_values2.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values2, num_bytes_in, &amp;Deallocator, 0));\n\n  std::vector&lt;TF_Output&gt; outputs;\n  outputs.push_back({TF_GraphOperationByName(graph, \"dense_3/Softmax\"), 0});\n\n  std::vector&lt;TF_Tensor*&gt; output_values;\n\n  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));\n  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));\n\n  fprintf(stdout, \"Running session...\\n\");\n  TF_SessionOptions* sess_opts = TF_NewSessionOptions();\n  TF_Session* session = TF_NewSession(graph, sess_opts, status);\n  assert(TF_GetCode(status) == TF_OK);\n\n  // about 140ms\n  TF_SessionRun(session, nullptr,\n                &amp;inputs[0], &amp;input_values[0], 1,\n                &amp;outputs[0], &amp;output_values[0], 1,\n                nullptr, 0, nullptr, status);\n  // about 3ms\n  TF_SessionRun(session, nullptr,\n                &amp;inputs[0], &amp;input_values[0], 1,\n                &amp;outputs[0], &amp;output_values[0], 1,\n                nullptr, 0, nullptr, status);\n  // about 3ms\n  TF_SessionRun(session, nullptr,\n                &amp;inputs[0], &amp;input_values[0], 1,\n                &amp;outputs[0], &amp;output_values[0], 1,\n                nullptr, 0, nullptr, status);\n\n  // n times...\n  // ...\n\n  // NOW CHANGES TENSOR DIMENSIONS ---&gt; about 70 ms\n  TF_SessionRun(session, nullptr,\n                &amp;inputs[0], &amp;input_values2[0], 1,\n                &amp;outputs[0], &amp;output_values2[0], 1,\n                nullptr, 0, nullptr, status);\n\n  // about 3ms\n  TF_SessionRun(session, nullptr,\n                &amp;inputs[0], &amp;input_values2[0], 1,\n                &amp;outputs[0], &amp;output_values2[0], 1,\n                nullptr, 0, nullptr, status);\n\n  // n times...\n  // ...\n\n  TF_Code c = TF_GetCode(status);\n\n  std::cout &lt;&lt; c &lt;&lt; std::endl;\n\n  for(size_t i = 0; i &lt; output_values.size(); ++i)\n  {\n      if (output_values.at(i) == nullptr)\n      {\n          std::cout &lt;&lt; \"bad parameters\" &lt;&lt; std::endl;\n      }\n      else\n      {\n          const auto data = static_cast&lt;float*&gt;(TF_TensorData(output_values.at(i)));\n          std::cout &lt;&lt; ((data[1] &gt; 0.5f) ? true : false) &lt;&lt; std::endl;\n      }\n  }\n\n  fprintf(stdout, \"Successfully run session\\n\");\n\n  TF_CloseSession(session, status);\n  TF_DeleteSession(session, status);\n  TF_DeleteSessionOptions(sess_opts);\n  TF_DeleteImportGraphDefOptions(graph_opts);\n  TF_DeleteGraph(graph);\n  TF_DeleteStatus(status);\n  return 0;\n}\n\nTF_Buffer* read_file(const char* file) {\n  FILE *f = fopen(file, \"rb\");\n  fseek(f, 0, SEEK_END);\n  long fsize = ftell(f);\n  fseek(f, 0, SEEK_SET);  //same as rewind(f);\n\n  void* data = malloc(fsize);\n  fread(data, fsize, 1, f);\n  fclose(f);\n\n  TF_Buffer* buf = TF_NewBuffer();\n  buf-&gt;data = data;\n  buf-&gt;length = fsize;\n  buf-&gt;data_deallocator = free_buffer;\n  return buf;\n}\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 lts\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.10\nPython version: 3.6\nBazel version (if compiling from source): 0.16\nGCC/Compiler version (if compiling from source): 7.3.0\nCUDA/cuDNN version: 9.2 / 7.3\nGPU model and memory: Nvidia GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\nExact command to reproduce: compile and execute my code\n\nDescribe the problem\nScenario:\nVGG16 from Tensorflow C API. We would like to inference a (potentially) different number of frames for each execution of\nSessionRun(), To do so we are allocating a tensor that holds data for the frames to execute (dimensions n_frames x 50 x 50 x 3).\nObserved behavior and questions:\n\nfor a given n_frames, e.g. n_frames = 3, executing SessionRun() n times gives a time profiling like this:\n~141ms\n~3.8ms\n~3.8ms\n....\n//n times\n\nWe are assuming the 1st  SessionRun() takes longer to upload the graph on GPU. Is that correct?\n\nif we run a pattern like this:\nn_frames = 3 --> call SessionRun() n times\nn_frames = 3 --> call SessionRun() n times\nn_frames = 1 --> call SessionRun() n times\n\nthe time profiling is like this:\n~141ms\n~3.8ms\n~3.8ms\n....\n//n times\n~3.8ms\n~3.8ms\n~3.8ms\n....\n//n times\n~70ms\n~3.8ms\n~3.8ms\n....\n//n times\nIt looks like every time the input tensor size changes, something happens and the 1st execution takes longer.\nQuestion:\nAssuming that my application scenario is n_frames = 32 is the largest size for the input tensor is there a way\nto \"reserve\" GPU resources to get the same inference time (~3.8ms in the example) even though the actual tensor parameter passed to SessionRun() is smaller (i.e. n_frames = 1, n_frames= 2, etc...)\nSource code / logs\n#include \"tensorflow/c/c_api.h\"\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <memory.h>\n#include <string.h>\n#include <assert.h>\n#include <vector>\n#include <algorithm>\n#include <iterator>\n#include <iostream>\n\n\nTF_Buffer* read_file(const char* file);\n\nvoid free_buffer(void* data, size_t length) {\n}\n\nstatic void Deallocator(void* data, size_t length, void* arg) {\n}\n\nint main() {\n  TF_Buffer* graph_def = read_file(\"data/graph.pb\");\n  TF_Graph* graph = TF_NewGraph();\n\n  TF_Status* status = TF_NewStatus();\n  TF_ImportGraphDefOptions* graph_opts = TF_NewImportGraphDefOptions();\n  TF_GraphImportGraphDef(graph, graph_def, graph_opts, status);\n  if (TF_GetCode(status) != TF_OK) {\n          fprintf(stderr, \"ERROR: Unable to import graph %s\", TF_Message(status));\n          return 1;\n  }\n  else {\n          fprintf(stdout, \"Successfully imported graph\\n\");\n  }\n  const int num_bytes_in = 1 * 50 * 50 * 3 * sizeof(float);\n  const int num_bytes_out = 1 * 2 * sizeof(float);\n\n  int64_t in_dims[] = {3, 50, 50, 3};\n  int64_t out_dims[] = {3, 2};\n\n  int64_t in_dims2[] = {2, 50, 50, 3};\n  int64_t out_dims2[] = {2, 2};\n\n  float values[3 * 50 * 50 * 3] = {0xff};\n  float values2[2 * 50 * 50 * 3] = {0xff};\n\n  std::vector<TF_Output> inputs;\n  std::vector<TF_Tensor*> input_values;\n  std::vector<TF_Tensor*> input_values2;\n\n  inputs.push_back({TF_GraphOperationByName(graph, \"input_1\"), 0});\n  input_values.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values, num_bytes_in, &Deallocator, 0));\n  input_values2.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values2, num_bytes_in, &Deallocator, 0));\n\n  std::vector<TF_Output> outputs;\n  outputs.push_back({TF_GraphOperationByName(graph, \"dense_3/Softmax\"), 0});\n\n  std::vector<TF_Tensor*> output_values;\n\n  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));\n  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));\n\n  fprintf(stdout, \"Running session...\\n\");\n  TF_SessionOptions* sess_opts = TF_NewSessionOptions();\n  TF_Session* session = TF_NewSession(graph, sess_opts, status);\n  assert(TF_GetCode(status) == TF_OK);\n\n  // about 140ms\n  TF_SessionRun(session, nullptr,\n                &inputs[0], &input_values[0], 1,\n                &outputs[0], &output_values[0], 1,\n                nullptr, 0, nullptr, status);\n  // about 3ms\n  TF_SessionRun(session, nullptr,\n                &inputs[0], &input_values[0], 1,\n                &outputs[0], &output_values[0], 1,\n                nullptr, 0, nullptr, status);\n  // about 3ms\n  TF_SessionRun(session, nullptr,\n                &inputs[0], &input_values[0], 1,\n                &outputs[0], &output_values[0], 1,\n                nullptr, 0, nullptr, status);\n\n  // n times...\n  // ...\n\n  // NOW CHANGES TENSOR DIMENSIONS ---> about 70 ms\n  TF_SessionRun(session, nullptr,\n                &inputs[0], &input_values2[0], 1,\n                &outputs[0], &output_values2[0], 1,\n                nullptr, 0, nullptr, status);\n\n  // about 3ms\n  TF_SessionRun(session, nullptr,\n                &inputs[0], &input_values2[0], 1,\n                &outputs[0], &output_values2[0], 1,\n                nullptr, 0, nullptr, status);\n\n  // n times...\n  // ...\n\n  TF_Code c = TF_GetCode(status);\n\n  std::cout << c << std::endl;\n\n  for(size_t i = 0; i < output_values.size(); ++i)\n  {\n      if (output_values.at(i) == nullptr)\n      {\n          std::cout << \"bad parameters\" << std::endl;\n      }\n      else\n      {\n          const auto data = static_cast<float*>(TF_TensorData(output_values.at(i)));\n          std::cout << ((data[1] > 0.5f) ? true : false) << std::endl;\n      }\n  }\n\n  fprintf(stdout, \"Successfully run session\\n\");\n\n  TF_CloseSession(session, status);\n  TF_DeleteSession(session, status);\n  TF_DeleteSessionOptions(sess_opts);\n  TF_DeleteImportGraphDefOptions(graph_opts);\n  TF_DeleteGraph(graph);\n  TF_DeleteStatus(status);\n  return 0;\n}\n\nTF_Buffer* read_file(const char* file) {\n  FILE *f = fopen(file, \"rb\");\n  fseek(f, 0, SEEK_END);\n  long fsize = ftell(f);\n  fseek(f, 0, SEEK_SET);  //same as rewind(f);\n\n  void* data = malloc(fsize);\n  fread(data, fsize, 1, f);\n  fclose(f);\n\n  TF_Buffer* buf = TF_NewBuffer();\n  buf->data = data;\n  buf->length = fsize;\n  buf->data_deallocator = free_buffer;\n  return buf;\n}", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 lts\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.16\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: 9.2 / 7.3\r\n- **GPU model and memory**: Nvidia GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\n- **Exact command to reproduce**: compile and execute my code\r\n\r\n### Describe the problem\r\nScenario:\r\nVGG16 from Tensorflow C API. We would like to inference a (potentially) different number of frames for each execution of \r\nSessionRun(), To do so we are allocating a tensor that holds data for the frames to execute (dimensions n_frames x 50 x 50 x 3).\r\n\r\nObserved behavior and questions:\r\n1. for a given n_frames, e.g. n_frames = 3, executing SessionRun() n times gives a time profiling like this:\r\n~141ms\r\n~3.8ms\r\n~3.8ms\r\n....\r\n//n times\r\n\r\nWe are assuming the 1st  SessionRun() takes longer to upload the graph on GPU. Is that correct?\r\n\r\n2. if we run a pattern like this:\r\nn_frames = 3 --> call SessionRun() n times\r\nn_frames = 3 --> call SessionRun() n times\r\nn_frames = 1 --> call SessionRun() n times\r\n\r\nthe time profiling is like this:\r\n~141ms\r\n~3.8ms\r\n~3.8ms\r\n....\r\n//n times\r\n~3.8ms\r\n~3.8ms\r\n~3.8ms\r\n....\r\n//n times\r\n**~70ms**\r\n~3.8ms\r\n~3.8ms\r\n....\r\n//n times\r\n\r\nIt looks like every time the input tensor size changes, something happens and the 1st execution takes longer.\r\nQuestion:\r\nAssuming that my application scenario is n_frames = 32 is the largest size for the input tensor is there a way \r\nto \"reserve\" GPU resources to get the same inference time (~3.8ms in the example) even though the actual tensor parameter passed to SessionRun() is smaller (i.e. n_frames = 1, n_frames= 2, etc...)\r\n\r\n\r\n\r\n### Source code / logs\r\n```\r\n#include \"tensorflow/c/c_api.h\"\r\n\r\n#include <stdio.h>\r\n#include <stdlib.h>\r\n#include <memory.h>\r\n#include <string.h>\r\n#include <assert.h>\r\n#include <vector>\r\n#include <algorithm>\r\n#include <iterator>\r\n#include <iostream>\r\n\r\n\r\nTF_Buffer* read_file(const char* file);\r\n\r\nvoid free_buffer(void* data, size_t length) {\r\n}\r\n\r\nstatic void Deallocator(void* data, size_t length, void* arg) {\r\n}\r\n\r\nint main() {\r\n  TF_Buffer* graph_def = read_file(\"data/graph.pb\");\r\n  TF_Graph* graph = TF_NewGraph();\r\n\r\n  TF_Status* status = TF_NewStatus();\r\n  TF_ImportGraphDefOptions* graph_opts = TF_NewImportGraphDefOptions();\r\n  TF_GraphImportGraphDef(graph, graph_def, graph_opts, status);\r\n  if (TF_GetCode(status) != TF_OK) {\r\n          fprintf(stderr, \"ERROR: Unable to import graph %s\", TF_Message(status));\r\n          return 1;\r\n  }\r\n  else {\r\n          fprintf(stdout, \"Successfully imported graph\\n\");\r\n  }\r\n  const int num_bytes_in = 1 * 50 * 50 * 3 * sizeof(float);\r\n  const int num_bytes_out = 1 * 2 * sizeof(float);\r\n\r\n  int64_t in_dims[] = {3, 50, 50, 3};\r\n  int64_t out_dims[] = {3, 2};\r\n\r\n  int64_t in_dims2[] = {2, 50, 50, 3};\r\n  int64_t out_dims2[] = {2, 2};\r\n\r\n  float values[3 * 50 * 50 * 3] = {0xff};\r\n  float values2[2 * 50 * 50 * 3] = {0xff};\r\n\r\n  std::vector<TF_Output> inputs;\r\n  std::vector<TF_Tensor*> input_values;\r\n  std::vector<TF_Tensor*> input_values2;\r\n\r\n  inputs.push_back({TF_GraphOperationByName(graph, \"input_1\"), 0});\r\n  input_values.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values, num_bytes_in, &Deallocator, 0));\r\n  input_values2.push_back(TF_NewTensor(TF_FLOAT, in_dims, 4, values2, num_bytes_in, &Deallocator, 0));\r\n\r\n  std::vector<TF_Output> outputs;\r\n  outputs.push_back({TF_GraphOperationByName(graph, \"dense_3/Softmax\"), 0});\r\n\r\n  std::vector<TF_Tensor*> output_values;\r\n\r\n  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));\r\n  output_values.push_back(TF_AllocateTensor(TF_FLOAT, out_dims, 2, num_bytes_out));\r\n\r\n  fprintf(stdout, \"Running session...\\n\");\r\n  TF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\n  TF_Session* session = TF_NewSession(graph, sess_opts, status);\r\n  assert(TF_GetCode(status) == TF_OK);\r\n\r\n  // about 140ms\r\n  TF_SessionRun(session, nullptr,\r\n                &inputs[0], &input_values[0], 1,\r\n                &outputs[0], &output_values[0], 1,\r\n                nullptr, 0, nullptr, status);\r\n  // about 3ms\r\n  TF_SessionRun(session, nullptr,\r\n                &inputs[0], &input_values[0], 1,\r\n                &outputs[0], &output_values[0], 1,\r\n                nullptr, 0, nullptr, status);\r\n  // about 3ms\r\n  TF_SessionRun(session, nullptr,\r\n                &inputs[0], &input_values[0], 1,\r\n                &outputs[0], &output_values[0], 1,\r\n                nullptr, 0, nullptr, status);\r\n\r\n  // n times...\r\n  // ...\r\n\r\n  // NOW CHANGES TENSOR DIMENSIONS ---> about 70 ms\r\n  TF_SessionRun(session, nullptr,\r\n                &inputs[0], &input_values2[0], 1,\r\n                &outputs[0], &output_values2[0], 1,\r\n                nullptr, 0, nullptr, status);\r\n\r\n  // about 3ms\r\n  TF_SessionRun(session, nullptr,\r\n                &inputs[0], &input_values2[0], 1,\r\n                &outputs[0], &output_values2[0], 1,\r\n                nullptr, 0, nullptr, status);\r\n\r\n  // n times...\r\n  // ...\r\n\r\n  TF_Code c = TF_GetCode(status);\r\n\r\n  std::cout << c << std::endl;\r\n\r\n  for(size_t i = 0; i < output_values.size(); ++i)\r\n  {\r\n      if (output_values.at(i) == nullptr)\r\n      {\r\n          std::cout << \"bad parameters\" << std::endl;\r\n      }\r\n      else\r\n      {\r\n          const auto data = static_cast<float*>(TF_TensorData(output_values.at(i)));\r\n          std::cout << ((data[1] > 0.5f) ? true : false) << std::endl;\r\n      }\r\n  }\r\n\r\n  fprintf(stdout, \"Successfully run session\\n\");\r\n\r\n  TF_CloseSession(session, status);\r\n  TF_DeleteSession(session, status);\r\n  TF_DeleteSessionOptions(sess_opts);\r\n  TF_DeleteImportGraphDefOptions(graph_opts);\r\n  TF_DeleteGraph(graph);\r\n  TF_DeleteStatus(status);\r\n  return 0;\r\n}\r\n\r\nTF_Buffer* read_file(const char* file) {\r\n  FILE *f = fopen(file, \"rb\");\r\n  fseek(f, 0, SEEK_END);\r\n  long fsize = ftell(f);\r\n  fseek(f, 0, SEEK_SET);  //same as rewind(f);\r\n\r\n  void* data = malloc(fsize);\r\n  fread(data, fsize, 1, f);\r\n  fclose(f);\r\n\r\n  TF_Buffer* buf = TF_NewBuffer();\r\n  buf->data = data;\r\n  buf->length = fsize;\r\n  buf->data_deallocator = free_buffer;\r\n  return buf;\r\n}\r\n```\r\n"}