{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/244465158", "html_url": "https://github.com/tensorflow/tensorflow/issues/1888#issuecomment-244465158", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1888", "id": 244465158, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NDQ2NTE1OA==", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-02T19:20:34Z", "updated_at": "2016-09-02T19:20:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Just tested this out today.</p>\n<p>It's closer than the previous one... but still not quite there -- reopen the issue again? The bug here is insidious and somewhat unexpected:</p>\n<p>:(</p>\n<p>It comes from the following line: <a href=\"https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/common_runtime/gpu/process_state.cc#L188\">gpu/process_state.cc:188</a></p>\n<p>If you take a look at that, you see that when you allocate memory on the host that is known to CUDA, you always use <code>ExecutorForDevice(0)</code>, even though <code>0</code> may not be in your <code>visible_device_list</code>.</p>\n<p>You can test this with:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nc <span class=\"pl-k\">=</span> tf.ConfigProto()\nc.gpu_options.visible_device_list<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1<span class=\"pl-pds\">\"</span></span>\n\ns <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>c)\ns.run(tf.constant(<span class=\"pl-c1\">10</span>))\ntime.sleep(<span class=\"pl-c1\">5</span>)</pre></div>\n<p>And then if you look at <code>nvidia-smi</code> you will see two contexts, one on 0 and one on 1, even though only one was requested. (The difference between this test and the previous one is that you use a <code>s.run</code>)</p>\n<p>I'm not sure this is fixable without changing stream executor, but the patch isn't super helpful in its current state :(</p>", "body_text": "Just tested this out today.\nIt's closer than the previous one... but still not quite there -- reopen the issue again? The bug here is insidious and somewhat unexpected:\n:(\nIt comes from the following line: gpu/process_state.cc:188\nIf you take a look at that, you see that when you allocate memory on the host that is known to CUDA, you always use ExecutorForDevice(0), even though 0 may not be in your visible_device_list.\nYou can test this with:\nimport time\nimport tensorflow as tf\n\nc = tf.ConfigProto()\nc.gpu_options.visible_device_list=\"1\"\n\ns = tf.Session(config=c)\ns.run(tf.constant(10))\ntime.sleep(5)\nAnd then if you look at nvidia-smi you will see two contexts, one on 0 and one on 1, even though only one was requested. (The difference between this test and the previous one is that you use a s.run)\nI'm not sure this is fixable without changing stream executor, but the patch isn't super helpful in its current state :(", "body": "Just tested this out today.\n\nIt's closer than the previous one... but still not quite there -- reopen the issue again? The bug here is insidious and somewhat unexpected:\n\n:(\n\nIt comes from the following line: [gpu/process_state.cc:188](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/common_runtime/gpu/process_state.cc#L188)\n\nIf you take a look at that, you see that when you allocate memory on the host that is known to CUDA, you always use `ExecutorForDevice(0)`, even though `0` may not be in your `visible_device_list`. \n\nYou can test this with:\n\n``` python\nimport time\nimport tensorflow as tf\n\nc = tf.ConfigProto()\nc.gpu_options.visible_device_list=\"1\"\n\ns = tf.Session(config=c)\ns.run(tf.constant(10))\ntime.sleep(5)\n```\n\nAnd then if you look at `nvidia-smi` you will see two contexts, one on 0 and one on 1, even though only one was requested. (The difference between this test and the previous one is that you use a `s.run`)\n\nI'm not sure this is fixable without changing stream executor, but the patch isn't super helpful in its current state :(\n"}