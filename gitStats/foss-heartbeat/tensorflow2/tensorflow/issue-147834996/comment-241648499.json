{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241648499", "html_url": "https://github.com/tensorflow/tensorflow/issues/1888#issuecomment-241648499", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1888", "id": 241648499, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTY0ODQ5OQ==", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-23T07:22:05Z", "updated_at": "2016-08-23T07:22:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I would like to implement this, as it's important for some things besides just convenience; for example, you cannot use <code>CUDA_VISIBLE_DEVICES</code> with CUDA IPC, so Tensorflow cannot be used with anything that needs access to the GPU memory in other processes (without copying it to CPU first).</p>\n<p>Is there a particular API that would make sense for this?</p>\n<p>Some options:</p>\n<ol>\n<li>Another environment variable, besides <code>CUDA_VISIBLE_DEVICES</code>, which tells Tensorflow which GPUs to use. For example, <code>TENSORFLOW_CUDA_DEVICES</code>, which <em>must</em> be a subset of <code>CUDA_VISIBLE_DEVICES</code> (if <code>CUDA_VISIBLE_DEVICES</code> is specified).</li>\n<li>Modify the <code>GPUOptions</code> proto with a <code>repeated</code> field along the lines of <code>gpu_id</code>, which select which of the <code>CUDA_VISIBLE_DEVICES</code> to use. This seems cleaner, but I'm not sure how invasive that change would be (when does Tensorflow allocate GPU memory, call <code>cuGetDevice</code>, and otherwise initialize the StreamExecutor? Is it upon <code>tf.Session()</code>? Or is it earlier? If it's earlier, this change may be tricky to implement.)</li>\n</ol>\n<p>Finally, if I were to implement option (2) or (1), what would I need to do in order to get the contribution accepted to Tensorflow?</p>", "body_text": "I would like to implement this, as it's important for some things besides just convenience; for example, you cannot use CUDA_VISIBLE_DEVICES with CUDA IPC, so Tensorflow cannot be used with anything that needs access to the GPU memory in other processes (without copying it to CPU first).\nIs there a particular API that would make sense for this?\nSome options:\n\nAnother environment variable, besides CUDA_VISIBLE_DEVICES, which tells Tensorflow which GPUs to use. For example, TENSORFLOW_CUDA_DEVICES, which must be a subset of CUDA_VISIBLE_DEVICES (if CUDA_VISIBLE_DEVICES is specified).\nModify the GPUOptions proto with a repeated field along the lines of gpu_id, which select which of the CUDA_VISIBLE_DEVICES to use. This seems cleaner, but I'm not sure how invasive that change would be (when does Tensorflow allocate GPU memory, call cuGetDevice, and otherwise initialize the StreamExecutor? Is it upon tf.Session()? Or is it earlier? If it's earlier, this change may be tricky to implement.)\n\nFinally, if I were to implement option (2) or (1), what would I need to do in order to get the contribution accepted to Tensorflow?", "body": "I would like to implement this, as it's important for some things besides just convenience; for example, you cannot use `CUDA_VISIBLE_DEVICES` with CUDA IPC, so Tensorflow cannot be used with anything that needs access to the GPU memory in other processes (without copying it to CPU first). \n\nIs there a particular API that would make sense for this?\n\nSome options:\n1. Another environment variable, besides `CUDA_VISIBLE_DEVICES`, which tells Tensorflow which GPUs to use. For example, `TENSORFLOW_CUDA_DEVICES`, which _must_ be a subset of `CUDA_VISIBLE_DEVICES` (if `CUDA_VISIBLE_DEVICES` is specified).\n2. Modify the `GPUOptions` proto with a `repeated` field along the lines of `gpu_id`, which select which of the `CUDA_VISIBLE_DEVICES` to use. This seems cleaner, but I'm not sure how invasive that change would be (when does Tensorflow allocate GPU memory, call `cuGetDevice`, and otherwise initialize the StreamExecutor? Is it upon `tf.Session()`? Or is it earlier? If it's earlier, this change may be tricky to implement.)\n\nFinally, if I were to implement option (2) or (1), what would I need to do in order to get the contribution accepted to Tensorflow?\n"}