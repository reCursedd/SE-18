{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242881223", "html_url": "https://github.com/tensorflow/tensorflow/issues/1888#issuecomment-242881223", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1888", "id": 242881223, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mjg4MTIyMw==", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-27T00:07:11Z", "updated_at": "2016-08-27T00:07:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I just tested this patch. It does not work. Please re-open this bug!</p>\n<p>You can verify as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nconfig <span class=\"pl-k\">=</span> tf.ConfigProto(tf.GPUOptions(<span class=\"pl-v\">visible_device_list</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>0<span class=\"pl-pds\">\"</span></span>))\nsession <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config)\n\n<span class=\"pl-k\">import</span> subprocess\nsubprocess.check_call(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>nvidia-smi<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>The output you will see will look like this on a 2-GPU system:</p>\n<pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     19236    C   /mnt/home/gibiansky/env/bin/python           10893MiB |\n|    1     19236    C   /mnt/home/gibiansky/env/bin/python              73MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>Specifically, note how there are <em>two</em> contexts opened, not just one! That means that no other process can access <em>either</em> GPU.</p>\n<p>The reason this is happening is because <code>platform-&gt;ExecutorForDevice(device)</code> is being called in more places than what you have addressed. For example, take a look at these pieces of code, that are called upon session startup:</p>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/stream_executor/platform.cc#L96\">platform.cc:96</a></li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/stream_executor/platform.cc#L113\">platform.cc:113</a></li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/common_runtime/gpu/gpu_init.cc#L33\">gpu_init.cc:38</a></li>\n</ul>\n<p>I believe there are others as well; you can get a complete list with <code>grep -iIr \"-&gt;ExecutorForDevice\" /path/to/tensorflow/directory</code>.</p>\n<p>I believe the trickiest bit is in <code>gpu_init.cc</code>. Specifically, if you look at <code>gpu_init.cc:161</code>, you see that the first time <code>GPUMachineManager()</code> is called (which happens upon Session creation), it calls <code>InitModule()</code>. <code>InitModule</code> in turn calls <code>InitGPU()</code>, which has the devilish line of code:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">int</span> dev_count = platform-&gt;<span class=\"pl-en\">VisibleDeviceCount</span>();\n...\n<span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> i = <span class=\"pl-c1\">0</span>; i &lt; dev_count; ++i) {\n  <span class=\"pl-k\">auto</span> executor = platform-&gt;<span class=\"pl-c1\">ExecutorForDevice</span>(i);\n  ...\n}</pre></div>\n<p>What this means is that the first time anything related to GPUs happens (and thus a GPU machine manager is created), you find out the number of visible devices and initialize every single one of them, <em>no matter what</em>. I emphasize the \"no matter what\" because GPUMachineManager() takes no parameters, and neither does <code>InitModule</code>. Because all of this is static global state there is nothing you can do per-session that fixes this.</p>\n<p>My approach was a bit different, and IMHO uglier but less fragile:</p>\n<ul>\n<li>Inside <code>CUDAPlatform</code>, check for a <code>TENSORFLOW_VISIBLE_DEVICES</code> environment variable, which was exactly the same in meaning as your <code>visible_device_list</code> configuration option.</li>\n<li>If that environment variable exists, then instead of returning <code>CUDADriver::DeviceCount()</code> from <code>VisibleDeviceCount()</code>, return <code>visible_device_list.size()</code>, where <code>visible_device_list</code> is the variable holding the parsed contents of <code>TENSORFLOW_VISIBLE_DEVICES</code>.</li>\n<li>Whenever you call <code>ExecutorForDevice</code> (or any variant thereof), remap the provided ordinal as directed by <code>visible_device_list</code>. That way, you <em>know</em> that Tensorflow cannot accidentally claim more GPUs than it is allowed to, because <code>CUDAPlatform</code> is the only available interface to GPUs, and it does not allow any caller to touch GPUs that it should not be seeing.</li>\n</ul>\n<p>What do you think? Is that a viable strategy? The current patch does not solve this problem at all (whereas the approach described above is uglier but fixes the issue). There <em>is</em> an alternative which would be refactoring <code>CUDAPlatform</code> to be a part of the session state instead of the global device, and then having things like <code>GPUMachineManager</code> take and store a <code>CUDAPlatform</code>. This might be a pretty invasive change though and I'm not sure how many other pieces of global state would have to be pulled into local state in order to get this to work.</p>", "body_text": "I just tested this patch. It does not work. Please re-open this bug!\nYou can verify as follows:\nimport tensorflow as tf\nconfig = tf.ConfigProto(tf.GPUOptions(visible_device_list=\"0\"))\nsession = tf.Session(config=config)\n\nimport subprocess\nsubprocess.check_call(\"nvidia-smi\")\nThe output you will see will look like this on a 2-GPU system:\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     19236    C   /mnt/home/gibiansky/env/bin/python           10893MiB |\n|    1     19236    C   /mnt/home/gibiansky/env/bin/python              73MiB |\n+-----------------------------------------------------------------------------+\n\nSpecifically, note how there are two contexts opened, not just one! That means that no other process can access either GPU.\nThe reason this is happening is because platform->ExecutorForDevice(device) is being called in more places than what you have addressed. For example, take a look at these pieces of code, that are called upon session startup:\n\nplatform.cc:96\nplatform.cc:113\ngpu_init.cc:38\n\nI believe there are others as well; you can get a complete list with grep -iIr \"->ExecutorForDevice\" /path/to/tensorflow/directory.\nI believe the trickiest bit is in gpu_init.cc. Specifically, if you look at gpu_init.cc:161, you see that the first time GPUMachineManager() is called (which happens upon Session creation), it calls InitModule(). InitModule in turn calls InitGPU(), which has the devilish line of code:\nint dev_count = platform->VisibleDeviceCount();\n...\nfor (int i = 0; i < dev_count; ++i) {\n  auto executor = platform->ExecutorForDevice(i);\n  ...\n}\nWhat this means is that the first time anything related to GPUs happens (and thus a GPU machine manager is created), you find out the number of visible devices and initialize every single one of them, no matter what. I emphasize the \"no matter what\" because GPUMachineManager() takes no parameters, and neither does InitModule. Because all of this is static global state there is nothing you can do per-session that fixes this.\nMy approach was a bit different, and IMHO uglier but less fragile:\n\nInside CUDAPlatform, check for a TENSORFLOW_VISIBLE_DEVICES environment variable, which was exactly the same in meaning as your visible_device_list configuration option.\nIf that environment variable exists, then instead of returning CUDADriver::DeviceCount() from VisibleDeviceCount(), return visible_device_list.size(), where visible_device_list is the variable holding the parsed contents of TENSORFLOW_VISIBLE_DEVICES.\nWhenever you call ExecutorForDevice (or any variant thereof), remap the provided ordinal as directed by visible_device_list. That way, you know that Tensorflow cannot accidentally claim more GPUs than it is allowed to, because CUDAPlatform is the only available interface to GPUs, and it does not allow any caller to touch GPUs that it should not be seeing.\n\nWhat do you think? Is that a viable strategy? The current patch does not solve this problem at all (whereas the approach described above is uglier but fixes the issue). There is an alternative which would be refactoring CUDAPlatform to be a part of the session state instead of the global device, and then having things like GPUMachineManager take and store a CUDAPlatform. This might be a pretty invasive change though and I'm not sure how many other pieces of global state would have to be pulled into local state in order to get this to work.", "body": "I just tested this patch. It does not work. Please re-open this bug!\n\nYou can verify as follows:\n\n``` python\nimport tensorflow as tf\nconfig = tf.ConfigProto(tf.GPUOptions(visible_device_list=\"0\"))\nsession = tf.Session(config=config)\n\nimport subprocess\nsubprocess.check_call(\"nvidia-smi\")\n```\n\nThe output you will see will look like this on a 2-GPU system:\n\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     19236    C   /mnt/home/gibiansky/env/bin/python           10893MiB |\n|    1     19236    C   /mnt/home/gibiansky/env/bin/python              73MiB |\n+-----------------------------------------------------------------------------+\n```\n\nSpecifically, note how there are _two_ contexts opened, not just one! That means that no other process can access _either_ GPU.\n\nThe reason this is happening is because `platform->ExecutorForDevice(device)` is being called in more places than what you have addressed. For example, take a look at these pieces of code, that are called upon session startup:\n- [platform.cc:96](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/stream_executor/platform.cc#L96)\n- [platform.cc:113](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/stream_executor/platform.cc#L113)\n- [gpu_init.cc:38](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/common_runtime/gpu/gpu_init.cc#L33)\n\nI believe there are others as well; you can get a complete list with `grep -iIr \"->ExecutorForDevice\" /path/to/tensorflow/directory`.\n\nI believe the trickiest bit is in `gpu_init.cc`. Specifically, if you look at `gpu_init.cc:161`, you see that the first time `GPUMachineManager()` is called (which happens upon Session creation), it calls `InitModule()`. `InitModule` in turn calls `InitGPU()`, which has the devilish line of code:\n\n``` c++\nint dev_count = platform->VisibleDeviceCount();\n...\nfor (int i = 0; i < dev_count; ++i) {\n  auto executor = platform->ExecutorForDevice(i);\n  ...\n}\n```\n\nWhat this means is that the first time anything related to GPUs happens (and thus a GPU machine manager is created), you find out the number of visible devices and initialize every single one of them, _no matter what_. I emphasize the \"no matter what\" because GPUMachineManager() takes no parameters, and neither does `InitModule`. Because all of this is static global state there is nothing you can do per-session that fixes this.\n\nMy approach was a bit different, and IMHO uglier but less fragile:\n- Inside `CUDAPlatform`, check for a `TENSORFLOW_VISIBLE_DEVICES` environment variable, which was exactly the same in meaning as your `visible_device_list` configuration option.\n- If that environment variable exists, then instead of returning `CUDADriver::DeviceCount()` from `VisibleDeviceCount()`, return `visible_device_list.size()`, where `visible_device_list` is the variable holding the parsed contents of `TENSORFLOW_VISIBLE_DEVICES`.\n- Whenever you call `ExecutorForDevice` (or any variant thereof), remap the provided ordinal as directed by `visible_device_list`. That way, you _know_ that Tensorflow cannot accidentally claim more GPUs than it is allowed to, because `CUDAPlatform` is the only available interface to GPUs, and it does not allow any caller to touch GPUs that it should not be seeing.\n\nWhat do you think? Is that a viable strategy? The current patch does not solve this problem at all (whereas the approach described above is uglier but fixes the issue). There _is_ an alternative which would be refactoring `CUDAPlatform` to be a part of the session state instead of the global device, and then having things like `GPUMachineManager` take and store a `CUDAPlatform`. This might be a pretty invasive change though and I'm not sure how many other pieces of global state would have to be pulled into local state in order to get this to work.\n"}