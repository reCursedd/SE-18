{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242239771", "html_url": "https://github.com/tensorflow/tensorflow/issues/1888#issuecomment-242239771", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1888", "id": 242239771, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MjIzOTc3MQ==", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-24T23:30:25Z", "updated_at": "2016-08-24T23:30:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7943790\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zffchen78\">@zffchen78</a> If I were to implement this, what strategy would be most likely to get accepted as a patch to tensorflow?</p>\n<p>The least invasive strategy is one which uses an environment variable, because GPUs are allocated  statically; In <code>gpu_init.cc</code> <code>InitModule()</code> is called, which eventually calls <code>ExecutorForDevice</code>, which allocates the GPUs. The GPUs aren't stored in the <code>Session</code>, and are not freed after a session ends; the <code>CUDAPlatform</code> is also stored statically and initialized on load. Effectively the architecture was not designed to allow the choice of GPUs to happen dynamically at runtime, but since environment variables are also global and exist at startup, this would be straightforward to implement.</p>\n<p>An alternative route would be to fix the main design issue: Have a <code>Session</code> track which GPUs its using, and, when it's done, deallocate the GPUs and destroy the <code>StreamExecutor</code> contexts. To do this we would probably have to move the <code>Platform</code> from a global static variable into a <code>Session</code>-local one, and have it be initialized on session startup rather than global startup.</p>\n<p>Please, let me know which of these is most likely to be accepted by the Tensorflow team if this is implemented and tested. If this is not a viable modification, please remove the \"contributions welcome\" tag.</p>", "body_text": "@zffchen78 If I were to implement this, what strategy would be most likely to get accepted as a patch to tensorflow?\nThe least invasive strategy is one which uses an environment variable, because GPUs are allocated  statically; In gpu_init.cc InitModule() is called, which eventually calls ExecutorForDevice, which allocates the GPUs. The GPUs aren't stored in the Session, and are not freed after a session ends; the CUDAPlatform is also stored statically and initialized on load. Effectively the architecture was not designed to allow the choice of GPUs to happen dynamically at runtime, but since environment variables are also global and exist at startup, this would be straightforward to implement.\nAn alternative route would be to fix the main design issue: Have a Session track which GPUs its using, and, when it's done, deallocate the GPUs and destroy the StreamExecutor contexts. To do this we would probably have to move the Platform from a global static variable into a Session-local one, and have it be initialized on session startup rather than global startup.\nPlease, let me know which of these is most likely to be accepted by the Tensorflow team if this is implemented and tested. If this is not a viable modification, please remove the \"contributions welcome\" tag.", "body": "@zffchen78 If I were to implement this, what strategy would be most likely to get accepted as a patch to tensorflow?\n\nThe least invasive strategy is one which uses an environment variable, because GPUs are allocated  statically; In `gpu_init.cc` `InitModule()` is called, which eventually calls `ExecutorForDevice`, which allocates the GPUs. The GPUs aren't stored in the `Session`, and are not freed after a session ends; the `CUDAPlatform` is also stored statically and initialized on load. Effectively the architecture was not designed to allow the choice of GPUs to happen dynamically at runtime, but since environment variables are also global and exist at startup, this would be straightforward to implement.\n\nAn alternative route would be to fix the main design issue: Have a `Session` track which GPUs its using, and, when it's done, deallocate the GPUs and destroy the `StreamExecutor` contexts. To do this we would probably have to move the `Platform` from a global static variable into a `Session`-local one, and have it be initialized on session startup rather than global startup.\n\nPlease, let me know which of these is most likely to be accepted by the Tensorflow team if this is implemented and tested. If this is not a viable modification, please remove the \"contributions welcome\" tag. \n"}