{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11365", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11365/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11365/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11365/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11365", "id": 241416991, "node_id": "MDU6SXNzdWUyNDE0MTY5OTE=", "number": 11365, "title": "seq2seq.AttentionWrapper cannot implement Bahdanau model (RNNsearch)", "user": {"login": "jagthebeetle", "id": 6564473, "node_id": "MDQ6VXNlcjY1NjQ0NzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6564473?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jagthebeetle", "html_url": "https://github.com/jagthebeetle", "followers_url": "https://api.github.com/users/jagthebeetle/followers", "following_url": "https://api.github.com/users/jagthebeetle/following{/other_user}", "gists_url": "https://api.github.com/users/jagthebeetle/gists{/gist_id}", "starred_url": "https://api.github.com/users/jagthebeetle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jagthebeetle/subscriptions", "organizations_url": "https://api.github.com/users/jagthebeetle/orgs", "repos_url": "https://api.github.com/users/jagthebeetle/repos", "events_url": "https://api.github.com/users/jagthebeetle/events{/privacy}", "received_events_url": "https://api.github.com/users/jagthebeetle/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-07-08T01:58:59Z", "updated_at": "2017-07-13T21:01:12Z", "closed_at": "2017-07-13T19:12:20Z", "author_association": "CONTRIBUTOR", "body_html": "<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes, but there is no stock example script</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: OSX 10.12.5</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source (via virtualenv/pip3)</li>\n<li><strong>TensorFlow version (use command below)</strong>:v1.2.0-5-g435cdfc 1.2.1</li>\n<li><strong>Python version</strong>: 3.6.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:  N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p><a href=\"https://arxiv.org/pdf/1409.0473.pdf\" rel=\"nofollow\">RNNsearch</a> has two different RNN cells in it (the encoder and decoder). At a given time step, the inputs to the attention mechanism are all the encoder's outputs (the \"annotations\" in the Bahdanau paper, aka the <code>memory</code> of the <code>BahdanauAttention</code> constructor) and the decoder's previous state.</p>\n<p>Crucially, <code>AttentionWrapper.call</code> is running its input through the passed in <code>cell</code> before applying <code>AttentionMechanism</code> to the cell's output. (<a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper#call\" rel=\"nofollow\">Docstring</a>; source confirms this.)</p>\n<p>This precludes the encoder cell from being passed to AttentionWrapper, because AttentionWrapper depends on BahdanauAttention, whose <code>memory</code> depends on the encoder cell's output. (So you would need to run the encoder once to get the <code>memory</code> and <code>AttentionWrapper</code> would run it again.)</p>\n<p>But the decoder cell can also not be the cell passed in, because in the Bahdanau paper, the decoder's output is not used by the attention mechanism at all. Moreover, neither cell takes the previous step's attention (in Bahdanau).</p>\n<p>I'm hoping I've misunderstood, but currently the API doesn't seem like it can be made to align with the paper, which would be sort of curious \u2013 but perhaps intentional!</p>\n<h3>Source code / logs</h3>\n<pre><code>bahdanau = tf.contrib.seq2seq.BahdanauAttention(\n    num_units=params['ATTENTION_SIZE'],\n    memory=annotations,  # annotations, _ = tf.nn.static_rnn(encoder, time_major_input)\n    normalize=False,\n    name='BahdanauAttention')\ndecoder = tf.nn.rnn_cell.BasicLSTMCell(\n    params['DECODER_SIZE'],\n    forget_bias=1.0)\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(\n    cell=decoder,\n    attention_mechanism=bahdanau,\n    output_attention=False,\n    name=\"AttentionWrappedDecoder\")\n</code></pre>", "body_text": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but there is no stock example script\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.12.5\nTensorFlow installed from (source or binary): source (via virtualenv/pip3)\nTensorFlow version (use command below):v1.2.0-5-g435cdfc 1.2.1\nPython version: 3.6.1\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce:  N/A\n\nDescribe the problem\nRNNsearch has two different RNN cells in it (the encoder and decoder). At a given time step, the inputs to the attention mechanism are all the encoder's outputs (the \"annotations\" in the Bahdanau paper, aka the memory of the BahdanauAttention constructor) and the decoder's previous state.\nCrucially, AttentionWrapper.call is running its input through the passed in cell before applying AttentionMechanism to the cell's output. (Docstring; source confirms this.)\nThis precludes the encoder cell from being passed to AttentionWrapper, because AttentionWrapper depends on BahdanauAttention, whose memory depends on the encoder cell's output. (So you would need to run the encoder once to get the memory and AttentionWrapper would run it again.)\nBut the decoder cell can also not be the cell passed in, because in the Bahdanau paper, the decoder's output is not used by the attention mechanism at all. Moreover, neither cell takes the previous step's attention (in Bahdanau).\nI'm hoping I've misunderstood, but currently the API doesn't seem like it can be made to align with the paper, which would be sort of curious \u2013 but perhaps intentional!\nSource code / logs\nbahdanau = tf.contrib.seq2seq.BahdanauAttention(\n    num_units=params['ATTENTION_SIZE'],\n    memory=annotations,  # annotations, _ = tf.nn.static_rnn(encoder, time_major_input)\n    normalize=False,\n    name='BahdanauAttention')\ndecoder = tf.nn.rnn_cell.BasicLSTMCell(\n    params['DECODER_SIZE'],\n    forget_bias=1.0)\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(\n    cell=decoder,\n    attention_mechanism=bahdanau,\n    output_attention=False,\n    name=\"AttentionWrappedDecoder\")", "body": "\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, but there is no stock example script\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.12.5\r\n- **TensorFlow installed from (source or binary)**: source (via virtualenv/pip3)\r\n- **TensorFlow version (use command below)**:v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:  N/A\r\n\r\n### Describe the problem\r\n[RNNsearch](https://arxiv.org/pdf/1409.0473.pdf) has two different RNN cells in it (the encoder and decoder). At a given time step, the inputs to the attention mechanism are all the encoder's outputs (the \"annotations\" in the Bahdanau paper, aka the `memory` of the `BahdanauAttention` constructor) and the decoder's previous state.\r\n\r\nCrucially, `AttentionWrapper.call` is running its input through the passed in `cell` before applying `AttentionMechanism` to the cell's output. ([Docstring](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper#call); source confirms this.)\r\n\r\nThis precludes the encoder cell from being passed to AttentionWrapper, because AttentionWrapper depends on BahdanauAttention, whose `memory` depends on the encoder cell's output. (So you would need to run the encoder once to get the `memory` and `AttentionWrapper` would run it again.)\r\n\r\nBut the decoder cell can also not be the cell passed in, because in the Bahdanau paper, the decoder's output is not used by the attention mechanism at all. Moreover, neither cell takes the previous step's attention (in Bahdanau).\r\n\r\nI'm hoping I've misunderstood, but currently the API doesn't seem like it can be made to align with the paper, which would be sort of curious \u2013 but perhaps intentional!\r\n\r\n### Source code / logs\r\n    bahdanau = tf.contrib.seq2seq.BahdanauAttention(\r\n        num_units=params['ATTENTION_SIZE'],\r\n        memory=annotations,  # annotations, _ = tf.nn.static_rnn(encoder, time_major_input)\r\n        normalize=False,\r\n        name='BahdanauAttention')\r\n    decoder = tf.nn.rnn_cell.BasicLSTMCell(\r\n        params['DECODER_SIZE'],\r\n        forget_bias=1.0)\r\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n        cell=decoder,\r\n        attention_mechanism=bahdanau,\r\n        output_attention=False,\r\n        name=\"AttentionWrappedDecoder\")\r\n"}