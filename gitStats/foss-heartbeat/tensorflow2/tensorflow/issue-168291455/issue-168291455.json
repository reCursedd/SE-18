{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3563", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3563/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3563/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3563/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3563", "id": 168291455, "node_id": "MDU6SXNzdWUxNjgyOTE0NTU=", "number": 3563, "title": "[BUG?] Memory overflow (?) on Nvidia Quadro M6000 using feed dictionary with conv2d", "user": {"login": "eickenberg", "id": 1306635, "node_id": "MDQ6VXNlcjEzMDY2MzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/1306635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eickenberg", "html_url": "https://github.com/eickenberg", "followers_url": "https://api.github.com/users/eickenberg/followers", "following_url": "https://api.github.com/users/eickenberg/following{/other_user}", "gists_url": "https://api.github.com/users/eickenberg/gists{/gist_id}", "starred_url": "https://api.github.com/users/eickenberg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eickenberg/subscriptions", "organizations_url": "https://api.github.com/users/eickenberg/orgs", "repos_url": "https://api.github.com/users/eickenberg/repos", "events_url": "https://api.github.com/users/eickenberg/events{/privacy}", "received_events_url": "https://api.github.com/users/eickenberg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-07-29T09:59:33Z", "updated_at": "2017-01-23T23:13:50Z", "closed_at": "2017-01-23T23:13:50Z", "author_association": "NONE", "body_html": "<p>Hi everybody,</p>\n<p>we just got several machines with Quadro M6000 (12G RAM) for our lab, and they all seem to suffer the same problem when feeding too much data to a convolution via a feed dictionary. The problem does not appear when using CPU on the same machine, and neither when using an equivalent machine with a different GPU (GeForce GTX Titan X).</p>\n<p>It looks like when using a feed dictionary to provide data for the evaluation of a graph, part of the weights in the graph may be overwritten depending on the size of the data fed through the dictionary.</p>\n<h4>Two ways to reproduce</h4>\n<h5>A minimal example</h5>\n<pre><code>import tensorflow as tf\nimport numpy as np\nx = tf.placeholder(tf.float32, shape=(None, 1024))\nx_r = tf.reshape(x, (-1, 32, 32, 1))\nW = tf.Variable(tf.ones((5, 5, 1, 32)))\nW_sum = tf.reduce_sum(W)\nconv = tf.nn.conv2d(x_r, W, strides=(1, 1, 1, 1), padding='SAME')\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\ndata = np.ones((5001, 1024), dtype='float32')\nprint(sess.run(W_sum))\nc = sess.run(conv, feed_dict={x: data})\nprint(sess.run(W_sum))\nsess.close()\n</code></pre>\n<p>It outputs</p>\n<pre><code>800.0\n0.0\n</code></pre>\n<p>which indicates that <code>W</code> has changed through the feed-dictionary evaluation of the convolution.</p>\n<p>Note that</p>\n<ul>\n<li>By reducing <code>data.shape[0]</code> to 5000 or less, the problem disappears</li>\n<li>Going up to 5070, the second value stays at 0.0.</li>\n<li>at exactly 5080 the problem seems to disappear (we get <code>800.0 800.0</code>)</li>\n<li>from 5100 the second number can take arbitrary values (not zero)</li>\n<li>The problem does not arise when replacing the <code>tf.nn.conv2d</code> operation by an equivalently sized  <code>matmul</code> operation (same dimensions of weight vector, not same dimensionality of output)</li>\n<li>EDIT: (that said, see below, in the MNIST tutorial all weight vectors are affected, not only ones pertaining to convolution. But that doesn't necessarily contradict the convolution being the culprit)</li>\n</ul>\n<h5>The expert MNIST tutorial</h5>\n<p>The problem also arises in one of the very first tutorials <a href=\"https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html\" rel=\"nofollow\">https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html</a></p>\n<p>Everything works perfect until the last line of the tutorial, where a large array is fed by feed-dictionary for test accuracy. This modifies all the weight vectors of the convnet!</p>\n<p>Again, the error disappears when feeding only 5000 test examples instead of 10000 to the architecture.</p>\n<h4>Environment</h4>\n<ul>\n<li>OS: Ubuntu 1504</li>\n<li>CUDA version 7.5</li>\n<li>cuDNN version 4</li>\n<li>library files</li>\n</ul>\n<pre><code>(tensorflow) meickenb@g-1504:~/code/tensorflow$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -&gt; libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug  15  2015 /usr/local/cuda/lib64/libcudart_static.a\n\n(tensorflow) meickenb@g-1504:/usr/lib/x86_64-linux-gnu$ ls -l libcudnn*\nlrwxrwxrwx 1 root root       13 Jul 21 18:22 libcudnn.so -&gt; libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Jul 21 18:22 libcudnn.so.4 -&gt; libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Jul 21 18:22 libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Jul 21 18:22 libcudnn_static.a\n</code></pre>\n<ul>\n<li>Anaconda 3</li>\n<li>pip install from wheel <code>https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl</code></li>\n<li>tf version 0.9.0</li>\n</ul>\n<pre><code>(tensorflow) meickenb@g-1504:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.9.0\n\n</code></pre>\n<p>Please let me know if you need any other info, or what else I could try to narrow down the reason for this overflow.</p>\n<p>CC @tmanglesl <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4996273\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/CarmineCella\">@CarmineCella</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10076572\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/AndreuxMath\">@AndreuxMath</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=14082309\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/beedotkiran\">@beedotkiran</a></p>", "body_text": "Hi everybody,\nwe just got several machines with Quadro M6000 (12G RAM) for our lab, and they all seem to suffer the same problem when feeding too much data to a convolution via a feed dictionary. The problem does not appear when using CPU on the same machine, and neither when using an equivalent machine with a different GPU (GeForce GTX Titan X).\nIt looks like when using a feed dictionary to provide data for the evaluation of a graph, part of the weights in the graph may be overwritten depending on the size of the data fed through the dictionary.\nTwo ways to reproduce\nA minimal example\nimport tensorflow as tf\nimport numpy as np\nx = tf.placeholder(tf.float32, shape=(None, 1024))\nx_r = tf.reshape(x, (-1, 32, 32, 1))\nW = tf.Variable(tf.ones((5, 5, 1, 32)))\nW_sum = tf.reduce_sum(W)\nconv = tf.nn.conv2d(x_r, W, strides=(1, 1, 1, 1), padding='SAME')\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\ndata = np.ones((5001, 1024), dtype='float32')\nprint(sess.run(W_sum))\nc = sess.run(conv, feed_dict={x: data})\nprint(sess.run(W_sum))\nsess.close()\n\nIt outputs\n800.0\n0.0\n\nwhich indicates that W has changed through the feed-dictionary evaluation of the convolution.\nNote that\n\nBy reducing data.shape[0] to 5000 or less, the problem disappears\nGoing up to 5070, the second value stays at 0.0.\nat exactly 5080 the problem seems to disappear (we get 800.0 800.0)\nfrom 5100 the second number can take arbitrary values (not zero)\nThe problem does not arise when replacing the tf.nn.conv2d operation by an equivalently sized  matmul operation (same dimensions of weight vector, not same dimensionality of output)\nEDIT: (that said, see below, in the MNIST tutorial all weight vectors are affected, not only ones pertaining to convolution. But that doesn't necessarily contradict the convolution being the culprit)\n\nThe expert MNIST tutorial\nThe problem also arises in one of the very first tutorials https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html\nEverything works perfect until the last line of the tutorial, where a large array is fed by feed-dictionary for test accuracy. This modifies all the weight vectors of the convnet!\nAgain, the error disappears when feeding only 5000 test examples instead of 10000 to the architecture.\nEnvironment\n\nOS: Ubuntu 1504\nCUDA version 7.5\ncuDNN version 4\nlibrary files\n\n(tensorflow) meickenb@g-1504:~/code/tensorflow$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug  15  2015 /usr/local/cuda/lib64/libcudart_static.a\n\n(tensorflow) meickenb@g-1504:/usr/lib/x86_64-linux-gnu$ ls -l libcudnn*\nlrwxrwxrwx 1 root root       13 Jul 21 18:22 libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Jul 21 18:22 libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Jul 21 18:22 libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Jul 21 18:22 libcudnn_static.a\n\n\nAnaconda 3\npip install from wheel https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl\ntf version 0.9.0\n\n(tensorflow) meickenb@g-1504:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.9.0\n\n\nPlease let me know if you need any other info, or what else I could try to narrow down the reason for this overflow.\nCC @tmanglesl @CarmineCella @AndreuxMath @beedotkiran", "body": "Hi everybody,\n\nwe just got several machines with Quadro M6000 (12G RAM) for our lab, and they all seem to suffer the same problem when feeding too much data to a convolution via a feed dictionary. The problem does not appear when using CPU on the same machine, and neither when using an equivalent machine with a different GPU (GeForce GTX Titan X).\n\nIt looks like when using a feed dictionary to provide data for the evaluation of a graph, part of the weights in the graph may be overwritten depending on the size of the data fed through the dictionary.\n#### Two ways to reproduce\n##### A minimal example\n\n```\nimport tensorflow as tf\nimport numpy as np\nx = tf.placeholder(tf.float32, shape=(None, 1024))\nx_r = tf.reshape(x, (-1, 32, 32, 1))\nW = tf.Variable(tf.ones((5, 5, 1, 32)))\nW_sum = tf.reduce_sum(W)\nconv = tf.nn.conv2d(x_r, W, strides=(1, 1, 1, 1), padding='SAME')\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\ndata = np.ones((5001, 1024), dtype='float32')\nprint(sess.run(W_sum))\nc = sess.run(conv, feed_dict={x: data})\nprint(sess.run(W_sum))\nsess.close()\n```\n\nIt outputs\n\n```\n800.0\n0.0\n```\n\nwhich indicates that `W` has changed through the feed-dictionary evaluation of the convolution.\n\nNote that\n- By reducing `data.shape[0]` to 5000 or less, the problem disappears\n- Going up to 5070, the second value stays at 0.0. \n- at exactly 5080 the problem seems to disappear (we get `800.0 800.0`)\n- from 5100 the second number can take arbitrary values (not zero)\n- The problem does not arise when replacing the `tf.nn.conv2d` operation by an equivalently sized  `matmul` operation (same dimensions of weight vector, not same dimensionality of output)\n- EDIT: (that said, see below, in the MNIST tutorial all weight vectors are affected, not only ones pertaining to convolution. But that doesn't necessarily contradict the convolution being the culprit)\n##### The expert MNIST tutorial\n\nThe problem also arises in one of the very first tutorials https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html\n\nEverything works perfect until the last line of the tutorial, where a large array is fed by feed-dictionary for test accuracy. This modifies all the weight vectors of the convnet!\n\nAgain, the error disappears when feeding only 5000 test examples instead of 10000 to the architecture.\n#### Environment\n- OS: Ubuntu 1504\n- CUDA version 7.5\n- cuDNN version 4\n- library files\n\n```\n(tensorflow) meickenb@g-1504:~/code/tensorflow$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug  15  2015 /usr/local/cuda/lib64/libcudart_static.a\n\n(tensorflow) meickenb@g-1504:/usr/lib/x86_64-linux-gnu$ ls -l libcudnn*\nlrwxrwxrwx 1 root root       13 Jul 21 18:22 libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Jul 21 18:22 libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Jul 21 18:22 libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Jul 21 18:22 libcudnn_static.a\n```\n- Anaconda 3\n- pip install from wheel `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl`\n- tf version 0.9.0\n\n```\n(tensorflow) meickenb@g-1504:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.9.0\n\n```\n\nPlease let me know if you need any other info, or what else I could try to narrow down the reason for this overflow.\n\nCC @tmanglesl @CarmineCella @andreuxmath @beedotkiran\n"}