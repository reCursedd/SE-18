{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21338", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21338/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21338/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21338/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21338", "id": 346963930, "node_id": "MDU6SXNzdWUzNDY5NjM5MzA=", "number": 21338, "title": "Max pooling cause error on empty batch", "user": {"login": "wrongtest", "id": 7600935, "node_id": "MDQ6VXNlcjc2MDA5MzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/7600935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wrongtest", "html_url": "https://github.com/wrongtest", "followers_url": "https://api.github.com/users/wrongtest/followers", "following_url": "https://api.github.com/users/wrongtest/following{/other_user}", "gists_url": "https://api.github.com/users/wrongtest/gists{/gist_id}", "starred_url": "https://api.github.com/users/wrongtest/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wrongtest/subscriptions", "organizations_url": "https://api.github.com/users/wrongtest/orgs", "repos_url": "https://api.github.com/users/wrongtest/repos", "events_url": "https://api.github.com/users/wrongtest/events{/privacy}", "received_events_url": "https://api.github.com/users/wrongtest/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "chsigg", "id": 7523982, "node_id": "MDQ6VXNlcjc1MjM5ODI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7523982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chsigg", "html_url": "https://github.com/chsigg", "followers_url": "https://api.github.com/users/chsigg/followers", "following_url": "https://api.github.com/users/chsigg/following{/other_user}", "gists_url": "https://api.github.com/users/chsigg/gists{/gist_id}", "starred_url": "https://api.github.com/users/chsigg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chsigg/subscriptions", "organizations_url": "https://api.github.com/users/chsigg/orgs", "repos_url": "https://api.github.com/users/chsigg/repos", "events_url": "https://api.github.com/users/chsigg/events{/privacy}", "received_events_url": "https://api.github.com/users/chsigg/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "chsigg", "id": 7523982, "node_id": "MDQ6VXNlcjc1MjM5ODI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7523982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chsigg", "html_url": "https://github.com/chsigg", "followers_url": "https://api.github.com/users/chsigg/followers", "following_url": "https://api.github.com/users/chsigg/following{/other_user}", "gists_url": "https://api.github.com/users/chsigg/gists{/gist_id}", "starred_url": "https://api.github.com/users/chsigg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chsigg/subscriptions", "organizations_url": "https://api.github.com/users/chsigg/orgs", "repos_url": "https://api.github.com/users/chsigg/repos", "events_url": "https://api.github.com/users/chsigg/events{/privacy}", "received_events_url": "https://api.github.com/users/chsigg/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2018-08-02T11:04:05Z", "updated_at": "2018-11-14T00:52:39Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: 3.10.0-693.2.2.el7.x86_64</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>: Python 2.7.14 :: Anaconda</li>\n<li><strong>Bazel version (if compiling from source)</strong>: None</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: None</li>\n<li><strong>CUDA/cuDNN version</strong>: cuda==9.0, cudnn==7.0.4</li>\n<li><strong>GPU model and memory</strong>: None</li>\n<li><strong>Exact command to reproduce</strong>: See below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When batch_size is 0, max pooling operation seems to produce an unhandled cudaError_t status. It may cause subsequent operations fail with odd error message. That is extremely difficult to debug.</p>\n<p>(This corner case bothers us, where we first extract some bounding boxes and then run traditional convolution operations on areas specified by them. The above error occurs in case that no bounding boxes are detected thus batch_size becomes 0. However, the python exception will be randomly thrown at following operation or following session run steps)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nx <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">1</span>])\npool_op <span class=\"pl-k\">=</span> tf.nn.pool(x, <span class=\"pl-v\">pooling_type</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MAX<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">window_shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SAME<span class=\"pl-pds\">\"</span></span>)\n\ny <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>])\nother_op <span class=\"pl-k\">=</span> tf.where(tf.equal(y, <span class=\"pl-c1\">1.0</span>))\n\nnormal_data <span class=\"pl-k\">=</span> np.zeros([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float32<span class=\"pl-pds\">\"</span></span>)\nempty_data <span class=\"pl-k\">=</span> np.zeros([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float32<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> cudaError is thread local, limit thread pool size to make it easy to reproduce</span>\nconfig <span class=\"pl-k\">=</span> tf.ConfigProto()\nconfig.inter_op_parallelism_threads <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> run other_op success</span>\n    <span class=\"pl-c1\">print</span> sess.run(other_op, {y: [<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">2.0</span>, <span class=\"pl-c1\">3.0</span>, <span class=\"pl-c1\">4.0</span>]})  <span class=\"pl-c\"><span class=\"pl-c\">#</span> [[0]]</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> run pooling on datas success</span>\n    <span class=\"pl-c1\">print</span> sess.run(pool_op, {x: normal_data}).shape  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (1, 4, 4, 1)</span>\n    <span class=\"pl-c1\">print</span> sess.run(pool_op, {x: empty_data}).shape  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (0, 4, 4, 1)</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> run other_op now failed</span>\n    <span class=\"pl-c1\">print</span> sess.run(other_op, {y: [<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">2.0</span>, <span class=\"pl-c1\">3.0</span>, <span class=\"pl-c1\">4.0</span>]})  <span class=\"pl-c\"><span class=\"pl-c\">#</span> err</span></pre></div>\n<p>Above code report error:<br>\ntensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices.  temp_storage_bytes: 1, status: invalid configuration argument</p>\n<p>\"invalid configuration argument\" seems to be message return by cudaGetError, which indicates a failed kernel launch due to zero or too large number of block threads.</p>\n<h3>Source code / logs</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/7600935/43579974-6dd4967a-9686-11e8-9b22-8288159d155c.png\"><img src=\"https://user-images.githubusercontent.com/7600935/43579974-6dd4967a-9686-11e8-9b22-8288159d155c.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): 3.10.0-693.2.2.el7.x86_64\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.8.0\nPython version: Python 2.7.14 :: Anaconda\nBazel version (if compiling from source): None\nGCC/Compiler version (if compiling from source): None\nCUDA/cuDNN version: cuda==9.0, cudnn==7.0.4\nGPU model and memory: None\nExact command to reproduce: See below\n\nDescribe the problem\nWhen batch_size is 0, max pooling operation seems to produce an unhandled cudaError_t status. It may cause subsequent operations fail with odd error message. That is extremely difficult to debug.\n(This corner case bothers us, where we first extract some bounding boxes and then run traditional convolution operations on areas specified by them. The above error occurs in case that no bounding boxes are detected thus batch_size becomes 0. However, the python exception will be randomly thrown at following operation or following session run steps)\nimport tensorflow as tf\nimport numpy as np\n\nx = tf.placeholder(dtype=tf.float32, shape=[None, 4, 4, 1])\npool_op = tf.nn.pool(x, pooling_type=\"MAX\", window_shape=[2, 2], strides=[1, 1], padding=\"SAME\")\n\ny = tf.placeholder(dtype=tf.float32, shape=[None])\nother_op = tf.where(tf.equal(y, 1.0))\n\nnormal_data = np.zeros([1, 4, 4, 1], dtype=\"float32\")\nempty_data = np.zeros([0, 4, 4, 1], dtype=\"float32\")\n\n# cudaError is thread local, limit thread pool size to make it easy to reproduce\nconfig = tf.ConfigProto()\nconfig.inter_op_parallelism_threads = 1\nwith tf.Session(config=config) as sess:\n    # run other_op success\n    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # [[0]]\n\n    # run pooling on datas success\n    print sess.run(pool_op, {x: normal_data}).shape  # (1, 4, 4, 1)\n    print sess.run(pool_op, {x: empty_data}).shape  # (0, 4, 4, 1)\n\n    # run other_op now failed\n    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # err\nAbove code report error:\ntensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices.  temp_storage_bytes: 1, status: invalid configuration argument\n\"invalid configuration argument\" seems to be message return by cudaGetError, which indicates a failed kernel launch due to zero or too large number of block threads.\nSource code / logs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 3.10.0-693.2.2.el7.x86_64\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: Python 2.7.14 :: Anaconda\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: cuda==9.0, cudnn==7.0.4\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nWhen batch_size is 0, max pooling operation seems to produce an unhandled cudaError_t status. It may cause subsequent operations fail with odd error message. That is extremely difficult to debug.\r\n\r\n(This corner case bothers us, where we first extract some bounding boxes and then run traditional convolution operations on areas specified by them. The above error occurs in case that no bounding boxes are detected thus batch_size becomes 0. However, the python exception will be randomly thrown at following operation or following session run steps)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.placeholder(dtype=tf.float32, shape=[None, 4, 4, 1])\r\npool_op = tf.nn.pool(x, pooling_type=\"MAX\", window_shape=[2, 2], strides=[1, 1], padding=\"SAME\")\r\n\r\ny = tf.placeholder(dtype=tf.float32, shape=[None])\r\nother_op = tf.where(tf.equal(y, 1.0))\r\n\r\nnormal_data = np.zeros([1, 4, 4, 1], dtype=\"float32\")\r\nempty_data = np.zeros([0, 4, 4, 1], dtype=\"float32\")\r\n\r\n# cudaError is thread local, limit thread pool size to make it easy to reproduce\r\nconfig = tf.ConfigProto()\r\nconfig.inter_op_parallelism_threads = 1\r\nwith tf.Session(config=config) as sess:\r\n    # run other_op success\r\n    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # [[0]]\r\n\r\n    # run pooling on datas success\r\n    print sess.run(pool_op, {x: normal_data}).shape  # (1, 4, 4, 1)\r\n    print sess.run(pool_op, {x: empty_data}).shape  # (0, 4, 4, 1)\r\n\r\n    # run other_op now failed\r\n    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # err\r\n``` \r\n\r\nAbove code report error:\r\ntensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices.  temp_storage_bytes: 1, status: invalid configuration argument\r\n\r\n\"invalid configuration argument\" seems to be message return by cudaGetError, which indicates a failed kernel launch due to zero or too large number of block threads.\r\n\r\n### Source code / logs\r\n![image](https://user-images.githubusercontent.com/7600935/43579974-6dd4967a-9686-11e8-9b22-8288159d155c.png)\r\n\r\n"}