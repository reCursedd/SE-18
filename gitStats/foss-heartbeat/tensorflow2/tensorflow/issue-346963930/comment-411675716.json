{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/411675716", "html_url": "https://github.com/tensorflow/tensorflow/issues/21338#issuecomment-411675716", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21338", "id": 411675716, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMTY3NTcxNg==", "user": {"login": "wrongtest", "id": 7600935, "node_id": "MDQ6VXNlcjc2MDA5MzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/7600935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wrongtest", "html_url": "https://github.com/wrongtest", "followers_url": "https://api.github.com/users/wrongtest/followers", "following_url": "https://api.github.com/users/wrongtest/following{/other_user}", "gists_url": "https://api.github.com/users/wrongtest/gists{/gist_id}", "starred_url": "https://api.github.com/users/wrongtest/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wrongtest/subscriptions", "organizations_url": "https://api.github.com/users/wrongtest/orgs", "repos_url": "https://api.github.com/users/wrongtest/repos", "events_url": "https://api.github.com/users/wrongtest/events{/privacy}", "received_events_url": "https://api.github.com/users/wrongtest/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-09T08:14:31Z", "updated_at": "2018-08-09T08:14:31Z", "author_association": "NONE", "body_html": "<p>At least in synchronous execution on gpu devices, whether the computation is buggy or not seems to be checked by OpContext's status, which requires OP developers to follow some error handling conventions, or else errors may \"leak\".</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L506-L508\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L506-L508</a></p>\n<p>I test to do some additional check here and recompile, do get nonzero cuda error code (9). Wonder why such kind of post op check not done currently.</p>", "body_text": "At least in synchronous execution on gpu devices, whether the computation is buggy or not seems to be checked by OpContext's status, which requires OP developers to follow some error handling conventions, or else errors may \"leak\".\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L506-L508\nI test to do some additional check here and recompile, do get nonzero cuda error code (9). Wonder why such kind of post op check not done currently.", "body": "At least in synchronous execution on gpu devices, whether the computation is buggy or not seems to be checked by OpContext's status, which requires OP developers to follow some error handling conventions, or else errors may \"leak\".\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L506-L508\r\n\r\nI test to do some additional check here and recompile, do get nonzero cuda error code (9). Wonder why such kind of post op check not done currently.\r\n\r\n\r\n  "}