{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14347", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14347/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14347/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14347/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14347", "id": 272081866, "node_id": "MDU6SXNzdWUyNzIwODE4NjY=", "number": 14347, "title": "Is my code right to use batch normalization layers in tensorflow?", "user": {"login": "yananchen1989", "id": 26405281, "node_id": "MDQ6VXNlcjI2NDA1Mjgx", "avatar_url": "https://avatars2.githubusercontent.com/u/26405281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yananchen1989", "html_url": "https://github.com/yananchen1989", "followers_url": "https://api.github.com/users/yananchen1989/followers", "following_url": "https://api.github.com/users/yananchen1989/following{/other_user}", "gists_url": "https://api.github.com/users/yananchen1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/yananchen1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yananchen1989/subscriptions", "organizations_url": "https://api.github.com/users/yananchen1989/orgs", "repos_url": "https://api.github.com/users/yananchen1989/repos", "events_url": "https://api.github.com/users/yananchen1989/events{/privacy}", "received_events_url": "https://api.github.com/users/yananchen1989/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-08T05:51:03Z", "updated_at": "2017-11-08T18:42:28Z", "closed_at": "2017-11-08T18:42:28Z", "author_association": "NONE", "body_html": "<p>I have two inputs: <code>qi_pos &amp; qi_neg</code> with the same shape. They should be processed by the two mlp layers, and finally get two results which acts as score. Here is my codes:</p>\n<pre><code>  self.mlp1_pos  =    nn_layers.full_connect_(qi_pos,        256, activation='relu', use_bn = None, keep_prob=self.keep_prob,  name = 'deep_mlp_1')\n  self.mlp2_pos  =    nn_layers.full_connect_(self.mlp1_pos, 128,  activation='relu', use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_2')\n  self.pos_pair_sim = nn_layers.full_connect_(self.mlp2_pos,  1,  activation=None, use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_3')\n  tf.get_variable_scope().reuse_variables()\n  self.mlp1_neg  =    nn_layers.full_connect_(qi_neg,        256, activation='relu', use_bn = None, keep_prob=self.keep_prob,  name = 'deep_mlp_1')\n  self.mlp2_neg  =    nn_layers.full_connect_(self.mlp1_neg, 128,  activation='relu', use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_2')\n  self.neg_pair_sim = nn_layers.full_connect_(self.mlp2_neg,  1,  activation=None, use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_3')\n</code></pre>\n<p>I use BN layer to normalize the nodes in hidden layers:</p>\n<pre><code>def full_connect_(inputs, num_units, activation=None, use_bn = None, keep_prob = 1.0, name='full_connect_'):\n  with tf.variable_scope(name):\n    shape = [inputs.get_shape()[-1], num_units]\n    weight = weight_variable(shape)\n    bias = bias_variable(shape[-1])\n    outputs_ = tf.matmul(inputs, weight) + bias\n    if use_bn:\n        outputs_ = tf.contrib.layers.batch_norm(outputs_, center=True, scale=True, is_training=True,decay=0.9,epsilon=1e-5, scope='bn')\n    if activation==\"relu\":\n      outputs = tf.nn.relu(outputs_)\n    elif activation == \"tanh\":\n      outputs = tf.tanh(outputs_)\n    elif activation == \"sigmoid\":\n      outputs = tf.nn.sigmoid(outputs_)\n    else:\n      outputs = outputs_\n    return  outputs\n\n   with tf.name_scope('predictions'):\n      self.sim_diff = self.pos_pair_sim - self.neg_pair_sim # shape = (batch_size, 1)\n      self.preds = tf.sigmoid(self.sim_diff) # shape = (batch_size, 1)\n      self.infers = self.pos_pair_sim\n</code></pre>\n<p>Below is the loss definition.It seems all right.</p>\n<p>with tf.name_scope('predictions'):<br>\nsim_diff = pos_pair_sim - neg_pair_sim<br>\npredictions = tf.sigmoid(sim_diff)<br>\nself.infers = pos_pair_sim</p>\n<h2>loss and optim</h2>\n<p>with tf.name_scope('loss'):<br>\nself.loss = nn_layers.cross_entropy_loss_with_reg(self.labels, self.preds)<br>\ntf.summary.scalar('loss', self.loss)</p>\n<p>I am not sure whether I have use the BN layers in right way. I mean that the BN parameters are derived from the hidden units from the two separate parts, which are based on qi_pos and qi_neg tensors as inputs. Anyway, anyone could help check it ?</p>", "body_text": "I have two inputs: qi_pos & qi_neg with the same shape. They should be processed by the two mlp layers, and finally get two results which acts as score. Here is my codes:\n  self.mlp1_pos  =    nn_layers.full_connect_(qi_pos,        256, activation='relu', use_bn = None, keep_prob=self.keep_prob,  name = 'deep_mlp_1')\n  self.mlp2_pos  =    nn_layers.full_connect_(self.mlp1_pos, 128,  activation='relu', use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_2')\n  self.pos_pair_sim = nn_layers.full_connect_(self.mlp2_pos,  1,  activation=None, use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_3')\n  tf.get_variable_scope().reuse_variables()\n  self.mlp1_neg  =    nn_layers.full_connect_(qi_neg,        256, activation='relu', use_bn = None, keep_prob=self.keep_prob,  name = 'deep_mlp_1')\n  self.mlp2_neg  =    nn_layers.full_connect_(self.mlp1_neg, 128,  activation='relu', use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_2')\n  self.neg_pair_sim = nn_layers.full_connect_(self.mlp2_neg,  1,  activation=None, use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_3')\n\nI use BN layer to normalize the nodes in hidden layers:\ndef full_connect_(inputs, num_units, activation=None, use_bn = None, keep_prob = 1.0, name='full_connect_'):\n  with tf.variable_scope(name):\n    shape = [inputs.get_shape()[-1], num_units]\n    weight = weight_variable(shape)\n    bias = bias_variable(shape[-1])\n    outputs_ = tf.matmul(inputs, weight) + bias\n    if use_bn:\n        outputs_ = tf.contrib.layers.batch_norm(outputs_, center=True, scale=True, is_training=True,decay=0.9,epsilon=1e-5, scope='bn')\n    if activation==\"relu\":\n      outputs = tf.nn.relu(outputs_)\n    elif activation == \"tanh\":\n      outputs = tf.tanh(outputs_)\n    elif activation == \"sigmoid\":\n      outputs = tf.nn.sigmoid(outputs_)\n    else:\n      outputs = outputs_\n    return  outputs\n\n   with tf.name_scope('predictions'):\n      self.sim_diff = self.pos_pair_sim - self.neg_pair_sim # shape = (batch_size, 1)\n      self.preds = tf.sigmoid(self.sim_diff) # shape = (batch_size, 1)\n      self.infers = self.pos_pair_sim\n\nBelow is the loss definition.It seems all right.\nwith tf.name_scope('predictions'):\nsim_diff = pos_pair_sim - neg_pair_sim\npredictions = tf.sigmoid(sim_diff)\nself.infers = pos_pair_sim\nloss and optim\nwith tf.name_scope('loss'):\nself.loss = nn_layers.cross_entropy_loss_with_reg(self.labels, self.preds)\ntf.summary.scalar('loss', self.loss)\nI am not sure whether I have use the BN layers in right way. I mean that the BN parameters are derived from the hidden units from the two separate parts, which are based on qi_pos and qi_neg tensors as inputs. Anyway, anyone could help check it ?", "body": "I have two inputs: `qi_pos & qi_neg` with the same shape. They should be processed by the two mlp layers, and finally get two results which acts as score. Here is my codes:\r\n```\r\n  self.mlp1_pos  =    nn_layers.full_connect_(qi_pos,        256, activation='relu', use_bn = None, keep_prob=self.keep_prob,  name = 'deep_mlp_1')\r\n  self.mlp2_pos  =    nn_layers.full_connect_(self.mlp1_pos, 128,  activation='relu', use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_2')\r\n  self.pos_pair_sim = nn_layers.full_connect_(self.mlp2_pos,  1,  activation=None, use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_3')\r\n  tf.get_variable_scope().reuse_variables()\r\n  self.mlp1_neg  =    nn_layers.full_connect_(qi_neg,        256, activation='relu', use_bn = None, keep_prob=self.keep_prob,  name = 'deep_mlp_1')\r\n  self.mlp2_neg  =    nn_layers.full_connect_(self.mlp1_neg, 128,  activation='relu', use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_2')\r\n  self.neg_pair_sim = nn_layers.full_connect_(self.mlp2_neg,  1,  activation=None, use_bn = True, keep_prob=self.keep_prob,  name = 'deep_mlp_3')\r\n```\r\n\r\nI use BN layer to normalize the nodes in hidden layers:\r\n\r\n```\r\ndef full_connect_(inputs, num_units, activation=None, use_bn = None, keep_prob = 1.0, name='full_connect_'):\r\n  with tf.variable_scope(name):\r\n    shape = [inputs.get_shape()[-1], num_units]\r\n    weight = weight_variable(shape)\r\n    bias = bias_variable(shape[-1])\r\n    outputs_ = tf.matmul(inputs, weight) + bias\r\n    if use_bn:\r\n        outputs_ = tf.contrib.layers.batch_norm(outputs_, center=True, scale=True, is_training=True,decay=0.9,epsilon=1e-5, scope='bn')\r\n    if activation==\"relu\":\r\n      outputs = tf.nn.relu(outputs_)\r\n    elif activation == \"tanh\":\r\n      outputs = tf.tanh(outputs_)\r\n    elif activation == \"sigmoid\":\r\n      outputs = tf.nn.sigmoid(outputs_)\r\n    else:\r\n      outputs = outputs_\r\n    return  outputs\r\n\r\n   with tf.name_scope('predictions'):\r\n      self.sim_diff = self.pos_pair_sim - self.neg_pair_sim # shape = (batch_size, 1)\r\n      self.preds = tf.sigmoid(self.sim_diff) # shape = (batch_size, 1)\r\n      self.infers = self.pos_pair_sim\r\n```\r\n\r\nBelow is the loss definition.It seems all right.\r\n\r\nwith tf.name_scope('predictions'):\r\n  sim_diff = pos_pair_sim - neg_pair_sim\r\n  predictions = tf.sigmoid(sim_diff)\r\n  self.infers = pos_pair_sim\r\n## loss and optim\r\nwith tf.name_scope('loss'):\r\n  self.loss = nn_layers.cross_entropy_loss_with_reg(self.labels, self.preds)\r\n  tf.summary.scalar('loss', self.loss)\r\n\r\nI am not sure whether I have use the BN layers in right way. I mean that the BN parameters are derived from the hidden units from the two separate parts, which are based on qi_pos and qi_neg tensors as inputs. Anyway, anyone could help check it ?"}