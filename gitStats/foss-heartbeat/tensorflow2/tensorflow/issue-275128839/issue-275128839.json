{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14698", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14698/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14698/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14698/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14698", "id": 275128839, "node_id": "MDU6SXNzdWUyNzUxMjg4Mzk=", "number": 14698, "title": "MemoryError from tensorflow.contrib.learning.datasets in Python3", "user": {"login": "quantitative-technologies", "id": 29150871, "node_id": "MDQ6VXNlcjI5MTUwODcx", "avatar_url": "https://avatars0.githubusercontent.com/u/29150871?v=4", "gravatar_id": "", "url": "https://api.github.com/users/quantitative-technologies", "html_url": "https://github.com/quantitative-technologies", "followers_url": "https://api.github.com/users/quantitative-technologies/followers", "following_url": "https://api.github.com/users/quantitative-technologies/following{/other_user}", "gists_url": "https://api.github.com/users/quantitative-technologies/gists{/gist_id}", "starred_url": "https://api.github.com/users/quantitative-technologies/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/quantitative-technologies/subscriptions", "organizations_url": "https://api.github.com/users/quantitative-technologies/orgs", "repos_url": "https://api.github.com/users/quantitative-technologies/repos", "events_url": "https://api.github.com/users/quantitative-technologies/events{/privacy}", "received_events_url": "https://api.github.com/users/quantitative-technologies/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 17, "created_at": "2017-11-19T03:24:39Z", "updated_at": "2018-07-27T17:02:06Z", "closed_at": "2017-12-06T02:09:33Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 17.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.4.0-rc1-11-g130a514 1.4.0</li>\n<li><strong>Python version</strong>: 3.5.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>: python -c \"import tensorflow as tf; tf.contrib.learn.datasets.load_dataset('dbpedia', size='full')\"</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The command results in an MemoryError, even though the dataset easily fits into my memory (64GB), and also works with Python 2.</p>\n<h3>Source code / logs</h3>\n<p>Here is the traceback:</p>\n<p>Traceback (most recent call last):<br>\nFile \"\", line 1, in <br>\nFile \"/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/<strong>init</strong>.py\", line 71, in load_dataset<br>\nreturn DATASETS[name](size, test_with_fake_data)<br>\nFile \"/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py\", line 65, in load_dbpedia<br>\ntrain_path, target_dtype=np.int32, features_dtype=np.str, target_column=0)<br>\nFile \"/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 72, in load_csv_without_header<br>\ndata = np.array(data)<br>\nMemoryError</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.4.0-rc1-11-g130a514 1.4.0\nPython version: 3.5.3\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce: python -c \"import tensorflow as tf; tf.contrib.learn.datasets.load_dataset('dbpedia', size='full')\"\n\nDescribe the problem\nThe command results in an MemoryError, even though the dataset easily fits into my memory (64GB), and also works with Python 2.\nSource code / logs\nHere is the traceback:\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/init.py\", line 71, in load_dataset\nreturn DATASETS[name](size, test_with_fake_data)\nFile \"/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py\", line 65, in load_dbpedia\ntrain_path, target_dtype=np.int32, features_dtype=np.str, target_column=0)\nFile \"/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 72, in load_csv_without_header\ndata = np.array(data)\nMemoryError", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: python -c \"import tensorflow as tf; tf.contrib.learn.datasets.load_dataset('dbpedia', size='full')\"\r\n\r\n\r\n\r\n### Describe the problem\r\nThe command results in an MemoryError, even though the dataset easily fits into my memory (64GB), and also works with Python 2. \r\n\r\n### Source code / logs\r\nHere is the traceback:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py\", line 71, in load_dataset\r\n    return DATASETS[name](size, test_with_fake_data)\r\n  File \"/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py\", line 65, in load_dbpedia\r\n    train_path, target_dtype=np.int32, features_dtype=np.str, target_column=0)\r\n  File \"/home/james/python3.5-ve/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 72, in load_csv_without_header\r\n    data = np.array(data)\r\nMemoryError\r\n\r\n"}