{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/426579667", "html_url": "https://github.com/tensorflow/tensorflow/issues/22630#issuecomment-426579667", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22630", "id": 426579667, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjU3OTY2Nw==", "user": {"login": "Elites2017", "id": 35799396, "node_id": "MDQ6VXNlcjM1Nzk5Mzk2", "avatar_url": "https://avatars2.githubusercontent.com/u/35799396?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Elites2017", "html_url": "https://github.com/Elites2017", "followers_url": "https://api.github.com/users/Elites2017/followers", "following_url": "https://api.github.com/users/Elites2017/following{/other_user}", "gists_url": "https://api.github.com/users/Elites2017/gists{/gist_id}", "starred_url": "https://api.github.com/users/Elites2017/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Elites2017/subscriptions", "organizations_url": "https://api.github.com/users/Elites2017/orgs", "repos_url": "https://api.github.com/users/Elites2017/repos", "events_url": "https://api.github.com/users/Elites2017/events{/privacy}", "received_events_url": "https://api.github.com/users/Elites2017/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-03T10:03:48Z", "updated_at": "2018-10-03T10:06:07Z", "author_association": "NONE", "body_html": "<p>I have used that but the app runs and crashes each time I launch it. Runtime Error, I don't know where that error comes from? Before setting my model on the app, at the step to convert my .pb file (graph from the ssd_mobilenet v1_1.0_224) to .tflite file. The model was converted with this warning:</p>\n<p>**the (tflite_graph.pb) file is converted to .tflite with the following warning.<br>\nIgnoring unsupported attribute type with key '_output_types'  should I have worried about it ?</p>\n<p>Or do you think this warning can cause the runtime error in the app?</p>\n<p>NB: But the model from tensorflow works in the app, but my own model doesn't**</p>\n<p>Exact comand used to convert my .pb file to .tflite file:<br>\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/david/pro/tflite_graph.pb --output_file=/home/david/pro/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess --output_arrays=TFLite_Detection_PostProcess:1 --output_arrays=TFLite_Detection_PostProcess:2 --output_arrays=TFLite_Detection_PostProcess:3 --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6</p>", "body_text": "I have used that but the app runs and crashes each time I launch it. Runtime Error, I don't know where that error comes from? Before setting my model on the app, at the step to convert my .pb file (graph from the ssd_mobilenet v1_1.0_224) to .tflite file. The model was converted with this warning:\n**the (tflite_graph.pb) file is converted to .tflite with the following warning.\nIgnoring unsupported attribute type with key '_output_types'  should I have worried about it ?\nOr do you think this warning can cause the runtime error in the app?\nNB: But the model from tensorflow works in the app, but my own model doesn't**\nExact comand used to convert my .pb file to .tflite file:\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/david/pro/tflite_graph.pb --output_file=/home/david/pro/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess --output_arrays=TFLite_Detection_PostProcess:1 --output_arrays=TFLite_Detection_PostProcess:2 --output_arrays=TFLite_Detection_PostProcess:3 --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6", "body": "I have used that but the app runs and crashes each time I launch it. Runtime Error, I don't know where that error comes from? Before setting my model on the app, at the step to convert my .pb file (graph from the ssd_mobilenet v1_1.0_224) to .tflite file. The model was converted with this warning:\r\n\r\n**the (tflite_graph.pb) file is converted to .tflite with the following warning.\r\nIgnoring unsupported attribute type with key '_output_types'  should I have worried about it ?\r\n\r\nOr do you think this warning can cause the runtime error in the app? \r\n\r\nNB: But the model from tensorflow works in the app, but my own model doesn't**\r\n\r\nExact comand used to convert my .pb file to .tflite file:\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/david/pro/tflite_graph.pb --output_file=/home/david/pro/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess --output_arrays=TFLite_Detection_PostProcess:1 --output_arrays=TFLite_Detection_PostProcess:2 --output_arrays=TFLite_Detection_PostProcess:3 --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6"}