{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/295502315", "html_url": "https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-295502315", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7187", "id": 295502315, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTUwMjMxNQ==", "user": {"login": "taion", "id": 3112159, "node_id": "MDQ6VXNlcjMxMTIxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/3112159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/taion", "html_url": "https://github.com/taion", "followers_url": "https://api.github.com/users/taion/followers", "following_url": "https://api.github.com/users/taion/following{/other_user}", "gists_url": "https://api.github.com/users/taion/gists{/gist_id}", "starred_url": "https://api.github.com/users/taion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/taion/subscriptions", "organizations_url": "https://api.github.com/users/taion/orgs", "repos_url": "https://api.github.com/users/taion/repos", "events_url": "https://api.github.com/users/taion/events{/privacy}", "received_events_url": "https://api.github.com/users/taion/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-20T00:03:56Z", "updated_at": "2017-04-20T02:30:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I was playing around earlier today with my implementation of Wide ResNets in TF (using NCHW and fused batch norm, with <code>feed_dict</code> but verifying that <code>nvidia-smi</code> showed near 100% GPU usage throughout) and comparing to a PyTorch implementation (<a href=\"https://github.com/xternalz/WideResNet-pytorch\">https://github.com/xternalz/WideResNet-pytorch</a>).</p>\n<p>I saw batches taking something like 55% as long in the PyTorch version as in my TF version \u2013 ~155 ms for the PyTorch impl v ~285 ms in my TF impl on a p2.xlarge on AWS, using WRN-16-4. I'll post my code.</p>", "body_text": "I was playing around earlier today with my implementation of Wide ResNets in TF (using NCHW and fused batch norm, with feed_dict but verifying that nvidia-smi showed near 100% GPU usage throughout) and comparing to a PyTorch implementation (https://github.com/xternalz/WideResNet-pytorch).\nI saw batches taking something like 55% as long in the PyTorch version as in my TF version \u2013 ~155 ms for the PyTorch impl v ~285 ms in my TF impl on a p2.xlarge on AWS, using WRN-16-4. I'll post my code.", "body": "I was playing around earlier today with my implementation of Wide ResNets in TF (using NCHW and fused batch norm, with `feed_dict` but verifying that `nvidia-smi` showed near 100% GPU usage throughout) and comparing to a PyTorch implementation (https://github.com/xternalz/WideResNet-pytorch).\r\n\r\nI saw batches taking something like 55% as long in the PyTorch version as in my TF version \u2013 ~155 ms for the PyTorch impl v ~285 ms in my TF impl on a p2.xlarge on AWS, using WRN-16-4. I'll post my code."}