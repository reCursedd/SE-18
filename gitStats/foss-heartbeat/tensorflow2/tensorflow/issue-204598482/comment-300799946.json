{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300799946", "html_url": "https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-300799946", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7187", "id": 300799946, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDc5OTk0Ng==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-11T14:06:07Z", "updated_at": "2017-05-11T14:06:19Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3028543\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Randl\">@Randl</a><br>\none line change to move input processing to CPU.  This would also impact the resnet test if I was reading it correctly.  I have no idea if the AlexNet implementations are the same between the platforms or if anything else is different.  My point is make sure to put your input processing on CPU it makes a difference.  Using your number I reduced the time by 83%.  On my testing I was getting 0.144 per step (I love the GTX 1080 but there are a lot of variations and the last dlbench was on TF .11 which was a long time ago), which is a reduction of 63%.  Put another way I went from 7K images/sec to 19K images per second in my apples-to-apples test (my numbers before and after) for a speedup of 2.7x.</p>\n<p>It is possible your numbers are from a different batch size than the default 1024.</p>\n<div class=\"highlight highlight-source-python\"><pre>      <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>):\n        images, labels <span class=\"pl-k\">=</span> cifar10_input.inputs(<span class=\"pl-c1\">False</span>, <span class=\"pl-c1\">FLAGS</span>.data_dir, <span class=\"pl-c1\">FLAGS</span>.batch_size)</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>python alexnet_cifar10.py --data_dir=/home/toby/CIFAR-10/cifar-10-batches-bin\n2017-05-11 06:51:40.045511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node <span class=\"pl-c1\">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-05-11 06:51:40.045815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\npciBusID 0000:01:00.0\nTotal memory: 7.91GiB\nFree memory: 7.34GiB\n2017-05-11 06:51:40.045828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 \n2017-05-11 06:51:40.045832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y \n2017-05-11 06:51:40.045843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -<span class=\"pl-k\">&gt;</span> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nmin_queue_examples:  20000\n(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Images: <span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">&lt;</span>tf.Tensor <span class=\"pl-s\"><span class=\"pl-pds\">'</span>shuffle_batch:0<span class=\"pl-pds\">'</span></span> shape=(1024, 32, 32, 3) dtype=float<span class=\"pl-k\">32&gt;</span>)\n(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>padded_input: <span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">&lt;</span>tf.Tensor <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Pad_1:0<span class=\"pl-pds\">'</span></span> shape=(1024, 36, 36, 3) dtype=float<span class=\"pl-k\">32&gt;</span>)\n(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>padded_input: <span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">&lt;</span>tf.Tensor <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Pad_2:0<span class=\"pl-pds\">'</span></span> shape=(1024, 19, 19, 32) dtype=float<span class=\"pl-k\">32&gt;</span>)\n(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>padded_input: <span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">&lt;</span>tf.Tensor <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Pad_3:0<span class=\"pl-pds\">'</span></span> shape=(1024, 11, 11, 32) dtype=float<span class=\"pl-k\">32&gt;</span>)\n(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool3: <span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">&lt;</span>tf.Tensor <span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool3:0<span class=\"pl-pds\">'</span></span> shape=(1024, 3, 3, 64) dtype=float<span class=\"pl-k\">32&gt;</span>)\n2017-05-11 06:51:41.810110: step 0, loss = 2.30 (736.7 examples/sec<span class=\"pl-k\">;</span> 1.390 sec/batch)\nepoch: 1, loss: 2.35\n2017-05-11 06:51:44.403091: step 50, loss = 2.30 (19262.2 examples/sec<span class=\"pl-k\">;</span> 0.053 sec/batch)\nepoch: 2, loss: 2.29\n2017-05-11 06:51:47.085227: step 100, loss = 2.28 (19319.7 examples/sec<span class=\"pl-k\">;</span> 0.053 sec/batch)\nepoch: 3, loss: 2.27\n2017-05-11 06:51:49.763903: step 150, loss = 2.25 (19135.9 examples/sec<span class=\"pl-k\">;</span> 0.054 sec/batch)\nepoch: 4, loss: 2.27\n2017-05-11 06:51:52.439839: step 200, loss = 2.30 (19603.0 examples/sec<span class=\"pl-k\">;</span> 0.052 sec/batch)\nepoch: 5, loss: 2.30\n2017-05-11 06:51:55.103668: step 250, loss = 2.30 (19260.7 examples/sec<span class=\"pl-k\">;</span> 0.053 sec/batch)\n</pre></div>", "body_text": "@Randl\none line change to move input processing to CPU.  This would also impact the resnet test if I was reading it correctly.  I have no idea if the AlexNet implementations are the same between the platforms or if anything else is different.  My point is make sure to put your input processing on CPU it makes a difference.  Using your number I reduced the time by 83%.  On my testing I was getting 0.144 per step (I love the GTX 1080 but there are a lot of variations and the last dlbench was on TF .11 which was a long time ago), which is a reduction of 63%.  Put another way I went from 7K images/sec to 19K images per second in my apples-to-apples test (my numbers before and after) for a speedup of 2.7x.\nIt is possible your numbers are from a different batch size than the default 1024.\n      with tf.device('/cpu:0'):\n        images, labels = cifar10_input.inputs(False, FLAGS.data_dir, FLAGS.batch_size)\npython alexnet_cifar10.py --data_dir=/home/toby/CIFAR-10/cifar-10-batches-bin\n2017-05-11 06:51:40.045511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-05-11 06:51:40.045815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\npciBusID 0000:01:00.0\nTotal memory: 7.91GiB\nFree memory: 7.34GiB\n2017-05-11 06:51:40.045828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 \n2017-05-11 06:51:40.045832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y \n2017-05-11 06:51:40.045843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nmin_queue_examples:  20000\n('Images: ', <tf.Tensor 'shuffle_batch:0' shape=(1024, 32, 32, 3) dtype=float32>)\n('padded_input: ', <tf.Tensor 'Pad_1:0' shape=(1024, 36, 36, 3) dtype=float32>)\n('padded_input: ', <tf.Tensor 'Pad_2:0' shape=(1024, 19, 19, 32) dtype=float32>)\n('padded_input: ', <tf.Tensor 'Pad_3:0' shape=(1024, 11, 11, 32) dtype=float32>)\n('pool3: ', <tf.Tensor 'pool3:0' shape=(1024, 3, 3, 64) dtype=float32>)\n2017-05-11 06:51:41.810110: step 0, loss = 2.30 (736.7 examples/sec; 1.390 sec/batch)\nepoch: 1, loss: 2.35\n2017-05-11 06:51:44.403091: step 50, loss = 2.30 (19262.2 examples/sec; 0.053 sec/batch)\nepoch: 2, loss: 2.29\n2017-05-11 06:51:47.085227: step 100, loss = 2.28 (19319.7 examples/sec; 0.053 sec/batch)\nepoch: 3, loss: 2.27\n2017-05-11 06:51:49.763903: step 150, loss = 2.25 (19135.9 examples/sec; 0.054 sec/batch)\nepoch: 4, loss: 2.27\n2017-05-11 06:51:52.439839: step 200, loss = 2.30 (19603.0 examples/sec; 0.052 sec/batch)\nepoch: 5, loss: 2.30\n2017-05-11 06:51:55.103668: step 250, loss = 2.30 (19260.7 examples/sec; 0.053 sec/batch)", "body": "@Randl \r\none line change to move input processing to CPU.  This would also impact the resnet test if I was reading it correctly.  I have no idea if the AlexNet implementations are the same between the platforms or if anything else is different.  My point is make sure to put your input processing on CPU it makes a difference.  Using your number I reduced the time by 83%.  On my testing I was getting 0.144 per step (I love the GTX 1080 but there are a lot of variations and the last dlbench was on TF .11 which was a long time ago), which is a reduction of 63%.  Put another way I went from 7K images/sec to 19K images per second in my apples-to-apples test (my numbers before and after) for a speedup of 2.7x.    \r\n\r\nIt is possible your numbers are from a different batch size than the default 1024.\r\n\r\n```python\r\n      with tf.device('/cpu:0'):\r\n        images, labels = cifar10_input.inputs(False, FLAGS.data_dir, FLAGS.batch_size)\r\n```\r\n\r\n```bash\r\npython alexnet_cifar10.py --data_dir=/home/toby/CIFAR-10/cifar-10-batches-bin\r\n2017-05-11 06:51:40.045511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-11 06:51:40.045815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.91GiB\r\nFree memory: 7.34GiB\r\n2017-05-11 06:51:40.045828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 \r\n2017-05-11 06:51:40.045832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y \r\n2017-05-11 06:51:40.045843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\nmin_queue_examples:  20000\r\n('Images: ', <tf.Tensor 'shuffle_batch:0' shape=(1024, 32, 32, 3) dtype=float32>)\r\n('padded_input: ', <tf.Tensor 'Pad_1:0' shape=(1024, 36, 36, 3) dtype=float32>)\r\n('padded_input: ', <tf.Tensor 'Pad_2:0' shape=(1024, 19, 19, 32) dtype=float32>)\r\n('padded_input: ', <tf.Tensor 'Pad_3:0' shape=(1024, 11, 11, 32) dtype=float32>)\r\n('pool3: ', <tf.Tensor 'pool3:0' shape=(1024, 3, 3, 64) dtype=float32>)\r\n2017-05-11 06:51:41.810110: step 0, loss = 2.30 (736.7 examples/sec; 1.390 sec/batch)\r\nepoch: 1, loss: 2.35\r\n2017-05-11 06:51:44.403091: step 50, loss = 2.30 (19262.2 examples/sec; 0.053 sec/batch)\r\nepoch: 2, loss: 2.29\r\n2017-05-11 06:51:47.085227: step 100, loss = 2.28 (19319.7 examples/sec; 0.053 sec/batch)\r\nepoch: 3, loss: 2.27\r\n2017-05-11 06:51:49.763903: step 150, loss = 2.25 (19135.9 examples/sec; 0.054 sec/batch)\r\nepoch: 4, loss: 2.27\r\n2017-05-11 06:51:52.439839: step 200, loss = 2.30 (19603.0 examples/sec; 0.052 sec/batch)\r\nepoch: 5, loss: 2.30\r\n2017-05-11 06:51:55.103668: step 250, loss = 2.30 (19260.7 examples/sec; 0.053 sec/batch)\r\n\r\n```"}