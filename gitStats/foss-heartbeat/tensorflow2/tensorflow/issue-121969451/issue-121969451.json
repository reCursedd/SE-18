{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/505", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/505/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/505/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/505/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/505", "id": 121969451, "node_id": "MDU6SXNzdWUxMjE5Njk0NTE=", "number": 505, "title": "Momentum and Adagrad don't work with reshape and embeddings", "user": {"login": "tomasmcz", "id": 4395563, "node_id": "MDQ6VXNlcjQzOTU1NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4395563?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomasmcz", "html_url": "https://github.com/tomasmcz", "followers_url": "https://api.github.com/users/tomasmcz/followers", "following_url": "https://api.github.com/users/tomasmcz/following{/other_user}", "gists_url": "https://api.github.com/users/tomasmcz/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomasmcz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomasmcz/subscriptions", "organizations_url": "https://api.github.com/users/tomasmcz/orgs", "repos_url": "https://api.github.com/users/tomasmcz/repos", "events_url": "https://api.github.com/users/tomasmcz/events{/privacy}", "received_events_url": "https://api.github.com/users/tomasmcz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2015-12-14T03:46:15Z", "updated_at": "2017-02-09T22:02:25Z", "closed_at": "2015-12-17T02:28:45Z", "author_association": "NONE", "body_html": "<p>Momentum and Adagrad optimizers do not work when I use <code>tf.reshape</code> like this:</p>\n<div class=\"highlight highlight-source-python\"><pre>embeddings <span class=\"pl-k\">=</span> tf.Variable(                                                                                                  \n    tf.random_uniform([<span class=\"pl-c1\">50000</span>, <span class=\"pl-c1\">50</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>))\n\nembed <span class=\"pl-k\">=</span> tf.reshape(                                                                                                        \n        tf.nn.embedding_lookup(embeddings, input_op),                                                                          \n        [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, em_layer_size])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> a few dense layers and a softmax would follow here</span></pre></div>\n<p>The code above works with SGD and AdamOptimizer. With Momentum or Adagrad it produces this error:</p>\n<pre><code>    optimizer = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 188, in minimize\n    name=name)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 289, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/momentum.py\", line 70, in _apply_sparse\n    self._momentum_tensor, use_locking=self._use_locking).op\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/gen_training_ops.py\", line 237, in sparse_apply_momentum\n    name=name)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1836, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1476, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/training_ops.py\", line 130, in _SparseApplyMomentumShape\n    tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 527, in merge_with\n    self.assert_same_rank(other)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 570, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes TensorShape([Dimension(None), Dimension(None), Dimension(None)]) and TensorShape([Dimension(None), Dimension(50)]) must have the same rank\n</code></pre>\n<p>This might be connected to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"121453300\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/464\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/464/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/464\">#464</a>, the error is similar and also appears with Momentum and Adagrad.</p>\n<p>When I replace the <code>tf.reshape</code> with following code, the error disappears:</p>\n<div class=\"highlight highlight-source-python\"><pre>_emb <span class=\"pl-k\">=</span> []                                                                                                                 \n<span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> tf.split(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>, input_op):                                                                          \n    _emb.append(tf.nn.embedding_lookup(embeddings, tf.squeeze(x)))                                                        \nembed <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">1</span>, _emb) </pre></div>", "body_text": "Momentum and Adagrad optimizers do not work when I use tf.reshape like this:\nembeddings = tf.Variable(                                                                                                  \n    tf.random_uniform([50000, 50], -1.0, 1.0))\n\nembed = tf.reshape(                                                                                                        \n        tf.nn.embedding_lookup(embeddings, input_op),                                                                          \n        [-1, em_layer_size])\n\n# a few dense layers and a softmax would follow here\nThe code above works with SGD and AdamOptimizer. With Momentum or Adagrad it produces this error:\n    optimizer = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 188, in minimize\n    name=name)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 289, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/momentum.py\", line 70, in _apply_sparse\n    self._momentum_tensor, use_locking=self._use_locking).op\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/gen_training_ops.py\", line 237, in sparse_apply_momentum\n    name=name)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1836, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1476, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/training_ops.py\", line 130, in _SparseApplyMomentumShape\n    tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 527, in merge_with\n    self.assert_same_rank(other)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 570, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes TensorShape([Dimension(None), Dimension(None), Dimension(None)]) and TensorShape([Dimension(None), Dimension(50)]) must have the same rank\n\nThis might be connected to #464, the error is similar and also appears with Momentum and Adagrad.\nWhen I replace the tf.reshape with following code, the error disappears:\n_emb = []                                                                                                                 \nfor x in tf.split(1, 4, input_op):                                                                          \n    _emb.append(tf.nn.embedding_lookup(embeddings, tf.squeeze(x)))                                                        \nembed = tf.concat(1, _emb)", "body": "Momentum and Adagrad optimizers do not work when I use `tf.reshape` like this:\n\n``` python\nembeddings = tf.Variable(                                                                                                  \n    tf.random_uniform([50000, 50], -1.0, 1.0))\n\nembed = tf.reshape(                                                                                                        \n        tf.nn.embedding_lookup(embeddings, input_op),                                                                          \n        [-1, em_layer_size])\n\n# a few dense layers and a softmax would follow here\n```\n\nThe code above works with SGD and AdamOptimizer. With Momentum or Adagrad it produces this error:\n\n```\n    optimizer = tf.train.MomentumOptimizer(0.01, 0.9).minimize(loss)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 188, in minimize\n    name=name)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 289, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/momentum.py\", line 70, in _apply_sparse\n    self._momentum_tensor, use_locking=self._use_locking).op\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/gen_training_ops.py\", line 237, in sparse_apply_momentum\n    name=name)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1836, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1476, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/training/training_ops.py\", line 130, in _SparseApplyMomentumShape\n    tensor_shape.TensorShape([None]).concatenate(accum_shape[1:]))\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 527, in merge_with\n    self.assert_same_rank(other)\n  File \"/home/tom/tf-env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 570, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes TensorShape([Dimension(None), Dimension(None), Dimension(None)]) and TensorShape([Dimension(None), Dimension(50)]) must have the same rank\n```\n\nThis might be connected to #464, the error is similar and also appears with Momentum and Adagrad. \n\nWhen I replace the `tf.reshape` with following code, the error disappears:\n\n``` python\n_emb = []                                                                                                                 \nfor x in tf.split(1, 4, input_op):                                                                          \n    _emb.append(tf.nn.embedding_lookup(embeddings, tf.squeeze(x)))                                                        \nembed = tf.concat(1, _emb) \n```\n"}