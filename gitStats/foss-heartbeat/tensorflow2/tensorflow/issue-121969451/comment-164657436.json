{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/164657436", "html_url": "https://github.com/tensorflow/tensorflow/issues/505#issuecomment-164657436", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/505", "id": 164657436, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NDY1NzQzNg==", "user": {"login": "fabiencro", "id": 6006273, "node_id": "MDQ6VXNlcjYwMDYyNzM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6006273?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fabiencro", "html_url": "https://github.com/fabiencro", "followers_url": "https://api.github.com/users/fabiencro/followers", "following_url": "https://api.github.com/users/fabiencro/following{/other_user}", "gists_url": "https://api.github.com/users/fabiencro/gists{/gist_id}", "starred_url": "https://api.github.com/users/fabiencro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fabiencro/subscriptions", "organizations_url": "https://api.github.com/users/fabiencro/orgs", "repos_url": "https://api.github.com/users/fabiencro/repos", "events_url": "https://api.github.com/users/fabiencro/events{/privacy}", "received_events_url": "https://api.github.com/users/fabiencro/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-15T06:26:45Z", "updated_at": "2015-12-15T06:26:45Z", "author_association": "NONE", "body_html": "<p>Here is a small self-contained example that demonstrates the issue:</p>\n<pre><code>import collections\nimport numpy as np\nimport tensorflow as tf\nimport logging\nimport codecs\nimport json\nimport itertools\nimport time\n\ndef device_for_node(n):\n    if n.type == \"MatMul\":\n        return \"/gpu:1\"\n    else:\n        return \"/cpu:0\"\n\nminibatch_size = 128\nhidden_size = 64\nembedding_size = 256\ninput_layer_size = 3\nvocab_size_input = 32\nvocab_size_output = 64\nnce_num_sampled = 16\nlearning_rate = 0.1\nlearning_momentum = 0.9\n\ndummy_input = np.zeros((minibatch_size, input_layer_size), dtype = np.int32)\ndummy_target = np.zeros((minibatch_size, 1), dtype = np.int32)\n\ninput_layer_flattened_size = input_layer_size * embedding_size\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    with graph.device(device_for_node):\n        input_layer = tf.placeholder(tf.int32, shape = (minibatch_size, input_layer_size), name = \"input_layer\")       \n        ref_input = tf.placeholder(tf.int32, shape = (minibatch_size, 1), name = \"ref_input\")\n\n        # Parameters\n\n        input_embeddings = tf.Variable(tf.random_normal([vocab_size_input, embedding_size]), name = \"i_embeddings\")\n\n        Wh_i = tf.Variable(tf.random_normal((input_layer_flattened_size, hidden_size), stddev = 0.2), name = \"Wh_i\")\n        bh_i = tf.Variable(tf.random_normal((hidden_size,), stddev = 0.2), name = \"bh_i\")\n\n        Wh_o = tf.Variable(tf.random_normal((vocab_size_output, hidden_size), stddev = 0.2), name = \"Wh_o\")\n        bh_o = tf.Variable(tf.random_normal((vocab_size_output,), stddev = 0.2), name = \"bh_o\")\n\n        # Layers\n\n        i_embedded = tf.nn.embedding_lookup(input_embeddings, input_layer)\n        i_embedded_flattened = tf.reshape(i_embedded, \n                        (\n                         (minibatch_size if minibatch_size is not None else -1), \n                         input_layer_flattened_size ) \n                        )\n\n        h = tf.tanh(tf.matmul(i_embedded_flattened, Wh_i) + bh_i)\n        nce_loss = tf.reduce_mean(tf.nn.nce_loss(Wh_o, bh_o, h, ref_input, nce_num_sampled, \n                                                     num_classes = vocab_size_output, name = \"nce\"))\n        optimizer = tf.train.MomentumOptimizer(learning_rate, learning_momentum).minimize(nce_loss)\n\n        init_op = tf.initialize_all_variables()\n\n\nwith tf.Session(graph=graph) as session:\n\n    feed_dict = {input_layer : dummy_input, ref_input : dummy_target}\n    _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n</code></pre>", "body_text": "Here is a small self-contained example that demonstrates the issue:\nimport collections\nimport numpy as np\nimport tensorflow as tf\nimport logging\nimport codecs\nimport json\nimport itertools\nimport time\n\ndef device_for_node(n):\n    if n.type == \"MatMul\":\n        return \"/gpu:1\"\n    else:\n        return \"/cpu:0\"\n\nminibatch_size = 128\nhidden_size = 64\nembedding_size = 256\ninput_layer_size = 3\nvocab_size_input = 32\nvocab_size_output = 64\nnce_num_sampled = 16\nlearning_rate = 0.1\nlearning_momentum = 0.9\n\ndummy_input = np.zeros((minibatch_size, input_layer_size), dtype = np.int32)\ndummy_target = np.zeros((minibatch_size, 1), dtype = np.int32)\n\ninput_layer_flattened_size = input_layer_size * embedding_size\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    with graph.device(device_for_node):\n        input_layer = tf.placeholder(tf.int32, shape = (minibatch_size, input_layer_size), name = \"input_layer\")       \n        ref_input = tf.placeholder(tf.int32, shape = (minibatch_size, 1), name = \"ref_input\")\n\n        # Parameters\n\n        input_embeddings = tf.Variable(tf.random_normal([vocab_size_input, embedding_size]), name = \"i_embeddings\")\n\n        Wh_i = tf.Variable(tf.random_normal((input_layer_flattened_size, hidden_size), stddev = 0.2), name = \"Wh_i\")\n        bh_i = tf.Variable(tf.random_normal((hidden_size,), stddev = 0.2), name = \"bh_i\")\n\n        Wh_o = tf.Variable(tf.random_normal((vocab_size_output, hidden_size), stddev = 0.2), name = \"Wh_o\")\n        bh_o = tf.Variable(tf.random_normal((vocab_size_output,), stddev = 0.2), name = \"bh_o\")\n\n        # Layers\n\n        i_embedded = tf.nn.embedding_lookup(input_embeddings, input_layer)\n        i_embedded_flattened = tf.reshape(i_embedded, \n                        (\n                         (minibatch_size if minibatch_size is not None else -1), \n                         input_layer_flattened_size ) \n                        )\n\n        h = tf.tanh(tf.matmul(i_embedded_flattened, Wh_i) + bh_i)\n        nce_loss = tf.reduce_mean(tf.nn.nce_loss(Wh_o, bh_o, h, ref_input, nce_num_sampled, \n                                                     num_classes = vocab_size_output, name = \"nce\"))\n        optimizer = tf.train.MomentumOptimizer(learning_rate, learning_momentum).minimize(nce_loss)\n\n        init_op = tf.initialize_all_variables()\n\n\nwith tf.Session(graph=graph) as session:\n\n    feed_dict = {input_layer : dummy_input, ref_input : dummy_target}\n    _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)", "body": "Here is a small self-contained example that demonstrates the issue:\n\n```\nimport collections\nimport numpy as np\nimport tensorflow as tf\nimport logging\nimport codecs\nimport json\nimport itertools\nimport time\n\ndef device_for_node(n):\n    if n.type == \"MatMul\":\n        return \"/gpu:1\"\n    else:\n        return \"/cpu:0\"\n\nminibatch_size = 128\nhidden_size = 64\nembedding_size = 256\ninput_layer_size = 3\nvocab_size_input = 32\nvocab_size_output = 64\nnce_num_sampled = 16\nlearning_rate = 0.1\nlearning_momentum = 0.9\n\ndummy_input = np.zeros((minibatch_size, input_layer_size), dtype = np.int32)\ndummy_target = np.zeros((minibatch_size, 1), dtype = np.int32)\n\ninput_layer_flattened_size = input_layer_size * embedding_size\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    with graph.device(device_for_node):\n        input_layer = tf.placeholder(tf.int32, shape = (minibatch_size, input_layer_size), name = \"input_layer\")       \n        ref_input = tf.placeholder(tf.int32, shape = (minibatch_size, 1), name = \"ref_input\")\n\n        # Parameters\n\n        input_embeddings = tf.Variable(tf.random_normal([vocab_size_input, embedding_size]), name = \"i_embeddings\")\n\n        Wh_i = tf.Variable(tf.random_normal((input_layer_flattened_size, hidden_size), stddev = 0.2), name = \"Wh_i\")\n        bh_i = tf.Variable(tf.random_normal((hidden_size,), stddev = 0.2), name = \"bh_i\")\n\n        Wh_o = tf.Variable(tf.random_normal((vocab_size_output, hidden_size), stddev = 0.2), name = \"Wh_o\")\n        bh_o = tf.Variable(tf.random_normal((vocab_size_output,), stddev = 0.2), name = \"bh_o\")\n\n        # Layers\n\n        i_embedded = tf.nn.embedding_lookup(input_embeddings, input_layer)\n        i_embedded_flattened = tf.reshape(i_embedded, \n                        (\n                         (minibatch_size if minibatch_size is not None else -1), \n                         input_layer_flattened_size ) \n                        )\n\n        h = tf.tanh(tf.matmul(i_embedded_flattened, Wh_i) + bh_i)\n        nce_loss = tf.reduce_mean(tf.nn.nce_loss(Wh_o, bh_o, h, ref_input, nce_num_sampled, \n                                                     num_classes = vocab_size_output, name = \"nce\"))\n        optimizer = tf.train.MomentumOptimizer(learning_rate, learning_momentum).minimize(nce_loss)\n\n        init_op = tf.initialize_all_variables()\n\n\nwith tf.Session(graph=graph) as session:\n\n    feed_dict = {input_layer : dummy_input, ref_input : dummy_target}\n    _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n```\n"}