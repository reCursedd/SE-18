{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/426892076", "html_url": "https://github.com/tensorflow/tensorflow/issues/22687#issuecomment-426892076", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22687", "id": 426892076, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjg5MjA3Ng==", "user": {"login": "blaueck", "id": 7766017, "node_id": "MDQ6VXNlcjc3NjYwMTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/7766017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/blaueck", "html_url": "https://github.com/blaueck", "followers_url": "https://api.github.com/users/blaueck/followers", "following_url": "https://api.github.com/users/blaueck/following{/other_user}", "gists_url": "https://api.github.com/users/blaueck/gists{/gist_id}", "starred_url": "https://api.github.com/users/blaueck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/blaueck/subscriptions", "organizations_url": "https://api.github.com/users/blaueck/orgs", "repos_url": "https://api.github.com/users/blaueck/repos", "events_url": "https://api.github.com/users/blaueck/events{/privacy}", "received_events_url": "https://api.github.com/users/blaueck/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-04T05:42:50Z", "updated_at": "2018-10-04T05:42:50Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42785337\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wt-huang\">@wt-huang</a> , thanks for your reply. What I means is that tf.keras.layers.ReLU don't behave as you say. When I set max_value as a not None value, such as 5, every thing is normal. But when I set max_value as None, there is not gradient backward thought the layer.<br>\nI took a look at the code and found the max_value may be the problem. tf.keras.layers.ReLU actually call tf.keras.backend.relu to do the computation. The following code show how it works (with some simplification).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> -- code in tf.keras.layers.ReLU.__init__ --</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The same as self.max_value = np.asarray(max_value, dtype='float32').</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> When max_value is None, self.max_value will equal NaN</span>\n<span class=\"pl-c1\">self</span>.max_value <span class=\"pl-k\">=</span> K.cast_to_floatx(max_value)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> -- code in tf.keras.backend.relu --</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> passing self.max_value to tf.keras.backend.relu as max_value</span>\n\nx <span class=\"pl-k\">=</span> tf.nn.relu(x)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Since max_value is not None, the condition will eval to True</span>\n<span class=\"pl-k\">if</span> max_value <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n    x <span class=\"pl-k\">=</span> tf.clip_by_value(x, <span class=\"pl-c1\">0</span>, max_value)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> When max_value is NaN: x = tf.clip_by_value(x, 0, NaN).</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> This clip op cause gradient to be zero, because no number is between</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> 0 and NaN?</span></pre></div>\n<p>Code to reproduce the bug:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\ntf.enable_eager_execution()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> a = [-0.07873772,  1.2001798 ,  0.27422413,  0.8521496 ,  1.0448941 ]</span>\na <span class=\"pl-k\">=</span> tf.random_normal([<span class=\"pl-c1\">5</span>])\n\n<span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n    tape.watch(a)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> b = [0.        , 1.2001798 , 0.27422413, 0.8521496 , 1.0448941 ]</span>\n    b <span class=\"pl-k\">=</span> tf.keras.layers.ReLU()(a)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> da1 = [0., 0., 0., 0., 0.]</span>\nda1 <span class=\"pl-k\">=</span> tape.gradient(b, a)\n\n<span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n    tape.watch(a)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> c = [0.        , 1.2001798 , 0.27422413, 0.8521496 , 1.0448941 ]</span>\n    c <span class=\"pl-k\">=</span> tf.keras.layers.ReLU(<span class=\"pl-v\">max_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)(a)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> da2 = [0., 1., 1., 1., 1.]</span>\nda2 <span class=\"pl-k\">=</span> tape.gradient(c, a)</pre></div>", "body_text": "Hi @wt-huang , thanks for your reply. What I means is that tf.keras.layers.ReLU don't behave as you say. When I set max_value as a not None value, such as 5, every thing is normal. But when I set max_value as None, there is not gradient backward thought the layer.\nI took a look at the code and found the max_value may be the problem. tf.keras.layers.ReLU actually call tf.keras.backend.relu to do the computation. The following code show how it works (with some simplification).\n# -- code in tf.keras.layers.ReLU.__init__ --\n# The same as self.max_value = np.asarray(max_value, dtype='float32').\n# When max_value is None, self.max_value will equal NaN\nself.max_value = K.cast_to_floatx(max_value)\n\n\n# -- code in tf.keras.backend.relu --\n# passing self.max_value to tf.keras.backend.relu as max_value\n\nx = tf.nn.relu(x)\n\n# Since max_value is not None, the condition will eval to True\nif max_value is not None:\n    x = tf.clip_by_value(x, 0, max_value)\n    # When max_value is NaN: x = tf.clip_by_value(x, 0, NaN).\n    # This clip op cause gradient to be zero, because no number is between\n    # 0 and NaN?\nCode to reproduce the bug:\nimport tensorflow as tf\ntf.enable_eager_execution()\n# a = [-0.07873772,  1.2001798 ,  0.27422413,  0.8521496 ,  1.0448941 ]\na = tf.random_normal([5])\n\nwith tf.GradientTape() as tape:\n    tape.watch(a)\n    # b = [0.        , 1.2001798 , 0.27422413, 0.8521496 , 1.0448941 ]\n    b = tf.keras.layers.ReLU()(a)\n# da1 = [0., 0., 0., 0., 0.]\nda1 = tape.gradient(b, a)\n\nwith tf.GradientTape() as tape:\n    tape.watch(a)\n    # c = [0.        , 1.2001798 , 0.27422413, 0.8521496 , 1.0448941 ]\n    c = tf.keras.layers.ReLU(max_value=5)(a)\n# da2 = [0., 1., 1., 1., 1.]\nda2 = tape.gradient(c, a)", "body": "Hi @wt-huang , thanks for your reply. What I means is that tf.keras.layers.ReLU don't behave as you say. When I set max_value as a not None value, such as 5, every thing is normal. But when I set max_value as None, there is not gradient backward thought the layer. \r\nI took a look at the code and found the max_value may be the problem. tf.keras.layers.ReLU actually call tf.keras.backend.relu to do the computation. The following code show how it works (with some simplification).\r\n```python\r\n# -- code in tf.keras.layers.ReLU.__init__ --\r\n# The same as self.max_value = np.asarray(max_value, dtype='float32').\r\n# When max_value is None, self.max_value will equal NaN\r\nself.max_value = K.cast_to_floatx(max_value)\r\n\r\n\r\n# -- code in tf.keras.backend.relu --\r\n# passing self.max_value to tf.keras.backend.relu as max_value\r\n\r\nx = tf.nn.relu(x)\r\n\r\n# Since max_value is not None, the condition will eval to True\r\nif max_value is not None:\r\n    x = tf.clip_by_value(x, 0, max_value)\r\n    # When max_value is NaN: x = tf.clip_by_value(x, 0, NaN).\r\n    # This clip op cause gradient to be zero, because no number is between\r\n    # 0 and NaN?\r\n```\r\nCode to reproduce the bug:\r\n``` python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n# a = [-0.07873772,  1.2001798 ,  0.27422413,  0.8521496 ,  1.0448941 ]\r\na = tf.random_normal([5])\r\n\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(a)\r\n    # b = [0.        , 1.2001798 , 0.27422413, 0.8521496 , 1.0448941 ]\r\n    b = tf.keras.layers.ReLU()(a)\r\n# da1 = [0., 0., 0., 0., 0.]\r\nda1 = tape.gradient(b, a)\r\n\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(a)\r\n    # c = [0.        , 1.2001798 , 0.27422413, 0.8521496 , 1.0448941 ]\r\n    c = tf.keras.layers.ReLU(max_value=5)(a)\r\n# da2 = [0., 1., 1., 1., 1.]\r\nda2 = tape.gradient(c, a)\r\n```"}