{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/426871901", "html_url": "https://github.com/tensorflow/tensorflow/issues/22687#issuecomment-426871901", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22687", "id": 426871901, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjg3MTkwMQ==", "user": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-04T03:15:47Z", "updated_at": "2018-10-04T03:15:47Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7766017\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/blaueck\">@blaueck</a> Actually this is not a bug. When max_value is set as default value None, it means that no clipping is applied. Gradient will be either 0 or 1 depending on whether x is positive or negative. There is a zero gradient issue that can cause the output to become zero. This is addressed by modified ReLU such as ELU or leaky ReLU.</p>", "body_text": "@blaueck Actually this is not a bug. When max_value is set as default value None, it means that no clipping is applied. Gradient will be either 0 or 1 depending on whether x is positive or negative. There is a zero gradient issue that can cause the output to become zero. This is addressed by modified ReLU such as ELU or leaky ReLU.", "body": "@blaueck Actually this is not a bug. When max_value is set as default value None, it means that no clipping is applied. Gradient will be either 0 or 1 depending on whether x is positive or negative. There is a zero gradient issue that can cause the output to become zero. This is addressed by modified ReLU such as ELU or leaky ReLU."}