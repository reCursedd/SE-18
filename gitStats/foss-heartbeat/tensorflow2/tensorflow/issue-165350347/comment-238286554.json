{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/238286554", "html_url": "https://github.com/tensorflow/tensorflow/issues/3295#issuecomment-238286554", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3295", "id": 238286554, "node_id": "MDEyOklzc3VlQ29tbWVudDIzODI4NjU1NA==", "user": {"login": "millisecond", "id": 73840, "node_id": "MDQ6VXNlcjczODQw", "avatar_url": "https://avatars1.githubusercontent.com/u/73840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/millisecond", "html_url": "https://github.com/millisecond", "followers_url": "https://api.github.com/users/millisecond/followers", "following_url": "https://api.github.com/users/millisecond/following{/other_user}", "gists_url": "https://api.github.com/users/millisecond/gists{/gist_id}", "starred_url": "https://api.github.com/users/millisecond/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/millisecond/subscriptions", "organizations_url": "https://api.github.com/users/millisecond/orgs", "repos_url": "https://api.github.com/users/millisecond/repos", "events_url": "https://api.github.com/users/millisecond/events{/privacy}", "received_events_url": "https://api.github.com/users/millisecond/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-08T16:09:57Z", "updated_at": "2016-08-08T16:09:57Z", "author_association": "NONE", "body_html": "<p>I'm not the original author, but I'm having the same performance degradation when using the server.</p>\n<p>Here's my training loop, which I believe is straight from one of the examples:</p>\n<p><code>for step in xrange(num_epochs * train_size // self.batch_size): offset = (step * self.batch_size) % train_size batch_data = train_data[offset:(offset + self.batch_size), :] batch_labels = train_labels[offset:(offset + self.batch_size)] train_step.run(feed_dict={x: batch_data, y_: batch_labels})</code></p>\n<p>Tried several variations on batch_size (50, 100, 1000) to saturate both processing power on the server and network usage to no avail.  Any tips / suggestions on making it more performant in the client-server scenario?</p>", "body_text": "I'm not the original author, but I'm having the same performance degradation when using the server.\nHere's my training loop, which I believe is straight from one of the examples:\nfor step in xrange(num_epochs * train_size // self.batch_size): offset = (step * self.batch_size) % train_size batch_data = train_data[offset:(offset + self.batch_size), :] batch_labels = train_labels[offset:(offset + self.batch_size)] train_step.run(feed_dict={x: batch_data, y_: batch_labels})\nTried several variations on batch_size (50, 100, 1000) to saturate both processing power on the server and network usage to no avail.  Any tips / suggestions on making it more performant in the client-server scenario?", "body": "I'm not the original author, but I'm having the same performance degradation when using the server.  \n\nHere's my training loop, which I believe is straight from one of the examples: \n\n`for step in xrange(num_epochs * train_size // self.batch_size):\n    offset = (step * self.batch_size) % train_size\n    batch_data = train_data[offset:(offset + self.batch_size), :]\n    batch_labels = train_labels[offset:(offset + self.batch_size)]\n    train_step.run(feed_dict={x: batch_data, y_: batch_labels})`\n\nTried several variations on batch_size (50, 100, 1000) to saturate both processing power on the server and network usage to no avail.  Any tips / suggestions on making it more performant in the client-server scenario?  \n"}