{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/433885149", "html_url": "https://github.com/tensorflow/tensorflow/issues/21614#issuecomment-433885149", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21614", "id": 433885149, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzg4NTE0OQ==", "user": {"login": "nils-noomi", "id": 40029850, "node_id": "MDQ6VXNlcjQwMDI5ODUw", "avatar_url": "https://avatars2.githubusercontent.com/u/40029850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nils-noomi", "html_url": "https://github.com/nils-noomi", "followers_url": "https://api.github.com/users/nils-noomi/followers", "following_url": "https://api.github.com/users/nils-noomi/following{/other_user}", "gists_url": "https://api.github.com/users/nils-noomi/gists{/gist_id}", "starred_url": "https://api.github.com/users/nils-noomi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nils-noomi/subscriptions", "organizations_url": "https://api.github.com/users/nils-noomi/orgs", "repos_url": "https://api.github.com/users/nils-noomi/repos", "events_url": "https://api.github.com/users/nils-noomi/events{/privacy}", "received_events_url": "https://api.github.com/users/nils-noomi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-29T12:02:59Z", "updated_at": "2018-10-29T12:04:01Z", "author_association": "NONE", "body_html": "<p>I've run into the same issue, and it seem like  _clone_functional_model ends up passing the initial state as both inputs and as a keyword argument to the <strong>call</strong>/_standardize_args method of the Recurrent layer. (1)</p>\n<p>A <strong>nasty workaround</strong> for us was to comment out the code extracting the initial state from the inputs as follows, since the shape of the state in the input list was not complete, but it was in the kwarg:</p>\n<p><code>   def _standardize_args(...): if isinstance(inputs, list): # assert initial_state is None and constants is None # if num_constants is not None: #   constants = inputs[-num_constants:] #   inputs = inputs[:-num_constants] # if len(inputs) &gt; 1: #   initial_state = inputs[1:] inputs = inputs[0]</code><br>\n...<br>\n(1) in the call to standardize_args when converting to an estimator I get this:<br>\n`inputs: [&lt;tf.Tensor 'time_distributed_7/transpose_1:0' shape=(1, ?, 320) dtype=float32&gt;, &lt;tf.Tensor 'input_state_h_2/input_state_h/Identity:0' shape=(?,) dtype=float32&gt;, &lt;tf.Tensor 'input_state_c_2/input_state_c/Identity:0' shape=(?,) dtype=float32&gt;]</p>\n<p>initial_state kwarg: [&lt;tf.Tensor 'input_state_h:0' shape=(1, 100) dtype=float32&gt;, &lt;tf.Tensor 'input_state_c:0' shape=(1, 100) dtype=float32&gt;]`</p>\n<p>I've not had time to debug where this happens, but I hope it helps someone.</p>", "body_text": "I've run into the same issue, and it seem like  _clone_functional_model ends up passing the initial state as both inputs and as a keyword argument to the call/_standardize_args method of the Recurrent layer. (1)\nA nasty workaround for us was to comment out the code extracting the initial state from the inputs as follows, since the shape of the state in the input list was not complete, but it was in the kwarg:\n   def _standardize_args(...): if isinstance(inputs, list): # assert initial_state is None and constants is None # if num_constants is not None: #   constants = inputs[-num_constants:] #   inputs = inputs[:-num_constants] # if len(inputs) > 1: #   initial_state = inputs[1:] inputs = inputs[0]\n...\n(1) in the call to standardize_args when converting to an estimator I get this:\n`inputs: [<tf.Tensor 'time_distributed_7/transpose_1:0' shape=(1, ?, 320) dtype=float32>, <tf.Tensor 'input_state_h_2/input_state_h/Identity:0' shape=(?,) dtype=float32>, <tf.Tensor 'input_state_c_2/input_state_c/Identity:0' shape=(?,) dtype=float32>]\ninitial_state kwarg: [<tf.Tensor 'input_state_h:0' shape=(1, 100) dtype=float32>, <tf.Tensor 'input_state_c:0' shape=(1, 100) dtype=float32>]`\nI've not had time to debug where this happens, but I hope it helps someone.", "body": "I've run into the same issue, and it seem like  _clone_functional_model ends up passing the initial state as both inputs and as a keyword argument to the __call__/_standardize_args method of the Recurrent layer. (1) \r\n\r\nA **nasty workaround** for us was to comment out the code extracting the initial state from the inputs as follows, since the shape of the state in the input list was not complete, but it was in the kwarg:\r\n\r\n`  \r\ndef _standardize_args(...):\r\n  if isinstance(inputs, list):\r\n    # assert initial_state is None and constants is None\r\n    # if num_constants is not None:\r\n    #   constants = inputs[-num_constants:]\r\n    #   inputs = inputs[:-num_constants]\r\n    # if len(inputs) > 1:\r\n    #   initial_state = inputs[1:]\r\n    inputs = inputs[0]`\r\n  ...\r\n(1) in the call to standardize_args when converting to an estimator I get this: \r\n`inputs: [<tf.Tensor 'time_distributed_7/transpose_1:0' shape=(1, ?, 320) dtype=float32>, <tf.Tensor 'input_state_h_2/input_state_h/Identity:0' shape=(?,) dtype=float32>, <tf.Tensor 'input_state_c_2/input_state_c/Identity:0' shape=(?,) dtype=float32>]\r\n\r\ninitial_state kwarg: [<tf.Tensor 'input_state_h:0' shape=(1, 100) dtype=float32>, <tf.Tensor 'input_state_c:0' shape=(1, 100) dtype=float32>]`\r\n\r\nI've not had time to debug where this happens, but I hope it helps someone."}