{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/414406301", "html_url": "https://github.com/tensorflow/tensorflow/issues/21614#issuecomment-414406301", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21614", "id": 414406301, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNDQwNjMwMQ==", "user": {"login": "qlzh727", "id": 5118881, "node_id": "MDQ6VXNlcjUxMTg4ODE=", "avatar_url": "https://avatars3.githubusercontent.com/u/5118881?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qlzh727", "html_url": "https://github.com/qlzh727", "followers_url": "https://api.github.com/users/qlzh727/followers", "following_url": "https://api.github.com/users/qlzh727/following{/other_user}", "gists_url": "https://api.github.com/users/qlzh727/gists{/gist_id}", "starred_url": "https://api.github.com/users/qlzh727/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qlzh727/subscriptions", "organizations_url": "https://api.github.com/users/qlzh727/orgs", "repos_url": "https://api.github.com/users/qlzh727/repos", "events_url": "https://api.github.com/users/qlzh727/events{/privacy}", "received_events_url": "https://api.github.com/users/qlzh727/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-20T17:54:59Z", "updated_at": "2018-08-20T17:54:59Z", "author_association": "MEMBER", "body_html": "<p>Probably is a bug in the model_to_estimator.</p>\n<p>Btw, the input of the model is not constructed correctly, namely the input shape.</p>\n<p>The input should be in shape (batch, input_length), which should be (None, seq_len) in your case. The line tf.keras.layers.Input(shape=(50, ) will cause the emb to return (None, 50, 32) and the expected input to LSTM should be (None, 32, 50) \"(batch, timestep, vocab)\".</p>", "body_text": "Probably is a bug in the model_to_estimator.\nBtw, the input of the model is not constructed correctly, namely the input shape.\nThe input should be in shape (batch, input_length), which should be (None, seq_len) in your case. The line tf.keras.layers.Input(shape=(50, ) will cause the emb to return (None, 50, 32) and the expected input to LSTM should be (None, 32, 50) \"(batch, timestep, vocab)\".", "body": "Probably is a bug in the model_to_estimator.\r\n\r\nBtw, the input of the model is not constructed correctly, namely the input shape.\r\n\r\nThe input should be in shape (batch, input_length), which should be (None, seq_len) in your case. The line tf.keras.layers.Input(shape=(50, ) will cause the emb to return (None, 50, 32) and the expected input to LSTM should be (None, 32, 50) \"(batch, timestep, vocab)\"."}