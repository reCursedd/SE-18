{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/314924572", "html_url": "https://github.com/tensorflow/tensorflow/issues/600#issuecomment-314924572", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/600", "id": 314924572, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNDkyNDU3Mg==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-12T23:17:23Z", "updated_at": "2017-07-12T23:17:23Z", "author_association": "MEMBER", "body_html": "<p>It depends how long. If it's into the thousands but not too high, say upto 4000 or so, our recent attention models from tensor2tensor (<a href=\"https://github.com/tensorflow/tensor2tensor\">https://github.com/tensorflow/tensor2tensor</a>) work quite well. If it's larger, then you need to think. Memory is only one problem, you also need to consider how will the network know where to propagate gradients, how will it find correlations if they are very-long-term? We managed to get sth working in many-thousands distance setting in the paper \"Learning to Remember Rare Events\", but at the cost of an extra loss that can be hard to train. I'd say it's an ongoing research problem at this point. But sometimes you can just work around it, e.g., put a bunch of strided convs or pooling first and down-size your input: if you don't need the specific tokens, just general summary, that often works best.</p>", "body_text": "It depends how long. If it's into the thousands but not too high, say upto 4000 or so, our recent attention models from tensor2tensor (https://github.com/tensorflow/tensor2tensor) work quite well. If it's larger, then you need to think. Memory is only one problem, you also need to consider how will the network know where to propagate gradients, how will it find correlations if they are very-long-term? We managed to get sth working in many-thousands distance setting in the paper \"Learning to Remember Rare Events\", but at the cost of an extra loss that can be hard to train. I'd say it's an ongoing research problem at this point. But sometimes you can just work around it, e.g., put a bunch of strided convs or pooling first and down-size your input: if you don't need the specific tokens, just general summary, that often works best.", "body": "It depends how long. If it's into the thousands but not too high, say upto 4000 or so, our recent attention models from tensor2tensor (https://github.com/tensorflow/tensor2tensor) work quite well. If it's larger, then you need to think. Memory is only one problem, you also need to consider how will the network know where to propagate gradients, how will it find correlations if they are very-long-term? We managed to get sth working in many-thousands distance setting in the paper \"Learning to Remember Rare Events\", but at the cost of an extra loss that can be hard to train. I'd say it's an ongoing research problem at this point. But sometimes you can just work around it, e.g., put a bunch of strided convs or pooling first and down-size your input: if you don't need the specific tokens, just general summary, that often works best."}