{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/314850452", "html_url": "https://github.com/tensorflow/tensorflow/issues/600#issuecomment-314850452", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/600", "id": 314850452, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNDg1MDQ1Mg==", "user": {"login": "RylanSchaeffer", "id": 8942987, "node_id": "MDQ6VXNlcjg5NDI5ODc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8942987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RylanSchaeffer", "html_url": "https://github.com/RylanSchaeffer", "followers_url": "https://api.github.com/users/RylanSchaeffer/followers", "following_url": "https://api.github.com/users/RylanSchaeffer/following{/other_user}", "gists_url": "https://api.github.com/users/RylanSchaeffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/RylanSchaeffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RylanSchaeffer/subscriptions", "organizations_url": "https://api.github.com/users/RylanSchaeffer/orgs", "repos_url": "https://api.github.com/users/RylanSchaeffer/repos", "events_url": "https://api.github.com/users/RylanSchaeffer/events{/privacy}", "received_events_url": "https://api.github.com/users/RylanSchaeffer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-12T18:05:50Z", "updated_at": "2017-07-12T18:05:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a> , in general, for sequence to sequence models with extremely long sequences, how do you recommend fitting them into memory without endangering the attention mechanism?</p>", "body_text": "@lukaszkaiser , in general, for sequence to sequence models with extremely long sequences, how do you recommend fitting them into memory without endangering the attention mechanism?", "body": "@lukaszkaiser , in general, for sequence to sequence models with extremely long sequences, how do you recommend fitting them into memory without endangering the attention mechanism?"}