{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/392586338", "html_url": "https://github.com/tensorflow/tensorflow/issues/18919#issuecomment-392586338", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18919", "id": 392586338, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjU4NjMzOA==", "user": {"login": "fgr1986", "id": 10627849, "node_id": "MDQ6VXNlcjEwNjI3ODQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/10627849?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fgr1986", "html_url": "https://github.com/fgr1986", "followers_url": "https://api.github.com/users/fgr1986/followers", "following_url": "https://api.github.com/users/fgr1986/following{/other_user}", "gists_url": "https://api.github.com/users/fgr1986/gists{/gist_id}", "starred_url": "https://api.github.com/users/fgr1986/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fgr1986/subscriptions", "organizations_url": "https://api.github.com/users/fgr1986/orgs", "repos_url": "https://api.github.com/users/fgr1986/repos", "events_url": "https://api.github.com/users/fgr1986/events{/privacy}", "received_events_url": "https://api.github.com/users/fgr1986/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-28T18:52:17Z", "updated_at": "2018-05-28T18:57:36Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1450614\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/suharshs\">@suharshs</a> Tahnk you very much, let me know if you need a hand with the documentation.<br>\nIn the mean time, I tried myself the following example, but unfortunately couldn't show how the quantization is correctly working.<br>\nPlease note that I use <code>experimental_create_training_graph</code> instead <code>create_training_graph</code> only to be able to determine the quantization bits more easily:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nD_SIZE = 2048\nBATCH_SIZE = 32\nEPOCHS = 1000\nW_B = 3\nA_B = 3\n\n\ndef train_quantization():\n\n    g_tr = tf.Graph()\n    with g_tr.as_default():\n\n        # prepare dataset\n        features_arr = tf.constant(2 * np.arange(D_SIZE), dtype=tf.float32)\n        labels_arr = tf.constant(np.arange(D_SIZE), dtype=tf.float32)\n        trn_data = tf.data.Dataset.zip(\n            (tf.data.Dataset.from_tensor_slices(features_arr),\n             tf.data.Dataset.from_tensor_slices(labels_arr))\n        )\n        trn_data = trn_data.shuffle(1000)\n        trn_data = trn_data.batch(BATCH_SIZE)\n        trn_data = trn_data.repeat(EPOCHS)\n        trn_data = trn_data.prefetch(1)\n\n        iterator = tf.data.Iterator.from_structure(trn_data.output_types,\n                                                   trn_data.output_shapes)\n        feat, labels = iterator.get_next()\n        data_init_op = iterator.make_initializer(trn_data)\n\n        # global step\n        global_step = tf.train.get_or_create_global_step()\n\n        # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION\n        flow = tf.identity(feat, name='input_tensor')\n        print(feat)\n        flow = tf.reshape(flow, (-1, BATCH_SIZE))\n        # require activation for quantization\n        flow = tf.layers.dense(flow, units=BATCH_SIZE,\n                               activation=tf.nn.relu, name='out')\n\n        # Build forward pass of model.\n        # Make the loss op\n        with tf.variable_scope('loss'):\n            tf.logging.debug('[loss] size of out: %s',\n                             flow.get_shape().as_list())\n            power = tf.pow(labels - flow, 2)\n            power = tf.reduce_sum(power, 1)\n            loss = tf.reduce_mean(power, name='m_loss')\n        tf.summary.scalar('loss', loss)\n\n        # Optimizer\n        opt_op = tf.train.AdamOptimizer(1e-4).minimize(loss, global_step)\n\n\n        # Call the training rewrite which rewrites the graph in-place with\n        # FakeQuantization nodes and folds batchnorm for training. It is\n        # often needed to fine tune a floating point model for quantization\n        # with this training tool. When training from scratch, quant_delay\n        # can be used to activate quantization after training to converge\n        # with the float graph, effectively fine-tuning the model.\n        tf.contrib.quantize.experimental_create_training_graph(weight_bits=W_B,\n                                                               activation_bits=A_B,\n                                                               quant_delay=int(EPOCHS / 2))\n\n        # Merge all the summaries\n        merged = tf.summary.merge_all()\n\n        with tf.Session(graph=g_tr) as sess:\n            # global variables Initializing\n            sess.run(tf.global_variables_initializer())\n            # local variables Initializing\n            sess.run(tf.local_variables_initializer())\n\n            train_writer = tf.summary.FileWriter('./tmp/train', sess.graph)\n\n            # re-initialize the iterator, but this time with training data\n            sess.run(data_init_op)\n\n            epoch_counter = 0\n            for i in range(int(D_SIZE/BATCH_SIZE) * EPOCHS):\n                _, nn_out, nn_loss, gs, summary = sess.run(\n                    [opt_op, flow, loss, global_step, merged])\n                # tensorboard and  statistics\n                train_writer.add_summary(summary, gs)\n                if i % BATCH_SIZE == 0:\n                    print(epoch_counter, ': loss ', nn_loss)\n                    epoch_counter += 1\n\n            saver = tf.train.Saver()\n            saver.save(sess, './tmp/oink.ckpt')\n\n        # debug\n        print('nn_out different values in BATCH_SIZE (32) elements:', len(np.unique(nn_out)))\n        print('nn_out [0:8]: ', nn_out[0:8])\n        print('nn_labels [0:8]: ', nn_labels[0:8])\n\n\n        print('[debug] exit training')\n\ndef inference_quantization():\n\n    #################################\n    # Reset default graph\n    tf.reset_default_graph()\n    #################################\n\n    g_inf = tf.Graph()\n    with g_inf.as_default():\n        # create inference graph\n        # prepare dataset\n        features_arr = tf.constant(2 * np.arange(D_SIZE), dtype=tf.float32)\n        labels_arr = tf.constant(np.arange(D_SIZE), dtype=tf.float32)\n        trn_data = tf.data.Dataset.zip(\n            (tf.data.Dataset.from_tensor_slices(features_arr),\n             tf.data.Dataset.from_tensor_slices(labels_arr))\n        )\n        trn_data = trn_data.shuffle(1000)\n        trn_data = trn_data.batch(BATCH_SIZE)\n        trn_data = trn_data.repeat(EPOCHS)\n        trn_data = trn_data.prefetch(1)\n\n        iterator = tf.data.Iterator.from_structure(trn_data.output_types,\n                                                   trn_data.output_shapes)\n        feat, labels = iterator.get_next()\n        data_init_op = iterator.make_initializer(trn_data)\n\n        # global step\n        global_step = tf.train.get_or_create_global_step()\n\n        # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION\n        flow = tf.identity(feat, name='input_tensor')\n        print(feat)\n        flow = tf.reshape(flow, (-1, BATCH_SIZE))\n        # require activation for quantization\n        flow = tf.layers.dense(flow, units=BATCH_SIZE,\n                               activation=tf.nn.relu, name='out')\n\n        # Build forward pass of model.\n        # Make the loss op\n        with tf.variable_scope('loss'):\n            tf.logging.debug('[loss] size of out: %s',\n                             flow.get_shape().as_list())\n            power = tf.pow(labels - flow, 2)\n            power = tf.reduce_sum(power, 1)\n            loss = tf.reduce_mean(power, name='m_loss')\n        tf.summary.scalar('loss', loss)\n\n        with tf.Session(graph=g_inf) as sess:\n            # Call the eval rewrite which rewrites the graph in-place with\n            # FakeQuantization nodes and fold batchnorm for eval.\n            tf.contrib.quantize.experimental_create_eval_graph(weight_bits=W_B,\n                                                               activation_bits=A_B)\n\n            saver = tf.train.Saver()\n            saver.restore(sess, \"./tmp/oink.ckpt\")\n            # frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n            #       sess, sess.graph_def, [\"out\"])\n            print('[debug] restored')\n\n            # re-initialize the iterator, but this time with training data\n            sess.run(data_init_op)\n\n            nn_out, nn_labels, nn_loss = sess.run([flow, labels, loss])\n\n        # debug\n        print('loss: ', nn_loss)\n        print('nn_out different values in BATCH_SIZE (32) elements:', len(np.unique(nn_out)))\n        print('nn_out [0:8]: ', nn_out[0:8])\n        print('nn_labels [0:8]: ', nn_labels[0:8])\n\ndef test_quantization():\n\n    train_quantization()\n    inference_quantization()\n\ntest_quantization()\n\n\n</code></pre>", "body_text": "@suharshs Tahnk you very much, let me know if you need a hand with the documentation.\nIn the mean time, I tried myself the following example, but unfortunately couldn't show how the quantization is correctly working.\nPlease note that I use experimental_create_training_graph instead create_training_graph only to be able to determine the quantization bits more easily:\nimport numpy as np\nimport tensorflow as tf\n\nD_SIZE = 2048\nBATCH_SIZE = 32\nEPOCHS = 1000\nW_B = 3\nA_B = 3\n\n\ndef train_quantization():\n\n    g_tr = tf.Graph()\n    with g_tr.as_default():\n\n        # prepare dataset\n        features_arr = tf.constant(2 * np.arange(D_SIZE), dtype=tf.float32)\n        labels_arr = tf.constant(np.arange(D_SIZE), dtype=tf.float32)\n        trn_data = tf.data.Dataset.zip(\n            (tf.data.Dataset.from_tensor_slices(features_arr),\n             tf.data.Dataset.from_tensor_slices(labels_arr))\n        )\n        trn_data = trn_data.shuffle(1000)\n        trn_data = trn_data.batch(BATCH_SIZE)\n        trn_data = trn_data.repeat(EPOCHS)\n        trn_data = trn_data.prefetch(1)\n\n        iterator = tf.data.Iterator.from_structure(trn_data.output_types,\n                                                   trn_data.output_shapes)\n        feat, labels = iterator.get_next()\n        data_init_op = iterator.make_initializer(trn_data)\n\n        # global step\n        global_step = tf.train.get_or_create_global_step()\n\n        # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION\n        flow = tf.identity(feat, name='input_tensor')\n        print(feat)\n        flow = tf.reshape(flow, (-1, BATCH_SIZE))\n        # require activation for quantization\n        flow = tf.layers.dense(flow, units=BATCH_SIZE,\n                               activation=tf.nn.relu, name='out')\n\n        # Build forward pass of model.\n        # Make the loss op\n        with tf.variable_scope('loss'):\n            tf.logging.debug('[loss] size of out: %s',\n                             flow.get_shape().as_list())\n            power = tf.pow(labels - flow, 2)\n            power = tf.reduce_sum(power, 1)\n            loss = tf.reduce_mean(power, name='m_loss')\n        tf.summary.scalar('loss', loss)\n\n        # Optimizer\n        opt_op = tf.train.AdamOptimizer(1e-4).minimize(loss, global_step)\n\n\n        # Call the training rewrite which rewrites the graph in-place with\n        # FakeQuantization nodes and folds batchnorm for training. It is\n        # often needed to fine tune a floating point model for quantization\n        # with this training tool. When training from scratch, quant_delay\n        # can be used to activate quantization after training to converge\n        # with the float graph, effectively fine-tuning the model.\n        tf.contrib.quantize.experimental_create_training_graph(weight_bits=W_B,\n                                                               activation_bits=A_B,\n                                                               quant_delay=int(EPOCHS / 2))\n\n        # Merge all the summaries\n        merged = tf.summary.merge_all()\n\n        with tf.Session(graph=g_tr) as sess:\n            # global variables Initializing\n            sess.run(tf.global_variables_initializer())\n            # local variables Initializing\n            sess.run(tf.local_variables_initializer())\n\n            train_writer = tf.summary.FileWriter('./tmp/train', sess.graph)\n\n            # re-initialize the iterator, but this time with training data\n            sess.run(data_init_op)\n\n            epoch_counter = 0\n            for i in range(int(D_SIZE/BATCH_SIZE) * EPOCHS):\n                _, nn_out, nn_loss, gs, summary = sess.run(\n                    [opt_op, flow, loss, global_step, merged])\n                # tensorboard and  statistics\n                train_writer.add_summary(summary, gs)\n                if i % BATCH_SIZE == 0:\n                    print(epoch_counter, ': loss ', nn_loss)\n                    epoch_counter += 1\n\n            saver = tf.train.Saver()\n            saver.save(sess, './tmp/oink.ckpt')\n\n        # debug\n        print('nn_out different values in BATCH_SIZE (32) elements:', len(np.unique(nn_out)))\n        print('nn_out [0:8]: ', nn_out[0:8])\n        print('nn_labels [0:8]: ', nn_labels[0:8])\n\n\n        print('[debug] exit training')\n\ndef inference_quantization():\n\n    #################################\n    # Reset default graph\n    tf.reset_default_graph()\n    #################################\n\n    g_inf = tf.Graph()\n    with g_inf.as_default():\n        # create inference graph\n        # prepare dataset\n        features_arr = tf.constant(2 * np.arange(D_SIZE), dtype=tf.float32)\n        labels_arr = tf.constant(np.arange(D_SIZE), dtype=tf.float32)\n        trn_data = tf.data.Dataset.zip(\n            (tf.data.Dataset.from_tensor_slices(features_arr),\n             tf.data.Dataset.from_tensor_slices(labels_arr))\n        )\n        trn_data = trn_data.shuffle(1000)\n        trn_data = trn_data.batch(BATCH_SIZE)\n        trn_data = trn_data.repeat(EPOCHS)\n        trn_data = trn_data.prefetch(1)\n\n        iterator = tf.data.Iterator.from_structure(trn_data.output_types,\n                                                   trn_data.output_shapes)\n        feat, labels = iterator.get_next()\n        data_init_op = iterator.make_initializer(trn_data)\n\n        # global step\n        global_step = tf.train.get_or_create_global_step()\n\n        # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION\n        flow = tf.identity(feat, name='input_tensor')\n        print(feat)\n        flow = tf.reshape(flow, (-1, BATCH_SIZE))\n        # require activation for quantization\n        flow = tf.layers.dense(flow, units=BATCH_SIZE,\n                               activation=tf.nn.relu, name='out')\n\n        # Build forward pass of model.\n        # Make the loss op\n        with tf.variable_scope('loss'):\n            tf.logging.debug('[loss] size of out: %s',\n                             flow.get_shape().as_list())\n            power = tf.pow(labels - flow, 2)\n            power = tf.reduce_sum(power, 1)\n            loss = tf.reduce_mean(power, name='m_loss')\n        tf.summary.scalar('loss', loss)\n\n        with tf.Session(graph=g_inf) as sess:\n            # Call the eval rewrite which rewrites the graph in-place with\n            # FakeQuantization nodes and fold batchnorm for eval.\n            tf.contrib.quantize.experimental_create_eval_graph(weight_bits=W_B,\n                                                               activation_bits=A_B)\n\n            saver = tf.train.Saver()\n            saver.restore(sess, \"./tmp/oink.ckpt\")\n            # frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n            #       sess, sess.graph_def, [\"out\"])\n            print('[debug] restored')\n\n            # re-initialize the iterator, but this time with training data\n            sess.run(data_init_op)\n\n            nn_out, nn_labels, nn_loss = sess.run([flow, labels, loss])\n\n        # debug\n        print('loss: ', nn_loss)\n        print('nn_out different values in BATCH_SIZE (32) elements:', len(np.unique(nn_out)))\n        print('nn_out [0:8]: ', nn_out[0:8])\n        print('nn_labels [0:8]: ', nn_labels[0:8])\n\ndef test_quantization():\n\n    train_quantization()\n    inference_quantization()\n\ntest_quantization()", "body": "@suharshs Tahnk you very much, let me know if you need a hand with the documentation.\r\nIn the mean time, I tried myself the following example, but unfortunately couldn't show how the quantization is correctly working.\r\nPlease note that I use `experimental_create_training_graph` instead `create_training_graph` only to be able to determine the quantization bits more easily:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nD_SIZE = 2048\r\nBATCH_SIZE = 32\r\nEPOCHS = 1000\r\nW_B = 3\r\nA_B = 3\r\n\r\n\r\ndef train_quantization():\r\n\r\n    g_tr = tf.Graph()\r\n    with g_tr.as_default():\r\n\r\n        # prepare dataset\r\n        features_arr = tf.constant(2 * np.arange(D_SIZE), dtype=tf.float32)\r\n        labels_arr = tf.constant(np.arange(D_SIZE), dtype=tf.float32)\r\n        trn_data = tf.data.Dataset.zip(\r\n            (tf.data.Dataset.from_tensor_slices(features_arr),\r\n             tf.data.Dataset.from_tensor_slices(labels_arr))\r\n        )\r\n        trn_data = trn_data.shuffle(1000)\r\n        trn_data = trn_data.batch(BATCH_SIZE)\r\n        trn_data = trn_data.repeat(EPOCHS)\r\n        trn_data = trn_data.prefetch(1)\r\n\r\n        iterator = tf.data.Iterator.from_structure(trn_data.output_types,\r\n                                                   trn_data.output_shapes)\r\n        feat, labels = iterator.get_next()\r\n        data_init_op = iterator.make_initializer(trn_data)\r\n\r\n        # global step\r\n        global_step = tf.train.get_or_create_global_step()\r\n\r\n        # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION\r\n        flow = tf.identity(feat, name='input_tensor')\r\n        print(feat)\r\n        flow = tf.reshape(flow, (-1, BATCH_SIZE))\r\n        # require activation for quantization\r\n        flow = tf.layers.dense(flow, units=BATCH_SIZE,\r\n                               activation=tf.nn.relu, name='out')\r\n\r\n        # Build forward pass of model.\r\n        # Make the loss op\r\n        with tf.variable_scope('loss'):\r\n            tf.logging.debug('[loss] size of out: %s',\r\n                             flow.get_shape().as_list())\r\n            power = tf.pow(labels - flow, 2)\r\n            power = tf.reduce_sum(power, 1)\r\n            loss = tf.reduce_mean(power, name='m_loss')\r\n        tf.summary.scalar('loss', loss)\r\n\r\n        # Optimizer\r\n        opt_op = tf.train.AdamOptimizer(1e-4).minimize(loss, global_step)\r\n\r\n\r\n        # Call the training rewrite which rewrites the graph in-place with\r\n        # FakeQuantization nodes and folds batchnorm for training. It is\r\n        # often needed to fine tune a floating point model for quantization\r\n        # with this training tool. When training from scratch, quant_delay\r\n        # can be used to activate quantization after training to converge\r\n        # with the float graph, effectively fine-tuning the model.\r\n        tf.contrib.quantize.experimental_create_training_graph(weight_bits=W_B,\r\n                                                               activation_bits=A_B,\r\n                                                               quant_delay=int(EPOCHS / 2))\r\n\r\n        # Merge all the summaries\r\n        merged = tf.summary.merge_all()\r\n\r\n        with tf.Session(graph=g_tr) as sess:\r\n            # global variables Initializing\r\n            sess.run(tf.global_variables_initializer())\r\n            # local variables Initializing\r\n            sess.run(tf.local_variables_initializer())\r\n\r\n            train_writer = tf.summary.FileWriter('./tmp/train', sess.graph)\r\n\r\n            # re-initialize the iterator, but this time with training data\r\n            sess.run(data_init_op)\r\n\r\n            epoch_counter = 0\r\n            for i in range(int(D_SIZE/BATCH_SIZE) * EPOCHS):\r\n                _, nn_out, nn_loss, gs, summary = sess.run(\r\n                    [opt_op, flow, loss, global_step, merged])\r\n                # tensorboard and  statistics\r\n                train_writer.add_summary(summary, gs)\r\n                if i % BATCH_SIZE == 0:\r\n                    print(epoch_counter, ': loss ', nn_loss)\r\n                    epoch_counter += 1\r\n\r\n            saver = tf.train.Saver()\r\n            saver.save(sess, './tmp/oink.ckpt')\r\n\r\n        # debug\r\n        print('nn_out different values in BATCH_SIZE (32) elements:', len(np.unique(nn_out)))\r\n        print('nn_out [0:8]: ', nn_out[0:8])\r\n        print('nn_labels [0:8]: ', nn_labels[0:8])\r\n\r\n\r\n        print('[debug] exit training')\r\n\r\ndef inference_quantization():\r\n\r\n    #################################\r\n    # Reset default graph\r\n    tf.reset_default_graph()\r\n    #################################\r\n\r\n    g_inf = tf.Graph()\r\n    with g_inf.as_default():\r\n        # create inference graph\r\n        # prepare dataset\r\n        features_arr = tf.constant(2 * np.arange(D_SIZE), dtype=tf.float32)\r\n        labels_arr = tf.constant(np.arange(D_SIZE), dtype=tf.float32)\r\n        trn_data = tf.data.Dataset.zip(\r\n            (tf.data.Dataset.from_tensor_slices(features_arr),\r\n             tf.data.Dataset.from_tensor_slices(labels_arr))\r\n        )\r\n        trn_data = trn_data.shuffle(1000)\r\n        trn_data = trn_data.batch(BATCH_SIZE)\r\n        trn_data = trn_data.repeat(EPOCHS)\r\n        trn_data = trn_data.prefetch(1)\r\n\r\n        iterator = tf.data.Iterator.from_structure(trn_data.output_types,\r\n                                                   trn_data.output_shapes)\r\n        feat, labels = iterator.get_next()\r\n        data_init_op = iterator.make_initializer(trn_data)\r\n\r\n        # global step\r\n        global_step = tf.train.get_or_create_global_step()\r\n\r\n        # simple model dense, WITH ACTIVATION TO TEST QUANTIFICATION\r\n        flow = tf.identity(feat, name='input_tensor')\r\n        print(feat)\r\n        flow = tf.reshape(flow, (-1, BATCH_SIZE))\r\n        # require activation for quantization\r\n        flow = tf.layers.dense(flow, units=BATCH_SIZE,\r\n                               activation=tf.nn.relu, name='out')\r\n\r\n        # Build forward pass of model.\r\n        # Make the loss op\r\n        with tf.variable_scope('loss'):\r\n            tf.logging.debug('[loss] size of out: %s',\r\n                             flow.get_shape().as_list())\r\n            power = tf.pow(labels - flow, 2)\r\n            power = tf.reduce_sum(power, 1)\r\n            loss = tf.reduce_mean(power, name='m_loss')\r\n        tf.summary.scalar('loss', loss)\r\n\r\n        with tf.Session(graph=g_inf) as sess:\r\n            # Call the eval rewrite which rewrites the graph in-place with\r\n            # FakeQuantization nodes and fold batchnorm for eval.\r\n            tf.contrib.quantize.experimental_create_eval_graph(weight_bits=W_B,\r\n                                                               activation_bits=A_B)\r\n\r\n            saver = tf.train.Saver()\r\n            saver.restore(sess, \"./tmp/oink.ckpt\")\r\n            # frozen_graphdef = tf.graph_util.convert_variables_to_constants(\r\n            #       sess, sess.graph_def, [\"out\"])\r\n            print('[debug] restored')\r\n\r\n            # re-initialize the iterator, but this time with training data\r\n            sess.run(data_init_op)\r\n\r\n            nn_out, nn_labels, nn_loss = sess.run([flow, labels, loss])\r\n\r\n        # debug\r\n        print('loss: ', nn_loss)\r\n        print('nn_out different values in BATCH_SIZE (32) elements:', len(np.unique(nn_out)))\r\n        print('nn_out [0:8]: ', nn_out[0:8])\r\n        print('nn_labels [0:8]: ', nn_labels[0:8])\r\n\r\ndef test_quantization():\r\n\r\n    train_quantization()\r\n    inference_quantization()\r\n\r\ntest_quantization()\r\n\r\n\r\n```\r\n"}