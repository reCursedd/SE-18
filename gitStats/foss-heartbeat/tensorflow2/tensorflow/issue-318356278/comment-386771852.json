{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/386771852", "html_url": "https://github.com/tensorflow/tensorflow/issues/18919#issuecomment-386771852", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18919", "id": 386771852, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Njc3MTg1Mg==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-05T02:04:32Z", "updated_at": "2018-05-05T02:05:59Z", "author_association": "MEMBER", "body_html": "<p>Note that the create_training_graph and create_eval_graph rewrite adds variables to the graph that store the quantization information needed by toco. So when you save a checkpoint from training, it will have these extra variables stored in it. Thus, when restoring that checkpoint into the eval graph, you need to restore <em>after</em> you call the rewrite.</p>\n<p>Think of the rewrite as part of graph construction in that you shouldn't run anything on the graph until the quantization rewrite has been called.</p>\n<p>In particular, your code should be updated like this:</p>\n<pre><code>    # Call the eval rewrite which rewrites the graph in-place with\n    # FakeQuantization nodes and fold batchnorm for eval.\n    tf.contrib.quantize.create_eval_graph()\n\n    saver.restore(sess, \"models/model.ckpt\")\n</code></pre>\n<p>Also check out this file for some instructions:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize\">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize</a></p>\n<p>That being said, the documents can likely be improved more and I will work to do that. Thanks for your feedback!</p>", "body_text": "Note that the create_training_graph and create_eval_graph rewrite adds variables to the graph that store the quantization information needed by toco. So when you save a checkpoint from training, it will have these extra variables stored in it. Thus, when restoring that checkpoint into the eval graph, you need to restore after you call the rewrite.\nThink of the rewrite as part of graph construction in that you shouldn't run anything on the graph until the quantization rewrite has been called.\nIn particular, your code should be updated like this:\n    # Call the eval rewrite which rewrites the graph in-place with\n    # FakeQuantization nodes and fold batchnorm for eval.\n    tf.contrib.quantize.create_eval_graph()\n\n    saver.restore(sess, \"models/model.ckpt\")\n\nAlso check out this file for some instructions:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize\nThat being said, the documents can likely be improved more and I will work to do that. Thanks for your feedback!", "body": "Note that the create_training_graph and create_eval_graph rewrite adds variables to the graph that store the quantization information needed by toco. So when you save a checkpoint from training, it will have these extra variables stored in it. Thus, when restoring that checkpoint into the eval graph, you need to restore *after* you call the rewrite.\r\n\r\nThink of the rewrite as part of graph construction in that you shouldn't run anything on the graph until the quantization rewrite has been called.\r\n\r\nIn particular, your code should be updated like this: \r\n\r\n\r\n        # Call the eval rewrite which rewrites the graph in-place with\r\n        # FakeQuantization nodes and fold batchnorm for eval.\r\n        tf.contrib.quantize.create_eval_graph()\r\n\r\n        saver.restore(sess, \"models/model.ckpt\")\r\n\r\nAlso check out this file for some instructions: \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize \r\n\r\nThat being said, the documents can likely be improved more and I will work to do that. Thanks for your feedback!"}