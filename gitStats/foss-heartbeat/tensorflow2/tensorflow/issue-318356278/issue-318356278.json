{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18919", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18919/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18919/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18919/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18919", "id": 318356278, "node_id": "MDU6SXNzdWUzMTgzNTYyNzg=", "number": 18919, "title": "Tf lite: Array activation1, which is an input to the Div operator producing the output array dropout/div, is lacking min/max data", "user": {"login": "mjuvilla", "id": 8360740, "node_id": "MDQ6VXNlcjgzNjA3NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8360740?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mjuvilla", "html_url": "https://github.com/mjuvilla", "followers_url": "https://api.github.com/users/mjuvilla/followers", "following_url": "https://api.github.com/users/mjuvilla/following{/other_user}", "gists_url": "https://api.github.com/users/mjuvilla/gists{/gist_id}", "starred_url": "https://api.github.com/users/mjuvilla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mjuvilla/subscriptions", "organizations_url": "https://api.github.com/users/mjuvilla/orgs", "repos_url": "https://api.github.com/users/mjuvilla/repos", "events_url": "https://api.github.com/users/mjuvilla/events{/privacy}", "received_events_url": "https://api.github.com/users/mjuvilla/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 19, "created_at": "2018-04-27T10:22:13Z", "updated_at": "2018-08-02T22:41:42Z", "closed_at": "2018-08-02T22:41:42Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS X High Sierra</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0rc1</li>\n<li><strong>Python version</strong>: 3.5</li>\n</ul>\n<p>I'm trying to save a very simple NN model to tflite format, with weight quantization, following this documentation: <a href=\"https://www.tensorflow.org/performance/quantization\" rel=\"nofollow\">https://www.tensorflow.org/performance/quantization</a>.</p>\n<p>However, when converting with toco, I get this error:</p>\n<p>Array Relu, which is an input to the Div operator producing the output array dropout/div, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\\n\"</p>\n<p>This is the graph:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8360740/39358075-28e18702-4a15-11e8-82de-8f2d705b1f78.png\"><img src=\"https://user-images.githubusercontent.com/8360740/39358075-28e18702-4a15-11e8-82de-8f2d705b1f78.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>The code to reproduce:</p>\n<pre><code>    inputs = tf.placeholder(tf.float32, shape=(1, train_X.shape[1]), name='inputs')\n    label = tf.placeholder(tf.float32, shape=(1, num_classes), name='labels')\n\n    # First layer\n    hid1_size = 128\n    w1 = tf.Variable(tf.random_normal([hid1_size, train_X.shape[1]], stddev=0.01), name='w1')\n    b1 = tf.Variable(tf.constant(0.1, shape=(hid1_size, 1)), name='b1')\n    y1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(w1, tf.transpose(inputs)), b1, name=\"layer1\"), \"activation1\"),\n                       keep_prob=0.5)\n\n    # Second layer\n    hid2_size = 256\n    w2 = tf.Variable(tf.random_normal([hid2_size, hid1_size], stddev=0.01), name='w2')\n    b2 = tf.Variable(tf.constant(0.1, shape=(hid2_size, 1)), name='b2')\n    y2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(w2, y1), b2, name=\"layer2\"), name=\"activation2\"), keep_prob=0.5)\n\n    # Output layer\n    wo = tf.Variable(tf.random_normal([num_classes, hid2_size], stddev=0.01), name='wo')\n    bo = tf.Variable(tf.random_normal([num_classes, 1]), name='bo')\n    yo = tf.transpose(tf.add(tf.matmul(wo, y2), bo), name=\"logits\")\n\n    # Loss function and optimizer\n    lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=yo, labels=label), name=\"loss\")\n\n    # Call the training rewrite which rewrites the graph in-place with\n    # FakeQuantization nodes and folds batchnorm for training. It is\n    # often needed to fine tune a floating point model for quantization\n    # with this training tool. When training from scratch, quant_delay\n    # can be used to activate quantization after training to converge\n    # with the float graph, effectively fine-tuning the model.\n    #tf.contrib.quantize.create_training_graph(quant_delay=3)\n\n    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n\n    # Prediction\n    pred = tf.nn.softmax(yo, name=\"prediction\")\n    pred_label = tf.argmax(pred, 1)\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(label, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n    # Create operation which will initialize all variables\n    init = tf.global_variables_initializer()\n\n    # Configure GPU not to use all memory\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    # Start a new tensorflow session and initialize variables\n    sess = tf.InteractiveSession(config=config)\n\n    writer = tf.summary.FileWriter(\"tensorboard\", sess.graph)\n\n    sess.run(init)\n\n    # This is the main training loop: we train for 50 epochs with a learning rate of 0.05 and another\n    # 50 epochs with a smaller learning rate of 0.01\n    for learning_rate in [0.05, 0.01]:\n        for epoch in range(50):\n            avg_cost = 0.0\n\n            # For each epoch, we go through all the samples we have.\n            for i in range(train_X.shape[0]):\n                # Finally, this is where the magic happens: run our optimizer, feed the current example into X and the current target into Y\n                _, c = sess.run([optimizer, loss], feed_dict={lr: learning_rate,\n                                                              inputs: train_X[i, None],\n                                                              label: train_y[i, None]})\n                avg_cost += c\n            avg_cost /= train_X.shape[0]\n\n            # Print the cost in this epcho to the console.\n            if epoch % 10 == 0:\n                print(\"Epoch: {:3d}    Train Cost: {:.4f}\".format(epoch, avg_cost))\n\n    # Call the eval rewrite which rewrites the graph in-place with\n    # FakeQuantization nodes and fold batchnorm for eval.\n    #tf.contrib.quantize.create_eval_graph()\n\n    writer.close()\n\n    graph_path = os.path.join(\"models\", \"model.pbtxt\")\n    checkpoint_path = os.path.join(\"models\", \"model.ckpt\")\n\n    # Save the checkpoint and eval graph proto to disk for freezing and providing to TFLite.\n    with open(graph_path, 'w') as f:\n        f.write(str(sess.graph.as_graph_def()))\n\n    saver = tf.train.Saver()\n    saver.save(sess, checkpoint_path)\n\n    frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n        sess, sess.graph_def, [\"prediction\"])\n    open(\"models/frozen_model.pb\", \"w\").write(str(frozen_graphdef))\n\n    tflite_model = tf.contrib.lite.toco_convert(\n        frozen_graphdef, [inputs], [pred], inference_type=tf.contrib.lite.QUANTIZED_UINT8,\n        quantized_input_stats=[(127.5, 127.5)])\n    open(\"models/converted_model.tflite\", \"wb\").write(tflite_model)\n</code></pre>\n<p>Not sure if this is the problem, but could it be that the quantization scripts (quantize.create_training_graph quantize.create_eval_graph) are not detecting the first layer, not fake quantizing it and for this reason I get an error at activation1 when converting?</p>", "body_text": "System information\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS X High Sierra\nTensorFlow version (use command below): 1.8.0rc1\nPython version: 3.5\n\nI'm trying to save a very simple NN model to tflite format, with weight quantization, following this documentation: https://www.tensorflow.org/performance/quantization.\nHowever, when converting with toco, I get this error:\nArray Relu, which is an input to the Div operator producing the output array dropout/div, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\\n\"\nThis is the graph:\n\nThe code to reproduce:\n    inputs = tf.placeholder(tf.float32, shape=(1, train_X.shape[1]), name='inputs')\n    label = tf.placeholder(tf.float32, shape=(1, num_classes), name='labels')\n\n    # First layer\n    hid1_size = 128\n    w1 = tf.Variable(tf.random_normal([hid1_size, train_X.shape[1]], stddev=0.01), name='w1')\n    b1 = tf.Variable(tf.constant(0.1, shape=(hid1_size, 1)), name='b1')\n    y1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(w1, tf.transpose(inputs)), b1, name=\"layer1\"), \"activation1\"),\n                       keep_prob=0.5)\n\n    # Second layer\n    hid2_size = 256\n    w2 = tf.Variable(tf.random_normal([hid2_size, hid1_size], stddev=0.01), name='w2')\n    b2 = tf.Variable(tf.constant(0.1, shape=(hid2_size, 1)), name='b2')\n    y2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(w2, y1), b2, name=\"layer2\"), name=\"activation2\"), keep_prob=0.5)\n\n    # Output layer\n    wo = tf.Variable(tf.random_normal([num_classes, hid2_size], stddev=0.01), name='wo')\n    bo = tf.Variable(tf.random_normal([num_classes, 1]), name='bo')\n    yo = tf.transpose(tf.add(tf.matmul(wo, y2), bo), name=\"logits\")\n\n    # Loss function and optimizer\n    lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=yo, labels=label), name=\"loss\")\n\n    # Call the training rewrite which rewrites the graph in-place with\n    # FakeQuantization nodes and folds batchnorm for training. It is\n    # often needed to fine tune a floating point model for quantization\n    # with this training tool. When training from scratch, quant_delay\n    # can be used to activate quantization after training to converge\n    # with the float graph, effectively fine-tuning the model.\n    #tf.contrib.quantize.create_training_graph(quant_delay=3)\n\n    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n\n    # Prediction\n    pred = tf.nn.softmax(yo, name=\"prediction\")\n    pred_label = tf.argmax(pred, 1)\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(label, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n    # Create operation which will initialize all variables\n    init = tf.global_variables_initializer()\n\n    # Configure GPU not to use all memory\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    # Start a new tensorflow session and initialize variables\n    sess = tf.InteractiveSession(config=config)\n\n    writer = tf.summary.FileWriter(\"tensorboard\", sess.graph)\n\n    sess.run(init)\n\n    # This is the main training loop: we train for 50 epochs with a learning rate of 0.05 and another\n    # 50 epochs with a smaller learning rate of 0.01\n    for learning_rate in [0.05, 0.01]:\n        for epoch in range(50):\n            avg_cost = 0.0\n\n            # For each epoch, we go through all the samples we have.\n            for i in range(train_X.shape[0]):\n                # Finally, this is where the magic happens: run our optimizer, feed the current example into X and the current target into Y\n                _, c = sess.run([optimizer, loss], feed_dict={lr: learning_rate,\n                                                              inputs: train_X[i, None],\n                                                              label: train_y[i, None]})\n                avg_cost += c\n            avg_cost /= train_X.shape[0]\n\n            # Print the cost in this epcho to the console.\n            if epoch % 10 == 0:\n                print(\"Epoch: {:3d}    Train Cost: {:.4f}\".format(epoch, avg_cost))\n\n    # Call the eval rewrite which rewrites the graph in-place with\n    # FakeQuantization nodes and fold batchnorm for eval.\n    #tf.contrib.quantize.create_eval_graph()\n\n    writer.close()\n\n    graph_path = os.path.join(\"models\", \"model.pbtxt\")\n    checkpoint_path = os.path.join(\"models\", \"model.ckpt\")\n\n    # Save the checkpoint and eval graph proto to disk for freezing and providing to TFLite.\n    with open(graph_path, 'w') as f:\n        f.write(str(sess.graph.as_graph_def()))\n\n    saver = tf.train.Saver()\n    saver.save(sess, checkpoint_path)\n\n    frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n        sess, sess.graph_def, [\"prediction\"])\n    open(\"models/frozen_model.pb\", \"w\").write(str(frozen_graphdef))\n\n    tflite_model = tf.contrib.lite.toco_convert(\n        frozen_graphdef, [inputs], [pred], inference_type=tf.contrib.lite.QUANTIZED_UINT8,\n        quantized_input_stats=[(127.5, 127.5)])\n    open(\"models/converted_model.tflite\", \"wb\").write(tflite_model)\n\nNot sure if this is the problem, but could it be that the quantization scripts (quantize.create_training_graph quantize.create_eval_graph) are not detecting the first layer, not fake quantizing it and for this reason I get an error at activation1 when converting?", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS X High Sierra\r\n- **TensorFlow version (use command below)**: 1.8.0rc1\r\n- **Python version**: 3.5\r\n\r\nI'm trying to save a very simple NN model to tflite format, with weight quantization, following this documentation: https://www.tensorflow.org/performance/quantization.\r\n\r\nHowever, when converting with toco, I get this error:\r\n\r\nArray Relu, which is an input to the Div operator producing the output array dropout/div, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\\n\"\r\n\r\nThis is the graph:\r\n\r\n![image](https://user-images.githubusercontent.com/8360740/39358075-28e18702-4a15-11e8-82de-8f2d705b1f78.png)\r\n\r\nThe code to reproduce:\r\n\r\n```\r\n    inputs = tf.placeholder(tf.float32, shape=(1, train_X.shape[1]), name='inputs')\r\n    label = tf.placeholder(tf.float32, shape=(1, num_classes), name='labels')\r\n\r\n    # First layer\r\n    hid1_size = 128\r\n    w1 = tf.Variable(tf.random_normal([hid1_size, train_X.shape[1]], stddev=0.01), name='w1')\r\n    b1 = tf.Variable(tf.constant(0.1, shape=(hid1_size, 1)), name='b1')\r\n    y1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(w1, tf.transpose(inputs)), b1, name=\"layer1\"), \"activation1\"),\r\n                       keep_prob=0.5)\r\n\r\n    # Second layer\r\n    hid2_size = 256\r\n    w2 = tf.Variable(tf.random_normal([hid2_size, hid1_size], stddev=0.01), name='w2')\r\n    b2 = tf.Variable(tf.constant(0.1, shape=(hid2_size, 1)), name='b2')\r\n    y2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(w2, y1), b2, name=\"layer2\"), name=\"activation2\"), keep_prob=0.5)\r\n\r\n    # Output layer\r\n    wo = tf.Variable(tf.random_normal([num_classes, hid2_size], stddev=0.01), name='wo')\r\n    bo = tf.Variable(tf.random_normal([num_classes, 1]), name='bo')\r\n    yo = tf.transpose(tf.add(tf.matmul(wo, y2), bo), name=\"logits\")\r\n\r\n    # Loss function and optimizer\r\n    lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\r\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=yo, labels=label), name=\"loss\")\r\n\r\n    # Call the training rewrite which rewrites the graph in-place with\r\n    # FakeQuantization nodes and folds batchnorm for training. It is\r\n    # often needed to fine tune a floating point model for quantization\r\n    # with this training tool. When training from scratch, quant_delay\r\n    # can be used to activate quantization after training to converge\r\n    # with the float graph, effectively fine-tuning the model.\r\n    #tf.contrib.quantize.create_training_graph(quant_delay=3)\r\n\r\n    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\r\n\r\n    # Prediction\r\n    pred = tf.nn.softmax(yo, name=\"prediction\")\r\n    pred_label = tf.argmax(pred, 1)\r\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(label, 1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n\r\n    # Create operation which will initialize all variables\r\n    init = tf.global_variables_initializer()\r\n\r\n    # Configure GPU not to use all memory\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n\r\n    # Start a new tensorflow session and initialize variables\r\n    sess = tf.InteractiveSession(config=config)\r\n\r\n    writer = tf.summary.FileWriter(\"tensorboard\", sess.graph)\r\n\r\n    sess.run(init)\r\n\r\n    # This is the main training loop: we train for 50 epochs with a learning rate of 0.05 and another\r\n    # 50 epochs with a smaller learning rate of 0.01\r\n    for learning_rate in [0.05, 0.01]:\r\n        for epoch in range(50):\r\n            avg_cost = 0.0\r\n\r\n            # For each epoch, we go through all the samples we have.\r\n            for i in range(train_X.shape[0]):\r\n                # Finally, this is where the magic happens: run our optimizer, feed the current example into X and the current target into Y\r\n                _, c = sess.run([optimizer, loss], feed_dict={lr: learning_rate,\r\n                                                              inputs: train_X[i, None],\r\n                                                              label: train_y[i, None]})\r\n                avg_cost += c\r\n            avg_cost /= train_X.shape[0]\r\n\r\n            # Print the cost in this epcho to the console.\r\n            if epoch % 10 == 0:\r\n                print(\"Epoch: {:3d}    Train Cost: {:.4f}\".format(epoch, avg_cost))\r\n\r\n    # Call the eval rewrite which rewrites the graph in-place with\r\n    # FakeQuantization nodes and fold batchnorm for eval.\r\n    #tf.contrib.quantize.create_eval_graph()\r\n\r\n    writer.close()\r\n\r\n    graph_path = os.path.join(\"models\", \"model.pbtxt\")\r\n    checkpoint_path = os.path.join(\"models\", \"model.ckpt\")\r\n\r\n    # Save the checkpoint and eval graph proto to disk for freezing and providing to TFLite.\r\n    with open(graph_path, 'w') as f:\r\n        f.write(str(sess.graph.as_graph_def()))\r\n\r\n    saver = tf.train.Saver()\r\n    saver.save(sess, checkpoint_path)\r\n\r\n    frozen_graphdef = tf.graph_util.convert_variables_to_constants(\r\n        sess, sess.graph_def, [\"prediction\"])\r\n    open(\"models/frozen_model.pb\", \"w\").write(str(frozen_graphdef))\r\n\r\n    tflite_model = tf.contrib.lite.toco_convert(\r\n        frozen_graphdef, [inputs], [pred], inference_type=tf.contrib.lite.QUANTIZED_UINT8,\r\n        quantized_input_stats=[(127.5, 127.5)])\r\n    open(\"models/converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nNot sure if this is the problem, but could it be that the quantization scripts (quantize.create_training_graph quantize.create_eval_graph) are not detecting the first layer, not fake quantizing it and for this reason I get an error at activation1 when converting?"}