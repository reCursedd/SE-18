{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/387827985", "html_url": "https://github.com/tensorflow/tensorflow/issues/18919#issuecomment-387827985", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18919", "id": 387827985, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzgyNzk4NQ==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T18:13:16Z", "updated_at": "2018-05-09T18:13:16Z", "author_association": "MEMBER", "body_html": "<p>Using your code I am able to successfully get a frozen graph. Here is the code I used (copy pasted from your snippets):</p>\n<pre><code>def train_model(num_feat, num_classes):\n  g = tf.Graph()\n  with tf.Session(graph=g) as sess:\n    inputs = tf.placeholder(tf.float32, shape=(1, num_feat), name=\"inputs\")\n    label = tf.placeholder(tf.float32, shape=(1, num_classes), name='labels')\n\n    # First layer\n    hid1_size = 1000\n    w1 = tf.Variable(tf.random_normal([num_feat, hid1_size], stddev=0.01), name='w1')\n    b1 = tf.Variable(tf.constant(0.1, shape=(1, hid1_size)), name='b1')\n    y1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(inputs, w1), b1, name=\"layer1\"), \"activation1\"), keep_prob=0.5)\n\n    # Second layer\n    hid2_size = 1000\n    w2 = tf.Variable(tf.random_normal([hid1_size, hid2_size], stddev=0.01), name='w2')\n    b2 = tf.Variable(tf.constant(0.1, shape=(1, hid2_size)), name='b2')\n    y2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(y1, w2), b2, name=\"layer2\"), name=\"activation2\"), keep_prob=0.5)\n\n    # Output layer\n    wo = tf.Variable(tf.random_normal([hid2_size, num_classes], stddev=0.01), name='wo')\n    bo = tf.Variable(tf.random_normal([1, num_classes]), name='bo')\n    yo = tf.add(tf.matmul(y2, wo), bo, name=\"logits\")\n\n    # Loss function and optimizer\n    # lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=yo, labels=label), name=\"loss\")\n\n    # Call the training rewrite which rewrites the graph in-place with\n    # FakeQuantization nodes and folds batchnorm for training. It is\n    # often needed to fine tune a floating point model for quantization\n    # with this training tool. When training from scratch, quant_delay\n    # can be used to activate quantization after training to converge\n    # with the float graph, effectively fine-tuning the model.\n    tf.contrib.quantize.create_training_graph(quant_delay=50)\n\n    optimizer = tf.train.AdamOptimizer().minimize(loss)\n\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    saver = tf.train.Saver()\n    saver.save(sess, '/tmp/oink.ckpt')\n\n\ndef eval_model(num_feat, num_classes):\n  g = tf.Graph()\n  with tf.Session(graph=g) as sess:\n    inputs = tf.placeholder(tf.float32, shape=(1, num_feat), name='inputs')\n\n    # First layer\n    hid1_size = 1000\n    w1 = tf.Variable(tf.random_normal([num_feat, hid1_size], stddev=0.01), name='w1')\n    b1 = tf.Variable(tf.constant(0.1, shape=(1, hid1_size)), name='b1')\n    y1 = tf.nn.relu(tf.add(tf.matmul(inputs, w1), b1, name=\"layer1\"), \"activation1\")\n\n    # Second layer\n    hid2_size = 1000\n    w2 = tf.Variable(tf.random_normal([hid1_size, hid2_size], stddev=0.01), name='w2')\n    b2 = tf.Variable(tf.constant(0.1, shape=(1, hid2_size)), name='b2')\n    y2 = tf.nn.relu(tf.add(tf.matmul(y1, w2), b2, name=\"layer2\"), name=\"activation2\")\n\n    # Output layer\n    wo = tf.Variable(tf.random_normal([hid2_size, num_classes], stddev=0.01), name='wo')\n    bo = tf.Variable(tf.random_normal([1, num_classes]), name='bo')\n    yo = tf.add(tf.matmul(y2, wo), bo, name=\"logits\")\n\n    pred = tf.nn.softmax(yo, name=\"prediction\")\n\n    tf.contrib.quantize.create_eval_graph()\n\n    saver = tf.Saver()\n    saver.restore(sess, \"/tmp/oink.ckpt\")\n\n    frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n          sess, sess.graph_def, [\"prediction\"])\n\n\ndef main(_):\n  train_model(3, 1000)\n  eval_model(3, 1000)\n</code></pre>\n<p>In particular make sure you delete your old checkpoints from before you swapped the order of the params, the above code seems to work for me. Hope that works! (I have a fix in progress for the ordering issue.)</p>", "body_text": "Using your code I am able to successfully get a frozen graph. Here is the code I used (copy pasted from your snippets):\ndef train_model(num_feat, num_classes):\n  g = tf.Graph()\n  with tf.Session(graph=g) as sess:\n    inputs = tf.placeholder(tf.float32, shape=(1, num_feat), name=\"inputs\")\n    label = tf.placeholder(tf.float32, shape=(1, num_classes), name='labels')\n\n    # First layer\n    hid1_size = 1000\n    w1 = tf.Variable(tf.random_normal([num_feat, hid1_size], stddev=0.01), name='w1')\n    b1 = tf.Variable(tf.constant(0.1, shape=(1, hid1_size)), name='b1')\n    y1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(inputs, w1), b1, name=\"layer1\"), \"activation1\"), keep_prob=0.5)\n\n    # Second layer\n    hid2_size = 1000\n    w2 = tf.Variable(tf.random_normal([hid1_size, hid2_size], stddev=0.01), name='w2')\n    b2 = tf.Variable(tf.constant(0.1, shape=(1, hid2_size)), name='b2')\n    y2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(y1, w2), b2, name=\"layer2\"), name=\"activation2\"), keep_prob=0.5)\n\n    # Output layer\n    wo = tf.Variable(tf.random_normal([hid2_size, num_classes], stddev=0.01), name='wo')\n    bo = tf.Variable(tf.random_normal([1, num_classes]), name='bo')\n    yo = tf.add(tf.matmul(y2, wo), bo, name=\"logits\")\n\n    # Loss function and optimizer\n    # lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=yo, labels=label), name=\"loss\")\n\n    # Call the training rewrite which rewrites the graph in-place with\n    # FakeQuantization nodes and folds batchnorm for training. It is\n    # often needed to fine tune a floating point model for quantization\n    # with this training tool. When training from scratch, quant_delay\n    # can be used to activate quantization after training to converge\n    # with the float graph, effectively fine-tuning the model.\n    tf.contrib.quantize.create_training_graph(quant_delay=50)\n\n    optimizer = tf.train.AdamOptimizer().minimize(loss)\n\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    saver = tf.train.Saver()\n    saver.save(sess, '/tmp/oink.ckpt')\n\n\ndef eval_model(num_feat, num_classes):\n  g = tf.Graph()\n  with tf.Session(graph=g) as sess:\n    inputs = tf.placeholder(tf.float32, shape=(1, num_feat), name='inputs')\n\n    # First layer\n    hid1_size = 1000\n    w1 = tf.Variable(tf.random_normal([num_feat, hid1_size], stddev=0.01), name='w1')\n    b1 = tf.Variable(tf.constant(0.1, shape=(1, hid1_size)), name='b1')\n    y1 = tf.nn.relu(tf.add(tf.matmul(inputs, w1), b1, name=\"layer1\"), \"activation1\")\n\n    # Second layer\n    hid2_size = 1000\n    w2 = tf.Variable(tf.random_normal([hid1_size, hid2_size], stddev=0.01), name='w2')\n    b2 = tf.Variable(tf.constant(0.1, shape=(1, hid2_size)), name='b2')\n    y2 = tf.nn.relu(tf.add(tf.matmul(y1, w2), b2, name=\"layer2\"), name=\"activation2\")\n\n    # Output layer\n    wo = tf.Variable(tf.random_normal([hid2_size, num_classes], stddev=0.01), name='wo')\n    bo = tf.Variable(tf.random_normal([1, num_classes]), name='bo')\n    yo = tf.add(tf.matmul(y2, wo), bo, name=\"logits\")\n\n    pred = tf.nn.softmax(yo, name=\"prediction\")\n\n    tf.contrib.quantize.create_eval_graph()\n\n    saver = tf.Saver()\n    saver.restore(sess, \"/tmp/oink.ckpt\")\n\n    frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n          sess, sess.graph_def, [\"prediction\"])\n\n\ndef main(_):\n  train_model(3, 1000)\n  eval_model(3, 1000)\n\nIn particular make sure you delete your old checkpoints from before you swapped the order of the params, the above code seems to work for me. Hope that works! (I have a fix in progress for the ordering issue.)", "body": "Using your code I am able to successfully get a frozen graph. Here is the code I used (copy pasted from your snippets): \r\n\r\n```\r\ndef train_model(num_feat, num_classes):\r\n  g = tf.Graph()\r\n  with tf.Session(graph=g) as sess:\r\n    inputs = tf.placeholder(tf.float32, shape=(1, num_feat), name=\"inputs\")\r\n    label = tf.placeholder(tf.float32, shape=(1, num_classes), name='labels')\r\n\r\n    # First layer\r\n    hid1_size = 1000\r\n    w1 = tf.Variable(tf.random_normal([num_feat, hid1_size], stddev=0.01), name='w1')\r\n    b1 = tf.Variable(tf.constant(0.1, shape=(1, hid1_size)), name='b1')\r\n    y1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(inputs, w1), b1, name=\"layer1\"), \"activation1\"), keep_prob=0.5)\r\n\r\n    # Second layer\r\n    hid2_size = 1000\r\n    w2 = tf.Variable(tf.random_normal([hid1_size, hid2_size], stddev=0.01), name='w2')\r\n    b2 = tf.Variable(tf.constant(0.1, shape=(1, hid2_size)), name='b2')\r\n    y2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(y1, w2), b2, name=\"layer2\"), name=\"activation2\"), keep_prob=0.5)\r\n\r\n    # Output layer\r\n    wo = tf.Variable(tf.random_normal([hid2_size, num_classes], stddev=0.01), name='wo')\r\n    bo = tf.Variable(tf.random_normal([1, num_classes]), name='bo')\r\n    yo = tf.add(tf.matmul(y2, wo), bo, name=\"logits\")\r\n\r\n    # Loss function and optimizer\r\n    # lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\r\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=yo, labels=label), name=\"loss\")\r\n\r\n    # Call the training rewrite which rewrites the graph in-place with\r\n    # FakeQuantization nodes and folds batchnorm for training. It is\r\n    # often needed to fine tune a floating point model for quantization\r\n    # with this training tool. When training from scratch, quant_delay\r\n    # can be used to activate quantization after training to converge\r\n    # with the float graph, effectively fine-tuning the model.\r\n    tf.contrib.quantize.create_training_graph(quant_delay=50)\r\n\r\n    optimizer = tf.train.AdamOptimizer().minimize(loss)\r\n\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    saver = tf.train.Saver()\r\n    saver.save(sess, '/tmp/oink.ckpt')\r\n\r\n\r\ndef eval_model(num_feat, num_classes):\r\n  g = tf.Graph()\r\n  with tf.Session(graph=g) as sess:\r\n    inputs = tf.placeholder(tf.float32, shape=(1, num_feat), name='inputs')\r\n\r\n    # First layer\r\n    hid1_size = 1000\r\n    w1 = tf.Variable(tf.random_normal([num_feat, hid1_size], stddev=0.01), name='w1')\r\n    b1 = tf.Variable(tf.constant(0.1, shape=(1, hid1_size)), name='b1')\r\n    y1 = tf.nn.relu(tf.add(tf.matmul(inputs, w1), b1, name=\"layer1\"), \"activation1\")\r\n\r\n    # Second layer\r\n    hid2_size = 1000\r\n    w2 = tf.Variable(tf.random_normal([hid1_size, hid2_size], stddev=0.01), name='w2')\r\n    b2 = tf.Variable(tf.constant(0.1, shape=(1, hid2_size)), name='b2')\r\n    y2 = tf.nn.relu(tf.add(tf.matmul(y1, w2), b2, name=\"layer2\"), name=\"activation2\")\r\n\r\n    # Output layer\r\n    wo = tf.Variable(tf.random_normal([hid2_size, num_classes], stddev=0.01), name='wo')\r\n    bo = tf.Variable(tf.random_normal([1, num_classes]), name='bo')\r\n    yo = tf.add(tf.matmul(y2, wo), bo, name=\"logits\")\r\n\r\n    pred = tf.nn.softmax(yo, name=\"prediction\")\r\n\r\n    tf.contrib.quantize.create_eval_graph()\r\n\r\n    saver = tf.Saver()\r\n    saver.restore(sess, \"/tmp/oink.ckpt\")\r\n\r\n    frozen_graphdef = tf.graph_util.convert_variables_to_constants(\r\n          sess, sess.graph_def, [\"prediction\"])\r\n\r\n\r\ndef main(_):\r\n  train_model(3, 1000)\r\n  eval_model(3, 1000)\r\n```\r\n\r\nIn particular make sure you delete your old checkpoints from before you swapped the order of the params, the above code seems to work for me. Hope that works! (I have a fix in progress for the ordering issue.)"}