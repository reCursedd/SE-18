{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9862", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9862/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9862/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9862/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9862", "id": 228329227, "node_id": "MDU6SXNzdWUyMjgzMjkyMjc=", "number": 9862, "title": "output of tf.nn.convolution is not consistent for bigger 3d nets", "user": {"login": "abhijithchunduru", "id": 4212999, "node_id": "MDQ6VXNlcjQyMTI5OTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4212999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhijithchunduru", "html_url": "https://github.com/abhijithchunduru", "followers_url": "https://api.github.com/users/abhijithchunduru/followers", "following_url": "https://api.github.com/users/abhijithchunduru/following{/other_user}", "gists_url": "https://api.github.com/users/abhijithchunduru/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhijithchunduru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhijithchunduru/subscriptions", "organizations_url": "https://api.github.com/users/abhijithchunduru/orgs", "repos_url": "https://api.github.com/users/abhijithchunduru/repos", "events_url": "https://api.github.com/users/abhijithchunduru/events{/privacy}", "received_events_url": "https://api.github.com/users/abhijithchunduru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-12T15:35:11Z", "updated_at": "2017-05-12T20:00:26Z", "closed_at": "2017-05-12T19:59:31Z", "author_association": "NONE", "body_html": "<p>Code :</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">Conv3D</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">filter_shape</span>, <span class=\"pl-smi\">stride</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">padding</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>, <span class=\"pl-smi\">dilation_rate</span> <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>)):\n      W <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weights<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span> <span class=\"pl-k\">=</span> filter_shape, <span class=\"pl-v\">initializer</span> <span class=\"pl-k\">=</span> tf.contrib.layers.xavier_initializer())\n      b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-v\">name</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span> <span class=\"pl-k\">=</span> filter_shape[<span class=\"pl-c1\">4</span>], <span class=\"pl-v\">initializer</span> <span class=\"pl-k\">=</span> tf.constant_initializer(<span class=\"pl-c1\">0.1</span>))\n      conv_out <span class=\"pl-k\">=</span> tf.nn.convolution(x, W, b, <span class=\"pl-v\">padding</span> <span class=\"pl-k\">=</span> padding, <span class=\"pl-v\">strides</span> <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> dilation_rate)\n     ret_val <span class=\"pl-k\">=</span> tf.nn.bias_add(conv_out,b)\n     <span class=\"pl-k\">return</span> ret_val\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">MaxPool3D</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">dilation_rate</span> <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>)):\n      ret_val <span class=\"pl-k\">=</span> tf.nn.pool(x, <span class=\"pl-v\">window_shape</span> <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">pooling_type</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>MAX<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">padding</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> dilation_rate, <span class=\"pl-v\">strides</span> <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>])\n      <span class=\"pl-k\">return</span> ret_val\n\n\nlayer <span class=\"pl-k\">=</span> {}\ndilation <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\ninputs <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">80</span>, <span class=\"pl-c1\">80</span>, <span class=\"pl-c1\">80</span>, <span class=\"pl-c1\">1</span>))\nlayer[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">=</span> Conv3D(inputs, [<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">128</span>], <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> (dilation, dilation, dilation))\nlayer[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">=</span> Conv3D(layer[<span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">128</span>,<span class=\"pl-c1\">64</span>], <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> (dilation, dilation, dilation))\nlayer[<span class=\"pl-c1\">3</span>] <span class=\"pl-k\">=</span> MaxPool3D(layer[<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> (dilation, dilation, dilation))\ndilation <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>dilation\nlayer[<span class=\"pl-c1\">4</span>] <span class=\"pl-k\">=</span> Conv3D(layer[<span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">128</span>], <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> (dilation, dilation, dilation))\nlayer[<span class=\"pl-c1\">5</span>] <span class=\"pl-k\">=</span> MaxPool3D(layer[<span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> (dilation, dilation, dilation))\ndilation <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>dilation\nlayer[<span class=\"pl-c1\">6</span>] <span class=\"pl-k\">=</span> Conv3D(layer[<span class=\"pl-c1\">5</span>], [<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>], <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> (dilation, dilation, dilation))\nlayer[<span class=\"pl-c1\">7</span>] <span class=\"pl-k\">=</span> MaxPool3D(layer[<span class=\"pl-c1\">6</span>], <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> (dilation, dilation, dilation))\ndilation <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>dilation\nlayer[<span class=\"pl-c1\">8</span>] <span class=\"pl-k\">=</span> Conv3D(layer[<span class=\"pl-c1\">7</span>], [<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">256</span>,<span class=\"pl-c1\">64</span>], <span class=\"pl-v\">dilation_rate</span> <span class=\"pl-k\">=</span> (dilation, dilation, dilation))\n\ninputs_np <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">80</span>,<span class=\"pl-c1\">80</span>,<span class=\"pl-c1\">80</span>,<span class=\"pl-c1\">1</span>)\nsess <span class=\"pl-k\">=</span> tf.Session()\nlogits1 <span class=\"pl-k\">=</span> sess.run(layer[<span class=\"pl-c1\">8</span>], <span class=\"pl-v\">feed_dict</span> <span class=\"pl-k\">=</span> {inputs : inputs_np})\nlogits1_duplicate <span class=\"pl-k\">=</span> sess.run(layer[<span class=\"pl-c1\">8</span>], <span class=\"pl-v\">feed_dict</span> <span class=\"pl-k\">=</span> {inputs : inputs_np})\n\nsum_diff <span class=\"pl-k\">=</span> np.sum(logits1 <span class=\"pl-k\">-</span> logits1_duplicate)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span> sum diff : <span class=\"pl-pds\">'</span></span>, sum_diff)\n</pre></div>\n<p>When I run this code, I get a non zero sum_diff. Why is the network's output not consistent with multiple runs ? I will appreciate some help</p>", "body_text": "Code :\nimport tensorflow as tf\ndef Conv3D(x, filter_shape, stride = 1, padding = 'VALID', dilation_rate = (1,1,1)):\n      W = tf.get_variable(name = 'weights', shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer())\n      b = tf.get_variable(name = 'bias', shape = filter_shape[4], initializer = tf.constant_initializer(0.1))\n      conv_out = tf.nn.convolution(x, W, b, padding = padding, strides = [1,1,1], dilation_rate = dilation_rate)\n     ret_val = tf.nn.bias_add(conv_out,b)\n     return ret_val\n\ndef MaxPool3D(x, dilation_rate = (1,1,1)):\n      ret_val = tf.nn.pool(x, window_shape = [2,2,2], pooling_type = 'MAX', padding = 'VALID', dilation_rate = dilation_rate, strides = [1,1,1])\n      return ret_val\n\n\nlayer = {}\ndilation = 1\ninputs = tf.placeholder(tf.float32, (None, 80, 80, 80, 1))\nlayer[1] = Conv3D(inputs, [3,3,3,1,128], dilation_rate = (dilation, dilation, dilation))\nlayer[2] = Conv3D(layer[1], [3,3,3,128,64], dilation_rate = (dilation, dilation, dilation))\nlayer[3] = MaxPool3D(layer[2], dilation_rate = (dilation, dilation, dilation))\ndilation = 2*dilation\nlayer[4] = Conv3D(layer[3], [3,3,3,64,128], dilation_rate = (dilation, dilation, dilation))\nlayer[5] = MaxPool3D(layer[4], dilation_rate = (dilation, dilation, dilation))\ndilation = 2*dilation\nlayer[6] = Conv3D(layer[5], [3,3,3,128, 256], dilation_rate = (dilation, dilation, dilation))\nlayer[7] = MaxPool3D(layer[6], dilation_rate = (dilation, dilation, dilation))\ndilation = 2*dilation\nlayer[8] = Conv3D(layer[7], [3,3,3,256,64], dilation_rate = (dilation, dilation, dilation))\n\ninputs_np = np.random.randn(1,80,80,80,1)\nsess = tf.Session()\nlogits1 = sess.run(layer[8], feed_dict = {inputs : inputs_np})\nlogits1_duplicate = sess.run(layer[8], feed_dict = {inputs : inputs_np})\n\nsum_diff = np.sum(logits1 - logits1_duplicate)\nprint(' sum diff : ', sum_diff)\n\nWhen I run this code, I get a non zero sum_diff. Why is the network's output not consistent with multiple runs ? I will appreciate some help", "body": "Code :\r\n```python\r\nimport tensorflow as tf\r\ndef Conv3D(x, filter_shape, stride = 1, padding = 'VALID', dilation_rate = (1,1,1)):\r\n      W = tf.get_variable(name = 'weights', shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer())\r\n      b = tf.get_variable(name = 'bias', shape = filter_shape[4], initializer = tf.constant_initializer(0.1))\r\n      conv_out = tf.nn.convolution(x, W, b, padding = padding, strides = [1,1,1], dilation_rate = dilation_rate)\r\n     ret_val = tf.nn.bias_add(conv_out,b)\r\n     return ret_val\r\n\r\ndef MaxPool3D(x, dilation_rate = (1,1,1)):\r\n      ret_val = tf.nn.pool(x, window_shape = [2,2,2], pooling_type = 'MAX', padding = 'VALID', dilation_rate = dilation_rate, strides = [1,1,1])\r\n      return ret_val\r\n\r\n\r\nlayer = {}\r\ndilation = 1\r\ninputs = tf.placeholder(tf.float32, (None, 80, 80, 80, 1))\r\nlayer[1] = Conv3D(inputs, [3,3,3,1,128], dilation_rate = (dilation, dilation, dilation))\r\nlayer[2] = Conv3D(layer[1], [3,3,3,128,64], dilation_rate = (dilation, dilation, dilation))\r\nlayer[3] = MaxPool3D(layer[2], dilation_rate = (dilation, dilation, dilation))\r\ndilation = 2*dilation\r\nlayer[4] = Conv3D(layer[3], [3,3,3,64,128], dilation_rate = (dilation, dilation, dilation))\r\nlayer[5] = MaxPool3D(layer[4], dilation_rate = (dilation, dilation, dilation))\r\ndilation = 2*dilation\r\nlayer[6] = Conv3D(layer[5], [3,3,3,128, 256], dilation_rate = (dilation, dilation, dilation))\r\nlayer[7] = MaxPool3D(layer[6], dilation_rate = (dilation, dilation, dilation))\r\ndilation = 2*dilation\r\nlayer[8] = Conv3D(layer[7], [3,3,3,256,64], dilation_rate = (dilation, dilation, dilation))\r\n\r\ninputs_np = np.random.randn(1,80,80,80,1)\r\nsess = tf.Session()\r\nlogits1 = sess.run(layer[8], feed_dict = {inputs : inputs_np})\r\nlogits1_duplicate = sess.run(layer[8], feed_dict = {inputs : inputs_np})\r\n\r\nsum_diff = np.sum(logits1 - logits1_duplicate)\r\nprint(' sum diff : ', sum_diff)\r\n\r\n```\r\nWhen I run this code, I get a non zero sum_diff. Why is the network's output not consistent with multiple runs ? I will appreciate some help"}