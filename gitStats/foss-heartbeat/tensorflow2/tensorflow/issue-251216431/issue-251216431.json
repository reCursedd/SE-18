{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12390", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12390/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12390/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12390/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12390", "id": 251216431, "node_id": "MDU6SXNzdWUyNTEyMTY0MzE=", "number": 12390, "title": "Running ops with restored model gives FailedPreconditionError", "user": {"login": "Aleexy", "id": 27819325, "node_id": "MDQ6VXNlcjI3ODE5MzI1", "avatar_url": "https://avatars2.githubusercontent.com/u/27819325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aleexy", "html_url": "https://github.com/Aleexy", "followers_url": "https://api.github.com/users/Aleexy/followers", "following_url": "https://api.github.com/users/Aleexy/following{/other_user}", "gists_url": "https://api.github.com/users/Aleexy/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aleexy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aleexy/subscriptions", "organizations_url": "https://api.github.com/users/Aleexy/orgs", "repos_url": "https://api.github.com/users/Aleexy/repos", "events_url": "https://api.github.com/users/Aleexy/events{/privacy}", "received_events_url": "https://api.github.com/users/Aleexy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-18T11:24:25Z", "updated_at": "2017-08-18T17:06:27Z", "closed_at": "2017-08-18T17:06:27Z", "author_association": "NONE", "body_html": "<p>I'm trying running some ops with a pre-trained model and the model was restored successfully. When running some ops that depend on one node in the graph, TF returns me</p>\n<pre><code>FailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_213\n\t [[Node: Variable_213/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_213\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_213)]]\n</code></pre>\n<p>It might be that I missed something , but this didn't show up before I updated my Tensorflow to 1.2. I'm running TF on MacOS X, Python 2.7. Below is the source code:</p>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf\nimport csv\nimport os\nimport glob\nfrom PIL import Image\nfrom datetime import datetime\n\ndef read_image_file(filename):\n    image = tf.image.decode_png(tf.read_file(filename), channels=3)\n    image = tf.cast(image, tf.float32)\n    image = tf.reshape(image, [224, 112, 3])\n    return image\n\ndef labelFiles(filename):\n    label_list = []\n    for name in filename:\n        label_list.append(getLabel(name))\n    return label_list\n\ndef readCSV():\n    with open('label.csv', 'r') as infile:\n        reader = csv.reader(infile)\n        label = {rows[0]:rows[1] for rows in reader}\n        return label\n\ndef getWord(filename):\n    word = ''\n    (name, extension) = os.path.splitext(filename)\n    (path, name) = os.path.split(name)\n    for i in name:\n        if i.isalpha():\n            start = name.index(i)\n            word = word + name[start:]\n            #print(word)\n            break\n    return word\n\ndef getLabel(filename):\n    label = [0]*len(labels)\n    index =  int(labels.get(getWord(str(filename)), 0))\n    label[index] = 1\n    label = tf.constant(label, dtype = tf.float32)\n    tf.reshape(label, [-1])\n    print(label)\n    return label\n\ndef weight(shape):\n    initial = tf.truncated_normal(shape, stddev=0.01)\n    return tf.Variable(initial)\n\ndef bias_0(shape):\n    initial = tf.constant(0.0, shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(image, weight, stride):\n    return tf.nn.conv2d(image, weight, strides=[1,stride,stride,1], padding='SAME')\n\ndef max_pool_3x3(image, stride):\n    return tf.nn.max_pool(image, ksize=[1, 3, 3, 1], strides=[1, stride, stride, 1], padding='SAME')\n\ndef batch_norm(image, out_size):\n    mean, var = tf.nn.moments(image, [0, 1, 2])\n    scale = tf.Variable(tf.ones([out_size]))\n    beta = tf.Variable(tf.zeros([out_size]))\n    epsilon = 0.001\n    bn = tf.nn.batch_normalization(image,mean,var,beta,scale,epsilon)\n    return bn\n\ndef inception(image, conv1_size, conv2_1_size, conv2_2_size, conv3_1_size, conv3_2_size, conv4_2_size):\n    conv1_weight = weight(conv1_size)\n    conv1_bias = bias_0([conv1_size[3]])\n    conv1 = tf.nn.relu(batch_norm(conv2d(image, conv1_weight, 1) + conv1_bias, conv1_size[3]))\n\n    conv2_1_weight = weight(conv2_1_size)\n    conv2_1_bias = bias_0([conv2_1_size[3]])\n    conv2_1 = tf.nn.relu(batch_norm(conv2d(image, conv2_1_weight, 1) + conv2_1_bias, conv2_1_size[3]))\n    conv2_2_weight = weight(conv2_2_size)\n    conv2_2_bias = bias_0([conv2_2_size[3]])\n    conv2_2 = tf.nn.relu(batch_norm(conv2d(conv2_1, conv2_2_weight, 1) + conv2_2_bias, conv2_2_size[3]))\n\n    conv3_1_weight = weight(conv3_1_size)\n    conv3_1_bias = bias_0([conv3_1_size[3]])\n    conv3_1 = tf.nn.relu(batch_norm(conv2d(image, conv3_1_weight, 1) + conv3_1_bias, conv3_1_size[3]))\n    conv3_2_weight = weight(conv3_2_size)\n    conv3_2_bias = bias_0([conv3_2_size[3]])\n    conv3_2 = tf.nn.relu(batch_norm(conv2d(conv3_1, conv3_2_weight, 1) + conv3_2_bias, conv3_2_size[3]))\n\n    pool4_1 = max_pool_3x3(image, 1)\n    conv4_2_weight = weight(conv4_2_size)\n    conv4_2_bias = bias_0([conv4_2_size[3]])\n    conv4_2 = tf.nn.relu(batch_norm(conv2d(pool4_1, conv4_2_weight, 1) + conv4_2_bias, conv4_2_size[3]))\n    return tf.concat([conv1, conv2_2, conv3_2, conv4_2], 3)\n\n\nnow = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n\nroot_logdir = \"/home/tensorflow/tf_logs\"\nlogdir = \"{}/run-{}/\".format(root_logdir, now)\nsavedir = \"/home/tensorflow/saves\"\n\nlabels = readCSV()\nnum_labels = len(labels)\ntrainpath = '/home/resized/trainset/'\ntestpath = '/home/resized/testset/'\n\ntrainnames = glob.glob(trainpath + '*.png')\ntestnames = glob.glob(testpath + '*.png')\n\n\ntrain_label = labelFiles(trainnames)\ntest_label = labelFiles(testnames)\n\nbatch_size = 256\nepochs = 50\nnum_batch = int(len(trainnames)/batch_size)\ninitial_learning_rate = 0.001\ndecay_steps = num_batch*8\ndecay_rate = 0.96\nglobal_step = tf.Variable(0, trainable = False)\nlearning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n\n\n# TRAIN QUEUE\ntrain_queue = tf.RandomShuffleQueue(len(trainnames)*1.5, 0, [tf.string, tf.float32], shapes=[[],[len(labels),]])\n\nenqueue_train = train_queue.enqueue_many([trainnames, train_label])\n\ntrain_image, train_image_label = train_queue.dequeue()\n\ntrain_image = read_image_file(train_image)\n\ntrain_batch, train_label_batch = tf.train.batch(\n    [train_image, train_image_label],\n    batch_size=batch_size,\n    num_threads=1,\n    capacity=10*batch_size,\n    enqueue_many=False,\n    shapes=[[224,112,3], [len(labels),]],\n    allow_smaller_final_batch=True\n)\n\ntrain_close = train_queue.close()\n\n# TEST QUEUE\n\ntest_queue = tf.FIFOQueue(len(testnames)*1.5, [tf.string, tf.float32], shapes=[[],[len(labels),]])\n\nenqueue_test = test_queue.enqueue_many([testnames, test_label])\n\ntest_image, test_image_label = test_queue.dequeue()\n\ntest_image = tf.expand_dims(read_image_file(test_image), 0)\n\ntest_close = test_queue.close()\n\n# MODEL\n\nkeep_prob = tf.placeholder(tf.float32)\n\nweights_conv1 = weight([7, 7, 3, 64])\n\nbias_conv1 = bias_0([64])\n\nweights_conv2 = weight([1, 1, 64, 64])\n\nbias_conv2 = bias_0([64])\n\nweights_conv3 = weight([3, 3, 64, 192])\n\nbias_conv3 = bias_0([192])\n\nweights_fc13 = weight([1024, num_labels])\n\nbias_fc13 = bias_0([num_labels])\n\n\ndef GoogleNet(data):\n\n    # First Conv. Layer\n    conv1 = tf.nn.relu(batch_norm(conv2d(data, weights_conv1, 2) + bias_conv1, 64))\n\n    pool1 = max_pool_3x3(conv1, 2)\n\n    lrn1 = tf.nn.local_response_normalization(pool1, 2, 1, 0.00002, 0.75)\n\n    # Second Conv. Layer\n    conv2 = tf.nn.relu(batch_norm(conv2d(lrn1, weights_conv2, 1) + bias_conv2, 64))\n\n    conv3 = tf.nn.relu(batch_norm(conv2d(conv2, weights_conv3, 1) + bias_conv3, 192))\n\n    lrn2 = tf.nn.local_response_normalization(conv3, 2, 1, 0.00002, 0.75)\n\n    pool2 = max_pool_3x3(lrn2, 2)\n\n    # First Inception Layer\n    incep4 = inception(\n        pool2,\n        [1, 1, 192, 64],\n        [1, 1, 192, 96],\n        [3, 3, 96, 128],\n        [1, 1, 192, 16],\n        [5, 5, 16, 32],\n        [1, 1, 192, 32]\n    )\n\n    incep5 = inception(\n        incep4,\n        [1, 1, 256, 128],\n        [1, 1, 256, 128],\n        [3, 3, 128, 192],\n        [1, 1, 256, 32],\n        [5, 5, 32, 96],\n        [1, 1, 256, 64]\n    )\n\n    pool3 = max_pool_3x3(incep5, 2)\n\n    # Second Inception Layer\n    incep6 = inception(\n        pool3,\n        [1, 1, 480, 192],\n        [1, 1, 480, 96],\n        [3, 3, 96, 208],\n        [1, 1, 480, 16],\n        [5, 5, 16, 48],\n        [1, 1, 480, 64]\n    )\n\n    incep7 = inception(\n        incep6,\n        [1, 1, 512, 160],\n        [1, 1, 512, 112],\n        [3, 3, 112, 224],\n        [1, 1, 512, 24],\n        [5, 5, 24, 64],\n        [1, 1, 512, 64]\n    )\n\n    incep8 = inception(\n        incep7,\n        [1, 1, 512, 128],\n        [1, 1, 512, 128],\n        [3, 3, 128, 256],\n        [1, 1, 512, 24],\n        [5, 5, 24, 64],\n        [1, 1, 512, 64]\n    )\n\n    incep9 = inception(\n        incep8,\n        [1, 1, 512, 112],\n        [1, 1, 512, 144],\n        [3, 3, 144, 288],\n        [1, 1, 512, 32],\n        [5, 5, 32, 64],\n        [1, 1, 512, 64]\n    )\n\n    incep10 = inception(\n        incep9,\n        [1, 1, 528, 256],\n        [1, 1, 528, 160],\n        [3, 3, 160, 320],\n        [1, 1, 528, 32],\n        [5, 5, 32, 128],\n        [1, 1, 528, 128]\n    )\n\n    pool4 = max_pool_3x3(incep10, 2)\n\n    # Third Inception Layer\n    incep11 = inception(\n        pool4,\n        [1, 1, 832, 256],\n        [1, 1, 832, 160],\n        [3, 3, 160, 320],\n        [1, 1, 832, 32],\n        [5, 5, 32, 128],\n        [1, 1, 832, 128]\n    )\n\n    incep12 = inception(\n        incep11,\n        [1, 1, 832, 384],\n        [1, 1, 832, 192],\n        [3, 3, 192, 384],\n        [1, 1, 832, 48],\n        [5, 5, 48, 128],\n        [1, 1, 832, 128]\n    )\n\n    pool5 = tf.nn.avg_pool(incep12, ksize = [1, 7, 4, 1], strides = [1, 1, 1, 1], padding = 'VALID')\n\n    pool5_flat = tf.reshape(pool5, [-1, 1024])\n\n    # FC Layers\n    fc13_drop = tf.nn.dropout(pool5_flat, keep_prob)\n\n    fc13 = tf.matmul(fc13_drop, weights_fc13) + bias_fc13\n\n    return fc13\n\n#Training\nwith tf.name_scope(\"cost_function\") as scope:\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_label_batch, logits=GoogleNet(train_batch)))\n    train_step = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(cost, global_step=global_step)\n\n\n#Accuracy\nwith tf.name_scope(\"accuracy\") as scope:\n    correct_prediction = tf.equal(tf.argmax(GoogleNet(test_image), 1), tf.argmax(test_image_label, 0))\n    accuracy = tf.cast(correct_prediction, tf.float32)\n\ncost_summary = tf.summary.scalar(\"cost_function\", cost)\nfile_writer = tf.summary.FileWriter(logdir)\n\n#Session\nwith tf.Session() as sess:\n    saver = tf.train.Saver()\n    saver.restore(sess, \"/Users/aleex/Dropbox/TensorFlow/GoogleNet/saves/GoogleNet.ckpt\")\n    print(\"Model restored...\")\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord, start=True)\n\n    sess.run(enqueue_test)\n    accuracy_vector = []\n   \n    print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))\n    \n    for num in range(len(testnames)):\n        accuracy_vector.append(sess.run(accuracy, feed_dict={keep_prob: 1.0}))\n    mean_accuracy = sess.run(tf.divide(tf.add_n(accuracy_vector), len(testnames)))\n\n    print(\"test accuracy %g\"%mean_accuracy)\n    sess.run(test_close)\n\n    coord.request_stop()\n    coord.join(threads)\n\n    file_writer.close()\n</code></pre>\n<p>It was the line <code>print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))</code> that messed this up.<br>\nA side question here: the model was trained well, but it gives me a poor test accuracy of 0.0004, which basically shows that the predictions with the test samples are all wrong. Is this a bug?</p>", "body_text": "I'm trying running some ops with a pre-trained model and the model was restored successfully. When running some ops that depend on one node in the graph, TF returns me\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_213\n\t [[Node: Variable_213/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_213\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_213)]]\n\nIt might be that I missed something , but this didn't show up before I updated my Tensorflow to 1.2. I'm running TF on MacOS X, Python 2.7. Below is the source code:\nSource code / logs\nimport tensorflow as tf\nimport csv\nimport os\nimport glob\nfrom PIL import Image\nfrom datetime import datetime\n\ndef read_image_file(filename):\n    image = tf.image.decode_png(tf.read_file(filename), channels=3)\n    image = tf.cast(image, tf.float32)\n    image = tf.reshape(image, [224, 112, 3])\n    return image\n\ndef labelFiles(filename):\n    label_list = []\n    for name in filename:\n        label_list.append(getLabel(name))\n    return label_list\n\ndef readCSV():\n    with open('label.csv', 'r') as infile:\n        reader = csv.reader(infile)\n        label = {rows[0]:rows[1] for rows in reader}\n        return label\n\ndef getWord(filename):\n    word = ''\n    (name, extension) = os.path.splitext(filename)\n    (path, name) = os.path.split(name)\n    for i in name:\n        if i.isalpha():\n            start = name.index(i)\n            word = word + name[start:]\n            #print(word)\n            break\n    return word\n\ndef getLabel(filename):\n    label = [0]*len(labels)\n    index =  int(labels.get(getWord(str(filename)), 0))\n    label[index] = 1\n    label = tf.constant(label, dtype = tf.float32)\n    tf.reshape(label, [-1])\n    print(label)\n    return label\n\ndef weight(shape):\n    initial = tf.truncated_normal(shape, stddev=0.01)\n    return tf.Variable(initial)\n\ndef bias_0(shape):\n    initial = tf.constant(0.0, shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(image, weight, stride):\n    return tf.nn.conv2d(image, weight, strides=[1,stride,stride,1], padding='SAME')\n\ndef max_pool_3x3(image, stride):\n    return tf.nn.max_pool(image, ksize=[1, 3, 3, 1], strides=[1, stride, stride, 1], padding='SAME')\n\ndef batch_norm(image, out_size):\n    mean, var = tf.nn.moments(image, [0, 1, 2])\n    scale = tf.Variable(tf.ones([out_size]))\n    beta = tf.Variable(tf.zeros([out_size]))\n    epsilon = 0.001\n    bn = tf.nn.batch_normalization(image,mean,var,beta,scale,epsilon)\n    return bn\n\ndef inception(image, conv1_size, conv2_1_size, conv2_2_size, conv3_1_size, conv3_2_size, conv4_2_size):\n    conv1_weight = weight(conv1_size)\n    conv1_bias = bias_0([conv1_size[3]])\n    conv1 = tf.nn.relu(batch_norm(conv2d(image, conv1_weight, 1) + conv1_bias, conv1_size[3]))\n\n    conv2_1_weight = weight(conv2_1_size)\n    conv2_1_bias = bias_0([conv2_1_size[3]])\n    conv2_1 = tf.nn.relu(batch_norm(conv2d(image, conv2_1_weight, 1) + conv2_1_bias, conv2_1_size[3]))\n    conv2_2_weight = weight(conv2_2_size)\n    conv2_2_bias = bias_0([conv2_2_size[3]])\n    conv2_2 = tf.nn.relu(batch_norm(conv2d(conv2_1, conv2_2_weight, 1) + conv2_2_bias, conv2_2_size[3]))\n\n    conv3_1_weight = weight(conv3_1_size)\n    conv3_1_bias = bias_0([conv3_1_size[3]])\n    conv3_1 = tf.nn.relu(batch_norm(conv2d(image, conv3_1_weight, 1) + conv3_1_bias, conv3_1_size[3]))\n    conv3_2_weight = weight(conv3_2_size)\n    conv3_2_bias = bias_0([conv3_2_size[3]])\n    conv3_2 = tf.nn.relu(batch_norm(conv2d(conv3_1, conv3_2_weight, 1) + conv3_2_bias, conv3_2_size[3]))\n\n    pool4_1 = max_pool_3x3(image, 1)\n    conv4_2_weight = weight(conv4_2_size)\n    conv4_2_bias = bias_0([conv4_2_size[3]])\n    conv4_2 = tf.nn.relu(batch_norm(conv2d(pool4_1, conv4_2_weight, 1) + conv4_2_bias, conv4_2_size[3]))\n    return tf.concat([conv1, conv2_2, conv3_2, conv4_2], 3)\n\n\nnow = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n\nroot_logdir = \"/home/tensorflow/tf_logs\"\nlogdir = \"{}/run-{}/\".format(root_logdir, now)\nsavedir = \"/home/tensorflow/saves\"\n\nlabels = readCSV()\nnum_labels = len(labels)\ntrainpath = '/home/resized/trainset/'\ntestpath = '/home/resized/testset/'\n\ntrainnames = glob.glob(trainpath + '*.png')\ntestnames = glob.glob(testpath + '*.png')\n\n\ntrain_label = labelFiles(trainnames)\ntest_label = labelFiles(testnames)\n\nbatch_size = 256\nepochs = 50\nnum_batch = int(len(trainnames)/batch_size)\ninitial_learning_rate = 0.001\ndecay_steps = num_batch*8\ndecay_rate = 0.96\nglobal_step = tf.Variable(0, trainable = False)\nlearning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n\n\n# TRAIN QUEUE\ntrain_queue = tf.RandomShuffleQueue(len(trainnames)*1.5, 0, [tf.string, tf.float32], shapes=[[],[len(labels),]])\n\nenqueue_train = train_queue.enqueue_many([trainnames, train_label])\n\ntrain_image, train_image_label = train_queue.dequeue()\n\ntrain_image = read_image_file(train_image)\n\ntrain_batch, train_label_batch = tf.train.batch(\n    [train_image, train_image_label],\n    batch_size=batch_size,\n    num_threads=1,\n    capacity=10*batch_size,\n    enqueue_many=False,\n    shapes=[[224,112,3], [len(labels),]],\n    allow_smaller_final_batch=True\n)\n\ntrain_close = train_queue.close()\n\n# TEST QUEUE\n\ntest_queue = tf.FIFOQueue(len(testnames)*1.5, [tf.string, tf.float32], shapes=[[],[len(labels),]])\n\nenqueue_test = test_queue.enqueue_many([testnames, test_label])\n\ntest_image, test_image_label = test_queue.dequeue()\n\ntest_image = tf.expand_dims(read_image_file(test_image), 0)\n\ntest_close = test_queue.close()\n\n# MODEL\n\nkeep_prob = tf.placeholder(tf.float32)\n\nweights_conv1 = weight([7, 7, 3, 64])\n\nbias_conv1 = bias_0([64])\n\nweights_conv2 = weight([1, 1, 64, 64])\n\nbias_conv2 = bias_0([64])\n\nweights_conv3 = weight([3, 3, 64, 192])\n\nbias_conv3 = bias_0([192])\n\nweights_fc13 = weight([1024, num_labels])\n\nbias_fc13 = bias_0([num_labels])\n\n\ndef GoogleNet(data):\n\n    # First Conv. Layer\n    conv1 = tf.nn.relu(batch_norm(conv2d(data, weights_conv1, 2) + bias_conv1, 64))\n\n    pool1 = max_pool_3x3(conv1, 2)\n\n    lrn1 = tf.nn.local_response_normalization(pool1, 2, 1, 0.00002, 0.75)\n\n    # Second Conv. Layer\n    conv2 = tf.nn.relu(batch_norm(conv2d(lrn1, weights_conv2, 1) + bias_conv2, 64))\n\n    conv3 = tf.nn.relu(batch_norm(conv2d(conv2, weights_conv3, 1) + bias_conv3, 192))\n\n    lrn2 = tf.nn.local_response_normalization(conv3, 2, 1, 0.00002, 0.75)\n\n    pool2 = max_pool_3x3(lrn2, 2)\n\n    # First Inception Layer\n    incep4 = inception(\n        pool2,\n        [1, 1, 192, 64],\n        [1, 1, 192, 96],\n        [3, 3, 96, 128],\n        [1, 1, 192, 16],\n        [5, 5, 16, 32],\n        [1, 1, 192, 32]\n    )\n\n    incep5 = inception(\n        incep4,\n        [1, 1, 256, 128],\n        [1, 1, 256, 128],\n        [3, 3, 128, 192],\n        [1, 1, 256, 32],\n        [5, 5, 32, 96],\n        [1, 1, 256, 64]\n    )\n\n    pool3 = max_pool_3x3(incep5, 2)\n\n    # Second Inception Layer\n    incep6 = inception(\n        pool3,\n        [1, 1, 480, 192],\n        [1, 1, 480, 96],\n        [3, 3, 96, 208],\n        [1, 1, 480, 16],\n        [5, 5, 16, 48],\n        [1, 1, 480, 64]\n    )\n\n    incep7 = inception(\n        incep6,\n        [1, 1, 512, 160],\n        [1, 1, 512, 112],\n        [3, 3, 112, 224],\n        [1, 1, 512, 24],\n        [5, 5, 24, 64],\n        [1, 1, 512, 64]\n    )\n\n    incep8 = inception(\n        incep7,\n        [1, 1, 512, 128],\n        [1, 1, 512, 128],\n        [3, 3, 128, 256],\n        [1, 1, 512, 24],\n        [5, 5, 24, 64],\n        [1, 1, 512, 64]\n    )\n\n    incep9 = inception(\n        incep8,\n        [1, 1, 512, 112],\n        [1, 1, 512, 144],\n        [3, 3, 144, 288],\n        [1, 1, 512, 32],\n        [5, 5, 32, 64],\n        [1, 1, 512, 64]\n    )\n\n    incep10 = inception(\n        incep9,\n        [1, 1, 528, 256],\n        [1, 1, 528, 160],\n        [3, 3, 160, 320],\n        [1, 1, 528, 32],\n        [5, 5, 32, 128],\n        [1, 1, 528, 128]\n    )\n\n    pool4 = max_pool_3x3(incep10, 2)\n\n    # Third Inception Layer\n    incep11 = inception(\n        pool4,\n        [1, 1, 832, 256],\n        [1, 1, 832, 160],\n        [3, 3, 160, 320],\n        [1, 1, 832, 32],\n        [5, 5, 32, 128],\n        [1, 1, 832, 128]\n    )\n\n    incep12 = inception(\n        incep11,\n        [1, 1, 832, 384],\n        [1, 1, 832, 192],\n        [3, 3, 192, 384],\n        [1, 1, 832, 48],\n        [5, 5, 48, 128],\n        [1, 1, 832, 128]\n    )\n\n    pool5 = tf.nn.avg_pool(incep12, ksize = [1, 7, 4, 1], strides = [1, 1, 1, 1], padding = 'VALID')\n\n    pool5_flat = tf.reshape(pool5, [-1, 1024])\n\n    # FC Layers\n    fc13_drop = tf.nn.dropout(pool5_flat, keep_prob)\n\n    fc13 = tf.matmul(fc13_drop, weights_fc13) + bias_fc13\n\n    return fc13\n\n#Training\nwith tf.name_scope(\"cost_function\") as scope:\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_label_batch, logits=GoogleNet(train_batch)))\n    train_step = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(cost, global_step=global_step)\n\n\n#Accuracy\nwith tf.name_scope(\"accuracy\") as scope:\n    correct_prediction = tf.equal(tf.argmax(GoogleNet(test_image), 1), tf.argmax(test_image_label, 0))\n    accuracy = tf.cast(correct_prediction, tf.float32)\n\ncost_summary = tf.summary.scalar(\"cost_function\", cost)\nfile_writer = tf.summary.FileWriter(logdir)\n\n#Session\nwith tf.Session() as sess:\n    saver = tf.train.Saver()\n    saver.restore(sess, \"/Users/aleex/Dropbox/TensorFlow/GoogleNet/saves/GoogleNet.ckpt\")\n    print(\"Model restored...\")\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord, start=True)\n\n    sess.run(enqueue_test)\n    accuracy_vector = []\n   \n    print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))\n    \n    for num in range(len(testnames)):\n        accuracy_vector.append(sess.run(accuracy, feed_dict={keep_prob: 1.0}))\n    mean_accuracy = sess.run(tf.divide(tf.add_n(accuracy_vector), len(testnames)))\n\n    print(\"test accuracy %g\"%mean_accuracy)\n    sess.run(test_close)\n\n    coord.request_stop()\n    coord.join(threads)\n\n    file_writer.close()\n\nIt was the line print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0})) that messed this up.\nA side question here: the model was trained well, but it gives me a poor test accuracy of 0.0004, which basically shows that the predictions with the test samples are all wrong. Is this a bug?", "body": "I'm trying running some ops with a pre-trained model and the model was restored successfully. When running some ops that depend on one node in the graph, TF returns me \r\n```\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_213\r\n\t [[Node: Variable_213/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_213\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_213)]]\r\n```\r\nIt might be that I missed something , but this didn't show up before I updated my Tensorflow to 1.2. I'm running TF on MacOS X, Python 2.7. Below is the source code:\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport csv\r\nimport os\r\nimport glob\r\nfrom PIL import Image\r\nfrom datetime import datetime\r\n\r\ndef read_image_file(filename):\r\n    image = tf.image.decode_png(tf.read_file(filename), channels=3)\r\n    image = tf.cast(image, tf.float32)\r\n    image = tf.reshape(image, [224, 112, 3])\r\n    return image\r\n\r\ndef labelFiles(filename):\r\n    label_list = []\r\n    for name in filename:\r\n        label_list.append(getLabel(name))\r\n    return label_list\r\n\r\ndef readCSV():\r\n    with open('label.csv', 'r') as infile:\r\n        reader = csv.reader(infile)\r\n        label = {rows[0]:rows[1] for rows in reader}\r\n        return label\r\n\r\ndef getWord(filename):\r\n    word = ''\r\n    (name, extension) = os.path.splitext(filename)\r\n    (path, name) = os.path.split(name)\r\n    for i in name:\r\n        if i.isalpha():\r\n            start = name.index(i)\r\n            word = word + name[start:]\r\n            #print(word)\r\n            break\r\n    return word\r\n\r\ndef getLabel(filename):\r\n    label = [0]*len(labels)\r\n    index =  int(labels.get(getWord(str(filename)), 0))\r\n    label[index] = 1\r\n    label = tf.constant(label, dtype = tf.float32)\r\n    tf.reshape(label, [-1])\r\n    print(label)\r\n    return label\r\n\r\ndef weight(shape):\r\n    initial = tf.truncated_normal(shape, stddev=0.01)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_0(shape):\r\n    initial = tf.constant(0.0, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef conv2d(image, weight, stride):\r\n    return tf.nn.conv2d(image, weight, strides=[1,stride,stride,1], padding='SAME')\r\n\r\ndef max_pool_3x3(image, stride):\r\n    return tf.nn.max_pool(image, ksize=[1, 3, 3, 1], strides=[1, stride, stride, 1], padding='SAME')\r\n\r\ndef batch_norm(image, out_size):\r\n    mean, var = tf.nn.moments(image, [0, 1, 2])\r\n    scale = tf.Variable(tf.ones([out_size]))\r\n    beta = tf.Variable(tf.zeros([out_size]))\r\n    epsilon = 0.001\r\n    bn = tf.nn.batch_normalization(image,mean,var,beta,scale,epsilon)\r\n    return bn\r\n\r\ndef inception(image, conv1_size, conv2_1_size, conv2_2_size, conv3_1_size, conv3_2_size, conv4_2_size):\r\n    conv1_weight = weight(conv1_size)\r\n    conv1_bias = bias_0([conv1_size[3]])\r\n    conv1 = tf.nn.relu(batch_norm(conv2d(image, conv1_weight, 1) + conv1_bias, conv1_size[3]))\r\n\r\n    conv2_1_weight = weight(conv2_1_size)\r\n    conv2_1_bias = bias_0([conv2_1_size[3]])\r\n    conv2_1 = tf.nn.relu(batch_norm(conv2d(image, conv2_1_weight, 1) + conv2_1_bias, conv2_1_size[3]))\r\n    conv2_2_weight = weight(conv2_2_size)\r\n    conv2_2_bias = bias_0([conv2_2_size[3]])\r\n    conv2_2 = tf.nn.relu(batch_norm(conv2d(conv2_1, conv2_2_weight, 1) + conv2_2_bias, conv2_2_size[3]))\r\n\r\n    conv3_1_weight = weight(conv3_1_size)\r\n    conv3_1_bias = bias_0([conv3_1_size[3]])\r\n    conv3_1 = tf.nn.relu(batch_norm(conv2d(image, conv3_1_weight, 1) + conv3_1_bias, conv3_1_size[3]))\r\n    conv3_2_weight = weight(conv3_2_size)\r\n    conv3_2_bias = bias_0([conv3_2_size[3]])\r\n    conv3_2 = tf.nn.relu(batch_norm(conv2d(conv3_1, conv3_2_weight, 1) + conv3_2_bias, conv3_2_size[3]))\r\n\r\n    pool4_1 = max_pool_3x3(image, 1)\r\n    conv4_2_weight = weight(conv4_2_size)\r\n    conv4_2_bias = bias_0([conv4_2_size[3]])\r\n    conv4_2 = tf.nn.relu(batch_norm(conv2d(pool4_1, conv4_2_weight, 1) + conv4_2_bias, conv4_2_size[3]))\r\n    return tf.concat([conv1, conv2_2, conv3_2, conv4_2], 3)\r\n\r\n\r\nnow = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\r\n\r\nroot_logdir = \"/home/tensorflow/tf_logs\"\r\nlogdir = \"{}/run-{}/\".format(root_logdir, now)\r\nsavedir = \"/home/tensorflow/saves\"\r\n\r\nlabels = readCSV()\r\nnum_labels = len(labels)\r\ntrainpath = '/home/resized/trainset/'\r\ntestpath = '/home/resized/testset/'\r\n\r\ntrainnames = glob.glob(trainpath + '*.png')\r\ntestnames = glob.glob(testpath + '*.png')\r\n\r\n\r\ntrain_label = labelFiles(trainnames)\r\ntest_label = labelFiles(testnames)\r\n\r\nbatch_size = 256\r\nepochs = 50\r\nnum_batch = int(len(trainnames)/batch_size)\r\ninitial_learning_rate = 0.001\r\ndecay_steps = num_batch*8\r\ndecay_rate = 0.96\r\nglobal_step = tf.Variable(0, trainable = False)\r\nlearning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate, staircase=True)\r\n\r\n\r\n# TRAIN QUEUE\r\ntrain_queue = tf.RandomShuffleQueue(len(trainnames)*1.5, 0, [tf.string, tf.float32], shapes=[[],[len(labels),]])\r\n\r\nenqueue_train = train_queue.enqueue_many([trainnames, train_label])\r\n\r\ntrain_image, train_image_label = train_queue.dequeue()\r\n\r\ntrain_image = read_image_file(train_image)\r\n\r\ntrain_batch, train_label_batch = tf.train.batch(\r\n    [train_image, train_image_label],\r\n    batch_size=batch_size,\r\n    num_threads=1,\r\n    capacity=10*batch_size,\r\n    enqueue_many=False,\r\n    shapes=[[224,112,3], [len(labels),]],\r\n    allow_smaller_final_batch=True\r\n)\r\n\r\ntrain_close = train_queue.close()\r\n\r\n# TEST QUEUE\r\n\r\ntest_queue = tf.FIFOQueue(len(testnames)*1.5, [tf.string, tf.float32], shapes=[[],[len(labels),]])\r\n\r\nenqueue_test = test_queue.enqueue_many([testnames, test_label])\r\n\r\ntest_image, test_image_label = test_queue.dequeue()\r\n\r\ntest_image = tf.expand_dims(read_image_file(test_image), 0)\r\n\r\ntest_close = test_queue.close()\r\n\r\n# MODEL\r\n\r\nkeep_prob = tf.placeholder(tf.float32)\r\n\r\nweights_conv1 = weight([7, 7, 3, 64])\r\n\r\nbias_conv1 = bias_0([64])\r\n\r\nweights_conv2 = weight([1, 1, 64, 64])\r\n\r\nbias_conv2 = bias_0([64])\r\n\r\nweights_conv3 = weight([3, 3, 64, 192])\r\n\r\nbias_conv3 = bias_0([192])\r\n\r\nweights_fc13 = weight([1024, num_labels])\r\n\r\nbias_fc13 = bias_0([num_labels])\r\n\r\n\r\ndef GoogleNet(data):\r\n\r\n    # First Conv. Layer\r\n    conv1 = tf.nn.relu(batch_norm(conv2d(data, weights_conv1, 2) + bias_conv1, 64))\r\n\r\n    pool1 = max_pool_3x3(conv1, 2)\r\n\r\n    lrn1 = tf.nn.local_response_normalization(pool1, 2, 1, 0.00002, 0.75)\r\n\r\n    # Second Conv. Layer\r\n    conv2 = tf.nn.relu(batch_norm(conv2d(lrn1, weights_conv2, 1) + bias_conv2, 64))\r\n\r\n    conv3 = tf.nn.relu(batch_norm(conv2d(conv2, weights_conv3, 1) + bias_conv3, 192))\r\n\r\n    lrn2 = tf.nn.local_response_normalization(conv3, 2, 1, 0.00002, 0.75)\r\n\r\n    pool2 = max_pool_3x3(lrn2, 2)\r\n\r\n    # First Inception Layer\r\n    incep4 = inception(\r\n        pool2,\r\n        [1, 1, 192, 64],\r\n        [1, 1, 192, 96],\r\n        [3, 3, 96, 128],\r\n        [1, 1, 192, 16],\r\n        [5, 5, 16, 32],\r\n        [1, 1, 192, 32]\r\n    )\r\n\r\n    incep5 = inception(\r\n        incep4,\r\n        [1, 1, 256, 128],\r\n        [1, 1, 256, 128],\r\n        [3, 3, 128, 192],\r\n        [1, 1, 256, 32],\r\n        [5, 5, 32, 96],\r\n        [1, 1, 256, 64]\r\n    )\r\n\r\n    pool3 = max_pool_3x3(incep5, 2)\r\n\r\n    # Second Inception Layer\r\n    incep6 = inception(\r\n        pool3,\r\n        [1, 1, 480, 192],\r\n        [1, 1, 480, 96],\r\n        [3, 3, 96, 208],\r\n        [1, 1, 480, 16],\r\n        [5, 5, 16, 48],\r\n        [1, 1, 480, 64]\r\n    )\r\n\r\n    incep7 = inception(\r\n        incep6,\r\n        [1, 1, 512, 160],\r\n        [1, 1, 512, 112],\r\n        [3, 3, 112, 224],\r\n        [1, 1, 512, 24],\r\n        [5, 5, 24, 64],\r\n        [1, 1, 512, 64]\r\n    )\r\n\r\n    incep8 = inception(\r\n        incep7,\r\n        [1, 1, 512, 128],\r\n        [1, 1, 512, 128],\r\n        [3, 3, 128, 256],\r\n        [1, 1, 512, 24],\r\n        [5, 5, 24, 64],\r\n        [1, 1, 512, 64]\r\n    )\r\n\r\n    incep9 = inception(\r\n        incep8,\r\n        [1, 1, 512, 112],\r\n        [1, 1, 512, 144],\r\n        [3, 3, 144, 288],\r\n        [1, 1, 512, 32],\r\n        [5, 5, 32, 64],\r\n        [1, 1, 512, 64]\r\n    )\r\n\r\n    incep10 = inception(\r\n        incep9,\r\n        [1, 1, 528, 256],\r\n        [1, 1, 528, 160],\r\n        [3, 3, 160, 320],\r\n        [1, 1, 528, 32],\r\n        [5, 5, 32, 128],\r\n        [1, 1, 528, 128]\r\n    )\r\n\r\n    pool4 = max_pool_3x3(incep10, 2)\r\n\r\n    # Third Inception Layer\r\n    incep11 = inception(\r\n        pool4,\r\n        [1, 1, 832, 256],\r\n        [1, 1, 832, 160],\r\n        [3, 3, 160, 320],\r\n        [1, 1, 832, 32],\r\n        [5, 5, 32, 128],\r\n        [1, 1, 832, 128]\r\n    )\r\n\r\n    incep12 = inception(\r\n        incep11,\r\n        [1, 1, 832, 384],\r\n        [1, 1, 832, 192],\r\n        [3, 3, 192, 384],\r\n        [1, 1, 832, 48],\r\n        [5, 5, 48, 128],\r\n        [1, 1, 832, 128]\r\n    )\r\n\r\n    pool5 = tf.nn.avg_pool(incep12, ksize = [1, 7, 4, 1], strides = [1, 1, 1, 1], padding = 'VALID')\r\n\r\n    pool5_flat = tf.reshape(pool5, [-1, 1024])\r\n\r\n    # FC Layers\r\n    fc13_drop = tf.nn.dropout(pool5_flat, keep_prob)\r\n\r\n    fc13 = tf.matmul(fc13_drop, weights_fc13) + bias_fc13\r\n\r\n    return fc13\r\n\r\n#Training\r\nwith tf.name_scope(\"cost_function\") as scope:\r\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_label_batch, logits=GoogleNet(train_batch)))\r\n    train_step = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(cost, global_step=global_step)\r\n\r\n\r\n#Accuracy\r\nwith tf.name_scope(\"accuracy\") as scope:\r\n    correct_prediction = tf.equal(tf.argmax(GoogleNet(test_image), 1), tf.argmax(test_image_label, 0))\r\n    accuracy = tf.cast(correct_prediction, tf.float32)\r\n\r\ncost_summary = tf.summary.scalar(\"cost_function\", cost)\r\nfile_writer = tf.summary.FileWriter(logdir)\r\n\r\n#Session\r\nwith tf.Session() as sess:\r\n    saver = tf.train.Saver()\r\n    saver.restore(sess, \"/Users/aleex/Dropbox/TensorFlow/GoogleNet/saves/GoogleNet.ckpt\")\r\n    print(\"Model restored...\")\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord, start=True)\r\n\r\n    sess.run(enqueue_test)\r\n    accuracy_vector = []\r\n   \r\n    print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))\r\n    \r\n    for num in range(len(testnames)):\r\n        accuracy_vector.append(sess.run(accuracy, feed_dict={keep_prob: 1.0}))\r\n    mean_accuracy = sess.run(tf.divide(tf.add_n(accuracy_vector), len(testnames)))\r\n\r\n    print(\"test accuracy %g\"%mean_accuracy)\r\n    sess.run(test_close)\r\n\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n\r\n    file_writer.close()\r\n```\r\n\r\nIt was the line `print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))` that messed this up.\r\nA side question here: the model was trained well, but it gives me a poor test accuracy of 0.0004, which basically shows that the predictions with the test samples are all wrong. Is this a bug?"}