{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/325511459", "html_url": "https://github.com/tensorflow/tensorflow/pull/12660#issuecomment-325511459", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12660", "id": 325511459, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNTUxMTQ1OQ==", "user": {"login": "tjingrant", "id": 6410074, "node_id": "MDQ6VXNlcjY0MTAwNzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6410074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjingrant", "html_url": "https://github.com/tjingrant", "followers_url": "https://api.github.com/users/tjingrant/followers", "following_url": "https://api.github.com/users/tjingrant/following{/other_user}", "gists_url": "https://api.github.com/users/tjingrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjingrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjingrant/subscriptions", "organizations_url": "https://api.github.com/users/tjingrant/orgs", "repos_url": "https://api.github.com/users/tjingrant/repos", "events_url": "https://api.github.com/users/tjingrant/events{/privacy}", "received_events_url": "https://api.github.com/users/tjingrant/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-28T23:31:33Z", "updated_at": "2017-08-29T00:54:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, I did a manual test on two sets of inputs. One transposes 4 matrices of size 1024x1024, the other transposes 4 matrices of size 512x512. It takes current implementation 549.36us and 152.84us to finish whilst only  283.75us and 75.842us for the new implementation on a K40 GPU.</p>\n<p>I actually do not know how to invoke this transpose method using Python scripts so I used nvprof on test cases I added in xla/tests/copy_test.cc. Also, nvprof automatically aggregates the performance metrics of kernels with the same name so I can only measure two test cases at one run which is quite inconvenient. Would you please tell me what's the best way to benchmark it? It may come in handy in the future as well.</p>\n<p>In addition, I am not sure how important this transpose kernel is. If it is performance critical then I can try to bring other specializations done in the reference cuda kernels here as well.</p>", "body_text": "Hi, I did a manual test on two sets of inputs. One transposes 4 matrices of size 1024x1024, the other transposes 4 matrices of size 512x512. It takes current implementation 549.36us and 152.84us to finish whilst only  283.75us and 75.842us for the new implementation on a K40 GPU.\nI actually do not know how to invoke this transpose method using Python scripts so I used nvprof on test cases I added in xla/tests/copy_test.cc. Also, nvprof automatically aggregates the performance metrics of kernels with the same name so I can only measure two test cases at one run which is quite inconvenient. Would you please tell me what's the best way to benchmark it? It may come in handy in the future as well.\nIn addition, I am not sure how important this transpose kernel is. If it is performance critical then I can try to bring other specializations done in the reference cuda kernels here as well.", "body": "Hi, I did a manual test on two sets of inputs. One transposes 4 matrices of size 1024x1024, the other transposes 4 matrices of size 512x512. It takes current implementation 549.36us and 152.84us to finish whilst only  283.75us and 75.842us for the new implementation on a K40 GPU.\r\n\r\nI actually do not know how to invoke this transpose method using Python scripts so I used nvprof on test cases I added in xla/tests/copy_test.cc. Also, nvprof automatically aggregates the performance metrics of kernels with the same name so I can only measure two test cases at one run which is quite inconvenient. Would you please tell me what's the best way to benchmark it? It may come in handy in the future as well. \r\n\r\nIn addition, I am not sure how important this transpose kernel is. If it is performance critical then I can try to bring other specializations done in the reference cuda kernels here as well."}