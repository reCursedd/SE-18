{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/367335016", "html_url": "https://github.com/tensorflow/tensorflow/issues/6883#issuecomment-367335016", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6883", "id": 367335016, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzMzNTAxNg==", "user": {"login": "aforslow", "id": 17109848, "node_id": "MDQ6VXNlcjE3MTA5ODQ4", "avatar_url": "https://avatars1.githubusercontent.com/u/17109848?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aforslow", "html_url": "https://github.com/aforslow", "followers_url": "https://api.github.com/users/aforslow/followers", "following_url": "https://api.github.com/users/aforslow/following{/other_user}", "gists_url": "https://api.github.com/users/aforslow/gists{/gist_id}", "starred_url": "https://api.github.com/users/aforslow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aforslow/subscriptions", "organizations_url": "https://api.github.com/users/aforslow/orgs", "repos_url": "https://api.github.com/users/aforslow/repos", "events_url": "https://api.github.com/users/aforslow/events{/privacy}", "received_events_url": "https://api.github.com/users/aforslow/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-21T14:02:03Z", "updated_at": "2018-02-21T14:02:03Z", "author_association": "NONE", "body_html": "<p>In my case, I got the error when distributing my training session into multiple worker threads. What solved it for me was surrounding the inference graph with a tf.variable_scope(name, reuse=tf.AUTO_REUSE). I guess tensorflow can't keep track of your variables when working in a distributed mode.</p>", "body_text": "In my case, I got the error when distributing my training session into multiple worker threads. What solved it for me was surrounding the inference graph with a tf.variable_scope(name, reuse=tf.AUTO_REUSE). I guess tensorflow can't keep track of your variables when working in a distributed mode.", "body": "In my case, I got the error when distributing my training session into multiple worker threads. What solved it for me was surrounding the inference graph with a tf.variable_scope(name, reuse=tf.AUTO_REUSE). I guess tensorflow can't keep track of your variables when working in a distributed mode."}