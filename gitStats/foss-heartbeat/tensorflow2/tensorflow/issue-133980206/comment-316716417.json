{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316716417", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-316716417", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 316716417, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjcxNjQxNw==", "user": {"login": "tano297", "id": 18667639, "node_id": "MDQ6VXNlcjE4NjY3NjM5", "avatar_url": "https://avatars3.githubusercontent.com/u/18667639?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tano297", "html_url": "https://github.com/tano297", "followers_url": "https://api.github.com/users/tano297/followers", "following_url": "https://api.github.com/users/tano297/following{/other_user}", "gists_url": "https://api.github.com/users/tano297/gists{/gist_id}", "starred_url": "https://api.github.com/users/tano297/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tano297/subscriptions", "organizations_url": "https://api.github.com/users/tano297/orgs", "repos_url": "https://api.github.com/users/tano297/repos", "events_url": "https://api.github.com/users/tano297/events{/privacy}", "received_events_url": "https://api.github.com/users/tano297/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-20T14:14:05Z", "updated_at": "2017-07-20T14:14:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16910475\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/MisayaZ\">@MisayaZ</a> I was having the same behavior using Batchnorm with a placeholder for \"is_training\". I see in the trace that the moments are being calculated even at test time, so I decided to go into the source code and I found this:</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> If `is_training` doesn't have a constant value, because it is a `Tensor`,</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> a `Variable` or `Placeholder` then is_training_value will be None and</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> `needs_moments` will be true.</span>\n    is_training_value <span class=\"pl-k\">=</span> utils.constant_value(is_training)\n    need_moments <span class=\"pl-k\">=</span> is_training_value <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">or</span> is_training_value\n    <span class=\"pl-k\">if</span> need_moments:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> here it defines the moments</span></pre></div>\n<p>It looks like when \"is_training\" is a variable or a placeholder the moments get defined and also get calculates them at runtime, even when you set the placeholder to \"False\". I would have preferred to leave it as a placeholder because this way I can do periodic testing during training without redefining the graph, but I decided to use it as a constant and define different behaviors for train vs test, and now the moments are not calculated at test time.</p>", "body_text": "@MisayaZ I was having the same behavior using Batchnorm with a placeholder for \"is_training\". I see in the trace that the moments are being calculated even at test time, so I decided to go into the source code and I found this:\n    # If `is_training` doesn't have a constant value, because it is a `Tensor`,\n    # a `Variable` or `Placeholder` then is_training_value will be None and\n    # `needs_moments` will be true.\n    is_training_value = utils.constant_value(is_training)\n    need_moments = is_training_value is None or is_training_value\n    if need_moments:\n        # here it defines the moments\nIt looks like when \"is_training\" is a variable or a placeholder the moments get defined and also get calculates them at runtime, even when you set the placeholder to \"False\". I would have preferred to leave it as a placeholder because this way I can do periodic testing during training without redefining the graph, but I decided to use it as a constant and define different behaviors for train vs test, and now the moments are not calculated at test time.", "body": "@MisayaZ I was having the same behavior using Batchnorm with a placeholder for \"is_training\". I see in the trace that the moments are being calculated even at test time, so I decided to go into the source code and I found this:\r\n\r\n```python\r\n    # If `is_training` doesn't have a constant value, because it is a `Tensor`,\r\n    # a `Variable` or `Placeholder` then is_training_value will be None and\r\n    # `needs_moments` will be true.\r\n    is_training_value = utils.constant_value(is_training)\r\n    need_moments = is_training_value is None or is_training_value\r\n    if need_moments:\r\n        # here it defines the moments\r\n```\r\nIt looks like when \"is_training\" is a variable or a placeholder the moments get defined and also get calculates them at runtime, even when you set the placeholder to \"False\". I would have preferred to leave it as a placeholder because this way I can do periodic testing during training without redefining the graph, but I decided to use it as a constant and define different behaviors for train vs test, and now the moments are not calculated at test time. "}