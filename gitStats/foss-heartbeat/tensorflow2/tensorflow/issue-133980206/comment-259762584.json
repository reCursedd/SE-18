{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259762584", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-259762584", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 259762584, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTc2MjU4NA==", "user": {"login": "davek44", "id": 172688, "node_id": "MDQ6VXNlcjE3MjY4OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/172688?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davek44", "html_url": "https://github.com/davek44", "followers_url": "https://api.github.com/users/davek44/followers", "following_url": "https://api.github.com/users/davek44/following{/other_user}", "gists_url": "https://api.github.com/users/davek44/gists{/gist_id}", "starred_url": "https://api.github.com/users/davek44/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davek44/subscriptions", "organizations_url": "https://api.github.com/users/davek44/orgs", "repos_url": "https://api.github.com/users/davek44/repos", "events_url": "https://api.github.com/users/davek44/events{/privacy}", "received_events_url": "https://api.github.com/users/davek44/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-10T18:04:48Z", "updated_at": "2016-11-10T18:04:48Z", "author_association": "NONE", "body_html": "<p>I greatly appreciate the work that the TF team has put in here to make batch_norm available and effective. From my searching, this thread is the best resource for how to use it. There are many different problems and ideas flying around here, and it's difficult to figure out the consensus advice for the simplest standard case of how to use the batch_norm layer. I think there'd be a lot of value in expanding the documentation to specify the exact recommended usage.</p>\n<p>My best attempt to figure that out brought me to the following code:</p>\n<pre><code>is_training_ph = tf.placeholder(tf.bool)\n...\nwith tf.variable_scope('bn_test_layer') as vs:\n    layer_output = tf.cond(is_training_ph,\n        lambda: tf.contrib.layers.batch_norm(layer_input, is_training=True, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=vs),\n        lambda: tf.contrib.layers.batch_norm(layer_input, is_training=False, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=vs, reuse=True))\n</code></pre>\n<p>Then I set is_training_ph to True for training and False for testing. This doesn't work for me. The model trains fine, but the test performance is terrible. In contrast, if I maintain is_training_ph=True for test time, it works great. Thus, I'm guessing I still have a scope issue so that it's not finding the proper existing variables.</p>", "body_text": "I greatly appreciate the work that the TF team has put in here to make batch_norm available and effective. From my searching, this thread is the best resource for how to use it. There are many different problems and ideas flying around here, and it's difficult to figure out the consensus advice for the simplest standard case of how to use the batch_norm layer. I think there'd be a lot of value in expanding the documentation to specify the exact recommended usage.\nMy best attempt to figure that out brought me to the following code:\nis_training_ph = tf.placeholder(tf.bool)\n...\nwith tf.variable_scope('bn_test_layer') as vs:\n    layer_output = tf.cond(is_training_ph,\n        lambda: tf.contrib.layers.batch_norm(layer_input, is_training=True, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=vs),\n        lambda: tf.contrib.layers.batch_norm(layer_input, is_training=False, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=vs, reuse=True))\n\nThen I set is_training_ph to True for training and False for testing. This doesn't work for me. The model trains fine, but the test performance is terrible. In contrast, if I maintain is_training_ph=True for test time, it works great. Thus, I'm guessing I still have a scope issue so that it's not finding the proper existing variables.", "body": "I greatly appreciate the work that the TF team has put in here to make batch_norm available and effective. From my searching, this thread is the best resource for how to use it. There are many different problems and ideas flying around here, and it's difficult to figure out the consensus advice for the simplest standard case of how to use the batch_norm layer. I think there'd be a lot of value in expanding the documentation to specify the exact recommended usage.\n\nMy best attempt to figure that out brought me to the following code:\n\n```\nis_training_ph = tf.placeholder(tf.bool)\n...\nwith tf.variable_scope('bn_test_layer') as vs:\n    layer_output = tf.cond(is_training_ph,\n        lambda: tf.contrib.layers.batch_norm(layer_input, is_training=True, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=vs),\n        lambda: tf.contrib.layers.batch_norm(layer_input, is_training=False, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=vs, reuse=True))\n```\n\nThen I set is_training_ph to True for training and False for testing. This doesn't work for me. The model trains fine, but the test performance is terrible. In contrast, if I maintain is_training_ph=True for test time, it works great. Thus, I'm guessing I still have a scope issue so that it's not finding the proper existing variables. \n"}