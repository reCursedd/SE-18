{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/291332856", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-291332856", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 291332856, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MTMzMjg1Ng==", "user": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-04T00:08:04Z", "updated_at": "2017-04-04T00:08:24Z", "author_association": "MEMBER", "body_html": "<p>Although is_traning can a placeholder reuse has to be a bool, and it cannot be a tensor nor a placeholder.</p>\n<p>I'm not sure what are you trying to do, in most cases using static values solve the problem. For example this pattern works well:</p>\n<pre><code>def model(data, is_training=False, reuse=None, scope='my_model'):\n  # Define a variable scope to contain all the variables of your model\n  with tf.variable_scope(scope, 'model', data, reuse=reuse):\n    # 1 layer\n    net = tf.contrib.layers.conv2d(data, ....)\n    ....\n    net = tf.contrib.layers.batch_norm(net, is_training)\n   return net\n\ntrain_outputs = model(train_data, is_training=True)\neval_outputs = model(eval_data, is_training=False, reuse=True)\n\neval_predictions = sess.run(eval_outputs, feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n\n</code></pre>\n<p>Unless you need to change the behavior of the model dynamically, you don't need to use a placeholder for is_training. The trick is to build the model twice, but sharing the variables the second time.</p>", "body_text": "Although is_traning can a placeholder reuse has to be a bool, and it cannot be a tensor nor a placeholder.\nI'm not sure what are you trying to do, in most cases using static values solve the problem. For example this pattern works well:\ndef model(data, is_training=False, reuse=None, scope='my_model'):\n  # Define a variable scope to contain all the variables of your model\n  with tf.variable_scope(scope, 'model', data, reuse=reuse):\n    # 1 layer\n    net = tf.contrib.layers.conv2d(data, ....)\n    ....\n    net = tf.contrib.layers.batch_norm(net, is_training)\n   return net\n\ntrain_outputs = model(train_data, is_training=True)\neval_outputs = model(eval_data, is_training=False, reuse=True)\n\neval_predictions = sess.run(eval_outputs, feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n\n\nUnless you need to change the behavior of the model dynamically, you don't need to use a placeholder for is_training. The trick is to build the model twice, but sharing the variables the second time.", "body": "Although is_traning can a placeholder reuse has to be a bool, and it cannot be a tensor nor a placeholder.\r\n\r\nI'm not sure what are you trying to do, in most cases using static values solve the problem. For example this pattern works well:\r\n\r\n```\r\ndef model(data, is_training=False, reuse=None, scope='my_model'):\r\n  # Define a variable scope to contain all the variables of your model\r\n  with tf.variable_scope(scope, 'model', data, reuse=reuse):\r\n    # 1 layer\r\n    net = tf.contrib.layers.conv2d(data, ....)\r\n    ....\r\n    net = tf.contrib.layers.batch_norm(net, is_training)\r\n   return net\r\n\r\ntrain_outputs = model(train_data, is_training=True)\r\neval_outputs = model(eval_data, is_training=False, reuse=True)\r\n\r\neval_predictions = sess.run(eval_outputs, feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\r\n\r\n```\r\nUnless you need to change the behavior of the model dynamically, you don't need to use a placeholder for is_training. The trick is to build the model twice, but sharing the variables the second time. \r\n\r\n"}