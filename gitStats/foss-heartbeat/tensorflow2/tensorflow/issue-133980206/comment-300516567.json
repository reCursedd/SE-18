{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300516567", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-300516567", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 300516567, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDUxNjU2Nw==", "user": {"login": "raghavgoyal14", "id": 2176778, "node_id": "MDQ6VXNlcjIxNzY3Nzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2176778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raghavgoyal14", "html_url": "https://github.com/raghavgoyal14", "followers_url": "https://api.github.com/users/raghavgoyal14/followers", "following_url": "https://api.github.com/users/raghavgoyal14/following{/other_user}", "gists_url": "https://api.github.com/users/raghavgoyal14/gists{/gist_id}", "starred_url": "https://api.github.com/users/raghavgoyal14/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raghavgoyal14/subscriptions", "organizations_url": "https://api.github.com/users/raghavgoyal14/orgs", "repos_url": "https://api.github.com/users/raghavgoyal14/repos", "events_url": "https://api.github.com/users/raghavgoyal14/events{/privacy}", "received_events_url": "https://api.github.com/users/raghavgoyal14/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-10T15:20:05Z", "updated_at": "2017-05-10T18:01:46Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>Using the above <code>batch_norm</code> layer in <code>contrib.layers</code>, I'm getting <code>nan</code> as an output for validation graph while the train graph runs seamlessly. Is there anything that I might be missing ?</p>\n<p>I'm using:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">batchnormlayer</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">numout</span>, <span class=\"pl-smi\">train_model</span>):\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_norm<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> scope_bn:\n        epsilon <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1e-3</span>\n        <span class=\"pl-k\">return</span> tf.contrib.layers.batch_norm(inputs, <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>, <span class=\"pl-v\">updates_collections</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                            <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope_bn,\n                                            <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>train_model, <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span>epsilon,\n                                            <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span>scope_bn.reuse)</pre></div>\n<p>Thanks</p>", "body_text": "Hi,\nUsing the above batch_norm layer in contrib.layers, I'm getting nan as an output for validation graph while the train graph runs seamlessly. Is there anything that I might be missing ?\nI'm using:\ndef batchnormlayer(inputs, numout, train_model):\n    with tf.variable_scope(\"batch_norm\") as scope_bn:\n        epsilon = 1e-3\n        return tf.contrib.layers.batch_norm(inputs, decay=0.9, updates_collections=None,\n                                            scale=True, scope=scope_bn,\n                                            is_training=train_model, epsilon=epsilon,\n                                            fused=True, reuse=scope_bn.reuse)\nThanks", "body": "Hi,\r\n\r\nUsing the above `batch_norm` layer in `contrib.layers`, I'm getting `nan` as an output for validation graph while the train graph runs seamlessly. Is there anything that I might be missing ?\r\n\r\nI'm using:\r\n```python\r\ndef batchnormlayer(inputs, numout, train_model):\r\n    with tf.variable_scope(\"batch_norm\") as scope_bn:\r\n        epsilon = 1e-3\r\n        return tf.contrib.layers.batch_norm(inputs, decay=0.9, updates_collections=None,\r\n                                            scale=True, scope=scope_bn,\r\n                                            is_training=train_model, epsilon=epsilon,\r\n                                            fused=True, reuse=scope_bn.reuse)\r\n```\r\nThanks "}