{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/236068575", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-236068575", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 236068575, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNjA2ODU3NQ==", "user": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-29T01:04:51Z", "updated_at": "2016-07-29T01:04:51Z", "author_association": "MEMBER", "body_html": "<p>The problem before was that you were not updating the <code>moving_mean</code> and <code>moving_variance</code> after each step, when updates_collections is None it forces the updates as part of the computation.<br>\nHowever when a network has many batch_norm layers it is more efficient to collect all the update ops and run them together, so each layer don't need to wait for the update to finish.</p>\n<pre><code>y = build_model_with_batch_norm(x, is_training)\nupdate_ops = tf.group(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n\nsess.run([y, update_ops])\n\n</code></pre>", "body_text": "The problem before was that you were not updating the moving_mean and moving_variance after each step, when updates_collections is None it forces the updates as part of the computation.\nHowever when a network has many batch_norm layers it is more efficient to collect all the update ops and run them together, so each layer don't need to wait for the update to finish.\ny = build_model_with_batch_norm(x, is_training)\nupdate_ops = tf.group(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n\nsess.run([y, update_ops])", "body": "The problem before was that you were not updating the `moving_mean` and `moving_variance` after each step, when updates_collections is None it forces the updates as part of the computation.\nHowever when a network has many batch_norm layers it is more efficient to collect all the update ops and run them together, so each layer don't need to wait for the update to finish.\n\n```\ny = build_model_with_batch_norm(x, is_training)\nupdate_ops = tf.group(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n\nsess.run([y, update_ops])\n\n```\n"}