{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/290937634", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-290937634", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 290937634, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MDkzNzYzNA==", "user": {"login": "Alexivia", "id": 23476569, "node_id": "MDQ6VXNlcjIzNDc2NTY5", "avatar_url": "https://avatars1.githubusercontent.com/u/23476569?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Alexivia", "html_url": "https://github.com/Alexivia", "followers_url": "https://api.github.com/users/Alexivia/followers", "following_url": "https://api.github.com/users/Alexivia/following{/other_user}", "gists_url": "https://api.github.com/users/Alexivia/gists{/gist_id}", "starred_url": "https://api.github.com/users/Alexivia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Alexivia/subscriptions", "organizations_url": "https://api.github.com/users/Alexivia/orgs", "repos_url": "https://api.github.com/users/Alexivia/repos", "events_url": "https://api.github.com/users/Alexivia/events{/privacy}", "received_events_url": "https://api.github.com/users/Alexivia/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-01T18:17:58Z", "updated_at": "2017-04-01T18:17:58Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nI tried to implement a batch normalisation layer with the help of the suggestions in this issue, but I still have a &gt;70% error in validation and testing... I do have a lower decay for non-training calls...</p>\n<p>Here is my code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">BatchNorm</span>(<span class=\"pl-smi\">inputT</span>, <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-k\">return</span> tf.cond(\n    is_training,\n    <span class=\"pl-k\">lambda</span>: tf.contrib.layers.batch_norm(inputT, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,  <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.999</span>, <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-5</span>, <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">updates_collections</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope),\n    <span class=\"pl-k\">lambda</span>: tf.contrib.layers.batch_norm(inputT, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.900</span>, <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-5</span>, <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">updates_collections</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope)\n    )</pre></div>\n<p>Thank you in advance.</p>", "body_text": "Hi,\nI tried to implement a batch normalisation layer with the help of the suggestions in this issue, but I still have a >70% error in validation and testing... I do have a lower decay for non-training calls...\nHere is my code:\ndef BatchNorm(inputT, is_training=False, scope=None):\n  return tf.cond(\n    is_training,\n    lambda: tf.contrib.layers.batch_norm(inputT, is_training=True,  reuse=None, decay=0.999, epsilon=1e-5, center=True, scale=True, updates_collections=None, scope=scope),\n    lambda: tf.contrib.layers.batch_norm(inputT, is_training=False, reuse=True, decay=0.900, epsilon=1e-5, center=True, scale=True, updates_collections=None, scope=scope)\n    )\nThank you in advance.", "body": "Hi,\r\nI tried to implement a batch normalisation layer with the help of the suggestions in this issue, but I still have a >70% error in validation and testing... I do have a lower decay for non-training calls...\r\n\r\nHere is my code:\r\n```python\r\ndef BatchNorm(inputT, is_training=False, scope=None):\r\n  return tf.cond(\r\n    is_training,\r\n    lambda: tf.contrib.layers.batch_norm(inputT, is_training=True,  reuse=None, decay=0.999, epsilon=1e-5, center=True, scale=True, updates_collections=None, scope=scope),\r\n    lambda: tf.contrib.layers.batch_norm(inputT, is_training=False, reuse=True, decay=0.900, epsilon=1e-5, center=True, scale=True, updates_collections=None, scope=scope)\r\n    )\r\n```\r\n\r\nThank you in advance."}