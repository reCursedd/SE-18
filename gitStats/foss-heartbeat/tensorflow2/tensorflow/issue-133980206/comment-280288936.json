{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280288936", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-280288936", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 280288936, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDI4ODkzNg==", "user": {"login": "soloice", "id": 8534653, "node_id": "MDQ6VXNlcjg1MzQ2NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8534653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soloice", "html_url": "https://github.com/soloice", "followers_url": "https://api.github.com/users/soloice/followers", "following_url": "https://api.github.com/users/soloice/following{/other_user}", "gists_url": "https://api.github.com/users/soloice/gists{/gist_id}", "starred_url": "https://api.github.com/users/soloice/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soloice/subscriptions", "organizations_url": "https://api.github.com/users/soloice/orgs", "repos_url": "https://api.github.com/users/soloice/repos", "events_url": "https://api.github.com/users/soloice/events{/privacy}", "received_events_url": "https://api.github.com/users/soloice/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-16T10:09:57Z", "updated_at": "2017-02-16T10:11:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1766524\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sguada\">@sguada</a> Sorry for trouble you, but is it possible to make an example on how to use slim.batch_norm when combined with slim.conv2d/slim.fully_connect in readme.md?</p>\n<p>I'm using slim.batch_norm, but get good training performance and poor validation/test performance. I think it must be due to improper use of <code>reuse</code> or <code>scope</code> or some other parameters. Though there are many issues on batch normalization, it's hard to find a complete code snippet on how to use it, esp. for how to pass different parameters in different phase.</p>\n<p>Say, in my <a href=\"https://github.com/soloice/mnist-bn/blob/master/mnist_bn.py\">mnist_bn</a> code, I controlled dependencies using <code>tf.GraphKeys.UPDATE_OPS</code> and set up <code>is_training</code> as a placeholder. But validation performance still is poor if I feed {is_training: False}.</p>\n<p>I would greatly appreciate it if there's an official and complete (which means training, validating, testing are all included) batch normalization example.</p>\n<p>Thank you in advance!</p>", "body_text": "@sguada Sorry for trouble you, but is it possible to make an example on how to use slim.batch_norm when combined with slim.conv2d/slim.fully_connect in readme.md?\nI'm using slim.batch_norm, but get good training performance and poor validation/test performance. I think it must be due to improper use of reuse or scope or some other parameters. Though there are many issues on batch normalization, it's hard to find a complete code snippet on how to use it, esp. for how to pass different parameters in different phase.\nSay, in my mnist_bn code, I controlled dependencies using tf.GraphKeys.UPDATE_OPS and set up is_training as a placeholder. But validation performance still is poor if I feed {is_training: False}.\nI would greatly appreciate it if there's an official and complete (which means training, validating, testing are all included) batch normalization example.\nThank you in advance!", "body": "@sguada Sorry for trouble you, but is it possible to make an example on how to use slim.batch_norm when combined with slim.conv2d/slim.fully_connect in readme.md? \r\n\r\nI'm using slim.batch_norm, but get good training performance and poor validation/test performance. I think it must be due to improper use of `reuse` or `scope` or some other parameters. Though there are many issues on batch normalization, it's hard to find a complete code snippet on how to use it, esp. for how to pass different parameters in different phase.\r\n\r\nSay, in my [mnist_bn](https://github.com/soloice/mnist-bn/blob/master/mnist_bn.py) code, I controlled dependencies using `tf.GraphKeys.UPDATE_OPS` and set up `is_training` as a placeholder. But validation performance still is poor if I feed {is_training: False}.\r\n\r\nI would greatly appreciate it if there's an official and complete (which means training, validating, testing are all included) batch normalization example.\r\n\r\nThank you in advance!"}