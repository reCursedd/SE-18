{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280325584", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-280325584", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 280325584, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDMyNTU4NA==", "user": {"login": "soloice", "id": 8534653, "node_id": "MDQ6VXNlcjg1MzQ2NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8534653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soloice", "html_url": "https://github.com/soloice", "followers_url": "https://api.github.com/users/soloice/followers", "following_url": "https://api.github.com/users/soloice/following{/other_user}", "gists_url": "https://api.github.com/users/soloice/gists{/gist_id}", "starred_url": "https://api.github.com/users/soloice/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soloice/subscriptions", "organizations_url": "https://api.github.com/users/soloice/orgs", "repos_url": "https://api.github.com/users/soloice/repos", "events_url": "https://api.github.com/users/soloice/events{/privacy}", "received_events_url": "https://api.github.com/users/soloice/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-16T13:07:51Z", "updated_at": "2017-04-27T17:49:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=25502989\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ishaybee\">@ishaybee</a> Thanks for you help. I've found my problem= = <strong>It's due to the cold start of moving_mean/moving_variance.</strong></p>\n<p>Since I haven't trained enough steps, the estimated moving mean/variance is not that stable. The result turns out to be: the model performs pretty well on training mini-batches (you know at the beginning loss goes down quickly), but validation performance is erratic (because the estimated population mean/variance are not stable enough).</p>\n<p>When I trained the model longer, validation accuracy becomes prettier, too.</p>\n<p><strong>Another important thing is, be sure to use <code>slim.learning.create_train_op</code> to create train op</strong>. Do not use tf native <code>tf.train.GradientDescentOptimizer(0.1).minimize(loss)</code>.</p>\n<p>So the answer is, I'm using batch normalization correctly, but I haven't fully understood its dynamics during training.</p>\n<p>================<br>\nWhat's more:</p>\n<ol>\n<li><a href=\"https://github.com/soloice/mnist-bn\">Here is a full example</a> on how to use BN layer on MNIST dataset.</li>\n<li>Use a smaller decay value will accelerate the warm-up phase. The default decay is 0.999, for small datasets such like MNIST, you can choose 0.99 or 0.95, and it warms up in a short time.</li>\n</ol>", "body_text": "@ishaybee Thanks for you help. I've found my problem= = It's due to the cold start of moving_mean/moving_variance.\nSince I haven't trained enough steps, the estimated moving mean/variance is not that stable. The result turns out to be: the model performs pretty well on training mini-batches (you know at the beginning loss goes down quickly), but validation performance is erratic (because the estimated population mean/variance are not stable enough).\nWhen I trained the model longer, validation accuracy becomes prettier, too.\nAnother important thing is, be sure to use slim.learning.create_train_op to create train op. Do not use tf native tf.train.GradientDescentOptimizer(0.1).minimize(loss).\nSo the answer is, I'm using batch normalization correctly, but I haven't fully understood its dynamics during training.\n================\nWhat's more:\n\nHere is a full example on how to use BN layer on MNIST dataset.\nUse a smaller decay value will accelerate the warm-up phase. The default decay is 0.999, for small datasets such like MNIST, you can choose 0.99 or 0.95, and it warms up in a short time.", "body": "@ishaybee Thanks for you help. I've found my problem= = **It's due to the cold start of moving_mean/moving_variance.**\r\n\r\nSince I haven't trained enough steps, the estimated moving mean/variance is not that stable. The result turns out to be: the model performs pretty well on training mini-batches (you know at the beginning loss goes down quickly), but validation performance is erratic (because the estimated population mean/variance are not stable enough).\r\n\r\nWhen I trained the model longer, validation accuracy becomes prettier, too.\r\n\r\n**Another important thing is, be sure to use `slim.learning.create_train_op` to create train op**. Do not use tf native `tf.train.GradientDescentOptimizer(0.1).minimize(loss)`.\r\n\r\nSo the answer is, I'm using batch normalization correctly, but I haven't fully understood its dynamics during training.\r\n\r\n================\r\nWhat's more:\r\n1. [Here is a full example](https://github.com/soloice/mnist-bn) on how to use BN layer on MNIST dataset.\r\n2. Use a smaller decay value will accelerate the warm-up phase. The default decay is 0.999, for small datasets such like MNIST, you can choose 0.99 or 0.95, and it warms up in a short time."}