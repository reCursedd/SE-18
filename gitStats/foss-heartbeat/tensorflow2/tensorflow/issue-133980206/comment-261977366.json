{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/261977366", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-261977366", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 261977366, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTk3NzM2Ng==", "user": {"login": "dominikandreas", "id": 13525040, "node_id": "MDQ6VXNlcjEzNTI1MDQw", "avatar_url": "https://avatars2.githubusercontent.com/u/13525040?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dominikandreas", "html_url": "https://github.com/dominikandreas", "followers_url": "https://api.github.com/users/dominikandreas/followers", "following_url": "https://api.github.com/users/dominikandreas/following{/other_user}", "gists_url": "https://api.github.com/users/dominikandreas/gists{/gist_id}", "starred_url": "https://api.github.com/users/dominikandreas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dominikandreas/subscriptions", "organizations_url": "https://api.github.com/users/dominikandreas/orgs", "repos_url": "https://api.github.com/users/dominikandreas/repos", "events_url": "https://api.github.com/users/dominikandreas/events{/privacy}", "received_events_url": "https://api.github.com/users/dominikandreas/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-21T15:53:18Z", "updated_at": "2016-11-21T15:53:18Z", "author_association": "NONE", "body_html": "<p>Just wanted to note that I also have the problem of poor test performance, specifically using small batch sizes (anything smaller than 10 instead of the 200 I used for training diminishes test accuracy). I've used a tf.placeholder to switch between testing/training mode.</p>\n<p>It's great that this batch normalization layer works for better training convergence, but if you can't apply the model in production, there isn't much of a point in using it. Can anyone confirm good test performance with small or single data samples using this batch norm layer?</p>", "body_text": "Just wanted to note that I also have the problem of poor test performance, specifically using small batch sizes (anything smaller than 10 instead of the 200 I used for training diminishes test accuracy). I've used a tf.placeholder to switch between testing/training mode.\nIt's great that this batch normalization layer works for better training convergence, but if you can't apply the model in production, there isn't much of a point in using it. Can anyone confirm good test performance with small or single data samples using this batch norm layer?", "body": "Just wanted to note that I also have the problem of poor test performance, specifically using small batch sizes (anything smaller than 10 instead of the 200 I used for training diminishes test accuracy). I've used a tf.placeholder to switch between testing/training mode.\r\n\r\nIt's great that this batch normalization layer works for better training convergence, but if you can't apply the model in production, there isn't much of a point in using it. Can anyone confirm good test performance with small or single data samples using this batch norm layer? "}