{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/291242337", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-291242337", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 291242337, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MTI0MjMzNw==", "user": {"login": "Alexivia", "id": 23476569, "node_id": "MDQ6VXNlcjIzNDc2NTY5", "avatar_url": "https://avatars1.githubusercontent.com/u/23476569?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Alexivia", "html_url": "https://github.com/Alexivia", "followers_url": "https://api.github.com/users/Alexivia/followers", "following_url": "https://api.github.com/users/Alexivia/following{/other_user}", "gists_url": "https://api.github.com/users/Alexivia/gists{/gist_id}", "starred_url": "https://api.github.com/users/Alexivia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Alexivia/subscriptions", "organizations_url": "https://api.github.com/users/Alexivia/orgs", "repos_url": "https://api.github.com/users/Alexivia/repos", "events_url": "https://api.github.com/users/Alexivia/events{/privacy}", "received_events_url": "https://api.github.com/users/Alexivia/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-03T19:06:19Z", "updated_at": "2017-04-03T19:06:19Z", "author_association": "NONE", "body_html": "<p>My code is like this now:<br>\n<strong>Batch Normalisation wrapper</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">BatchNorm</span>(<span class=\"pl-smi\">inputT</span>, <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-k\">with</span> tf.variable_scope(scope):\n    <span class=\"pl-k\">return</span> tf.contrib.layers.batch_norm(inputT, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span>reuse, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope, <span class=\"pl-v\">updates_collections</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>, <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p><strong>Model definition</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">model</span>(<span class=\"pl-smi\">data</span>, <span class=\"pl-smi\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1st conv layer</span>\n  <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> scope:\n    conv <span class=\"pl-k\">=</span> tf.nn.conv2d(\n    <span class=\"pl-k\">&lt;</span><span class=\"pl-c1\">...</span><span class=\"pl-k\">&gt;</span>\n    <span class=\"pl-v\">norm</span> <span class=\"pl-k\">=</span> BatchNorm(pool, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span>reuse, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope)</pre></div>\n<p><strong>Training</strong></p>\n<div class=\"highlight highlight-source-python\"><pre>feed_dict <span class=\"pl-k\">=</span> {train_data_node: batch_data,\n      train_labels_node: batch_labels,\n      is_training: <span class=\"pl-c1\">True</span>,\n      reuse: <span class=\"pl-c1\">None</span>}\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the optimizer to update weights.</span>\n  sess.run(optimizer, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed_dict)</pre></div>\n<p><strong>Validation</strong></p>\n<div class=\"highlight highlight-source-python\"><pre>batch_predictions <span class=\"pl-k\">=</span> sess.run(eval_prediction, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{eval_data: data[<span class=\"pl-k\">-</span><span class=\"pl-c1\">EVAL_BATCH_SIZE</span>:, <span class=\"pl-c1\">...</span>], is_training: <span class=\"pl-c1\">False</span>, reuse: <span class=\"pl-c1\">True</span>})</pre></div>", "body_text": "My code is like this now:\nBatch Normalisation wrapper\ndef BatchNorm(inputT, is_training=False, reuse=None, scope=None):\n  with tf.variable_scope(scope):\n    return tf.contrib.layers.batch_norm(inputT, is_training=is_training, reuse=reuse, scope=scope, updates_collections=None, decay=0.9, center=True, scale=True)\nModel definition\ndef model(data, train=False, is_training=False, reuse=None):\n  # 1st conv layer\n  with tf.name_scope('conv1') as scope:\n    conv = tf.nn.conv2d(\n    <...>\n    norm = BatchNorm(pool, is_training=is_training, reuse=reuse, scope=scope)\nTraining\nfeed_dict = {train_data_node: batch_data,\n      train_labels_node: batch_labels,\n      is_training: True,\n      reuse: None}\n  # Run the optimizer to update weights.\n  sess.run(optimizer, feed_dict=feed_dict)\nValidation\nbatch_predictions = sess.run(eval_prediction, feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...], is_training: False, reuse: True})", "body": "My code is like this now:\r\n**Batch Normalisation wrapper**\r\n```python\r\ndef BatchNorm(inputT, is_training=False, reuse=None, scope=None):\r\n  with tf.variable_scope(scope):\r\n    return tf.contrib.layers.batch_norm(inputT, is_training=is_training, reuse=reuse, scope=scope, updates_collections=None, decay=0.9, center=True, scale=True)\r\n```\r\n**Model definition**\r\n```python\r\ndef model(data, train=False, is_training=False, reuse=None):\r\n  # 1st conv layer\r\n  with tf.name_scope('conv1') as scope:\r\n    conv = tf.nn.conv2d(\r\n    <...>\r\n    norm = BatchNorm(pool, is_training=is_training, reuse=reuse, scope=scope)\r\n```\r\n**Training**\r\n```python\r\nfeed_dict = {train_data_node: batch_data,\r\n      train_labels_node: batch_labels,\r\n      is_training: True,\r\n      reuse: None}\r\n  # Run the optimizer to update weights.\r\n  sess.run(optimizer, feed_dict=feed_dict)\r\n```\r\n**Validation**\r\n```python\r\nbatch_predictions = sess.run(eval_prediction, feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...], is_training: False, reuse: True})\r\n```"}