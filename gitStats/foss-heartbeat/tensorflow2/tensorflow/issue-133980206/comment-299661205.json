{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299661205", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-299661205", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 299661205, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTY2MTIwNQ==", "user": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-06T19:32:36Z", "updated_at": "2017-05-06T19:32:36Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6593422\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/OktayGardener\">@OktayGardener</a> not sure what model are you trying to create, it seems that the variables are not saved in your checkpoint.</p>\n<p>batch_norm also works with fully_connected layers.</p>\n<pre><code>slim = tf.contrib.slim\ndef model(data, is_training=False, reuse=None, scope='my_model'):\n  # Define a variable scope to contain all the variables of your model\n  with tf.variable_scope(scope, 'model', data, reuse=reuse):\n    # Configure arguments of fully_connected layers\n    with slim.arg_scope([slim.fully_connected],\n                        activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_nom):\n      # Configure arguments of batch_norm layers\n      with slim.arg_scope([slim.batch_norm],\n                          decay=0.9,  # Adjust decay to the number of iterations\n                          update_collections=None, # Make sure updates happen automatically\n                          is_training=is_training, # Switch behavior from training to non-training):\n        net = slim.fully_connected(data, 100, scope='fc1')\n        net = slim.fully_connected(net, 200, scope='fc2')\n        ....\n        # Don't use activation_fn nor batch_norm in the last layer        \n        net = slim.fully_connected(net, 10, activation_fn=None, normalizer_fn=None, scope='fc10')\n       return net\n</code></pre>", "body_text": "@OktayGardener not sure what model are you trying to create, it seems that the variables are not saved in your checkpoint.\nbatch_norm also works with fully_connected layers.\nslim = tf.contrib.slim\ndef model(data, is_training=False, reuse=None, scope='my_model'):\n  # Define a variable scope to contain all the variables of your model\n  with tf.variable_scope(scope, 'model', data, reuse=reuse):\n    # Configure arguments of fully_connected layers\n    with slim.arg_scope([slim.fully_connected],\n                        activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_nom):\n      # Configure arguments of batch_norm layers\n      with slim.arg_scope([slim.batch_norm],\n                          decay=0.9,  # Adjust decay to the number of iterations\n                          update_collections=None, # Make sure updates happen automatically\n                          is_training=is_training, # Switch behavior from training to non-training):\n        net = slim.fully_connected(data, 100, scope='fc1')\n        net = slim.fully_connected(net, 200, scope='fc2')\n        ....\n        # Don't use activation_fn nor batch_norm in the last layer        \n        net = slim.fully_connected(net, 10, activation_fn=None, normalizer_fn=None, scope='fc10')\n       return net", "body": "@OktayGardener not sure what model are you trying to create, it seems that the variables are not saved in your checkpoint.\r\n\r\nbatch_norm also works with fully_connected layers.\r\n\r\n```\r\nslim = tf.contrib.slim\r\ndef model(data, is_training=False, reuse=None, scope='my_model'):\r\n  # Define a variable scope to contain all the variables of your model\r\n  with tf.variable_scope(scope, 'model', data, reuse=reuse):\r\n    # Configure arguments of fully_connected layers\r\n    with slim.arg_scope([slim.fully_connected],\r\n                        activation_fn=tf.nn.relu,\r\n                        normalizer_fn=slim.batch_nom):\r\n      # Configure arguments of batch_norm layers\r\n      with slim.arg_scope([slim.batch_norm],\r\n                          decay=0.9,  # Adjust decay to the number of iterations\r\n                          update_collections=None, # Make sure updates happen automatically\r\n                          is_training=is_training, # Switch behavior from training to non-training):\r\n        net = slim.fully_connected(data, 100, scope='fc1')\r\n        net = slim.fully_connected(net, 200, scope='fc2')\r\n        ....\r\n        # Don't use activation_fn nor batch_norm in the last layer        \r\n        net = slim.fully_connected(net, 10, activation_fn=None, normalizer_fn=None, scope='fc10')\r\n       return net\r\n```\r\n\r\n\r\n"}