{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259782395", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-259782395", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 259782395, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTc4MjM5NQ==", "user": {"login": "zhongyuk", "id": 6901075, "node_id": "MDQ6VXNlcjY5MDEwNzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/6901075?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhongyuk", "html_url": "https://github.com/zhongyuk", "followers_url": "https://api.github.com/users/zhongyuk/followers", "following_url": "https://api.github.com/users/zhongyuk/following{/other_user}", "gists_url": "https://api.github.com/users/zhongyuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhongyuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhongyuk/subscriptions", "organizations_url": "https://api.github.com/users/zhongyuk/orgs", "repos_url": "https://api.github.com/users/zhongyuk/repos", "events_url": "https://api.github.com/users/zhongyuk/events{/privacy}", "received_events_url": "https://api.github.com/users/zhongyuk/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-10T19:22:39Z", "updated_at": "2016-11-10T19:22:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=172688\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/davek44\">@davek44</a> I'm using the same code framework that you are using and I observed the same thing: when turns on <code>is_training=True</code> during training phase and turns off <code>is_training=False</code> for validation and/or testing phase, the model trains well like the paper described (model converges faster and I was able to use a larger learning rate), however the testing performance is terrible. If I turns on <code>is_training=True</code> all the time, the model trains the same as without inserting batch norm layer. I haven't figured out what I did wrong, I'm planning to use TensorBoard to monitor the parameters. Would you please update if you diagnose the cause of this behavior?</p>", "body_text": "@davek44 I'm using the same code framework that you are using and I observed the same thing: when turns on is_training=True during training phase and turns off is_training=False for validation and/or testing phase, the model trains well like the paper described (model converges faster and I was able to use a larger learning rate), however the testing performance is terrible. If I turns on is_training=True all the time, the model trains the same as without inserting batch norm layer. I haven't figured out what I did wrong, I'm planning to use TensorBoard to monitor the parameters. Would you please update if you diagnose the cause of this behavior?", "body": "@davek44 I'm using the same code framework that you are using and I observed the same thing: when turns on `is_training=True` during training phase and turns off `is_training=False` for validation and/or testing phase, the model trains well like the paper described (model converges faster and I was able to use a larger learning rate), however the testing performance is terrible. If I turns on `is_training=True` all the time, the model trains the same as without inserting batch norm layer. I haven't figured out what I did wrong, I'm planning to use TensorBoard to monitor the parameters. Would you please update if you diagnose the cause of this behavior? \n"}