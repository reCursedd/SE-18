{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300821227", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-300821227", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 300821227, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDgyMTIyNw==", "user": {"login": "danrsc", "id": 11574457, "node_id": "MDQ6VXNlcjExNTc0NDU3", "avatar_url": "https://avatars0.githubusercontent.com/u/11574457?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danrsc", "html_url": "https://github.com/danrsc", "followers_url": "https://api.github.com/users/danrsc/followers", "following_url": "https://api.github.com/users/danrsc/following{/other_user}", "gists_url": "https://api.github.com/users/danrsc/gists{/gist_id}", "starred_url": "https://api.github.com/users/danrsc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danrsc/subscriptions", "organizations_url": "https://api.github.com/users/danrsc/orgs", "repos_url": "https://api.github.com/users/danrsc/repos", "events_url": "https://api.github.com/users/danrsc/events{/privacy}", "received_events_url": "https://api.github.com/users/danrsc/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-11T15:14:28Z", "updated_at": "2017-05-11T15:14:28Z", "author_association": "NONE", "body_html": "<p>I've just been noticing that if I kill the tensorflow process and restart it, my error gets worse for a few epochs (i.e. worse than it should be at the last checkpoint). I also observe that if I remove batch_norm, this problem goes away. After looking at the code for a while, I think this may be because the values of the variables are not restored from the shadow variables as they would be if the ExponentialMovingAverages class were used to manage the moving averages. This also means that if I use a separate process to evaluate, I'm getting whatever the last value of the variable was and not the moving average. Am I interpreting this correctly and is this the intended behavior? It seems like you want the shadow variable values to be restored...</p>", "body_text": "I've just been noticing that if I kill the tensorflow process and restart it, my error gets worse for a few epochs (i.e. worse than it should be at the last checkpoint). I also observe that if I remove batch_norm, this problem goes away. After looking at the code for a while, I think this may be because the values of the variables are not restored from the shadow variables as they would be if the ExponentialMovingAverages class were used to manage the moving averages. This also means that if I use a separate process to evaluate, I'm getting whatever the last value of the variable was and not the moving average. Am I interpreting this correctly and is this the intended behavior? It seems like you want the shadow variable values to be restored...", "body": "I've just been noticing that if I kill the tensorflow process and restart it, my error gets worse for a few epochs (i.e. worse than it should be at the last checkpoint). I also observe that if I remove batch_norm, this problem goes away. After looking at the code for a while, I think this may be because the values of the variables are not restored from the shadow variables as they would be if the ExponentialMovingAverages class were used to manage the moving averages. This also means that if I use a separate process to evaluate, I'm getting whatever the last value of the variable was and not the moving average. Am I interpreting this correctly and is this the intended behavior? It seems like you want the shadow variable values to be restored..."}