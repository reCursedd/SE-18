{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/350537403", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-350537403", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 350537403, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDUzNzQwMw==", "user": {"login": "ZahlGraf", "id": 10481491, "node_id": "MDQ6VXNlcjEwNDgxNDkx", "avatar_url": "https://avatars0.githubusercontent.com/u/10481491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZahlGraf", "html_url": "https://github.com/ZahlGraf", "followers_url": "https://api.github.com/users/ZahlGraf/followers", "following_url": "https://api.github.com/users/ZahlGraf/following{/other_user}", "gists_url": "https://api.github.com/users/ZahlGraf/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZahlGraf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZahlGraf/subscriptions", "organizations_url": "https://api.github.com/users/ZahlGraf/orgs", "repos_url": "https://api.github.com/users/ZahlGraf/repos", "events_url": "https://api.github.com/users/ZahlGraf/events{/privacy}", "received_events_url": "https://api.github.com/users/ZahlGraf/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-10T10:05:16Z", "updated_at": "2017-12-10T10:05:16Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15737127\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vincentvanhoucke\">@vincentvanhoucke</a> You wrote in another post in this thread:</p>\n<blockquote>\n<p>The slim batch_norm wrapper normalizes over the last dimension of your input tensor. So if it's a 2D input tensor coming from a fully connected layer, it normalizes over batch, and thus performs per-activation normalization. If it's a 4D tensor coming from a convolution, it will normalize over the three first dimensions (batch, width, depth), and thus perform per-feature normalization. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1766524\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sguada\">@sguada</a> maybe forth being a bit more descriptive about this.</p>\n</blockquote>\n<p>Do you mean with \"slim batch_norm wrapper\" the function <code>tf.contrib.layers.batch_norm</code>? If so, I would suggest to add this information to the documentation text of this function. Thus it gets very clear, that this function performs the batch normalization exactly like described in the paper... for both FC-Layer and Conv2D-Layer. At the moment there is only the text \"Can be used as a normalizer function for conv2d and fully_connected.\", where it is not clear if this is related to the normalization axis topic.</p>", "body_text": "@vincentvanhoucke You wrote in another post in this thread:\n\nThe slim batch_norm wrapper normalizes over the last dimension of your input tensor. So if it's a 2D input tensor coming from a fully connected layer, it normalizes over batch, and thus performs per-activation normalization. If it's a 4D tensor coming from a convolution, it will normalize over the three first dimensions (batch, width, depth), and thus perform per-feature normalization. @sguada maybe forth being a bit more descriptive about this.\n\nDo you mean with \"slim batch_norm wrapper\" the function tf.contrib.layers.batch_norm? If so, I would suggest to add this information to the documentation text of this function. Thus it gets very clear, that this function performs the batch normalization exactly like described in the paper... for both FC-Layer and Conv2D-Layer. At the moment there is only the text \"Can be used as a normalizer function for conv2d and fully_connected.\", where it is not clear if this is related to the normalization axis topic.", "body": "@vincentvanhoucke You wrote in another post in this thread: \r\n\r\n> The slim batch_norm wrapper normalizes over the last dimension of your input tensor. So if it's a 2D input tensor coming from a fully connected layer, it normalizes over batch, and thus performs per-activation normalization. If it's a 4D tensor coming from a convolution, it will normalize over the three first dimensions (batch, width, depth), and thus perform per-feature normalization. @sguada maybe forth being a bit more descriptive about this.\r\n\r\nDo you mean with \"slim batch_norm wrapper\" the function `tf.contrib.layers.batch_norm`? If so, I would suggest to add this information to the documentation text of this function. Thus it gets very clear, that this function performs the batch normalization exactly like described in the paper... for both FC-Layer and Conv2D-Layer. At the moment there is only the text \"Can be used as a normalizer function for conv2d and fully_connected.\", where it is not clear if this is related to the normalization axis topic."}