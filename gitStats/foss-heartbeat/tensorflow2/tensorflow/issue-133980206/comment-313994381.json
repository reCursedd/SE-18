{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313994381", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-313994381", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 313994381, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzk5NDM4MQ==", "user": {"login": "zmlmanly", "id": 22173241, "node_id": "MDQ6VXNlcjIyMTczMjQx", "avatar_url": "https://avatars1.githubusercontent.com/u/22173241?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zmlmanly", "html_url": "https://github.com/zmlmanly", "followers_url": "https://api.github.com/users/zmlmanly/followers", "following_url": "https://api.github.com/users/zmlmanly/following{/other_user}", "gists_url": "https://api.github.com/users/zmlmanly/gists{/gist_id}", "starred_url": "https://api.github.com/users/zmlmanly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zmlmanly/subscriptions", "organizations_url": "https://api.github.com/users/zmlmanly/orgs", "repos_url": "https://api.github.com/users/zmlmanly/repos", "events_url": "https://api.github.com/users/zmlmanly/events{/privacy}", "received_events_url": "https://api.github.com/users/zmlmanly/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-10T03:40:08Z", "updated_at": "2017-07-10T03:40:08Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1766524\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sguada\">@sguada</a>  Hi, sguada, I have a problem.<br>\nThe definition of contrib.layers.batch_norm in tensorflow:<br>\ndef batch_norm(inputs,<br>\ndecay=0.999,<br>\ncenter=True,<br>\nscale=False,<br>\nepsilon=0.001,<br>\nactivation_fn=None,<br>\nparam_initializers=None,<br>\nparam_regularizers=None,<br>\nupdates_collections=ops.GraphKeys.UPDATE_OPS,<br>\nis_training=True,<br>\nreuse=None,<br>\nvariables_collections=None,<br>\noutputs_collections=None,<br>\ntrainable=True,<br>\nbatch_weights=None,<br>\nfused=False,<br>\ndata_format=DATA_FORMAT_NHWC,<br>\nzero_debias_moving_mean=False,<br>\nscope=None,<br>\nrenorm=False,<br>\nrenorm_clipping=None,<br>\nrenorm_decay=0.99):<br>\nscale: If True, multiply by gamma. If False, gamma is<br>\nnot used. When the next layer is linear (also e.g. nn.relu), this can be<br>\ndisabled since the scaling can be done by the next layer.</p>\n<p>If I use tf.contrib.layers.batch_norm(input, scale=False) , the\"scale =False\" means whether the gamma is zero in \"y = gamma*x+beta\" while training. Thank you very much.</p>", "body_text": "@sguada  Hi, sguada, I have a problem.\nThe definition of contrib.layers.batch_norm in tensorflow:\ndef batch_norm(inputs,\ndecay=0.999,\ncenter=True,\nscale=False,\nepsilon=0.001,\nactivation_fn=None,\nparam_initializers=None,\nparam_regularizers=None,\nupdates_collections=ops.GraphKeys.UPDATE_OPS,\nis_training=True,\nreuse=None,\nvariables_collections=None,\noutputs_collections=None,\ntrainable=True,\nbatch_weights=None,\nfused=False,\ndata_format=DATA_FORMAT_NHWC,\nzero_debias_moving_mean=False,\nscope=None,\nrenorm=False,\nrenorm_clipping=None,\nrenorm_decay=0.99):\nscale: If True, multiply by gamma. If False, gamma is\nnot used. When the next layer is linear (also e.g. nn.relu), this can be\ndisabled since the scaling can be done by the next layer.\nIf I use tf.contrib.layers.batch_norm(input, scale=False) , the\"scale =False\" means whether the gamma is zero in \"y = gamma*x+beta\" while training. Thank you very much.", "body": "@sguada  Hi, sguada, I have a problem.\r\nThe definition of contrib.layers.batch_norm in tensorflow:\r\ndef batch_norm(inputs,\r\ndecay=0.999,\r\ncenter=True,\r\nscale=False,\r\nepsilon=0.001,\r\nactivation_fn=None,\r\nparam_initializers=None,\r\nparam_regularizers=None,\r\nupdates_collections=ops.GraphKeys.UPDATE_OPS,\r\nis_training=True,\r\nreuse=None,\r\nvariables_collections=None,\r\noutputs_collections=None,\r\ntrainable=True,\r\nbatch_weights=None,\r\nfused=False,\r\ndata_format=DATA_FORMAT_NHWC,\r\nzero_debias_moving_mean=False,\r\nscope=None,\r\nrenorm=False,\r\nrenorm_clipping=None,\r\nrenorm_decay=0.99):\r\nscale: If True, multiply by gamma. If False, gamma is\r\nnot used. When the next layer is linear (also e.g. nn.relu), this can be\r\ndisabled since the scaling can be done by the next layer.\r\n\r\nIf I use tf.contrib.layers.batch_norm(input, scale=False) , the\"scale =False\" means whether the gamma is zero in \"y = gamma*x+beta\" while training. Thank you very much."}