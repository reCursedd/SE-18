{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/234408791", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-234408791", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 234408791, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDQwODc5MQ==", "user": {"login": "diegoAtAlpine", "id": 19438656, "node_id": "MDQ6VXNlcjE5NDM4NjU2", "avatar_url": "https://avatars1.githubusercontent.com/u/19438656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/diegoAtAlpine", "html_url": "https://github.com/diegoAtAlpine", "followers_url": "https://api.github.com/users/diegoAtAlpine/followers", "following_url": "https://api.github.com/users/diegoAtAlpine/following{/other_user}", "gists_url": "https://api.github.com/users/diegoAtAlpine/gists{/gist_id}", "starred_url": "https://api.github.com/users/diegoAtAlpine/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/diegoAtAlpine/subscriptions", "organizations_url": "https://api.github.com/users/diegoAtAlpine/orgs", "repos_url": "https://api.github.com/users/diegoAtAlpine/repos", "events_url": "https://api.github.com/users/diegoAtAlpine/events{/privacy}", "received_events_url": "https://api.github.com/users/diegoAtAlpine/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-21T22:55:26Z", "updated_at": "2016-07-25T15:16:00Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16869368\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nmhkahn\">@nmhkahn</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2412413\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pawni\">@pawni</a> thanks for the code snippets. They were very useful in adding batch normalization to my convolution network.  Training seems to work very well. Testing is not.  In some versions of the code training accuracies are much higher than testing accuracies, which probably mean I am not sharing batch normalization parameters.  In other versions of the code I get \"ValueError: Variable conv1/beta already exists, disallowed. Did you mean to set reuse=True in VarScope?\" which seem to indicate that I am trying to relearn the parameter... when I was trying to reuse.</p>\n<p>Can someone provide an example of how to call the \"def BatchNorm\" function during training and testing so that variable sharing happen correctly.</p>\n<p>Thanks for any help.</p>\n<p>UPDATE July 25, 2016:</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16869368\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nmhkahn\">@nmhkahn</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2412413\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pawni\">@pawni</a> thanks for your comments.  After taking a closer look at the code in contrib I realized what my problem was.  During training and testing we are either updating or reusing four variables (beta, gamma, moving_mean and moving_variance). To make those unique I had to set a scope per layer.  I did it like this:</p>\n<p>conv1 = tf.nn.relu(batch_norm_layer(conv2d_stride2_valid(data, W_conv1) + b_conv1, train_phase, scope=\"conv1\"))</p>\n<p>where batch_norm_layer is similar to the examples from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16869368\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nmhkahn\">@nmhkahn</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2412413\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pawni\">@pawni</a>, conv2d_stride2_valid is just a def to define a convolutional layer, and W_conv1 and b_conv1 are variables holding the weights and biases. I could probably remove the bias term because we are using batch normalization.</p>\n<p>The net is working well now.  I noticed after plotting accuracies in training and test mode that the testing accuracies start climbing after the training accuracies.  In retrospect it make sense since we are collecting dataset statistics for testing.  But it appeared as if I was doing something wrong during my initial tests. Thanks for your comments and making batch normalization available to the community.</p>", "body_text": "@nmhkahn @pawni thanks for the code snippets. They were very useful in adding batch normalization to my convolution network.  Training seems to work very well. Testing is not.  In some versions of the code training accuracies are much higher than testing accuracies, which probably mean I am not sharing batch normalization parameters.  In other versions of the code I get \"ValueError: Variable conv1/beta already exists, disallowed. Did you mean to set reuse=True in VarScope?\" which seem to indicate that I am trying to relearn the parameter... when I was trying to reuse.\nCan someone provide an example of how to call the \"def BatchNorm\" function during training and testing so that variable sharing happen correctly.\nThanks for any help.\nUPDATE July 25, 2016:\n@nmhkahn @pawni thanks for your comments.  After taking a closer look at the code in contrib I realized what my problem was.  During training and testing we are either updating or reusing four variables (beta, gamma, moving_mean and moving_variance). To make those unique I had to set a scope per layer.  I did it like this:\nconv1 = tf.nn.relu(batch_norm_layer(conv2d_stride2_valid(data, W_conv1) + b_conv1, train_phase, scope=\"conv1\"))\nwhere batch_norm_layer is similar to the examples from @nmhkahn @pawni, conv2d_stride2_valid is just a def to define a convolutional layer, and W_conv1 and b_conv1 are variables holding the weights and biases. I could probably remove the bias term because we are using batch normalization.\nThe net is working well now.  I noticed after plotting accuracies in training and test mode that the testing accuracies start climbing after the training accuracies.  In retrospect it make sense since we are collecting dataset statistics for testing.  But it appeared as if I was doing something wrong during my initial tests. Thanks for your comments and making batch normalization available to the community.", "body": "@nmhkahn @pawni thanks for the code snippets. They were very useful in adding batch normalization to my convolution network.  Training seems to work very well. Testing is not.  In some versions of the code training accuracies are much higher than testing accuracies, which probably mean I am not sharing batch normalization parameters.  In other versions of the code I get \"ValueError: Variable conv1/beta already exists, disallowed. Did you mean to set reuse=True in VarScope?\" which seem to indicate that I am trying to relearn the parameter... when I was trying to reuse.\n\nCan someone provide an example of how to call the \"def BatchNorm\" function during training and testing so that variable sharing happen correctly.\n\nThanks for any help.\n\nUPDATE July 25, 2016:\n\n@nmhkahn @pawni thanks for your comments.  After taking a closer look at the code in contrib I realized what my problem was.  During training and testing we are either updating or reusing four variables (beta, gamma, moving_mean and moving_variance). To make those unique I had to set a scope per layer.  I did it like this:\n\nconv1 = tf.nn.relu(batch_norm_layer(conv2d_stride2_valid(data, W_conv1) + b_conv1, train_phase, scope=\"conv1\"))\n\nwhere batch_norm_layer is similar to the examples from @nmhkahn @pawni, conv2d_stride2_valid is just a def to define a convolutional layer, and W_conv1 and b_conv1 are variables holding the weights and biases. I could probably remove the bias term because we are using batch normalization.\n\nThe net is working well now.  I noticed after plotting accuracies in training and test mode that the testing accuracies start climbing after the training accuracies.  In retrospect it make sense since we are collecting dataset statistics for testing.  But it appeared as if I was doing something wrong during my initial tests. Thanks for your comments and making batch normalization available to the community.\n"}