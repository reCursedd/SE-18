{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317355258", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-317355258", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 317355258, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzM1NTI1OA==", "user": {"login": "MisayaZ", "id": 16910475, "node_id": "MDQ6VXNlcjE2OTEwNDc1", "avatar_url": "https://avatars2.githubusercontent.com/u/16910475?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MisayaZ", "html_url": "https://github.com/MisayaZ", "followers_url": "https://api.github.com/users/MisayaZ/followers", "following_url": "https://api.github.com/users/MisayaZ/following{/other_user}", "gists_url": "https://api.github.com/users/MisayaZ/gists{/gist_id}", "starred_url": "https://api.github.com/users/MisayaZ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MisayaZ/subscriptions", "organizations_url": "https://api.github.com/users/MisayaZ/orgs", "repos_url": "https://api.github.com/users/MisayaZ/repos", "events_url": "https://api.github.com/users/MisayaZ/events{/privacy}", "received_events_url": "https://api.github.com/users/MisayaZ/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-24T08:34:08Z", "updated_at": "2017-07-24T08:38:18Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=18667639\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tano297\">@tano297</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1835958\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/abred\">@abred</a>  you right. The moving mean and moving variance are changed when I used batchnorm like this:</p>\n<pre><code>def batch_norm_layer(self, x,train_phase, scope_bn):\n        bn_train = batch_norm(x, decay=0.9, center=False, scale=True,\n        updates_collections=None,\n        is_training=True,\n        reuse=None,\n        variables_collections= [UPDATE_OPS_COLLECTION],\n        trainable=True,\n        scope=scope_bn)\n        bn_inference = batch_norm(x, decay=0.9, center=False, scale=True,\n        updates_collections=None,\n        is_training=False,\n        reuse=True,\n        variables_collections= [UPDATE_OPS_COLLECTION],\n        trainable=True,\n        scope=scope_bn)\n        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n        return z\n</code></pre>\n<p>If you use like following:</p>\n<pre><code>z = batch_norm(x, decay=0.9, center=False, scale=True, updates_collections=None, \n                         is_training=train_phase, scope=scope_bn)\n</code></pre>\n<p>The moving mean and moving variance will not be changed during test, but the speed is very slow.</p>", "body_text": "@tano297 @abred  you right. The moving mean and moving variance are changed when I used batchnorm like this:\ndef batch_norm_layer(self, x,train_phase, scope_bn):\n        bn_train = batch_norm(x, decay=0.9, center=False, scale=True,\n        updates_collections=None,\n        is_training=True,\n        reuse=None,\n        variables_collections= [UPDATE_OPS_COLLECTION],\n        trainable=True,\n        scope=scope_bn)\n        bn_inference = batch_norm(x, decay=0.9, center=False, scale=True,\n        updates_collections=None,\n        is_training=False,\n        reuse=True,\n        variables_collections= [UPDATE_OPS_COLLECTION],\n        trainable=True,\n        scope=scope_bn)\n        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n        return z\n\nIf you use like following:\nz = batch_norm(x, decay=0.9, center=False, scale=True, updates_collections=None, \n                         is_training=train_phase, scope=scope_bn)\n\nThe moving mean and moving variance will not be changed during test, but the speed is very slow.", "body": "@tano297 @abred  you right. The moving mean and moving variance are changed when I used batchnorm like this:\r\n\r\n    def batch_norm_layer(self, x,train_phase, scope_bn):\r\n            bn_train = batch_norm(x, decay=0.9, center=False, scale=True,\r\n            updates_collections=None,\r\n            is_training=True,\r\n            reuse=None,\r\n            variables_collections= [UPDATE_OPS_COLLECTION],\r\n            trainable=True,\r\n            scope=scope_bn)\r\n            bn_inference = batch_norm(x, decay=0.9, center=False, scale=True,\r\n            updates_collections=None,\r\n            is_training=False,\r\n            reuse=True,\r\n            variables_collections= [UPDATE_OPS_COLLECTION],\r\n            trainable=True,\r\n            scope=scope_bn)\r\n            z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\r\n            return z\r\n\r\nIf you use like following:\r\n\r\n    z = batch_norm(x, decay=0.9, center=False, scale=True, updates_collections=None, \r\n                             is_training=train_phase, scope=scope_bn)\r\n\r\n\r\nThe moving mean and moving variance will not be changed during test, but the speed is very slow."}