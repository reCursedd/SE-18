{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/301046558", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-301046558", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 301046558, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTA0NjU1OA==", "user": {"login": "raghavgoyal14", "id": 2176778, "node_id": "MDQ6VXNlcjIxNzY3Nzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2176778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raghavgoyal14", "html_url": "https://github.com/raghavgoyal14", "followers_url": "https://api.github.com/users/raghavgoyal14/followers", "following_url": "https://api.github.com/users/raghavgoyal14/following{/other_user}", "gists_url": "https://api.github.com/users/raghavgoyal14/gists{/gist_id}", "starred_url": "https://api.github.com/users/raghavgoyal14/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raghavgoyal14/subscriptions", "organizations_url": "https://api.github.com/users/raghavgoyal14/orgs", "repos_url": "https://api.github.com/users/raghavgoyal14/repos", "events_url": "https://api.github.com/users/raghavgoyal14/events{/privacy}", "received_events_url": "https://api.github.com/users/raghavgoyal14/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-12T10:52:49Z", "updated_at": "2017-05-12T10:52:49Z", "author_association": "NONE", "body_html": "<p>I caught the problem, the moving variance in my case goes negative after some iterations.</p>\n<p>The output of the tensor : <code>Model/clip_logits/batch_norm/moving_variance:0</code> present in <code>tf.model_variables()</code> is</p>\n<pre><code>Moving variance (shape = (101,)) = \n[ 214.70379639   95.36338043    0.57885742  189.49542236  102.72473145\n  137.14886475  286.57333374  111.06427002  154.98750305  167.75219727\n  207.83955383  211.14007568  158.23495483  171.61665344  116.81361389\n  115.77380371   43.59399796  137.75064087  181.75245667  161.37339783\n  215.21934509   92.88521576  191.23846436  336.3946228   259.85919189\n  299.47039795  186.23222351  165.19311523  262.82446289  170.11567688\n  233.56843567  209.35050964  115.96807861  154.34109497  295.5770874\n  123.6055603   295.76187134  296.88583374  240.88217163  247.32983398\n   87.15661621  217.69897461  133.00698853   -4.80375671  344.77462769\n  291.50601196  117.77174377  265.83712769  207.90093994  194.186203\n  220.21418762  178.03738403  115.27571869  196.62184143  228.8089447\n  191.53205872  331.36807251  151.55435181  197.2951355   179.67504883\n  181.09727478   90.09922791  173.30133057  102.6836853   160.9434967\n  236.59512329  168.05305481  403.36340332   41.14326096  185.93409729\n  130.57434082  266.31509399  101.44387817  163.88059998  290.25015259\n  244.52597046  229.86647034  158.14352417  202.68774414  187.78227234\n  248.78218079  126.0978241   171.41891479  274.40740967  119.84254456\n  202.53045654  200.20608521  214.04730225  111.53284454  222.03184509\n  244.81187439  172.23052979  187.09806824  194.62802124  255.26345825\n  293.63598633  307.91036987  210.86982727  308.88919067  144.94792175\n  229.69013977]\n</code></pre>\n<p>As you can see, there's negative variance for one of the dimension. How is this even possible ?<br>\nP.S. The batch norm layer is used just after the last fully connected layer of the network and before softmax.</p>", "body_text": "I caught the problem, the moving variance in my case goes negative after some iterations.\nThe output of the tensor : Model/clip_logits/batch_norm/moving_variance:0 present in tf.model_variables() is\nMoving variance (shape = (101,)) = \n[ 214.70379639   95.36338043    0.57885742  189.49542236  102.72473145\n  137.14886475  286.57333374  111.06427002  154.98750305  167.75219727\n  207.83955383  211.14007568  158.23495483  171.61665344  116.81361389\n  115.77380371   43.59399796  137.75064087  181.75245667  161.37339783\n  215.21934509   92.88521576  191.23846436  336.3946228   259.85919189\n  299.47039795  186.23222351  165.19311523  262.82446289  170.11567688\n  233.56843567  209.35050964  115.96807861  154.34109497  295.5770874\n  123.6055603   295.76187134  296.88583374  240.88217163  247.32983398\n   87.15661621  217.69897461  133.00698853   -4.80375671  344.77462769\n  291.50601196  117.77174377  265.83712769  207.90093994  194.186203\n  220.21418762  178.03738403  115.27571869  196.62184143  228.8089447\n  191.53205872  331.36807251  151.55435181  197.2951355   179.67504883\n  181.09727478   90.09922791  173.30133057  102.6836853   160.9434967\n  236.59512329  168.05305481  403.36340332   41.14326096  185.93409729\n  130.57434082  266.31509399  101.44387817  163.88059998  290.25015259\n  244.52597046  229.86647034  158.14352417  202.68774414  187.78227234\n  248.78218079  126.0978241   171.41891479  274.40740967  119.84254456\n  202.53045654  200.20608521  214.04730225  111.53284454  222.03184509\n  244.81187439  172.23052979  187.09806824  194.62802124  255.26345825\n  293.63598633  307.91036987  210.86982727  308.88919067  144.94792175\n  229.69013977]\n\nAs you can see, there's negative variance for one of the dimension. How is this even possible ?\nP.S. The batch norm layer is used just after the last fully connected layer of the network and before softmax.", "body": "I caught the problem, the moving variance in my case goes negative after some iterations.\r\n\r\nThe output of the tensor : `Model/clip_logits/batch_norm/moving_variance:0` present in `tf.model_variables()` is \r\n```\r\nMoving variance (shape = (101,)) = \r\n[ 214.70379639   95.36338043    0.57885742  189.49542236  102.72473145\r\n  137.14886475  286.57333374  111.06427002  154.98750305  167.75219727\r\n  207.83955383  211.14007568  158.23495483  171.61665344  116.81361389\r\n  115.77380371   43.59399796  137.75064087  181.75245667  161.37339783\r\n  215.21934509   92.88521576  191.23846436  336.3946228   259.85919189\r\n  299.47039795  186.23222351  165.19311523  262.82446289  170.11567688\r\n  233.56843567  209.35050964  115.96807861  154.34109497  295.5770874\r\n  123.6055603   295.76187134  296.88583374  240.88217163  247.32983398\r\n   87.15661621  217.69897461  133.00698853   -4.80375671  344.77462769\r\n  291.50601196  117.77174377  265.83712769  207.90093994  194.186203\r\n  220.21418762  178.03738403  115.27571869  196.62184143  228.8089447\r\n  191.53205872  331.36807251  151.55435181  197.2951355   179.67504883\r\n  181.09727478   90.09922791  173.30133057  102.6836853   160.9434967\r\n  236.59512329  168.05305481  403.36340332   41.14326096  185.93409729\r\n  130.57434082  266.31509399  101.44387817  163.88059998  290.25015259\r\n  244.52597046  229.86647034  158.14352417  202.68774414  187.78227234\r\n  248.78218079  126.0978241   171.41891479  274.40740967  119.84254456\r\n  202.53045654  200.20608521  214.04730225  111.53284454  222.03184509\r\n  244.81187439  172.23052979  187.09806824  194.62802124  255.26345825\r\n  293.63598633  307.91036987  210.86982727  308.88919067  144.94792175\r\n  229.69013977]\r\n```\r\nAs you can see, there's negative variance for one of the dimension. How is this even possible ?\r\nP.S. The batch norm layer is used just after the last fully connected layer of the network and before softmax."}