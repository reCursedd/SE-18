{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/329943038", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-329943038", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 329943038, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTk0MzAzOA==", "user": {"login": "zhimengfan1990", "id": 12429412, "node_id": "MDQ6VXNlcjEyNDI5NDEy", "avatar_url": "https://avatars3.githubusercontent.com/u/12429412?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhimengfan1990", "html_url": "https://github.com/zhimengfan1990", "followers_url": "https://api.github.com/users/zhimengfan1990/followers", "following_url": "https://api.github.com/users/zhimengfan1990/following{/other_user}", "gists_url": "https://api.github.com/users/zhimengfan1990/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhimengfan1990/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhimengfan1990/subscriptions", "organizations_url": "https://api.github.com/users/zhimengfan1990/orgs", "repos_url": "https://api.github.com/users/zhimengfan1990/repos", "events_url": "https://api.github.com/users/zhimengfan1990/events{/privacy}", "received_events_url": "https://api.github.com/users/zhimengfan1990/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-16T03:51:25Z", "updated_at": "2017-09-16T04:04:29Z", "author_association": "NONE", "body_html": "<p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>\nMAYBE YOU NEED READ THIS<br>\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p>\n<p>It seems there are still problems with TF v1.3. I'm sure I note the following details, but still failed to use the official <code>tf.contrib.layers.batch_norm</code>, with <code>is_training=False</code> during evaluation(but when I keep <code>is_training=True</code> unchanged during evaluation, it is ok):<br>\n1.<code> decay</code>,  exponential moving average is actually alpha filter in signal processing, the time to converge is approximately 1/(1-decay) steps of train. For decay=0.999, you need 1/0.001=1000 steps to converge. So set the appropriate decay for your training step numbers.<br>\n2. using placeholder to switch between train and test evaluation<br>\n3. use<code> updates_collections=None</code> if you don't want to add control dependencies of update op to train_op<br>\n4. set <code>reuse</code> to appropriate value.</p>\n<p>It seems the only way to use the official batch_norm is to build two graphs, one for train and one for evaluation, with <code>is_training=True</code> and <code>is_training=False</code>, respectively. In this way, you don't need to switch dynamically between train and evaluation. But this is a stupid way since you need to build more than one graph.</p>\n<p>Finally, I write a moving average by myself, and I find it worked! It's as follows(based on code on the web and modified by myself)</p>\n<pre><code>def bn_layer(x, scope, is_training, epsilon=0.001, decay=0.99, reuse=None):\n    \"\"\"\n    Performs a batch normalization layer\n\n    Args:\n        x: input tensor\n        scope: scope name\n        is_training: python boolean value\n        epsilon: the variance epsilon - a small float number to avoid dividing by 0\n        decay: the moving average decay\n\n    Returns:\n        The ops of a batch normalization layer\n    \"\"\"\n    with tf.variable_scope(scope, reuse=reuse):\n        shape = x.get_shape().as_list()\n        # gamma: a trainable scale factor\n        gamma = tf.get_variable(\"gamma\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=True)\n        # beta: a trainable shift value\n        beta = tf.get_variable(\"beta\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=True)\n        moving_avg = tf.get_variable(\"moving_avg\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\n        moving_var = tf.get_variable(\"moving_var\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\n        if is_training:\n            # tf.nn.moments == Calculate the mean and the variance of the tensor x\n            avg, var = tf.nn.moments(x, np.arange(len(shape)-1), keep_dims=True)\n            avg=tf.reshape(avg, [avg.shape.as_list()[-1]])\n            var=tf.reshape(var, [var.shape.as_list()[-1]])\n            #update_moving_avg = moving_averages.assign_moving_average(moving_avg, avg, decay)\n            update_moving_avg=tf.assign(moving_avg, moving_avg*decay+avg*(1-decay))\n            #update_moving_var = moving_averages.assign_moving_average(moving_var, var, decay)\n            update_moving_var=tf.assign(moving_var, moving_var*decay+var*(1-decay))\n            control_inputs = [update_moving_avg, update_moving_var]\n        else:\n            avg = moving_avg\n            var = moving_var\n            control_inputs = []\n        with tf.control_dependencies(control_inputs):\n            output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\n\n    return output\n\n\ndef bn_layer_top(x, scope, is_training, epsilon=0.001, decay=0.99):\n    \"\"\"\n    Returns a batch normalization layer that automatically switch between train and test phases based on the \n    tensor is_training\n\n    Args:\n        x: input tensor\n        scope: scope name\n        is_training: boolean tensor or variable\n        epsilon: epsilon parameter - see batch_norm_layer\n        decay: epsilon parameter - see batch_norm_layer\n\n    Returns:\n        The correct batch normalization layer based on the value of is_training\n    \"\"\"\n    #assert isinstance(is_training, (ops.Tensor, variables.Variable)) and is_training.dtype == tf.bool\n\n    return tf.cond(\n        is_training,\n        lambda: bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=True, reuse=None),\n        lambda: bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=False, reuse=True),\n    )\n</code></pre>\n<p>Just use the <code>bn_layer_top</code> function during building a graph, the is_training parameter is a <code>tf.placeholder</code><br>\n. Then you are free to switch the placeholder to True during train and False during evaluation, with <code>feed_dict</code>.</p>\n<p>Hope it helps the community.</p>", "body_text": "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nMAYBE YOU NEED READ THIS\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nIt seems there are still problems with TF v1.3. I'm sure I note the following details, but still failed to use the official tf.contrib.layers.batch_norm, with is_training=False during evaluation(but when I keep is_training=True unchanged during evaluation, it is ok):\n1. decay,  exponential moving average is actually alpha filter in signal processing, the time to converge is approximately 1/(1-decay) steps of train. For decay=0.999, you need 1/0.001=1000 steps to converge. So set the appropriate decay for your training step numbers.\n2. using placeholder to switch between train and test evaluation\n3. use updates_collections=None if you don't want to add control dependencies of update op to train_op\n4. set reuse to appropriate value.\nIt seems the only way to use the official batch_norm is to build two graphs, one for train and one for evaluation, with is_training=True and is_training=False, respectively. In this way, you don't need to switch dynamically between train and evaluation. But this is a stupid way since you need to build more than one graph.\nFinally, I write a moving average by myself, and I find it worked! It's as follows(based on code on the web and modified by myself)\ndef bn_layer(x, scope, is_training, epsilon=0.001, decay=0.99, reuse=None):\n    \"\"\"\n    Performs a batch normalization layer\n\n    Args:\n        x: input tensor\n        scope: scope name\n        is_training: python boolean value\n        epsilon: the variance epsilon - a small float number to avoid dividing by 0\n        decay: the moving average decay\n\n    Returns:\n        The ops of a batch normalization layer\n    \"\"\"\n    with tf.variable_scope(scope, reuse=reuse):\n        shape = x.get_shape().as_list()\n        # gamma: a trainable scale factor\n        gamma = tf.get_variable(\"gamma\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=True)\n        # beta: a trainable shift value\n        beta = tf.get_variable(\"beta\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=True)\n        moving_avg = tf.get_variable(\"moving_avg\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\n        moving_var = tf.get_variable(\"moving_var\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\n        if is_training:\n            # tf.nn.moments == Calculate the mean and the variance of the tensor x\n            avg, var = tf.nn.moments(x, np.arange(len(shape)-1), keep_dims=True)\n            avg=tf.reshape(avg, [avg.shape.as_list()[-1]])\n            var=tf.reshape(var, [var.shape.as_list()[-1]])\n            #update_moving_avg = moving_averages.assign_moving_average(moving_avg, avg, decay)\n            update_moving_avg=tf.assign(moving_avg, moving_avg*decay+avg*(1-decay))\n            #update_moving_var = moving_averages.assign_moving_average(moving_var, var, decay)\n            update_moving_var=tf.assign(moving_var, moving_var*decay+var*(1-decay))\n            control_inputs = [update_moving_avg, update_moving_var]\n        else:\n            avg = moving_avg\n            var = moving_var\n            control_inputs = []\n        with tf.control_dependencies(control_inputs):\n            output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\n\n    return output\n\n\ndef bn_layer_top(x, scope, is_training, epsilon=0.001, decay=0.99):\n    \"\"\"\n    Returns a batch normalization layer that automatically switch between train and test phases based on the \n    tensor is_training\n\n    Args:\n        x: input tensor\n        scope: scope name\n        is_training: boolean tensor or variable\n        epsilon: epsilon parameter - see batch_norm_layer\n        decay: epsilon parameter - see batch_norm_layer\n\n    Returns:\n        The correct batch normalization layer based on the value of is_training\n    \"\"\"\n    #assert isinstance(is_training, (ops.Tensor, variables.Variable)) and is_training.dtype == tf.bool\n\n    return tf.cond(\n        is_training,\n        lambda: bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=True, reuse=None),\n        lambda: bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=False, reuse=True),\n    )\n\nJust use the bn_layer_top function during building a graph, the is_training parameter is a tf.placeholder\n. Then you are free to switch the placeholder to True during train and False during evaluation, with feed_dict.\nHope it helps the community.", "body": "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\n                                                                 MAYBE YOU NEED READ THIS\r\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\n\r\n\r\n\r\nIt seems there are still problems with TF v1.3. I'm sure I note the following details, but still failed to use the official `tf.contrib.layers.batch_norm`, with `is_training=False` during evaluation(but when I keep `is_training=True` unchanged during evaluation, it is ok):\r\n1.` decay`,  exponential moving average is actually alpha filter in signal processing, the time to converge is approximately 1/(1-decay) steps of train. For decay=0.999, you need 1/0.001=1000 steps to converge. So set the appropriate decay for your training step numbers.\r\n2. using placeholder to switch between train and test evaluation\r\n3. use` updates_collections=None` if you don't want to add control dependencies of update op to train_op\r\n4. set `reuse` to appropriate value.\r\n\r\nIt seems the only way to use the official batch_norm is to build two graphs, one for train and one for evaluation, with `is_training=True` and `is_training=False`, respectively. In this way, you don't need to switch dynamically between train and evaluation. But this is a stupid way since you need to build more than one graph.\r\n\r\nFinally, I write a moving average by myself, and I find it worked! It's as follows(based on code on the web and modified by myself)\r\n\r\n```\r\ndef bn_layer(x, scope, is_training, epsilon=0.001, decay=0.99, reuse=None):\r\n    \"\"\"\r\n    Performs a batch normalization layer\r\n\r\n    Args:\r\n        x: input tensor\r\n        scope: scope name\r\n        is_training: python boolean value\r\n        epsilon: the variance epsilon - a small float number to avoid dividing by 0\r\n        decay: the moving average decay\r\n\r\n    Returns:\r\n        The ops of a batch normalization layer\r\n    \"\"\"\r\n    with tf.variable_scope(scope, reuse=reuse):\r\n        shape = x.get_shape().as_list()\r\n        # gamma: a trainable scale factor\r\n        gamma = tf.get_variable(\"gamma\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=True)\r\n        # beta: a trainable shift value\r\n        beta = tf.get_variable(\"beta\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=True)\r\n        moving_avg = tf.get_variable(\"moving_avg\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\r\n        moving_var = tf.get_variable(\"moving_var\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\r\n        if is_training:\r\n            # tf.nn.moments == Calculate the mean and the variance of the tensor x\r\n            avg, var = tf.nn.moments(x, np.arange(len(shape)-1), keep_dims=True)\r\n            avg=tf.reshape(avg, [avg.shape.as_list()[-1]])\r\n            var=tf.reshape(var, [var.shape.as_list()[-1]])\r\n            #update_moving_avg = moving_averages.assign_moving_average(moving_avg, avg, decay)\r\n            update_moving_avg=tf.assign(moving_avg, moving_avg*decay+avg*(1-decay))\r\n            #update_moving_var = moving_averages.assign_moving_average(moving_var, var, decay)\r\n            update_moving_var=tf.assign(moving_var, moving_var*decay+var*(1-decay))\r\n            control_inputs = [update_moving_avg, update_moving_var]\r\n        else:\r\n            avg = moving_avg\r\n            var = moving_var\r\n            control_inputs = []\r\n        with tf.control_dependencies(control_inputs):\r\n            output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\r\n\r\n    return output\r\n\r\n\r\ndef bn_layer_top(x, scope, is_training, epsilon=0.001, decay=0.99):\r\n    \"\"\"\r\n    Returns a batch normalization layer that automatically switch between train and test phases based on the \r\n    tensor is_training\r\n\r\n    Args:\r\n        x: input tensor\r\n        scope: scope name\r\n        is_training: boolean tensor or variable\r\n        epsilon: epsilon parameter - see batch_norm_layer\r\n        decay: epsilon parameter - see batch_norm_layer\r\n\r\n    Returns:\r\n        The correct batch normalization layer based on the value of is_training\r\n    \"\"\"\r\n    #assert isinstance(is_training, (ops.Tensor, variables.Variable)) and is_training.dtype == tf.bool\r\n\r\n    return tf.cond(\r\n        is_training,\r\n        lambda: bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=True, reuse=None),\r\n        lambda: bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=False, reuse=True),\r\n    )\r\n```\r\n    \r\nJust use the `bn_layer_top` function during building a graph, the is_training parameter is a `tf.placeholder`\r\n. Then you are free to switch the placeholder to True during train and False during evaluation, with `feed_dict`.\r\n\r\nHope it helps the community. "}