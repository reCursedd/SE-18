{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/231908288", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-231908288", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 231908288, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMTkwODI4OA==", "user": {"login": "brando90", "id": 1855278, "node_id": "MDQ6VXNlcjE4NTUyNzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1855278?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brando90", "html_url": "https://github.com/brando90", "followers_url": "https://api.github.com/users/brando90/followers", "following_url": "https://api.github.com/users/brando90/following{/other_user}", "gists_url": "https://api.github.com/users/brando90/gists{/gist_id}", "starred_url": "https://api.github.com/users/brando90/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brando90/subscriptions", "organizations_url": "https://api.github.com/users/brando90/orgs", "repos_url": "https://api.github.com/users/brando90/repos", "events_url": "https://api.github.com/users/brando90/events{/privacy}", "received_events_url": "https://api.github.com/users/brando90/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-12T00:51:27Z", "updated_at": "2016-07-12T00:51:55Z", "author_association": "NONE", "body_html": "<p>sorry for the spamming, but what is wrong with just using something like this:</p>\n<pre><code>def standard_batch_norm(l, x, n_out, phase_train, scope='BN'):\n    \"\"\"\n    Batch normalization on feedforward maps.\n    Args:\n        x:           Vector\n        n_out:       integer, depth of input maps\n        phase_train: boolean tf.Varialbe, true indicates training phase\n        scope:       string, variable scope\n    Return:\n        normed:      batch-normalized maps\n    \"\"\"\n    with tf.variable_scope(scope+l):\n        #beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=tf.float64 ), name='beta', trainable=True, dtype=tf.float64 )\n        #gamma = tf.Variable(tf.constant(1.0, shape=[n_out],dtype=tf.float64 ), name='gamma', trainable=True, dtype=tf.float64 )\n        init_beta = tf.constant(0.0, shape=[n_out], dtype=tf.float64)\n        init_gamma = tf.constant(1.0, shape=[n_out],dtype=tf.float64)\n        beta = tf.get_variable(name='beta'+l, dtype=tf.float64, initializer=init_beta, regularizer=None, trainable=True)\n        gamma = tf.get_variable(name='gamma'+l, dtype=tf.float64, initializer=init_gamma, regularizer=None, trainable=True)\n        batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(phase_train, mean_var_with_update, lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n</code></pre>\n<p>then its simple to tell tensorflow which one to use with a feed dictionary as in:</p>\n<pre><code>feed_dict = {x: Xminibatch, y_: Yminibatch, phase_train: True}\nsess.run(fetches=[merged,train_step], feed_dict=feed_dict)\n</code></pre>\n<p>since its unclear if the implementation will change, I wanted to give a suggestion (note its easy to extend to convolutions and stuff I just didn't paste that code).</p>", "body_text": "sorry for the spamming, but what is wrong with just using something like this:\ndef standard_batch_norm(l, x, n_out, phase_train, scope='BN'):\n    \"\"\"\n    Batch normalization on feedforward maps.\n    Args:\n        x:           Vector\n        n_out:       integer, depth of input maps\n        phase_train: boolean tf.Varialbe, true indicates training phase\n        scope:       string, variable scope\n    Return:\n        normed:      batch-normalized maps\n    \"\"\"\n    with tf.variable_scope(scope+l):\n        #beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=tf.float64 ), name='beta', trainable=True, dtype=tf.float64 )\n        #gamma = tf.Variable(tf.constant(1.0, shape=[n_out],dtype=tf.float64 ), name='gamma', trainable=True, dtype=tf.float64 )\n        init_beta = tf.constant(0.0, shape=[n_out], dtype=tf.float64)\n        init_gamma = tf.constant(1.0, shape=[n_out],dtype=tf.float64)\n        beta = tf.get_variable(name='beta'+l, dtype=tf.float64, initializer=init_beta, regularizer=None, trainable=True)\n        gamma = tf.get_variable(name='gamma'+l, dtype=tf.float64, initializer=init_gamma, regularizer=None, trainable=True)\n        batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(phase_train, mean_var_with_update, lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n\nthen its simple to tell tensorflow which one to use with a feed dictionary as in:\nfeed_dict = {x: Xminibatch, y_: Yminibatch, phase_train: True}\nsess.run(fetches=[merged,train_step], feed_dict=feed_dict)\n\nsince its unclear if the implementation will change, I wanted to give a suggestion (note its easy to extend to convolutions and stuff I just didn't paste that code).", "body": "sorry for the spamming, but what is wrong with just using something like this:\n\n```\ndef standard_batch_norm(l, x, n_out, phase_train, scope='BN'):\n    \"\"\"\n    Batch normalization on feedforward maps.\n    Args:\n        x:           Vector\n        n_out:       integer, depth of input maps\n        phase_train: boolean tf.Varialbe, true indicates training phase\n        scope:       string, variable scope\n    Return:\n        normed:      batch-normalized maps\n    \"\"\"\n    with tf.variable_scope(scope+l):\n        #beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=tf.float64 ), name='beta', trainable=True, dtype=tf.float64 )\n        #gamma = tf.Variable(tf.constant(1.0, shape=[n_out],dtype=tf.float64 ), name='gamma', trainable=True, dtype=tf.float64 )\n        init_beta = tf.constant(0.0, shape=[n_out], dtype=tf.float64)\n        init_gamma = tf.constant(1.0, shape=[n_out],dtype=tf.float64)\n        beta = tf.get_variable(name='beta'+l, dtype=tf.float64, initializer=init_beta, regularizer=None, trainable=True)\n        gamma = tf.get_variable(name='gamma'+l, dtype=tf.float64, initializer=init_gamma, regularizer=None, trainable=True)\n        batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(phase_train, mean_var_with_update, lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n```\n\nthen its simple to tell tensorflow which one to use with a feed dictionary as in:\n\n```\nfeed_dict = {x: Xminibatch, y_: Yminibatch, phase_train: True}\nsess.run(fetches=[merged,train_step], feed_dict=feed_dict)\n```\n\nsince its unclear if the implementation will change, I wanted to give a suggestion (note its easy to extend to convolutions and stuff I just didn't paste that code).\n"}