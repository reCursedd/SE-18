{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235928564", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-235928564", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 235928564, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTkyODU2NA==", "user": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-28T15:22:08Z", "updated_at": "2016-07-28T15:22:08Z", "author_association": "MEMBER", "body_html": "<p>The latest version of <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L110\">tf.contrib.layers.batch_norm</a> now accepts a placeholder for is_training so not need to do it yourself.</p>\n<p>But what it is important is that either you pass <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L142\">updates_collections=None</a> so the moving_mean and moving_variance are updated in-place, otherwise you will need gather the update_ops and make sure they are run.</p>\n<p>I would like to encourage you to use <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/layers\"><code>tf.contrib.layers</code></a> or <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim\"><code>tf.contrib.slim</code></a> to build your model.</p>\n<pre><code>slim = tf.contrib.slim\n\ndef build_NN_two_hidden_layers(x, is_training):\n batch_norm_params = {'is_training': is_training, 'decay': 0.9, 'updates_collections': None}\n with slim.arg_scope([slim.fully_connected], \n    activation_fn=tf.nn.relu,\n    weigths_initializer=tf.contrib.layers.xavier_initializer(),\n    biases_initializer=tf.constant_initializer(0.1),\n    normalizer_fn=slim.batch_norm,\n    normalizer_params=batch_norm_params):\n   net = slim.fully_connected(x, 50, scope='A1')\n   net = slim.fully_connected(net, 49, scope='A2')\n   y = slim.fully_connected(net, 10, activation_fn=tf.nn.softmax, normalizer_fn=None, scope='A3')\n return y\n\n\n</code></pre>", "body_text": "The latest version of tf.contrib.layers.batch_norm now accepts a placeholder for is_training so not need to do it yourself.\nBut what it is important is that either you pass updates_collections=None so the moving_mean and moving_variance are updated in-place, otherwise you will need gather the update_ops and make sure they are run.\nI would like to encourage you to use tf.contrib.layers or tf.contrib.slim to build your model.\nslim = tf.contrib.slim\n\ndef build_NN_two_hidden_layers(x, is_training):\n batch_norm_params = {'is_training': is_training, 'decay': 0.9, 'updates_collections': None}\n with slim.arg_scope([slim.fully_connected], \n    activation_fn=tf.nn.relu,\n    weigths_initializer=tf.contrib.layers.xavier_initializer(),\n    biases_initializer=tf.constant_initializer(0.1),\n    normalizer_fn=slim.batch_norm,\n    normalizer_params=batch_norm_params):\n   net = slim.fully_connected(x, 50, scope='A1')\n   net = slim.fully_connected(net, 49, scope='A2')\n   y = slim.fully_connected(net, 10, activation_fn=tf.nn.softmax, normalizer_fn=None, scope='A3')\n return y", "body": "The latest version of [tf.contrib.layers.batch_norm](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L110) now accepts a placeholder for is_training so not need to do it yourself.\n\nBut what it is important is that either you pass [updates_collections=None](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L142) so the moving_mean and moving_variance are updated in-place, otherwise you will need gather the update_ops and make sure they are run.\n\nI would like to encourage you to use [`tf.contrib.layers`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/layers) or [`tf.contrib.slim`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim) to build your model.\n\n```\nslim = tf.contrib.slim\n\ndef build_NN_two_hidden_layers(x, is_training):\n batch_norm_params = {'is_training': is_training, 'decay': 0.9, 'updates_collections': None}\n with slim.arg_scope([slim.fully_connected], \n    activation_fn=tf.nn.relu,\n    weigths_initializer=tf.contrib.layers.xavier_initializer(),\n    biases_initializer=tf.constant_initializer(0.1),\n    normalizer_fn=slim.batch_norm,\n    normalizer_params=batch_norm_params):\n   net = slim.fully_connected(x, 50, scope='A1')\n   net = slim.fully_connected(net, 49, scope='A2')\n   y = slim.fully_connected(net, 10, activation_fn=tf.nn.softmax, normalizer_fn=None, scope='A3')\n return y\n\n\n```\n"}