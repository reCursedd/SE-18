{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/258005625", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-258005625", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 258005625, "node_id": "MDEyOklzc3VlQ29tbWVudDI1ODAwNTYyNQ==", "user": {"login": "zhongyuk", "id": 6901075, "node_id": "MDQ6VXNlcjY5MDEwNzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/6901075?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhongyuk", "html_url": "https://github.com/zhongyuk", "followers_url": "https://api.github.com/users/zhongyuk/followers", "following_url": "https://api.github.com/users/zhongyuk/following{/other_user}", "gists_url": "https://api.github.com/users/zhongyuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhongyuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhongyuk/subscriptions", "organizations_url": "https://api.github.com/users/zhongyuk/orgs", "repos_url": "https://api.github.com/users/zhongyuk/repos", "events_url": "https://api.github.com/users/zhongyuk/events{/privacy}", "received_events_url": "https://api.github.com/users/zhongyuk/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-02T21:32:24Z", "updated_at": "2016-11-02T21:38:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16869368\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nmhkahn\">@nmhkahn</a> Regarding your code snippet, may I ask why is <code>reuse</code> set to be <code>None</code> when <code>is_training=True</code>? Wouldn't that trigger the scaling parameter <code>gamma</code> and the offset parameter <code>beta</code> be re-initialized in every training step? I thought in the original paper, <code>beta</code> and <code>gamma</code> are \"learned along with the original model parameters\". To do that, shouldn't they be only initialized once and then reused in all training steps?</p>\n<p><code>tf.cond(is_training,  lambda: batch_norm(inputT, is_training=True,  updates_collections=None, scope=scope),  lambda: batch_norm(inputT, is_training=False,  updates_collections=None, scope=scope, reuse = True))</code></p>", "body_text": "@nmhkahn Regarding your code snippet, may I ask why is reuse set to be None when is_training=True? Wouldn't that trigger the scaling parameter gamma and the offset parameter beta be re-initialized in every training step? I thought in the original paper, beta and gamma are \"learned along with the original model parameters\". To do that, shouldn't they be only initialized once and then reused in all training steps?\ntf.cond(is_training,  lambda: batch_norm(inputT, is_training=True,  updates_collections=None, scope=scope),  lambda: batch_norm(inputT, is_training=False,  updates_collections=None, scope=scope, reuse = True))", "body": "@nmhkahn Regarding your code snippet, may I ask why is `reuse` set to be `None` when `is_training=True`? Wouldn't that trigger the scaling parameter `gamma` and the offset parameter `beta` be re-initialized in every training step? I thought in the original paper, `beta` and `gamma` are \"learned along with the original model parameters\". To do that, shouldn't they be only initialized once and then reused in all training steps?\n\n`tf.cond(is_training, \n lambda: batch_norm(inputT, is_training=True,  updates_collections=None, scope=scope), \n lambda: batch_norm(inputT, is_training=False,  updates_collections=None, scope=scope, reuse = True))`\n"}