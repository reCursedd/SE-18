{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/320225653", "html_url": "https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-320225653", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1122", "id": 320225653, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDIyNTY1Mw==", "user": {"login": "tyshiwo", "id": 19333875, "node_id": "MDQ6VXNlcjE5MzMzODc1", "avatar_url": "https://avatars0.githubusercontent.com/u/19333875?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tyshiwo", "html_url": "https://github.com/tyshiwo", "followers_url": "https://api.github.com/users/tyshiwo/followers", "following_url": "https://api.github.com/users/tyshiwo/following{/other_user}", "gists_url": "https://api.github.com/users/tyshiwo/gists{/gist_id}", "starred_url": "https://api.github.com/users/tyshiwo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tyshiwo/subscriptions", "organizations_url": "https://api.github.com/users/tyshiwo/orgs", "repos_url": "https://api.github.com/users/tyshiwo/repos", "events_url": "https://api.github.com/users/tyshiwo/events{/privacy}", "received_events_url": "https://api.github.com/users/tyshiwo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-04T11:26:51Z", "updated_at": "2017-08-04T11:26:51Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6901075\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zhongyuk\">@zhongyuk</a> ,</p>\n<p>I also met the problem that I could get good results when using is_training=True for both training and inference, but get bad results when setting is_training=False during inference (worse than the case using is_training=True). According to your analysis, If I understand correctly, by simply setting decay=0.9 in BN can solve this problem. Am I right?</p>\n<p>BTW, do I need to retrain the model using decay=0.9 from scratch? Or resuming training from the checkpoint (i.e., trained when decay=0.999) is also ok?</p>\n<p>Thanks!</p>", "body_text": "Hi @zhongyuk ,\nI also met the problem that I could get good results when using is_training=True for both training and inference, but get bad results when setting is_training=False during inference (worse than the case using is_training=True). According to your analysis, If I understand correctly, by simply setting decay=0.9 in BN can solve this problem. Am I right?\nBTW, do I need to retrain the model using decay=0.9 from scratch? Or resuming training from the checkpoint (i.e., trained when decay=0.999) is also ok?\nThanks!", "body": "Hi @zhongyuk ,\r\n\r\nI also met the problem that I could get good results when using is_training=True for both training and inference, but get bad results when setting is_training=False during inference (worse than the case using is_training=True). According to your analysis, If I understand correctly, by simply setting decay=0.9 in BN can solve this problem. Am I right? \r\n\r\nBTW, do I need to retrain the model using decay=0.9 from scratch? Or resuming training from the checkpoint (i.e., trained when decay=0.999) is also ok?\r\n\r\nThanks!"}