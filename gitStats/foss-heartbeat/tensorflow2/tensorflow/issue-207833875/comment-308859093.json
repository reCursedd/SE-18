{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308859093", "html_url": "https://github.com/tensorflow/tensorflow/issues/7531#issuecomment-308859093", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7531", "id": 308859093, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODg1OTA5Mw==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-15T20:38:14Z", "updated_at": "2017-06-15T20:40:29Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7976315\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lakshayg\">@lakshayg</a> thanks for your help! Really looking forward to testing this new activation out as I think it could hopefully be an improved activation compared to <code>tanh</code> and <code>relu</code>.</p>\n<p>Selu is pretty straight forward:</p>\n<div class=\"highlight highlight-source-python\"><pre>    alpha <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1.6732632423543772848170429916717</span>\n    scale <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1.0507009873554804934193349852946</span>\n\n    <span class=\"pl-k\">return</span> scale<span class=\"pl-k\">*</span>tf.where(_x<span class=\"pl-k\">&gt;=</span><span class=\"pl-c1\">0.0</span>, _x, alpha<span class=\"pl-k\">*</span>tf.exp(_x)<span class=\"pl-k\">-</span>alpha)</pre></div>", "body_text": "@lakshayg thanks for your help! Really looking forward to testing this new activation out as I think it could hopefully be an improved activation compared to tanh and relu.\nSelu is pretty straight forward:\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    return scale*tf.where(_x>=0.0, _x, alpha*tf.exp(_x)-alpha)", "body": "@lakshayg thanks for your help! Really looking forward to testing this new activation out as I think it could hopefully be an improved activation compared to `tanh` and `relu`. \r\n\r\nSelu is pretty straight forward:\r\n```python\r\n    alpha = 1.6732632423543772848170429916717\r\n    scale = 1.0507009873554804934193349852946\r\n\r\n    return scale*tf.where(_x>=0.0, _x, alpha*tf.exp(_x)-alpha)\r\n```\r\n\r\n"}