{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/379997624", "html_url": "https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-379997624", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15633", "id": 379997624, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTk5NzYyNA==", "user": {"login": "mpeniak", "id": 5175036, "node_id": "MDQ6VXNlcjUxNzUwMzY=", "avatar_url": "https://avatars0.githubusercontent.com/u/5175036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mpeniak", "html_url": "https://github.com/mpeniak", "followers_url": "https://api.github.com/users/mpeniak/followers", "following_url": "https://api.github.com/users/mpeniak/following{/other_user}", "gists_url": "https://api.github.com/users/mpeniak/gists{/gist_id}", "starred_url": "https://api.github.com/users/mpeniak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mpeniak/subscriptions", "organizations_url": "https://api.github.com/users/mpeniak/orgs", "repos_url": "https://api.github.com/users/mpeniak/repos", "events_url": "https://api.github.com/users/mpeniak/events{/privacy}", "received_events_url": "https://api.github.com/users/mpeniak/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-10T07:10:34Z", "updated_at": "2018-04-10T07:10:34Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38025138\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zhangbo0325\">@zhangbo0325</a> Thanks for the details. Since the squeeze is not supported by the NNAPI, does that mean that the NNAPI is not used at all and inference will remain as slow as it is? As I mentioned in the earlier comment, I get really poor performance on Pixel XL. I would expect inference times somewhere around 80-120ms. Thanks!</p>", "body_text": "@zhangbo0325 Thanks for the details. Since the squeeze is not supported by the NNAPI, does that mean that the NNAPI is not used at all and inference will remain as slow as it is? As I mentioned in the earlier comment, I get really poor performance on Pixel XL. I would expect inference times somewhere around 80-120ms. Thanks!", "body": "@zhangbo0325 Thanks for the details. Since the squeeze is not supported by the NNAPI, does that mean that the NNAPI is not used at all and inference will remain as slow as it is? As I mentioned in the earlier comment, I get really poor performance on Pixel XL. I would expect inference times somewhere around 80-120ms. Thanks!"}