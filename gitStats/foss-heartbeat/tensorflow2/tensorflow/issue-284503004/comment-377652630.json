{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/377652630", "html_url": "https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15633", "id": 377652630, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzY1MjYzMA==", "user": {"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-31T00:37:09Z", "updated_at": "2018-03-31T00:41:26Z", "author_association": "MEMBER", "body_html": "<p>It's live now at <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android\">tensorflow/contrib/lite/examples/android</a>! This is a more complete port of the original TF Android demo (only lacking the Stylize example), and will be replacing the other demo in tensorflow/contrib/lite/java/demo going forward.</p>\n<p>A converted TF Lite flatbuffer can be found in <a href=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_ssd_tflite_v1.zip\" rel=\"nofollow\">mobilenet_ssd_tflite_v1.zip</a>, and you can find the Java inference implementation in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/src/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\">TFLiteObjectDetectionAPIModel.java</a>. Note that this differs from the original TF implementation in that the boxes must be manually decoded in Java, and a box prior txt file needs to be packaged in the apps assets (I think the one included in the model zip above should be valid for most graphs).</p>\n<p>During TOCO conversion a different input node (Preprocessor/sub) is used, as well as different output nodes (concat,concat_1). This skips some parts that are problematic for tflite, until either the graph is restructured or TF Lite reaches TF parity.</p>\n<p>Here are the quick steps for converting an SSD MobileNet model to tflite format and building the demo to use it:</p>\n<pre><code># Download and extract SSD MobileNet model\nwget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz\ntar -xvf ssd_mobilenet_v1_coco_2017_11_17.tar.gz \nDETECT_PB=$PWD/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb\nSTRIPPED_PB=$PWD/frozen_inference_graph_stripped.pb\nDETECT_FB=$PWD/tensorflow/contrib/lite/examples/android/assets/mobilenet_ssd.tflite\n\n# Strip out problematic nodes before even letting TOCO see the graphdef\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\n--input_names=Preprocessor/sub --output_names=concat,concat_1 \\\n--alsologtostderr\n\n# Run TOCO conversion.\nbazel run tensorflow/contrib/lite/toco:toco -- \\\n--input_file=$STRIPPED_PB --output_file=$DETECT_FB \\\n--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\n--input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub \\\n--output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr\n\n# Build and install the demo\nbazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo\nadb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk\n</code></pre>", "body_text": "It's live now at tensorflow/contrib/lite/examples/android! This is a more complete port of the original TF Android demo (only lacking the Stylize example), and will be replacing the other demo in tensorflow/contrib/lite/java/demo going forward.\nA converted TF Lite flatbuffer can be found in mobilenet_ssd_tflite_v1.zip, and you can find the Java inference implementation in TFLiteObjectDetectionAPIModel.java. Note that this differs from the original TF implementation in that the boxes must be manually decoded in Java, and a box prior txt file needs to be packaged in the apps assets (I think the one included in the model zip above should be valid for most graphs).\nDuring TOCO conversion a different input node (Preprocessor/sub) is used, as well as different output nodes (concat,concat_1). This skips some parts that are problematic for tflite, until either the graph is restructured or TF Lite reaches TF parity.\nHere are the quick steps for converting an SSD MobileNet model to tflite format and building the demo to use it:\n# Download and extract SSD MobileNet model\nwget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz\ntar -xvf ssd_mobilenet_v1_coco_2017_11_17.tar.gz \nDETECT_PB=$PWD/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb\nSTRIPPED_PB=$PWD/frozen_inference_graph_stripped.pb\nDETECT_FB=$PWD/tensorflow/contrib/lite/examples/android/assets/mobilenet_ssd.tflite\n\n# Strip out problematic nodes before even letting TOCO see the graphdef\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\n--input_names=Preprocessor/sub --output_names=concat,concat_1 \\\n--alsologtostderr\n\n# Run TOCO conversion.\nbazel run tensorflow/contrib/lite/toco:toco -- \\\n--input_file=$STRIPPED_PB --output_file=$DETECT_FB \\\n--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\n--input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub \\\n--output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr\n\n# Build and install the demo\nbazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo\nadb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk", "body": "It's live now at [tensorflow/contrib/lite/examples/android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android)! This is a more complete port of the original TF Android demo (only lacking the Stylize example), and will be replacing the other demo in tensorflow/contrib/lite/java/demo going forward.\r\n\r\nA converted TF Lite flatbuffer can be found in [mobilenet_ssd_tflite_v1.zip](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_ssd_tflite_v1.zip), and you can find the Java inference implementation in [TFLiteObjectDetectionAPIModel.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/src/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java). Note that this differs from the original TF implementation in that the boxes must be manually decoded in Java, and a box prior txt file needs to be packaged in the apps assets (I think the one included in the model zip above should be valid for most graphs). \r\n\r\nDuring TOCO conversion a different input node (Preprocessor/sub) is used, as well as different output nodes (concat,concat_1). This skips some parts that are problematic for tflite, until either the graph is restructured or TF Lite reaches TF parity.\r\n\r\nHere are the quick steps for converting an SSD MobileNet model to tflite format and building the demo to use it:\r\n```\r\n# Download and extract SSD MobileNet model\r\nwget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz\r\ntar -xvf ssd_mobilenet_v1_coco_2017_11_17.tar.gz \r\nDETECT_PB=$PWD/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb\r\nSTRIPPED_PB=$PWD/frozen_inference_graph_stripped.pb\r\nDETECT_FB=$PWD/tensorflow/contrib/lite/examples/android/assets/mobilenet_ssd.tflite\r\n\r\n# Strip out problematic nodes before even letting TOCO see the graphdef\r\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\r\n--input_names=Preprocessor/sub --output_names=concat,concat_1 \\\r\n--alsologtostderr\r\n\r\n# Run TOCO conversion.\r\nbazel run tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$STRIPPED_PB --output_file=$DETECT_FB \\\r\n--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n--input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub \\\r\n--output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr\r\n\r\n# Build and install the demo\r\nbazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo\r\nadb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk\r\n```"}