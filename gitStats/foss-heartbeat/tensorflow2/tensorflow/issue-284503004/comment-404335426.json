{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404335426", "html_url": "https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-404335426", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15633", "id": 404335426, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDMzNTQyNg==", "user": {"login": "tenoyart", "id": 17884949, "node_id": "MDQ6VXNlcjE3ODg0OTQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/17884949?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tenoyart", "html_url": "https://github.com/tenoyart", "followers_url": "https://api.github.com/users/tenoyart/followers", "following_url": "https://api.github.com/users/tenoyart/following{/other_user}", "gists_url": "https://api.github.com/users/tenoyart/gists{/gist_id}", "starred_url": "https://api.github.com/users/tenoyart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tenoyart/subscriptions", "organizations_url": "https://api.github.com/users/tenoyart/orgs", "repos_url": "https://api.github.com/users/tenoyart/repos", "events_url": "https://api.github.com/users/tenoyart/events{/privacy}", "received_events_url": "https://api.github.com/users/tenoyart/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-11T22:53:40Z", "updated_at": "2018-07-11T22:53:40Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4723042\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/achowdhery\">@achowdhery</a> Hi, I saw some updates of the building instructions for the latest android demo here (<a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app\">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app</a>), but it didn't indicate how we can actually convert the frozen pb model to tflite model (the quantized detect.tflite which was used in the latest demo). Any further instructions on the quantized model conversion flow? Also, I think we should first run quantized training with fake quantization operations as instructed here (<a href=\"https://www.tensorflow.org/performance/quantization\" rel=\"nofollow\">https://www.tensorflow.org/performance/quantization</a>) and then perform the model conversion, correct? Also, is it possible to enable NNAPI in the latest android demo? I tried to use tfLite.setUseNNAPI(true) in TFLiteObjectDetectionAPIModel.java but it crashed on my Pixel 2 running Android 8.1 (it can work well without NNAPI). Any suggestions? Thanks!</p>", "body_text": "@achowdhery Hi, I saw some updates of the building instructions for the latest android demo here (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app), but it didn't indicate how we can actually convert the frozen pb model to tflite model (the quantized detect.tflite which was used in the latest demo). Any further instructions on the quantized model conversion flow? Also, I think we should first run quantized training with fake quantization operations as instructed here (https://www.tensorflow.org/performance/quantization) and then perform the model conversion, correct? Also, is it possible to enable NNAPI in the latest android demo? I tried to use tfLite.setUseNNAPI(true) in TFLiteObjectDetectionAPIModel.java but it crashed on my Pixel 2 running Android 8.1 (it can work well without NNAPI). Any suggestions? Thanks!", "body": "@achowdhery Hi, I saw some updates of the building instructions for the latest android demo here (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app), but it didn't indicate how we can actually convert the frozen pb model to tflite model (the quantized detect.tflite which was used in the latest demo). Any further instructions on the quantized model conversion flow? Also, I think we should first run quantized training with fake quantization operations as instructed here (https://www.tensorflow.org/performance/quantization) and then perform the model conversion, correct? Also, is it possible to enable NNAPI in the latest android demo? I tried to use tfLite.setUseNNAPI(true) in TFLiteObjectDetectionAPIModel.java but it crashed on my Pixel 2 running Android 8.1 (it can work well without NNAPI). Any suggestions? Thanks!"}