{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321380067", "html_url": "https://github.com/tensorflow/tensorflow/issues/11290#issuecomment-321380067", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11290", "id": 321380067, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTM4MDA2Nw==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-09T21:00:39Z", "updated_at": "2017-08-09T21:00:39Z", "author_association": "MEMBER", "body_html": "<p>I think the larger question raised by this is what our position should be regarding serialization formats. Regenerating the testdata would make this test pass, but we'd still have issues where say a SavedModel produced by a little endian machine will be unreadable on a big endian machine and vice-versa. Similarly, any tensors sent over RPCs across between machines with different architectures would likely fail.</p>\n<p>A more principled fix would be something that perhaps includes the encoding format in the serialization and translates if necessary when deserializing. Something like that, I believe, would be the right way to go about ensuring TensorFlow is big-endian friendly.</p>\n<p>That said, for specifically this test, you could regenerate a saved model in big-endian format in the testdata directory by running <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/saved_model/saved_model_half_plus_two.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/saved_model/saved_model_half_plus_two.py</a> , committing the output and changing <code>c_api_test.cc</code> to use one path or the other based on the architecture it's running on.</p>\n<p>Unassigning myself as both changes outlined above is not something that I, or anyone else on the TensorFlow team (I think), will be getting to in the near future. But we'd be happy to accept PRs.</p>", "body_text": "I think the larger question raised by this is what our position should be regarding serialization formats. Regenerating the testdata would make this test pass, but we'd still have issues where say a SavedModel produced by a little endian machine will be unreadable on a big endian machine and vice-versa. Similarly, any tensors sent over RPCs across between machines with different architectures would likely fail.\nA more principled fix would be something that perhaps includes the encoding format in the serialization and translates if necessary when deserializing. Something like that, I believe, would be the right way to go about ensuring TensorFlow is big-endian friendly.\nThat said, for specifically this test, you could regenerate a saved model in big-endian format in the testdata directory by running https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/saved_model/saved_model_half_plus_two.py , committing the output and changing c_api_test.cc to use one path or the other based on the architecture it's running on.\nUnassigning myself as both changes outlined above is not something that I, or anyone else on the TensorFlow team (I think), will be getting to in the near future. But we'd be happy to accept PRs.", "body": "I think the larger question raised by this is what our position should be regarding serialization formats. Regenerating the testdata would make this test pass, but we'd still have issues where say a SavedModel produced by a little endian machine will be unreadable on a big endian machine and vice-versa. Similarly, any tensors sent over RPCs across between machines with different architectures would likely fail.\r\n\r\nA more principled fix would be something that perhaps includes the encoding format in the serialization and translates if necessary when deserializing. Something like that, I believe, would be the right way to go about ensuring TensorFlow is big-endian friendly.\r\n\r\nThat said, for specifically this test, you could regenerate a saved model in big-endian format in the testdata directory by running https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/saved_model/saved_model_half_plus_two.py , committing the output and changing `c_api_test.cc` to use one path or the other based on the architecture it's running on.\r\n\r\nUnassigning myself as both changes outlined above is not something that I, or anyone else on the TensorFlow team (I think), will be getting to in the near future. But we'd be happy to accept PRs."}