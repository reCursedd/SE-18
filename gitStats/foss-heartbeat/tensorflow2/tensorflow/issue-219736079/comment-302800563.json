{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/302800563", "html_url": "https://github.com/tensorflow/tensorflow/issues/9001#issuecomment-302800563", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9001", "id": 302800563, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjgwMDU2Mw==", "user": {"login": "samjabrahams", "id": 11607205, "node_id": "MDQ6VXNlcjExNjA3MjA1", "avatar_url": "https://avatars0.githubusercontent.com/u/11607205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samjabrahams", "html_url": "https://github.com/samjabrahams", "followers_url": "https://api.github.com/users/samjabrahams/followers", "following_url": "https://api.github.com/users/samjabrahams/following{/other_user}", "gists_url": "https://api.github.com/users/samjabrahams/gists{/gist_id}", "starred_url": "https://api.github.com/users/samjabrahams/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samjabrahams/subscriptions", "organizations_url": "https://api.github.com/users/samjabrahams/orgs", "repos_url": "https://api.github.com/users/samjabrahams/repos", "events_url": "https://api.github.com/users/samjabrahams/events{/privacy}", "received_events_url": "https://api.github.com/users/samjabrahams/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-19T20:13:57Z", "updated_at": "2017-05-19T20:13:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=24483645\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/awav\">@awav</a> The <code>trainable</code> parameter just puts that <code>Variable</code> object into the <code>TRAINABLE_VARIABLES</code> collection. I don't think TensorFlow is designed to have the <code>var_list</code> passed into <code>Optimizer.compute_gradients()</code> changed (the <code>compute_gradients</code> operation has the variables it updates statically defined).</p>\n<p>Since we're using <code>tf.cond</code> already in order to implement layer skipping in the forward pass, I'm not sure that it's easier to explicitly block the gradients as opposed to taking advantage of it via <code>tf.cond</code> for free :)</p>", "body_text": "@awav The trainable parameter just puts that Variable object into the TRAINABLE_VARIABLES collection. I don't think TensorFlow is designed to have the var_list passed into Optimizer.compute_gradients() changed (the compute_gradients operation has the variables it updates statically defined).\nSince we're using tf.cond already in order to implement layer skipping in the forward pass, I'm not sure that it's easier to explicitly block the gradients as opposed to taking advantage of it via tf.cond for free :)", "body": "@awav The `trainable` parameter just puts that `Variable` object into the `TRAINABLE_VARIABLES` collection. I don't think TensorFlow is designed to have the `var_list` passed into `Optimizer.compute_gradients()` changed (the `compute_gradients` operation has the variables it updates statically defined).\r\n\r\nSince we're using `tf.cond` already in order to implement layer skipping in the forward pass, I'm not sure that it's easier to explicitly block the gradients as opposed to taking advantage of it via `tf.cond` for free :)"}