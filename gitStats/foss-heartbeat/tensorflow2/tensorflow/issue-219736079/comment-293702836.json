{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293702836", "html_url": "https://github.com/tensorflow/tensorflow/issues/9001#issuecomment-293702836", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9001", "id": 293702836, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MzcwMjgzNg==", "user": {"login": "alexanderalang", "id": 22649804, "node_id": "MDQ6VXNlcjIyNjQ5ODA0", "avatar_url": "https://avatars1.githubusercontent.com/u/22649804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexanderalang", "html_url": "https://github.com/alexanderalang", "followers_url": "https://api.github.com/users/alexanderalang/followers", "following_url": "https://api.github.com/users/alexanderalang/following{/other_user}", "gists_url": "https://api.github.com/users/alexanderalang/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexanderalang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexanderalang/subscriptions", "organizations_url": "https://api.github.com/users/alexanderalang/orgs", "repos_url": "https://api.github.com/users/alexanderalang/repos", "events_url": "https://api.github.com/users/alexanderalang/events{/privacy}", "received_events_url": "https://api.github.com/users/alexanderalang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-12T20:47:47Z", "updated_at": "2017-04-12T21:04:55Z", "author_association": "NONE", "body_html": "<p>this isn't very elegant, but it works.</p>\n<pre><code>def layer_wrapper(layer,layer_args,layer_kwargs={},switch=1):\n    output = layer(*layer_args,**layer_kwargs) #normal output of layer\n        \n    forward = tf.cond(tf.cast(switch,tf.bool), #if layer is disabled, inputs are passed on to next layer. \n                     lambda: output,\n                     lambda: layer_args[0]) # requires 1st value of args to be the input\n        \n    backward = tf.cond(tf.cast(switch,tf.bool), #gradient is stopped for backprop if layer is disabled\n                     lambda: forward,\n                     lambda: tf.zeros_like(forward))\n        \n    return backward + tf.stop_gradient(forward-backward)\n\n\ndef assign_switches(vec):\n    with tf.variable_scope('layer_dropout', reuse=True):\n        switches = tf.get_collection('switches')\n    assert len(vec) == len(switches)\n    return [switches[i].assign(vec[i]) for i in range(len(switches))]\n\ndef make_dropout_vars():\n    with tf.variable_scope('layer_dropout', reuse=True):\n        switches = tf.get_collection('switches')\n        switch = tf.Variable(1,name=\"switches/\"+str(len(switches)))\n        drop_prob = tf.Variable(0.5,name=\"probability/\"+str(len(switches)))\n        tf.add_to_collection('switches',switch)\n        tf.add_to_collection('dropout_probabilities',drop_prob)\n        return switch\n</code></pre>\n<p>To mirror <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> 's <a href=\"https://github.com/yaroslavvb/stuff/blob/master/conditional_backprop.py\">example</a></p>\n<pre><code>def add(x,name):\n    return x+1\nx = tf.ones((), name=\"x\")\ny0 = layer_wrapper(add,(x,\"y0\"),switch=make_dropout_vars())\ny1 = layer_wrapper(add,(x,\"y1\"),switch=make_dropout_vars())\n\ny = tf.stack([y0, y1], name=\"y\")\n\nz = tf.reduce_sum(y)\n\ngrad = tf.gradients(z, [x])[0]\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nprint(\"\\nValue %.1f, gradient %.1f\"%tuple(sess.run([z, grad])))\n\nsess.run(assign_switches([0,0]))\nprint(\"Value %.1f, gradient %.1f\"%tuple(sess.run([z, grad])))\n\nsess.run(assign_switches([0,1]))\nprint(\"Value %.1f, gradient %.1f\"%tuple(sess.run([z, grad])))\n</code></pre>\n<p>It produces the same gradients as in the example, but the value depends on whether the add nodes are activated (since <code>add</code> adds 1 and not 0 like in the example).</p>\n<p>so the output is</p>\n<pre><code>Value 4.0, gradient 2.0\nValue 2.0, gradient 0.0\nValue 3.0, gradient 1.0\n</code></pre>", "body_text": "this isn't very elegant, but it works.\ndef layer_wrapper(layer,layer_args,layer_kwargs={},switch=1):\n    output = layer(*layer_args,**layer_kwargs) #normal output of layer\n        \n    forward = tf.cond(tf.cast(switch,tf.bool), #if layer is disabled, inputs are passed on to next layer. \n                     lambda: output,\n                     lambda: layer_args[0]) # requires 1st value of args to be the input\n        \n    backward = tf.cond(tf.cast(switch,tf.bool), #gradient is stopped for backprop if layer is disabled\n                     lambda: forward,\n                     lambda: tf.zeros_like(forward))\n        \n    return backward + tf.stop_gradient(forward-backward)\n\n\ndef assign_switches(vec):\n    with tf.variable_scope('layer_dropout', reuse=True):\n        switches = tf.get_collection('switches')\n    assert len(vec) == len(switches)\n    return [switches[i].assign(vec[i]) for i in range(len(switches))]\n\ndef make_dropout_vars():\n    with tf.variable_scope('layer_dropout', reuse=True):\n        switches = tf.get_collection('switches')\n        switch = tf.Variable(1,name=\"switches/\"+str(len(switches)))\n        drop_prob = tf.Variable(0.5,name=\"probability/\"+str(len(switches)))\n        tf.add_to_collection('switches',switch)\n        tf.add_to_collection('dropout_probabilities',drop_prob)\n        return switch\n\nTo mirror @yaroslavvb 's example\ndef add(x,name):\n    return x+1\nx = tf.ones((), name=\"x\")\ny0 = layer_wrapper(add,(x,\"y0\"),switch=make_dropout_vars())\ny1 = layer_wrapper(add,(x,\"y1\"),switch=make_dropout_vars())\n\ny = tf.stack([y0, y1], name=\"y\")\n\nz = tf.reduce_sum(y)\n\ngrad = tf.gradients(z, [x])[0]\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nprint(\"\\nValue %.1f, gradient %.1f\"%tuple(sess.run([z, grad])))\n\nsess.run(assign_switches([0,0]))\nprint(\"Value %.1f, gradient %.1f\"%tuple(sess.run([z, grad])))\n\nsess.run(assign_switches([0,1]))\nprint(\"Value %.1f, gradient %.1f\"%tuple(sess.run([z, grad])))\n\nIt produces the same gradients as in the example, but the value depends on whether the add nodes are activated (since add adds 1 and not 0 like in the example).\nso the output is\nValue 4.0, gradient 2.0\nValue 2.0, gradient 0.0\nValue 3.0, gradient 1.0", "body": "this isn't very elegant, but it works.\r\n```\r\ndef layer_wrapper(layer,layer_args,layer_kwargs={},switch=1):\r\n    output = layer(*layer_args,**layer_kwargs) #normal output of layer\r\n        \r\n    forward = tf.cond(tf.cast(switch,tf.bool), #if layer is disabled, inputs are passed on to next layer. \r\n                     lambda: output,\r\n                     lambda: layer_args[0]) # requires 1st value of args to be the input\r\n        \r\n    backward = tf.cond(tf.cast(switch,tf.bool), #gradient is stopped for backprop if layer is disabled\r\n                     lambda: forward,\r\n                     lambda: tf.zeros_like(forward))\r\n        \r\n    return backward + tf.stop_gradient(forward-backward)\r\n\r\n\r\ndef assign_switches(vec):\r\n    with tf.variable_scope('layer_dropout', reuse=True):\r\n        switches = tf.get_collection('switches')\r\n    assert len(vec) == len(switches)\r\n    return [switches[i].assign(vec[i]) for i in range(len(switches))]\r\n\r\ndef make_dropout_vars():\r\n    with tf.variable_scope('layer_dropout', reuse=True):\r\n        switches = tf.get_collection('switches')\r\n        switch = tf.Variable(1,name=\"switches/\"+str(len(switches)))\r\n        drop_prob = tf.Variable(0.5,name=\"probability/\"+str(len(switches)))\r\n        tf.add_to_collection('switches',switch)\r\n        tf.add_to_collection('dropout_probabilities',drop_prob)\r\n        return switch\r\n```\r\n\r\nTo mirror @yaroslavvb 's [example](https://github.com/yaroslavvb/stuff/blob/master/conditional_backprop.py)\r\n\r\n```\r\ndef add(x,name):\r\n    return x+1\r\nx = tf.ones((), name=\"x\")\r\ny0 = layer_wrapper(add,(x,\"y0\"),switch=make_dropout_vars())\r\ny1 = layer_wrapper(add,(x,\"y1\"),switch=make_dropout_vars())\r\n\r\ny = tf.stack([y0, y1], name=\"y\")\r\n\r\nz = tf.reduce_sum(y)\r\n\r\ngrad = tf.gradients(z, [x])[0]\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nprint(\"\\nValue %.1f, gradient %.1f\"%tuple(sess.run([z, grad])))\r\n\r\nsess.run(assign_switches([0,0]))\r\nprint(\"Value %.1f, gradient %.1f\"%tuple(sess.run([z, grad])))\r\n\r\nsess.run(assign_switches([0,1]))\r\nprint(\"Value %.1f, gradient %.1f\"%tuple(sess.run([z, grad])))\r\n```\r\nIt produces the same gradients as in the example, but the value depends on whether the add nodes are activated (since `add` adds 1 and not 0 like in the example).\r\n\r\nso the output is\r\n```\r\nValue 4.0, gradient 2.0\r\nValue 2.0, gradient 0.0\r\nValue 3.0, gradient 1.0\r\n``` "}