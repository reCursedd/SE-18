{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300055749", "html_url": "https://github.com/tensorflow/tensorflow/issues/9001#issuecomment-300055749", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9001", "id": 300055749, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDA1NTc0OQ==", "user": {"login": "samjabrahams", "id": 11607205, "node_id": "MDQ6VXNlcjExNjA3MjA1", "avatar_url": "https://avatars0.githubusercontent.com/u/11607205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samjabrahams", "html_url": "https://github.com/samjabrahams", "followers_url": "https://api.github.com/users/samjabrahams/followers", "following_url": "https://api.github.com/users/samjabrahams/following{/other_user}", "gists_url": "https://api.github.com/users/samjabrahams/gists{/gist_id}", "starred_url": "https://api.github.com/users/samjabrahams/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samjabrahams/subscriptions", "organizations_url": "https://api.github.com/users/samjabrahams/orgs", "repos_url": "https://api.github.com/users/samjabrahams/repos", "events_url": "https://api.github.com/users/samjabrahams/events{/privacy}", "received_events_url": "https://api.github.com/users/samjabrahams/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-09T04:05:38Z", "updated_at": "2017-05-09T04:05:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22649804\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alexanderalang\">@alexanderalang</a> the initial tests look promising. Here's some sample code and respective timing outputs with a very simple CNN (no pooling, batch norm, etc) doing a single training step on dummy data/labels and batch size 32. This is from running on a 2013 macbook pro CPU, so focus on the relative times instead of the total time.</p>\n<h3>Stochastic depth</h3>\n<p><strong>Training code</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Stochastic version</span>\ntf.reset_default_graph()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Building graph...<span class=\"pl-pds\">'</span></span>)\ninputs <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">228</span>, <span class=\"pl-c1\">228</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>inputs<span class=\"pl-pds\">'</span></span>)\nlabels <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, <span class=\"pl-c1\">None</span>)\nlabels_onehot <span class=\"pl-k\">=</span> tf.one_hot(labels, <span class=\"pl-c1\">10</span>)\nkeep_prob <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>keep_prob<span class=\"pl-pds\">'</span></span>)\nconv, _ <span class=\"pl-k\">=</span> stochastic_depth_conv2d(inputs, <span class=\"pl-c1\">32</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], keep_prob)\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n    conv, _ <span class=\"pl-k\">=</span> stochastic_depth_conv2d(conv, <span class=\"pl-c1\">32</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], keep_prob)\nflat <span class=\"pl-k\">=</span> tf.reshape(conv, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">228</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">228</span>])\nlogits <span class=\"pl-k\">=</span> tf.layers.dense(flat, <span class=\"pl-c1\">10</span>)\nloss <span class=\"pl-k\">=</span> tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels_onehot, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\n\ntrain <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.05</span>).minimize(loss)\n\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Done.<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(init)\n    feed_dict <span class=\"pl-k\">=</span> {\n        inputs: np.random.normal(<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">228</span>, <span class=\"pl-c1\">228</span>, <span class=\"pl-c1\">3</span>]),\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Adjust this parameter to see run speed adjust</span>\n        keep_prob: <span class=\"pl-c1\">0.5</span>,\n        labels: np.random.randint(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">32</span>])\n    }\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Timing stochastic depth run...<span class=\"pl-pds\">'</span></span>)\n    start_t <span class=\"pl-k\">=</span> time.time()\n    _ <span class=\"pl-k\">=</span> sess.run(train, feed_dict)\n    end_t <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Done.<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">print</span>(end_t <span class=\"pl-k\">-</span> start_t)</pre></div>\n<p><strong>Output:</strong></p>\n<pre><code>Building graph...\nDone.\nTiming stochastic depth run...\nDone.\n13.219794034957886\n</code></pre>\n<h3>\"Normal\" Conv2D layer</h3>\n<p><strong>Training code:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Standard CNN version</span>\ntf.reset_default_graph()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Building graph...<span class=\"pl-pds\">'</span></span>)\ninputs <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">228</span>, <span class=\"pl-c1\">228</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>inputs<span class=\"pl-pds\">'</span></span>)\nlabels <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, <span class=\"pl-c1\">None</span>)\nlabels_onehot <span class=\"pl-k\">=</span> tf.one_hot(labels, <span class=\"pl-c1\">10</span>)\nconv <span class=\"pl-k\">=</span> tf.layers.conv2d(inputs, <span class=\"pl-c1\">32</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu)\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n    conv <span class=\"pl-k\">=</span> tf.layers.conv2d(conv, <span class=\"pl-c1\">32</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu)\nflat <span class=\"pl-k\">=</span> tf.reshape(conv, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">228</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">228</span>])\nlogits <span class=\"pl-k\">=</span> tf.layers.dense(flat, <span class=\"pl-c1\">10</span>)\nloss <span class=\"pl-k\">=</span> tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels_onehot, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\n\ntrain <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.05</span>).minimize(loss)\n\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Done.<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(init)\n    feed_dict <span class=\"pl-k\">=</span> {\n        inputs: np.random.normal(<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">228</span>, <span class=\"pl-c1\">228</span>, <span class=\"pl-c1\">3</span>]),\n        labels: np.random.randint(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">32</span>])\n    }\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Timing standard run...<span class=\"pl-pds\">'</span></span>)\n    start_t <span class=\"pl-k\">=</span> time.time()\n    _ <span class=\"pl-k\">=</span> sess.run(train, feed_dict)\n    end_t <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Done.<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">print</span>(end_t <span class=\"pl-k\">-</span> start_t)</pre></div>\n<p><strong>Output:</strong></p>\n<pre><code>Building graph...\nDone.\nTiming standard run...\nDone.\n25.8157160282135\n</code></pre>\n<p>Adjusting the <code>keep_prob</code> feed value in the first code alters the average relative runtime as you'd expect.</p>\n<pre><code>keep_prob = 0.2 --&gt;  5.950854063034058 seconds\nkeep_prob = 0.8 --&gt; 18.41518807411194 seconds\n</code></pre>", "body_text": "@alexanderalang the initial tests look promising. Here's some sample code and respective timing outputs with a very simple CNN (no pooling, batch norm, etc) doing a single training step on dummy data/labels and batch size 32. This is from running on a 2013 macbook pro CPU, so focus on the relative times instead of the total time.\nStochastic depth\nTraining code\n# Stochastic version\ntf.reset_default_graph()\nprint('Building graph...')\ninputs = tf.placeholder(tf.float32, [None, 228, 228, 3], name='inputs')\nlabels = tf.placeholder(tf.int32, None)\nlabels_onehot = tf.one_hot(labels, 10)\nkeep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\nconv, _ = stochastic_depth_conv2d(inputs, 32, [3, 3], keep_prob)\nfor i in range(10):\n    conv, _ = stochastic_depth_conv2d(conv, 32, [3, 3], keep_prob)\nflat = tf.reshape(conv, [-1, 32 * 228 * 228])\nlogits = tf.layers.dense(flat, 10)\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits)\n\ntrain = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n\ninit = tf.global_variables_initializer()\n\nprint('Done.')\n\nwith tf.Session() as sess:\n    sess.run(init)\n    feed_dict = {\n        inputs: np.random.normal(size=[32, 228, 228, 3]),\n        # Adjust this parameter to see run speed adjust\n        keep_prob: 0.5,\n        labels: np.random.randint(0, 10, size=[32])\n    }\n    print('Timing stochastic depth run...')\n    start_t = time.time()\n    _ = sess.run(train, feed_dict)\n    end_t = time.time()\n    print('Done.')\n    print(end_t - start_t)\nOutput:\nBuilding graph...\nDone.\nTiming stochastic depth run...\nDone.\n13.219794034957886\n\n\"Normal\" Conv2D layer\nTraining code:\n# Standard CNN version\ntf.reset_default_graph()\nprint('Building graph...')\ninputs = tf.placeholder(tf.float32, [None, 228, 228, 3], name='inputs')\nlabels = tf.placeholder(tf.int32, None)\nlabels_onehot = tf.one_hot(labels, 10)\nconv = tf.layers.conv2d(inputs, 32, [3, 3], padding='same', activation=tf.nn.relu)\nfor i in range(10):\n    conv = tf.layers.conv2d(conv, 32, [3, 3], padding='same', activation=tf.nn.relu)\nflat = tf.reshape(conv, [-1, 32 * 228 * 228])\nlogits = tf.layers.dense(flat, 10)\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits)\n\ntrain = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n\ninit = tf.global_variables_initializer()\n\nprint('Done.')\n\nwith tf.Session() as sess:\n    sess.run(init)\n    feed_dict = {\n        inputs: np.random.normal(size=[32, 228, 228, 3]),\n        labels: np.random.randint(0, 10, size=[32])\n    }\n    print('Timing standard run...')\n    start_t = time.time()\n    _ = sess.run(train, feed_dict)\n    end_t = time.time()\n    print('Done.')\n    print(end_t - start_t)\nOutput:\nBuilding graph...\nDone.\nTiming standard run...\nDone.\n25.8157160282135\n\nAdjusting the keep_prob feed value in the first code alters the average relative runtime as you'd expect.\nkeep_prob = 0.2 -->  5.950854063034058 seconds\nkeep_prob = 0.8 --> 18.41518807411194 seconds", "body": "@alexanderalang the initial tests look promising. Here's some sample code and respective timing outputs with a very simple CNN (no pooling, batch norm, etc) doing a single training step on dummy data/labels and batch size 32. This is from running on a 2013 macbook pro CPU, so focus on the relative times instead of the total time.\r\n\r\n### Stochastic depth \r\n\r\n**Training code**\r\n\r\n```python\r\n# Stochastic version\r\ntf.reset_default_graph()\r\nprint('Building graph...')\r\ninputs = tf.placeholder(tf.float32, [None, 228, 228, 3], name='inputs')\r\nlabels = tf.placeholder(tf.int32, None)\r\nlabels_onehot = tf.one_hot(labels, 10)\r\nkeep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\r\nconv, _ = stochastic_depth_conv2d(inputs, 32, [3, 3], keep_prob)\r\nfor i in range(10):\r\n    conv, _ = stochastic_depth_conv2d(conv, 32, [3, 3], keep_prob)\r\nflat = tf.reshape(conv, [-1, 32 * 228 * 228])\r\nlogits = tf.layers.dense(flat, 10)\r\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits)\r\n\r\ntrain = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nprint('Done.')\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    feed_dict = {\r\n        inputs: np.random.normal(size=[32, 228, 228, 3]),\r\n        # Adjust this parameter to see run speed adjust\r\n        keep_prob: 0.5,\r\n        labels: np.random.randint(0, 10, size=[32])\r\n    }\r\n    print('Timing stochastic depth run...')\r\n    start_t = time.time()\r\n    _ = sess.run(train, feed_dict)\r\n    end_t = time.time()\r\n    print('Done.')\r\n    print(end_t - start_t)\r\n```\r\n\r\n**Output:**\r\n\r\n```\r\nBuilding graph...\r\nDone.\r\nTiming stochastic depth run...\r\nDone.\r\n13.219794034957886\r\n```\r\n\r\n### \"Normal\" Conv2D layer\r\n\r\n**Training code:**\r\n\r\n```python\r\n# Standard CNN version\r\ntf.reset_default_graph()\r\nprint('Building graph...')\r\ninputs = tf.placeholder(tf.float32, [None, 228, 228, 3], name='inputs')\r\nlabels = tf.placeholder(tf.int32, None)\r\nlabels_onehot = tf.one_hot(labels, 10)\r\nconv = tf.layers.conv2d(inputs, 32, [3, 3], padding='same', activation=tf.nn.relu)\r\nfor i in range(10):\r\n    conv = tf.layers.conv2d(conv, 32, [3, 3], padding='same', activation=tf.nn.relu)\r\nflat = tf.reshape(conv, [-1, 32 * 228 * 228])\r\nlogits = tf.layers.dense(flat, 10)\r\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits)\r\n\r\ntrain = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nprint('Done.')\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    feed_dict = {\r\n        inputs: np.random.normal(size=[32, 228, 228, 3]),\r\n        labels: np.random.randint(0, 10, size=[32])\r\n    }\r\n    print('Timing standard run...')\r\n    start_t = time.time()\r\n    _ = sess.run(train, feed_dict)\r\n    end_t = time.time()\r\n    print('Done.')\r\n    print(end_t - start_t)\r\n```\r\n\r\n**Output:**\r\n\r\n```\r\nBuilding graph...\r\nDone.\r\nTiming standard run...\r\nDone.\r\n25.8157160282135\r\n```\r\n\r\nAdjusting the `keep_prob` feed value in the first code alters the average relative runtime as you'd expect.\r\n\r\n```\r\nkeep_prob = 0.2 -->  5.950854063034058 seconds\r\nkeep_prob = 0.8 --> 18.41518807411194 seconds\r\n```"}