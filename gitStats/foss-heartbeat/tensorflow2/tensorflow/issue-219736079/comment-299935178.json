{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299935178", "html_url": "https://github.com/tensorflow/tensorflow/issues/9001#issuecomment-299935178", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9001", "id": 299935178, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTkzNTE3OA==", "user": {"login": "alexanderalang", "id": 22649804, "node_id": "MDQ6VXNlcjIyNjQ5ODA0", "avatar_url": "https://avatars1.githubusercontent.com/u/22649804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexanderalang", "html_url": "https://github.com/alexanderalang", "followers_url": "https://api.github.com/users/alexanderalang/followers", "following_url": "https://api.github.com/users/alexanderalang/following{/other_user}", "gists_url": "https://api.github.com/users/alexanderalang/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexanderalang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexanderalang/subscriptions", "organizations_url": "https://api.github.com/users/alexanderalang/orgs", "repos_url": "https://api.github.com/users/alexanderalang/repos", "events_url": "https://api.github.com/users/alexanderalang/events{/privacy}", "received_events_url": "https://api.github.com/users/alexanderalang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-08T17:34:04Z", "updated_at": "2017-05-08T17:34:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11607205\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samjabrahams\">@samjabrahams</a> so theres no need for the <code>gradient_override_map</code> with the Identity op? Does your code actually skip the layer during backprop, or does the weight update get carried out all the same, just with gradients that are all zero? I guess what I'm trying to understand is, for a very deep network, will this method still save time / memory?</p>", "body_text": "@samjabrahams so theres no need for the gradient_override_map with the Identity op? Does your code actually skip the layer during backprop, or does the weight update get carried out all the same, just with gradients that are all zero? I guess what I'm trying to understand is, for a very deep network, will this method still save time / memory?", "body": "@samjabrahams so theres no need for the `gradient_override_map` with the Identity op? Does your code actually skip the layer during backprop, or does the weight update get carried out all the same, just with gradients that are all zero? I guess what I'm trying to understand is, for a very deep network, will this method still save time / memory?\r\n"}