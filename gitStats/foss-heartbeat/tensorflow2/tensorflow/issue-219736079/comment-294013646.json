{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/294013646", "html_url": "https://github.com/tensorflow/tensorflow/issues/9001#issuecomment-294013646", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9001", "id": 294013646, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDAxMzY0Ng==", "user": {"login": "alexanderalang", "id": 22649804, "node_id": "MDQ6VXNlcjIyNjQ5ODA0", "avatar_url": "https://avatars1.githubusercontent.com/u/22649804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexanderalang", "html_url": "https://github.com/alexanderalang", "followers_url": "https://api.github.com/users/alexanderalang/followers", "following_url": "https://api.github.com/users/alexanderalang/following{/other_user}", "gists_url": "https://api.github.com/users/alexanderalang/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexanderalang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexanderalang/subscriptions", "organizations_url": "https://api.github.com/users/alexanderalang/orgs", "repos_url": "https://api.github.com/users/alexanderalang/repos", "events_url": "https://api.github.com/users/alexanderalang/events{/privacy}", "received_events_url": "https://api.github.com/users/alexanderalang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-13T20:31:11Z", "updated_at": "2017-04-13T23:33:39Z", "author_association": "NONE", "body_html": "<p>Ok, this fixes the problem of the gradient not being passed on to the previous layer during backprop if a layer is disabled.</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n@tf.RegisterGradient(\"PassThroughGrad\")\ndef _pass_through_grad(unused_op, grad):\n    return grad, grad\n\ndef pass_through(inputs,name):\n    with tf.Graph().as_default() as g:\n        with g.gradient_override_map({name:\"PassThroughGrad\"}):\n            I = tf.identity(inputs,name)\n            return I\n\ndef layer_wrapper(layer,layer_args,layer_kwargs={},switch=1,id_op_num=None):\n    id_op_name= 'my_id_op/{}'.format(id_op_num)\n    output = layer(*layer_args,**layer_kwargs) #normal output of layer\n        \n    forward = tf.cond(tf.cast(switch,tf.bool), #if layer is disabled, inputs are passed on to next layer. \n                     lambda: output,\n                     lambda: layer_args[0])\n        \n    backward = tf.cond(tf.cast(switch,tf.bool), #gradient is stopped for backprop if layer is disabled\n                     lambda: forward,\n                     lambda: pass_through(forward,id_op_name))\n        \n    return backward + tf.stop_gradient(forward-backward)\n\n\ndef dropout(prob):\n    def dec(layer):\n        def wrapperwrapper(*layer_args,**layer_kwargs):\n            switch = make_dropout_vars(prob)\n            return layer_wrapper(layer,layer_args,layer_kwargs,switch=switch,id_op_num=switch.name[-1])\n        return wrapperwrapper\n    return dec\n\n\n\ndef assign_switches(vec):\n    with tf.variable_scope('layer_dropout', reuse=True):\n        switches = tf.get_collection('switches')\n    assert len(vec) == len(switches)\n    return [switches[i].assign(vec[i]) for i in range(len(switches))]\n\ndef make_dropout_vars(prob=0.5):\n    with tf.variable_scope('layer_dropout', reuse=True):\n        switches = tf.get_collection('switches')\n        switch = tf.Variable(1,name=\"switches/\"+str(len(switches)))\n        drop_prob = tf.Variable(prob,name=\"probability/\"+str(len(switches)))\n        tf.add_to_collection('switches',switch)\n        tf.add_to_collection('dropout_probabilities',drop_prob)\n        return switch\n</code></pre>\n<p>Then you just decorate your function with <code>@dropout(0.5)</code> or whatever probability you want.<br>\nAt this point I'm just wondering if there is a safer way of storing/assigning switching and dropout_probability variables. I'm still a tensorflow noob so variable namespacing and reuse kind of scares me. Also i'll take any tips for the naming of the Identity ops in pass_through to make sure that they are all unique, the numbering method I'm currently using seems sketchy.</p>", "body_text": "Ok, this fixes the problem of the gradient not being passed on to the previous layer during backprop if a layer is disabled.\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n@tf.RegisterGradient(\"PassThroughGrad\")\ndef _pass_through_grad(unused_op, grad):\n    return grad, grad\n\ndef pass_through(inputs,name):\n    with tf.Graph().as_default() as g:\n        with g.gradient_override_map({name:\"PassThroughGrad\"}):\n            I = tf.identity(inputs,name)\n            return I\n\ndef layer_wrapper(layer,layer_args,layer_kwargs={},switch=1,id_op_num=None):\n    id_op_name= 'my_id_op/{}'.format(id_op_num)\n    output = layer(*layer_args,**layer_kwargs) #normal output of layer\n        \n    forward = tf.cond(tf.cast(switch,tf.bool), #if layer is disabled, inputs are passed on to next layer. \n                     lambda: output,\n                     lambda: layer_args[0])\n        \n    backward = tf.cond(tf.cast(switch,tf.bool), #gradient is stopped for backprop if layer is disabled\n                     lambda: forward,\n                     lambda: pass_through(forward,id_op_name))\n        \n    return backward + tf.stop_gradient(forward-backward)\n\n\ndef dropout(prob):\n    def dec(layer):\n        def wrapperwrapper(*layer_args,**layer_kwargs):\n            switch = make_dropout_vars(prob)\n            return layer_wrapper(layer,layer_args,layer_kwargs,switch=switch,id_op_num=switch.name[-1])\n        return wrapperwrapper\n    return dec\n\n\n\ndef assign_switches(vec):\n    with tf.variable_scope('layer_dropout', reuse=True):\n        switches = tf.get_collection('switches')\n    assert len(vec) == len(switches)\n    return [switches[i].assign(vec[i]) for i in range(len(switches))]\n\ndef make_dropout_vars(prob=0.5):\n    with tf.variable_scope('layer_dropout', reuse=True):\n        switches = tf.get_collection('switches')\n        switch = tf.Variable(1,name=\"switches/\"+str(len(switches)))\n        drop_prob = tf.Variable(prob,name=\"probability/\"+str(len(switches)))\n        tf.add_to_collection('switches',switch)\n        tf.add_to_collection('dropout_probabilities',drop_prob)\n        return switch\n\nThen you just decorate your function with @dropout(0.5) or whatever probability you want.\nAt this point I'm just wondering if there is a safer way of storing/assigning switching and dropout_probability variables. I'm still a tensorflow noob so variable namespacing and reuse kind of scares me. Also i'll take any tips for the naming of the Identity ops in pass_through to make sure that they are all unique, the numbering method I'm currently using seems sketchy.", "body": "Ok, this fixes the problem of the gradient not being passed on to the previous layer during backprop if a layer is disabled.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\n\r\n@tf.RegisterGradient(\"PassThroughGrad\")\r\ndef _pass_through_grad(unused_op, grad):\r\n    return grad, grad\r\n\r\ndef pass_through(inputs,name):\r\n    with tf.Graph().as_default() as g:\r\n        with g.gradient_override_map({name:\"PassThroughGrad\"}):\r\n            I = tf.identity(inputs,name)\r\n            return I\r\n\r\ndef layer_wrapper(layer,layer_args,layer_kwargs={},switch=1,id_op_num=None):\r\n    id_op_name= 'my_id_op/{}'.format(id_op_num)\r\n    output = layer(*layer_args,**layer_kwargs) #normal output of layer\r\n        \r\n    forward = tf.cond(tf.cast(switch,tf.bool), #if layer is disabled, inputs are passed on to next layer. \r\n                     lambda: output,\r\n                     lambda: layer_args[0])\r\n        \r\n    backward = tf.cond(tf.cast(switch,tf.bool), #gradient is stopped for backprop if layer is disabled\r\n                     lambda: forward,\r\n                     lambda: pass_through(forward,id_op_name))\r\n        \r\n    return backward + tf.stop_gradient(forward-backward)\r\n\r\n\r\ndef dropout(prob):\r\n    def dec(layer):\r\n        def wrapperwrapper(*layer_args,**layer_kwargs):\r\n            switch = make_dropout_vars(prob)\r\n            return layer_wrapper(layer,layer_args,layer_kwargs,switch=switch,id_op_num=switch.name[-1])\r\n        return wrapperwrapper\r\n    return dec\r\n\r\n\r\n\r\ndef assign_switches(vec):\r\n    with tf.variable_scope('layer_dropout', reuse=True):\r\n        switches = tf.get_collection('switches')\r\n    assert len(vec) == len(switches)\r\n    return [switches[i].assign(vec[i]) for i in range(len(switches))]\r\n\r\ndef make_dropout_vars(prob=0.5):\r\n    with tf.variable_scope('layer_dropout', reuse=True):\r\n        switches = tf.get_collection('switches')\r\n        switch = tf.Variable(1,name=\"switches/\"+str(len(switches)))\r\n        drop_prob = tf.Variable(prob,name=\"probability/\"+str(len(switches)))\r\n        tf.add_to_collection('switches',switch)\r\n        tf.add_to_collection('dropout_probabilities',drop_prob)\r\n        return switch\r\n```\r\nThen you just decorate your function with `@dropout(0.5)` or whatever probability you want.\r\nAt this point I'm just wondering if there is a safer way of storing/assigning switching and dropout_probability variables. I'm still a tensorflow noob so variable namespacing and reuse kind of scares me. Also i'll take any tips for the naming of the Identity ops in pass_through to make sure that they are all unique, the numbering method I'm currently using seems sketchy."}