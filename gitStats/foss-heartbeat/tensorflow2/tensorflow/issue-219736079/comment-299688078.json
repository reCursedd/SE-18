{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299688078", "html_url": "https://github.com/tensorflow/tensorflow/issues/9001#issuecomment-299688078", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9001", "id": 299688078, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTY4ODA3OA==", "user": {"login": "samjabrahams", "id": 11607205, "node_id": "MDQ6VXNlcjExNjA3MjA1", "avatar_url": "https://avatars0.githubusercontent.com/u/11607205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samjabrahams", "html_url": "https://github.com/samjabrahams", "followers_url": "https://api.github.com/users/samjabrahams/followers", "following_url": "https://api.github.com/users/samjabrahams/following{/other_user}", "gists_url": "https://api.github.com/users/samjabrahams/gists{/gist_id}", "starred_url": "https://api.github.com/users/samjabrahams/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samjabrahams/subscriptions", "organizations_url": "https://api.github.com/users/samjabrahams/orgs", "repos_url": "https://api.github.com/users/samjabrahams/repos", "events_url": "https://api.github.com/users/samjabrahams/events{/privacy}", "received_events_url": "https://api.github.com/users/samjabrahams/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-07T07:31:53Z", "updated_at": "2017-05-07T07:31:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi guys,</p>\n<p>I wanted to chip in a bit. I may be completely missing something, but I believe <code>tf.cond</code> already sets the gradient for the unused path to zero. This should prevent unwanted updates to Variables. Here is some code from a presentation/demo I'm giving tomorrow:</p>\n<p>Stochastic depth layer (with 1x1 convolution depth mapping, if needed). Not totally full-featured, but works for a simple illustration.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">stochastic_depth_conv2d</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">filters</span>, <span class=\"pl-smi\">kernel_size</span>, <span class=\"pl-smi\">keep_prob</span>, <span class=\"pl-smi\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>, <span class=\"pl-smi\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu, <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    default_name <span class=\"pl-k\">=</span> tf.get_default_graph().unique_name(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>stochastic_depth_conv<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">with</span> tf.variable_scope(name, default_name):\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">full_layer</span>():\n            <span class=\"pl-k\">return</span> tf.layers.conv2d(inputs, filters, kernel_size, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span>padding, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>activation, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">skip_layer</span>():\n            <span class=\"pl-k\">if</span> inputs.get_shape().as_list()[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>] <span class=\"pl-k\">!=</span> filters:\n                <span class=\"pl-k\">return</span> tf.layers.conv2d(inputs, filters, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span>padding, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>activation, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>skip<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-k\">else</span>:\n                <span class=\"pl-k\">return</span> inputs\n        pred <span class=\"pl-k\">=</span> tf.random_uniform([]) <span class=\"pl-k\">&lt;</span> keep_prob\n        <span class=\"pl-k\">return</span> tf.cond(pred, full_layer, skip_layer), pred</pre></div>\n<p>Some code to print out gradients based on randomly switching some layers \"on\" or \"off\":</p>\n<div class=\"highlight highlight-source-python\"><pre>tf.reset_default_graph()\ninputs <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>inputs<span class=\"pl-pds\">'</span></span>)\nkeep_prob <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>keep_prob<span class=\"pl-pds\">'</span></span>)\nconv1, pred1 <span class=\"pl-k\">=</span> stochastic_depth_conv2d(inputs, <span class=\"pl-c1\">3</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], keep_prob, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1<span class=\"pl-pds\">'</span></span>)\nconv2, pred2 <span class=\"pl-k\">=</span> stochastic_depth_conv2d(conv1, <span class=\"pl-c1\">3</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], keep_prob, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2<span class=\"pl-pds\">'</span></span>)\nconv3, pred3 <span class=\"pl-k\">=</span> stochastic_depth_conv2d(conv2, <span class=\"pl-c1\">3</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], keep_prob, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv3<span class=\"pl-pds\">'</span></span>)\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n\nopt <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.05</span>)\nvar_list <span class=\"pl-k\">=</span> tf.trainable_variables()\ngrads <span class=\"pl-k\">=</span> opt.compute_gradients(conv3, var_list)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(init)\n    feed_dict <span class=\"pl-k\">=</span> {\n        inputs: np.random.normal(<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>]),\n        keep_prob: <span class=\"pl-c1\">0.5</span>\n    }\n    g, p1, p2, p3 <span class=\"pl-k\">=</span> sess.run([grads, pred1, pred2, pred3], feed_dict)\n    <span class=\"pl-c1\">print</span>(p1, p2, p3)\n    <span class=\"pl-k\">for</span> var, grad_value <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(var_list, g):\n        grad, value <span class=\"pl-k\">=</span> grad_value\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>,var.op.name, grad.squeeze(), <span class=\"pl-v\">sep</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>Sample output from the above:</p>\n<pre><code>False True False\n\nconv1/conv/kernel\n[[[ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]]\n\n [[ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]]\n\n [[ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]]]\n\nconv1/conv/bias\n[ 0.  0.  0.]\n\nconv1/skip/kernel\n[ 0.88905168  1.15772355  3.13942909]\n\nconv1/skip/bias\n[-0.24839747  0.25073451  2.12950516]\n\nconv2/conv/kernel\n[[[[ 1.11243725  0.06358987  0.06358987]\n   [ 1.97543406  0.11292107  0.11292107]\n   [ 1.45493352  0.08316787  0.08316787]]\n\n  [[ 0.06358987  0.          1.04884744]\n   [ 0.11292107  0.          1.86251295]\n   [ 0.08316787  0.          1.37176561]]\n\n  [[ 0.          0.          0.06358987]\n   [ 0.          0.          0.11292107]\n   [ 0.          0.          0.08316787]]]\n\n\n [[[ 0.          0.          1.93598056]\n   [ 0.          0.          3.43785858]\n   [ 0.          0.          2.53202868]]\n\n  [[ 1.87239075  1.87239075  0.06358987]\n   [ 3.32493734  3.32493734  0.11292107]\n   [ 2.44886065  2.44886065  0.08316787]]\n\n  [[ 0.06358987  0.06358987  0.        ]\n   [ 0.11292107  0.11292107  0.        ]\n   [ 0.08316787  0.08316787  0.        ]]]\n\n\n [[[ 0.82354331  0.82354331  0.82354331]\n   [ 1.4624244   1.4624244   1.4624244 ]\n   [ 1.07709503  1.07709503  1.07709503]]\n\n  [[ 0.82354331  0.          0.        ]\n   [ 1.4624244   0.          0.        ]\n   [ 1.07709503  0.          0.        ]]\n\n  [[ 0.          0.          0.82354331]\n   [ 0.          0.          1.4624244 ]\n   [ 0.          0.          1.07709503]]]]\n\nconv2/conv/bias\n[ 4.  3.  5.]\n\nconv3/conv/kernel\n[[[[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]]\n\n\n [[[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]]\n\n\n [[[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]]]\n\nconv3/conv/bias\n[ 0.  0.  0.]\n</code></pre>", "body_text": "Hi guys,\nI wanted to chip in a bit. I may be completely missing something, but I believe tf.cond already sets the gradient for the unused path to zero. This should prevent unwanted updates to Variables. Here is some code from a presentation/demo I'm giving tomorrow:\nStochastic depth layer (with 1x1 convolution depth mapping, if needed). Not totally full-featured, but works for a simple illustration.\ndef stochastic_depth_conv2d(inputs, filters, kernel_size, keep_prob, padding='same', activation=tf.nn.relu, name=None):\n    default_name = tf.get_default_graph().unique_name('stochastic_depth_conv')\n    with tf.variable_scope(name, default_name):\n        def full_layer():\n            return tf.layers.conv2d(inputs, filters, kernel_size, padding=padding, activation=activation, name='conv')\n        def skip_layer():\n            if inputs.get_shape().as_list()[-1] != filters:\n                return tf.layers.conv2d(inputs, filters, [1, 1], padding=padding, activation=activation, name='skip')\n            else:\n                return inputs\n        pred = tf.random_uniform([]) < keep_prob\n        return tf.cond(pred, full_layer, skip_layer), pred\nSome code to print out gradients based on randomly switching some layers \"on\" or \"off\":\ntf.reset_default_graph()\ninputs = tf.placeholder(tf.float32, [None, 3, 3, 1], name='inputs')\nkeep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\nconv1, pred1 = stochastic_depth_conv2d(inputs, 3, [3, 3], keep_prob, name='conv1')\nconv2, pred2 = stochastic_depth_conv2d(conv1, 3, [3, 3], keep_prob, name='conv2')\nconv3, pred3 = stochastic_depth_conv2d(conv2, 3, [3, 3], keep_prob, name='conv3')\ninit = tf.global_variables_initializer()\n\nopt = tf.train.GradientDescentOptimizer(0.05)\nvar_list = tf.trainable_variables()\ngrads = opt.compute_gradients(conv3, var_list)\n\nwith tf.Session() as sess:\n    sess.run(init)\n    feed_dict = {\n        inputs: np.random.normal(size=[1, 3, 3, 1]),\n        keep_prob: 0.5\n    }\n    g, p1, p2, p3 = sess.run([grads, pred1, pred2, pred3], feed_dict)\n    print(p1, p2, p3)\n    for var, grad_value in zip(var_list, g):\n        grad, value = grad_value\n        print('',var.op.name, grad.squeeze(), sep='\\n')\nSample output from the above:\nFalse True False\n\nconv1/conv/kernel\n[[[ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]]\n\n [[ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]]\n\n [[ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]]]\n\nconv1/conv/bias\n[ 0.  0.  0.]\n\nconv1/skip/kernel\n[ 0.88905168  1.15772355  3.13942909]\n\nconv1/skip/bias\n[-0.24839747  0.25073451  2.12950516]\n\nconv2/conv/kernel\n[[[[ 1.11243725  0.06358987  0.06358987]\n   [ 1.97543406  0.11292107  0.11292107]\n   [ 1.45493352  0.08316787  0.08316787]]\n\n  [[ 0.06358987  0.          1.04884744]\n   [ 0.11292107  0.          1.86251295]\n   [ 0.08316787  0.          1.37176561]]\n\n  [[ 0.          0.          0.06358987]\n   [ 0.          0.          0.11292107]\n   [ 0.          0.          0.08316787]]]\n\n\n [[[ 0.          0.          1.93598056]\n   [ 0.          0.          3.43785858]\n   [ 0.          0.          2.53202868]]\n\n  [[ 1.87239075  1.87239075  0.06358987]\n   [ 3.32493734  3.32493734  0.11292107]\n   [ 2.44886065  2.44886065  0.08316787]]\n\n  [[ 0.06358987  0.06358987  0.        ]\n   [ 0.11292107  0.11292107  0.        ]\n   [ 0.08316787  0.08316787  0.        ]]]\n\n\n [[[ 0.82354331  0.82354331  0.82354331]\n   [ 1.4624244   1.4624244   1.4624244 ]\n   [ 1.07709503  1.07709503  1.07709503]]\n\n  [[ 0.82354331  0.          0.        ]\n   [ 1.4624244   0.          0.        ]\n   [ 1.07709503  0.          0.        ]]\n\n  [[ 0.          0.          0.82354331]\n   [ 0.          0.          1.4624244 ]\n   [ 0.          0.          1.07709503]]]]\n\nconv2/conv/bias\n[ 4.  3.  5.]\n\nconv3/conv/kernel\n[[[[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]]\n\n\n [[[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]]\n\n\n [[[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]\n\n  [[ 0.  0.  0.]\n   [ 0.  0.  0.]\n   [ 0.  0.  0.]]]]\n\nconv3/conv/bias\n[ 0.  0.  0.]", "body": "Hi guys,\r\n\r\nI wanted to chip in a bit. I may be completely missing something, but I believe `tf.cond` already sets the gradient for the unused path to zero. This should prevent unwanted updates to Variables. Here is some code from a presentation/demo I'm giving tomorrow:\r\n\r\nStochastic depth layer (with 1x1 convolution depth mapping, if needed). Not totally full-featured, but works for a simple illustration.\r\n\r\n```python\r\ndef stochastic_depth_conv2d(inputs, filters, kernel_size, keep_prob, padding='same', activation=tf.nn.relu, name=None):\r\n    default_name = tf.get_default_graph().unique_name('stochastic_depth_conv')\r\n    with tf.variable_scope(name, default_name):\r\n        def full_layer():\r\n            return tf.layers.conv2d(inputs, filters, kernel_size, padding=padding, activation=activation, name='conv')\r\n        def skip_layer():\r\n            if inputs.get_shape().as_list()[-1] != filters:\r\n                return tf.layers.conv2d(inputs, filters, [1, 1], padding=padding, activation=activation, name='skip')\r\n            else:\r\n                return inputs\r\n        pred = tf.random_uniform([]) < keep_prob\r\n        return tf.cond(pred, full_layer, skip_layer), pred\r\n```\r\n\r\nSome code to print out gradients based on randomly switching some layers \"on\" or \"off\":\r\n\r\n```python\r\ntf.reset_default_graph()\r\ninputs = tf.placeholder(tf.float32, [None, 3, 3, 1], name='inputs')\r\nkeep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\r\nconv1, pred1 = stochastic_depth_conv2d(inputs, 3, [3, 3], keep_prob, name='conv1')\r\nconv2, pred2 = stochastic_depth_conv2d(conv1, 3, [3, 3], keep_prob, name='conv2')\r\nconv3, pred3 = stochastic_depth_conv2d(conv2, 3, [3, 3], keep_prob, name='conv3')\r\ninit = tf.global_variables_initializer()\r\n\r\nopt = tf.train.GradientDescentOptimizer(0.05)\r\nvar_list = tf.trainable_variables()\r\ngrads = opt.compute_gradients(conv3, var_list)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    feed_dict = {\r\n        inputs: np.random.normal(size=[1, 3, 3, 1]),\r\n        keep_prob: 0.5\r\n    }\r\n    g, p1, p2, p3 = sess.run([grads, pred1, pred2, pred3], feed_dict)\r\n    print(p1, p2, p3)\r\n    for var, grad_value in zip(var_list, g):\r\n        grad, value = grad_value\r\n        print('',var.op.name, grad.squeeze(), sep='\\n')\r\n```\r\n\r\nSample output from the above:\r\n\r\n```\r\nFalse True False\r\n\r\nconv1/conv/kernel\r\n[[[ 0.  0.  0.]\r\n  [ 0.  0.  0.]\r\n  [ 0.  0.  0.]]\r\n\r\n [[ 0.  0.  0.]\r\n  [ 0.  0.  0.]\r\n  [ 0.  0.  0.]]\r\n\r\n [[ 0.  0.  0.]\r\n  [ 0.  0.  0.]\r\n  [ 0.  0.  0.]]]\r\n\r\nconv1/conv/bias\r\n[ 0.  0.  0.]\r\n\r\nconv1/skip/kernel\r\n[ 0.88905168  1.15772355  3.13942909]\r\n\r\nconv1/skip/bias\r\n[-0.24839747  0.25073451  2.12950516]\r\n\r\nconv2/conv/kernel\r\n[[[[ 1.11243725  0.06358987  0.06358987]\r\n   [ 1.97543406  0.11292107  0.11292107]\r\n   [ 1.45493352  0.08316787  0.08316787]]\r\n\r\n  [[ 0.06358987  0.          1.04884744]\r\n   [ 0.11292107  0.          1.86251295]\r\n   [ 0.08316787  0.          1.37176561]]\r\n\r\n  [[ 0.          0.          0.06358987]\r\n   [ 0.          0.          0.11292107]\r\n   [ 0.          0.          0.08316787]]]\r\n\r\n\r\n [[[ 0.          0.          1.93598056]\r\n   [ 0.          0.          3.43785858]\r\n   [ 0.          0.          2.53202868]]\r\n\r\n  [[ 1.87239075  1.87239075  0.06358987]\r\n   [ 3.32493734  3.32493734  0.11292107]\r\n   [ 2.44886065  2.44886065  0.08316787]]\r\n\r\n  [[ 0.06358987  0.06358987  0.        ]\r\n   [ 0.11292107  0.11292107  0.        ]\r\n   [ 0.08316787  0.08316787  0.        ]]]\r\n\r\n\r\n [[[ 0.82354331  0.82354331  0.82354331]\r\n   [ 1.4624244   1.4624244   1.4624244 ]\r\n   [ 1.07709503  1.07709503  1.07709503]]\r\n\r\n  [[ 0.82354331  0.          0.        ]\r\n   [ 1.4624244   0.          0.        ]\r\n   [ 1.07709503  0.          0.        ]]\r\n\r\n  [[ 0.          0.          0.82354331]\r\n   [ 0.          0.          1.4624244 ]\r\n   [ 0.          0.          1.07709503]]]]\r\n\r\nconv2/conv/bias\r\n[ 4.  3.  5.]\r\n\r\nconv3/conv/kernel\r\n[[[[ 0.  0.  0.]\r\n   [ 0.  0.  0.]\r\n   [ 0.  0.  0.]]\r\n\r\n  [[ 0.  0.  0.]\r\n   [ 0.  0.  0.]\r\n   [ 0.  0.  0.]]\r\n\r\n  [[ 0.  0.  0.]\r\n   [ 0.  0.  0.]\r\n   [ 0.  0.  0.]]]\r\n\r\n\r\n [[[ 0.  0.  0.]\r\n   [ 0.  0.  0.]\r\n   [ 0.  0.  0.]]\r\n\r\n  [[ 0.  0.  0.]\r\n   [ 0.  0.  0.]\r\n   [ 0.  0.  0.]]\r\n\r\n  [[ 0.  0.  0.]\r\n   [ 0.  0.  0.]\r\n   [ 0.  0.  0.]]]\r\n\r\n\r\n [[[ 0.  0.  0.]\r\n   [ 0.  0.  0.]\r\n   [ 0.  0.  0.]]\r\n\r\n  [[ 0.  0.  0.]\r\n   [ 0.  0.  0.]\r\n   [ 0.  0.  0.]]\r\n\r\n  [[ 0.  0.  0.]\r\n   [ 0.  0.  0.]\r\n   [ 0.  0.  0.]]]]\r\n\r\nconv3/conv/bias\r\n[ 0.  0.  0.]\r\n```"}