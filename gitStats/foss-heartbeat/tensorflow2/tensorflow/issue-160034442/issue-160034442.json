{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2840", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2840/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2840/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2840/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2840", "id": 160034442, "node_id": "MDU6SXNzdWUxNjAwMzQ0NDI=", "number": 2840, "title": "segfault in perftools::gputools::StreamExecutor::DeviceMemoryUsage - on a busy gpu", "user": {"login": "MInner", "id": 5229267, "node_id": "MDQ6VXNlcjUyMjkyNjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5229267?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MInner", "html_url": "https://github.com/MInner", "followers_url": "https://api.github.com/users/MInner/followers", "following_url": "https://api.github.com/users/MInner/following{/other_user}", "gists_url": "https://api.github.com/users/MInner/gists{/gist_id}", "starred_url": "https://api.github.com/users/MInner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MInner/subscriptions", "organizations_url": "https://api.github.com/users/MInner/orgs", "repos_url": "https://api.github.com/users/MInner/repos", "events_url": "https://api.github.com/users/MInner/events{/privacy}", "received_events_url": "https://api.github.com/users/MInner/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 284463744, "node_id": "MDU6TGFiZWwyODQ0NjM3NDQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cuda", "name": "cuda", "color": "f7c6c7", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2016-06-13T20:12:11Z", "updated_at": "2017-01-23T23:31:28Z", "closed_at": "2017-01-23T23:31:28Z", "author_association": "NONE", "body_html": "<p>I'm using tensorflow 0.9.0rc0 with cuda 7.5 on Tesla K40c .</p>\n<p>The GPU I'm specifying via <code>device_id</code> and</p>\n<pre><code>    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)\n    sess_cfg = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n</code></pre>\n<p>is running under a heavy load now (multiple instances of tf are working with it in parallel, so it has 70-80% gpu voltage), but still has ~20-30% free memory, so I wanted to run a one more tiny script.</p>\n<p>If I run the script under <code>gdb</code>, I get:</p>\n<pre><code>Program received signal SIGSEGV, Segmentation fault.\n0x00007fffd51237a1 in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /data2/users/usman/anaconda2/envs/tfpy3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\n</code></pre>\n<p>is it indeed \"out of resources\" or there's something wrong with the installation? Shouldn't it somehow signalize about those problems in user code, rather than just segfaulting?</p>", "body_text": "I'm using tensorflow 0.9.0rc0 with cuda 7.5 on Tesla K40c .\nThe GPU I'm specifying via device_id and\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)\n    sess_cfg = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n\nis running under a heavy load now (multiple instances of tf are working with it in parallel, so it has 70-80% gpu voltage), but still has ~20-30% free memory, so I wanted to run a one more tiny script.\nIf I run the script under gdb, I get:\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007fffd51237a1 in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /data2/users/usman/anaconda2/envs/tfpy3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\n\nis it indeed \"out of resources\" or there's something wrong with the installation? Shouldn't it somehow signalize about those problems in user code, rather than just segfaulting?", "body": "I'm using tensorflow 0.9.0rc0 with cuda 7.5 on Tesla K40c .\n\nThe GPU I'm specifying via `device_id` and\n\n```\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7, allow_growth=True)\n    sess_cfg = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n```\n\nis running under a heavy load now (multiple instances of tf are working with it in parallel, so it has 70-80% gpu voltage), but still has ~20-30% free memory, so I wanted to run a one more tiny script.\n\nIf I run the script under `gdb`, I get:\n\n```\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007fffd51237a1 in perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const ()\n   from /data2/users/usman/anaconda2/envs/tfpy3/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\n```\n\nis it indeed \"out of resources\" or there's something wrong with the installation? Shouldn't it somehow signalize about those problems in user code, rather than just segfaulting?\n"}