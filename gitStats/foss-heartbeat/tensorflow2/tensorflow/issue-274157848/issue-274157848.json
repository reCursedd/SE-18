{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14584", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14584/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14584/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14584/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14584", "id": 274157848, "node_id": "MDU6SXNzdWUyNzQxNTc4NDg=", "number": 14584, "title": "Contradicting behaviour in variations of tf.cond usage with tf.nn.static_state_saving_rnn", "user": {"login": "dakshvar22", "id": 8708249, "node_id": "MDQ6VXNlcjg3MDgyNDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/8708249?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dakshvar22", "html_url": "https://github.com/dakshvar22", "followers_url": "https://api.github.com/users/dakshvar22/followers", "following_url": "https://api.github.com/users/dakshvar22/following{/other_user}", "gists_url": "https://api.github.com/users/dakshvar22/gists{/gist_id}", "starred_url": "https://api.github.com/users/dakshvar22/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dakshvar22/subscriptions", "organizations_url": "https://api.github.com/users/dakshvar22/orgs", "repos_url": "https://api.github.com/users/dakshvar22/repos", "events_url": "https://api.github.com/users/dakshvar22/events{/privacy}", "received_events_url": "https://api.github.com/users/dakshvar22/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2017-11-15T13:34:08Z", "updated_at": "2018-11-14T19:13:58Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><a href=\"https://github.com/tensorflow/tensorflow/files/1474854/Model.txt\">Model.txt</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1474855/Training.txt\">Training.txt</a></p>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Custom</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2/1.3/1.4 (tested on all)</li>\n<li><strong>Bazel Version</strong> : N/A</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>CUDA/cuDNN version</strong>: Cuda 8, CuDNN 6</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1080 , 12 GB</li>\n<li><strong>Exact command to reproduce</strong>: python Training.py</li>\n</ul>\n<h3>Problem</h3>\n<p>I am dealing with long sequential data which has to be passed to an RNN. To do truncated BPTT and batching, I am using tf.contrib.training.batch_sequences_with_states API with tf.nn.static_state_saving_rnn API to transfer RNN state information to subsequent segments of the same sequence. I am using tf.RandomShuffleQueue() to store my data and to decouple the I/O from training I am running the enqueue operations asynchronously in a different thread.</p>\n<p>To facilitate a testing run after each training epoch I am using two separate <code>tf.RandomShuffleQueue()</code> structures and hence two different <code>tf.contrib.training.batch_sequences_with_states()</code> and <code>tf.nn.static_state_saving_rnn()</code> instances for train/test data correspondingly. Just the RNN cell which is passed to <code>tf.nn.static_state_saving_rnn</code> instances remains the same, so that the modified set of weights are used at test time.</p>\n<p>Moreover, I use a placeholder which is a boolean flag using which the appropriate nodes in the computation graph are switched at train/test time. This switching is done using <code>tf.cond()</code> operation.</p>\n<h4>Situation 1</h4>\n<p>The problem is that of a deadlock situation at a specific stage between the enqueue operations and training operations, both running in separate threads. The enqueue operation timeouts mostly because the queue has reached the maximum capacity and for some reason training operation never returns and is waiting to get some more data and hence no dequeue operation is called.</p>\n<h4>Situation 2</h4>\n<p>In file Model.py, if I uncomment the lines from 97-101 and comment line 104, then there is no such deadlock situation. The only difference is in the way that specific <code>tf.cond()</code> operation is written. One is in a declarative form(working code) and other is in an inline form(broken/deadlock code).</p>\n<h4>Situation 3</h4>\n<p>In file - Training.py, dummy data is generated by the <code>gen_data()</code> procedure(lines - 43-48) and called on line 61. The second parameter to this function is the number of time steps for each sequence. If this number is fixed to a value which is less than the unroll length parameter of <code>tf.contrib.training.batch_sequences_with_states()</code> instances(i.e. each sequence can very well fit in one batch itself), this deadlock does not occur irrespective of situation 1 or situation 2 described above.</p>\n<p>Hence, we suspect there is some minute intricacy in <code>tf.cond()</code> and <code>tf.nn.static_state_saving_rnn()</code> which gives rise to such a deadlock.</p>\n<h3>Source code / logs</h3>\n<p>Two files are attached(.txt files since the interface does not allow attaching files with extension .py) -</p>\n<ol>\n<li>Model.txt - Contains the Model class and the inference() method where the majority of the computation graph is built.</li>\n<li>Training.txt - Contains the client code which generates dummy data and calls to sess.run()</li>\n</ol>\n<p>The current code is according to Situation 1 as described above and the behaviour can be seen by running the command - <code>python Training.py</code></p>\n<p>Regards,<br>\nDaksh</p>", "body_text": "Model.txt\nTraining.txt\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.2/1.3/1.4 (tested on all)\nBazel Version : N/A\nPython version: 2.7\nCUDA/cuDNN version: Cuda 8, CuDNN 6\nGPU model and memory: GeForce GTX 1080 , 12 GB\nExact command to reproduce: python Training.py\n\nProblem\nI am dealing with long sequential data which has to be passed to an RNN. To do truncated BPTT and batching, I am using tf.contrib.training.batch_sequences_with_states API with tf.nn.static_state_saving_rnn API to transfer RNN state information to subsequent segments of the same sequence. I am using tf.RandomShuffleQueue() to store my data and to decouple the I/O from training I am running the enqueue operations asynchronously in a different thread.\nTo facilitate a testing run after each training epoch I am using two separate tf.RandomShuffleQueue() structures and hence two different tf.contrib.training.batch_sequences_with_states() and tf.nn.static_state_saving_rnn() instances for train/test data correspondingly. Just the RNN cell which is passed to tf.nn.static_state_saving_rnn instances remains the same, so that the modified set of weights are used at test time.\nMoreover, I use a placeholder which is a boolean flag using which the appropriate nodes in the computation graph are switched at train/test time. This switching is done using tf.cond() operation.\nSituation 1\nThe problem is that of a deadlock situation at a specific stage between the enqueue operations and training operations, both running in separate threads. The enqueue operation timeouts mostly because the queue has reached the maximum capacity and for some reason training operation never returns and is waiting to get some more data and hence no dequeue operation is called.\nSituation 2\nIn file Model.py, if I uncomment the lines from 97-101 and comment line 104, then there is no such deadlock situation. The only difference is in the way that specific tf.cond() operation is written. One is in a declarative form(working code) and other is in an inline form(broken/deadlock code).\nSituation 3\nIn file - Training.py, dummy data is generated by the gen_data() procedure(lines - 43-48) and called on line 61. The second parameter to this function is the number of time steps for each sequence. If this number is fixed to a value which is less than the unroll length parameter of tf.contrib.training.batch_sequences_with_states() instances(i.e. each sequence can very well fit in one batch itself), this deadlock does not occur irrespective of situation 1 or situation 2 described above.\nHence, we suspect there is some minute intricacy in tf.cond() and tf.nn.static_state_saving_rnn() which gives rise to such a deadlock.\nSource code / logs\nTwo files are attached(.txt files since the interface does not allow attaching files with extension .py) -\n\nModel.txt - Contains the Model class and the inference() method where the majority of the computation graph is built.\nTraining.txt - Contains the client code which generates dummy data and calls to sess.run()\n\nThe current code is according to Situation 1 as described above and the behaviour can be seen by running the command - python Training.py\nRegards,\nDaksh", "body": "[Model.txt](https://github.com/tensorflow/tensorflow/files/1474854/Model.txt)\r\n[Training.txt](https://github.com/tensorflow/tensorflow/files/1474855/Training.txt)\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.2/1.3/1.4 (tested on all)\r\n- **Bazel Version** : N/A\r\n- **Python version**: 2.7\r\n- **CUDA/cuDNN version**: Cuda 8, CuDNN 6\r\n- **GPU model and memory**: GeForce GTX 1080 , 12 GB\r\n- **Exact command to reproduce**: python Training.py\r\n\r\n### Problem\r\nI am dealing with long sequential data which has to be passed to an RNN. To do truncated BPTT and batching, I am using tf.contrib.training.batch_sequences_with_states API with tf.nn.static_state_saving_rnn API to transfer RNN state information to subsequent segments of the same sequence. I am using tf.RandomShuffleQueue() to store my data and to decouple the I/O from training I am running the enqueue operations asynchronously in a different thread. \r\n\r\nTo facilitate a testing run after each training epoch I am using two separate `tf.RandomShuffleQueue()` structures and hence two different `tf.contrib.training.batch_sequences_with_states()` and `tf.nn.static_state_saving_rnn()` instances for train/test data correspondingly. Just the RNN cell which is passed to `tf.nn.static_state_saving_rnn` instances remains the same, so that the modified set of weights are used at test time.\r\n\r\nMoreover, I use a placeholder which is a boolean flag using which the appropriate nodes in the computation graph are switched at train/test time. This switching is done using `tf.cond()` operation.\r\n\r\n#### Situation 1\r\n\r\nThe problem is that of a deadlock situation at a specific stage between the enqueue operations and training operations, both running in separate threads. The enqueue operation timeouts mostly because the queue has reached the maximum capacity and for some reason training operation never returns and is waiting to get some more data and hence no dequeue operation is called.\r\n\r\n#### Situation 2\r\n\r\nIn file Model.py, if I uncomment the lines from 97-101 and comment line 104, then there is no such deadlock situation. The only difference is in the way that specific `tf.cond()` operation is written. One is in a declarative form(working code) and other is in an inline form(broken/deadlock code).\r\n\r\n#### Situation 3\r\n\r\nIn file - Training.py, dummy data is generated by the `gen_data()` procedure(lines - 43-48) and called on line 61. The second parameter to this function is the number of time steps for each sequence. If this number is fixed to a value which is less than the unroll length parameter of `tf.contrib.training.batch_sequences_with_states()` instances(i.e. each sequence can very well fit in one batch itself), this deadlock does not occur irrespective of situation 1 or situation 2 described above.\r\n\r\nHence, we suspect there is some minute intricacy in `tf.cond()` and `tf.nn.static_state_saving_rnn()` which gives rise to such a deadlock.\r\n\r\n### Source code / logs\r\nTwo files are attached(.txt files since the interface does not allow attaching files with extension .py) - \r\n\r\n1. Model.txt - Contains the Model class and the inference() method where the majority of the computation graph is built.\r\n2. Training.txt - Contains the client code which generates dummy data and calls to sess.run()\r\n\r\nThe current code is according to Situation 1 as described above and the behaviour can be seen by running the command - `python Training.py`\r\n\r\nRegards,\r\nDaksh\r\n\r\n"}