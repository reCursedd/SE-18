{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/224507646", "html_url": "https://github.com/tensorflow/tensorflow/issues/2665#issuecomment-224507646", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2665", "id": 224507646, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDUwNzY0Ng==", "user": {"login": "tobegit3hub", "id": 2715000, "node_id": "MDQ6VXNlcjI3MTUwMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2715000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tobegit3hub", "html_url": "https://github.com/tobegit3hub", "followers_url": "https://api.github.com/users/tobegit3hub/followers", "following_url": "https://api.github.com/users/tobegit3hub/following{/other_user}", "gists_url": "https://api.github.com/users/tobegit3hub/gists{/gist_id}", "starred_url": "https://api.github.com/users/tobegit3hub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tobegit3hub/subscriptions", "organizations_url": "https://api.github.com/users/tobegit3hub/orgs", "repos_url": "https://api.github.com/users/tobegit3hub/repos", "events_url": "https://api.github.com/users/tobegit3hub/events{/privacy}", "received_events_url": "https://api.github.com/users/tobegit3hub/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-08T07:11:00Z", "updated_at": "2016-06-08T07:11:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thank <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> for helping. But actually I have used <code>worker_hosts = FLAGS.worker_hosts.split(\",\")</code>, just like what the official document said.</p>\n<p>Here's the complete code.</p>\n<pre><code>import tensorflow as tf\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n\n      # Build model...\n      loss = ...\n      global_step = tf.Variable(0)\n\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\n          loss, global_step=global_step)\n\n      saver = tf.train.Saver()\n      summary_op = tf.merge_all_summaries()\n      init_op = tf.initialize_all_variables()\n\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             logdir=\"/tmp/train_logs\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             saver=saver,\n                             global_step=global_step,\n                             save_model_secs=600)\n\n    # The supervisor takes care of session initialization and restoring from\n    # a checkpoint.\n    sess = sv.prepare_or_wait_for_session(server.target)\n\n    # Start queue runners for the input pipelines (if any).\n    sv.start_queue_runners(sess)\n\n    # Loop until the supervisor shuts down (or 1000000 steps have completed).\n    step = 0\n    while not sv.should_stop() and step &lt; 1000000:\n      # Run a training step asynchronously.\n      # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n      # perform *synchronous* training.\n      _, step = sess.run([train_op, global_step])\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n</code></pre>", "body_text": "Thank @mrry for helping. But actually I have used worker_hosts = FLAGS.worker_hosts.split(\",\"), just like what the official document said.\nHere's the complete code.\nimport tensorflow as tf\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n\n      # Build model...\n      loss = ...\n      global_step = tf.Variable(0)\n\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\n          loss, global_step=global_step)\n\n      saver = tf.train.Saver()\n      summary_op = tf.merge_all_summaries()\n      init_op = tf.initialize_all_variables()\n\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             logdir=\"/tmp/train_logs\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             saver=saver,\n                             global_step=global_step,\n                             save_model_secs=600)\n\n    # The supervisor takes care of session initialization and restoring from\n    # a checkpoint.\n    sess = sv.prepare_or_wait_for_session(server.target)\n\n    # Start queue runners for the input pipelines (if any).\n    sv.start_queue_runners(sess)\n\n    # Loop until the supervisor shuts down (or 1000000 steps have completed).\n    step = 0\n    while not sv.should_stop() and step < 1000000:\n      # Run a training step asynchronously.\n      # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n      # perform *synchronous* training.\n      _, step = sess.run([train_op, global_step])\n\n\nif __name__ == \"__main__\":\n  tf.app.run()", "body": "Thank @mrry for helping. But actually I have used `worker_hosts = FLAGS.worker_hosts.split(\",\")`, just like what the official document said. \n\nHere's the complete code.\n\n```\nimport tensorflow as tf\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n\n      # Build model...\n      loss = ...\n      global_step = tf.Variable(0)\n\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\n          loss, global_step=global_step)\n\n      saver = tf.train.Saver()\n      summary_op = tf.merge_all_summaries()\n      init_op = tf.initialize_all_variables()\n\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             logdir=\"/tmp/train_logs\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             saver=saver,\n                             global_step=global_step,\n                             save_model_secs=600)\n\n    # The supervisor takes care of session initialization and restoring from\n    # a checkpoint.\n    sess = sv.prepare_or_wait_for_session(server.target)\n\n    # Start queue runners for the input pipelines (if any).\n    sv.start_queue_runners(sess)\n\n    # Loop until the supervisor shuts down (or 1000000 steps have completed).\n    step = 0\n    while not sv.should_stop() and step < 1000000:\n      # Run a training step asynchronously.\n      # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n      # perform *synchronous* training.\n      _, step = sess.run([train_op, global_step])\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n```\n"}