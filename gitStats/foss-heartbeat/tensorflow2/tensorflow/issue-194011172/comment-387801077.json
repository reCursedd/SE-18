{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/387801077", "html_url": "https://github.com/tensorflow/tensorflow/issues/6153#issuecomment-387801077", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6153", "id": 387801077, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzgwMTA3Nw==", "user": {"login": "annemenini", "id": 13631130, "node_id": "MDQ6VXNlcjEzNjMxMTMw", "avatar_url": "https://avatars0.githubusercontent.com/u/13631130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/annemenini", "html_url": "https://github.com/annemenini", "followers_url": "https://api.github.com/users/annemenini/followers", "following_url": "https://api.github.com/users/annemenini/following{/other_user}", "gists_url": "https://api.github.com/users/annemenini/gists{/gist_id}", "starred_url": "https://api.github.com/users/annemenini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/annemenini/subscriptions", "organizations_url": "https://api.github.com/users/annemenini/orgs", "repos_url": "https://api.github.com/users/annemenini/repos", "events_url": "https://api.github.com/users/annemenini/events{/privacy}", "received_events_url": "https://api.github.com/users/annemenini/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T16:41:35Z", "updated_at": "2018-05-09T20:00:08Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1837393\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Mojusko\">@Mojusko</a> : At the end, was the seed the problem?<br>\nI am also seeing a similar problem (Ubuntu, python 3.5, TF1.8).<br>\nSame code, same data, same machine, just switching between GPU and CPU execution by adding <code>os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"</code> gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).</p>\n<p>I gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a <code>tf.multiply</code> (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.<br>\nUnfortunately, I am not able to reproduce the problem in a unit test. If I feed the same inputs to <code>tf.multiply</code> in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.</p>\n<p>Regarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.</p>\n<p>Note that the problem is very reproducible.</p>\n<p>If I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.</p>\n<p>Any idea or debugging experiment suggestion is very welcome.</p>", "body_text": "@Mojusko : At the end, was the seed the problem?\nI am also seeing a similar problem (Ubuntu, python 3.5, TF1.8).\nSame code, same data, same machine, just switching between GPU and CPU execution by adding os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).\nI gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a tf.multiply (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.\nUnfortunately, I am not able to reproduce the problem in a unit test. If I feed the same inputs to tf.multiply in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.\nRegarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.\nNote that the problem is very reproducible.\nIf I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.\nAny idea or debugging experiment suggestion is very welcome.", "body": "@Mojusko : At the end, was the seed the problem? \r\nI am also seeing a similar problem (Ubuntu, python 3.5, TF1.8).\r\nSame code, same data, same machine, just switching between GPU and CPU execution by adding `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).\r\n\r\nI gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a `tf.multiply` (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.\r\nUnfortunately, I am not able to reproduce the problem in a unit test. If I feed the same inputs to `tf.multiply` in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.\r\n\r\nRegarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.\r\n\r\nNote that the problem is very reproducible.\r\n\r\nIf I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.\r\n\r\nAny idea or debugging experiment suggestion is very welcome. "}