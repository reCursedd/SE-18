{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/352414910", "html_url": "https://github.com/tensorflow/tensorflow/issues/6153#issuecomment-352414910", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6153", "id": 352414910, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MjQxNDkxMA==", "user": {"login": "fvisconti", "id": 9214242, "node_id": "MDQ6VXNlcjkyMTQyNDI=", "avatar_url": "https://avatars1.githubusercontent.com/u/9214242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fvisconti", "html_url": "https://github.com/fvisconti", "followers_url": "https://api.github.com/users/fvisconti/followers", "following_url": "https://api.github.com/users/fvisconti/following{/other_user}", "gists_url": "https://api.github.com/users/fvisconti/gists{/gist_id}", "starred_url": "https://api.github.com/users/fvisconti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fvisconti/subscriptions", "organizations_url": "https://api.github.com/users/fvisconti/orgs", "repos_url": "https://api.github.com/users/fvisconti/repos", "events_url": "https://api.github.com/users/fvisconti/events{/privacy}", "received_events_url": "https://api.github.com/users/fvisconti/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-18T12:40:28Z", "updated_at": "2017-12-18T12:40:28Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>I have exactly the <strong>opposite</strong> behaviour.<br>\nMy net is quite simple:</p>\n<div class=\"highlight highlight-source-python\"><pre>model <span class=\"pl-k\">=</span> Sequential()\n    model.add(Conv2D(<span class=\"pl-c1\">32</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>input_shape))\n    model.add(Activation(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\n    model.add(MaxPooling2D(<span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)))\n    model.add(Conv2D(<span class=\"pl-c1\">32</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>)))\n    model.add(Activation(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\n    model.add(MaxPooling2D(<span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)))\n    model.add(Conv2D(<span class=\"pl-c1\">64</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>)))\n    model.add(Activation(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\n    model.add(MaxPooling2D(<span class=\"pl-v\">pool_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)))\n    model.add(Flatten())\n    model.add(Dense(<span class=\"pl-c1\">64</span>))\n    model.add(Activation(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\n    model.add(Dropout(<span class=\"pl-c1\">0.5</span>))\n    model.add(Dense(<span class=\"pl-c1\">1</span>))\n    model.add(Activation(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>))\n\n    model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>binary_crossentropy<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>adam<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>])</pre></div>\n<p>same inputs, running on two systems, CPU (Intel Core i5, compiled from source with SIMD support) and GPU (Nvidia Tesla K20).</p>\n<p>GPU run is significantly faster (as expected, and I'm quite surprised to read this issue here); what is surprising is that after the first epoch on <strong>CPU I have</strong><br>\n<code>4951s 566ms/step - loss: 0.7037 - acc: 0.4903  - val_loss: 0.6931 - val_acc: 0.5000</code><br>\nwhile <strong>on GPU</strong><br>\n<code>2892s 331ms/step - loss: 0.8896 - acc: 0.7267  - val_loss: 0.2933 - val_acc: 0.8777</code>.</p>\n<p>How can I investigate in this?</p>", "body_text": "Hi all,\nI have exactly the opposite behaviour.\nMy net is quite simple:\nmodel = Sequential()\n    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(32, (3, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(64, (3, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(64))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nsame inputs, running on two systems, CPU (Intel Core i5, compiled from source with SIMD support) and GPU (Nvidia Tesla K20).\nGPU run is significantly faster (as expected, and I'm quite surprised to read this issue here); what is surprising is that after the first epoch on CPU I have\n4951s 566ms/step - loss: 0.7037 - acc: 0.4903  - val_loss: 0.6931 - val_acc: 0.5000\nwhile on GPU\n2892s 331ms/step - loss: 0.8896 - acc: 0.7267  - val_loss: 0.2933 - val_acc: 0.8777.\nHow can I investigate in this?", "body": "Hi all,\r\n\r\nI have exactly the **opposite** behaviour.\r\nMy net is quite simple:\r\n```python\r\nmodel = Sequential()\r\n    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n    model.add(Conv2D(32, (3, 3)))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n    model.add(Conv2D(64, (3, 3)))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n    model.add(Flatten())\r\n    model.add(Dense(64))\r\n    model.add(Activation('relu'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(1))\r\n    model.add(Activation('sigmoid'))\r\n\r\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n```\r\nsame inputs, running on two systems, CPU (Intel Core i5, compiled from source with SIMD support) and GPU (Nvidia Tesla K20).\r\n\r\nGPU run is significantly faster (as expected, and I'm quite surprised to read this issue here); what is surprising is that after the first epoch on **CPU I have**\r\n```4951s 566ms/step - loss: 0.7037 - acc: 0.4903  - val_loss: 0.6931 - val_acc: 0.5000```\r\nwhile **on GPU**\r\n```2892s 331ms/step - loss: 0.8896 - acc: 0.7267  - val_loss: 0.2933 - val_acc: 0.8777```.\r\n\r\nHow can I investigate in this?"}