{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19333", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19333/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19333/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19333/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19333", "id": 323766654, "node_id": "MDU6SXNzdWUzMjM3NjY2NTQ=", "number": 19333, "title": "Bug in EluGradGrad", "user": {"login": "zero-impact", "id": 1796974, "node_id": "MDQ6VXNlcjE3OTY5NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1796974?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zero-impact", "html_url": "https://github.com/zero-impact", "followers_url": "https://api.github.com/users/zero-impact/followers", "following_url": "https://api.github.com/users/zero-impact/following{/other_user}", "gists_url": "https://api.github.com/users/zero-impact/gists{/gist_id}", "starred_url": "https://api.github.com/users/zero-impact/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zero-impact/subscriptions", "organizations_url": "https://api.github.com/users/zero-impact/orgs", "repos_url": "https://api.github.com/users/zero-impact/repos", "events_url": "https://api.github.com/users/zero-impact/events{/privacy}", "received_events_url": "https://api.github.com/users/zero-impact/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-05-16T20:04:24Z", "updated_at": "2018-06-15T01:16:14Z", "closed_at": "2018-06-15T01:16:14Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes, see below</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary - latest pip tf_nightly</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.8.0-1674-gd8fac4cb80 1.9.0-dev20180515</li>\n<li><strong>Python version</strong>: Python 3.6.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA Version 9.1.85</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 970</li>\n<li><strong>Exact command to reproduce</strong>: See script attached</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>There seems to be an issue with EluGradGrad that I have uncovered. Please see the post here: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"234418055\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/renmengye/tensorflow-forward-ad/issues/2\" data-hovercard-type=\"issue\" data-hovercard-url=\"/renmengye/tensorflow-forward-ad/issues/2/hovercard?comment_id=389321546&amp;comment_type=issue_comment\" href=\"https://github.com/renmengye/tensorflow-forward-ad/issues/2#issuecomment-389321546\">renmengye/tensorflow-forward-ad#2 (comment)</a></p>\n<h3>Source code / logs</h3>\n<p>Running this script will demonstrate the incorrect values:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">fwd_gradients</span>(<span class=\"pl-smi\">ys</span>, <span class=\"pl-smi\">xs</span>, <span class=\"pl-smi\">d_xs</span>):\n    dummy <span class=\"pl-k\">=</span> tf.zeros_like(ys)\n    g <span class=\"pl-k\">=</span> tf.gradients(ys, xs, <span class=\"pl-v\">grad_ys</span><span class=\"pl-k\">=</span>dummy, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gradients<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">return</span> tf.gradients(g, dummy, <span class=\"pl-v\">grad_ys</span><span class=\"pl-k\">=</span>d_xs, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>jvp<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">my_elu</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> tf.where(x <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">0.0</span>, x, tf.exp(x) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1.0</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span class=\"pl-c1\">print</span>(tf.<span class=\"pl-c1\">__version__</span>)\n\n    sess <span class=\"pl-k\">=</span> tf.InteractiveSession()\n    init <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> activation = my_elu # Works correctly tf.nn.relu (or any other non-elu activation)</span>\n    activation <span class=\"pl-k\">=</span> tf.nn.elu\n\n    x_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\n    y_size <span class=\"pl-k\">=</span> x_size\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Single ELU or RELU op</span>\n    X <span class=\"pl-k\">=</span> tf.placeholder(tf.float64, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[x_size]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Input</span>\n    Y <span class=\"pl-k\">=</span> activation(X) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Output</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define vjp and jvp</span>\n    Vx <span class=\"pl-k\">=</span> tf.placeholder(tf.float64, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[x_size]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Jac-vector product input V</span>\n    Vy <span class=\"pl-k\">=</span> tf.placeholder(tf.float64, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[y_size]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> vector-jac product input V</span>\n    jvp <span class=\"pl-k\">=</span> fwd_gradients(Y, X, <span class=\"pl-v\">d_xs</span><span class=\"pl-k\">=</span>Vx)\n    vjp <span class=\"pl-k\">=</span> tf.gradients(Y, X, <span class=\"pl-v\">grad_ys</span><span class=\"pl-k\">=</span>Vy)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Compute jacobians</span>\n    x <span class=\"pl-k\">=</span> np.ones(x_size) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1.5</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> Bug only occurs in x &lt; 0 region</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> x = np.random.normal(-1, 1, x_size)</span>\n    tf_jac, numeric_jac <span class=\"pl-k\">=</span> tf.test.compute_gradient(X, [x_size], Y, [y_size], <span class=\"pl-v\">x_init_value</span><span class=\"pl-k\">=</span>x)\n    vjp_jac <span class=\"pl-k\">=</span> np.array([sess.run(vjp, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{X: x, Vy: v})[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> np.identity(y_size)])\n    jvp_jac <span class=\"pl-k\">=</span> np.array([sess.run(jvp, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{X: x, Vx: v})[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> np.identity(x_size)])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Print results as maximum absolute error</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Numeric jac:<span class=\"pl-pds\">\"</span></span>, numeric_jac)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>jvp jac:<span class=\"pl-pds\">\"</span></span>, jvp_jac)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tf error:<span class=\"pl-pds\">\"</span></span>, np.max(np.abs(numeric_jac <span class=\"pl-k\">-</span> tf_jac)))   <span class=\"pl-c\"><span class=\"pl-c\">#</span> ~0.0</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>vjp error:<span class=\"pl-pds\">\"</span></span>, np.max(np.abs(numeric_jac <span class=\"pl-k\">-</span> vjp_jac))) <span class=\"pl-c\"><span class=\"pl-c\">#</span> ~0.0</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>jvp error:<span class=\"pl-pds\">\"</span></span>, np.max(np.abs(numeric_jac <span class=\"pl-k\">-</span> jvp_jac))) <span class=\"pl-c\"><span class=\"pl-c\">#</span> LARGE! for ELU</span>\n\n    sess.close()\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>\n<p>The solution is to edit the implementation of <code>_EluGradGrad</code> <a href=\"https://github.com/tensorflow/tensorflow/blob/f318765ad5a50b2fbd7cc08dd4ebc249b3924270/tensorflow/python/ops/nn_grad.py#L364\">here</a>.</p>\n<p>I've created a pull request that references this issue.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary - latest pip tf_nightly\nTensorFlow version (use command below): v1.8.0-1674-gd8fac4cb80 1.9.0-dev20180515\nPython version: Python 3.6.3\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: CUDA Version 9.1.85\nGPU model and memory: GeForce GTX 970\nExact command to reproduce: See script attached\n\nDescribe the problem\nThere seems to be an issue with EluGradGrad that I have uncovered. Please see the post here: renmengye/tensorflow-forward-ad#2 (comment)\nSource code / logs\nRunning this script will demonstrate the incorrect values:\nimport tensorflow as tf\nimport numpy as np\n\ndef fwd_gradients(ys, xs, d_xs):\n    dummy = tf.zeros_like(ys)\n    g = tf.gradients(ys, xs, grad_ys=dummy, name=\"gradients\")\n    return tf.gradients(g, dummy, grad_ys=d_xs, name=\"jvp\")\n\ndef my_elu(x):\n    return tf.where(x >= 0.0, x, tf.exp(x) - 1.0)\n\ndef main():\n    print(tf.__version__)\n\n    sess = tf.InteractiveSession()\n    init = tf.global_variables_initializer()\n    \n    # activation = my_elu # Works correctly tf.nn.relu (or any other non-elu activation)\n    activation = tf.nn.elu\n\n    x_size = 3\n    y_size = x_size\n\n    # Single ELU or RELU op\n    X = tf.placeholder(tf.float64, shape=[x_size]) # Input\n    Y = activation(X) # Output\n\n    # Define vjp and jvp\n    Vx = tf.placeholder(tf.float64, shape=[x_size]) # Jac-vector product input V\n    Vy = tf.placeholder(tf.float64, shape=[y_size]) # vector-jac product input V\n    jvp = fwd_gradients(Y, X, d_xs=Vx)\n    vjp = tf.gradients(Y, X, grad_ys=Vy)\n\n    # Compute jacobians\n    x = np.ones(x_size) - 1.5 # Bug only occurs in x < 0 region\n    # x = np.random.normal(-1, 1, x_size)\n    tf_jac, numeric_jac = tf.test.compute_gradient(X, [x_size], Y, [y_size], x_init_value=x)\n    vjp_jac = np.array([sess.run(vjp, feed_dict={X: x, Vy: v})[0] for v in np.identity(y_size)])\n    jvp_jac = np.array([sess.run(jvp, feed_dict={X: x, Vx: v})[0] for v in np.identity(x_size)])\n\n    # Print results as maximum absolute error\n    print(\"Numeric jac:\", numeric_jac)\n    print(\"jvp jac:\", jvp_jac)\n    print(\"tf error:\", np.max(np.abs(numeric_jac - tf_jac)))   # ~0.0\n    print(\"vjp error:\", np.max(np.abs(numeric_jac - vjp_jac))) # ~0.0\n    print(\"jvp error:\", np.max(np.abs(numeric_jac - jvp_jac))) # LARGE! for ELU\n\n    sess.close()\n\nif __name__ == '__main__':\n    main()\nThe solution is to edit the implementation of _EluGradGrad here.\nI've created a pull request that references this issue.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary - latest pip tf_nightly\r\n- **TensorFlow version (use command below)**: v1.8.0-1674-gd8fac4cb80 1.9.0-dev20180515\r\n- **Python version**: Python 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA Version 9.1.85\r\n- **GPU model and memory**: GeForce GTX 970\r\n- **Exact command to reproduce**: See script attached\r\n\r\n### Describe the problem\r\n\r\nThere seems to be an issue with EluGradGrad that I have uncovered. Please see the post here: https://github.com/renmengye/tensorflow-forward-ad/issues/2#issuecomment-389321546\r\n\r\n### Source code / logs\r\n\r\nRunning this script will demonstrate the incorrect values:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef fwd_gradients(ys, xs, d_xs):\r\n    dummy = tf.zeros_like(ys)\r\n    g = tf.gradients(ys, xs, grad_ys=dummy, name=\"gradients\")\r\n    return tf.gradients(g, dummy, grad_ys=d_xs, name=\"jvp\")\r\n\r\ndef my_elu(x):\r\n    return tf.where(x >= 0.0, x, tf.exp(x) - 1.0)\r\n\r\ndef main():\r\n    print(tf.__version__)\r\n\r\n    sess = tf.InteractiveSession()\r\n    init = tf.global_variables_initializer()\r\n    \r\n    # activation = my_elu # Works correctly tf.nn.relu (or any other non-elu activation)\r\n    activation = tf.nn.elu\r\n\r\n    x_size = 3\r\n    y_size = x_size\r\n\r\n    # Single ELU or RELU op\r\n    X = tf.placeholder(tf.float64, shape=[x_size]) # Input\r\n    Y = activation(X) # Output\r\n\r\n    # Define vjp and jvp\r\n    Vx = tf.placeholder(tf.float64, shape=[x_size]) # Jac-vector product input V\r\n    Vy = tf.placeholder(tf.float64, shape=[y_size]) # vector-jac product input V\r\n    jvp = fwd_gradients(Y, X, d_xs=Vx)\r\n    vjp = tf.gradients(Y, X, grad_ys=Vy)\r\n\r\n    # Compute jacobians\r\n    x = np.ones(x_size) - 1.5 # Bug only occurs in x < 0 region\r\n    # x = np.random.normal(-1, 1, x_size)\r\n    tf_jac, numeric_jac = tf.test.compute_gradient(X, [x_size], Y, [y_size], x_init_value=x)\r\n    vjp_jac = np.array([sess.run(vjp, feed_dict={X: x, Vy: v})[0] for v in np.identity(y_size)])\r\n    jvp_jac = np.array([sess.run(jvp, feed_dict={X: x, Vx: v})[0] for v in np.identity(x_size)])\r\n\r\n    # Print results as maximum absolute error\r\n    print(\"Numeric jac:\", numeric_jac)\r\n    print(\"jvp jac:\", jvp_jac)\r\n    print(\"tf error:\", np.max(np.abs(numeric_jac - tf_jac)))   # ~0.0\r\n    print(\"vjp error:\", np.max(np.abs(numeric_jac - vjp_jac))) # ~0.0\r\n    print(\"jvp error:\", np.max(np.abs(numeric_jac - jvp_jac))) # LARGE! for ELU\r\n\r\n    sess.close()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nThe solution is to edit the implementation of `_EluGradGrad` [here](https://github.com/tensorflow/tensorflow/blob/f318765ad5a50b2fbd7cc08dd4ebc249b3924270/tensorflow/python/ops/nn_grad.py#L364).\r\n\r\nI've created a pull request that references this issue."}