{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/419678817", "html_url": "https://github.com/tensorflow/tensorflow/issues/8858#issuecomment-419678817", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8858", "id": 419678817, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTY3ODgxNw==", "user": {"login": "Miffyli", "id": 1633800, "node_id": "MDQ6VXNlcjE2MzM4MDA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1633800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Miffyli", "html_url": "https://github.com/Miffyli", "followers_url": "https://api.github.com/users/Miffyli/followers", "following_url": "https://api.github.com/users/Miffyli/following{/other_user}", "gists_url": "https://api.github.com/users/Miffyli/gists{/gist_id}", "starred_url": "https://api.github.com/users/Miffyli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Miffyli/subscriptions", "organizations_url": "https://api.github.com/users/Miffyli/orgs", "repos_url": "https://api.github.com/users/Miffyli/repos", "events_url": "https://api.github.com/users/Miffyli/events{/privacy}", "received_events_url": "https://api.github.com/users/Miffyli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-08T23:09:20Z", "updated_at": "2018-09-09T15:26:12Z", "author_association": "NONE", "body_html": "<p>Ubuntu 16.04 (KDE Neon), Tensorflow 1.9.0, CUDA 9 + CuDNN 7.0<br>\nCPU: Xeon W-2125<br>\nGPU: Asus GTX 1080 Turbo with version 384 drivers (from distribution)<br>\nPSU: Fujitsu proprietary 800W</p>\n<p>Having similar trouble when training VAE for longer than ~30mins, after which computer reboots without any errors in logs. Temperatures were at ok range (max 80C for GPU). Running Hashcat on GPU + stressing CPU with primesieve did not crash the computer over night.</p>\n<p>After checking this thread I decided to record the power usage with nvidia-smi under different loads, if that would shed more light into this.</p>\n<p>Recorded with <code> nvidia-smi --loop-ms=20 --format=csv,noheader,nounits --query-gpu=power.draw &gt; out.txt</code>.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/a47090d066ddc74aba730e9b45957b502916c714/68747470733a2f2f692e696d6775722e636f6d2f736352513542742e706e67\"><img src=\"https://camo.githubusercontent.com/a47090d066ddc74aba730e9b45957b502916c714/68747470733a2f2f692e696d6775722e636f6d2f736352513542742e706e67\" alt=\"Imgur\" data-canonical-src=\"https://i.imgur.com/scRQ5Bt.png\" style=\"max-width:100%;\"></a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/e7cfa26ff496ce6c5733d4b3855942ef3effe9e2/68747470733a2f2f692e696d6775722e636f6d2f4376316d3977452e706e67\"><img src=\"https://camo.githubusercontent.com/e7cfa26ff496ce6c5733d4b3855942ef3effe9e2/68747470733a2f2f692e696d6775722e636f6d2f4376316d3977452e706e67\" alt=\"Imgur\" data-canonical-src=\"https://i.imgur.com/Cv1m9wE.png\" style=\"max-width:100%;\"></a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/5c360409f9735e6ffe5a5fdd9a5a35c2742fe162/68747470733a2f2f692e696d6775722e636f6d2f655638416745712e706e67\"><img src=\"https://camo.githubusercontent.com/5c360409f9735e6ffe5a5fdd9a5a35c2742fe162/68747470733a2f2f692e696d6775722e636f6d2f655638416745712e706e67\" alt=\"Imgur\" data-canonical-src=\"https://i.imgur.com/eV8AgEq.png\" style=\"max-width:100%;\"></a></p>\n<p>Default max of this GTX 1080 is 180W.  Draw drops with Hashcat because of throttling, but the draw is more or less constant over time. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=103067\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nanoant\">@nanoant</a> 's code is a horrible mess for PSU to handle, peaking at 230W. My VAE training code is not that terrible, but could still cause issues.</p>\n<p>General \"wattage\" of PSU does not guarantee its capability to handle spiky draws like these. Drawing too much from one line may cause drops in voltage and such, and especially cheap PSUs die under these conditions. A 8-pin power connector (150W) and power from PCI-E port (75W) supply supported 225W, so the 230W peak draws more power from either connector than specified.</p>\n<p>Now that was an interesting little side-project to dig in, now to fix the situation... I guess the best bet is to reduce maximum allowed draw.</p>\n<p>Edit: Changing from batch-size 64 to 32 reduced average power draw by 20W and training has lasted over a hour now (previously crashed well before 1h).<br>\nEdit2: Reducing batch size worked for me, and now run has been going for 48h without issues.</p>", "body_text": "Ubuntu 16.04 (KDE Neon), Tensorflow 1.9.0, CUDA 9 + CuDNN 7.0\nCPU: Xeon W-2125\nGPU: Asus GTX 1080 Turbo with version 384 drivers (from distribution)\nPSU: Fujitsu proprietary 800W\nHaving similar trouble when training VAE for longer than ~30mins, after which computer reboots without any errors in logs. Temperatures were at ok range (max 80C for GPU). Running Hashcat on GPU + stressing CPU with primesieve did not crash the computer over night.\nAfter checking this thread I decided to record the power usage with nvidia-smi under different loads, if that would shed more light into this.\nRecorded with  nvidia-smi --loop-ms=20 --format=csv,noheader,nounits --query-gpu=power.draw > out.txt.\n\n\n\nDefault max of this GTX 1080 is 180W.  Draw drops with Hashcat because of throttling, but the draw is more or less constant over time. @nanoant 's code is a horrible mess for PSU to handle, peaking at 230W. My VAE training code is not that terrible, but could still cause issues.\nGeneral \"wattage\" of PSU does not guarantee its capability to handle spiky draws like these. Drawing too much from one line may cause drops in voltage and such, and especially cheap PSUs die under these conditions. A 8-pin power connector (150W) and power from PCI-E port (75W) supply supported 225W, so the 230W peak draws more power from either connector than specified.\nNow that was an interesting little side-project to dig in, now to fix the situation... I guess the best bet is to reduce maximum allowed draw.\nEdit: Changing from batch-size 64 to 32 reduced average power draw by 20W and training has lasted over a hour now (previously crashed well before 1h).\nEdit2: Reducing batch size worked for me, and now run has been going for 48h without issues.", "body": "Ubuntu 16.04 (KDE Neon), Tensorflow 1.9.0, CUDA 9 + CuDNN 7.0\r\nCPU: Xeon W-2125\r\nGPU: Asus GTX 1080 Turbo with version 384 drivers (from distribution)\r\nPSU: Fujitsu proprietary 800W\r\n\r\nHaving similar trouble when training VAE for longer than ~30mins, after which computer reboots without any errors in logs. Temperatures were at ok range (max 80C for GPU). Running Hashcat on GPU + stressing CPU with primesieve did not crash the computer over night.\r\n\r\nAfter checking this thread I decided to record the power usage with nvidia-smi under different loads, if that would shed more light into this.\r\n\r\nRecorded with ` nvidia-smi --loop-ms=20 --format=csv,noheader,nounits --query-gpu=power.draw > out.txt`.\r\n\r\n![Imgur](https://i.imgur.com/scRQ5Bt.png)\r\n\r\n![Imgur](https://i.imgur.com/Cv1m9wE.png)\r\n\r\n![Imgur](https://i.imgur.com/eV8AgEq.png)\r\n\r\nDefault max of this GTX 1080 is 180W.  Draw drops with Hashcat because of throttling, but the draw is more or less constant over time. @nanoant 's code is a horrible mess for PSU to handle, peaking at 230W. My VAE training code is not that terrible, but could still cause issues.\r\n\r\nGeneral \"wattage\" of PSU does not guarantee its capability to handle spiky draws like these. Drawing too much from one line may cause drops in voltage and such, and especially cheap PSUs die under these conditions. A 8-pin power connector (150W) and power from PCI-E port (75W) supply supported 225W, so the 230W peak draws more power from either connector than specified.\r\n\r\nNow that was an interesting little side-project to dig in, now to fix the situation... I guess the best bet is to reduce maximum allowed draw.\r\n\r\nEdit: Changing from batch-size 64 to 32 reduced average power draw by 20W and training has lasted over a hour now (previously crashed well before 1h).\r\nEdit2: Reducing batch size worked for me, and now run has been going for 48h without issues. \r\n"}