{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345835506", "html_url": "https://github.com/tensorflow/tensorflow/issues/8858#issuecomment-345835506", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8858", "id": 345835506, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTgzNTUwNg==", "user": {"login": "bzamecnik", "id": 446124, "node_id": "MDQ6VXNlcjQ0NjEyNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/446124?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bzamecnik", "html_url": "https://github.com/bzamecnik", "followers_url": "https://api.github.com/users/bzamecnik/followers", "following_url": "https://api.github.com/users/bzamecnik/following{/other_user}", "gists_url": "https://api.github.com/users/bzamecnik/gists{/gist_id}", "starred_url": "https://api.github.com/users/bzamecnik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bzamecnik/subscriptions", "organizations_url": "https://api.github.com/users/bzamecnik/orgs", "repos_url": "https://api.github.com/users/bzamecnik/repos", "events_url": "https://api.github.com/users/bzamecnik/events{/privacy}", "received_events_url": "https://api.github.com/users/bzamecnik/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-20T21:20:33Z", "updated_at": "2017-11-20T22:42:58Z", "author_association": "NONE", "body_html": "<p>Fortron FSP650-80EGN (650 W) with GTX 980 Ti also reboots in my case. Even limiting power usage to 150 W doesn't help. A strange thing is that one model fails at around 90 W with limit to 150 W, whereas different model runs fine at 219 W.</p>\n<pre><code># OK:\n# https://github.com/rossumai/keras-multi-gpu/blob/6ee0efe5b2e5d7ac8deec1af86ce03f27418780c/keras_tf_multigpu/examples/benchmark_inception3_resnet50.py\n$ python keras_tf_multigpu/examples/benchmark_inception3_resnet50.py --b 32 -a inception3\n# higher batch results in OOM, not reboot\n```\n\n```\n# reboot:\n# https://github.com/bzamecnik/ml/blob/42bd25ea4c4b34012f375ea03bbaa402ee064c46/chord-recognition/lstm_chord_classification_training.py\n# with batch_size=64\n$ python lstm_chord_classification_training.py\n```\n\nWhen I tried to decrease sequence length of the LSTM from 100 to 10 and increase batch size from 32 to 512 the machine didn't restart and GPU drew 200 W.\n</code></pre>", "body_text": "Fortron FSP650-80EGN (650 W) with GTX 980 Ti also reboots in my case. Even limiting power usage to 150 W doesn't help. A strange thing is that one model fails at around 90 W with limit to 150 W, whereas different model runs fine at 219 W.\n# OK:\n# https://github.com/rossumai/keras-multi-gpu/blob/6ee0efe5b2e5d7ac8deec1af86ce03f27418780c/keras_tf_multigpu/examples/benchmark_inception3_resnet50.py\n$ python keras_tf_multigpu/examples/benchmark_inception3_resnet50.py --b 32 -a inception3\n# higher batch results in OOM, not reboot\n```\n\n```\n# reboot:\n# https://github.com/bzamecnik/ml/blob/42bd25ea4c4b34012f375ea03bbaa402ee064c46/chord-recognition/lstm_chord_classification_training.py\n# with batch_size=64\n$ python lstm_chord_classification_training.py\n```\n\nWhen I tried to decrease sequence length of the LSTM from 100 to 10 and increase batch size from 32 to 512 the machine didn't restart and GPU drew 200 W.", "body": "Fortron FSP650-80EGN (650 W) with GTX 980 Ti also reboots in my case. Even limiting power usage to 150 W doesn't help. A strange thing is that one model fails at around 90 W with limit to 150 W, whereas different model runs fine at 219 W.\r\n\r\n````\r\n# OK:\r\n# https://github.com/rossumai/keras-multi-gpu/blob/6ee0efe5b2e5d7ac8deec1af86ce03f27418780c/keras_tf_multigpu/examples/benchmark_inception3_resnet50.py\r\n$ python keras_tf_multigpu/examples/benchmark_inception3_resnet50.py --b 32 -a inception3\r\n# higher batch results in OOM, not reboot\r\n```\r\n\r\n```\r\n# reboot:\r\n# https://github.com/bzamecnik/ml/blob/42bd25ea4c4b34012f375ea03bbaa402ee064c46/chord-recognition/lstm_chord_classification_training.py\r\n# with batch_size=64\r\n$ python lstm_chord_classification_training.py\r\n```\r\n\r\nWhen I tried to decrease sequence length of the LSTM from 100 to 10 and increase batch size from 32 to 512 the machine didn't restart and GPU drew 200 W."}