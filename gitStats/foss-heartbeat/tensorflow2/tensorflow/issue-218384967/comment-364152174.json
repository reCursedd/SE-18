{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364152174", "html_url": "https://github.com/tensorflow/tensorflow/issues/8858#issuecomment-364152174", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8858", "id": 364152174, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDE1MjE3NA==", "user": {"login": "nanoant", "id": 103067, "node_id": "MDQ6VXNlcjEwMzA2Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/103067?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nanoant", "html_url": "https://github.com/nanoant", "followers_url": "https://api.github.com/users/nanoant/followers", "following_url": "https://api.github.com/users/nanoant/following{/other_user}", "gists_url": "https://api.github.com/users/nanoant/gists{/gist_id}", "starred_url": "https://api.github.com/users/nanoant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nanoant/subscriptions", "organizations_url": "https://api.github.com/users/nanoant/orgs", "repos_url": "https://api.github.com/users/nanoant/repos", "events_url": "https://api.github.com/users/nanoant/events{/privacy}", "received_events_url": "https://api.github.com/users/nanoant/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-08T15:44:12Z", "updated_at": "2018-02-08T15:44:33Z", "author_association": "NONE", "body_html": "<p>Hello everyone.</p>\n<p>I want to report similar problem. Script from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2715382\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/surmenok\">@surmenok</a> does not restart my machine. What does is Keras VGGNet training running on TensorFlow backend (see below) on Arch Linux with NVIDIA 390.25 drivers.</p>\n<p>What's more interesting, it crashes only when using batch size of 64. Also, it reports few out of memory errors, continues to run, then my machine reboots after ~1.5 epochs.</p>\n<pre><code>$ python vggnet_keras.py 64\nUsing TensorFlow backend.\n2018-02-08 15:04:21.206315: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-02-08 15:04:21.345612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-02-08 15:04:21.346057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.83GiB\n2018-02-08 15:04:21.346073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\nTrain on 1224 samples, validate on 136 samples\nEpoch 1/500\n2018-02-08 15:04:28.236834: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-02-08 15:04:29.400414: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.45GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n1224/1224 [==============================] - 25s 20ms/step - loss: 3.3798 - acc: 0.1389 - val_loss: 14.8144 - val_acc: 0.0809\nEpoch 2/500\n 832/1224 [===================&gt;..........] - ETA: 5s - loss: 3.0877 - acc: 0.1719\npacket_write_wait: Connection to ... port 22: Broken pipe\n</code></pre>\n<p>Reducing batch size to 32, makes memory errors disappear, and everything works well, no restart.<br>\nAlso I tried it on Windows (10) and it works well - no restarts, noticeably slower than on Linux though.</p>\n<p>I also tried VGGNet using TFLearn but it fails to run with batch size of 64, claiming out of memory.</p>\n<p>I believe this can be a indeed PSU problem, but also very likely some corner-case of NVIDIA GPU that draws some \"out of specs\" current when running specific TensorFlow setup, causing restarts on machines using weaker but within-spec PSUs.</p>\n<p>I am using gaming PC (Lenovo Y710 Cube) that came with GTX 1070 and 450W PSU, and it was assembled by Lenovo - not me, so I believe the PSU should be good enough for this GPU. NOTE: This box is also sold with GTX 1080 and same 450W PSU. Moreover, I had no such a problem with this machine.</p>\n<p>Now I wonder if we should report this to NVIDIA?</p>\n<p>I was able to narrow down the example to following script:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> VGGNet learning with NVIDIA 1070 restarts my Linux machine</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Using Miniconda Python 3.6 + keras-gpu</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Ported from https://github.com/the-deep-learners/TensorFlow-LiveLessons/blob/master/notebooks/vggnet_in_keras.ipynb</span>\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\nnp.random.seed(<span class=\"pl-c1\">42</span>)\n\n<span class=\"pl-k\">import</span> keras\n<span class=\"pl-k\">from</span> keras.models <span class=\"pl-k\">import</span> Sequential\n<span class=\"pl-k\">from</span> keras.layers <span class=\"pl-k\">import</span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n<span class=\"pl-k\">from</span> keras.layers.normalization <span class=\"pl-k\">import</span> BatchNormalization\n<span class=\"pl-k\">from</span> keras.callbacks <span class=\"pl-k\">import</span> TensorBoard  <span class=\"pl-c\"><span class=\"pl-c\">#</span> for part 3.5 on TensorBoard</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> import tflearn.datasets.oxflower17 as oxflower17</span>\nX, Y <span class=\"pl-k\">=</span> np.random.random((<span class=\"pl-c1\">1360</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">3</span>)), np.random.random((<span class=\"pl-c1\">1360</span>, <span class=\"pl-c1\">17</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> oxflower17.load_data(one_hot=True)</span>\n\nmodel <span class=\"pl-k\">=</span> Sequential()\n\nmodel.add(Conv2D(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">3</span>)))\nmodel.add(Conv2D(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(MaxPooling2D(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Conv2D(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(MaxPooling2D(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Conv2D(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Conv2D(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(MaxPooling2D(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Conv2D(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Conv2D(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(MaxPooling2D(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Conv2D(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Conv2D(<span class=\"pl-c1\">512</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(MaxPooling2D(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\nmodel.add(Dense(<span class=\"pl-c1\">4096</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Dropout(<span class=\"pl-c1\">0.5</span>))\nmodel.add(Dense(<span class=\"pl-c1\">4096</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>))\nmodel.add(Dropout(<span class=\"pl-c1\">0.5</span>))\n\nmodel.add(Dense(<span class=\"pl-c1\">17</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>softmax<span class=\"pl-pds\">'</span></span>))\n\nmodel.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>categorical_crossentropy<span class=\"pl-pds\">'</span></span>,\n              <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>adam<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>])\n\n<span class=\"pl-k\">import</span> sys\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(sys.argv[<span class=\"pl-c1\">1</span>]) <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(sys.argv) <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">64</span>\nmodel.fit(X, Y,\n          <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n          <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">500</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n          <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">validation_split</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)</pre></div>\n<p>I will try to find out if I can produce some more minimal example with TensorFlow rather than Keras and let you know.</p>", "body_text": "Hello everyone.\nI want to report similar problem. Script from @surmenok does not restart my machine. What does is Keras VGGNet training running on TensorFlow backend (see below) on Arch Linux with NVIDIA 390.25 drivers.\nWhat's more interesting, it crashes only when using batch size of 64. Also, it reports few out of memory errors, continues to run, then my machine reboots after ~1.5 epochs.\n$ python vggnet_keras.py 64\nUsing TensorFlow backend.\n2018-02-08 15:04:21.206315: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-02-08 15:04:21.345612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-02-08 15:04:21.346057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 7.83GiB\n2018-02-08 15:04:21.346073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\nTrain on 1224 samples, validate on 136 samples\nEpoch 1/500\n2018-02-08 15:04:28.236834: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-02-08 15:04:29.400414: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.45GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n1224/1224 [==============================] - 25s 20ms/step - loss: 3.3798 - acc: 0.1389 - val_loss: 14.8144 - val_acc: 0.0809\nEpoch 2/500\n 832/1224 [===================>..........] - ETA: 5s - loss: 3.0877 - acc: 0.1719\npacket_write_wait: Connection to ... port 22: Broken pipe\n\nReducing batch size to 32, makes memory errors disappear, and everything works well, no restart.\nAlso I tried it on Windows (10) and it works well - no restarts, noticeably slower than on Linux though.\nI also tried VGGNet using TFLearn but it fails to run with batch size of 64, claiming out of memory.\nI believe this can be a indeed PSU problem, but also very likely some corner-case of NVIDIA GPU that draws some \"out of specs\" current when running specific TensorFlow setup, causing restarts on machines using weaker but within-spec PSUs.\nI am using gaming PC (Lenovo Y710 Cube) that came with GTX 1070 and 450W PSU, and it was assembled by Lenovo - not me, so I believe the PSU should be good enough for this GPU. NOTE: This box is also sold with GTX 1080 and same 450W PSU. Moreover, I had no such a problem with this machine.\nNow I wonder if we should report this to NVIDIA?\nI was able to narrow down the example to following script:\n# VGGNet learning with NVIDIA 1070 restarts my Linux machine\n# Using Miniconda Python 3.6 + keras-gpu\n# Ported from https://github.com/the-deep-learners/TensorFlow-LiveLessons/blob/master/notebooks/vggnet_in_keras.ipynb\n\nimport numpy as np\nnp.random.seed(42)\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import TensorBoard  # for part 3.5 on TensorBoard\n\n# import tflearn.datasets.oxflower17 as oxflower17\nX, Y = np.random.random((1360, 224, 224, 3)), np.random.random((1360, 17)) # oxflower17.load_data(one_hot=True)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(64, 3, activation='relu', input_shape=(224, 224, 3)))\nmodel.add(Conv2D(64, 3, activation='relu'))\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(128, 3, activation='relu'))\nmodel.add(Conv2D(128, 3, activation='relu'))\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(256, 3, activation='relu'))\nmodel.add(Conv2D(256, 3, activation='relu'))\nmodel.add(Conv2D(256, 3, activation='relu'))\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(512, 3, activation='relu'))\nmodel.add(Conv2D(512, 3, activation='relu'))\nmodel.add(Conv2D(512, 3, activation='relu'))\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(512, 3, activation='relu'))\nmodel.add(Conv2D(512, 3, activation='relu'))\nmodel.add(Conv2D(512, 3, activation='relu'))\nmodel.add(MaxPooling2D(2, 2))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(17, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n\nimport sys\nbatch_size = int(sys.argv[1]) if len(sys.argv) > 1 else 64\nmodel.fit(X, Y,\n          batch_size=batch_size,\n          epochs=500, shuffle=True,\n          verbose=1, validation_split=0.1)\nI will try to find out if I can produce some more minimal example with TensorFlow rather than Keras and let you know.", "body": "Hello everyone.\r\n\r\nI want to report similar problem. Script from @surmenok does not restart my machine. What does is Keras VGGNet training running on TensorFlow backend (see below) on Arch Linux with NVIDIA 390.25 drivers.\r\n\r\nWhat's more interesting, it crashes only when using batch size of 64. Also, it reports few out of memory errors, continues to run, then my machine reboots after ~1.5 epochs.\r\n\r\n~~~\r\n$ python vggnet_keras.py 64\r\nUsing TensorFlow backend.\r\n2018-02-08 15:04:21.206315: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-02-08 15:04:21.345612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-08 15:04:21.346057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.83GiB\r\n2018-02-08 15:04:21.346073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTrain on 1224 samples, validate on 136 samples\r\nEpoch 1/500\r\n2018-02-08 15:04:28.236834: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-02-08 15:04:29.400414: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.45GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n1224/1224 [==============================] - 25s 20ms/step - loss: 3.3798 - acc: 0.1389 - val_loss: 14.8144 - val_acc: 0.0809\r\nEpoch 2/500\r\n 832/1224 [===================>..........] - ETA: 5s - loss: 3.0877 - acc: 0.1719\r\npacket_write_wait: Connection to ... port 22: Broken pipe\r\n~~~\r\n\r\nReducing batch size to 32, makes memory errors disappear, and everything works well, no restart.\r\nAlso I tried it on Windows (10) and it works well - no restarts, noticeably slower than on Linux though.\r\n\r\nI also tried VGGNet using TFLearn but it fails to run with batch size of 64, claiming out of memory.\r\n\r\nI believe this can be a indeed PSU problem, but also very likely some corner-case of NVIDIA GPU that draws some \"out of specs\" current when running specific TensorFlow setup, causing restarts on machines using weaker but within-spec PSUs.\r\n\r\nI am using gaming PC (Lenovo Y710 Cube) that came with GTX 1070 and 450W PSU, and it was assembled by Lenovo - not me, so I believe the PSU should be good enough for this GPU. NOTE: This box is also sold with GTX 1080 and same 450W PSU. Moreover, I had no such a problem with this machine.\r\n\r\nNow I wonder if we should report this to NVIDIA?\r\n\r\nI was able to narrow down the example to following script:\r\n~~~py\r\n# VGGNet learning with NVIDIA 1070 restarts my Linux machine\r\n# Using Miniconda Python 3.6 + keras-gpu\r\n# Ported from https://github.com/the-deep-learners/TensorFlow-LiveLessons/blob/master/notebooks/vggnet_in_keras.ipynb\r\n\r\nimport numpy as np\r\nnp.random.seed(42)\r\n\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.callbacks import TensorBoard  # for part 3.5 on TensorBoard\r\n\r\n# import tflearn.datasets.oxflower17 as oxflower17\r\nX, Y = np.random.random((1360, 224, 224, 3)), np.random.random((1360, 17)) # oxflower17.load_data(one_hot=True)\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Conv2D(64, 3, activation='relu', input_shape=(224, 224, 3)))\r\nmodel.add(Conv2D(64, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(128, 3, activation='relu'))\r\nmodel.add(Conv2D(128, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(256, 3, activation='relu'))\r\nmodel.add(Conv2D(256, 3, activation='relu'))\r\nmodel.add(Conv2D(256, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(Conv2D(512, 3, activation='relu'))\r\nmodel.add(MaxPooling2D(2, 2))\r\nmodel.add(BatchNormalization())\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(4096, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(4096, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(17, activation='softmax'))\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer='adam', metrics=['accuracy'])\r\n\r\nimport sys\r\nbatch_size = int(sys.argv[1]) if len(sys.argv) > 1 else 64\r\nmodel.fit(X, Y,\r\n          batch_size=batch_size,\r\n          epochs=500, shuffle=True,\r\n          verbose=1, validation_split=0.1)\r\n~~~\r\n\r\nI will try to find out if I can produce some more minimal example with TensorFlow rather than Keras and let you know."}