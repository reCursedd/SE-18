{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/290817514", "html_url": "https://github.com/tensorflow/tensorflow/issues/8858#issuecomment-290817514", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8858", "id": 290817514, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MDgxNzUxNA==", "user": {"login": "surmenok", "id": 2715382, "node_id": "MDQ6VXNlcjI3MTUzODI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2715382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/surmenok", "html_url": "https://github.com/surmenok", "followers_url": "https://api.github.com/users/surmenok/followers", "following_url": "https://api.github.com/users/surmenok/following{/other_user}", "gists_url": "https://api.github.com/users/surmenok/gists{/gist_id}", "starred_url": "https://api.github.com/users/surmenok/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/surmenok/subscriptions", "organizations_url": "https://api.github.com/users/surmenok/orgs", "repos_url": "https://api.github.com/users/surmenok/repos", "events_url": "https://api.github.com/users/surmenok/events{/privacy}", "received_events_url": "https://api.github.com/users/surmenok/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-31T20:11:11Z", "updated_at": "2017-03-31T20:11:39Z", "author_association": "NONE", "body_html": "<p>Formatting of my code in the original post was not good. I updated it, hopefully, it is more clear now.<br>\nBasically, the function train_model creates a tf.Graph and a session, runs a simple computation: get a variable, multiply a constant tf_valid_dataset with a variable weights0, then add biases0 variable, then run tf.nn.relu , then multiply with weights1 and add biases1.<br>\nConstant tf_valid_dataset is initialized randomly.<br>\nThen this train_model function is executed sequentially 10 times. It runs well first 2-3 times and then the machine crashes.<br>\nI doubt that there are any memory related issues in this code. I think TensorFlow is designed to throw nice OOM exceptions if there is not enough memory.</p>\n<p>I tried to add a 3 second delay between function executions to be able to monitor nvidia-smi output better.<br>\nOutput of nvidia-smi when program just started:</p>\n<pre><code>Fri Mar 31 13:03:17 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\n| 26%   37C    P2    65W / 250W |   5827MiB /  6076MiB |      8%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1790    C   python                                        5825MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>Output of nvidia-smi in the middle (when running the function 2nd or 3rd time):</p>\n<pre><code>Fri Mar 31 13:03:21 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\n| 26%   39C    P2    77W / 250W |   5838MiB /  6076MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1790    C   python                                        5836MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>The last output of nvidia-smi, one second or less before machine restarts:</p>\n<pre><code>Fri Mar 31 13:03:29 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\n| 26%   40C    P2    77W / 250W |   5838MiB /  6076MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1790    C   python                                        5836MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>Doesn't seem to overheat. Memory consumtpion is tricky to measure. I think TensorFlow grabs almost all GPU memory from the beginning and manages memory itself, so for nvidia-smi it always looks like memory consumption is near maximum.</p>\n<blockquote>\n<p>Can I ask what GPU options you have set up for your session calls please?</p>\n</blockquote>\n<p>Where can I see these options?<br>\nI don't pass any special options to the Session constructor in Python code. This is the code for session initialization:</p>\n<p><code>with tf.Session(graph=graph) as session:</code></p>\n<p>Are there any other options I should look at?</p>", "body_text": "Formatting of my code in the original post was not good. I updated it, hopefully, it is more clear now.\nBasically, the function train_model creates a tf.Graph and a session, runs a simple computation: get a variable, multiply a constant tf_valid_dataset with a variable weights0, then add biases0 variable, then run tf.nn.relu , then multiply with weights1 and add biases1.\nConstant tf_valid_dataset is initialized randomly.\nThen this train_model function is executed sequentially 10 times. It runs well first 2-3 times and then the machine crashes.\nI doubt that there are any memory related issues in this code. I think TensorFlow is designed to throw nice OOM exceptions if there is not enough memory.\nI tried to add a 3 second delay between function executions to be able to monitor nvidia-smi output better.\nOutput of nvidia-smi when program just started:\nFri Mar 31 13:03:17 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\n| 26%   37C    P2    65W / 250W |   5827MiB /  6076MiB |      8%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1790    C   python                                        5825MiB |\n+-----------------------------------------------------------------------------+\n\nOutput of nvidia-smi in the middle (when running the function 2nd or 3rd time):\nFri Mar 31 13:03:21 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\n| 26%   39C    P2    77W / 250W |   5838MiB /  6076MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1790    C   python                                        5836MiB |\n+-----------------------------------------------------------------------------+\n\nThe last output of nvidia-smi, one second or less before machine restarts:\nFri Mar 31 13:03:29 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\n| 26%   40C    P2    77W / 250W |   5838MiB /  6076MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1790    C   python                                        5836MiB |\n+-----------------------------------------------------------------------------+\n\nDoesn't seem to overheat. Memory consumtpion is tricky to measure. I think TensorFlow grabs almost all GPU memory from the beginning and manages memory itself, so for nvidia-smi it always looks like memory consumption is near maximum.\n\nCan I ask what GPU options you have set up for your session calls please?\n\nWhere can I see these options?\nI don't pass any special options to the Session constructor in Python code. This is the code for session initialization:\nwith tf.Session(graph=graph) as session:\nAre there any other options I should look at?", "body": "Formatting of my code in the original post was not good. I updated it, hopefully, it is more clear now. \r\nBasically, the function train_model creates a tf.Graph and a session, runs a simple computation: get a variable, multiply a constant tf_valid_dataset with a variable weights0, then add biases0 variable, then run tf.nn.relu , then multiply with weights1 and add biases1. \r\nConstant tf_valid_dataset is initialized randomly.\r\nThen this train_model function is executed sequentially 10 times. It runs well first 2-3 times and then the machine crashes.\r\nI doubt that there are any memory related issues in this code. I think TensorFlow is designed to throw nice OOM exceptions if there is not enough memory.\r\n\r\nI tried to add a 3 second delay between function executions to be able to monitor nvidia-smi output better. \r\nOutput of nvidia-smi when program just started:\r\n\r\n```\r\nFri Mar 31 13:03:17 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\r\n| 26%   37C    P2    65W / 250W |   5827MiB /  6076MiB |      8%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1790    C   python                                        5825MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\nOutput of nvidia-smi in the middle (when running the function 2nd or 3rd time):\r\n\r\n```\r\nFri Mar 31 13:03:21 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\r\n| 26%   39C    P2    77W / 250W |   5838MiB /  6076MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1790    C   python                                        5836MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\nThe last output of nvidia-smi, one second or less before machine restarts:\r\n\r\n```\r\nFri Mar 31 13:03:29 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\r\n| 26%   40C    P2    77W / 250W |   5838MiB /  6076MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1790    C   python                                        5836MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nDoesn't seem to overheat. Memory consumtpion is tricky to measure. I think TensorFlow grabs almost all GPU memory from the beginning and manages memory itself, so for nvidia-smi it always looks like memory consumption is near maximum.\r\n\r\n> Can I ask what GPU options you have set up for your session calls please?\r\n\r\nWhere can I see these options?\r\nI don't pass any special options to the Session constructor in Python code. This is the code for session initialization:\r\n\r\n`with tf.Session(graph=graph) as session:`\r\n\r\nAre there any other options I should look at?"}