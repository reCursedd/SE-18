{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/303981118", "html_url": "https://github.com/tensorflow/tensorflow/issues/10171#issuecomment-303981118", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10171", "id": 303981118, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzk4MTExOA==", "user": {"login": "danielwatson6", "id": 3270063, "node_id": "MDQ6VXNlcjMyNzAwNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3270063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielwatson6", "html_url": "https://github.com/danielwatson6", "followers_url": "https://api.github.com/users/danielwatson6/followers", "following_url": "https://api.github.com/users/danielwatson6/following{/other_user}", "gists_url": "https://api.github.com/users/danielwatson6/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielwatson6/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielwatson6/subscriptions", "organizations_url": "https://api.github.com/users/danielwatson6/orgs", "repos_url": "https://api.github.com/users/danielwatson6/repos", "events_url": "https://api.github.com/users/danielwatson6/events{/privacy}", "received_events_url": "https://api.github.com/users/danielwatson6/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-25T10:46:32Z", "updated_at": "2017-05-25T10:46:32Z", "author_association": "NONE", "body_html": "<p>+1. There is no full usage example available that I'm aware of. The closest thing to a complete example that I have found is in the unit tests; see <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/kernel_tests/attention_wrapper_test.py\">test for attention wrapper</a> that builds the decoder.</p>\n<p>The test makes the point that <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6641793\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/leesunfreshing\">@leesunfreshing</a> made valid. There's a difference between the encoder outputs and decoder inputs, but it's unclear what the decoder inputs even are (they're random numbers in the test).</p>", "body_text": "+1. There is no full usage example available that I'm aware of. The closest thing to a complete example that I have found is in the unit tests; see test for attention wrapper that builds the decoder.\nThe test makes the point that @leesunfreshing made valid. There's a difference between the encoder outputs and decoder inputs, but it's unclear what the decoder inputs even are (they're random numbers in the test).", "body": "+1. There is no full usage example available that I'm aware of. The closest thing to a complete example that I have found is in the unit tests; see [test for attention wrapper](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/kernel_tests/attention_wrapper_test.py) that builds the decoder.\r\n\r\nThe test makes the point that @leesunfreshing made valid. There's a difference between the encoder outputs and decoder inputs, but it's unclear what the decoder inputs even are (they're random numbers in the test)."}