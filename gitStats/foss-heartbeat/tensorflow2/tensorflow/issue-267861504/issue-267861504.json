{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13933", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13933/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13933/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13933/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13933", "id": 267861504, "node_id": "MDU6SXNzdWUyNjc4NjE1MDQ=", "number": 13933, "title": "gradient registry has no entry for: FloorMod", "user": {"login": "ashwhall", "id": 14365341, "node_id": "MDQ6VXNlcjE0MzY1MzQx", "avatar_url": "https://avatars3.githubusercontent.com/u/14365341?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ashwhall", "html_url": "https://github.com/ashwhall", "followers_url": "https://api.github.com/users/ashwhall/followers", "following_url": "https://api.github.com/users/ashwhall/following{/other_user}", "gists_url": "https://api.github.com/users/ashwhall/gists{/gist_id}", "starred_url": "https://api.github.com/users/ashwhall/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ashwhall/subscriptions", "organizations_url": "https://api.github.com/users/ashwhall/orgs", "repos_url": "https://api.github.com/users/ashwhall/repos", "events_url": "https://api.github.com/users/ashwhall/events{/privacy}", "received_events_url": "https://api.github.com/users/ashwhall/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-24T00:45:16Z", "updated_at": "2017-12-01T03:05:55Z", "closed_at": "2017-12-01T03:05:55Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nA small reproducible example has been provided.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nFrom docker gpu image</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.3.0<br>\nv1.3.0-rc2-20-g0787eee 1.3.0</li>\n<li><strong>Python version</strong>:<br>\n3.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nV8.0.61</li>\n<li><strong>GPU model and memory</strong>:<br>\nGeForce GTX 1080, 8GB</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The mod operation claims to have no gradient defined. When running the below code, I receive these messages:</p>\n<pre><code>LookupError: gradient registry has no entry for: FloorMod\n</code></pre>\n<p>and</p>\n<pre><code>LookupError: No gradient defined for operation 'mod' (op type: FloorMod)\n</code></pre>\n<h3>Source code / logs</h3>\n<p>A minimal reproducible example is found here:</p>\n<pre><code>import tensorflow as tf\n\nsess = tf.InteractiveSession()\na = tf.placeholder(dtype=tf.float32, shape=[5, 2])\n\n# b = snt.Linear(output_size=4)(a)\nW = tf.Variable(tf.zeros([2, 10]))\nb = tf.Variable(tf.zeros([10]))\nb = tf.matmul(a, W) + b\n\nloss = tf.reduce_sum(b) % 2\nupdate_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n\nsess.run(tf.global_variables_initializer())\nsess.run(update_op, {a: [[1, 2], [3, 4]]})\n</code></pre>\n<p>This results in the following traceback:</p>\n<pre><code>---------------------------------------------------------------------------\nLookupError                               Traceback (most recent call last)\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\n    511             try:\n--&gt; 512               grad_fn = ops.get_gradient_function(op)\n    513             except LookupError:\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in get_gradient_function(op)\n   1835     op_type = op.type\n-&gt; 1836   return _gradient_registry.lookup(op_type)\n   1837 \n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/registry.py in lookup(self, name)\n     92       raise LookupError(\n---&gt; 93           \"%s registry has no entry for: %s\" % (self._name, name))\n\nLookupError: gradient registry has no entry for: FloorMod\n\nDuring handling of the above exception, another exception occurred:\n\nLookupError                               Traceback (most recent call last)\n&lt;ipython-input-1-7b9ad04151d6&gt; in &lt;module&gt;()\n     10 \n     11 loss = tf.reduce_sum(b) % 2\n---&gt; 12 update_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n     13 \n     14 sess.run(tf.global_variables_initializer())\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n    313         aggregation_method=aggregation_method,\n    314         colocate_gradients_with_ops=colocate_gradients_with_ops,\n--&gt; 315         grad_loss=grad_loss)\n    316 \n    317     vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\n    384         gate_gradients=(gate_gradients == Optimizer.GATE_OP),\n    385         aggregation_method=aggregation_method,\n--&gt; 386         colocate_gradients_with_ops=colocate_gradients_with_ops)\n    387     if gate_gradients == Optimizer.GATE_GRAPH:\n    388       grads = control_flow_ops.tuple(grads)\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\n    514               raise LookupError(\n    515                   \"No gradient defined for operation '%s' (op type: %s)\" %\n--&gt; 516                   (op.name, op.type))\n    517         if loop_state:\n    518           loop_state.EnterGradWhileContext(op, before=False)\n\nLookupError: No gradient defined for operation 'mod' (op type: FloorMod)\n</code></pre>\n<p>Any idea on a work-around? I need to use modulo as part of my loss function.<br>\nI use it to convert some coordinates from global space to their relative position with a specific grid-cell (ask me if you want a better explanation - I don't suppose it's particularly relevant though).</p>\n<p><strong>Thank you!</strong></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nA small reproducible example has been provided.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04\nTensorFlow installed from (source or binary):\nFrom docker gpu image\nTensorFlow version (use command below):\n1.3.0\nv1.3.0-rc2-20-g0787eee 1.3.0\nPython version:\n3.5.2\nCUDA/cuDNN version:\nV8.0.61\nGPU model and memory:\nGeForce GTX 1080, 8GB\n\nDescribe the problem\nThe mod operation claims to have no gradient defined. When running the below code, I receive these messages:\nLookupError: gradient registry has no entry for: FloorMod\n\nand\nLookupError: No gradient defined for operation 'mod' (op type: FloorMod)\n\nSource code / logs\nA minimal reproducible example is found here:\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\na = tf.placeholder(dtype=tf.float32, shape=[5, 2])\n\n# b = snt.Linear(output_size=4)(a)\nW = tf.Variable(tf.zeros([2, 10]))\nb = tf.Variable(tf.zeros([10]))\nb = tf.matmul(a, W) + b\n\nloss = tf.reduce_sum(b) % 2\nupdate_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n\nsess.run(tf.global_variables_initializer())\nsess.run(update_op, {a: [[1, 2], [3, 4]]})\n\nThis results in the following traceback:\n---------------------------------------------------------------------------\nLookupError                               Traceback (most recent call last)\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\n    511             try:\n--> 512               grad_fn = ops.get_gradient_function(op)\n    513             except LookupError:\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in get_gradient_function(op)\n   1835     op_type = op.type\n-> 1836   return _gradient_registry.lookup(op_type)\n   1837 \n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/registry.py in lookup(self, name)\n     92       raise LookupError(\n---> 93           \"%s registry has no entry for: %s\" % (self._name, name))\n\nLookupError: gradient registry has no entry for: FloorMod\n\nDuring handling of the above exception, another exception occurred:\n\nLookupError                               Traceback (most recent call last)\n<ipython-input-1-7b9ad04151d6> in <module>()\n     10 \n     11 loss = tf.reduce_sum(b) % 2\n---> 12 update_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n     13 \n     14 sess.run(tf.global_variables_initializer())\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n    313         aggregation_method=aggregation_method,\n    314         colocate_gradients_with_ops=colocate_gradients_with_ops,\n--> 315         grad_loss=grad_loss)\n    316 \n    317     vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\n    384         gate_gradients=(gate_gradients == Optimizer.GATE_OP),\n    385         aggregation_method=aggregation_method,\n--> 386         colocate_gradients_with_ops=colocate_gradients_with_ops)\n    387     if gate_gradients == Optimizer.GATE_GRAPH:\n    388       grads = control_flow_ops.tuple(grads)\n\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\n    514               raise LookupError(\n    515                   \"No gradient defined for operation '%s' (op type: %s)\" %\n--> 516                   (op.name, op.type))\n    517         if loop_state:\n    518           loop_state.EnterGradWhileContext(op, before=False)\n\nLookupError: No gradient defined for operation 'mod' (op type: FloorMod)\n\nAny idea on a work-around? I need to use modulo as part of my loss function.\nI use it to convert some coordinates from global space to their relative position with a specific grid-cell (ask me if you want a better explanation - I don't suppose it's particularly relevant though).\nThank you!", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nA small reproducible example has been provided.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nFrom docker gpu image\r\n- **TensorFlow version (use command below)**:\r\n1.3.0\r\nv1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: \r\n3.5.2\r\n- **CUDA/cuDNN version**:\r\n V8.0.61\r\n- **GPU model and memory**:\r\nGeForce GTX 1080, 8GB\r\n\r\n\r\n### Describe the problem\r\nThe mod operation claims to have no gradient defined. When running the below code, I receive these messages:\r\n```\r\nLookupError: gradient registry has no entry for: FloorMod\r\n```\r\nand\r\n```\r\nLookupError: No gradient defined for operation 'mod' (op type: FloorMod)\r\n```\r\n\r\n### Source code / logs\r\nA minimal reproducible example is found here:\r\n```\r\nimport tensorflow as tf\r\n\r\nsess = tf.InteractiveSession()\r\na = tf.placeholder(dtype=tf.float32, shape=[5, 2])\r\n\r\n# b = snt.Linear(output_size=4)(a)\r\nW = tf.Variable(tf.zeros([2, 10]))\r\nb = tf.Variable(tf.zeros([10]))\r\nb = tf.matmul(a, W) + b\r\n\r\nloss = tf.reduce_sum(b) % 2\r\nupdate_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\r\n\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(update_op, {a: [[1, 2], [3, 4]]})\r\n```\r\n\r\nThis results in the following traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nLookupError                               Traceback (most recent call last)\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\r\n    511             try:\r\n--> 512               grad_fn = ops.get_gradient_function(op)\r\n    513             except LookupError:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in get_gradient_function(op)\r\n   1835     op_type = op.type\r\n-> 1836   return _gradient_registry.lookup(op_type)\r\n   1837 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/registry.py in lookup(self, name)\r\n     92       raise LookupError(\r\n---> 93           \"%s registry has no entry for: %s\" % (self._name, name))\r\n\r\nLookupError: gradient registry has no entry for: FloorMod\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nLookupError                               Traceback (most recent call last)\r\n<ipython-input-1-7b9ad04151d6> in <module>()\r\n     10 \r\n     11 loss = tf.reduce_sum(b) % 2\r\n---> 12 update_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\r\n     13 \r\n     14 sess.run(tf.global_variables_initializer())\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    313         aggregation_method=aggregation_method,\r\n    314         colocate_gradients_with_ops=colocate_gradients_with_ops,\r\n--> 315         grad_loss=grad_loss)\r\n    316 \r\n    317     vars_with_grad = [v for g, v in grads_and_vars if g is not None]\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\r\n    384         gate_gradients=(gate_gradients == Optimizer.GATE_OP),\r\n    385         aggregation_method=aggregation_method,\r\n--> 386         colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n    387     if gate_gradients == Optimizer.GATE_GRAPH:\r\n    388       grads = control_flow_ops.tuple(grads)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\r\n    514               raise LookupError(\r\n    515                   \"No gradient defined for operation '%s' (op type: %s)\" %\r\n--> 516                   (op.name, op.type))\r\n    517         if loop_state:\r\n    518           loop_state.EnterGradWhileContext(op, before=False)\r\n\r\nLookupError: No gradient defined for operation 'mod' (op type: FloorMod)\r\n```\r\n\r\n\r\n\r\nAny idea on a work-around? I need to use modulo as part of my loss function.\r\nI use it to convert some coordinates from global space to their relative position with a specific grid-cell (ask me if you want a better explanation - I don't suppose it's particularly relevant though).\r\n\r\n\r\n__Thank you!__"}