{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17707", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17707/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17707/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17707/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17707", "id": 305102766, "node_id": "MDU6SXNzdWUzMDUxMDI3NjY=", "number": 17707, "title": "Centered padded batch on tf.data.dataset with image-features and bounding boxes", "user": {"login": "StphMe", "id": 37112607, "node_id": "MDQ6VXNlcjM3MTEyNjA3", "avatar_url": "https://avatars1.githubusercontent.com/u/37112607?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StphMe", "html_url": "https://github.com/StphMe", "followers_url": "https://api.github.com/users/StphMe/followers", "following_url": "https://api.github.com/users/StphMe/following{/other_user}", "gists_url": "https://api.github.com/users/StphMe/gists{/gist_id}", "starred_url": "https://api.github.com/users/StphMe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StphMe/subscriptions", "organizations_url": "https://api.github.com/users/StphMe/orgs", "repos_url": "https://api.github.com/users/StphMe/repos", "events_url": "https://api.github.com/users/StphMe/events{/privacy}", "received_events_url": "https://api.github.com/users/StphMe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-14T10:33:43Z", "updated_at": "2018-03-15T19:45:47Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hello everyone,</p>\n<p>I have a feature request to the <code>tf.data.dataset</code> API which might also benefit many other users.<br>\nThe use case is this:<br>\nSuppose you have a regular instance based segmentation dataset containing images features, image-targets (for segmentation), bounding boxes, and a class- and instance id to each bounding box. (<a href=\"https://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html\" rel=\"nofollow\">Mapillary</a> is a nice example for that, even though it natively does not contain bounding boxes, but they can be inferred, because it has pixel wise instances)</p>\n<p>If you use the <code>tf.data.dataset</code> API to create a dataset in one or another way (for instance use a <code>generator</code> and then create the dataset with <code>tf.data.Dataset.from_generator</code>), you can create a dataset including the feature_images, target_images and bounding boxes.</p>\n<p>Now you want to batch them together.<br>\nThe problem is, that the images may have different sizes and the number of bounding boxes per image may be different for each entry of the dataset. Therefore you can not batch them unless you make sure they have the same size. To keep the aspect ratio in the images one could consider padding the images according to the biggest size in the batch. Also the bounding boxes can be padded, according to the maximum number of bounding boxes in the batch. And there already is a function that does that: <code>tf.data.dataset.padded_batch</code>. Unfortunately the padding is always applied to the end of the dimension:<br>\nA picture with</p>\n<pre><code>[[1,1,1,1]\n [1,1,1,1]\n [1,1,1,1]\n [1,1,1,1]]\n</code></pre>\n<p>padded by 2 would look like:</p>\n<pre><code>[[1,1,1,1,0,0]\n [1,1,1,1,0,0]\n [1,1,1,1,0,0]\n [1,1,1,1,0,0]\n [0,0,0,0,0,0]\n [0,0,0,0,0,0]]\n</code></pre>\n<p>This would likely lead to dead or irrelevant neurons within the receptive field at the right or bottom of the image. Therefore a centered padding would be nice:</p>\n<pre><code>[[0,0,0,0,0,0]\n [0,1,1,1,1,0]\n [0,1,1,1,1,0]\n [0,1,1,1,1,0]\n [0,1,1,1,1,0]\n [0,0,0,0,0,0]]\n</code></pre>\n<p>Up until here there already exists an issue that covers the behaviour <a href=\"https://github.com/tensorflow/tensorflow/issues/13969#issuecomment-370902365\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/13969/hovercard\">here</a> . But now comes the tricky part: If we pad the image at the top or the left, we need an offset to the bounding boxes according to the padding because the edges of the bounding boxes are in image coordinates and shift when the image is padded.</p>\n<p>So here is my suggestion:<br>\nOne could extend the <code>tf.data.dataset.padded_patch</code> to perform centered padding on each dimension with an additional <code>centered</code> flag or so, similar as suggested in the issue above, but also returns the maximum dimension within each element of the batch. This information can then be used to adjust the bounding box location.</p>\n<p>Alternative:<br>\nOne could also consider an operator to return the maximum dimensions within the bath to use that information for padding. I could imagine, to have a <code>self written padding function</code> and use <code>tf.data.map</code> and the maximum batch dimensions information to incorporate the padding to the dataset per element using <code>tf.pad</code> or similar. The nice thing about this alternative approach is, that the users can build the padding as they want since <code>tf.pad</code> already can do central padding and other styles.</p>\n<p>I had a little dive into the functionality of <code>padded_batch</code> and it seems that the calculation of the maximum dimension is done in <code>tensorflow/core/kernels/data/padded_batch_dataset_op.cc</code> line 264 ff. This however is in the iterator class, which I presume is called when the data is actually passed through. So I'm not sure if or how to return the maximum batch dimensions from there to the point where I can use it to pad the images. If this is indeed not possible, I'd appreciate other suggestions because the way it is, the <code>tf.data.dataset</code> can not be used for this kind of data.</p>", "body_text": "Hello everyone,\nI have a feature request to the tf.data.dataset API which might also benefit many other users.\nThe use case is this:\nSuppose you have a regular instance based segmentation dataset containing images features, image-targets (for segmentation), bounding boxes, and a class- and instance id to each bounding box. (Mapillary is a nice example for that, even though it natively does not contain bounding boxes, but they can be inferred, because it has pixel wise instances)\nIf you use the tf.data.dataset API to create a dataset in one or another way (for instance use a generator and then create the dataset with tf.data.Dataset.from_generator), you can create a dataset including the feature_images, target_images and bounding boxes.\nNow you want to batch them together.\nThe problem is, that the images may have different sizes and the number of bounding boxes per image may be different for each entry of the dataset. Therefore you can not batch them unless you make sure they have the same size. To keep the aspect ratio in the images one could consider padding the images according to the biggest size in the batch. Also the bounding boxes can be padded, according to the maximum number of bounding boxes in the batch. And there already is a function that does that: tf.data.dataset.padded_batch. Unfortunately the padding is always applied to the end of the dimension:\nA picture with\n[[1,1,1,1]\n [1,1,1,1]\n [1,1,1,1]\n [1,1,1,1]]\n\npadded by 2 would look like:\n[[1,1,1,1,0,0]\n [1,1,1,1,0,0]\n [1,1,1,1,0,0]\n [1,1,1,1,0,0]\n [0,0,0,0,0,0]\n [0,0,0,0,0,0]]\n\nThis would likely lead to dead or irrelevant neurons within the receptive field at the right or bottom of the image. Therefore a centered padding would be nice:\n[[0,0,0,0,0,0]\n [0,1,1,1,1,0]\n [0,1,1,1,1,0]\n [0,1,1,1,1,0]\n [0,1,1,1,1,0]\n [0,0,0,0,0,0]]\n\nUp until here there already exists an issue that covers the behaviour here . But now comes the tricky part: If we pad the image at the top or the left, we need an offset to the bounding boxes according to the padding because the edges of the bounding boxes are in image coordinates and shift when the image is padded.\nSo here is my suggestion:\nOne could extend the tf.data.dataset.padded_patch to perform centered padding on each dimension with an additional centered flag or so, similar as suggested in the issue above, but also returns the maximum dimension within each element of the batch. This information can then be used to adjust the bounding box location.\nAlternative:\nOne could also consider an operator to return the maximum dimensions within the bath to use that information for padding. I could imagine, to have a self written padding function and use tf.data.map and the maximum batch dimensions information to incorporate the padding to the dataset per element using tf.pad or similar. The nice thing about this alternative approach is, that the users can build the padding as they want since tf.pad already can do central padding and other styles.\nI had a little dive into the functionality of padded_batch and it seems that the calculation of the maximum dimension is done in tensorflow/core/kernels/data/padded_batch_dataset_op.cc line 264 ff. This however is in the iterator class, which I presume is called when the data is actually passed through. So I'm not sure if or how to return the maximum batch dimensions from there to the point where I can use it to pad the images. If this is indeed not possible, I'd appreciate other suggestions because the way it is, the tf.data.dataset can not be used for this kind of data.", "body": "Hello everyone,\r\n\r\nI have a feature request to the `tf.data.dataset` API which might also benefit many other users.\r\nThe use case is this:\r\nSuppose you have a regular instance based segmentation dataset containing images features, image-targets (for segmentation), bounding boxes, and a class- and instance id to each bounding box. ([Mapillary](https://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html) is a nice example for that, even though it natively does not contain bounding boxes, but they can be inferred, because it has pixel wise instances)\r\n\r\nIf you use the `tf.data.dataset` API to create a dataset in one or another way (for instance use a `generator` and then create the dataset with `tf.data.Dataset.from_generator`), you can create a dataset including the feature_images, target_images and bounding boxes.\r\n\r\nNow you want to batch them together.\r\nThe problem is, that the images may have different sizes and the number of bounding boxes per image may be different for each entry of the dataset. Therefore you can not batch them unless you make sure they have the same size. To keep the aspect ratio in the images one could consider padding the images according to the biggest size in the batch. Also the bounding boxes can be padded, according to the maximum number of bounding boxes in the batch. And there already is a function that does that: `tf.data.dataset.padded_batch`. Unfortunately the padding is always applied to the end of the dimension:\r\nA picture with\r\n```\r\n[[1,1,1,1]\r\n [1,1,1,1]\r\n [1,1,1,1]\r\n [1,1,1,1]]\r\n```\r\n\r\npadded by 2 would look like:\r\n```\r\n[[1,1,1,1,0,0]\r\n [1,1,1,1,0,0]\r\n [1,1,1,1,0,0]\r\n [1,1,1,1,0,0]\r\n [0,0,0,0,0,0]\r\n [0,0,0,0,0,0]]\r\n```\r\n\r\nThis would likely lead to dead or irrelevant neurons within the receptive field at the right or bottom of the image. Therefore a centered padding would be nice:\r\n```\r\n[[0,0,0,0,0,0]\r\n [0,1,1,1,1,0]\r\n [0,1,1,1,1,0]\r\n [0,1,1,1,1,0]\r\n [0,1,1,1,1,0]\r\n [0,0,0,0,0,0]]\r\n```\r\n\r\nUp until here there already exists an issue that covers the behaviour [here](https://github.com/tensorflow/tensorflow/issues/13969#issuecomment-370902365) . But now comes the tricky part: If we pad the image at the top or the left, we need an offset to the bounding boxes according to the padding because the edges of the bounding boxes are in image coordinates and shift when the image is padded.\r\n\r\nSo here is my suggestion:\r\nOne could extend the `tf.data.dataset.padded_patch` to perform centered padding on each dimension with an additional `centered` flag or so, similar as suggested in the issue above, but also returns the maximum dimension within each element of the batch. This information can then be used to adjust the bounding box location.\r\n\r\nAlternative:\r\nOne could also consider an operator to return the maximum dimensions within the bath to use that information for padding. I could imagine, to have a `self written padding function` and use `tf.data.map` and the maximum batch dimensions information to incorporate the padding to the dataset per element using `tf.pad` or similar. The nice thing about this alternative approach is, that the users can build the padding as they want since `tf.pad` already can do central padding and other styles.\r\n\r\nI had a little dive into the functionality of `padded_batch` and it seems that the calculation of the maximum dimension is done in `tensorflow/core/kernels/data/padded_batch_dataset_op.cc` line 264 ff. This however is in the iterator class, which I presume is called when the data is actually passed through. So I'm not sure if or how to return the maximum batch dimensions from there to the point where I can use it to pad the images. If this is indeed not possible, I'd appreciate other suggestions because the way it is, the `tf.data.dataset` can not be used for this kind of data.\r\n\r\n"}