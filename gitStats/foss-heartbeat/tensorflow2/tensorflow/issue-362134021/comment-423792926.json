{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423792926", "html_url": "https://github.com/tensorflow/tensorflow/issues/22408#issuecomment-423792926", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22408", "id": 423792926, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzc5MjkyNg==", "user": {"login": "PanXiebit", "id": 24931560, "node_id": "MDQ6VXNlcjI0OTMxNTYw", "avatar_url": "https://avatars2.githubusercontent.com/u/24931560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PanXiebit", "html_url": "https://github.com/PanXiebit", "followers_url": "https://api.github.com/users/PanXiebit/followers", "following_url": "https://api.github.com/users/PanXiebit/following{/other_user}", "gists_url": "https://api.github.com/users/PanXiebit/gists{/gist_id}", "starred_url": "https://api.github.com/users/PanXiebit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PanXiebit/subscriptions", "organizations_url": "https://api.github.com/users/PanXiebit/orgs", "repos_url": "https://api.github.com/users/PanXiebit/repos", "events_url": "https://api.github.com/users/PanXiebit/events{/privacy}", "received_events_url": "https://api.github.com/users/PanXiebit/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-23T05:25:45Z", "updated_at": "2018-09-23T05:25:45Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42785337\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wt-huang\">@wt-huang</a>  thank you, yes, this is not a bug. I had trained the model during the training. But when I am going to test, and restore the model by  reload the <code>rcmodel</code>, and  then<br>\n<code>rcmodel.saver.restore(sess, save_path=tf.train.latest_checkpoint(\"./model/train_v2/\"))</code><br>\nbut I cannot feed the test dataset to the model, because the test_dataset.types is difficult to the train_dataset.shape.</p>\n<pre><code>data_iter = tf.data.Iterator.from_string_handle(string_handle=data_handle,\n                                                    output_types=train_dataset.output_types,\n                                                    output_shapes=train_dataset.output_shapes)\n</code></pre>\n<p>So  I want to feed the this input one by one like this:</p>\n<pre><code>self.passage = tf.placeholder(tf.int32, [None, None], name=\"passage\")\nself.query = tf.placeholder(tf.int32, [None, None], name=\"query\")\nself.answer = tf.placeholder(tf.int32, [None, None], name=\"answer\")\nself.passage_len = tf.placeholder(tf.int32, [None], name=\"passage_len\")\nself.query_len = tf.placeholder(tf.int32, [None], name=\"query_len\")\nself.query_id = tf.placeholder(tf.int32, [None], name=\"query_id\")\nself.answer_len = tf.placeholder(tf.int32, [None], name=\"answer_len\")\nself.alter0 = tf.placeholder(tf.int32, [None, None], name=\"alter0\")\nself.alter1 = tf.placeholder(tf.int32, [None, None], name=\"alter1\")\nself.alter2 = tf.placeholder(tf.int32, [None, None], name=\"alter2\")\n</code></pre>\n<p>then I find the issue above,  <code>passage:data_iter[\"passage\"]</code> consumed a batch, and <code>query:data_iter[\"passage_len\"]</code> consumed an other batch. So the data pasage and passage_len is not match.</p>", "body_text": "@wt-huang  thank you, yes, this is not a bug. I had trained the model during the training. But when I am going to test, and restore the model by  reload the rcmodel, and  then\nrcmodel.saver.restore(sess, save_path=tf.train.latest_checkpoint(\"./model/train_v2/\"))\nbut I cannot feed the test dataset to the model, because the test_dataset.types is difficult to the train_dataset.shape.\ndata_iter = tf.data.Iterator.from_string_handle(string_handle=data_handle,\n                                                    output_types=train_dataset.output_types,\n                                                    output_shapes=train_dataset.output_shapes)\n\nSo  I want to feed the this input one by one like this:\nself.passage = tf.placeholder(tf.int32, [None, None], name=\"passage\")\nself.query = tf.placeholder(tf.int32, [None, None], name=\"query\")\nself.answer = tf.placeholder(tf.int32, [None, None], name=\"answer\")\nself.passage_len = tf.placeholder(tf.int32, [None], name=\"passage_len\")\nself.query_len = tf.placeholder(tf.int32, [None], name=\"query_len\")\nself.query_id = tf.placeholder(tf.int32, [None], name=\"query_id\")\nself.answer_len = tf.placeholder(tf.int32, [None], name=\"answer_len\")\nself.alter0 = tf.placeholder(tf.int32, [None, None], name=\"alter0\")\nself.alter1 = tf.placeholder(tf.int32, [None, None], name=\"alter1\")\nself.alter2 = tf.placeholder(tf.int32, [None, None], name=\"alter2\")\n\nthen I find the issue above,  passage:data_iter[\"passage\"] consumed a batch, and query:data_iter[\"passage_len\"] consumed an other batch. So the data pasage and passage_len is not match.", "body": "@wt-huang  thank you, yes, this is not a bug. I had trained the model during the training. But when I am going to test, and restore the model by  reload the `rcmodel`, and  then\r\n```rcmodel.saver.restore(sess, save_path=tf.train.latest_checkpoint(\"./model/train_v2/\"))```\r\nbut I cannot feed the test dataset to the model, because the test_dataset.types is difficult to the train_dataset.shape. \r\n\r\n```\r\ndata_iter = tf.data.Iterator.from_string_handle(string_handle=data_handle,\r\n                                                    output_types=train_dataset.output_types,\r\n                                                    output_shapes=train_dataset.output_shapes)\r\n``` \r\nSo  I want to feed the this input one by one like this:\r\n```\r\nself.passage = tf.placeholder(tf.int32, [None, None], name=\"passage\")\r\nself.query = tf.placeholder(tf.int32, [None, None], name=\"query\")\r\nself.answer = tf.placeholder(tf.int32, [None, None], name=\"answer\")\r\nself.passage_len = tf.placeholder(tf.int32, [None], name=\"passage_len\")\r\nself.query_len = tf.placeholder(tf.int32, [None], name=\"query_len\")\r\nself.query_id = tf.placeholder(tf.int32, [None], name=\"query_id\")\r\nself.answer_len = tf.placeholder(tf.int32, [None], name=\"answer_len\")\r\nself.alter0 = tf.placeholder(tf.int32, [None, None], name=\"alter0\")\r\nself.alter1 = tf.placeholder(tf.int32, [None, None], name=\"alter1\")\r\nself.alter2 = tf.placeholder(tf.int32, [None, None], name=\"alter2\")\r\n```\r\nthen I find the issue above,  ```passage:data_iter[\"passage\"]``` consumed a batch, and ```query:data_iter[\"passage_len\"]``` consumed an other batch. So the data pasage and passage_len is not match."}