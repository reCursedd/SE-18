{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22223", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22223/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22223/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22223/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22223", "id": 359328193, "node_id": "MDU6SXNzdWUzNTkzMjgxOTM=", "number": 22223, "title": "tf.unstack did not work with tf 1.8 CudnnGRU tensors", "user": {"login": "zheolong", "id": 5104916, "node_id": "MDQ6VXNlcjUxMDQ5MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/5104916?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheolong", "html_url": "https://github.com/zheolong", "followers_url": "https://api.github.com/users/zheolong/followers", "following_url": "https://api.github.com/users/zheolong/following{/other_user}", "gists_url": "https://api.github.com/users/zheolong/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheolong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheolong/subscriptions", "organizations_url": "https://api.github.com/users/zheolong/orgs", "repos_url": "https://api.github.com/users/zheolong/repos", "events_url": "https://api.github.com/users/zheolong/events{/privacy}", "received_events_url": "https://api.github.com/users/zheolong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "bignamehyp", "id": 3474655, "node_id": "MDQ6VXNlcjM0NzQ2NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3474655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bignamehyp", "html_url": "https://github.com/bignamehyp", "followers_url": "https://api.github.com/users/bignamehyp/followers", "following_url": "https://api.github.com/users/bignamehyp/following{/other_user}", "gists_url": "https://api.github.com/users/bignamehyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/bignamehyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bignamehyp/subscriptions", "organizations_url": "https://api.github.com/users/bignamehyp/orgs", "repos_url": "https://api.github.com/users/bignamehyp/repos", "events_url": "https://api.github.com/users/bignamehyp/events{/privacy}", "received_events_url": "https://api.github.com/users/bignamehyp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "bignamehyp", "id": 3474655, "node_id": "MDQ6VXNlcjM0NzQ2NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3474655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bignamehyp", "html_url": "https://github.com/bignamehyp", "followers_url": "https://api.github.com/users/bignamehyp/followers", "following_url": "https://api.github.com/users/bignamehyp/following{/other_user}", "gists_url": "https://api.github.com/users/bignamehyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/bignamehyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bignamehyp/subscriptions", "organizations_url": "https://api.github.com/users/bignamehyp/orgs", "repos_url": "https://api.github.com/users/bignamehyp/repos", "events_url": "https://api.github.com/users/bignamehyp/events{/privacy}", "received_events_url": "https://api.github.com/users/bignamehyp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-09-12T06:06:06Z", "updated_at": "2018-09-18T03:54:35Z", "closed_at": "2018-09-12T19:16:20Z", "author_association": "NONE", "body_html": "<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</li>\n</ul>\n<pre><code>$uname -r\n3.10.0-327.el7.x86_64\n</code></pre>\n<ul>\n<li><strong>Mobile device</strong></li>\n</ul>\n<pre><code>Not mobile\n</code></pre>\n<ul>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nanaconda tf 1.8</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:</p>\n</li>\n</ul>\n<pre><code>$conda list|grep tensor\ntensorboard               1.8.0            py36hf484d3e_0\ntensorflow                1.8.0                hb381393_0\ntensorflow-base           1.8.0            py36h4df133c_0\ntensorflow-gpu            1.8.0                h7b35bdc_0\n</code></pre>\n<ul>\n<li><strong>Python version</strong>:</li>\n</ul>\n<pre><code>$python3.6 -V\nPython 3.6.2 :: Continuum Analytics, Inc.\n</code></pre>\n<ul>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n</ul>\n<pre><code>$bazel version\nBuild label: 0.4.5\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\nBuild timestamp: 1489666778\nBuild timestamp as int: 1489666778\n</code></pre>\n<ul>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n</ul>\n<pre><code>$conda list|grep -i cuda\ncudatoolkit               8.0                           3    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\ncudnn                     7.0.5                 cuda8.0_0\n</code></pre>\n<ul>\n<li><strong>GPU model and memory</strong>:</li>\n</ul>\n<pre><code>\n== cat /etc/issue ===============================================\nLinux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"7.2 (Paladin)\"\nVERSION_ID=\"7.2\"\nQihoo360_BUGZILLA_PRODUCT_VERSION=7.2\nQihoo360_SUPPORT_PRODUCT_VERSION=7.2\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (GCC) 4.9.2\nCopyright (C) 2014 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.13.3)\nprotobuf (3.5.1)\ntensorflow (1.8.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.8.0\ntf.GIT_VERSION = b'unknown'\ntf.COMPILER_VERSION = b'unknown'\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH :/usr/local/mpc-0.8.1/lib:/usr/local/gmp-4.3.2/lib:/usr/local/mpfr-2.4.2/lib:/gruntdata/qihoo360/cuda/lib64\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nWed Sep 12 13:34:30 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40m          On   | 0000:02:00.0     Off |                    0 |\n| N/A   36C    P0    67W / 235W |   1161MiB / 11439MiB |     39%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K40m          On   | 0000:03:00.0     Off |                    0 |\n| N/A   35C    P0    60W / 235W |     73MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     13950    C   bin/arks                                       868MiB |\n|    0     27880    C   python3.6                                      288MiB |\n|    1     27880    C   python3.6                                       71MiB |\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\n/usr/local/cuda-8.0/lib64/libcudart_static.a\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n/usr/local/cuda-7.5/lib64/libcudart_static.a\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\n/usr/local/cuda-7.5/lib/libcudart_static.a\n</code></pre>\n<h3>Describe the problem</h3>\n<p><code>tf.unstack</code> did not work as expected. It did not reduce <code>R</code> rank tensor to <code>R-1</code> rank tensor</p>\n<h3>Source code / logs</h3>\n<p>code:</p>\n<pre><code>#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nrnn_model = tf.contrib.cudnn_rnn.CudnnGRU(\n        num_layers=1,\n        num_units=64,\n        direction='unidirectional')\nrnn_model.build([3, 1, 3])\ninputs=[[[1,1,1],[1,1,1],[1,1,1]]]\ninputs_tensor= tf.convert_to_tensor(inputs, dtype=tf.float32)\nprint(tf.shape(inputs_tensor))\nrnn_out, rnn_state = rnn_model(inputs_tensor)\nprint(\"rnn_state: \", rnn_state)\nrnn_layers = tf.unstack(rnn_state)\nprint(\"rnn_layers\", rnn_layers)\n</code></pre>\n<p>paste the code to file <code>demo.py</code>, then run from linux command line:</p>\n<pre><code>$ python3.6 demo.py\n</code></pre>\n<p>output:</p>\n<pre><code>Tensor(\"Shape:0\", shape=(3,), dtype=int32)\nrnn_state:  (&lt;tf.Tensor 'cudnn_gru/CudnnRNN:1' shape=(1, ?, 64) dtype=float32&gt;,)\nrnn_layers [&lt;tf.Tensor 'unstack:0' shape=(1, ?, 64) dtype=float32&gt;]\n</code></pre>\n<p>the <code>rnn_layers</code> should be <code> rnn_layers [&lt;tf.Tensor 'unstack:0' shape=(?, 64) dtype=float32&gt;]</code></p>", "body_text": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\n$uname -r\n3.10.0-327.el7.x86_64\n\n\nMobile device\n\nNot mobile\n\n\n\nTensorFlow installed from (source or binary):\nanaconda tf 1.8\n\n\nTensorFlow version (use command below):\n\n\n$conda list|grep tensor\ntensorboard               1.8.0            py36hf484d3e_0\ntensorflow                1.8.0                hb381393_0\ntensorflow-base           1.8.0            py36h4df133c_0\ntensorflow-gpu            1.8.0                h7b35bdc_0\n\n\nPython version:\n\n$python3.6 -V\nPython 3.6.2 :: Continuum Analytics, Inc.\n\n\nBazel version (if compiling from source):\n\n$bazel version\nBuild label: 0.4.5\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\nBuild timestamp: 1489666778\nBuild timestamp as int: 1489666778\n\n\nCUDA/cuDNN version:\n\n$conda list|grep -i cuda\ncudatoolkit               8.0                           3    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\ncudnn                     7.0.5                 cuda8.0_0\n\n\nGPU model and memory:\n\n\n== cat /etc/issue ===============================================\nLinux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"7.2 (Paladin)\"\nVERSION_ID=\"7.2\"\nQihoo360_BUGZILLA_PRODUCT_VERSION=7.2\nQihoo360_SUPPORT_PRODUCT_VERSION=7.2\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (GCC) 4.9.2\nCopyright (C) 2014 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.13.3)\nprotobuf (3.5.1)\ntensorflow (1.8.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.8.0\ntf.GIT_VERSION = b'unknown'\ntf.COMPILER_VERSION = b'unknown'\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH :/usr/local/mpc-0.8.1/lib:/usr/local/gmp-4.3.2/lib:/usr/local/mpfr-2.4.2/lib:/gruntdata/qihoo360/cuda/lib64\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nWed Sep 12 13:34:30 2018\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40m          On   | 0000:02:00.0     Off |                    0 |\n| N/A   36C    P0    67W / 235W |   1161MiB / 11439MiB |     39%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K40m          On   | 0000:03:00.0     Off |                    0 |\n| N/A   35C    P0    60W / 235W |     73MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     13950    C   bin/arks                                       868MiB |\n|    0     27880    C   python3.6                                      288MiB |\n|    1     27880    C   python3.6                                       71MiB |\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\n/usr/local/cuda-8.0/lib64/libcudart_static.a\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n/usr/local/cuda-7.5/lib64/libcudart_static.a\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\n/usr/local/cuda-7.5/lib/libcudart_static.a\n\nDescribe the problem\ntf.unstack did not work as expected. It did not reduce R rank tensor to R-1 rank tensor\nSource code / logs\ncode:\n#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport tensorflow as tf\nrnn_model = tf.contrib.cudnn_rnn.CudnnGRU(\n        num_layers=1,\n        num_units=64,\n        direction='unidirectional')\nrnn_model.build([3, 1, 3])\ninputs=[[[1,1,1],[1,1,1],[1,1,1]]]\ninputs_tensor= tf.convert_to_tensor(inputs, dtype=tf.float32)\nprint(tf.shape(inputs_tensor))\nrnn_out, rnn_state = rnn_model(inputs_tensor)\nprint(\"rnn_state: \", rnn_state)\nrnn_layers = tf.unstack(rnn_state)\nprint(\"rnn_layers\", rnn_layers)\n\npaste the code to file demo.py, then run from linux command line:\n$ python3.6 demo.py\n\noutput:\nTensor(\"Shape:0\", shape=(3,), dtype=int32)\nrnn_state:  (<tf.Tensor 'cudnn_gru/CudnnRNN:1' shape=(1, ?, 64) dtype=float32>,)\nrnn_layers [<tf.Tensor 'unstack:0' shape=(1, ?, 64) dtype=float32>]\n\nthe rnn_layers should be  rnn_layers [<tf.Tensor 'unstack:0' shape=(?, 64) dtype=float32>]", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n```\r\n$uname -r\r\n3.10.0-327.el7.x86_64\r\n```\r\n- **Mobile device**\r\n```\r\nNot mobile\r\n```\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nanaconda tf 1.8\r\n\r\n- **TensorFlow version (use command below)**:\r\n```\r\n$conda list|grep tensor\r\ntensorboard               1.8.0            py36hf484d3e_0\r\ntensorflow                1.8.0                hb381393_0\r\ntensorflow-base           1.8.0            py36h4df133c_0\r\ntensorflow-gpu            1.8.0                h7b35bdc_0\r\n```\r\n- **Python version**:\r\n```\r\n$python3.6 -V\r\nPython 3.6.2 :: Continuum Analytics, Inc.\r\n```\r\n- **Bazel version (if compiling from source)**:\r\n```\r\n$bazel version\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n```\r\n\r\n- **CUDA/cuDNN version**:\r\n```\r\n$conda list|grep -i cuda\r\ncudatoolkit               8.0                           3    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free\r\ncudnn                     7.0.5                 cuda8.0_0\r\n```\r\n- **GPU model and memory**:\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7.2 (Paladin)\"\r\nVERSION_ID=\"7.2\"\r\nQihoo360_BUGZILLA_PRODUCT_VERSION=7.2\r\nQihoo360_SUPPORT_PRODUCT_VERSION=7.2\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.9.2\r\nCopyright (C) 2014 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.5.1)\r\ntensorflow (1.8.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH :/usr/local/mpc-0.8.1/lib:/usr/local/gmp-4.3.2/lib:/usr/local/mpfr-2.4.2/lib:/gruntdata/qihoo360/cuda/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Sep 12 13:34:30 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K40m          On   | 0000:02:00.0     Off |                    0 |\r\n| N/A   36C    P0    67W / 235W |   1161MiB / 11439MiB |     39%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K40m          On   | 0000:03:00.0     Off |                    0 |\r\n| N/A   35C    P0    60W / 235W |     73MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0     13950    C   bin/arks                                       868MiB |\r\n|    0     27880    C   python3.6                                      288MiB |\r\n|    1     27880    C   python3.6                                       71MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n```\r\n\r\n### Describe the problem\r\n`tf.unstack` did not work as expected. It did not reduce `R` rank tensor to `R-1` rank tensor\r\n\r\n### Source code / logs\r\ncode:\r\n```\r\n#! /usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport sys\r\nimport tensorflow as tf\r\nrnn_model = tf.contrib.cudnn_rnn.CudnnGRU(\r\n        num_layers=1,\r\n        num_units=64,\r\n        direction='unidirectional')\r\nrnn_model.build([3, 1, 3])\r\ninputs=[[[1,1,1],[1,1,1],[1,1,1]]]\r\ninputs_tensor= tf.convert_to_tensor(inputs, dtype=tf.float32)\r\nprint(tf.shape(inputs_tensor))\r\nrnn_out, rnn_state = rnn_model(inputs_tensor)\r\nprint(\"rnn_state: \", rnn_state)\r\nrnn_layers = tf.unstack(rnn_state)\r\nprint(\"rnn_layers\", rnn_layers)\r\n```\r\n\r\npaste the code to file `demo.py`, then run from linux command line:\r\n```\r\n$ python3.6 demo.py\r\n```\r\n\r\noutput:\r\n```\r\nTensor(\"Shape:0\", shape=(3,), dtype=int32)\r\nrnn_state:  (<tf.Tensor 'cudnn_gru/CudnnRNN:1' shape=(1, ?, 64) dtype=float32>,)\r\nrnn_layers [<tf.Tensor 'unstack:0' shape=(1, ?, 64) dtype=float32>]\r\n```\r\n\r\nthe `rnn_layers` should be ` rnn_layers [<tf.Tensor 'unstack:0' shape=(?, 64) dtype=float32>]`\r\n"}