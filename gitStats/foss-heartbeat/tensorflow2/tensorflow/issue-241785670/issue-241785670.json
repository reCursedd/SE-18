{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11413", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11413/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11413/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11413/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11413", "id": 241785670, "node_id": "MDU6SXNzdWUyNDE3ODU2NzA=", "number": 11413, "title": "Request genuine consecutive scheme batch generation for RNN Trainning", "user": {"login": "YMMS", "id": 2020957, "node_id": "MDQ6VXNlcjIwMjA5NTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2020957?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YMMS", "html_url": "https://github.com/YMMS", "followers_url": "https://api.github.com/users/YMMS/followers", "following_url": "https://api.github.com/users/YMMS/following{/other_user}", "gists_url": "https://api.github.com/users/YMMS/gists{/gist_id}", "starred_url": "https://api.github.com/users/YMMS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YMMS/subscriptions", "organizations_url": "https://api.github.com/users/YMMS/orgs", "repos_url": "https://api.github.com/users/YMMS/repos", "events_url": "https://api.github.com/users/YMMS/events{/privacy}", "received_events_url": "https://api.github.com/users/YMMS/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-07-10T17:20:17Z", "updated_at": "2017-07-26T06:06:25Z", "closed_at": "2017-07-12T23:52:39Z", "author_association": "NONE", "body_html": "<p>The concept of \u201cgenuine consecutive scheme \u201d can be seen at <a href=\"http://www.sciencedirect.com/science/article/pii/S088523081400093X?via%3Dihub\" rel=\"nofollow\">here(5.4. Batch)</a>.</p>\n<p>My scenario is as follows:</p>\n<p>I have some files with different sequence lengths.</p>\n<p>First, do buckecting to generate file-batches with parameter <code>batch_size</code></p>\n<p>Then, split each  file-batch with parameter <code>seq_len</code> to generate trainning sample-batches</p>\n<p>Last, use each sample-batch for one step of trainning.</p>\n<p>Following is my test code:</p>\n<pre><code># -*- coding:utf8 -*-\n\nimport os\nimport time\nimport random\nimport tensorflow as tf\nfrom tensorflow.contrib.training import bucket_by_sequence_length, batch_sequences_with_states\n\n\ncontext_features = {\n    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n}\n\nsequence_features = {\n            \"inputs\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n}\n\ndef GenerateFakeData():\n    FILE_NUM = 100\n    DATA_PATH = \"test_dataset\"\n    file_path_list, file_len_list = [], []\n    for idx in range(FILE_NUM):\n        filename = \"{fileno}-of-{idx}\".format(idx=idx+1, fileno=FILE_NUM)\n        token_length = random.randint(50, 100)\n        ex = tf.train.SequenceExample()\n        ex.context.feature[\"length\"].int64_list.value.append(token_length)\n        ###########################################\n        ex_tokens = ex.feature_lists.feature_list[\"inputs\"]\n        for tok in range(token_length):\n            ex_tokens.feature.add().int64_list.value.append(tok)\n        with tf.python_io.TFRecordWriter(os.path.join(DATA_PATH, filename) + \".tfrecord\") as filew:\n            filew.write(ex.SerializeToString())\n        file_len_list.append(token_length)\n        file_path_list.append(os.path.join(DATA_PATH, filename) + \".tfrecord\")\n    with open(\"filelist.txt\", \"w\") as filew:\n        for file_name, file_len in zip(file_path_list, file_len_list):\n            filew.write(\"{fn}\\t{fl}\\n\".format(fn=os.path.join(file_name), fl=file_len))\n\ndef LoadFileList(filepath):\n    with open(filepath, \"r\") as filer:\n        wfilelist, wfilelengthlist = tuple(zip(*[tuple(line.strip().split(\"\\t\")) for line in filer if line.strip() != \"\"]))\n        return list(wfilelist), [int(item) for item in wfilelengthlist]\n\n        \ndef InputProducer():\n    batch_size = 2\n    seq_len = 75\n    state_size = 1024\n    bucket_boundaries = [60, 70, 80, 90]\n    #####################################\n    filelist, filelengthlist = LoadFileList(\"filelist.txt\")\n    #####################################\n    tf_file_queue = tf.train.string_input_producer(\n            string_tensor = filelist, \n            num_epochs = 1, \n            shuffle = False, \n            seed = None, \n            capacity = 32, \n            shared_name = None,\n            name = \"tf_file_queue\",\n            cancel_op=None\n    )\n    ######################################\n    tf_reader = tf.TFRecordReader()\n    tf_key, tf_serialized = tf_reader.read(tf_file_queue)\n    tf_context, tf_sequence = tf.parse_single_sequence_example(\n            serialized = tf_serialized,\n            context_features = context_features,\n            sequence_features = sequence_features\n    )\n    ######################################\n    tf_bucket_sequence_length, tf_bucket_outputs = bucket_by_sequence_length(\n        input_length = tf.cast(tf_context[\"length\"], dtype=tf.int32), \n        tensors = tf_sequence, \n        batch_size = batch_size, \n        bucket_boundaries = bucket_boundaries, \n        num_threads=1, \n        capacity=32, \n        shapes=None, \n        dynamic_pad=True,\n        allow_smaller_final_batch=False, \n        keep_input=True, \n        shared_name=None, \n        name=\"bucket_files\"\n    )\n    #######################################\n    tf_bbucket_outputs = {}\n    for fkey in tf_bucket_outputs:\n        tf_bbucket_outputs[fkey]=tf_bucket_outputs[fkey][0]\n    #######################################\n    # Solution 1:\n    tf_fb_key=time.strftime('%Y-%m-%d-%H-%M-%S',time.localtime(time.time())) + str(random.randint(1,100000000))\n    initial_state_values = tf.zeros((state_size,), dtype=tf.float32)\n    initial_states = {\"lstm_state\": initial_state_values}\n    tf_batch=batch_sequences_with_states(\n        input_key = tf_fb_key, \n        input_sequences = tf_bbucket_outputs, \n        input_context = {}, \n        input_length = tf.reduce_max(tf_bucket_sequence_length), \n        initial_states=initial_states, \n        num_unroll=seq_len, \n        batch_size=batch_size, \n        num_threads=3, \n        capacity=1000, \n        allow_small_batch=False, \n        pad=True, \n        name=None)\n    #######################################\n    # Solution 2:\n    '''\n    tf_index_queue=tf.train.range_input_producer(\n        limit=tf.reduce_max(tf_bucket_sequence_length),\n        num_epochs=1, \n        shuffle=False, \n        seed=None, \n        capacity=32, \n        shared_name=None, \n        name=None\n    )\n    tf_index=tf_index_queue.dequeue()\n    tf_batch=tf_bbucket_outputs[\"inputs\"][tf_index]\n    '''\n    #######################################\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n        try:\n            while True:\n                #####################################\n                # Test Bucketing\n                #bucket_sequence_length, bucket_outputs = sess.run([tf_bucket_sequence_length, tf_bucket_outputs])\n                #print(bucket_sequence_length)\n                #print(bucket_outputs)\n                #print(\"#################\")\n                #####################################\n                # Test Solution 1:\n                batch = sess.run(tf_batch)\n                print(batch)\n                #####################################\n                # Test Solution 2:\n                #bucket_sequence_length, bucket_outputs, index = sess.run([tf_bucket_sequence_length, tf_bucket_outputs, tf_index])\n                #print(bucket_sequence_length)\n                #print(bucket_outputs)\n                #print(index)\n                #print(\"#################\")\n        except tf.errors.OutOfRangeError:\n            pass\n        except tf.errors.InvalidArgumentError:\n            pass\n        finally:\n            coord.request_stop()\n        coord.join(threads)\n    \nif __name__ == \"__main__\":\n    #GenerateFakeData()\n    InputProducer()\n    pass\n</code></pre>\n<p>With Solution 1, I raised the error as below:</p>\n<pre><code>Traceback (most recent call last):\n  \n    File \"/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py\", line 157, in &lt;module&gt;\n    \nInputProducer()\n  \n    File \"/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py\", line 107, in InputProducer\n    \nname=None)\n  \n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 1522, in batch_sequences_with_states\n    \nallow_small_batch=allow_small_batch)\n  \n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 849, in __init__\n    \ninitial_states)\n  \n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 332, in _prepare_sequence_inputs\n    \n\"sequence\", inputs.sequences, ignore_first_dimension=True)\n  \n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 326, in _assert_fully_defined\n    \nignore_first_dimension else \"\", v.get_shape()))\nValueError: Shape for sequence inputs is not fully defined (ignoring first dimension): (?, ?)\n</code></pre>\n<p>See the document of <code>batch_sequences_with_states</code>, I found that</p>\n<ol>\n<li>it seems only support only one sequence and don't support multiple sequences .</li>\n<li>it don't support the situation of <code>Shape for sequence inputs is not fully defined</code>, which means <code>bucket_by_sequence_length</code> can not be followed with <code>batch_sequences_with_states</code>.</li>\n</ol>\n<p>What more, I have tried Solution 2, but I failed because of thread synchronization problem between <code>tf.train.string_input_producer</code> and <code>tf.train.range_input_producer</code>.</p>\n<p>So, how to relize my request ?</p>\n<p>Hope for your help.</p>", "body_text": "The concept of \u201cgenuine consecutive scheme \u201d can be seen at here(5.4. Batch).\nMy scenario is as follows:\nI have some files with different sequence lengths.\nFirst, do buckecting to generate file-batches with parameter batch_size\nThen, split each  file-batch with parameter seq_len to generate trainning sample-batches\nLast, use each sample-batch for one step of trainning.\nFollowing is my test code:\n# -*- coding:utf8 -*-\n\nimport os\nimport time\nimport random\nimport tensorflow as tf\nfrom tensorflow.contrib.training import bucket_by_sequence_length, batch_sequences_with_states\n\n\ncontext_features = {\n    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n}\n\nsequence_features = {\n            \"inputs\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n}\n\ndef GenerateFakeData():\n    FILE_NUM = 100\n    DATA_PATH = \"test_dataset\"\n    file_path_list, file_len_list = [], []\n    for idx in range(FILE_NUM):\n        filename = \"{fileno}-of-{idx}\".format(idx=idx+1, fileno=FILE_NUM)\n        token_length = random.randint(50, 100)\n        ex = tf.train.SequenceExample()\n        ex.context.feature[\"length\"].int64_list.value.append(token_length)\n        ###########################################\n        ex_tokens = ex.feature_lists.feature_list[\"inputs\"]\n        for tok in range(token_length):\n            ex_tokens.feature.add().int64_list.value.append(tok)\n        with tf.python_io.TFRecordWriter(os.path.join(DATA_PATH, filename) + \".tfrecord\") as filew:\n            filew.write(ex.SerializeToString())\n        file_len_list.append(token_length)\n        file_path_list.append(os.path.join(DATA_PATH, filename) + \".tfrecord\")\n    with open(\"filelist.txt\", \"w\") as filew:\n        for file_name, file_len in zip(file_path_list, file_len_list):\n            filew.write(\"{fn}\\t{fl}\\n\".format(fn=os.path.join(file_name), fl=file_len))\n\ndef LoadFileList(filepath):\n    with open(filepath, \"r\") as filer:\n        wfilelist, wfilelengthlist = tuple(zip(*[tuple(line.strip().split(\"\\t\")) for line in filer if line.strip() != \"\"]))\n        return list(wfilelist), [int(item) for item in wfilelengthlist]\n\n        \ndef InputProducer():\n    batch_size = 2\n    seq_len = 75\n    state_size = 1024\n    bucket_boundaries = [60, 70, 80, 90]\n    #####################################\n    filelist, filelengthlist = LoadFileList(\"filelist.txt\")\n    #####################################\n    tf_file_queue = tf.train.string_input_producer(\n            string_tensor = filelist, \n            num_epochs = 1, \n            shuffle = False, \n            seed = None, \n            capacity = 32, \n            shared_name = None,\n            name = \"tf_file_queue\",\n            cancel_op=None\n    )\n    ######################################\n    tf_reader = tf.TFRecordReader()\n    tf_key, tf_serialized = tf_reader.read(tf_file_queue)\n    tf_context, tf_sequence = tf.parse_single_sequence_example(\n            serialized = tf_serialized,\n            context_features = context_features,\n            sequence_features = sequence_features\n    )\n    ######################################\n    tf_bucket_sequence_length, tf_bucket_outputs = bucket_by_sequence_length(\n        input_length = tf.cast(tf_context[\"length\"], dtype=tf.int32), \n        tensors = tf_sequence, \n        batch_size = batch_size, \n        bucket_boundaries = bucket_boundaries, \n        num_threads=1, \n        capacity=32, \n        shapes=None, \n        dynamic_pad=True,\n        allow_smaller_final_batch=False, \n        keep_input=True, \n        shared_name=None, \n        name=\"bucket_files\"\n    )\n    #######################################\n    tf_bbucket_outputs = {}\n    for fkey in tf_bucket_outputs:\n        tf_bbucket_outputs[fkey]=tf_bucket_outputs[fkey][0]\n    #######################################\n    # Solution 1:\n    tf_fb_key=time.strftime('%Y-%m-%d-%H-%M-%S',time.localtime(time.time())) + str(random.randint(1,100000000))\n    initial_state_values = tf.zeros((state_size,), dtype=tf.float32)\n    initial_states = {\"lstm_state\": initial_state_values}\n    tf_batch=batch_sequences_with_states(\n        input_key = tf_fb_key, \n        input_sequences = tf_bbucket_outputs, \n        input_context = {}, \n        input_length = tf.reduce_max(tf_bucket_sequence_length), \n        initial_states=initial_states, \n        num_unroll=seq_len, \n        batch_size=batch_size, \n        num_threads=3, \n        capacity=1000, \n        allow_small_batch=False, \n        pad=True, \n        name=None)\n    #######################################\n    # Solution 2:\n    '''\n    tf_index_queue=tf.train.range_input_producer(\n        limit=tf.reduce_max(tf_bucket_sequence_length),\n        num_epochs=1, \n        shuffle=False, \n        seed=None, \n        capacity=32, \n        shared_name=None, \n        name=None\n    )\n    tf_index=tf_index_queue.dequeue()\n    tf_batch=tf_bbucket_outputs[\"inputs\"][tf_index]\n    '''\n    #######################################\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n        try:\n            while True:\n                #####################################\n                # Test Bucketing\n                #bucket_sequence_length, bucket_outputs = sess.run([tf_bucket_sequence_length, tf_bucket_outputs])\n                #print(bucket_sequence_length)\n                #print(bucket_outputs)\n                #print(\"#################\")\n                #####################################\n                # Test Solution 1:\n                batch = sess.run(tf_batch)\n                print(batch)\n                #####################################\n                # Test Solution 2:\n                #bucket_sequence_length, bucket_outputs, index = sess.run([tf_bucket_sequence_length, tf_bucket_outputs, tf_index])\n                #print(bucket_sequence_length)\n                #print(bucket_outputs)\n                #print(index)\n                #print(\"#################\")\n        except tf.errors.OutOfRangeError:\n            pass\n        except tf.errors.InvalidArgumentError:\n            pass\n        finally:\n            coord.request_stop()\n        coord.join(threads)\n    \nif __name__ == \"__main__\":\n    #GenerateFakeData()\n    InputProducer()\n    pass\n\nWith Solution 1, I raised the error as below:\nTraceback (most recent call last):\n  \n    File \"/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py\", line 157, in <module>\n    \nInputProducer()\n  \n    File \"/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py\", line 107, in InputProducer\n    \nname=None)\n  \n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 1522, in batch_sequences_with_states\n    \nallow_small_batch=allow_small_batch)\n  \n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 849, in __init__\n    \ninitial_states)\n  \n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 332, in _prepare_sequence_inputs\n    \n\"sequence\", inputs.sequences, ignore_first_dimension=True)\n  \n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 326, in _assert_fully_defined\n    \nignore_first_dimension else \"\", v.get_shape()))\nValueError: Shape for sequence inputs is not fully defined (ignoring first dimension): (?, ?)\n\nSee the document of batch_sequences_with_states, I found that\n\nit seems only support only one sequence and don't support multiple sequences .\nit don't support the situation of Shape for sequence inputs is not fully defined, which means bucket_by_sequence_length can not be followed with batch_sequences_with_states.\n\nWhat more, I have tried Solution 2, but I failed because of thread synchronization problem between tf.train.string_input_producer and tf.train.range_input_producer.\nSo, how to relize my request ?\nHope for your help.", "body": "The concept of \u201cgenuine consecutive scheme \u201d can be seen at [here(5.4. Batch)](http://www.sciencedirect.com/science/article/pii/S088523081400093X?via%3Dihub).\r\n\r\nMy scenario is as follows:\r\n\r\nI have some files with different sequence lengths.\r\n\r\nFirst, do buckecting to generate file-batches with parameter `batch_size`\r\n\r\nThen, split each  file-batch with parameter `seq_len` to generate trainning sample-batches\r\n\r\nLast, use each sample-batch for one step of trainning.\r\n\r\nFollowing is my test code:\r\n\r\n```\r\n# -*- coding:utf8 -*-\r\n\r\nimport os\r\nimport time\r\nimport random\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.training import bucket_by_sequence_length, batch_sequences_with_states\r\n\r\n\r\ncontext_features = {\r\n    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\r\n}\r\n\r\nsequence_features = {\r\n            \"inputs\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\r\n}\r\n\r\ndef GenerateFakeData():\r\n    FILE_NUM = 100\r\n    DATA_PATH = \"test_dataset\"\r\n    file_path_list, file_len_list = [], []\r\n    for idx in range(FILE_NUM):\r\n        filename = \"{fileno}-of-{idx}\".format(idx=idx+1, fileno=FILE_NUM)\r\n        token_length = random.randint(50, 100)\r\n        ex = tf.train.SequenceExample()\r\n        ex.context.feature[\"length\"].int64_list.value.append(token_length)\r\n        ###########################################\r\n        ex_tokens = ex.feature_lists.feature_list[\"inputs\"]\r\n        for tok in range(token_length):\r\n            ex_tokens.feature.add().int64_list.value.append(tok)\r\n        with tf.python_io.TFRecordWriter(os.path.join(DATA_PATH, filename) + \".tfrecord\") as filew:\r\n            filew.write(ex.SerializeToString())\r\n        file_len_list.append(token_length)\r\n        file_path_list.append(os.path.join(DATA_PATH, filename) + \".tfrecord\")\r\n    with open(\"filelist.txt\", \"w\") as filew:\r\n        for file_name, file_len in zip(file_path_list, file_len_list):\r\n            filew.write(\"{fn}\\t{fl}\\n\".format(fn=os.path.join(file_name), fl=file_len))\r\n\r\ndef LoadFileList(filepath):\r\n    with open(filepath, \"r\") as filer:\r\n        wfilelist, wfilelengthlist = tuple(zip(*[tuple(line.strip().split(\"\\t\")) for line in filer if line.strip() != \"\"]))\r\n        return list(wfilelist), [int(item) for item in wfilelengthlist]\r\n\r\n        \r\ndef InputProducer():\r\n    batch_size = 2\r\n    seq_len = 75\r\n    state_size = 1024\r\n    bucket_boundaries = [60, 70, 80, 90]\r\n    #####################################\r\n    filelist, filelengthlist = LoadFileList(\"filelist.txt\")\r\n    #####################################\r\n    tf_file_queue = tf.train.string_input_producer(\r\n            string_tensor = filelist, \r\n            num_epochs = 1, \r\n            shuffle = False, \r\n            seed = None, \r\n            capacity = 32, \r\n            shared_name = None,\r\n            name = \"tf_file_queue\",\r\n            cancel_op=None\r\n    )\r\n    ######################################\r\n    tf_reader = tf.TFRecordReader()\r\n    tf_key, tf_serialized = tf_reader.read(tf_file_queue)\r\n    tf_context, tf_sequence = tf.parse_single_sequence_example(\r\n            serialized = tf_serialized,\r\n            context_features = context_features,\r\n            sequence_features = sequence_features\r\n    )\r\n    ######################################\r\n    tf_bucket_sequence_length, tf_bucket_outputs = bucket_by_sequence_length(\r\n        input_length = tf.cast(tf_context[\"length\"], dtype=tf.int32), \r\n        tensors = tf_sequence, \r\n        batch_size = batch_size, \r\n        bucket_boundaries = bucket_boundaries, \r\n        num_threads=1, \r\n        capacity=32, \r\n        shapes=None, \r\n        dynamic_pad=True,\r\n        allow_smaller_final_batch=False, \r\n        keep_input=True, \r\n        shared_name=None, \r\n        name=\"bucket_files\"\r\n    )\r\n    #######################################\r\n    tf_bbucket_outputs = {}\r\n    for fkey in tf_bucket_outputs:\r\n        tf_bbucket_outputs[fkey]=tf_bucket_outputs[fkey][0]\r\n    #######################################\r\n    # Solution 1:\r\n    tf_fb_key=time.strftime('%Y-%m-%d-%H-%M-%S',time.localtime(time.time())) + str(random.randint(1,100000000))\r\n    initial_state_values = tf.zeros((state_size,), dtype=tf.float32)\r\n    initial_states = {\"lstm_state\": initial_state_values}\r\n    tf_batch=batch_sequences_with_states(\r\n        input_key = tf_fb_key, \r\n        input_sequences = tf_bbucket_outputs, \r\n        input_context = {}, \r\n        input_length = tf.reduce_max(tf_bucket_sequence_length), \r\n        initial_states=initial_states, \r\n        num_unroll=seq_len, \r\n        batch_size=batch_size, \r\n        num_threads=3, \r\n        capacity=1000, \r\n        allow_small_batch=False, \r\n        pad=True, \r\n        name=None)\r\n    #######################################\r\n    # Solution 2:\r\n    '''\r\n    tf_index_queue=tf.train.range_input_producer(\r\n        limit=tf.reduce_max(tf_bucket_sequence_length),\r\n        num_epochs=1, \r\n        shuffle=False, \r\n        seed=None, \r\n        capacity=32, \r\n        shared_name=None, \r\n        name=None\r\n    )\r\n    tf_index=tf_index_queue.dequeue()\r\n    tf_batch=tf_bbucket_outputs[\"inputs\"][tf_index]\r\n    '''\r\n    #######################################\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess, coord)\r\n        try:\r\n            while True:\r\n                #####################################\r\n                # Test Bucketing\r\n                #bucket_sequence_length, bucket_outputs = sess.run([tf_bucket_sequence_length, tf_bucket_outputs])\r\n                #print(bucket_sequence_length)\r\n                #print(bucket_outputs)\r\n                #print(\"#################\")\r\n                #####################################\r\n                # Test Solution 1:\r\n                batch = sess.run(tf_batch)\r\n                print(batch)\r\n                #####################################\r\n                # Test Solution 2:\r\n                #bucket_sequence_length, bucket_outputs, index = sess.run([tf_bucket_sequence_length, tf_bucket_outputs, tf_index])\r\n                #print(bucket_sequence_length)\r\n                #print(bucket_outputs)\r\n                #print(index)\r\n                #print(\"#################\")\r\n        except tf.errors.OutOfRangeError:\r\n            pass\r\n        except tf.errors.InvalidArgumentError:\r\n            pass\r\n        finally:\r\n            coord.request_stop()\r\n        coord.join(threads)\r\n    \r\nif __name__ == \"__main__\":\r\n    #GenerateFakeData()\r\n    InputProducer()\r\n    pass\r\n```\r\n\r\nWith Solution 1, I raised the error as below:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  \r\n    File \"/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py\", line 157, in <module>\r\n    \r\nInputProducer()\r\n  \r\n    File \"/home/yangming/workspace/tfstudy-3.5.3-tf-1.1.0/BatchSchemas/make_test_dataset.py\", line 107, in InputProducer\r\n    \r\nname=None)\r\n  \r\n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 1522, in batch_sequences_with_states\r\n    \r\nallow_small_batch=allow_small_batch)\r\n  \r\n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 849, in __init__\r\n    \r\ninitial_states)\r\n  \r\n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 332, in _prepare_sequence_inputs\r\n    \r\n\"sequence\", inputs.sequences, ignore_first_dimension=True)\r\n  \r\n    File \"/home/yangming/.pyenv/versions/tfstudy-3.5.3/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/sequence_queueing_state_saver.py\", line 326, in _assert_fully_defined\r\n    \r\nignore_first_dimension else \"\", v.get_shape()))\r\nValueError: Shape for sequence inputs is not fully defined (ignoring first dimension): (?, ?)\r\n```\r\n\r\nSee the document of `batch_sequences_with_states`, I found that \r\n\r\n1. it seems only support only one sequence and don't support multiple sequences .\r\n2. it don't support the situation of `Shape for sequence inputs is not fully defined`, which means `bucket_by_sequence_length` can not be followed with `batch_sequences_with_states`.\r\n\r\nWhat more, I have tried Solution 2, but I failed because of thread synchronization problem between `tf.train.string_input_producer` and `tf.train.range_input_producer`.\r\n\r\nSo, how to relize my request ?\r\n\r\nHope for your help."}