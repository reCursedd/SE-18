{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259240080", "html_url": "https://github.com/tensorflow/tensorflow/issues/5412#issuecomment-259240080", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5412", "id": 259240080, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTI0MDA4MA==", "user": {"login": "rdipietro", "id": 5150559, "node_id": "MDQ6VXNlcjUxNTA1NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5150559?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rdipietro", "html_url": "https://github.com/rdipietro", "followers_url": "https://api.github.com/users/rdipietro/followers", "following_url": "https://api.github.com/users/rdipietro/following{/other_user}", "gists_url": "https://api.github.com/users/rdipietro/gists{/gist_id}", "starred_url": "https://api.github.com/users/rdipietro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rdipietro/subscriptions", "organizations_url": "https://api.github.com/users/rdipietro/orgs", "repos_url": "https://api.github.com/users/rdipietro/repos", "events_url": "https://api.github.com/users/rdipietro/events{/privacy}", "received_events_url": "https://api.github.com/users/rdipietro/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-08T19:49:30Z", "updated_at": "2016-11-25T14:20:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Yes, that's my point really: In order to access the <code>TensorArray</code> intermediate states, I need to use <code>pack</code> or <code>gather</code>. This creates a new <code>Tensor</code> which isn't on the route in the graph between the intermediate states and the final <code>loss</code>, and therefore if we differentiate with respect to this new node, we'll get a bunch of zeros.</p>\n<p>But I need the intermediate gradients, as shown in this simple example that uses an explicit loop instead of <code>tf.while_loop</code>:</p>\n<pre><code>x = [tf.constant(1.0)]\nfor _ in xrange(2):\n    x.append(2*x[-1])\n\nloss = x[-1]\ndloss_dx = tf.gradients(loss, x)\n\nwith tf.Session() as sess:\n    print(sess.run(dloss_dx))\n</code></pre>\n<p>This will print <code>[4., 2., 1.]</code>.</p>", "body_text": "Yes, that's my point really: In order to access the TensorArray intermediate states, I need to use pack or gather. This creates a new Tensor which isn't on the route in the graph between the intermediate states and the final loss, and therefore if we differentiate with respect to this new node, we'll get a bunch of zeros.\nBut I need the intermediate gradients, as shown in this simple example that uses an explicit loop instead of tf.while_loop:\nx = [tf.constant(1.0)]\nfor _ in xrange(2):\n    x.append(2*x[-1])\n\nloss = x[-1]\ndloss_dx = tf.gradients(loss, x)\n\nwith tf.Session() as sess:\n    print(sess.run(dloss_dx))\n\nThis will print [4., 2., 1.].", "body": "Yes, that's my point really: In order to access the `TensorArray` intermediate states, I need to use `pack` or `gather`. This creates a new `Tensor` which isn't on the route in the graph between the intermediate states and the final `loss`, and therefore if we differentiate with respect to this new node, we'll get a bunch of zeros.\r\n\r\nBut I need the intermediate gradients, as shown in this simple example that uses an explicit loop instead of `tf.while_loop`:\r\n\r\n```\r\nx = [tf.constant(1.0)]\r\nfor _ in xrange(2):\r\n    x.append(2*x[-1])\r\n\r\nloss = x[-1]\r\ndloss_dx = tf.gradients(loss, x)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(dloss_dx))\r\n```\r\n\r\nThis will print `[4., 2., 1.]`."}