{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5412", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5412/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5412/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5412/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5412", "id": 187487662, "node_id": "MDU6SXNzdWUxODc0ODc2NjI=", "number": 5412, "title": "Request: Functionality for obtaining gradients that are internal to a while_loop", "user": {"login": "rdipietro", "id": 5150559, "node_id": "MDQ6VXNlcjUxNTA1NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5150559?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rdipietro", "html_url": "https://github.com/rdipietro", "followers_url": "https://api.github.com/users/rdipietro/followers", "following_url": "https://api.github.com/users/rdipietro/following{/other_user}", "gists_url": "https://api.github.com/users/rdipietro/gists{/gist_id}", "starred_url": "https://api.github.com/users/rdipietro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rdipietro/subscriptions", "organizations_url": "https://api.github.com/users/rdipietro/orgs", "repos_url": "https://api.github.com/users/rdipietro/repos", "events_url": "https://api.github.com/users/rdipietro/events{/privacy}", "received_events_url": "https://api.github.com/users/rdipietro/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-11-05T09:53:11Z", "updated_at": "2017-06-16T16:52:58Z", "closed_at": "2017-06-16T16:52:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'd like to manipulate the gradients that are internal to a while loop. The actual purpose is to view / manipulate recurrent-neural-network behavior, but here is a simplified example to get the idea across:</p>\n<pre><code>t_0 = tf.constant(0)\nx_0 = tf.constant(1.0)\nsize = 3\nx_ta = tf.TensorArray(tf.float32, size=size)\n\ndef cond(t, x_prev, x_ta):\n    return tf.less(t, size)\n\ndef body(t, x_prev, x_ta):\n    x = 2*x_prev\n    x_ta = x_ta.write(t, x)\n    return t + 1, x, x_ta\n\n_, _, x_ta = tf.while_loop(cond, body, [t_0, x_0, x_ta])\nx = x_ta.pack()\n\nloss = x[size-1]\n\ndloss_dx, = tf.gradients(loss, x)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(dloss_dx))\n</code></pre>\n<p>In this example, <code>x[t] = 2*x[t-1]</code> for all <code>t</code>, so we'd like <code>dloss_dx</code> to be <code>[4., 2., 1.]</code>. We instead get <code>[ 0.  0.  1.]</code>. I don't think this is a bug: we're actually taking the derivative with respect to the pack operation, which <code>x[2]</code> doesn't actually depend on.</p>\n<p>Please correct me if I'm wrong, but I think it's currently impossible to obtain <code>[4., 2., 1.]</code> in the general case. (One hack here would be to compute gradients within the <code>while_loop</code> and then form a cumulative product corresponding to the chain rule, but this won't work with vector-valued <code>x[t]</code>'s because there's no way to get the Jacobians).</p>\n<p>But the gradients with respect to each of these time steps must live somewhere. Can we add a property to the <code>TensorArray</code> class to be able to obtain these?</p>", "body_text": "I'd like to manipulate the gradients that are internal to a while loop. The actual purpose is to view / manipulate recurrent-neural-network behavior, but here is a simplified example to get the idea across:\nt_0 = tf.constant(0)\nx_0 = tf.constant(1.0)\nsize = 3\nx_ta = tf.TensorArray(tf.float32, size=size)\n\ndef cond(t, x_prev, x_ta):\n    return tf.less(t, size)\n\ndef body(t, x_prev, x_ta):\n    x = 2*x_prev\n    x_ta = x_ta.write(t, x)\n    return t + 1, x, x_ta\n\n_, _, x_ta = tf.while_loop(cond, body, [t_0, x_0, x_ta])\nx = x_ta.pack()\n\nloss = x[size-1]\n\ndloss_dx, = tf.gradients(loss, x)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(dloss_dx))\n\nIn this example, x[t] = 2*x[t-1] for all t, so we'd like dloss_dx to be [4., 2., 1.]. We instead get [ 0.  0.  1.]. I don't think this is a bug: we're actually taking the derivative with respect to the pack operation, which x[2] doesn't actually depend on.\nPlease correct me if I'm wrong, but I think it's currently impossible to obtain [4., 2., 1.] in the general case. (One hack here would be to compute gradients within the while_loop and then form a cumulative product corresponding to the chain rule, but this won't work with vector-valued x[t]'s because there's no way to get the Jacobians).\nBut the gradients with respect to each of these time steps must live somewhere. Can we add a property to the TensorArray class to be able to obtain these?", "body": "I'd like to manipulate the gradients that are internal to a while loop. The actual purpose is to view / manipulate recurrent-neural-network behavior, but here is a simplified example to get the idea across:\r\n\r\n```\r\nt_0 = tf.constant(0)\r\nx_0 = tf.constant(1.0)\r\nsize = 3\r\nx_ta = tf.TensorArray(tf.float32, size=size)\r\n\r\ndef cond(t, x_prev, x_ta):\r\n    return tf.less(t, size)\r\n\r\ndef body(t, x_prev, x_ta):\r\n    x = 2*x_prev\r\n    x_ta = x_ta.write(t, x)\r\n    return t + 1, x, x_ta\r\n\r\n_, _, x_ta = tf.while_loop(cond, body, [t_0, x_0, x_ta])\r\nx = x_ta.pack()\r\n\r\nloss = x[size-1]\r\n\r\ndloss_dx, = tf.gradients(loss, x)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.initialize_all_variables())\r\n    print(sess.run(dloss_dx))\r\n```\r\n\r\nIn this example, `x[t] = 2*x[t-1]` for all `t`, so we'd like `dloss_dx` to be `[4., 2., 1.]`. We instead get `[ 0.  0.  1.]`. I don't think this is a bug: we're actually taking the derivative with respect to the pack operation, which `x[2]` doesn't actually depend on.\r\n\r\nPlease correct me if I'm wrong, but I think it's currently impossible to obtain `[4., 2., 1.]` in the general case. (One hack here would be to compute gradients within the `while_loop` and then form a cumulative product corresponding to the chain rule, but this won't work with vector-valued `x[t]`'s because there's no way to get the Jacobians).\r\n\r\nBut the gradients with respect to each of these time steps must live somewhere. Can we add a property to the `TensorArray` class to be able to obtain these?"}