{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/379802419", "html_url": "https://github.com/tensorflow/tensorflow/issues/18187#issuecomment-379802419", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18187", "id": 379802419, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTgwMjQxOQ==", "user": {"login": "jjahanip", "id": 14131478, "node_id": "MDQ6VXNlcjE0MTMxNDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/14131478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjahanip", "html_url": "https://github.com/jjahanip", "followers_url": "https://api.github.com/users/jjahanip/followers", "following_url": "https://api.github.com/users/jjahanip/following{/other_user}", "gists_url": "https://api.github.com/users/jjahanip/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjahanip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjahanip/subscriptions", "organizations_url": "https://api.github.com/users/jjahanip/orgs", "repos_url": "https://api.github.com/users/jjahanip/repos", "events_url": "https://api.github.com/users/jjahanip/events{/privacy}", "received_events_url": "https://api.github.com/users/jjahanip/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-09T15:57:51Z", "updated_at": "2018-04-09T15:57:51Z", "author_association": "NONE", "body_html": "<p>I already checked the stackoverflow and found <a href=\"https://stackoverflow.com/questions/37842913/tensorflow-confusion-regarding-the-adam-optimizer?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa\" rel=\"nofollow\">this</a> post explaining the concept. I am familiar with the concept. But I think there is an issue that the TensorFlow is not using the decaying learning rate. That is why I submitted this issue.</p>\n<p>Full code to reproduce the results:</p>\n<pre><code>import tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\ndef fc_layer(x, num_units, name, use_relu=True):\n    in_dim = x.get_shape()[1]\n    W = tf.get_variable('W_' + name,\n                    dtype=tf.float32,\n                    shape=[in_dim, num_units],\n                    initializer=tf.truncated_normal_initializer(stddev=0.01))\n    b = tf.get_variable('b_' + name,\n                           dtype=tf.float32,\n                           initializer=tf.constant(0., shape=[num_units], dtype=tf.float32))\n    layer = tf.matmul(x, W)\n    layer += b\n    if use_relu:\n        layer = tf.nn.relu(layer)\n    return layer\n\n# Data Dimensions\nimg_size_flat = 784\nn_classes = 10\n\n\n# Hyper-parameters\nlr = 10  # The optimization initial learning rate\nbatch_size = 100  # Training batch size\ndisplay_freq = 100  # Frequency of displaying the training results\nepochs = 10\n\nh1 = 128  # Number of neurons in hidden layer layer.\n\n\n# Create the network graph\n# Placeholders for inputs (x), outputs(y)\nx = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\ny = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n\nfc1 = fc_layer(x, h1, 'FC1', use_relu=True)\noutput_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)\n\n# Define the loss function, optimizer, and accuracy\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n\noptimizer_obj = tf.train.AdamOptimizer(learning_rate=lr,beta1=0.7, beta2=.3, name='Adam-op')\noptimizer = optimizer_obj.minimize(loss)\n\n# Initialize the variables\ninit = tf.global_variables_initializer()\n\n\n# Launch the graph (session)\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Number of training iterations in each epoch\n    num_tr_iter = int(mnist.train.num_examples / batch_size)\n    step = 0\n    for epoch in range(epochs):\n        for iteration in range(num_tr_iter):\n            start = iteration * batch_size\n            end = (iteration + 1) * batch_size\n            x_batch, y_batch = mnist.train.next_batch(batch_size)\n\n            # Run optimization op (backprop)\n            feed_dict_batch = {x: x_batch, y: y_batch}\n            sess.run(optimizer, feed_dict=feed_dict_batch)\n            print('step:{} \\t learning rate: {}'.format(step, sess.run(optimizer_obj._lr_t, feed_dict=feed_dict_batch)))\n            step = step + 1\n</code></pre>\n<p>And this is the output that I get for each step:</p>\n<pre><code>step:0 \t learning rate: 10\nstep:1 \t learning rate: 10\nstep:2 \t learning rate: 10\nstep:3 \t learning rate: 10\nstep:4 \t learning rate: 10\nstep:5 \t learning rate: 10\nstep:6 \t learning rate: 10\nstep:7 \t learning rate: 10\nstep:8 \t learning rate: 10\nstep:9 \t learning rate: 10\nstep:10 \t learning rate: 10\nstep:11 \t learning rate: 10\nstep:12 \t learning rate: 10\nstep:13 \t learning rate: 10\nstep:14 \t learning rate: 10\nstep:15 \t learning rate: 10\nstep:16 \t learning rate: 10\nstep:17 \t learning rate: 10\nstep:18 \t learning rate: 10\nstep:19 \t learning rate: 10\nstep:20 \t learning rate: 10\nstep:21 \t learning rate: 10\nstep:22 \t learning rate: 10\nstep:23 \t learning rate: 10\nstep:24 \t learning rate: 10\nstep:25 \t learning rate: 10\nstep:26 \t learning rate: 10\nstep:27 \t learning rate: 10\nstep:28 \t learning rate: 10\nstep:29 \t learning rate: 10\nstep:30 \t learning rate: 10\nstep:31 \t learning rate: 10\nstep:32 \t learning rate: 10\nstep:33 \t learning rate: 10\nstep:34 \t learning rate: 10\nstep:35 \t learning rate: 10\nstep:36 \t learning rate: 10\nstep:37 \t learning rate: 10\nstep:38 \t learning rate: 10\nstep:39 \t learning rate: 10\nstep:40 \t learning rate: 10\nstep:41 \t learning rate: 10\nstep:42 \t learning rate: 10\nstep:43 \t learning rate: 10\nstep:44 \t learning rate: 10\nstep:45 \t learning rate: 10\nstep:46 \t learning rate: 10\nstep:47 \t learning rate: 10\nstep:48 \t learning rate: 10\nstep:49 \t learning rate: 10\nstep:50 \t learning rate: 10\n</code></pre>\n<p>I just output the first 50 steps. But the learning rate is fixed all the time.</p>", "body_text": "I already checked the stackoverflow and found this post explaining the concept. I am familiar with the concept. But I think there is an issue that the TensorFlow is not using the decaying learning rate. That is why I submitted this issue.\nFull code to reproduce the results:\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\ndef fc_layer(x, num_units, name, use_relu=True):\n    in_dim = x.get_shape()[1]\n    W = tf.get_variable('W_' + name,\n                    dtype=tf.float32,\n                    shape=[in_dim, num_units],\n                    initializer=tf.truncated_normal_initializer(stddev=0.01))\n    b = tf.get_variable('b_' + name,\n                           dtype=tf.float32,\n                           initializer=tf.constant(0., shape=[num_units], dtype=tf.float32))\n    layer = tf.matmul(x, W)\n    layer += b\n    if use_relu:\n        layer = tf.nn.relu(layer)\n    return layer\n\n# Data Dimensions\nimg_size_flat = 784\nn_classes = 10\n\n\n# Hyper-parameters\nlr = 10  # The optimization initial learning rate\nbatch_size = 100  # Training batch size\ndisplay_freq = 100  # Frequency of displaying the training results\nepochs = 10\n\nh1 = 128  # Number of neurons in hidden layer layer.\n\n\n# Create the network graph\n# Placeholders for inputs (x), outputs(y)\nx = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\ny = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n\nfc1 = fc_layer(x, h1, 'FC1', use_relu=True)\noutput_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)\n\n# Define the loss function, optimizer, and accuracy\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n\noptimizer_obj = tf.train.AdamOptimizer(learning_rate=lr,beta1=0.7, beta2=.3, name='Adam-op')\noptimizer = optimizer_obj.minimize(loss)\n\n# Initialize the variables\ninit = tf.global_variables_initializer()\n\n\n# Launch the graph (session)\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Number of training iterations in each epoch\n    num_tr_iter = int(mnist.train.num_examples / batch_size)\n    step = 0\n    for epoch in range(epochs):\n        for iteration in range(num_tr_iter):\n            start = iteration * batch_size\n            end = (iteration + 1) * batch_size\n            x_batch, y_batch = mnist.train.next_batch(batch_size)\n\n            # Run optimization op (backprop)\n            feed_dict_batch = {x: x_batch, y: y_batch}\n            sess.run(optimizer, feed_dict=feed_dict_batch)\n            print('step:{} \\t learning rate: {}'.format(step, sess.run(optimizer_obj._lr_t, feed_dict=feed_dict_batch)))\n            step = step + 1\n\nAnd this is the output that I get for each step:\nstep:0 \t learning rate: 10\nstep:1 \t learning rate: 10\nstep:2 \t learning rate: 10\nstep:3 \t learning rate: 10\nstep:4 \t learning rate: 10\nstep:5 \t learning rate: 10\nstep:6 \t learning rate: 10\nstep:7 \t learning rate: 10\nstep:8 \t learning rate: 10\nstep:9 \t learning rate: 10\nstep:10 \t learning rate: 10\nstep:11 \t learning rate: 10\nstep:12 \t learning rate: 10\nstep:13 \t learning rate: 10\nstep:14 \t learning rate: 10\nstep:15 \t learning rate: 10\nstep:16 \t learning rate: 10\nstep:17 \t learning rate: 10\nstep:18 \t learning rate: 10\nstep:19 \t learning rate: 10\nstep:20 \t learning rate: 10\nstep:21 \t learning rate: 10\nstep:22 \t learning rate: 10\nstep:23 \t learning rate: 10\nstep:24 \t learning rate: 10\nstep:25 \t learning rate: 10\nstep:26 \t learning rate: 10\nstep:27 \t learning rate: 10\nstep:28 \t learning rate: 10\nstep:29 \t learning rate: 10\nstep:30 \t learning rate: 10\nstep:31 \t learning rate: 10\nstep:32 \t learning rate: 10\nstep:33 \t learning rate: 10\nstep:34 \t learning rate: 10\nstep:35 \t learning rate: 10\nstep:36 \t learning rate: 10\nstep:37 \t learning rate: 10\nstep:38 \t learning rate: 10\nstep:39 \t learning rate: 10\nstep:40 \t learning rate: 10\nstep:41 \t learning rate: 10\nstep:42 \t learning rate: 10\nstep:43 \t learning rate: 10\nstep:44 \t learning rate: 10\nstep:45 \t learning rate: 10\nstep:46 \t learning rate: 10\nstep:47 \t learning rate: 10\nstep:48 \t learning rate: 10\nstep:49 \t learning rate: 10\nstep:50 \t learning rate: 10\n\nI just output the first 50 steps. But the learning rate is fixed all the time.", "body": "I already checked the stackoverflow and found [this](https://stackoverflow.com/questions/37842913/tensorflow-confusion-regarding-the-adam-optimizer?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) post explaining the concept. I am familiar with the concept. But I think there is an issue that the TensorFlow is not using the decaying learning rate. That is why I submitted this issue.\r\n\r\nFull code to reproduce the results:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n\r\ndef fc_layer(x, num_units, name, use_relu=True):\r\n    in_dim = x.get_shape()[1]\r\n    W = tf.get_variable('W_' + name,\r\n                    dtype=tf.float32,\r\n                    shape=[in_dim, num_units],\r\n                    initializer=tf.truncated_normal_initializer(stddev=0.01))\r\n    b = tf.get_variable('b_' + name,\r\n                           dtype=tf.float32,\r\n                           initializer=tf.constant(0., shape=[num_units], dtype=tf.float32))\r\n    layer = tf.matmul(x, W)\r\n    layer += b\r\n    if use_relu:\r\n        layer = tf.nn.relu(layer)\r\n    return layer\r\n\r\n# Data Dimensions\r\nimg_size_flat = 784\r\nn_classes = 10\r\n\r\n\r\n# Hyper-parameters\r\nlr = 10  # The optimization initial learning rate\r\nbatch_size = 100  # Training batch size\r\ndisplay_freq = 100  # Frequency of displaying the training results\r\nepochs = 10\r\n\r\nh1 = 128  # Number of neurons in hidden layer layer.\r\n\r\n\r\n# Create the network graph\r\n# Placeholders for inputs (x), outputs(y)\r\nx = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\r\ny = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\r\n\r\nfc1 = fc_layer(x, h1, 'FC1', use_relu=True)\r\noutput_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)\r\n\r\n# Define the loss function, optimizer, and accuracy\r\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\r\n\r\noptimizer_obj = tf.train.AdamOptimizer(learning_rate=lr,beta1=0.7, beta2=.3, name='Adam-op')\r\noptimizer = optimizer_obj.minimize(loss)\r\n\r\n# Initialize the variables\r\ninit = tf.global_variables_initializer()\r\n\r\n\r\n# Launch the graph (session)\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n\r\n    # Number of training iterations in each epoch\r\n    num_tr_iter = int(mnist.train.num_examples / batch_size)\r\n    step = 0\r\n    for epoch in range(epochs):\r\n        for iteration in range(num_tr_iter):\r\n            start = iteration * batch_size\r\n            end = (iteration + 1) * batch_size\r\n            x_batch, y_batch = mnist.train.next_batch(batch_size)\r\n\r\n            # Run optimization op (backprop)\r\n            feed_dict_batch = {x: x_batch, y: y_batch}\r\n            sess.run(optimizer, feed_dict=feed_dict_batch)\r\n            print('step:{} \\t learning rate: {}'.format(step, sess.run(optimizer_obj._lr_t, feed_dict=feed_dict_batch)))\r\n            step = step + 1\r\n```\r\n\r\nAnd this is the output that I get for each step:\r\n```\r\nstep:0 \t learning rate: 10\r\nstep:1 \t learning rate: 10\r\nstep:2 \t learning rate: 10\r\nstep:3 \t learning rate: 10\r\nstep:4 \t learning rate: 10\r\nstep:5 \t learning rate: 10\r\nstep:6 \t learning rate: 10\r\nstep:7 \t learning rate: 10\r\nstep:8 \t learning rate: 10\r\nstep:9 \t learning rate: 10\r\nstep:10 \t learning rate: 10\r\nstep:11 \t learning rate: 10\r\nstep:12 \t learning rate: 10\r\nstep:13 \t learning rate: 10\r\nstep:14 \t learning rate: 10\r\nstep:15 \t learning rate: 10\r\nstep:16 \t learning rate: 10\r\nstep:17 \t learning rate: 10\r\nstep:18 \t learning rate: 10\r\nstep:19 \t learning rate: 10\r\nstep:20 \t learning rate: 10\r\nstep:21 \t learning rate: 10\r\nstep:22 \t learning rate: 10\r\nstep:23 \t learning rate: 10\r\nstep:24 \t learning rate: 10\r\nstep:25 \t learning rate: 10\r\nstep:26 \t learning rate: 10\r\nstep:27 \t learning rate: 10\r\nstep:28 \t learning rate: 10\r\nstep:29 \t learning rate: 10\r\nstep:30 \t learning rate: 10\r\nstep:31 \t learning rate: 10\r\nstep:32 \t learning rate: 10\r\nstep:33 \t learning rate: 10\r\nstep:34 \t learning rate: 10\r\nstep:35 \t learning rate: 10\r\nstep:36 \t learning rate: 10\r\nstep:37 \t learning rate: 10\r\nstep:38 \t learning rate: 10\r\nstep:39 \t learning rate: 10\r\nstep:40 \t learning rate: 10\r\nstep:41 \t learning rate: 10\r\nstep:42 \t learning rate: 10\r\nstep:43 \t learning rate: 10\r\nstep:44 \t learning rate: 10\r\nstep:45 \t learning rate: 10\r\nstep:46 \t learning rate: 10\r\nstep:47 \t learning rate: 10\r\nstep:48 \t learning rate: 10\r\nstep:49 \t learning rate: 10\r\nstep:50 \t learning rate: 10\r\n```\r\n\r\nI just output the first 50 steps. But the learning rate is fixed all the time."}