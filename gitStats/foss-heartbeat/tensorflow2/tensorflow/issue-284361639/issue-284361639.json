{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15614", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15614/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15614/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15614/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/15614", "id": 284361639, "node_id": "MDExOlB1bGxSZXF1ZXN0MTU5OTk5MDUy", "number": 15614, "title": "Support Negativo17 Fedora Packaging", "user": {"login": "njwhite", "id": 912123, "node_id": "MDQ6VXNlcjkxMjEyMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/912123?v=4", "gravatar_id": "", "url": "https://api.github.com/users/njwhite", "html_url": "https://github.com/njwhite", "followers_url": "https://api.github.com/users/njwhite/followers", "following_url": "https://api.github.com/users/njwhite/following{/other_user}", "gists_url": "https://api.github.com/users/njwhite/gists{/gist_id}", "starred_url": "https://api.github.com/users/njwhite/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/njwhite/subscriptions", "organizations_url": "https://api.github.com/users/njwhite/orgs", "repos_url": "https://api.github.com/users/njwhite/repos", "events_url": "https://api.github.com/users/njwhite/events{/privacy}", "received_events_url": "https://api.github.com/users/njwhite/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 390482148, "node_id": "MDU6TGFiZWwzOTA0ODIxNDg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20review", "name": "awaiting review", "color": "fef2c0", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-24T13:05:18Z", "updated_at": "2018-01-23T19:21:34Z", "closed_at": "2018-01-23T19:21:34Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15614", "html_url": "https://github.com/tensorflow/tensorflow/pull/15614", "diff_url": "https://github.com/tensorflow/tensorflow/pull/15614.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/15614.patch"}, "body_html": "<p>Support the <a href=\"https://negativo17.org/nvidia-driver/\" rel=\"nofollow\">Negativo17</a> Nvidia driver packaging for Fedora. <code>libdevice</code> libraries are under <code>/usr/share/cuda</code>, includes are under <code>/usr/include/cuda</code> and libraries are under <code>/usr/lib64</code>. This PR should help <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"213250747\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8264\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8264/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/8264\">#8264</a> too.</p>\n<p>In addition, the gcc 5.3 in the Negativo17 repository (installed as <code>/usr/bin/gcc53</code>) only has a static non-PIC version of <code>libgomp.a</code>, so I have this local patch to force Tensorflow to link to the global (<code>/usr/lib64</code>) shared version:</p>\n<div class=\"highlight highlight-source-diff\"><pre><span class=\"pl-c1\">diff --git a/tensorflow/contrib/cmake/tf_stream_executor.cmake b/tensorflow/contrib/cmake/tf_stream_executor.cmake</span>\nindex 91ca33f4c4..7719ee096d 100644\n<span class=\"pl-md\">--- a/tensorflow/contrib/cmake/tf_stream_executor.cmake</span>\n<span class=\"pl-mi1\">+++ b/tensorflow/contrib/cmake/tf_stream_executor.cmake</span>\n<span class=\"pl-mdr\">@@ -75,7 +75,7 @@</span> endif()\n #list(REMOVE_ITEM tf_stream_executor_srcs ${tf_stream_executor_test_srcs})\n \n if (NOT WIN32)\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -lgomp\")</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -l:libgomp.so.1\")</span>\n endif (NOT WIN32)\n add_library(tf_stream_executor OBJECT ${tf_stream_executor_srcs})\n \n<span class=\"pl-c1\">diff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl</span>\nindex b752734a08..0ce972291e 100644\n<span class=\"pl-md\">--- a/third_party/gpus/cuda/BUILD.tpl</span>\n<span class=\"pl-mi1\">+++ b/third_party/gpus/cuda/BUILD.tpl</span>\n<span class=\"pl-mdr\">@@ -109,7 +109,7 @@</span> cc_library(\n         \".\",\n         \"cuda/include\",\n     ],\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    linkopts = [\"-lgomp\"],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    linkopts = [\"-l:libgomp.so.1\"],</span>\n     linkstatic = 1,\n     visibility = [\"//visibility:public\"],\n )\n<span class=\"pl-c1\">diff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD</span>\nindex 39136de99c..6f697919fd 100644\n<span class=\"pl-md\">--- a/third_party/toolchains/gpus/cuda/BUILD</span>\n<span class=\"pl-mi1\">+++ b/third_party/toolchains/gpus/cuda/BUILD</span>\n<span class=\"pl-mdr\">@@ -114,7 +114,7 @@</span> cc_library(\n         \".\",\n         \"cuda/include\",\n     ],\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    linkopts = [\"-lgomp\"],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    linkopts = [\"-l:libgomp.so.1\"],</span>\n     linkstatic = 1,\n     visibility = [\"//visibility:public\"],\n )</pre></div>\n<p>Building with clang is a lot more difficult, as it'd require making Tensorflow's CUDA symlink repo look enough like the unpacked tarball to pass <a href=\"https://github.com/jyknight/llvm-monorepo/blob/6a6c3cae76a0839429c0b552572c46af9b194b86/clang/lib/Driver/ToolChains/Cuda.cpp\">this detection logic</a>!</p>\n<p>My <code>.tf_configure.bazelrc</code> (for FC 26) looks like:</p>\n<pre><code>build --action_env PYTHON_BIN_PATH=\"/home/nicholas/miniconda3/bin/python\"\nbuild --action_env PYTHON_LIB_PATH=\"/home/nicholas/miniconda3/lib/python3.6/site-packages\"\nbuild --force_python=py3\nbuild --host_force_python=py3\nbuild --python_path=\"/home/nicholas/miniconda3/bin/python\"\nbuild --define with_jemalloc=true\nbuild:gcp --define with_gcp_support=true\nbuild:hdfs --define with_hdfs_support=true\nbuild:s3 --define with_s3_support=true\nbuild:xla --define with_xla_support=true\nbuild:gdr --define with_gdr_support=true\nbuild:verbs --define with_verbs_support=true\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\nbuild --action_env TF_NEED_CUDA=\"1\"\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr\"\nbuild --action_env TF_CUDA_VERSION=\"8.0\"\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr\"\nbuild --action_env TF_CUDNN_VERSION=\"7\"\nbuild --action_env NVVMIR_LIBRARY_DIR=\"/usr/share/cuda\"\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\nbuild --action_env TF_CUDA_CLANG=\"0\"\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc53\"\nbuild --config=cuda\ntest --config=cuda\nbuild --define grpc_no_ares=true\nbuild:opt --copt=-march=native\nbuild:opt --host_copt=-march=native\nbuild:opt --define with_default_optimizations=true\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\nbuild:mkl --define using_mkl=true\nbuild:mkl -c opt\nbuild:monolithic --define framework_shared_object=false\nbuild --define framework_shared_object=true\nbuild:android --crosstool_top=//external:android/crosstool\nbuild:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\nbuild:android_arm --config=android\nbuild:android_arm --cpu=armeabi-v7a\nbuild:android_arm64 --config=android\nbuild:android_arm64 --cpu=arm64-v8a\n</code></pre>", "body_text": "Support the Negativo17 Nvidia driver packaging for Fedora. libdevice libraries are under /usr/share/cuda, includes are under /usr/include/cuda and libraries are under /usr/lib64. This PR should help #8264 too.\nIn addition, the gcc 5.3 in the Negativo17 repository (installed as /usr/bin/gcc53) only has a static non-PIC version of libgomp.a, so I have this local patch to force Tensorflow to link to the global (/usr/lib64) shared version:\ndiff --git a/tensorflow/contrib/cmake/tf_stream_executor.cmake b/tensorflow/contrib/cmake/tf_stream_executor.cmake\nindex 91ca33f4c4..7719ee096d 100644\n--- a/tensorflow/contrib/cmake/tf_stream_executor.cmake\n+++ b/tensorflow/contrib/cmake/tf_stream_executor.cmake\n@@ -75,7 +75,7 @@ endif()\n #list(REMOVE_ITEM tf_stream_executor_srcs ${tf_stream_executor_test_srcs})\n \n if (NOT WIN32)\n-  set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -lgomp\")\n+  set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -l:libgomp.so.1\")\n endif (NOT WIN32)\n add_library(tf_stream_executor OBJECT ${tf_stream_executor_srcs})\n \ndiff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl\nindex b752734a08..0ce972291e 100644\n--- a/third_party/gpus/cuda/BUILD.tpl\n+++ b/third_party/gpus/cuda/BUILD.tpl\n@@ -109,7 +109,7 @@ cc_library(\n         \".\",\n         \"cuda/include\",\n     ],\n-    linkopts = [\"-lgomp\"],\n+    linkopts = [\"-l:libgomp.so.1\"],\n     linkstatic = 1,\n     visibility = [\"//visibility:public\"],\n )\ndiff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD\nindex 39136de99c..6f697919fd 100644\n--- a/third_party/toolchains/gpus/cuda/BUILD\n+++ b/third_party/toolchains/gpus/cuda/BUILD\n@@ -114,7 +114,7 @@ cc_library(\n         \".\",\n         \"cuda/include\",\n     ],\n-    linkopts = [\"-lgomp\"],\n+    linkopts = [\"-l:libgomp.so.1\"],\n     linkstatic = 1,\n     visibility = [\"//visibility:public\"],\n )\nBuilding with clang is a lot more difficult, as it'd require making Tensorflow's CUDA symlink repo look enough like the unpacked tarball to pass this detection logic!\nMy .tf_configure.bazelrc (for FC 26) looks like:\nbuild --action_env PYTHON_BIN_PATH=\"/home/nicholas/miniconda3/bin/python\"\nbuild --action_env PYTHON_LIB_PATH=\"/home/nicholas/miniconda3/lib/python3.6/site-packages\"\nbuild --force_python=py3\nbuild --host_force_python=py3\nbuild --python_path=\"/home/nicholas/miniconda3/bin/python\"\nbuild --define with_jemalloc=true\nbuild:gcp --define with_gcp_support=true\nbuild:hdfs --define with_hdfs_support=true\nbuild:s3 --define with_s3_support=true\nbuild:xla --define with_xla_support=true\nbuild:gdr --define with_gdr_support=true\nbuild:verbs --define with_verbs_support=true\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\nbuild --action_env TF_NEED_CUDA=\"1\"\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr\"\nbuild --action_env TF_CUDA_VERSION=\"8.0\"\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr\"\nbuild --action_env TF_CUDNN_VERSION=\"7\"\nbuild --action_env NVVMIR_LIBRARY_DIR=\"/usr/share/cuda\"\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\nbuild --action_env TF_CUDA_CLANG=\"0\"\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc53\"\nbuild --config=cuda\ntest --config=cuda\nbuild --define grpc_no_ares=true\nbuild:opt --copt=-march=native\nbuild:opt --host_copt=-march=native\nbuild:opt --define with_default_optimizations=true\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\nbuild:mkl --define using_mkl=true\nbuild:mkl -c opt\nbuild:monolithic --define framework_shared_object=false\nbuild --define framework_shared_object=true\nbuild:android --crosstool_top=//external:android/crosstool\nbuild:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\nbuild:android_arm --config=android\nbuild:android_arm --cpu=armeabi-v7a\nbuild:android_arm64 --config=android\nbuild:android_arm64 --cpu=arm64-v8a", "body": "Support the [Negativo17](https://negativo17.org/nvidia-driver/) Nvidia driver packaging for Fedora. `libdevice` libraries are under `/usr/share/cuda`, includes are under `/usr/include/cuda` and libraries are under `/usr/lib64`. This PR should help #8264 too.\r\n\r\nIn addition, the gcc 5.3 in the Negativo17 repository (installed as `/usr/bin/gcc53`) only has a static non-PIC version of `libgomp.a`, so I have this local patch to force Tensorflow to link to the global (`/usr/lib64`) shared version:\r\n\r\n````diff\r\ndiff --git a/tensorflow/contrib/cmake/tf_stream_executor.cmake b/tensorflow/contrib/cmake/tf_stream_executor.cmake\r\nindex 91ca33f4c4..7719ee096d 100644\r\n--- a/tensorflow/contrib/cmake/tf_stream_executor.cmake\r\n+++ b/tensorflow/contrib/cmake/tf_stream_executor.cmake\r\n@@ -75,7 +75,7 @@ endif()\r\n #list(REMOVE_ITEM tf_stream_executor_srcs ${tf_stream_executor_test_srcs})\r\n \r\n if (NOT WIN32)\r\n-  set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -lgomp\")\r\n+  set (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -l:libgomp.so.1\")\r\n endif (NOT WIN32)\r\n add_library(tf_stream_executor OBJECT ${tf_stream_executor_srcs})\r\n \r\ndiff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl\r\nindex b752734a08..0ce972291e 100644\r\n--- a/third_party/gpus/cuda/BUILD.tpl\r\n+++ b/third_party/gpus/cuda/BUILD.tpl\r\n@@ -109,7 +109,7 @@ cc_library(\r\n         \".\",\r\n         \"cuda/include\",\r\n     ],\r\n-    linkopts = [\"-lgomp\"],\r\n+    linkopts = [\"-l:libgomp.so.1\"],\r\n     linkstatic = 1,\r\n     visibility = [\"//visibility:public\"],\r\n )\r\ndiff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD\r\nindex 39136de99c..6f697919fd 100644\r\n--- a/third_party/toolchains/gpus/cuda/BUILD\r\n+++ b/third_party/toolchains/gpus/cuda/BUILD\r\n@@ -114,7 +114,7 @@ cc_library(\r\n         \".\",\r\n         \"cuda/include\",\r\n     ],\r\n-    linkopts = [\"-lgomp\"],\r\n+    linkopts = [\"-l:libgomp.so.1\"],\r\n     linkstatic = 1,\r\n     visibility = [\"//visibility:public\"],\r\n )\r\n````\r\n\r\nBuilding with clang is a lot more difficult, as it'd require making Tensorflow's CUDA symlink repo look enough like the unpacked tarball to pass [this detection logic](https://github.com/jyknight/llvm-monorepo/blob/6a6c3cae76a0839429c0b552572c46af9b194b86/clang/lib/Driver/ToolChains/Cuda.cpp)!\r\n\r\nMy `.tf_configure.bazelrc` (for FC 26) looks like:\r\n\r\n````\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/nicholas/miniconda3/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/nicholas/miniconda3/lib/python3.6/site-packages\"\r\nbuild --force_python=py3\r\nbuild --host_force_python=py3\r\nbuild --python_path=\"/home/nicholas/miniconda3/bin/python\"\r\nbuild --define with_jemalloc=true\r\nbuild:gcp --define with_gcp_support=true\r\nbuild:hdfs --define with_hdfs_support=true\r\nbuild:s3 --define with_s3_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr\"\r\nbuild --action_env TF_CUDA_VERSION=\"8.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env NVVMIR_LIBRARY_DIR=\"/usr/share/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc53\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild:mkl --define using_mkl=true\r\nbuild:mkl -c opt\r\nbuild:monolithic --define framework_shared_object=false\r\nbuild --define framework_shared_object=true\r\nbuild:android --crosstool_top=//external:android/crosstool\r\nbuild:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nbuild:android_arm --config=android\r\nbuild:android_arm --cpu=armeabi-v7a\r\nbuild:android_arm64 --config=android\r\nbuild:android_arm64 --cpu=arm64-v8a\r\n````"}