{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5043", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5043/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5043/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5043/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5043", "id": 183722310, "node_id": "MDU6SXNzdWUxODM3MjIzMTA=", "number": 5043, "title": "Sudden OOM error with Tensorflow embedding_attention_seq2seq after successful runs", "user": {"login": "hanskrupakar", "id": 17511695, "node_id": "MDQ6VXNlcjE3NTExNjk1", "avatar_url": "https://avatars3.githubusercontent.com/u/17511695?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hanskrupakar", "html_url": "https://github.com/hanskrupakar", "followers_url": "https://api.github.com/users/hanskrupakar/followers", "following_url": "https://api.github.com/users/hanskrupakar/following{/other_user}", "gists_url": "https://api.github.com/users/hanskrupakar/gists{/gist_id}", "starred_url": "https://api.github.com/users/hanskrupakar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hanskrupakar/subscriptions", "organizations_url": "https://api.github.com/users/hanskrupakar/orgs", "repos_url": "https://api.github.com/users/hanskrupakar/repos", "events_url": "https://api.github.com/users/hanskrupakar/events{/privacy}", "received_events_url": "https://api.github.com/users/hanskrupakar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-10-18T15:33:29Z", "updated_at": "2017-02-27T22:48:19Z", "closed_at": "2016-11-11T17:45:17Z", "author_association": "NONE", "body_html": "<p>I've already found plenty of issues on the particular topic of memory issues with TensorFlow <a href=\"https://github.com/tensorflow/tensorflow/issues/352\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/352/hovercard\">here</a>, <a href=\"https://github.com/tensorflow/tensorflow/issues/136\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/136/hovercard\">here</a>, <a href=\"https://github.com/tensorflow/tensorflow/issues/138\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/138/hovercard\">here</a>, <a href=\"https://github.com/tensorflow/tensorflow/issues/1355\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1355/hovercard\">here</a> and <a href=\"https://github.com/tensorflow/tensorflow/issues/492\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/492/hovercard\">here</a> and we are still awaiting fixes for the same.</p>\n<p>I have implemented the RNN embedding Encoder Attention Decoder model as described by this <a href=\"https://www.tensorflow.org/versions/r0.11/tutorials/seq2seq/index.html\" rel=\"nofollow\">tutorial</a> using a custom language pair.</p>\n<p>I <strong>successfully implemented</strong> it and the training started after I determined the maximum parameter values of:</p>\n<p>Size of NN: 340<br>\nBatch Size: 32<br>\nTotal Vocabulary:<br>\nEnglish: 50000 words<br>\nTamil: 40000 words</p>\n<p>and trained on it quite a few times.</p>\n<pre><code>Preparing WMT data in /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/\nCreating 3 layers of 325 units.\nReading model parameters from /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/checkpoints/translate.ckpt-9500\nReading development and training data (limit: 0).\n  reading data line 100000\n   Word2Vec English Encoding Successful!\n   Word2Vec Tamil Encoding Successful!\nglobal step 10000 learning rate 0.5000 step-time 1.56 perplexity 33.47\n  eval: bucket 0 perplexity 10.47\n  eval: bucket 1 perplexity 14.57\n  eval: bucket 2 perplexity 20.74\n  eval: bucket 3 perplexity 35.82\n  eval: bucket 4 perplexity 42.77\n\n.......................................\n.......................................\n.......................................\n\nglobal step 109000 learning rate 0.4522 step-time 1.04 perplexity 3.79\n  eval: bucket 0 perplexity 20.10\n  eval: bucket 1 perplexity 18.52\n  eval: bucket 2 perplexity 17.32\n  eval: bucket 3 perplexity 28.79\n  eval: bucket 4 perplexity 18.48\n  eval: bucket 5 perplexity 17.07\n  eval: bucket 6 perplexity 15.35\n  eval: bucket 7 perplexity 29.22\n  eval: bucket 8 perplexity 26.05\n  eval: bucket 9 perplexity 28.58\n\n</code></pre>\n<p>I'm using Ubuntu 16.04 LTS with a 2GB Nvidia 650M GeForce card with:</p>\n<pre><code>nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\n</code></pre>\n<pre><code>root@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls\nlibcudnn.so  libcudnn.so.4  libcudnn.so.4.0.7  libcudnn_static.a\n</code></pre>\n<pre><code>root@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# pip list | grep tensorflow\ntensorflow (0.8.0)\n\n</code></pre>\n<pre><code>root@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls -l /usr/lib/x86_64-linux-gnu/libcud*\n-rw-r--r-- 1 root root   322936 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so -&gt; libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5 -&gt; libcudart.so.7.5.18\n-rw-r--r-- 1 root root   383336 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart_static.a\nlrwxrwxrwx 1 root root       12 Apr 14  2016 /usr/lib/x86_64-linux-gnu/libcuda.so -&gt; libcuda.so.1\nlrwxrwxrwx 1 root root       17 Aug 14 16:25 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -&gt; libcuda.so.367.35\n-rw-r--r-- 1 root root 16881416 Mar 23  2016 /usr/lib/x86_64-linux-gnu/libcuda.so.361.42\n-rwxr-xr-x 1 root root  8121032 Aug 14 14:14 /usr/lib/x86_64-linux-gnu/libcuda.so.367.35\nlrwxrwxrwx 1 root root       13 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so -&gt; libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4 -&gt; libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4.0.7\nlrwxrwxrwx 1 root root       17 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -&gt; libcudnn.so.5.1.3\n-rwxr-xr-x 1 root root 60696704 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 62025862 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn_static.a\n\n</code></pre>\n<p>Since then, I required the cuDNN package for other purposes and installed cuDNN 5 first. found out that Tensorflow 0.8 works with cudnn 4 only and reinstalled the version 4 and the model testing worked correctly. But when I tried to run the model after the cudnn install:</p>\n<pre><code>\n................................\n................................\n...............................\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1640704 totalling 1.56MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1660928 totalling 1.58MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1676544 totalling 1.60MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 235 Chunks of size 1849600 totalling 414.52MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 1893120 totalling 3.61MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 1936640 totalling 5.54MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2023680 totalling 1.93MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2041088 totalling 1.95MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2067200 totalling 1.97MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2151936 totalling 2.05MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2154240 totalling 2.05MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2174208 totalling 2.07MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2328320 totalling 2.22MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2543872 totalling 2.43MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2590208 totalling 2.47MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2610176 totalling 2.49MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 2774272 totalling 7.94MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2861312 totalling 2.73MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2974976 totalling 2.84MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2994176 totalling 2.86MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3042816 totalling 2.90MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3445760 totalling 3.29MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 120 Chunks of size 3699200 totalling 423.34MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 3939072 totalling 7.51MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4348416 totalling 4.15MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4418816 totalling 4.21MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4980736 totalling 4.75MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5232128 totalling 4.99MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5960960 totalling 5.68MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54400000 totalling 51.88MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54545920 totalling 52.02MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] Sum Total of in-use chunks: 1.40GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:694] Stats: \nLimit:                  1534525440\nInUse:                  1506072064\nMaxInUse:               1506336256\nNumAllocs:                  286635\nMaxAllocSize:             92850432\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ****************************************************************************************************\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 903.1KiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:900] Resource exhausted: OOM when allocating tensor with shape[680,340]\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=9028 evicted_count=9000 eviction_rate=0.996899 and unsatisfied allocation rate=0\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 350, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 347, in main\n    train()\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 186, in train\n    target_weights, bucket_id, False)\n  File \"src/seq2seq_model.py\", line 205, in step\n    outputs = session.run(output_feed, input_feed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[680,680]\n     [[Node: gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/concat, gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/add_grad/Reshape)]]\n     [[Node: clip_by_global_norm_9/clip_by_global_norm_9/_2/_7992 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_624162_clip_by_global_norm_9/clip_by_global_norm_9/_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 350, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 347, in main\n    train()\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 122, in train\n    model = create_model(sess, False)\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 98, in create_model\n    forward_only=forward_only)\n  File \"src/seq2seq_model.py\", line 142, in __init__\n    gradients = tf.gradients(self.losses[b], params)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 511, in _MatMulGrad\n    math_ops.matmul(op.inputs[0], grad, transpose_a=True))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1036, in matmul\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 911, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n[elided 5 identical lines from previous traceback]\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 98, in create_model\n    forward_only=forward_only)\n  File \"src/seq2seq_model.py\", line 133, in __init__\n    softmax_loss_function=softmax_loss_function)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 961, in model_with_buckets\n    decoder_inputs[:bucket[1]])\n  File \"src/seq2seq_model.py\", line 132, in &lt;lambda&gt;\n    lambda x, y: seq2seq_f(x, y, False),\n  File \"src/seq2seq_model.py\", line 96, in seq2seq_f\n    feed_previous=do_decode)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 718, in embedding_attention_seq2seq\n    initial_state_attention=initial_state_attention)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 643, in embedding_attention_decoder\n    initial_state_attention=initial_state_attention)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 556, in attention_decoder\n    cell_output, state = cell(x, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell.py\", line 659, in __call__\n    cur_inp, new_state = cell(cur_inp, cur_state)\n\nhans@hans-Lenovo-IdeaPad-Y500:~/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec$ \n\n</code></pre>\n<p>The pool allocator for my GPU is only going up to 300 from 100 when before, it went up to somewhere north of 6500. I don't know if this happened because of cudnn but this started right after I installed it. Please help.</p>", "body_text": "I've already found plenty of issues on the particular topic of memory issues with TensorFlow here, here, here, here and here and we are still awaiting fixes for the same.\nI have implemented the RNN embedding Encoder Attention Decoder model as described by this tutorial using a custom language pair.\nI successfully implemented it and the training started after I determined the maximum parameter values of:\nSize of NN: 340\nBatch Size: 32\nTotal Vocabulary:\nEnglish: 50000 words\nTamil: 40000 words\nand trained on it quite a few times.\nPreparing WMT data in /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/\nCreating 3 layers of 325 units.\nReading model parameters from /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/checkpoints/translate.ckpt-9500\nReading development and training data (limit: 0).\n  reading data line 100000\n   Word2Vec English Encoding Successful!\n   Word2Vec Tamil Encoding Successful!\nglobal step 10000 learning rate 0.5000 step-time 1.56 perplexity 33.47\n  eval: bucket 0 perplexity 10.47\n  eval: bucket 1 perplexity 14.57\n  eval: bucket 2 perplexity 20.74\n  eval: bucket 3 perplexity 35.82\n  eval: bucket 4 perplexity 42.77\n\n.......................................\n.......................................\n.......................................\n\nglobal step 109000 learning rate 0.4522 step-time 1.04 perplexity 3.79\n  eval: bucket 0 perplexity 20.10\n  eval: bucket 1 perplexity 18.52\n  eval: bucket 2 perplexity 17.32\n  eval: bucket 3 perplexity 28.79\n  eval: bucket 4 perplexity 18.48\n  eval: bucket 5 perplexity 17.07\n  eval: bucket 6 perplexity 15.35\n  eval: bucket 7 perplexity 29.22\n  eval: bucket 8 perplexity 26.05\n  eval: bucket 9 perplexity 28.58\n\n\nI'm using Ubuntu 16.04 LTS with a 2GB Nvidia 650M GeForce card with:\nnvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\n\nroot@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls\nlibcudnn.so  libcudnn.so.4  libcudnn.so.4.0.7  libcudnn_static.a\n\nroot@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# pip list | grep tensorflow\ntensorflow (0.8.0)\n\n\nroot@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls -l /usr/lib/x86_64-linux-gnu/libcud*\n-rw-r--r-- 1 root root   322936 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rw-r--r-- 1 root root   383336 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart_static.a\nlrwxrwxrwx 1 root root       12 Apr 14  2016 /usr/lib/x86_64-linux-gnu/libcuda.so -> libcuda.so.1\nlrwxrwxrwx 1 root root       17 Aug 14 16:25 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -> libcuda.so.367.35\n-rw-r--r-- 1 root root 16881416 Mar 23  2016 /usr/lib/x86_64-linux-gnu/libcuda.so.361.42\n-rwxr-xr-x 1 root root  8121032 Aug 14 14:14 /usr/lib/x86_64-linux-gnu/libcuda.so.367.35\nlrwxrwxrwx 1 root root       13 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4.0.7\nlrwxrwxrwx 1 root root       17 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -> libcudnn.so.5.1.3\n-rwxr-xr-x 1 root root 60696704 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 62025862 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn_static.a\n\n\nSince then, I required the cuDNN package for other purposes and installed cuDNN 5 first. found out that Tensorflow 0.8 works with cudnn 4 only and reinstalled the version 4 and the model testing worked correctly. But when I tried to run the model after the cudnn install:\n\n................................\n................................\n...............................\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1640704 totalling 1.56MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1660928 totalling 1.58MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1676544 totalling 1.60MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 235 Chunks of size 1849600 totalling 414.52MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 1893120 totalling 3.61MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 1936640 totalling 5.54MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2023680 totalling 1.93MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2041088 totalling 1.95MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2067200 totalling 1.97MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2151936 totalling 2.05MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2154240 totalling 2.05MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2174208 totalling 2.07MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2328320 totalling 2.22MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2543872 totalling 2.43MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2590208 totalling 2.47MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2610176 totalling 2.49MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 2774272 totalling 7.94MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2861312 totalling 2.73MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2974976 totalling 2.84MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2994176 totalling 2.86MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3042816 totalling 2.90MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3445760 totalling 3.29MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 120 Chunks of size 3699200 totalling 423.34MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 3939072 totalling 7.51MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4348416 totalling 4.15MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4418816 totalling 4.21MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4980736 totalling 4.75MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5232128 totalling 4.99MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5960960 totalling 5.68MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54400000 totalling 51.88MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54545920 totalling 52.02MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] Sum Total of in-use chunks: 1.40GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:694] Stats: \nLimit:                  1534525440\nInUse:                  1506072064\nMaxInUse:               1506336256\nNumAllocs:                  286635\nMaxAllocSize:             92850432\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ****************************************************************************************************\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 903.1KiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:900] Resource exhausted: OOM when allocating tensor with shape[680,340]\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=9028 evicted_count=9000 eviction_rate=0.996899 and unsatisfied allocation rate=0\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 350, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 347, in main\n    train()\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 186, in train\n    target_weights, bucket_id, False)\n  File \"src/seq2seq_model.py\", line 205, in step\n    outputs = session.run(output_feed, input_feed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[680,680]\n     [[Node: gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/concat, gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/add_grad/Reshape)]]\n     [[Node: clip_by_global_norm_9/clip_by_global_norm_9/_2/_7992 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_624162_clip_by_global_norm_9/clip_by_global_norm_9/_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 350, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 347, in main\n    train()\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 122, in train\n    model = create_model(sess, False)\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 98, in create_model\n    forward_only=forward_only)\n  File \"src/seq2seq_model.py\", line 142, in __init__\n    gradients = tf.gradients(self.losses[b], params)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 511, in _MatMulGrad\n    math_ops.matmul(op.inputs[0], grad, transpose_a=True))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1036, in matmul\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 911, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n[elided 5 identical lines from previous traceback]\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 98, in create_model\n    forward_only=forward_only)\n  File \"src/seq2seq_model.py\", line 133, in __init__\n    softmax_loss_function=softmax_loss_function)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 961, in model_with_buckets\n    decoder_inputs[:bucket[1]])\n  File \"src/seq2seq_model.py\", line 132, in <lambda>\n    lambda x, y: seq2seq_f(x, y, False),\n  File \"src/seq2seq_model.py\", line 96, in seq2seq_f\n    feed_previous=do_decode)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 718, in embedding_attention_seq2seq\n    initial_state_attention=initial_state_attention)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 643, in embedding_attention_decoder\n    initial_state_attention=initial_state_attention)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 556, in attention_decoder\n    cell_output, state = cell(x, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell.py\", line 659, in __call__\n    cur_inp, new_state = cell(cur_inp, cur_state)\n\nhans@hans-Lenovo-IdeaPad-Y500:~/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec$ \n\n\nThe pool allocator for my GPU is only going up to 300 from 100 when before, it went up to somewhere north of 6500. I don't know if this happened because of cudnn but this started right after I installed it. Please help.", "body": "I've already found plenty of issues on the particular topic of memory issues with TensorFlow [here](https://github.com/tensorflow/tensorflow/issues/352), [here](https://github.com/tensorflow/tensorflow/issues/136), [here](https://github.com/tensorflow/tensorflow/issues/138), [here](https://github.com/tensorflow/tensorflow/issues/1355) and [here](https://github.com/tensorflow/tensorflow/issues/492) and we are still awaiting fixes for the same.\n\nI have implemented the RNN embedding Encoder Attention Decoder model as described by this [tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/seq2seq/index.html) using a custom language pair.\n\nI **successfully implemented** it and the training started after I determined the maximum parameter values of:\n\nSize of NN: 340\nBatch Size: 32\nTotal Vocabulary:\nEnglish: 50000 words\nTamil: 40000 words\n\nand trained on it quite a few times.\n\n```\nPreparing WMT data in /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/\nCreating 3 layers of 325 units.\nReading model parameters from /home/hans/Documents/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/checkpoints/translate.ckpt-9500\nReading development and training data (limit: 0).\n  reading data line 100000\n   Word2Vec English Encoding Successful!\n   Word2Vec Tamil Encoding Successful!\nglobal step 10000 learning rate 0.5000 step-time 1.56 perplexity 33.47\n  eval: bucket 0 perplexity 10.47\n  eval: bucket 1 perplexity 14.57\n  eval: bucket 2 perplexity 20.74\n  eval: bucket 3 perplexity 35.82\n  eval: bucket 4 perplexity 42.77\n\n.......................................\n.......................................\n.......................................\n\nglobal step 109000 learning rate 0.4522 step-time 1.04 perplexity 3.79\n  eval: bucket 0 perplexity 20.10\n  eval: bucket 1 perplexity 18.52\n  eval: bucket 2 perplexity 17.32\n  eval: bucket 3 perplexity 28.79\n  eval: bucket 4 perplexity 18.48\n  eval: bucket 5 perplexity 17.07\n  eval: bucket 6 perplexity 15.35\n  eval: bucket 7 perplexity 29.22\n  eval: bucket 8 perplexity 26.05\n  eval: bucket 9 perplexity 28.58\n\n```\n\nI'm using Ubuntu 16.04 LTS with a 2GB Nvidia 650M GeForce card with:\n\n```\nnvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\n```\n\n```\nroot@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls\nlibcudnn.so  libcudnn.so.4  libcudnn.so.4.0.7  libcudnn_static.a\n```\n\n```\nroot@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# pip list | grep tensorflow\ntensorflow (0.8.0)\n\n```\n\n```\nroot@hans-Lenovo-IdeaPad-Y500:/opt/cuda/lib64# ls -l /usr/lib/x86_64-linux-gnu/libcud*\n-rw-r--r-- 1 root root   322936 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Mar 30  2016 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rw-r--r-- 1 root root   383336 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Sep 19  2015 /usr/lib/x86_64-linux-gnu/libcudart_static.a\nlrwxrwxrwx 1 root root       12 Apr 14  2016 /usr/lib/x86_64-linux-gnu/libcuda.so -> libcuda.so.1\nlrwxrwxrwx 1 root root       17 Aug 14 16:25 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -> libcuda.so.367.35\n-rw-r--r-- 1 root root 16881416 Mar 23  2016 /usr/lib/x86_64-linux-gnu/libcuda.so.361.42\n-rwxr-xr-x 1 root root  8121032 Aug 14 14:14 /usr/lib/x86_64-linux-gnu/libcuda.so.367.35\nlrwxrwxrwx 1 root root       13 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn.so.4.0.7\nlrwxrwxrwx 1 root root       17 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -> libcudnn.so.5.1.3\n-rwxr-xr-x 1 root root 60696704 Oct  1 05:43 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 62025862 Oct  1 05:55 /usr/lib/x86_64-linux-gnu/libcudnn_static.a\n\n```\n\nSince then, I required the cuDNN package for other purposes and installed cuDNN 5 first. found out that Tensorflow 0.8 works with cudnn 4 only and reinstalled the version 4 and the model testing worked correctly. But when I tried to run the model after the cudnn install:\n\n```\n\n................................\n................................\n...............................\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1640704 totalling 1.56MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1660928 totalling 1.58MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1676544 totalling 1.60MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 235 Chunks of size 1849600 totalling 414.52MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 1893120 totalling 3.61MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 1936640 totalling 5.54MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2023680 totalling 1.93MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2041088 totalling 1.95MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2067200 totalling 1.97MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2151936 totalling 2.05MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2154240 totalling 2.05MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2174208 totalling 2.07MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2328320 totalling 2.22MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2543872 totalling 2.43MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2590208 totalling 2.47MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2610176 totalling 2.49MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 2774272 totalling 7.94MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2861312 totalling 2.73MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2974976 totalling 2.84MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 2994176 totalling 2.86MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3042816 totalling 2.90MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 3445760 totalling 3.29MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 120 Chunks of size 3699200 totalling 423.34MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 2 Chunks of size 3939072 totalling 7.51MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4348416 totalling 4.15MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4418816 totalling 4.21MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 4980736 totalling 4.75MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5232128 totalling 4.99MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 5960960 totalling 5.68MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54400000 totalling 51.88MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 54545920 totalling 52.02MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] Sum Total of in-use chunks: 1.40GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:694] Stats: \nLimit:                  1534525440\nInUse:                  1506072064\nMaxInUse:               1506336256\nNumAllocs:                  286635\nMaxAllocSize:             92850432\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ****************************************************************************************************\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 903.1KiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:900] Resource exhausted: OOM when allocating tensor with shape[680,340]\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=9028 evicted_count=9000 eviction_rate=0.996899 and unsatisfied allocation rate=0\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 350, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 347, in main\n    train()\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 186, in train\n    target_weights, bucket_id, False)\n  File \"src/seq2seq_model.py\", line 205, in step\n    outputs = session.run(output_feed, input_feed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[680,680]\n     [[Node: gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/concat, gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/add_grad/Reshape)]]\n     [[Node: clip_by_global_norm_9/clip_by_global_norm_9/_2/_7992 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_624162_clip_by_global_norm_9/clip_by_global_norm_9/_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'gradients_9/model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul_grad/MatMul_1', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 350, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 347, in main\n    train()\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 122, in train\n    model = create_model(sess, False)\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 98, in create_model\n    forward_only=forward_only)\n  File \"src/seq2seq_model.py\", line 142, in __init__\n    gradients = tf.gradients(self.losses[b], params)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 511, in _MatMulGrad\n    math_ops.matmul(op.inputs[0], grad, transpose_a=True))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1036, in matmul\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 911, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'model_with_buckets/embedding_attention_seq2seq_9/embedding_attention_decoder/attention_decoder/MultiRNNCell_34/Cell0/GRUCell/Gates/Linear/MatMul', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n[elided 5 identical lines from previous traceback]\n  File \"/home/hans/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec/src/translate.py\", line 98, in create_model\n    forward_only=forward_only)\n  File \"src/seq2seq_model.py\", line 133, in __init__\n    softmax_loss_function=softmax_loss_function)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 961, in model_with_buckets\n    decoder_inputs[:bucket[1]])\n  File \"src/seq2seq_model.py\", line 132, in <lambda>\n    lambda x, y: seq2seq_f(x, y, False),\n  File \"src/seq2seq_model.py\", line 96, in seq2seq_f\n    feed_previous=do_decode)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 718, in embedding_attention_seq2seq\n    initial_state_attention=initial_state_attention)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 643, in embedding_attention_decoder\n    initial_state_attention=initial_state_attention)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 556, in attention_decoder\n    cell_output, state = cell(x, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell.py\", line 659, in __call__\n    cur_inp, new_state = cell(cur_inp, cur_state)\n\nhans@hans-Lenovo-IdeaPad-Y500:~/Documents/HANS/MAC/SUCCESSFUL MODELS/ADD/Tensorflow with Eng and Tam Word2Vec$ \n\n```\n\nThe pool allocator for my GPU is only going up to 300 from 100 when before, it went up to somewhere north of 6500. I don't know if this happened because of cudnn but this started right after I installed it. Please help.\n"}