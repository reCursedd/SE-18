{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/398735973", "html_url": "https://github.com/tensorflow/tensorflow/issues/15564#issuecomment-398735973", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15564", "id": 398735973, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODczNTk3Mw==", "user": {"login": "clarken92", "id": 1445226, "node_id": "MDQ6VXNlcjE0NDUyMjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1445226?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clarken92", "html_url": "https://github.com/clarken92", "followers_url": "https://api.github.com/users/clarken92/followers", "following_url": "https://api.github.com/users/clarken92/following{/other_user}", "gists_url": "https://api.github.com/users/clarken92/gists{/gist_id}", "starred_url": "https://api.github.com/users/clarken92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clarken92/subscriptions", "organizations_url": "https://api.github.com/users/clarken92/orgs", "repos_url": "https://api.github.com/users/clarken92/repos", "events_url": "https://api.github.com/users/clarken92/events{/privacy}", "received_events_url": "https://api.github.com/users/clarken92/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-20T12:46:23Z", "updated_at": "2018-06-20T12:46:23Z", "author_association": "NONE", "body_html": "<p>Hi guys,</p>\n<p>I think I have solved the problem of sparsemax. The main problem is that the code for normalizing the logits in sparsemax() function substracts the <strong>mean</strong> instead of <strong>max</strong>. mean is very sensitive to noise (very positive or very negative numbers) since it includes all elements. For example, if we have 3 elements, one of them is -1e30 while the others 2 are 0.1 and 0.2, subtracting the mean will result in all three values around 1e29, causing a precision problem and other problem like division of a very large number by a very large number. Therefore, I replaced<br>\n<code>z = logits - math_ops.reduce_mean(logits, axis=1)[:, array_ops.newaxis]</code><br>\nwith<br>\n<code>z = logits - tf.reduce_max(logits, axis=1, keep_dims=True)</code><br>\nand the code works well. This is similar to the trick for robust computation in softmax.</p>\n<p>Another problem I discovered is that the maximum range that we can detect small change is about [-1e6, 1e6]. So if you have 2 values like 1e7 and 1e7 + 0.5, they will be treated equally. I show this phenomenon in my unit test. To prevent this, it added in the code of sparsemax() a control_dependency that checks whether there is at least one value within this range or not. If not, we should not compute sparsemax since it will cause a precision error later.</p>\n<p>I attatch here my code and my unit tests. Could you please check whether it is OK or not. Thank you.<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/2119313/my_sparsemax.zip\">my_sparsemax.zip</a></p>", "body_text": "Hi guys,\nI think I have solved the problem of sparsemax. The main problem is that the code for normalizing the logits in sparsemax() function substracts the mean instead of max. mean is very sensitive to noise (very positive or very negative numbers) since it includes all elements. For example, if we have 3 elements, one of them is -1e30 while the others 2 are 0.1 and 0.2, subtracting the mean will result in all three values around 1e29, causing a precision problem and other problem like division of a very large number by a very large number. Therefore, I replaced\nz = logits - math_ops.reduce_mean(logits, axis=1)[:, array_ops.newaxis]\nwith\nz = logits - tf.reduce_max(logits, axis=1, keep_dims=True)\nand the code works well. This is similar to the trick for robust computation in softmax.\nAnother problem I discovered is that the maximum range that we can detect small change is about [-1e6, 1e6]. So if you have 2 values like 1e7 and 1e7 + 0.5, they will be treated equally. I show this phenomenon in my unit test. To prevent this, it added in the code of sparsemax() a control_dependency that checks whether there is at least one value within this range or not. If not, we should not compute sparsemax since it will cause a precision error later.\nI attatch here my code and my unit tests. Could you please check whether it is OK or not. Thank you.\nmy_sparsemax.zip", "body": "Hi guys,\r\n\r\nI think I have solved the problem of sparsemax. The main problem is that the code for normalizing the logits in sparsemax() function substracts the **mean** instead of **max**. mean is very sensitive to noise (very positive or very negative numbers) since it includes all elements. For example, if we have 3 elements, one of them is -1e30 while the others 2 are 0.1 and 0.2, subtracting the mean will result in all three values around 1e29, causing a precision problem and other problem like division of a very large number by a very large number. Therefore, I replaced \r\n`z = logits - math_ops.reduce_mean(logits, axis=1)[:, array_ops.newaxis]`\r\nwith \r\n`z = logits - tf.reduce_max(logits, axis=1, keep_dims=True)` \r\nand the code works well. This is similar to the trick for robust computation in softmax.\r\n\r\nAnother problem I discovered is that the maximum range that we can detect small change is about [-1e6, 1e6]. So if you have 2 values like 1e7 and 1e7 + 0.5, they will be treated equally. I show this phenomenon in my unit test. To prevent this, it added in the code of sparsemax() a control_dependency that checks whether there is at least one value within this range or not. If not, we should not compute sparsemax since it will cause a precision error later.\r\n\r\nI attatch here my code and my unit tests. Could you please check whether it is OK or not. Thank you.\r\n[my_sparsemax.zip](https://github.com/tensorflow/tensorflow/files/2119313/my_sparsemax.zip)\r\n\r\n  "}