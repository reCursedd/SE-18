{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/246688727", "html_url": "https://github.com/tensorflow/tensorflow/issues/4187#issuecomment-246688727", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4187", "id": 246688727, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NjY4ODcyNw==", "user": {"login": "trevorwelch", "id": 1761458, "node_id": "MDQ6VXNlcjE3NjE0NTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1761458?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trevorwelch", "html_url": "https://github.com/trevorwelch", "followers_url": "https://api.github.com/users/trevorwelch/followers", "following_url": "https://api.github.com/users/trevorwelch/following{/other_user}", "gists_url": "https://api.github.com/users/trevorwelch/gists{/gist_id}", "starred_url": "https://api.github.com/users/trevorwelch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trevorwelch/subscriptions", "organizations_url": "https://api.github.com/users/trevorwelch/orgs", "repos_url": "https://api.github.com/users/trevorwelch/repos", "events_url": "https://api.github.com/users/trevorwelch/events{/privacy}", "received_events_url": "https://api.github.com/users/trevorwelch/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-13T13:54:46Z", "updated_at": "2016-09-13T13:55:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5283042\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/davidzchen\">@davidzchen</a></p>\n<blockquote>\n<p>When you ran your <code>configure</code> script, did you explicitly provide a CUDA version?</p>\n</blockquote>\n<p>Yes, I've tried both ways (system default, or explicitly pointing).</p>\n<p>I imagine that in future versions of bazel I won't have this issue, but bazel seems to just not work in general on my particular system configuration as it relates to cuda. For example, on tensorflow/magenta, all of my bazel tests fail, but magenta runs fine on the GPU.</p>\n<pre><code>$ bazel run //magenta/models/lookback_rnn:lookback_rnn_generate -- \\\n&gt; --run_dir=/tmp/lookback_rnn/logdir/run1 \\\n&gt; --hparams=\"{'batch_size':64,'rnn_layer_sizes':[64,64]}\" \\\n&gt; --output_dir=/tmp/lookback_rnn/generated \\\n&gt; --num_outputs=10 \\\n&gt; --num_steps=128 \\\n&gt; --primer_melody=\"[60]\"\nINFO: Found 1 target...\nTarget //magenta/models/lookback_rnn:lookback_rnn_generate up-to-date:\n  bazel-bin/magenta/models/lookback_rnn/lookback_rnn_generate\nINFO: Elapsed time: 0.460s, Critical Path: 0.00s\n\nINFO: Running command line: bazel-bin/magenta/models/lookback_rnn/lookback_rnn_generate '--run_dir=/tmp/lookback_rnn/logdir/run1' '--hparams={'\\''batch_size'\\'':64,'\\''rnn_layer_sizes'\\'':[64,64]}' '--output_dir=/tmp/lookback_rnn/generated' '--num_outputs=10' '--num_steps=128' '--primer_melody=[60]'\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\nINFO:tensorflow:hparams = {'rnn_layer_sizes': [64, 64], 'temperature': 1.0, 'decay_rate': 0.95, 'dropout_keep_prob': 1.0, 'batch_size': 1, 'decay_steps': 1000, 'clip_norm': 5, 'initial_learning_rate': 0.01, 'skip_first_n_losses': 0}\nWARNING:tensorflow:&lt;tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x1274a7a50&gt;: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:&lt;tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x127126b50&gt;: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] OS X does not support NUMA - returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GT 650M\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9\npciBusID 0000:01:00.0\nTotal memory: 1023.69MiB\nFree memory: 151.57MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)\nINFO:tensorflow:Checkpoint used: /tmp/lookback_rnn/logdir/run1/train/model.ckpt-111\nINFO:tensorflow:Wrote 10 MIDI files to /tmp/lookback_rnn/generated\n\n</code></pre>\n<p>Anyway, in the meantime I have TF running on the GPU, and I don't need to build from source for any particular project at the moment.</p>", "body_text": "@davidzchen\n\nWhen you ran your configure script, did you explicitly provide a CUDA version?\n\nYes, I've tried both ways (system default, or explicitly pointing).\nI imagine that in future versions of bazel I won't have this issue, but bazel seems to just not work in general on my particular system configuration as it relates to cuda. For example, on tensorflow/magenta, all of my bazel tests fail, but magenta runs fine on the GPU.\n$ bazel run //magenta/models/lookback_rnn:lookback_rnn_generate -- \\\n> --run_dir=/tmp/lookback_rnn/logdir/run1 \\\n> --hparams=\"{'batch_size':64,'rnn_layer_sizes':[64,64]}\" \\\n> --output_dir=/tmp/lookback_rnn/generated \\\n> --num_outputs=10 \\\n> --num_steps=128 \\\n> --primer_melody=\"[60]\"\nINFO: Found 1 target...\nTarget //magenta/models/lookback_rnn:lookback_rnn_generate up-to-date:\n  bazel-bin/magenta/models/lookback_rnn/lookback_rnn_generate\nINFO: Elapsed time: 0.460s, Critical Path: 0.00s\n\nINFO: Running command line: bazel-bin/magenta/models/lookback_rnn/lookback_rnn_generate '--run_dir=/tmp/lookback_rnn/logdir/run1' '--hparams={'\\''batch_size'\\'':64,'\\''rnn_layer_sizes'\\'':[64,64]}' '--output_dir=/tmp/lookback_rnn/generated' '--num_outputs=10' '--num_steps=128' '--primer_melody=[60]'\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\nINFO:tensorflow:hparams = {'rnn_layer_sizes': [64, 64], 'temperature': 1.0, 'decay_rate': 0.95, 'dropout_keep_prob': 1.0, 'batch_size': 1, 'decay_steps': 1000, 'clip_norm': 5, 'initial_learning_rate': 0.01, 'skip_first_n_losses': 0}\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x1274a7a50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x127126b50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] OS X does not support NUMA - returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GT 650M\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9\npciBusID 0000:01:00.0\nTotal memory: 1023.69MiB\nFree memory: 151.57MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)\nINFO:tensorflow:Checkpoint used: /tmp/lookback_rnn/logdir/run1/train/model.ckpt-111\nINFO:tensorflow:Wrote 10 MIDI files to /tmp/lookback_rnn/generated\n\n\nAnyway, in the meantime I have TF running on the GPU, and I don't need to build from source for any particular project at the moment.", "body": "@davidzchen \n\n> When you ran your `configure` script, did you explicitly provide a CUDA version?\n\nYes, I've tried both ways (system default, or explicitly pointing). \n\nI imagine that in future versions of bazel I won't have this issue, but bazel seems to just not work in general on my particular system configuration as it relates to cuda. For example, on tensorflow/magenta, all of my bazel tests fail, but magenta runs fine on the GPU.\n\n```\n$ bazel run //magenta/models/lookback_rnn:lookback_rnn_generate -- \\\n> --run_dir=/tmp/lookback_rnn/logdir/run1 \\\n> --hparams=\"{'batch_size':64,'rnn_layer_sizes':[64,64]}\" \\\n> --output_dir=/tmp/lookback_rnn/generated \\\n> --num_outputs=10 \\\n> --num_steps=128 \\\n> --primer_melody=\"[60]\"\nINFO: Found 1 target...\nTarget //magenta/models/lookback_rnn:lookback_rnn_generate up-to-date:\n  bazel-bin/magenta/models/lookback_rnn/lookback_rnn_generate\nINFO: Elapsed time: 0.460s, Critical Path: 0.00s\n\nINFO: Running command line: bazel-bin/magenta/models/lookback_rnn/lookback_rnn_generate '--run_dir=/tmp/lookback_rnn/logdir/run1' '--hparams={'\\''batch_size'\\'':64,'\\''rnn_layer_sizes'\\'':[64,64]}' '--output_dir=/tmp/lookback_rnn/generated' '--num_outputs=10' '--num_steps=128' '--primer_melody=[60]'\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\nINFO:tensorflow:hparams = {'rnn_layer_sizes': [64, 64], 'temperature': 1.0, 'decay_rate': 0.95, 'dropout_keep_prob': 1.0, 'batch_size': 1, 'decay_steps': 1000, 'clip_norm': 5, 'initial_learning_rate': 0.01, 'skip_first_n_losses': 0}\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x1274a7a50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x127126b50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] OS X does not support NUMA - returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GT 650M\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9\npciBusID 0000:01:00.0\nTotal memory: 1023.69MiB\nFree memory: 151.57MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 650M, pci bus id: 0000:01:00.0)\nINFO:tensorflow:Checkpoint used: /tmp/lookback_rnn/logdir/run1/train/model.ckpt-111\nINFO:tensorflow:Wrote 10 MIDI files to /tmp/lookback_rnn/generated\n\n```\n\nAnyway, in the meantime I have TF running on the GPU, and I don't need to build from source for any particular project at the moment. \n"}