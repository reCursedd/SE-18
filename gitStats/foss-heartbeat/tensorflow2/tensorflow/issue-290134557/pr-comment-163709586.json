{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163709586", "pull_request_review_id": 91367097, "id": 163709586, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzcwOTU4Ng==", "diff_hunk": "@@ -0,0 +1,183 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include \"tensorflow/contrib/tensorrt/kernels/trt_engine_op.h\"\n+#include <cuda_runtime_api.h>\n+#include <sstream>\n+#include \"tensorflow/contrib/tensorrt/log/trt_logger.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+// Use TF logging f\n+\n+\n+namespace tensorflow {\n+static ::tensorflow::tensorrt::Logger gLogger;\n+\n+using namespace nvinfer1;\n+\n+namespace tensorrt {\n+\n+TRTEngineOp::TRTEngineOp(OpKernelConstruction* context) : OpKernel(context) {\n+  // char *gieModelStream{nullptr};\n+  // size_t size{0};\n+\n+  // read serialized_engine\n+  std::string serialized_engine;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"serialized_engine\", &serialized_engine));\n+\n+  // register input output node name in trt_sub_graph\n+  OP_REQUIRES_OK(context, context->GetAttr(\"input_nodes\", &input_nodes_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"output_nodes\", &output_nodes_));\n+\n+  // TODO(samikama) runtime should be taken from a resourcemanager as well.\n+  //  Only engine should be in the op and context and runtime should be taken\n+  //  from resourcemanager\n+  IRuntime* infer = createInferRuntime(gLogger);\n+  trt_engine_ptr_.reset(infer->deserializeCudaEngine(\n+      serialized_engine.c_str(), serialized_engine.size(), nullptr));\n+\n+  trt_context_ptr_.reset(trt_engine_ptr_->createExecutionContext());\n+  // runtime is safe to delete after engine creation\n+  infer->destroy();\n+  std::stringstream oss;\n+  // debug iterate through all binding instances\n+  for (int i = 0; i < trt_engine_ptr_->getNbBindings(); i++) {\n+    LOG(INFO) << \"index: \" << i\n+              << \", binding name: \" << trt_engine_ptr_->getBindingName(i);\n+\n+    if (trt_engine_ptr_->bindingIsInput(i)) {\n+      LOG(INFO) << \"INPUT\";\n+    } else {\n+      LOG(INFO) << \"OUTPUT\";\n+    }\n+    oss << \"Dimension: \";\n+    auto dims = trt_engine_ptr_->getBindingDimensions(i);\n+    oss << \" nbDims: \" << dims.nbDims << \" -> \";\n+    for (int j = 0; j < Dims::MAX_DIMS; j++) {\n+      oss << dims.d[j] << \", \";\n+    }\n+    LOG(INFO) << oss.str();\n+    oss.str(\"\");\n+    switch (trt_engine_ptr_->getBindingDataType(i)) {\n+      case nvinfer1::DataType::kFLOAT:\n+        LOG(INFO) << \"data type float\" << std::endl;\n+        break;\n+      case nvinfer1::DataType::kHALF:\n+        LOG(INFO) << \"data type half\" << std::endl;\n+        break;\n+      case nvinfer1::DataType::kINT8:\n+        LOG(INFO) << \"data type int8\" << std::endl;\n+        break;\n+    }\n+  }\n+\n+  // CHECK_NE(cudaStreamCreate(&stream_),0); // logic here is wrong\n+  // cudaStreamCreate(&stream_);\n+}\n+\n+void TRTEngineOp::Compute(OpKernelContext* context) {\n+  int nbBindings = context->num_inputs() + context->num_outputs();\n+  // TODO(jjsjann123) multiple input/output\n+  std::vector<void*> buffers(nbBindings);\n+\n+  size_t bindingIndex;\n+  int nbBatch = 0;\n+  bool valid = true;\n+  for (int i = 0; i < context->num_inputs(); i++) {\n+    // Grab the input tensor\n+    bindingIndex = trt_engine_ptr_->getBindingIndex(input_nodes_[i].c_str());\n+\n+    const Tensor& input_tensor = context->input(i);\n+    const TensorShape& input_shape = input_tensor.shape();\n+    if (i == 0) {\n+      nbBatch = input_shape.dim_size(0);\n+    } else if (nbBatch != input_shape.dim_size(0)) {\n+      valid = false;\n+      break;\n+    }\n+    // int64 input_shape.dim_size(int d)\n+    // int input_shape.dims()\n+    switch (trt_engine_ptr_->getBindingDataType(bindingIndex)) {\n+      case nvinfer1::DataType::kFLOAT:\n+        LOG(INFO) << \"float\";\n+        buffers[bindingIndex] = (void*)(input_tensor.flat<float>().data());\n+        break;\n+      case nvinfer1::DataType::kHALF:\n+        LOG(INFO) << \"half\";\n+        // buffers[bindingIndex] = (void*)input_tensor.flat<float16>().data();\n+        break;\n+      case nvinfer1::DataType::kINT8:\n+        LOG(INFO) << \"int8\";\n+        // buffers[bindingIndex] = (void*)input_tensor.flat<int8>().data();\n+        break;\n+    }\n+  }\n+\n+  if (!valid) LOG(WARNING) << \"input data inconsistent batch size\";\n+\n+  for (int i = 0; i < static_cast<int>(output_nodes_.size()); i++) {\n+    // This is bad that we have to reallocate output buffer every run.\n+    // Create an output tensor\n+    bindingIndex = trt_engine_ptr_->getBindingIndex(output_nodes_[i].c_str());\n+    Tensor* output_tensor = NULL;\n+\n+    TensorShape output_shape;\n+    if (bindingIndex != -1) {\n+      LOG(INFO) << \"got binding \" << bindingIndex;\n+      auto dims = trt_engine_ptr_->getBindingDimensions(bindingIndex);\n+      std::vector<int> trt_shape(dims.nbDims + 1);\n+      trt_shape[0] = nbBatch;\n+      for (int j = 0; j < dims.nbDims; j++) trt_shape[j + 1] = dims.d[j];\n+      TensorShapeUtils::MakeShape(trt_shape.data(), trt_shape.size(),\n+                                  &output_shape);\n+    } else {\n+      LOG(INFO) << \"no binding \";\n+      break;\n+    }\n+\n+    OP_REQUIRES_OK(context,\n+                   context->allocate_output(i, output_shape, &output_tensor));\n+    // buffers[bindingIndex] = (void*)output_tensor->flat<float>();\n+    // buffers[bindingIndex] = output_tensor->flat<float>().data();\n+    switch (trt_engine_ptr_->getBindingDataType(bindingIndex)) {\n+      case nvinfer1::DataType::kFLOAT:\n+        LOG(INFO) << \"float\";\n+        buffers[bindingIndex] =\n+            reinterpret_cast<void*>(output_tensor->flat<float>().data());\n+        break;\n+      case nvinfer1::DataType::kHALF:\n+        LOG(INFO) << \"half\";\n+        // buffers[bindingIndex] = (void*)output_tensor->flat<float16>().data();\n+        break;\n+      case nvinfer1::DataType::kINT8:\n+        LOG(INFO) << \"int8\";\n+        // buffers[bindingIndex] = (void*)output_tensor->flat<int8>().data();\n+        break;\n+    }\n+  }\n+  // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n+  const cudaStream_t* stream = CHECK_NOTNULL(\n+      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\n+                                                ->stream()\n+                                                ->implementation()\n+                                                ->CudaStreamMemberHack()));\n+\n+  trt_context_ptr_->enqueue(nbBatch, &buffers[0], *stream, nullptr);\n+  cudaStreamSynchronize(*stream);", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 178, "commit_id": "1e4b5b8c0cc1675b9ecac3569c91563a2a4f9984", "original_commit_id": "7035501c1b35d52d80d8fde3a95492d83a96f495", "user": {"login": "wujingyue", "id": 2772612, "node_id": "MDQ6VXNlcjI3NzI2MTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2772612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wujingyue", "html_url": "https://github.com/wujingyue", "followers_url": "https://api.github.com/users/wujingyue/followers", "following_url": "https://api.github.com/users/wujingyue/following{/other_user}", "gists_url": "https://api.github.com/users/wujingyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/wujingyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wujingyue/subscriptions", "organizations_url": "https://api.github.com/users/wujingyue/orgs", "repos_url": "https://api.github.com/users/wujingyue/repos", "events_url": "https://api.github.com/users/wujingyue/events{/privacy}", "received_events_url": "https://api.github.com/users/wujingyue/received_events", "type": "User", "site_admin": false}, "body": "@samikama tf::Tensors are ref-counted. `OpKernelContext` holds a reference of the result of an allocate_output even after `Compute` returns. ", "created_at": "2018-01-24T23:20:30Z", "updated_at": "2018-02-12T23:36:57Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16253#discussion_r163709586", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16253", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/163709586"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16253#discussion_r163709586"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16253"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10539540\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samikama\">@samikama</a> tf::Tensors are ref-counted. <code>OpKernelContext</code> holds a reference of the result of an allocate_output even after <code>Compute</code> returns.</p>", "body_text": "@samikama tf::Tensors are ref-counted. OpKernelContext holds a reference of the result of an allocate_output even after Compute returns.", "in_reply_to_id": 163673589}