{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/365090027", "html_url": "https://github.com/tensorflow/tensorflow/pull/16253#issuecomment-365090027", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16253", "id": 365090027, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTA5MDAyNw==", "user": {"login": "tadeegan", "id": 1289052, "node_id": "MDQ6VXNlcjEyODkwNTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1289052?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tadeegan", "html_url": "https://github.com/tadeegan", "followers_url": "https://api.github.com/users/tadeegan/followers", "following_url": "https://api.github.com/users/tadeegan/following{/other_user}", "gists_url": "https://api.github.com/users/tadeegan/gists{/gist_id}", "starred_url": "https://api.github.com/users/tadeegan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tadeegan/subscriptions", "organizations_url": "https://api.github.com/users/tadeegan/orgs", "repos_url": "https://api.github.com/users/tadeegan/repos", "events_url": "https://api.github.com/users/tadeegan/events{/privacy}", "received_events_url": "https://api.github.com/users/tadeegan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-12T22:49:03Z", "updated_at": "2018-02-12T22:49:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Very interesting PR.  I have a few questions..</p>\n<ol>\n<li>Will this support int8 quantization?</li>\n<li>What is the difference in strategy between this and the UFF parser / converter Nvidia makes?  This doesn't seem to reuse that project so TensorFlow conversion code will diverge / have differences in behavior between tf.contrib.tensorrt and uff. The ops that this supports is very limited are there more in the works? The UFF parser supports these:</li>\n</ol>\n<pre><code>    \"Add\",\n    \"AvgPool\",\n    \"BiasAdd\",\n    \"ConcatV2\",\n    \"Const\",\n    \"Conv2D\",\n    \"Conv2DBackpropInput\",\n    \"DepthwiseConv2dNative\",\n    \"Div\",\n    \"FusedBatchNorm\",\n    \"MatMul\",\n    \"Maximum\",\n    \"MaxPool\",\n    \"Minimum\",\n    \"Mul\",\n    \"Pack\",\n    \"Placeholder\",\n    \"Relu\",\n    \"Reshape\",\n    \"Shape\",\n    \"Sigmoid\",\n    \"Softmax\",\n    \"StridedSlice\",\n    \"Sub\",\n    \"Tanh\",\n    \"Transpose\",\n    # https://devtalk.nvidia.com/default/topic/1026657/jetson-tx2/unary-layer-in-tensorrt-3-0-rc/post/5234281/\n    # Unary ops don't currently work.\n    # \"Negative\",\n    # \"Abs\",\n    # \"Sqrt\",\n    # \"Rsqrt\",\n    # \"Pow\",\n    # \"Exp\"\n    # \"Log\"\n    # Ops that use reduce are currently buggy\n    # \"Sum\",\n    # \"Prod\",\n    # \"Min\",\n    # \"Max\",\n    # \"Mean\",\n</code></pre>", "body_text": "Very interesting PR.  I have a few questions..\n\nWill this support int8 quantization?\nWhat is the difference in strategy between this and the UFF parser / converter Nvidia makes?  This doesn't seem to reuse that project so TensorFlow conversion code will diverge / have differences in behavior between tf.contrib.tensorrt and uff. The ops that this supports is very limited are there more in the works? The UFF parser supports these:\n\n    \"Add\",\n    \"AvgPool\",\n    \"BiasAdd\",\n    \"ConcatV2\",\n    \"Const\",\n    \"Conv2D\",\n    \"Conv2DBackpropInput\",\n    \"DepthwiseConv2dNative\",\n    \"Div\",\n    \"FusedBatchNorm\",\n    \"MatMul\",\n    \"Maximum\",\n    \"MaxPool\",\n    \"Minimum\",\n    \"Mul\",\n    \"Pack\",\n    \"Placeholder\",\n    \"Relu\",\n    \"Reshape\",\n    \"Shape\",\n    \"Sigmoid\",\n    \"Softmax\",\n    \"StridedSlice\",\n    \"Sub\",\n    \"Tanh\",\n    \"Transpose\",\n    # https://devtalk.nvidia.com/default/topic/1026657/jetson-tx2/unary-layer-in-tensorrt-3-0-rc/post/5234281/\n    # Unary ops don't currently work.\n    # \"Negative\",\n    # \"Abs\",\n    # \"Sqrt\",\n    # \"Rsqrt\",\n    # \"Pow\",\n    # \"Exp\"\n    # \"Log\"\n    # Ops that use reduce are currently buggy\n    # \"Sum\",\n    # \"Prod\",\n    # \"Min\",\n    # \"Max\",\n    # \"Mean\",", "body": "Very interesting PR.  I have a few questions..\r\n\r\n1) Will this support int8 quantization?\r\n2) What is the difference in strategy between this and the UFF parser / converter Nvidia makes?  This doesn't seem to reuse that project so TensorFlow conversion code will diverge / have differences in behavior between tf.contrib.tensorrt and uff. The ops that this supports is very limited are there more in the works? The UFF parser supports these:\r\n```\r\n    \"Add\",\r\n    \"AvgPool\",\r\n    \"BiasAdd\",\r\n    \"ConcatV2\",\r\n    \"Const\",\r\n    \"Conv2D\",\r\n    \"Conv2DBackpropInput\",\r\n    \"DepthwiseConv2dNative\",\r\n    \"Div\",\r\n    \"FusedBatchNorm\",\r\n    \"MatMul\",\r\n    \"Maximum\",\r\n    \"MaxPool\",\r\n    \"Minimum\",\r\n    \"Mul\",\r\n    \"Pack\",\r\n    \"Placeholder\",\r\n    \"Relu\",\r\n    \"Reshape\",\r\n    \"Shape\",\r\n    \"Sigmoid\",\r\n    \"Softmax\",\r\n    \"StridedSlice\",\r\n    \"Sub\",\r\n    \"Tanh\",\r\n    \"Transpose\",\r\n    # https://devtalk.nvidia.com/default/topic/1026657/jetson-tx2/unary-layer-in-tensorrt-3-0-rc/post/5234281/\r\n    # Unary ops don't currently work.\r\n    # \"Negative\",\r\n    # \"Abs\",\r\n    # \"Sqrt\",\r\n    # \"Rsqrt\",\r\n    # \"Pow\",\r\n    # \"Exp\"\r\n    # \"Log\"\r\n    # Ops that use reduce are currently buggy\r\n    # \"Sum\",\r\n    # \"Prod\",\r\n    # \"Min\",\r\n    # \"Max\",\r\n    # \"Mean\",\r\n```"}