{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/164690996", "pull_request_review_id": 92499811, "id": 164690996, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDY5MDk5Ng==", "diff_hunk": "@@ -0,0 +1,138 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include <sstream>\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+\n+#if GOOGLE_CUDA\n+#if GOOGLE_TENSORRT\n+\n+#include <cuda_runtime_api.h>\n+#include \"tensorflow/contrib/tensorrt/kernels/trt_engine_op.h\"\n+#include \"tensorflow/contrib/tensorrt/log/trt_logger.h\"\n+\n+namespace tensorflow {\n+static ::tensorflow::tensorrt::Logger gLogger;\n+\n+namespace tensorrt {\n+\n+TRTEngineOp::TRTEngineOp(OpKernelConstruction* context) : OpKernel(context) {\n+  // read serialized_engine\n+  std::string serialized_engine;\n+  OP_REQUIRES_OK(context,\n+                 context->GetAttr(\"serialized_engine\", &serialized_engine));\n+\n+  // register input output node name in trt_sub_graph\n+  OP_REQUIRES_OK(context, context->GetAttr(\"input_nodes\", &input_nodes_));\n+  OP_REQUIRES_OK(context, context->GetAttr(\"output_nodes\", &output_nodes_));\n+\n+  // TODO(samikama) runtime should be taken from a resourcemanager as well.\n+  //  Only engine should be in the op and context and runtime should be taken\n+  //  from resourcemanager\n+  nvinfer1::IRuntime* infer = nvinfer1::createInferRuntime(gLogger);\n+  trt_engine_ptr_.reset(infer->deserializeCudaEngine(\n+      serialized_engine.c_str(), serialized_engine.size(), nullptr));\n+\n+  trt_execution_context_ptr_.reset(trt_engine_ptr_->createExecutionContext());\n+  // runtime is safe to delete after engine creation\n+  infer->destroy();\n+}\n+\n+void TRTEngineOp::Compute(OpKernelContext* context) {\n+  int num_binding = context->num_inputs() + context->num_outputs();\n+  std::vector<void*> buffers(num_binding);\n+\n+  size_t binding_index;\n+  int num_batch = 0;\n+  bool valid = true;\n+  for (int i = 0; i < context->num_inputs(); i++) {\n+    // Grab the input tensor\n+    binding_index = trt_engine_ptr_->getBindingIndex(input_nodes_[i].c_str());\n+\n+    const Tensor& input_tensor = context->input(i);\n+    const TensorShape& input_shape = input_tensor.shape();\n+    if (i == 0) {\n+      num_batch = input_shape.dim_size(0);\n+    } else if (num_batch != input_shape.dim_size(0)) {\n+      valid = false;\n+      break;\n+    }\n+    switch (trt_engine_ptr_->getBindingDataType(binding_index)) {\n+      case nvinfer1::DataType::kFLOAT:\n+        buffers[binding_index] = (void*)(input_tensor.flat<float>().data());\n+        break;\n+      case nvinfer1::DataType::kHALF:\n+        LOG(FATAL) << \"half size is not supported yet!\";\n+        break;\n+      case nvinfer1::DataType::kINT8:\n+        LOG(FATAL) << \"int8 is not supported yet!\";\n+        break;\n+    }\n+  }\n+\n+  // Might want a different way to inform the user of batch size inconsistency\n+  if (!valid) LOG(WARNING) << \"input data inconsistent batch size\";\n+\n+  for (int i = 0; i < static_cast<int>(output_nodes_.size()); i++) {\n+    // This is bad that we have to reallocate output buffer every run.\n+    // Create an output tensor\n+    binding_index = trt_engine_ptr_->getBindingIndex(output_nodes_[i].c_str());\n+    Tensor* output_tensor = NULL;\n+\n+    TensorShape output_shape;\n+    if (binding_index != -1) {\n+      auto dims = trt_engine_ptr_->getBindingDimensions(binding_index);\n+      std::vector<int> trt_shape(dims.nbDims + 1);\n+      trt_shape[0] = num_batch;\n+      for (int j = 0; j < dims.nbDims; j++) trt_shape[j + 1] = dims.d[j];\n+      TensorShapeUtils::MakeShape(trt_shape.data(), trt_shape.size(),\n+                                  &output_shape);\n+    } else {\n+      LOG(FATAL) << \"output node not found, at \" << output_nodes_[i];\n+      break;\n+    }\n+\n+    OP_REQUIRES_OK(context,\n+                   context->allocate_output(i, output_shape, &output_tensor));\n+    switch (trt_engine_ptr_->getBindingDataType(binding_index)) {\n+      case nvinfer1::DataType::kFLOAT:\n+        buffers[binding_index] =\n+            reinterpret_cast<void*>(output_tensor->flat<float>().data());\n+        break;\n+      case nvinfer1::DataType::kHALF:\n+        LOG(FATAL) << \"half size is not supported yet!\";\n+        break;\n+      case nvinfer1::DataType::kINT8:\n+        LOG(FATAL) << \"int8 is not supported yet!\";\n+        break;\n+    }\n+  }\n+  // copied from cuda_kernel_helper since it seems only valid in *.cu.cc files\n+  const cudaStream_t* stream = CHECK_NOTNULL(\n+      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\n+                                                ->stream()\n+                                                ->implementation()\n+                                                ->CudaStreamMemberHack()));\n+\n+  // execution handled by TF since we are getting stream from TF.\n+  trt_execution_context_ptr_->enqueue(num_batch, &buffers[0], *stream, nullptr);", "path": "tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc", "position": null, "original_position": 130, "commit_id": "1e4b5b8c0cc1675b9ecac3569c91563a2a4f9984", "original_commit_id": "84cab7b04b178ed63c19c9d618926f09da327fd7", "user": {"login": "jjsjann123", "id": 3709243, "node_id": "MDQ6VXNlcjM3MDkyNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjsjann123", "html_url": "https://github.com/jjsjann123", "followers_url": "https://api.github.com/users/jjsjann123/followers", "following_url": "https://api.github.com/users/jjsjann123/following{/other_user}", "gists_url": "https://api.github.com/users/jjsjann123/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjsjann123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjsjann123/subscriptions", "organizations_url": "https://api.github.com/users/jjsjann123/orgs", "repos_url": "https://api.github.com/users/jjsjann123/repos", "events_url": "https://api.github.com/users/jjsjann123/events{/privacy}", "received_events_url": "https://api.github.com/users/jjsjann123/received_events", "type": "User", "site_admin": false}, "body": "Sorry I missed your last comment about buffers run out of scope.\r\n\r\nI checked TRT code, looks like enqueue() is not making copy of the array. I will double check with TRT team tomorrow.\r\nMeanwhile, enqueue() provides a callback function after memory has been consumed. I can use that to maintain temporary buffer for enqueue() to avoid synchronization.", "created_at": "2018-01-30T09:58:56Z", "updated_at": "2018-02-12T23:36:57Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16253#discussion_r164690996", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16253", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/164690996"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16253#discussion_r164690996"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16253"}}, "body_html": "<p>Sorry I missed your last comment about buffers run out of scope.</p>\n<p>I checked TRT code, looks like enqueue() is not making copy of the array. I will double check with TRT team tomorrow.<br>\nMeanwhile, enqueue() provides a callback function after memory has been consumed. I can use that to maintain temporary buffer for enqueue() to avoid synchronization.</p>", "body_text": "Sorry I missed your last comment about buffers run out of scope.\nI checked TRT code, looks like enqueue() is not making copy of the array. I will double check with TRT team tomorrow.\nMeanwhile, enqueue() provides a callback function after memory has been consumed. I can use that to maintain temporary buffer for enqueue() to avoid synchronization.", "in_reply_to_id": 164653285}