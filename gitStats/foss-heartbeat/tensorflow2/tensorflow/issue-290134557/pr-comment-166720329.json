{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/166720329", "pull_request_review_id": 94829302, "id": 166720329, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjcyMDMyOQ==", "diff_hunk": "@@ -0,0 +1,1660 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <memory>\n+#include <set>\n+#include <unordered_map>\n+#include <utility>\n+#include <vector>\n+\n+#include \"tensorflow/core/framework/attr_value.pb.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/lib/strings/strcat.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/tensor_coding.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+\n+#if GOOGLE_CUDA\n+#if GOOGLE_TENSORRT\n+#include \"tensorflow/contrib/tensorrt/log/trt_logger.h\"\n+#include \"tensorrt/include/NvInfer.h\"\n+\n+//  Check if the types are equal. Cast to int first so that failure log message\n+//  would work!\n+#define CHECK_EQ_TYPE(val1, val2) CHECK_EQ((int)val1, (int)val2)\n+\n+namespace tensorflow {\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+inline tensorflow::Status ConvertDType(tensorflow::DataType tf_dtype,\n+                                       nvinfer1::DataType* trt_dtype) {\n+  switch (tf_dtype) {\n+    case tensorflow::DataType::DT_FLOAT:\n+      *trt_dtype = nvinfer1::DataType::kFLOAT;\n+      break;\n+    case tensorflow::DataType::DT_INT8:\n+      *trt_dtype = nvinfer1::DataType::kINT8;\n+      break;\n+    case tensorflow::DataType::DT_HALF:\n+      *trt_dtype = nvinfer1::DataType::kHALF;\n+      break;\n+    default:\n+      return tensorflow::errors::InvalidArgument(\"Unsupported data type\");\n+  }\n+  return tensorflow::Status::OK();\n+}\n+\n+inline nvinfer1::Dims GetTensorShape(const tensorflow::Tensor& tensor) {\n+  nvinfer1::Dims dims;\n+  dims.nbDims = tensor.dims();\n+  for (int i = 0; i < dims.nbDims; i++) {\n+    dims.d[i] = tensor.dim_size(i);\n+  }\n+  return dims;\n+}\n+\n+inline int64_t GetShapeSize(nvinfer1::Dims shape) {\n+  // Returns total number of elements in shape\n+  int64_t count = 1;\n+  for (int d = 0; d < shape.nbDims; ++d) {\n+    count *= shape.d[d];\n+  }\n+  return count;\n+}\n+\n+static std::vector<std::pair<int, int>> CreateSamePadding(\n+    const nvinfer1::DimsHW& stride, const nvinfer1::DimsHW& kernel,\n+    const std::vector<int64_t>& input_dims) {\n+  std::vector<std::pair<int, int>> padding(input_dims.size());\n+  CHECK_EQ((size_t)stride.nbDims, input_dims.size());  // TODO(jie): N+C? NC+?\n+\n+  for (size_t i = 0; i < input_dims.size(); ++i) {\n+    // Formula to calculate the padding\n+    int p = ((input_dims[i] - 1) / stride.d[i]) * stride.d[i] + kernel.d[i] -\n+            input_dims[i];\n+    p = (p > 0) ? p : 0;\n+\n+    // Right precedence padding, like in TensorFlow\n+    int left = p / 2;\n+    int right = p - left;\n+\n+    VLOG(2) << \"PADDING_\" << i << \" pre: \" << left << \", post: \" << right\n+            << \"paras: \" << input_dims[i] << \", \" << stride.d[i] << \", \"\n+            << \"kernel: \" << kernel.d[i];\n+    padding[i] = {left, right};\n+  }\n+  return padding;\n+}\n+\n+class TRT_ShapedWeights {\n+ public:\n+  TRT_ShapedWeights(tensorflow::DataType type, const void* values,\n+                    nvinfer1::Dims shape,\n+                    const std::vector<char>* owned_values = nullptr)\n+      : shape_(shape),\n+        type_(type),\n+        values_(values),\n+        owned_values_(owned_values ? *owned_values : std::vector<char>({})),\n+        dummy_flag_(false) {\n+    // Note: this->shape.type[] is not used\n+  }\n+\n+  explicit TRT_ShapedWeights(tensorflow::DataType type)\n+      : shape_(),\n+        type_(type),\n+        values_(nullptr),\n+        owned_values_(),\n+        dummy_flag_(true) {}\n+\n+  TRT_ShapedWeights(const TRT_ShapedWeights& rhs)\n+      : shape_(rhs.shape_),\n+        type_(rhs.type_),\n+        values_(rhs.values_),\n+        owned_values_(rhs.owned_values_),\n+        dummy_flag_(rhs.dummy_flag_) {}\n+\n+  int64_t count() const {\n+    int64_t c = 1;\n+    for (int i = 0; i < shape_.nbDims; i++) c *= shape_.d[i];\n+    return c;\n+  }\n+\n+  nvinfer1::Weights GetWeightsForTRT() const {\n+    nvinfer1::DataType trt_type(nvinfer1::DataType::kFLOAT);\n+    TF_CHECK_OK(ConvertDType(type_, &trt_type));\n+    if (dummy_flag_) return nvinfer1::Weights{trt_type, nullptr, 0};\n+\n+    // Note: this->shape.type[] is not used\n+    return nvinfer1::Weights{trt_type, GetValues(), GetShapeSize(shape_)};\n+  }\n+\n+  const void* GetValues() const {\n+    if (values_) return values_;\n+    if (owned_values_.size()) return owned_values_.data();\n+    return nullptr;\n+  }\n+\n+  void SetValues(const void* values) {\n+    values_ = values;\n+    owned_values_.clear();\n+  }\n+\n+  size_t size_bytes() const {\n+    int type_size = tensorflow::DataTypeSize(this->type_);\n+    return this->count() * type_size;\n+  }\n+\n+  // Default converter\n+  operator nvinfer1::Weights() const { return GetWeightsForTRT(); }\n+\n+  nvinfer1::Dims shape_;\n+  tensorflow::DataType type_;\n+\n+ private:\n+  const void* values_;\n+  std::vector<char> owned_values_;\n+  bool dummy_flag_;\n+};\n+\n+class TRT_TensorOrWeights {\n+ public:\n+  explicit TRT_TensorOrWeights(nvinfer1::ITensor* tensor)\n+      : _tensor_(tensor), _weights_(DT_FLOAT), _variant_(TRT_NODE_TENSOR) {}\n+  explicit TRT_TensorOrWeights(const TRT_ShapedWeights& weights)\n+      : _tensor_(nullptr), _weights_(weights), _variant_(TRT_NODE_WEIGHTS) {}\n+  TRT_TensorOrWeights(const TRT_TensorOrWeights& rhs)\n+      : _tensor_(rhs._tensor_),\n+        _weights_(rhs._weights_),\n+        _variant_(rhs._variant_) {}\n+  ~TRT_TensorOrWeights() {}\n+\n+  bool is_tensor() const { return _variant_ == TRT_NODE_TENSOR; }\n+  bool is_weights() const { return _variant_ == TRT_NODE_WEIGHTS; }\n+\n+  nvinfer1::ITensor* tensor() {\n+    CHECK_EQ(is_tensor(), true);\n+    return _tensor_;\n+  }\n+  const nvinfer1::ITensor* tensor() const {\n+    CHECK_EQ(is_tensor(), true);\n+    return _tensor_;\n+  }\n+  TRT_ShapedWeights& weights() {\n+    CHECK_EQ(is_weights(), true);\n+    return _weights_;\n+  }\n+  const TRT_ShapedWeights& weights() const {\n+    CHECK_EQ(is_weights(), true);\n+    return _weights_;\n+  }\n+  nvinfer1::Dims shape() const {\n+    if (is_tensor()) {\n+      return tensor()->getDimensions();\n+    } else {\n+      return weights().shape_;\n+    }\n+  }\n+\n+ private:\n+  nvinfer1::ITensor* _tensor_;\n+  TRT_ShapedWeights _weights_;\n+  enum { TRT_NODE_TENSOR, TRT_NODE_WEIGHTS } _variant_;\n+};\n+\n+class TRT_LayerOrWeights {\n+ public:\n+  explicit TRT_LayerOrWeights(nvinfer1::ILayer* layer)\n+      : _layer_(layer), _variant_(TRT_NODE_LAYER) {}\n+  explicit TRT_LayerOrWeights(const TRT_ShapedWeights& weights)\n+      : _weights_(weights), _variant_(TRT_NODE_WEIGHTS) {}\n+  bool is_layer() const { return _variant_ == TRT_NODE_LAYER; }\n+  bool is_weights() const { return _variant_ == TRT_NODE_WEIGHTS; }\n+  nvinfer1::ILayer* layer() {\n+    CHECK_EQ(this->is_layer(), true);\n+    return _layer_;\n+  }\n+  TRT_ShapedWeights& weights() {\n+    CHECK_EQ(this->is_weights(), true);\n+    return _weights_;\n+  }\n+  TRT_TensorOrWeights output(int index = 0) const {\n+    if (this->is_layer()) {\n+      nvinfer1::ITensor* tensor = _layer_->getOutput(index);\n+      return TRT_TensorOrWeights(tensor);\n+    } else {\n+      CHECK_EQ(index, 0);\n+      return TRT_TensorOrWeights(_weights_);\n+    }\n+  }\n+\n+ private:\n+  union {\n+    nvinfer1::ILayer* _layer_;\n+    TRT_ShapedWeights _weights_;\n+  };\n+  enum { TRT_NODE_LAYER, TRT_NODE_WEIGHTS } _variant_;\n+};\n+\n+class TFAttrs {\n+ public:\n+  explicit TFAttrs(const tensorflow::NodeDef& tf_node) {\n+    for (const auto& attr : tf_node.attr()) {\n+      _attrs.insert({attr.first, &attr.second});\n+    }\n+  }\n+  bool count(string key) const { return _attrs.count(key); }\n+  tensorflow::AttrValue const* at(string key) const {\n+    if (!_attrs.count(key)) {\n+      LOG(FATAL) << \"Attribute not found: \" << key;\n+    }\n+    return _attrs.at(key);\n+  }\n+  template <typename T>\n+  T get(string key) const;\n+  template <typename T>\n+  T get(string key, const T& default_value) const {\n+    return _attrs.count(key) ? this->get<T>(key) : default_value;\n+  }\n+\n+ private:\n+  typedef std::map<string, tensorflow::AttrValue const*> AttrMap;\n+  AttrMap _attrs;\n+};\n+\n+template <>\n+string TFAttrs::get<string>(string key) const {\n+  return this->at(key)->s();\n+}\n+\n+template <>\n+std::vector<int> TFAttrs::get<std::vector<int>>(string key) const {\n+  auto attr = this->at(key)->list().i();\n+  return std::vector<int>(attr.begin(), attr.end());\n+}\n+\n+template <>\n+nvinfer1::Dims TFAttrs::get<nvinfer1::Dims>(string key) const {\n+  auto values = this->get<std::vector<int>>(key);\n+  nvinfer1::Dims dims;\n+  dims.nbDims = values.size();\n+  std::copy(values.begin(), values.end(), dims.d);\n+  // Note: No dimension type information is included\n+  return dims;\n+}\n+\n+template <>\n+nvinfer1::DataType TFAttrs::get<nvinfer1::DataType>(string key) const {\n+  nvinfer1::DataType trt_dtype(nvinfer1::DataType::kFLOAT);\n+  TF_CHECK_OK(ConvertDType(this->at(key)->type(), &trt_dtype));\n+  return trt_dtype;\n+}\n+\n+template <>\n+tensorflow::DataType TFAttrs::get<tensorflow::DataType>(string key) const {\n+  return this->at(key)->type();\n+}\n+\n+template <typename T>\n+void Reorder4(nvinfer1::DimsNCHW shape, const T* idata,\n+              nvinfer1::DimsNCHW istrides, T* odata,\n+              nvinfer1::DimsNCHW ostrides) {\n+  for (int n = 0; n < shape.n(); ++n) {\n+    for (int c = 0; c < shape.c(); ++c) {\n+      for (int h = 0; h < shape.h(); ++h) {\n+        for (int w = 0; w < shape.w(); ++w) {\n+          odata[n * ostrides.n() + c * ostrides.c() + h * ostrides.h() +\n+                w * ostrides.w()] = idata[n * istrides.n() + c * istrides.c() +\n+                                          h * istrides.h() + w * istrides.w()];\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void ReorderRSCKToKCRS(const TRT_ShapedWeights& iweights,\n+                       TRT_ShapedWeights* oweights) {\n+  CHECK_EQ(iweights.type_, oweights->type_);\n+  CHECK_EQ(iweights.size_bytes(), oweights->size_bytes());\n+  int r = iweights.shape_.d[0];\n+  int s = iweights.shape_.d[1];\n+  int c = iweights.shape_.d[2];\n+  int k = iweights.shape_.d[3];\n+  oweights->shape_.d[0] = k;\n+  oweights->shape_.d[1] = c;\n+  oweights->shape_.d[2] = r;\n+  oweights->shape_.d[3] = s;\n+  nvinfer1::DimsNCHW istrides = {1, k, s * k * c, c * k};\n+  nvinfer1::DimsNCHW ostrides = {c * r * s, r * s, s, 1};\n+  switch (iweights.type_) {\n+    case tensorflow::DataType::DT_FLOAT:\n+      Reorder4({k, c, r, s}, static_cast<float const*>(iweights.GetValues()),\n+               istrides,\n+               static_cast<float*>(const_cast<void*>(oweights->GetValues())),\n+               ostrides);\n+      break;\n+    default:\n+      LOG(FATAL) << \"!!!!!!!!!!!!!!!!!!!!!!!!broke!!!!!!!!!!!!\";\n+  }\n+}\n+\n+struct InferDeleter {\n+  template <typename T>\n+  void operator()(T* obj) const {\n+    if (obj) {\n+      obj->destroy();\n+    }\n+  }\n+};\n+\n+template <typename T>\n+inline std::shared_ptr<T> infer_object(T* obj) {\n+  return std::shared_ptr<T>(obj, InferDeleter());\n+}\n+\n+// Logger for GIE info/warning/errors\n+class Converter;\n+\n+using OpConverter =\n+    std::function<tensorflow::Status(Converter&, const tensorflow::NodeDef&,\n+                                     std::vector<TRT_TensorOrWeights> const&,\n+                                     std::vector<TRT_TensorOrWeights>*)>;\n+\n+class Converter {\n+  std::unordered_map<string, TRT_TensorOrWeights> _trt_tensors;\n+  std::unordered_map<string, OpConverter> _op_registry;\n+  nvinfer1::INetworkDefinition* _trt_network;\n+  std::list<std::vector<uint8_t>> _temp_bufs;\n+\n+  void register_op_converters();\n+\n+  std::vector<TRT_TensorOrWeights> get_inputs(\n+      const tensorflow::NodeDef& node_def) {\n+    std::vector<TRT_TensorOrWeights> inputs;\n+    for (const auto& input_name : node_def.input()) {\n+      VLOG(2) << \"Retrieve input: \" << input_name;\n+      inputs.push_back(_trt_tensors.at(input_name));\n+    }\n+    return inputs;\n+  }\n+\n+ public:\n+  explicit Converter(nvinfer1::INetworkDefinition* trt_network)\n+      : _trt_network(trt_network) {\n+    this->register_op_converters();\n+  }\n+\n+  TRT_ShapedWeights get_temp_weights(tensorflow::DataType type,\n+                                     nvinfer1::Dims shape) {\n+    TRT_ShapedWeights weights(type, nullptr, shape);\n+    // TODO(jie): check weights size_bytes. 0 means type error\n+    _temp_bufs.push_back(std::vector<uint8_t>(weights.size_bytes()));\n+    weights.SetValues(_temp_bufs.back().data());\n+    return weights;\n+  }\n+\n+  TRT_ShapedWeights get_temp_weights_like(const TRT_ShapedWeights& weights) {\n+    return this->get_temp_weights(weights.type_, weights.shape_);\n+  }\n+\n+  tensorflow::Status convert_node(const tensorflow::NodeDef& node_def) {\n+    std::vector<TRT_TensorOrWeights> inputs = this->get_inputs(node_def);\n+    string op = node_def.op();\n+    if (!_op_registry.count(op)) {\n+      return tensorflow::errors::Unimplemented(\n+          \"No converter registered for op: \" + op);\n+    }\n+    OpConverter op_converter = _op_registry.at(op);\n+    std::vector<TRT_TensorOrWeights> outputs;\n+    TF_RETURN_IF_ERROR(op_converter(*this, node_def, inputs, &outputs));\n+    for (size_t i = 0; i < outputs.size(); ++i) {\n+      TRT_TensorOrWeights output = outputs.at(i);\n+      // TODO(jie): tf protobuf seems to be omitting the :0 suffix\n+      string output_name = node_def.name();\n+      if (i != 0) output_name = output_name + \":\" + std::to_string(i);\n+      if (output.is_tensor()) {\n+        output.tensor()->setName(output_name.c_str());\n+      }\n+      VLOG(2) << \"Write out tensor: \" << output_name;\n+      if (!_trt_tensors.insert({output_name, output}).second) {\n+        return tensorflow::errors::AlreadyExists(\n+            \"Output tensor already exists for op: \" + op);\n+      }\n+    }\n+    return tensorflow::Status::OK();\n+  }\n+\n+  nvinfer1::INetworkDefinition* network() { return _trt_network; }\n+\n+  TRT_TensorOrWeights get_tensor(string name) {\n+    if (!_trt_tensors.count(name)) {\n+      return TRT_TensorOrWeights(nullptr);\n+    }\n+    return _trt_tensors.at(name);\n+  }\n+\n+  bool insert_input_tensor(string name, nvinfer1::ITensor* tensor) {\n+    return _trt_tensors.insert({name, TRT_TensorOrWeights(tensor)}).second;\n+  }\n+\n+  nvinfer1::ITensor* TransposeTensor(nvinfer1::ITensor* input_tensor,\n+                                     std::vector<int> order) {\n+    auto dims = input_tensor->getDimensions();\n+\n+    // TODO(jie): change the return to status and properly exit\n+    if (order.size() - 1 != size_t(dims.nbDims))\n+      LOG(ERROR) << \"Dimension does not match, fail gracefully\";\n+\n+    nvinfer1::IShuffleLayer* layer = this->network()->addShuffle(*input_tensor);\n+    nvinfer1::Permutation permutation;\n+    for (int32_t i = 0; i < dims.nbDims; ++i) {\n+      permutation.order[i] = order[i + 1] - 1;\n+    }\n+    layer->setFirstTranspose(permutation);\n+\n+    nvinfer1::Dims reshape_dims;\n+    reshape_dims.nbDims = dims.nbDims;\n+    for (int32_t i = 0; i < reshape_dims.nbDims; ++i) {\n+      reshape_dims.d[i] = 0;\n+      reshape_dims.type[i] = dims.type[i];\n+    }\n+    layer->setReshapeDimensions(reshape_dims);\n+    return layer->getOutput(0);\n+  }\n+};\n+\n+// ****************************************************************************\n+// Constant folding functions\n+// TODO(jie): once optimizer kicks in, we should have done constant folding\n+// there.\n+//*****************************************************************************/\n+struct LambdaFactory {\n+  enum class OP_CATEGORY : int { RSQRT = 0, NEG, ADD, MUL, SUB };\n+  OP_CATEGORY op;\n+\n+  template <typename T>\n+  std::function<T(T)> unary() {\n+    switch (op) {\n+      case OP_CATEGORY::RSQRT: {\n+        VLOG(2) << \"RSQRT GETS DONE\";\n+        return [](T t) -> T { return 1.0 / std::sqrt(t); };\n+      }\n+      case OP_CATEGORY::NEG:\n+        return [](T t) -> T { return -t; };\n+      default:\n+        VLOG(2) << \"Not supported op for unary: \" << static_cast<int>(op);\n+        return nullptr;\n+    }\n+  }\n+\n+  template <typename T>\n+  std::function<T(T, T)> binary() {\n+    switch (op) {\n+      case OP_CATEGORY::ADD:\n+        return [](T l, T r) -> T { return l + r; };\n+      case OP_CATEGORY::SUB:\n+        return [](T l, T r) -> T { return l - r; };\n+      case OP_CATEGORY::MUL:\n+        return [](T l, T r) -> T { return l * r; };\n+      default:\n+        LOG(WARNING) << \"Not supported op for binary: \" << static_cast<int>(op);\n+    }\n+    return [](T l, T r) -> T {\n+      LOG(FATAL) << \"Unsupported op type \";\n+      return l;\n+    };\n+  }\n+\n+  template <typename T>\n+  std::function<T(T)> broadcast_r(T val) {\n+    VLOG(2) << \"LAMBDA VAL : \" << val;\n+    switch (op) {\n+      case OP_CATEGORY::ADD:\n+        return [val](T l) -> T {\n+          VLOG(2) << \"LAMBDA VAL : \" << val;\n+          return l + val;\n+        };\n+      // Return [val](T l)-> T {return l+val;};\n+      case OP_CATEGORY::SUB:\n+        return [val](T l) -> T {\n+          VLOG(2) << \"LAMBDA VAL : \" << val;\n+          return l - val;\n+        };\n+      case OP_CATEGORY::MUL:\n+        return [val](T l) -> T {\n+          VLOG(2) << \"LAMBDA VAL : \" << val;\n+          return l * val;\n+        };\n+      default:\n+        LOG(WARNING) << \"Not supported op for binary: \" << static_cast<int>(op);\n+    }\n+    return [val](T l) -> T {\n+      LOG(FATAL) << \"Unsupported op type \";\n+      return l;\n+    };\n+  }\n+\n+  template <typename T>\n+  std::function<T(T)> broadcast_l(T val) {\n+    VLOG(2) << \"LAMBDA VAL : \" << val;\n+    switch (op) {\n+      case OP_CATEGORY::ADD:\n+        return [val](T l) -> T {\n+          VLOG(2) << \"LAMBDA VAL : \" << val;\n+          return val + l;\n+        };\n+      case OP_CATEGORY::SUB:\n+        return [val](T l) -> T {\n+          VLOG(2) << \"LAMBDA VAL : \" << val;\n+          return val - l;\n+        };\n+      case OP_CATEGORY::MUL:\n+        return [val](T l) -> T {\n+          VLOG(2) << \"LAMBDA VAL : \" << val;\n+          return val * l;\n+        };\n+      default:\n+        LOG(ERROR) << \"Not supported op for binary: \" << static_cast<int>(op);\n+    }\n+    return [val](T l) -> T {\n+      LOG(FATAL) << \"Unsupported op type \";\n+      return l;\n+    };\n+  }\n+};\n+\n+tensorflow::Status UnaryCompute(const TRT_ShapedWeights& iweights,\n+                                TRT_ShapedWeights* oweights,\n+                                LambdaFactory unary_op) {\n+  CHECK_EQ(iweights.type_, oweights->type_);\n+  switch (iweights.type_) {\n+    case tensorflow::DataType::DT_FLOAT: {\n+      auto inp = static_cast<float const*>(iweights.GetValues());\n+      auto oup = static_cast<float*>(const_cast<void*>(oweights->GetValues()));\n+      std::transform(inp, inp + iweights.count(), oup, unary_op.unary<float>());\n+      break;\n+    }\n+    default:\n+      return tensorflow::errors::Unimplemented(\n+          \"Data type not supported: \" +\n+          tensorflow::DataTypeString(iweights.type_));\n+  }\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status BinaryCompute(const TRT_ShapedWeights& iweights_l,\n+                                 const TRT_ShapedWeights& iweights_r,\n+                                 TRT_ShapedWeights* oweights,\n+                                 LambdaFactory binary_op) {\n+  // Assume iweights_l.type == iweight_r.type\n+  CHECK_EQ(iweights_l.type_, oweights->type_);\n+  CHECK_EQ(iweights_r.type_, oweights->type_);\n+  VLOG(2) << \"SANITY CHECK!\";\n+\n+  switch (iweights_l.type_) {\n+    case tensorflow::DataType::DT_FLOAT: {\n+      auto inp_l = static_cast<const float*>(iweights_l.GetValues());\n+      auto inp_r = static_cast<const float*>(iweights_r.GetValues());\n+      auto oup = static_cast<float*>(const_cast<void*>(oweights->GetValues()));\n+\n+      if (iweights_l.count() != iweights_r.count()) {\n+        // We only supports broadcast of RankZero\n+        if (iweights_l.count() == 1) {\n+          VLOG(2) << \"I bet it is not working!\" << (*inp_l);\n+          std::transform(inp_r, inp_r + iweights_r.count(), oup,\n+                         binary_op.broadcast_l<float>(*inp_l));\n+        } else if (iweights_r.count() == 1) {\n+          VLOG(2) << \"I bet it is not working!\" << (*inp_r);\n+          std::transform(inp_l, inp_l + iweights_l.count(), oup,\n+                         binary_op.broadcast_r<float>(*inp_r));\n+        } else {\n+          return tensorflow::errors::Unimplemented(\n+              \"Binary op with non-rankZero broadcast not supported\");\n+        }\n+      } else {\n+        std::transform(inp_l, inp_l + iweights_l.count(), inp_r, oup,\n+                       binary_op.binary<float>());\n+      }\n+      break;\n+    }\n+    default:\n+      return tensorflow::errors::Unimplemented(\n+          \"Data type not supported: \" +\n+          tensorflow::DataTypeString(iweights_l.type_));\n+  }\n+\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConstantFoldUnary(\n+    Converter& ctx, const tensorflow::NodeDef& node_def,\n+    std::vector<TRT_TensorOrWeights> const& inputs,\n+    std::vector<TRT_TensorOrWeights>* outputs) {\n+  TRT_ShapedWeights weights_input = inputs.at(0).weights();\n+\n+  // Allocate output weights\n+  TRT_ShapedWeights weights_output = ctx.get_temp_weights_like(weights_input);\n+\n+  // FIXME assume type matches input weights\n+  // Get trt type & shape\n+  // Maybe this part has to be moved into the block of rsqrt later\n+  // Check type consistency\n+  CHECK_EQ(weights_input.type_,\n+           TFAttrs(node_def).get<tensorflow::DataType>(\"T\"));\n+\n+  // Maybe I should do a switch\n+  LambdaFactory unary_op;\n+  if (node_def.op() == \"Rsqrt\") {\n+    // Compute rsqrt\n+    unary_op.op = LambdaFactory::OP_CATEGORY::RSQRT;\n+    auto ret = UnaryCompute(weights_input, &weights_output, unary_op);\n+    // PAss the output\n+    if (ret == tensorflow::Status::OK()) {\n+      outputs->push_back(TRT_TensorOrWeights(weights_output));\n+    }\n+    return ret;\n+  } else {\n+    return tensorflow::errors::Unimplemented(\"Binary op not supported: \" +\n+                                             node_def.op());\n+  }\n+}\n+\n+// TODO(jie,ben) broadcast is needed yet not implemented\n+// Let's get the simple stuff working first. Maybe we should fall bakc to TF\n+//   approach for constant folding\n+tensorflow::Status ConstantFoldBinary(\n+    Converter& ctx, const tensorflow::NodeDef& node_def,\n+    std::vector<TRT_TensorOrWeights> const& inputs,\n+    std::vector<TRT_TensorOrWeights>* outputs) {\n+  TRT_ShapedWeights weights_input_l = inputs.at(0).weights();\n+  TRT_ShapedWeights weights_input_r = inputs.at(1).weights();\n+\n+  // Check type consistency\n+  CHECK_EQ(weights_input_l.type_, weights_input_r.type_);\n+\n+  if (weights_input_l.shape_.nbDims != weights_input_r.shape_.nbDims)\n+    return tensorflow::errors::Unimplemented(\n+        \"Binary op implicit broadcast not supported: \" + node_def.op());\n+\n+  // TODO(jie): constant fold should really fall back to TF.\n+  int nb_dims = weights_input_l.shape_.nbDims;\n+  nvinfer1::Dims output_shape;\n+  output_shape.nbDims = nb_dims;\n+  VLOG(2) << \"nb_dims: \" << nb_dims\n+          << \", the other: \" << weights_input_r.shape_.nbDims;\n+  for (int i = 0; i < nb_dims; i++) {\n+    if (weights_input_l.shape_.d[i] == weights_input_r.shape_.d[i]) {\n+      output_shape.d[i] = weights_input_l.shape_.d[i];\n+    } else if (weights_input_l.shape_.d[i] == 1 ||\n+               weights_input_r.shape_.d[i] == 1) {\n+      output_shape.d[i] =\n+          std::max(weights_input_l.shape_.d[i], weights_input_r.shape_.d[i]);\n+    } else {\n+      return tensorflow::errors::Unimplemented(\n+          \"Binary op with incompatible shape at, \" + node_def.op());\n+    }\n+    VLOG(2) << \"left: \" << weights_input_l.shape_.d[i]\n+            << \"right: \" << weights_input_r.shape_.d[i]\n+            << \"output: \" << output_shape.d[i];\n+  }\n+\n+  // FIXME assume type matches input weights\n+  // Get trt type & shape\n+  TFAttrs attrs(node_def);\n+  // Maybe this part has to be moved into the block of rsqrt later\n+  tensorflow::DataType dtype = attrs.get<tensorflow::DataType>(\"T\");\n+\n+  // Allocate output weights\n+  TRT_ShapedWeights weights_output = ctx.get_temp_weights(dtype, output_shape);\n+\n+  // Maybe I should do a switch\n+  LambdaFactory binary_op;\n+  if (node_def.op() == \"Sub\") {\n+    binary_op.op = LambdaFactory::OP_CATEGORY::SUB;\n+  } else if (node_def.op() == \"Mul\") {\n+    binary_op.op = LambdaFactory::OP_CATEGORY::MUL;\n+  } else if (node_def.op() == \"Add\") {\n+    binary_op.op = LambdaFactory::OP_CATEGORY::ADD;\n+  } else {\n+    return tensorflow::errors::Unimplemented(\"Binary op not supported: \" +\n+                                             node_def.op());\n+  }\n+  auto ret = BinaryCompute(weights_input_l, weights_input_r, &weights_output,\n+                           binary_op);\n+\n+  // Pass the output\n+  if (ret == tensorflow::Status::OK()) {\n+    outputs->push_back(TRT_TensorOrWeights(weights_output));\n+  }\n+\n+  return ret;\n+}\n+\n+// TODO(jie): broadcast is needed yet not implemented.\n+// Only implemented channel wise for the time being\n+tensorflow::Status BinaryTensorOpWeight(\n+    Converter& ctx, const tensorflow::NodeDef& node_def,\n+    const nvinfer1::ITensor* tensor, TRT_ShapedWeights weights,\n+    std::vector<TRT_TensorOrWeights>* outputs) {\n+  // FIXME assume type matches input weights\n+  // Get trt type & shape\n+  // Maybe this part has to be moved into the block of rsqrt later\n+\n+  // Check type consistency\n+  auto dtype = TFAttrs(node_def).get<nvinfer1::DataType>(\"T\");\n+  CHECK_EQ_TYPE(tensor->getType(), dtype);  // Cast to int for error messages\n+  nvinfer1::DataType ttype;\n+  TF_CHECK_OK(ConvertDType(weights.type_, &ttype));\n+  CHECK_EQ_TYPE(ttype, dtype);  // Cast to int for error message\n+\n+  // Check scale mode\n+  auto dims_w = weights.shape_;\n+  auto dims_t = tensor->getDimensions();\n+\n+  // Default to channel-wise\n+  auto scale_mode = nvinfer1::ScaleMode::kELEMENTWISE;\n+\n+  if (weights.count() == 1) {\n+    VLOG(2) << \"UNIFORM\";\n+    scale_mode = nvinfer1::ScaleMode::kUNIFORM;\n+  } else {\n+    // No broadcasting on Batch dimension;\n+    assert(dims_w.d[0] == 1);\n+\n+    // Broadcasting on Channel dimension only allowed in kUNIFORM\n+    assert(dims_w.d[1] == dims_t.d[0]);\n+    assert(dims_w.nbDims == dims_t.nbDims);\n+\n+    // Default is element;\n+    for (int i = 2; i < dims_w.nbDims; i++) {\n+      if (dims_w.d[i] != dims_t.d[i - 1]) {\n+        scale_mode = nvinfer1::ScaleMode::kCHANNEL;\n+        break;\n+      }\n+    }\n+    if (scale_mode == nvinfer1::ScaleMode::kELEMENTWISE) {\n+      scale_mode = nvinfer1::ScaleMode::kELEMENTWISE;\n+      for (int i = 2; i < dims_w.nbDims; i++) {\n+        if (dims_w.d[i] != 1)\n+          return tensorflow::errors::InvalidArgument(\n+              \"Weight shape not compatible at, \" + node_def.name());\n+      }\n+    }\n+  }\n+\n+  // Prepare weights\n+  TRT_ShapedWeights shift_weights(weights.type_);\n+  TRT_ShapedWeights scale_weights(weights.type_);\n+  TRT_ShapedWeights power_weights(weights.type_);\n+\n+  // Maybe I should do a switch\n+  if (node_def.op() == \"Sub\") {\n+    TRT_ShapedWeights neg_weights = ctx.get_temp_weights_like(weights);\n+    LambdaFactory unary_op;\n+    unary_op.op = LambdaFactory::OP_CATEGORY::NEG;\n+    TF_RETURN_IF_ERROR(UnaryCompute(weights, &neg_weights, unary_op));\n+    shift_weights = neg_weights;\n+  } else if (node_def.op() == \"Mul\") {\n+    scale_weights = weights;\n+  } else if (node_def.op() == \"Add\") {\n+    shift_weights = weights;\n+  } else {\n+    return tensorflow::errors::Unimplemented(\"Binary op not supported: \" +\n+                                             node_def.op());\n+  }\n+\n+  nvinfer1::IScaleLayer* layer = ctx.network()->addScale(\n+      *const_cast<nvinfer1::ITensor*>(tensor), scale_mode, shift_weights,\n+      scale_weights, power_weights);\n+\n+  nvinfer1::ITensor* output_tensor = layer->getOutput(0);\n+\n+  // Pass the output\n+  outputs->push_back(TRT_TensorOrWeights(output_tensor));\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status BinaryTensorOpTensor(\n+    Converter& ctx, const tensorflow::NodeDef& node_def,\n+    const nvinfer1::ITensor* tensor_l, const nvinfer1::ITensor* tensor_r,\n+    std::vector<TRT_TensorOrWeights>* outputs) {\n+  static const std::unordered_map<string, nvinfer1::ElementWiseOperation> ops{\n+      {\"Add\", nvinfer1::ElementWiseOperation::kSUM},\n+      {\"Mul\", nvinfer1::ElementWiseOperation::kPROD},\n+      // {\"max\", nvinfer1::ElementWiseOperation::kMAX},\n+      // {\"min\", nvinfer1::ElementWiseOperation::kMIN},\n+      {\"Sub\", nvinfer1::ElementWiseOperation::kSUB},\n+      {\"Div\", nvinfer1::ElementWiseOperation::kDIV},\n+  };\n+\n+  // FIXME assume type matches input weights\n+  // Get trt type & shape\n+  TFAttrs attrs(node_def);\n+  // Maybe this part has to be moved into the block of rsqrt later\n+  nvinfer1::DataType dtype = attrs.get<nvinfer1::DataType>(\"T\");\n+\n+  // Check type consistency\n+  CHECK_EQ_TYPE(tensor_l->getType(), dtype);\n+  CHECK_EQ_TYPE(tensor_r->getType(), dtype);\n+  auto op_pair = ops.find(node_def.op());\n+  if (op_pair == ops.end())\n+    return tensorflow::errors::Unimplemented(\n+        \"binary op: \" + node_def.op() +\n+        \" not supported at: \" + node_def.name());\n+\n+  nvinfer1::IElementWiseLayer* layer = ctx.network()->addElementWise(\n+      *const_cast<nvinfer1::ITensor*>(tensor_l),\n+      *const_cast<nvinfer1::ITensor*>(tensor_r), op_pair->second);\n+\n+  nvinfer1::ITensor* output_tensor = layer->getOutput(0);\n+\n+  // Pass the output\n+  outputs->push_back(TRT_TensorOrWeights(output_tensor));\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConvertPlaceholder(\n+    Converter& ctx, const tensorflow::NodeDef& node_def,\n+    std::vector<TRT_TensorOrWeights> const& inputs,\n+    std::vector<TRT_TensorOrWeights>* outputs) {\n+  VLOG(2) << \"Placeholder should have been replace already\";\n+  return tensorflow::errors::Unimplemented(\", cannot convert Placeholder op\");\n+  // OK this make sense since we are supposed to replace it with input\n+  TFAttrs attrs(node_def);\n+  nvinfer1::DataType dtype = attrs.get<nvinfer1::DataType>(\"dtype\");\n+  nvinfer1::Dims dims = attrs.get<nvinfer1::Dims>(\"shape\");\n+\n+  dims.nbDims--;\n+  for (int i = 0; i < dims.nbDims; i++) dims.d[i] = dims.d[i + 1];\n+\n+  nvinfer1::ITensor* output =\n+      ctx.network()->addInput(node_def.name().c_str(), dtype, dims);\n+  if (!output) {\n+    return tensorflow::errors::InvalidArgument(\"Failed to create Input layer\");\n+  }\n+  outputs->push_back(TRT_TensorOrWeights(output));\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConvertConv2D(Converter& ctx,\n+                                 const tensorflow::NodeDef& node_def,\n+                                 const std::vector<TRT_TensorOrWeights>& inputs,\n+                                 std::vector<TRT_TensorOrWeights>* outputs) {\n+  nvinfer1::ITensor const* tensor = inputs.at(0).tensor();\n+  // TODO(jie): handle NHWC/NCHW transpose;\n+  TRT_ShapedWeights weights_rsck = inputs.at(1).weights();\n+  TRT_ShapedWeights weights = ctx.get_temp_weights_like(weights_rsck);\n+  ReorderRSCKToKCRS(weights_rsck, &weights);\n+  TRT_ShapedWeights biases(weights.type_);\n+  int noutput = weights.shape_.d[0];\n+  nvinfer1::DimsHW kernel_size;\n+  kernel_size.h() = weights.shape_.d[2];\n+  kernel_size.w() = weights.shape_.d[3];\n+  TFAttrs attrs(node_def);\n+\n+  int h_index = 2;\n+  int w_index = 3;\n+  auto data_format = attrs.get<string>(\"data_format\");\n+  if (data_format == \"NHWC\") {\n+    tensor = ctx.TransposeTensor(const_cast<nvinfer1::ITensor*>(tensor),\n+                                 {0, 3, 1, 2});\n+    h_index = 1;\n+    w_index = 2;\n+    // TODO(jie): transpose it\n+  }\n+\n+  // TODO(jie): stride. (NHWC/NCHW)\n+  auto tf_stride = attrs.get<std::vector<int>>(\"strides\");\n+  nvinfer1::DimsHW stride(tf_stride[h_index], tf_stride[w_index]);\n+\n+  auto tensor_dim = tensor->getDimensions();\n+  std::vector<std::pair<int, int>> padding;\n+  // TODO(jie): padding.\n+  if (attrs.get<string>(\"padding\") == \"SAME\") {\n+    // This is NCHW tensor with no batch dimension.\n+    //  1 -> h\n+    //  2 -> w\n+    padding = CreateSamePadding(\n+        stride, kernel_size,\n+        {static_cast<int>(tensor_dim.d[1]), static_cast<int>(tensor_dim.d[2])});\n+  } else {\n+    padding = {{0, 0}, {0, 0}};\n+  }\n+\n+  if (padding[0].first != padding[0].second ||\n+      padding[1].first != padding[1].second) {\n+    // TODO(jie): handle asymmetric padding\n+    VLOG(2) << \"Padding!!!: \" << padding[0].first << padding[0].second\n+            << padding[1].first << padding[1].second;\n+\n+    auto dim_before = tensor->getDimensions();\n+    VLOG(2) << \"TENSOR before: \" << dim_before.d[0] << \", \" << dim_before.d[1]\n+            << dim_before.d[2] << \", \" << dim_before.d[3];\n+    auto pad_layer = ctx.network()->addPadding(\n+        *const_cast<nvinfer1::ITensor*>(tensor),\n+        nvinfer1::DimsHW(padding[0].first, padding[1].first),\n+        nvinfer1::DimsHW(padding[0].second, padding[1].second));\n+    padding = {{0, 0}, {0, 0}};\n+    tensor = pad_layer->getOutput(0);\n+    auto dim_after = tensor->getDimensions();\n+    VLOG(2) << \"TENSOR after: \" << dim_after.d[0] << \", \" << dim_after.d[1]\n+            << dim_after.d[2] << \", \" << dim_after.d[3];\n+  }\n+\n+  nvinfer1::IConvolutionLayer* layer =\n+      ctx.network()->addConvolution(*const_cast<nvinfer1::ITensor*>(tensor),\n+                                    noutput, kernel_size, weights, biases);\n+\n+  layer->setStride(stride);\n+  layer->setPadding({padding[0].first, padding[1].first});\n+  layer->setName(node_def.name().c_str());\n+  nvinfer1::ITensor* output_tensor = layer->getOutput(0);\n+\n+  auto dim_after = output_tensor->getDimensions();\n+  VLOG(2) << \"TENSOR out: \" << dim_after.d[0] << \", \" << dim_after.d[1]\n+          << dim_after.d[2] << \", \" << dim_after.d[3];\n+\n+  if (data_format == \"NHWC\") {\n+    // TODO(jie): transpose it back!\n+    output_tensor = ctx.TransposeTensor(output_tensor, {0, 2, 3, 1});\n+  } else {\n+    VLOG(2) << \"NCHW !!!!\";\n+  }\n+  outputs->push_back(TRT_TensorOrWeights(output_tensor));\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConvertPool(Converter& ctx,\n+                               const tensorflow::NodeDef& node_def,\n+                               std::vector<TRT_TensorOrWeights> const& inputs,\n+                               std::vector<TRT_TensorOrWeights>* outputs) {\n+  nvinfer1::ITensor const* tensor = inputs.at(0).tensor();\n+  TFAttrs attrs(node_def);\n+\n+  int h_index = 2;\n+  int w_index = 3;\n+  auto data_format = attrs.get<string>(\"data_format\");\n+  if (data_format == \"NHWC\") {\n+    h_index = 1;\n+    w_index = 2;\n+    tensor = ctx.TransposeTensor(const_cast<nvinfer1::ITensor*>(tensor),\n+                                 {0, 3, 1, 2});\n+  } else {\n+    VLOG(2) << \"NCHW !!!!\";\n+  }\n+  nvinfer1::PoolingType type;\n+  // TODO(jie): support other pooling type\n+  if (node_def.op() == \"MaxPool\")\n+    type = nvinfer1::PoolingType::kMAX;\n+  else\n+    return tensorflow::errors::Unimplemented(\"Only supports Max pool\");\n+\n+  // TODO(jie): NCHW\n+  auto tf_stride = attrs.get<std::vector<int>>(\"strides\");\n+  nvinfer1::DimsHW stride(tf_stride[h_index], tf_stride[w_index]);\n+\n+  auto tf_kernel = attrs.get<std::vector<int>>(\"ksize\");\n+  nvinfer1::DimsHW ksize(tf_kernel[h_index], tf_kernel[w_index]);\n+\n+  auto tensor_dim = tensor->getDimensions();\n+  std::vector<std::pair<int, int>> padding;\n+  // TODO(jie): padding.\n+  if (attrs.get<string>(\"padding\") == \"SAME\") {\n+    // This is NCHW tensor with no batch dimension.\n+    //  1 -> h\n+    //  2 -> w\n+    padding = CreateSamePadding(\n+        stride, ksize,\n+        {static_cast<int>(tensor_dim.d[1]), static_cast<int>(tensor_dim.d[2])});\n+  } else if (attrs.get<string>(\"padding\") == \"VALID\") {\n+    // No padding for valid padding here\n+    VLOG(2) << \"No padding added for VALID padding in pool\" << node_def.name();\n+    padding = {{0, 0}, {0, 0}};\n+  } else {\n+    return tensorflow::errors::Unimplemented(\n+        \"Current MaxPool cannot support padding other than SAME\");\n+  }\n+\n+  if (padding[0].first != padding[0].second ||\n+      padding[1].first != padding[1].second) {\n+    // TODO(jie): handle asymmetric padding\n+    VLOG(2) << \"Padding!!!: \" << padding[0].first << padding[0].second\n+            << padding[1].first << padding[1].second;\n+    auto pad_layer = ctx.network()->addPadding(\n+        *const_cast<nvinfer1::ITensor*>(tensor),\n+        nvinfer1::DimsHW(padding[0].first, padding[1].first),\n+        nvinfer1::DimsHW(padding[0].second, padding[1].second));\n+    padding = {{0, 0}, {0, 0}};\n+    tensor = pad_layer->getOutput(0);\n+  }\n+\n+  nvinfer1::IPoolingLayer* layer = ctx.network()->addPooling(\n+      *const_cast<nvinfer1::ITensor*>(tensor), type, ksize);\n+\n+  layer->setStride(stride);\n+  layer->setPadding({padding[0].first, padding[1].first});\n+  layer->setName(node_def.name().c_str());\n+  nvinfer1::ITensor* output_tensor = layer->getOutput(0);\n+\n+  if (data_format == \"NHWC\") {\n+    // TODO(jie): transpose it back!\n+    output_tensor = ctx.TransposeTensor(output_tensor, {0, 2, 3, 1});\n+  } else {\n+    VLOG(2) << \"NCHW !!!!\";\n+  }\n+  outputs->push_back(TRT_TensorOrWeights(output_tensor));\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConvertActivation(\n+    Converter& ctx, const tensorflow::NodeDef& node_def,\n+    std::vector<TRT_TensorOrWeights> const& inputs,\n+    std::vector<TRT_TensorOrWeights>* outputs) {\n+  nvinfer1::ITensor const* tensor = inputs.at(0).tensor();\n+  nvinfer1::IActivationLayer* layer = ctx.network()->addActivation(\n+      *const_cast<nvinfer1::ITensor*>(tensor), nvinfer1::ActivationType::kRELU);\n+  nvinfer1::ITensor* output_tensor = layer->getOutput(0);\n+  outputs->push_back(TRT_TensorOrWeights(output_tensor));\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConvertScale(Converter& ctx,\n+                                const tensorflow::NodeDef& node_def,\n+                                std::vector<TRT_TensorOrWeights> const& inputs,\n+                                std::vector<TRT_TensorOrWeights>* outputs) {\n+  if (inputs.size() != 2 || !inputs.at(0).is_tensor() ||\n+      !inputs.at(1).is_weights())\n+    return tensorflow::errors::Unimplemented(\n+        \"Only supports tensor op weight for now, at \" + node_def.name());\n+  // Implement tensor binaryOp weight [channel wise] for now;\n+  nvinfer1::ITensor const* tensor = inputs.at(0).tensor();\n+\n+  // TODO(jie): handle NHWC/NCHW transpose;\n+  TRT_ShapedWeights weights = inputs.at(1).weights();\n+  TRT_ShapedWeights empty_weights(weights.type_);\n+\n+  TFAttrs attrs(node_def);\n+\n+  // Transpose NHWC\n+  auto data_format = attrs.get<string>(\"data_format\");\n+  if (data_format == \"NHWC\") {\n+    tensor = ctx.TransposeTensor(const_cast<nvinfer1::ITensor*>(tensor),\n+                                 {0, 3, 1, 2});\n+    // TODO(jie): transpose it\n+  } else {\n+    VLOG(2) << \"NCHW !!!!\";\n+  }\n+  nvinfer1::IScaleLayer* layer = ctx.network()->addScale(\n+      *const_cast<nvinfer1::ITensor*>(tensor), nvinfer1::ScaleMode::kCHANNEL,\n+      weights, empty_weights, empty_weights);\n+\n+  nvinfer1::ITensor* output_tensor = layer->getOutput(0);\n+  if (data_format == \"NHWC\") {\n+    // TODO(jie): transpose it back!\n+    output_tensor = ctx.TransposeTensor(output_tensor, {0, 2, 3, 1});\n+  } else {\n+    VLOG(2) << \"NCHW !!!!\";\n+  }\n+  outputs->push_back(TRT_TensorOrWeights(output_tensor));\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConvertConst(Converter& ctx,\n+                                const tensorflow::NodeDef& node_def,\n+                                std::vector<TRT_TensorOrWeights> const& inputs,\n+                                std::vector<TRT_TensorOrWeights>* outputs) {\n+  const auto& weights_tensor = node_def.attr().at(\"value\").tensor();\n+\n+  // Get trt type & shape\n+  TFAttrs attrs(node_def);\n+  const tensorflow::DataType dtype = attrs.get<tensorflow::DataType>(\"dtype\");\n+\n+  // Create shaped weights as output\n+  tensorflow::Tensor tensor;\n+  if (!tensor.FromProto(weights_tensor))\n+    return tensorflow::errors::Internal(\"Cannot parse weight tensor proto: \" +\n+                                        node_def.name());\n+\n+  TRT_ShapedWeights weights(dtype);\n+  if (!weights_tensor.float_val().empty()) {\n+    VLOG(2) << \"SCALAR!!!\" << node_def.name();\n+    nvinfer1::Dims scalar_shape;\n+    if (tensor.dims() > 0) {\n+      VLOG(2) << \"Dimensions: \" << tensor.dims();\n+      weights = TRT_ShapedWeights(dtype, weights_tensor.float_val().data(),\n+                                  GetTensorShape(tensor));\n+    } else {\n+      VLOG(2) << \"Dimensions: \" << tensor.dims();\n+      scalar_shape.nbDims = 1;\n+      scalar_shape.d[0] = 1;\n+      scalar_shape.type[0] = nvinfer1::DimensionType::kSPATIAL;\n+      for (int i = 1; i < nvinfer1::Dims::MAX_DIMS; i++) {\n+        scalar_shape.d[i] = 0;\n+        scalar_shape.type[i] = nvinfer1::DimensionType::kSPATIAL;\n+      }\n+      weights = TRT_ShapedWeights(dtype, weights_tensor.float_val().data(),\n+                                  scalar_shape);\n+    }\n+  } else if (!weights_tensor.tensor_content().empty()) {\n+    VLOG(2) << \"TENSOR!!!\" << node_def.name();\n+    const auto& content = weights_tensor.tensor_content();\n+\n+    std::vector<char> values;\n+    if (content.size() > 0) {\n+      const int dtype_size = tensorflow::DataTypeSize(dtype);\n+      CHECK_EQ(0, content.size() % dtype_size)\n+          << \"Tensor content size (\" << content.size()\n+          << \") is not a multiple of \" << dtype_size;\n+      values.resize(content.size());\n+      port::CopyToArray(content, values.data());\n+    }\n+    weights =\n+        TRT_ShapedWeights(dtype, nullptr, GetTensorShape(tensor), &values);\n+  } else {\n+    return tensorflow::errors::Unimplemented(\n+        \"Not supported constant type, at \" + node_def.name());\n+  }\n+  // Pass the output\n+  outputs->push_back(TRT_TensorOrWeights(weights));\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConvertIdentity(\n+    Converter& ctx, const tensorflow::NodeDef& node_def,\n+    std::vector<TRT_TensorOrWeights> const& inputs,\n+    std::vector<TRT_TensorOrWeights>* outputs) {\n+  outputs->push_back(inputs.at(0));\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConvertBinary(Converter& ctx,\n+                                 const tensorflow::NodeDef& node_def,\n+                                 std::vector<TRT_TensorOrWeights> const& inputs,\n+                                 std::vector<TRT_TensorOrWeights>* outputs) {\n+  if (inputs.size() != 2)\n+    return tensorflow::errors::FailedPrecondition(\n+        \"Binary ops require two tensor input, at \" + node_def.name());\n+\n+  if (inputs.at(0).is_weights() && inputs.at(1).is_weights())\n+    return ConstantFoldBinary(ctx, node_def, inputs, outputs);\n+\n+  if (inputs.at(0).is_tensor() && inputs.at(1).is_weights())\n+    return BinaryTensorOpWeight(ctx, node_def, inputs.at(0).tensor(),\n+                                inputs.at(1).weights(), outputs);\n+\n+  if (inputs.at(0).is_weights() && inputs.at(1).is_tensor())\n+    return BinaryTensorOpWeight(ctx, node_def, inputs.at(1).tensor(),\n+                                inputs.at(0).weights(), outputs);\n+\n+  if (inputs.at(0).is_tensor() && inputs.at(1).is_tensor())\n+    return BinaryTensorOpTensor(ctx, node_def, inputs.at(0).tensor(),\n+                                inputs.at(1).tensor(), outputs);\n+\n+  return tensorflow::errors::Unknown(\"Binary op input error, at \" +\n+                                     node_def.name());\n+}\n+\n+tensorflow::Status ConvertUnary(Converter& ctx,\n+                                const tensorflow::NodeDef& node_def,\n+                                std::vector<TRT_TensorOrWeights> const& inputs,\n+                                std::vector<TRT_TensorOrWeights>* outputs) {\n+  if (inputs.size() != 1)\n+    return tensorflow::errors::FailedPrecondition(\n+        \"Unary ops require single tensor input, at \" + node_def.name());\n+\n+  if (inputs.at(0).is_weights())\n+    return ConstantFoldUnary(ctx, node_def, inputs, outputs);\n+  else if (inputs.at(0).is_tensor())\n+    return tensorflow::errors::Unimplemented(\n+        \"Unary op for tensor not supported, at \" + node_def.name());\n+\n+  return tensorflow::errors::Unknown(\"Binary op input error, at \" +\n+                                     node_def.name());\n+}\n+\n+tensorflow::Status ConvertReduce(Converter& ctx,\n+                                 const tensorflow::NodeDef& node_def,\n+                                 std::vector<TRT_TensorOrWeights> const& inputs,\n+                                 std::vector<TRT_TensorOrWeights>* outputs) {\n+  if (inputs.size() != 2 || !inputs.at(0).is_tensor() ||\n+      !inputs.at(1).is_weights())\n+    return tensorflow::errors::InvalidArgument(\n+        \"Input expects tensor and weights, at\" + node_def.name());\n+\n+  // Implement tensor binaryOp weight [channel wise] for now;\n+  nvinfer1::ITensor const* tensor = inputs.at(0).tensor();\n+  auto dims = tensor->getDimensions();\n+  // Restore implicit batch dimension\n+  int nb_dims = dims.nbDims + 1;\n+\n+  TRT_ShapedWeights index_list = inputs.at(1).weights();\n+\n+  TFAttrs attrs(node_def);\n+  // TODO(jie): handle data type.\n+  // Index type here is done through TF type, so I can leverage their\n+  // EnumToDataType for my cast\n+  auto index_type = attrs.get<tensorflow::DataType>(\"Tidx\");\n+\n+  // Only expect to handle INT32 as attributes for now\n+  if (index_type != tensorflow::DataType::DT_INT32)\n+    return tensorflow::errors::Unimplemented(\"Tidx supports only DT_INT32\");\n+  auto index_list_data =\n+      static_cast<int*>(const_cast<void*>(index_list.GetValues()));\n+\n+  // Hack warning: have to fall back to pool layer since reduce is not in public\n+  // TRT yet.\n+  if (nb_dims != 4)\n+    return tensorflow::errors::InvalidArgument(\n+        \"TRT only support reduce on 4 dimensional tensors, at\" +\n+        node_def.name());\n+  if (index_list.count() > 2)\n+    return tensorflow::errors::InvalidArgument(\n+        \"TRT cannot support reduce on more than 2 dimensions, at\" +\n+        node_def.name());\n+\n+  std::set<int> idx_set;\n+  // We cannot operate on Channel. permutation flag used to transpose tensor\n+  int permuted_index = -1;\n+  for (int i = 0; i < index_list.count(); i++) {\n+    if (index_list_data[i] == 0)\n+      return tensorflow::errors::InvalidArgument(\"TRT cannot reduce at 0, at\" +\n+                                                 node_def.name());\n+    if (index_list_data[i] == 1) permuted_index = 1;\n+    idx_set.emplace(index_list_data[i]);\n+  }\n+\n+  std::vector<int> permutation_order(nb_dims);\n+  nvinfer1::DimsHW pool_kernel;\n+  if (permuted_index == 1) {\n+    for (int i = 2; i < nb_dims; i++) {\n+      if (idx_set.count(i)) {\n+        permuted_index = i;\n+        break;\n+      }\n+    }\n+    for (int i = 0; i < nb_dims; i++) permutation_order[i] = i;\n+\n+    permutation_order[permuted_index] = 1;\n+    permutation_order[1] = permuted_index;\n+\n+    // Apply permutation before extracting dimension for pool_kernel\n+    tensor = ctx.TransposeTensor(const_cast<nvinfer1::ITensor*>(tensor),\n+                                 permutation_order);\n+  }\n+\n+  // Apply permutation before extracting dimension for pool_kernel\n+  pool_kernel.d[0] = (idx_set.count(2) || permuted_index == 2) ? dims.d[1] : 1;\n+  pool_kernel.d[1] = (idx_set.count(3) || permuted_index == 3) ? dims.d[2] : 1;\n+\n+  nvinfer1::ITensor* output_tensor;\n+\n+  if (node_def.op() == \"Mean\") {\n+    nvinfer1::IPoolingLayer* layer =\n+        ctx.network()->addPooling(*const_cast<nvinfer1::ITensor*>(tensor),\n+                                  nvinfer1::PoolingType::kAVERAGE, pool_kernel);\n+    output_tensor = layer->getOutput(0);\n+  } else {\n+    return tensorflow::errors::Unimplemented(\n+        \"Op not supported \" + node_def.op() + \" , at \" + node_def.name());\n+  }\n+  if (permuted_index != -1) {\n+    // Apply permutation before extracting dimension for pool_kernel\n+    output_tensor = ctx.TransposeTensor(\n+        const_cast<nvinfer1::ITensor*>(output_tensor), permutation_order);\n+  }\n+  return tensorflow::Status::OK();\n+}\n+\n+tensorflow::Status ConvertPad(Converter& ctx,\n+                              const tensorflow::NodeDef& node_def,\n+                              std::vector<TRT_TensorOrWeights> const& inputs,\n+                              std::vector<TRT_TensorOrWeights>* outputs) {\n+  if (inputs.size() != 2 || !inputs.at(0).is_tensor() ||\n+      !inputs.at(1).is_weights())\n+    return tensorflow::errors::InvalidArgument(\n+        \"Input expects tensor and weights, at\" + node_def.name());\n+\n+  // Implement tensor binaryOp weight [channel wise] for now;\n+  nvinfer1::ITensor const* tensor = inputs.at(0).tensor();\n+  auto dims = tensor->getDimensions();\n+  // Restore implicit batch dimension\n+  int nb_dims = dims.nbDims + 1;\n+\n+  TRT_ShapedWeights pads = inputs.at(1).weights();\n+\n+  TFAttrs attrs(node_def);\n+  // Padding type here is done through TF type\n+  //   so I can leverage their EnumToDataType for my cast\n+  auto padding_type = attrs.get<tensorflow::DataType>(\"Tpaddings\");\n+  // TODO(jie): handle data type conversion for TRT?\n+\n+  if (pads.shape_.d[0] != nb_dims || pads.shape_.d[1] != 2)\n+    return tensorflow::errors::InvalidArgument(\n+        \"Pad only supports explicit padding on 4 dimensional tensor, at \" +\n+        node_def.name());\n+\n+  // Only expect to handle INT32 as attributes for now\n+  if (padding_type != tensorflow::DataType::DT_INT32)\n+    return tensorflow::errors::Unimplemented(\n+        \"Tpaddings supports only DT_INT32\");\n+  auto pad_data = static_cast<int*>(const_cast<void*>(pads.GetValues()));\n+\n+  std::vector<int32_t> pad_index;\n+  for (int i = 0; i < nb_dims; i++) {\n+    if (pad_data[2 * i] != 0 || pad_data[2 * i + 1] != 0)\n+      pad_index.push_back(i);\n+  }\n+\n+  // No padding at all, we should exit\n+  if (pad_index.size() == 0) {\n+    outputs->push_back(inputs.at(0));\n+    return tensorflow::Status::OK();\n+  }\n+\n+  // Only supports padding on less than 2 axis GIE-2579\n+  if (pad_index.size() > 2)\n+    return tensorflow::errors::InvalidArgument(\n+        \"Padding layer does not support padding on > 2\");\n+\n+  // Padding on batch dimension is not supported\n+  if (pad_index[0] == 0)\n+    return tensorflow::errors::InvalidArgument(\n+        \"Padding layer does not support padding on batch dimension\");\n+\n+  // Not doing the legit thing here. ignoring padding on dim 1 and 3;\n+  // TODO(jie): implement pad as uff parser\n+  if (pad_index.size() == 2 && pad_index[0] == 0 && pad_index[1] == 3)\n+    return tensorflow::errors::Unimplemented(\n+        \"Padding layer does not support padding on dimension 1 and 3 yet\");\n+\n+  bool legit_pad = true;\n+  nvinfer1::DimsHW pre_padding(0, 0);\n+  nvinfer1::DimsHW post_padding(0, 0);\n+\n+  std::vector<int32_t> permuted_pad_index(pad_index);\n+  if (pad_index[0] == 1) {\n+    legit_pad = false;\n+    tensor = ctx.TransposeTensor(const_cast<nvinfer1::ITensor*>(tensor),\n+                                 {0, 3, 2, 1});\n+    permuted_pad_index[0] = 3;\n+  }\n+\n+  for (size_t i = 0; i < pad_index.size(); i++) {\n+    int index = pad_index[i];\n+    if (permuted_pad_index[i] == 2) {\n+      pre_padding.h() = pad_data[index * 2];\n+      post_padding.h() = pad_data[index * 2 + 1];\n+    } else if (permuted_pad_index[i] == 3) {\n+      pre_padding.w() = pad_data[index * 2];\n+      post_padding.w() = pad_data[index * 2 + 1];\n+    }\n+  }\n+\n+  nvinfer1::IPaddingLayer* layer = ctx.network()->addPadding(\n+      *const_cast<nvinfer1::ITensor*>(tensor), pre_padding, post_padding);\n+  nvinfer1::ITensor* output_tensor = layer->getOutput(0);\n+\n+  if (!legit_pad)\n+    output_tensor = ctx.TransposeTensor(\n+        const_cast<nvinfer1::ITensor*>(output_tensor), {0, 3, 2, 1});\n+\n+  outputs->push_back(TRT_TensorOrWeights(output_tensor));\n+  return tensorflow::Status::OK();\n+}\n+\n+void Converter::register_op_converters() {\n+  // vgg_16 slim implementation\n+  _op_registry[\"Placeholder\"] = ConvertPlaceholder;\n+  _op_registry[\"Conv2D\"] = ConvertConv2D;\n+  _op_registry[\"Relu\"] = ConvertActivation;\n+  _op_registry[\"MaxPool\"] = ConvertPool;\n+  // This could be really handled as ConvertBinary\n+  _op_registry[\"BiasAdd\"] = ConvertScale;\n+  _op_registry[\"Const\"] = ConvertConst;\n+  // _op_registry[\"MatMul\"] = ConvertFullyConnected;  // Not used in vgg\n+  // TODO(ben,jie): this is a temp hack.\n+  _op_registry[\"Identity\"] = ConvertIdentity;  // Identity should be removed\n+  // _op_registry[\"AvgPool\"] = ConvertPool;\n+\n+  // resnet_50_v1 slim implementation\n+  _op_registry[\"Add\"] = ConvertBinary;\n+  _op_registry[\"Mul\"] = ConvertBinary;\n+  _op_registry[\"Sub\"] = ConvertBinary;\n+  _op_registry[\"Rsqrt\"] = ConvertUnary;\n+  _op_registry[\"Mean\"] = ConvertReduce;\n+  _op_registry[\"Pad\"] = ConvertPad;\n+  // TODO(ben,jie): Add more ops\n+}\n+\n+}  // namespace\n+\n+tensorflow::Status ConvertSubGraphToTensorRTNodeDef(\n+    const tensorflow::Graph& graph, const std::set<int>& subgraph_node_ids,\n+    const std::vector<std::pair<int, int>>& input_inds,\n+    const std::vector<std::pair<int, int>>& output_inds, size_t max_batch_size,\n+    size_t max_workspace_size_bytes,\n+    const tensorflow::grappler::GraphProperties& graph_properties,\n+    tensorflow::NodeDef* trt_node) {\n+  // Visit nodes in reverse topological order and construct the TRT network.\n+\n+  // Toposort\n+  std::vector<tensorflow::Node*> order_vec;\n+  tensorflow::GetPostOrder(graph, &order_vec);\n+  // Select just the subgraph\n+  std::list<tensorflow::Node*> order;\n+  for (tensorflow::Node* node : order_vec) {\n+    if (subgraph_node_ids.count(node->id())) {\n+      // We want topological order to contstruct the\n+      // network layer by layer\n+      order.push_front(node);\n+    }\n+  }\n+  // Topological order is needed to build TRT network\n+\n+  tensorflow::tensorrt::Logger trt_logger;\n+\n+  auto trt_builder = infer_object(nvinfer1::createInferBuilder(trt_logger));\n+  if (!trt_builder) {\n+    return tensorflow::errors::Internal(\n+        \"Failed to create TensorRT builder object\");\n+  }\n+\n+  auto trt_network = infer_object(trt_builder->createNetwork());\n+  if (!trt_network) {\n+    return tensorflow::errors::Internal(\n+        \"Failed to create TensorRT network object\");\n+  }\n+\n+  // Build the network\n+  Converter converter(trt_network.get());\n+\n+  std::vector<string> input_names;\n+  std::vector<tensorflow::DataType> input_dtypes;\n+  for (std::pair<int, int> const& input : input_inds) {\n+    int node_id = input.first;\n+    int output_idx = input.second;\n+    tensorflow::Node* node = graph.FindNodeId(node_id);\n+    auto node_name = node->name();\n+    input_names.push_back(node_name);  // Insert original node name without port\n+    // TODO(jie): alternative :)\n+    if (!graph_properties.HasOutputProperties(node_name))\n+      return tensorflow::errors::Internal(\"Failed to find input node: \" +\n+                                          node_name);\n+\n+    auto op_info_vec = graph_properties.GetOutputProperties(node_name);\n+    if (static_cast<int>(op_info_vec.size()) < output_idx)\n+      return tensorflow::errors::Internal(\n+          \"Accessing output index of: \" + std::to_string(output_idx) +\n+          \", at node: \" + node_name + \" with output entry from shape_map: \" +\n+          std::to_string(op_info_vec.size()));\n+\n+    auto op_info = op_info_vec.at(output_idx);\n+\n+    tensorflow::DataType tf_dtype = op_info.dtype();\n+    input_dtypes.push_back(tf_dtype);\n+\n+    nvinfer1::DataType dtype(nvinfer1::DataType::kFLOAT);\n+    TF_CHECK_OK(ConvertDType(tf_dtype, &dtype));\n+\n+    VLOG(2) << \"Accessing output index of: \" << std::to_string(output_idx)\n+            << \", at node: \" << node_name\n+            << \" with output entry from shape_map: \"\n+            << std::to_string(op_info_vec.size());\n+\n+    // TODO(ben,jie): update TRT input format/dimension\n+    nvinfer1::DimsCHW input_dim_psuedo_chw;\n+    for (int i = 0; i < 3; i++) input_dim_psuedo_chw.d[i] = 1;\n+\n+    for (int i = 1; i < op_info.shape().dim_size(); i++) {\n+      VLOG(2) << \"dimension: \" << i\n+              << \" , size: \" << op_info.shape().dim(i).size();\n+      input_dim_psuedo_chw.d[i - 1] = op_info.shape().dim(i).size();\n+    }\n+\n+    // TODO(ben,jie): proper way to restore input tensor name?\n+    auto input_tensor_name = node_name;\n+    if (output_idx != 0)\n+      input_tensor_name = node_name + \":\" + std::to_string(output_idx);\n+\n+    nvinfer1::ITensor* input_tensor = converter.network()->addInput(\n+        input_tensor_name.c_str(), dtype, input_dim_psuedo_chw);\n+\n+    if (!input_tensor)\n+      return tensorflow::errors::InvalidArgument(\n+          \"Failed to create Input layer\");\n+    VLOG(2) << \"Input tensor name :\" << input_tensor_name;\n+\n+    if (!converter.insert_input_tensor(input_tensor_name, input_tensor))\n+      return tensorflow::errors::AlreadyExists(\n+          \"Output tensor already exists for op: \" + input_tensor_name);\n+  }\n+\n+  VLOG(2) << \"Finished sorting\";\n+\n+  for (const tensorflow::Node* node : order) {\n+    const tensorflow::NodeDef& node_def = node->def();\n+    VLOG(2) << \"Converting node: \" << node_def.name() << \" , \" << node_def.op();\n+    TF_RETURN_IF_ERROR(converter.convert_node(node_def));\n+  }\n+\n+  VLOG(2) << \"Finished conversion\";\n+\n+  // Gather output metadata\n+  std::vector<string> output_names;\n+  std::vector<tensorflow::DataType> output_dtypes;\n+  for (std::pair<int, int> const& output : output_inds) {\n+    int node_id = output.first;\n+    int output_idx = output.second;\n+    tensorflow::Node* node = graph.FindNodeId(node_id);\n+    string op_name = node->name();\n+    string tensor_name = op_name;\n+    if (output_idx != 0)\n+      tensor_name = tensor_name + \":\" + std::to_string(output_idx);\n+    VLOG(2) << \"Output tensor name: \" << tensor_name;\n+    output_names.push_back(tensor_name);\n+    auto tensor_or_weights = converter.get_tensor(tensor_name);\n+    if (!tensor_or_weights.is_tensor()) {\n+      return tensorflow::errors::InvalidArgument(\n+          \"Output node is weights not tensor\");\n+    }\n+    nvinfer1::ITensor* tensor = tensor_or_weights.tensor();\n+    if (!tensor) {\n+      return tensorflow::errors::NotFound(\"Output tensor not found: \" +\n+                                          tensor_name);\n+    }\n+    converter.network()->markOutput(*tensor);\n+    tensorflow::DataType tf_dtype = node->output_type(output_idx);\n+    output_dtypes.push_back(tf_dtype);\n+    nvinfer1::DataType trt_dtype = nvinfer1::DataType::kFLOAT;\n+    TF_RETURN_IF_ERROR(ConvertDType(tf_dtype, &trt_dtype));\n+    tensor->setType(trt_dtype);\n+  }\n+\n+  VLOG(2) << \"Finished output\";\n+  static int static_id = 0;", "path": "tensorflow/contrib/tensorrt/convert/convert_nodes.cc", "position": null, "original_position": 1603, "commit_id": "1e4b5b8c0cc1675b9ecac3569c91563a2a4f9984", "original_commit_id": "cfa374cefe132be886c26a374c51454177c68868", "user": {"login": "jjsjann123", "id": 3709243, "node_id": "MDQ6VXNlcjM3MDkyNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjsjann123", "html_url": "https://github.com/jjsjann123", "followers_url": "https://api.github.com/users/jjsjann123/followers", "following_url": "https://api.github.com/users/jjsjann123/following{/other_user}", "gists_url": "https://api.github.com/users/jjsjann123/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjsjann123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjsjann123/subscriptions", "organizations_url": "https://api.github.com/users/jjsjann123/orgs", "repos_url": "https://api.github.com/users/jjsjann123/repos", "events_url": "https://api.github.com/users/jjsjann123/events{/privacy}", "received_events_url": "https://api.github.com/users/jjsjann123/received_events", "type": "User", "site_admin": false}, "body": "This is a temporary thing here.\r\nWe would refactor to use Tensorflow naming mechanics in the future. Will put TODO here.", "created_at": "2018-02-07T19:00:21Z", "updated_at": "2018-02-12T23:36:57Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16253#discussion_r166720329", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16253", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/166720329"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16253#discussion_r166720329"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16253"}}, "body_html": "<p>This is a temporary thing here.<br>\nWe would refactor to use Tensorflow naming mechanics in the future. Will put TODO here.</p>", "body_text": "This is a temporary thing here.\nWe would refactor to use Tensorflow naming mechanics in the future. Will put TODO here.", "in_reply_to_id": 166713750}