{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/166499998", "pull_request_review_id": 94567568, "id": 166499998, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjQ5OTk5OA==", "diff_hunk": "@@ -0,0 +1,1634 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/contrib/tensorrt/convert/convert_nodes.h\"\n+\n+#include <algorithm>\n+#include <list>\n+#include <map>\n+#include <memory>\n+#include <set>\n+#include <unordered_map>\n+#include <utility>\n+#include <vector>\n+\n+#include \"tensorflow/core/framework/attr_value.pb.h\"\n+#include \"tensorflow/core/framework/graph.pb.h\"\n+#include \"tensorflow/core/framework/node_def.pb.h\"\n+#include \"tensorflow/core/framework/node_def_builder.h\"\n+#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/framework/types.pb.h\"\n+#include \"tensorflow/core/graph/algorithm.h\"\n+#include \"tensorflow/core/graph/graph.h\"\n+#include \"tensorflow/core/graph/graph_constructor.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/lib/strings/strcat.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/tensor_coding.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+\n+#if GOOGLE_CUDA\n+#if GOOGLE_TENSORRT\n+#include \"tensorflow/contrib/tensorrt/log/trt_logger.h\"\n+#include \"tensorrt/include/NvInfer.h\"\n+\n+//  Check if the types are equal. Cast to int first so that failure log message\n+//  would work!\n+#define CHECK_EQ_TYPE(val1, val2) CHECK_EQ((int)val1, (int)val2)\n+\n+namespace tensorflow {\n+namespace tensorrt {\n+namespace convert {\n+\n+namespace {\n+\n+inline tensorflow::Status ConvertDType(tensorflow::DataType tf_dtype,\n+                                       nvinfer1::DataType* trt_dtype) {\n+  switch (tf_dtype) {\n+    case tensorflow::DataType::DT_FLOAT:\n+      *trt_dtype = nvinfer1::DataType::kFLOAT;\n+      break;\n+    case tensorflow::DataType::DT_INT8:\n+      *trt_dtype = nvinfer1::DataType::kINT8;\n+      break;\n+    case tensorflow::DataType::DT_HALF:\n+      *trt_dtype = nvinfer1::DataType::kHALF;\n+      break;\n+    default:\n+      return tensorflow::errors::InvalidArgument(\"Unsupported data type\");\n+  }\n+  return tensorflow::Status::OK();\n+}\n+\n+inline nvinfer1::Dims GetTensorShape(const tensorflow::Tensor& tensor) {\n+  nvinfer1::Dims dims;\n+  dims.nbDims = tensor.dims();\n+  for (int i = 0; i < dims.nbDims; i++) {\n+    dims.d[i] = tensor.dim_size(i);\n+  }\n+  return dims;\n+}\n+\n+inline int64_t GetShapeSize(nvinfer1::Dims shape) {\n+  // Returns total number of elements in shape\n+  int64_t count = 1;\n+  for (int d = 0; d < shape.nbDims; ++d) {\n+    count *= shape.d[d];\n+  }\n+  return count;\n+}\n+\n+static std::vector<std::pair<int, int>> CreateSamePadding(\n+    const nvinfer1::DimsHW& stride, const nvinfer1::DimsHW& kernel,\n+    const std::vector<int64_t>& input_dims) {\n+  std::vector<std::pair<int, int>> padding(input_dims.size());\n+  CHECK_EQ((size_t)stride.nbDims, input_dims.size());  // TODO(jie): N+C? NC+?\n+\n+  for (size_t i = 0; i < input_dims.size(); ++i) {\n+    // Formula to calculate the padding\n+    int p = ((input_dims[i] - 1) / stride.d[i]) * stride.d[i] + kernel.d[i] -\n+            input_dims[i];\n+    p = (p > 0) ? p : 0;\n+\n+    // Right precedence padding, like in TensorFlow\n+    int left = p / 2;\n+    int right = p - left;\n+\n+    VLOG(2) << \"PADDING_\" << i << \" pre: \" << left << \", post: \" << right\n+            << \"paras: \" << input_dims[i] << \", \" << stride.d[i] << \", \"\n+            << \"kernel: \" << kernel.d[i];\n+    padding[i] = {left, right};\n+  }\n+  return padding;\n+}\n+\n+class TRT_ShapedWeights {\n+ public:\n+  TRT_ShapedWeights(tensorflow::DataType type, const void* values,\n+                    nvinfer1::Dims shape, bool owned_values = false)\n+      : shape_(shape),\n+        type_(type),\n+        values_(values),\n+        owned_values_(owned_values),\n+        dummy_flag_(false) {\n+    // Note: this->shape.type[] is not used\n+  }\n+\n+  explicit TRT_ShapedWeights(tensorflow::DataType type)\n+      : type_(type),\n+        values_(nullptr),\n+        owned_values_(false),\n+        dummy_flag_(true) {}\n+\n+  ~TRT_ShapedWeights() {\n+    if (values_ && owned_values_) delete static_cast<const char*>(values_);\n+  }\n+\n+  TRT_ShapedWeights(const TRT_ShapedWeights&) = default;\n+\n+  int64_t count() const {\n+    int64_t c = 1;\n+    for (int i = 0; i < shape_.nbDims; i++) c *= shape_.d[i];\n+    return c;\n+  }\n+\n+  nvinfer1::Weights GetWeightsForTRT() const {\n+    nvinfer1::DataType trt_type(nvinfer1::DataType::kFLOAT);\n+    TF_CHECK_OK(ConvertDType(type_, &trt_type));\n+    if (dummy_flag_) return nvinfer1::Weights{trt_type, nullptr, 0};\n+\n+    // Note: this->shape.type[] is not used\n+    return nvinfer1::Weights{trt_type, values_, GetShapeSize(shape_)};\n+  }\n+\n+  size_t size_bytes() const {\n+    int type_size = tensorflow::DataTypeSize(this->type_);\n+    return this->count() * type_size;\n+  }\n+\n+  // Default converter\n+  operator nvinfer1::Weights() const { return GetWeightsForTRT(); }\n+\n+  nvinfer1::Dims shape_;\n+  tensorflow::DataType type_;\n+  const void* values_;\n+  bool owned_values_;\n+  bool dummy_flag_;\n+};\n+\n+class TRT_TensorOrWeights {\n+ public:\n+  explicit TRT_TensorOrWeights(nvinfer1::ITensor* tensor)\n+      : _tensor_(tensor), _variant_(TRT_NODE_TENSOR) {}\n+  TRT_TensorOrWeights(const TRT_ShapedWeights& weights)\n+      : _weights_(weights), _variant_(TRT_NODE_WEIGHTS) {}\n+  ~TRT_TensorOrWeights() {}\n+\n+  bool is_tensor() const { return _variant_ == TRT_NODE_TENSOR; }\n+  bool is_weights() const { return _variant_ == TRT_NODE_WEIGHTS; }\n+\n+  nvinfer1::ITensor* tensor() {\n+    CHECK_EQ(this->is_tensor(), true);\n+    return _tensor_;\n+  }\n+  nvinfer1::ITensor const* tensor() const {\n+    CHECK_EQ(this->is_tensor(), true);\n+    return _tensor_;\n+  }\n+  TRT_ShapedWeights& weights() {\n+    CHECK_EQ(this->is_weights(), true);\n+    return _weights_;\n+  }\n+  const TRT_ShapedWeights& weights() const {\n+    CHECK_EQ(this->is_weights(), true);\n+    return _weights_;\n+  }\n+  nvinfer1::Dims shape() const {\n+    if (this->is_tensor()) {\n+      return this->tensor()->getDimensions();\n+    } else {\n+      return this->weights().shape_;\n+    }\n+  }\n+\n+ private:\n+  union {\n+    nvinfer1::ITensor* _tensor_;\n+    TRT_ShapedWeights _weights_;\n+  };\n+  enum { TRT_NODE_TENSOR, TRT_NODE_WEIGHTS } _variant_;\n+};\n+\n+class TRT_LayerOrWeights {\n+ public:\n+  explicit TRT_LayerOrWeights(nvinfer1::ILayer* layer)\n+      : _layer_(layer), _variant_(TRT_NODE_LAYER) {}\n+  explicit TRT_LayerOrWeights(const TRT_ShapedWeights& weights)\n+      : _weights_(weights), _variant_(TRT_NODE_WEIGHTS) {}\n+  bool is_layer() const { return _variant_ == TRT_NODE_LAYER; }\n+  bool is_weights() const { return _variant_ == TRT_NODE_WEIGHTS; }\n+  nvinfer1::ILayer* layer() {\n+    CHECK_EQ(this->is_layer(), true);\n+    return _layer_;\n+  }\n+  TRT_ShapedWeights& weights() {\n+    CHECK_EQ(this->is_weights(), true);\n+    return _weights_;\n+  }\n+  TRT_TensorOrWeights output(int index = 0) const {\n+    if (this->is_layer()) {\n+      nvinfer1::ITensor* tensor = _layer_->getOutput(index);\n+      return TRT_TensorOrWeights(tensor);\n+    } else {\n+      CHECK_EQ(index, 0);\n+      return TRT_TensorOrWeights(_weights_);\n+    }\n+  }\n+\n+ private:\n+  union {\n+    nvinfer1::ILayer* _layer_;\n+    TRT_ShapedWeights _weights_;\n+  };\n+  enum { TRT_NODE_LAYER, TRT_NODE_WEIGHTS } _variant_;\n+};\n+\n+class TFAttrs {\n+ public:\n+  explicit TFAttrs(const tensorflow::NodeDef& tf_node) {\n+    for (const auto& attr : tf_node.attr()) {\n+      _attrs.insert({attr.first, &attr.second});\n+    }\n+  }\n+  bool count(string key) const { return _attrs.count(key); }\n+  tensorflow::AttrValue const* at(string key) const {\n+    if (!_attrs.count(key)) {\n+      LOG(FATAL) << \"Attribute not found: \" << key;\n+    }\n+    return _attrs.at(key);\n+  }\n+  template <typename T>\n+  T get(string key) const;\n+  template <typename T>\n+  T get(string key, const T& default_value) const {\n+    return _attrs.count(key) ? this->get<T>(key) : default_value;\n+  }\n+\n+ private:\n+  typedef std::map<string, tensorflow::AttrValue const*> AttrMap;\n+  AttrMap _attrs;\n+};\n+\n+template <>\n+string TFAttrs::get<string>(string key) const {\n+  return this->at(key)->s();\n+}\n+\n+template <>\n+std::vector<int> TFAttrs::get<std::vector<int>>(string key) const {\n+  auto attr = this->at(key)->list().i();\n+  return std::vector<int>(attr.begin(), attr.end());\n+}\n+\n+template <>\n+nvinfer1::Dims TFAttrs::get<nvinfer1::Dims>(string key) const {\n+  auto values = this->get<std::vector<int>>(key);\n+  nvinfer1::Dims dims;\n+  dims.nbDims = values.size();\n+  std::copy(values.begin(), values.end(), dims.d);\n+  // Note: No dimension type information is included\n+  return dims;\n+}\n+\n+template <>\n+nvinfer1::DataType TFAttrs::get<nvinfer1::DataType>(string key) const {\n+  nvinfer1::DataType trt_dtype(nvinfer1::DataType::kFLOAT);\n+  TF_CHECK_OK(ConvertDType(this->at(key)->type(), &trt_dtype));\n+  return trt_dtype;\n+}\n+\n+template <>\n+tensorflow::DataType TFAttrs::get<tensorflow::DataType>(string key) const {\n+  return this->at(key)->type();\n+}\n+\n+template <typename T>\n+void Reorder4(nvinfer1::DimsNCHW shape, T const* idata,\n+              nvinfer1::DimsNCHW istrides, T* odata,\n+              nvinfer1::DimsNCHW ostrides) {\n+  for (int n = 0; n < shape.n(); ++n) {\n+    for (int c = 0; c < shape.c(); ++c) {\n+      for (int h = 0; h < shape.h(); ++h) {\n+        for (int w = 0; w < shape.w(); ++w) {\n+          odata[n * ostrides.n() + c * ostrides.c() + h * ostrides.h() +\n+                w * ostrides.w()] = idata[n * istrides.n() + c * istrides.c() +\n+                                          h * istrides.h() + w * istrides.w()];\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void ReorderRSCKToKCRS(const TRT_ShapedWeights& iweights,\n+                       TRT_ShapedWeights* oweights) {\n+  CHECK_EQ(iweights.type_, oweights->type_);\n+  CHECK_EQ(iweights.size_bytes(), oweights->size_bytes());\n+  int r = iweights.shape_.d[0];\n+  int s = iweights.shape_.d[1];\n+  int c = iweights.shape_.d[2];\n+  int k = iweights.shape_.d[3];\n+  oweights->shape_.d[0] = k;\n+  oweights->shape_.d[1] = c;\n+  oweights->shape_.d[2] = r;\n+  oweights->shape_.d[3] = s;\n+  nvinfer1::DimsNCHW istrides = {1, k, s * k * c, c * k};\n+  nvinfer1::DimsNCHW ostrides = {c * r * s, r * s, s, 1};\n+  switch (iweights.type_) {\n+    case tensorflow::DataType::DT_FLOAT:\n+      Reorder4(\n+          {k, c, r, s}, static_cast<float const*>(iweights.values_), istrides,\n+          static_cast<float*>(const_cast<void*>(oweights->values_)), ostrides);\n+      break;\n+    default:\n+      LOG(FATAL) << \"!!!!!!!!!!!!!!!!!!!!!!!!broke!!!!!!!!!!!!\";\n+  }\n+}\n+\n+struct InferDeleter {\n+  template <typename T>\n+  void operator()(T* obj) const {\n+    if (obj) {\n+      obj->destroy();\n+    }\n+  }\n+};\n+\n+template <typename T>\n+inline std::shared_ptr<T> infer_object(T* obj) {\n+  return std::shared_ptr<T>(obj, InferDeleter());\n+}\n+\n+// Logger for GIE info/warning/errors\n+class Converter;\n+\n+using OpConverter =\n+    std::function<tensorflow::Status(Converter&, const tensorflow::NodeDef&,\n+                                     std::vector<TRT_TensorOrWeights> const&,\n+                                     std::vector<TRT_TensorOrWeights>*)>;\n+\n+class Converter {\n+  std::unordered_map<string, TRT_TensorOrWeights> _trt_tensors;", "path": "tensorflow/contrib/tensorrt/convert/convert_nodes.cc", "position": null, "original_position": 374, "commit_id": "1e4b5b8c0cc1675b9ecac3569c91563a2a4f9984", "original_commit_id": "bfe8b85cad3be1a82234500fce3064c98dd20d09", "user": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "body": "Please fix the naming..", "created_at": "2018-02-07T01:56:12Z", "updated_at": "2018-02-12T23:36:57Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16253#discussion_r166499998", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16253", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/166499998"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16253#discussion_r166499998"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16253"}}, "body_html": "<p>Please fix the naming..</p>", "body_text": "Please fix the naming.."}