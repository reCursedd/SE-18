{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/372546360", "html_url": "https://github.com/tensorflow/tensorflow/issues/10842#issuecomment-372546360", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10842", "id": 372546360, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjU0NjM2MA==", "user": {"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-13T04:51:21Z", "updated_at": "2018-03-13T04:51:21Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>Sorry for a late reply. Here's my brief answer:</p>\n<ul>\n<li>\n<p>Local attention, predictive alignments: unfortunately, we don't have the bandwidth for this. Local attention, while being an interesting idea, is more complicated to implement and train. Contributions are welcome though I expect the contributors need to demonstrate convincing gains on over nowadays strong <a href=\"github.com/tensorflow/nmt\">baselines</a> and new <a href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow\">advances in attention</a>.</p>\n</li>\n<li>\n<p>Input-feeding approach: yes, we missed the tanh() part but the performance is found to be similar. See answers in  <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"300774228\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/nmt/issues/257\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/nmt/issues/257/hovercard\" href=\"https://github.com/tensorflow/nmt/issues/257\">tensorflow/nmt#257</a>.</p>\n</li>\n<li>\n<p>Different scoring functions: in fact, we are implementing the general approach for Luong attention, see this <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L402\">line</a>. Similarly for Bahdanau attention if you read the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L546\">code</a> carefully.</p>\n</li>\n</ul>\n<p>Hope that clarifies!</p>", "body_text": "Hi all,\nSorry for a late reply. Here's my brief answer:\n\n\nLocal attention, predictive alignments: unfortunately, we don't have the bandwidth for this. Local attention, while being an interesting idea, is more complicated to implement and train. Contributions are welcome though I expect the contributors need to demonstrate convincing gains on over nowadays strong baselines and new advances in attention.\n\n\nInput-feeding approach: yes, we missed the tanh() part but the performance is found to be similar. See answers in  tensorflow/nmt#257.\n\n\nDifferent scoring functions: in fact, we are implementing the general approach for Luong attention, see this line. Similarly for Bahdanau attention if you read the code carefully.\n\n\nHope that clarifies!", "body": "Hi all,\r\n\r\nSorry for a late reply. Here's my brief answer:\r\n* Local attention, predictive alignments: unfortunately, we don't have the bandwidth for this. Local attention, while being an interesting idea, is more complicated to implement and train. Contributions are welcome though I expect the contributors need to demonstrate convincing gains on over nowadays strong [baselines](github.com/tensorflow/nmt) and new [advances in attention](https://arxiv.org/abs/1706.03762).\r\n\r\n* Input-feeding approach: yes, we missed the tanh() part but the performance is found to be similar. See answers in  https://github.com/tensorflow/nmt/issues/257.\r\n\r\n* Different scoring functions: in fact, we are implementing the general approach for Luong attention, see this [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L402). Similarly for Bahdanau attention if you read the [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L546) carefully. \r\n\r\nHope that clarifies!\r\n"}