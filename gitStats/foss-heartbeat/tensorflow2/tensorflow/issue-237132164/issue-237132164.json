{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10842", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10842/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10842/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10842/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10842", "id": 237132164, "node_id": "MDU6SXNzdWUyMzcxMzIxNjQ=", "number": 10842, "title": "Feature request: add more options to Luong attention", "user": {"login": "danielwatson6", "id": 3270063, "node_id": "MDQ6VXNlcjMyNzAwNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3270063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielwatson6", "html_url": "https://github.com/danielwatson6", "followers_url": "https://api.github.com/users/danielwatson6/followers", "following_url": "https://api.github.com/users/danielwatson6/following{/other_user}", "gists_url": "https://api.github.com/users/danielwatson6/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielwatson6/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielwatson6/subscriptions", "organizations_url": "https://api.github.com/users/danielwatson6/orgs", "repos_url": "https://api.github.com/users/danielwatson6/repos", "events_url": "https://api.github.com/users/danielwatson6/events{/privacy}", "received_events_url": "https://api.github.com/users/danielwatson6/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}, {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2017-06-20T08:32:56Z", "updated_at": "2018-05-27T06:13:07Z", "closed_at": "2018-05-27T06:13:07Z", "author_association": "NONE", "body_html": "<p>Currently, the best results of the Luong attention paper <a href=\"https://arxiv.org/abs/1508.04025\" rel=\"nofollow\">(Minh-Thang Luong, Hieu Pham, Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" EMNLP 2015.)</a> cannot be reproduced with the implementation of <code>tf.contrib.seq2seq.LuongAttention</code>. Features that are missing:</p>\n<ul>\n<li><strong>Local attention</strong>: attend to a window of time steps, rather than to all of the time steps of the encoder output (global attention). The window size should be a hyperparameter that the user can tune.</li>\n<li><strong>Different scoring functions</strong>: currently, the scoring function is limited to a dot products between each encoder output and the decoder output. The paper shows better results with \"general\" scoring (all of the encoder outputs are multiplied by one learnable matrix) and also explores the option of using Bahdanau-like scoring (concatenate and multiply by a learnable matrix, then apply tanh and take a dot product with a learnable vector).</li>\n<li><strong>Predictive alignments</strong>: while the probability function can be replaced, it would be nice to add predictive alignment as a function, and make the implementation of both monotonic and predictive alignments behave well with local attention limited to a time window (changes shape of learnables).</li>\n<li><strong>Input-feeding approach</strong>: (please correct me in this one if I am mistaken) the current implementation is missing the final step that computes a prediction by concatenating the context vector with the decoder output, weights them and applies tanh (let this be <code>s_t=tanh(W [c_t; h_t])</code>). Passing <code>s_t</code> through a softmax layer gives the prediction distribution, but passing it as is to the next input improved performance in the paper.</li>\n</ul>\n<p>On a sidenote: it also seems to be that there is no difference in the key and query vectors between the Bahdanau and Luong attention mechanisms, when there should be. Bahdanau attention has a computation pathway starting from the previous decoder output <code>h_{t-1} -&gt; a_t -&gt; c_t -&gt; h_t</code>, while Luong attention starts from the current output <code>h_t -&gt; a_t -&gt; c_t -&gt; s_t</code>.</p>", "body_text": "Currently, the best results of the Luong attention paper (Minh-Thang Luong, Hieu Pham, Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" EMNLP 2015.) cannot be reproduced with the implementation of tf.contrib.seq2seq.LuongAttention. Features that are missing:\n\nLocal attention: attend to a window of time steps, rather than to all of the time steps of the encoder output (global attention). The window size should be a hyperparameter that the user can tune.\nDifferent scoring functions: currently, the scoring function is limited to a dot products between each encoder output and the decoder output. The paper shows better results with \"general\" scoring (all of the encoder outputs are multiplied by one learnable matrix) and also explores the option of using Bahdanau-like scoring (concatenate and multiply by a learnable matrix, then apply tanh and take a dot product with a learnable vector).\nPredictive alignments: while the probability function can be replaced, it would be nice to add predictive alignment as a function, and make the implementation of both monotonic and predictive alignments behave well with local attention limited to a time window (changes shape of learnables).\nInput-feeding approach: (please correct me in this one if I am mistaken) the current implementation is missing the final step that computes a prediction by concatenating the context vector with the decoder output, weights them and applies tanh (let this be s_t=tanh(W [c_t; h_t])). Passing s_t through a softmax layer gives the prediction distribution, but passing it as is to the next input improved performance in the paper.\n\nOn a sidenote: it also seems to be that there is no difference in the key and query vectors between the Bahdanau and Luong attention mechanisms, when there should be. Bahdanau attention has a computation pathway starting from the previous decoder output h_{t-1} -> a_t -> c_t -> h_t, while Luong attention starts from the current output h_t -> a_t -> c_t -> s_t.", "body": "Currently, the best results of the Luong attention paper [(Minh-Thang Luong, Hieu Pham, Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" EMNLP 2015.)](https://arxiv.org/abs/1508.04025) cannot be reproduced with the implementation of `tf.contrib.seq2seq.LuongAttention`. Features that are missing:\r\n\r\n- **Local attention**: attend to a window of time steps, rather than to all of the time steps of the encoder output (global attention). The window size should be a hyperparameter that the user can tune.\r\n- **Different scoring functions**: currently, the scoring function is limited to a dot products between each encoder output and the decoder output. The paper shows better results with \"general\" scoring (all of the encoder outputs are multiplied by one learnable matrix) and also explores the option of using Bahdanau-like scoring (concatenate and multiply by a learnable matrix, then apply tanh and take a dot product with a learnable vector).\r\n- **Predictive alignments**: while the probability function can be replaced, it would be nice to add predictive alignment as a function, and make the implementation of both monotonic and predictive alignments behave well with local attention limited to a time window (changes shape of learnables).\r\n- **Input-feeding approach**: (please correct me in this one if I am mistaken) the current implementation is missing the final step that computes a prediction by concatenating the context vector with the decoder output, weights them and applies tanh (let this be `s_t=tanh(W [c_t; h_t])`). Passing `s_t` through a softmax layer gives the prediction distribution, but passing it as is to the next input improved performance in the paper.\r\n\r\nOn a sidenote: it also seems to be that there is no difference in the key and query vectors between the Bahdanau and Luong attention mechanisms, when there should be. Bahdanau attention has a computation pathway starting from the previous decoder output `h_{t-1} -> a_t -> c_t -> h_t`, while Luong attention starts from the current output `h_t -> a_t -> c_t -> s_t`."}