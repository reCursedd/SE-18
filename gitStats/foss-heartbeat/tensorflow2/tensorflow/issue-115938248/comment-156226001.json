{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/156226001", "html_url": "https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156226001", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/25", "id": 156226001, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NjIyNjAwMQ==", "user": {"login": "erikbern", "id": 1027979, "node_id": "MDQ6VXNlcjEwMjc5Nzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1027979?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erikbern", "html_url": "https://github.com/erikbern", "followers_url": "https://api.github.com/users/erikbern/followers", "following_url": "https://api.github.com/users/erikbern/following{/other_user}", "gists_url": "https://api.github.com/users/erikbern/gists{/gist_id}", "starred_url": "https://api.github.com/users/erikbern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erikbern/subscriptions", "organizations_url": "https://api.github.com/users/erikbern/orgs", "repos_url": "https://api.github.com/users/erikbern/repos", "events_url": "https://api.github.com/users/erikbern/events{/privacy}", "received_events_url": "https://api.github.com/users/erikbern/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-12T20:33:54Z", "updated_at": "2015-11-12T21:26:44Z", "author_association": "NONE", "body_html": "<p>I was able to install it on AWS after lots of pain. See <a href=\"https://gist.github.com/erikbern/78ba519b97b440e10640\">https://gist.github.com/erikbern/78ba519b97b440e10640</a> \u2013\u00a0I also built an AMI: ami-cf5028a5 (in Virginia region)</p>\n<p>It works on g2.2xlarge and g2.8xlarge and it detects the devices correctly (1 and 4 respectively). However I'm not seeing any speedup from the 4 GPU cards on the g2.8xlarge. Both machines process about 330 examples/sec running the CIFAR 10 example with multiple GPUs. Also very similar performance on the MNIST convolutional example. It also crashes after about 15 minutes with \"Out of GPU memory, see memory state dump above\" as some other people mentioned above</p>\n<p>I've run the CIFAR example for about an hour and it seems to chug along quite well so far</p>", "body_text": "I was able to install it on AWS after lots of pain. See https://gist.github.com/erikbern/78ba519b97b440e10640 \u2013\u00a0I also built an AMI: ami-cf5028a5 (in Virginia region)\nIt works on g2.2xlarge and g2.8xlarge and it detects the devices correctly (1 and 4 respectively). However I'm not seeing any speedup from the 4 GPU cards on the g2.8xlarge. Both machines process about 330 examples/sec running the CIFAR 10 example with multiple GPUs. Also very similar performance on the MNIST convolutional example. It also crashes after about 15 minutes with \"Out of GPU memory, see memory state dump above\" as some other people mentioned above\nI've run the CIFAR example for about an hour and it seems to chug along quite well so far", "body": "I was able to install it on AWS after lots of pain. See https://gist.github.com/erikbern/78ba519b97b440e10640 \u2013\u00a0I also built an AMI: ami-cf5028a5 (in Virginia region)\n\nIt works on g2.2xlarge and g2.8xlarge and it detects the devices correctly (1 and 4 respectively). However I'm not seeing any speedup from the 4 GPU cards on the g2.8xlarge. Both machines process about 330 examples/sec running the CIFAR 10 example with multiple GPUs. Also very similar performance on the MNIST convolutional example. It also crashes after about 15 minutes with \"Out of GPU memory, see memory state dump above\" as some other people mentioned above\n\nI've run the CIFAR example for about an hour and it seems to chug along quite well so far\n"}