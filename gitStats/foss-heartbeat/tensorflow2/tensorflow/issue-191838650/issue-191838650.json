{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5876", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5876/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5876/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5876/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5876", "id": 191838650, "node_id": "MDU6SXNzdWUxOTE4Mzg2NTA=", "number": 5876, "title": "Error in hessians() and _hessian_vector_product() for tf.nn.sparse_softmax_cross_entropy_with_logits()", "user": {"login": "kohpangwei", "id": 1113199, "node_id": "MDQ6VXNlcjExMTMxOTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/1113199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kohpangwei", "html_url": "https://github.com/kohpangwei", "followers_url": "https://api.github.com/users/kohpangwei/followers", "following_url": "https://api.github.com/users/kohpangwei/following{/other_user}", "gists_url": "https://api.github.com/users/kohpangwei/gists{/gist_id}", "starred_url": "https://api.github.com/users/kohpangwei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kohpangwei/subscriptions", "organizations_url": "https://api.github.com/users/kohpangwei/orgs", "repos_url": "https://api.github.com/users/kohpangwei/repos", "events_url": "https://api.github.com/users/kohpangwei/events{/privacy}", "received_events_url": "https://api.github.com/users/kohpangwei/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2016-11-27T02:47:29Z", "updated_at": "2017-02-09T22:02:01Z", "closed_at": "2017-02-01T21:20:32Z", "author_association": "NONE", "body_html": "<p>On a simple model that implements logistic regression, constructing the loss using tf.nn.sparse_softmax_cross_entropy_with_logits() makes both hessians() and _hessian_vector_product() return identically zero vectors, which is incorrect. If I instead write the loss function manually using tf.log, tf.sigmoid, etc., hessians() and _hessian_vector_product return the correct answer. These two versions of the loss function agree on their values and their gradients; however, the Hessian output is different.</p>\n<p>Here is some sample output:</p>\n<pre><code>Using sparse_softmax_cross_entropy_with_logits:\nLoss before first step: 0.686726\nLoss after first step : 0.686181\nActual diff in grad:\n[ 0.000122    0.00014928]\nPredicted diff in grad using _hessian_vector_product:\n[array([ 0.,  0.], dtype=float32)]\nHessian:\n[array([[ 0.,  0.],\n       [ 0.,  0.]], dtype=float32)]\n\nUsing custom loss function:\nLoss before first step: 0.686726\nLoss after first step : 0.686181\nActual diff in grad:\n[ 0.00012201  0.00014931]\nPredicted diff in grad using _hessian_vector_product:\n[array([ 0.00012199,  0.0001493 ], dtype=float32)]\nHessian:\n[array([[ 0.08229966,  0.        ],\n       [ 0.        ,  0.08278375]], dtype=float32)]\n</code></pre>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>None that I can find. The code below uses hessians() and _hessian_vector_product() from <a href=\"https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/python/ops/gradients_impl.py\">https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/python/ops/gradients_impl.py</a></p>\n<p>Here is the PR that implemented hessians(): <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"186570907\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5329\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/5329/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/5329\">#5329</a></p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 16.04</p>\n<p>Installed version of CUDA and cuDNN:</p>\n<pre><code>-rw-r--r-- 1 root root   558720 Oct  1 00:18 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44\n-rwxr-xr-x 1 root root   415432 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root   775162 Oct  1 00:18 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 68709594 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn_static.a\n</code></pre>\n<p>The same behavior occurs when running on CPU only.</p>\n<p>Installed from: <a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl</a><br>\nv0.11.0</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>tf.set_random_seed(0)\n\n### Setup toy data and weights\nimages_placeholder = tf.placeholder(tf.float32, shape=(3, 2))\nlabels_placeholder = tf.placeholder(tf.int32, shape=(3))\nfeed_dict = {\n    images_placeholder: np.array([[0, 0], [0, 1], [1, 0]]),\n    labels_placeholder: np.array([0, 1, 1]),\n}\n  \nweights = tf.Variable(\n  tf.truncated_normal([2],\n                      stddev=1.0 / math.sqrt(float(2))),\n  name='weights')\n\n### Calculate loss using built-in TF function\nweights_with_zeros = tf.pack([tf.zeros([2]), weights], axis=1)\nlogits = tf.matmul(images_placeholder, weights_with_zeros)\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels_placeholder)\nloss = tf.reduce_mean(cross_entropy)\n\n### Calculate loss using manually constructed TF function\nlogits2 = tf.matmul(images_placeholder, tf.reshape(weights, [2, 1]))\nlabels2 = (tf.to_float(labels_placeholder) * 2) - 1\nlogits_mul_labels = tf.mul(tf.reshape(logits2, [-1]), tf.reshape(labels2, [-1]))\ncross_entropy2 = - tf.log(tf.sigmoid(logits_mul_labels))\nloss2 = tf.reduce_mean(cross_entropy2)\n\n### Create train_op\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\ntrain_op = optimizer.minimize(loss, global_step=global_step)\n\n### Calculate gradients, Hessians, and Hessian-vector products for both versions of loss\ngrad = tf.gradients(loss, [weights])\ngrad2 = tf.gradients(loss2, [weights])\nv_placeholder = tf.placeholder(tf.float32, shape=weights.get_shape())\nhessian_vector = _hessian_vector_product(loss, [weights], [v_placeholder])\nhessian_vector2 = _hessian_vector_product(loss2, [weights], [v_placeholder])\nhessian = hessians(loss, [weights])\nhessian2 = hessians(loss2, [weights])\n\n### Run training for a single step to get the parameters to change.\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n\nold_weights_val, old_loss_val, old_grad_val, old_loss2_val, old_grad2_val= sess.run(\n  [weights, loss, grad, loss2, grad2], \n  feed_dict=feed_dict)\n\n_ = sess.run(train_op, feed_dict=feed_dict)\n\nnew_weights_val, new_loss_val, new_grad_val, new_loss2_val, new_grad2_val = sess.run(\n  [weights, loss, grad, loss2, grad2], \n  feed_dict=feed_dict)\n\nhessian_val, hessian2_val = sess.run(\n  [hessian, hessian2], \n  feed_dict=feed_dict)\n\n### Calculate the actual difference in gradients before and after the train step,\n### and compare with the predicted difference in gradients based on the Hessian.\ndiff_in_weights = new_weights_val - old_weights_val\nactual_diff_in_grad = new_grad_val[0] - old_grad_val[0]\nactual_diff_in_grad2 = new_grad2_val[0] - old_grad2_val[0]\n\nfeed_dict[v_placeholder] = diff_in_weights\npredicted_diff_in_grad = sess.run(hessian_vector, feed_dict=feed_dict)\npredicted_diff_in_grad2 = sess.run(hessian_vector2, feed_dict=feed_dict)\n\nprint('Diff in weights:\\n%s' % diff_in_weights)\n\nprint('\\nUsing sparse_softmax_cross_entropy_with_logits:')\nprint('Loss before first step: %s' % old_loss_val)\nprint('Loss after first step : %s' % new_loss_val)\nprint('Actual diff in grad:\\n%s' % actual_diff_in_grad)\nprint('Predicted diff in grad using _hessian_vector_product:\\n%s' % predicted_diff_in_grad)\nprint('Hessian:\\n%s' % hessian_val)\n\nprint('\\nUsing custom loss function:')\nprint('Loss before first step: %s' % old_loss2_val)\nprint('Loss after first step : %s' % new_loss2_val)\nprint('Actual diff in grad:\\n%s' % actual_diff_in_grad2)\nprint('Predicted diff in grad using _hessian_vector_product:\\n%s' % predicted_diff_in_grad2)\nprint('Hessian:\\n%s' % hessian2_val)\n\nsess.close()\n\n</code></pre>\n<h3>What other attempted solutions have you tried?</h3>\n<p>Running in CPU or GPU makes no difference.</p>\n<p>Using more complicated networks (i.e., adding some non-linear hidden layers before the linear softmax step) makes the Hessian returned from sparse_softmax_cross_entropy_with_logits() non-zero, but the returned value is still wrong in the sense that it does not match the empirical values. In contrast, using the same custom loss function above returns the correct Hessians.</p>\n<p>The same problem occurs when using \"real\" data (e.g., MNIST) or with more examples.</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>Full output when using CPU:</p>\n<pre><code>Diff in weights:\n[ 0.00148226  0.0018035 ]\n\nUsing sparse_softmax_cross_entropy_with_logits:\nLoss before first step: 0.686726\nLoss after first step : 0.686181\nActual diff in grad:\n[ 0.000122    0.00014928]\nPredicted diff in grad using _hessian_vector_product:\n[array([ 0.,  0.], dtype=float32)]\nHessian:\n[array([[ 0.,  0.],\n       [ 0.,  0.]], dtype=float32)]\n\nUsing custom loss function:\nLoss before first step: 0.686726\nLoss after first step : 0.686181\nActual diff in grad:\n[ 0.00012201  0.00014931]\nPredicted diff in grad using _hessian_vector_product:\n[array([ 0.00012199,  0.0001493 ], dtype=float32)]\nHessian:\n[array([[ 0.08229966,  0.        ],\n       [ 0.        ,  0.08278375]], dtype=float32)]\n</code></pre>", "body_text": "On a simple model that implements logistic regression, constructing the loss using tf.nn.sparse_softmax_cross_entropy_with_logits() makes both hessians() and _hessian_vector_product() return identically zero vectors, which is incorrect. If I instead write the loss function manually using tf.log, tf.sigmoid, etc., hessians() and _hessian_vector_product return the correct answer. These two versions of the loss function agree on their values and their gradients; however, the Hessian output is different.\nHere is some sample output:\nUsing sparse_softmax_cross_entropy_with_logits:\nLoss before first step: 0.686726\nLoss after first step : 0.686181\nActual diff in grad:\n[ 0.000122    0.00014928]\nPredicted diff in grad using _hessian_vector_product:\n[array([ 0.,  0.], dtype=float32)]\nHessian:\n[array([[ 0.,  0.],\n       [ 0.,  0.]], dtype=float32)]\n\nUsing custom loss function:\nLoss before first step: 0.686726\nLoss after first step : 0.686181\nActual diff in grad:\n[ 0.00012201  0.00014931]\nPredicted diff in grad using _hessian_vector_product:\n[array([ 0.00012199,  0.0001493 ], dtype=float32)]\nHessian:\n[array([[ 0.08229966,  0.        ],\n       [ 0.        ,  0.08278375]], dtype=float32)]\n\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone that I can find. The code below uses hessians() and _hessian_vector_product() from https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/python/ops/gradients_impl.py\nHere is the PR that implemented hessians(): #5329\nEnvironment info\nOperating System: Ubuntu 16.04\nInstalled version of CUDA and cuDNN:\n-rw-r--r-- 1 root root   558720 Oct  1 00:18 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rwxr-xr-x 1 root root   415432 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root   775162 Oct  1 00:18 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 68709594 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn_static.a\n\nThe same behavior occurs when running on CPU only.\nInstalled from: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\nv0.11.0\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\ntf.set_random_seed(0)\n\n### Setup toy data and weights\nimages_placeholder = tf.placeholder(tf.float32, shape=(3, 2))\nlabels_placeholder = tf.placeholder(tf.int32, shape=(3))\nfeed_dict = {\n    images_placeholder: np.array([[0, 0], [0, 1], [1, 0]]),\n    labels_placeholder: np.array([0, 1, 1]),\n}\n  \nweights = tf.Variable(\n  tf.truncated_normal([2],\n                      stddev=1.0 / math.sqrt(float(2))),\n  name='weights')\n\n### Calculate loss using built-in TF function\nweights_with_zeros = tf.pack([tf.zeros([2]), weights], axis=1)\nlogits = tf.matmul(images_placeholder, weights_with_zeros)\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels_placeholder)\nloss = tf.reduce_mean(cross_entropy)\n\n### Calculate loss using manually constructed TF function\nlogits2 = tf.matmul(images_placeholder, tf.reshape(weights, [2, 1]))\nlabels2 = (tf.to_float(labels_placeholder) * 2) - 1\nlogits_mul_labels = tf.mul(tf.reshape(logits2, [-1]), tf.reshape(labels2, [-1]))\ncross_entropy2 = - tf.log(tf.sigmoid(logits_mul_labels))\nloss2 = tf.reduce_mean(cross_entropy2)\n\n### Create train_op\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\ntrain_op = optimizer.minimize(loss, global_step=global_step)\n\n### Calculate gradients, Hessians, and Hessian-vector products for both versions of loss\ngrad = tf.gradients(loss, [weights])\ngrad2 = tf.gradients(loss2, [weights])\nv_placeholder = tf.placeholder(tf.float32, shape=weights.get_shape())\nhessian_vector = _hessian_vector_product(loss, [weights], [v_placeholder])\nhessian_vector2 = _hessian_vector_product(loss2, [weights], [v_placeholder])\nhessian = hessians(loss, [weights])\nhessian2 = hessians(loss2, [weights])\n\n### Run training for a single step to get the parameters to change.\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n\nold_weights_val, old_loss_val, old_grad_val, old_loss2_val, old_grad2_val= sess.run(\n  [weights, loss, grad, loss2, grad2], \n  feed_dict=feed_dict)\n\n_ = sess.run(train_op, feed_dict=feed_dict)\n\nnew_weights_val, new_loss_val, new_grad_val, new_loss2_val, new_grad2_val = sess.run(\n  [weights, loss, grad, loss2, grad2], \n  feed_dict=feed_dict)\n\nhessian_val, hessian2_val = sess.run(\n  [hessian, hessian2], \n  feed_dict=feed_dict)\n\n### Calculate the actual difference in gradients before and after the train step,\n### and compare with the predicted difference in gradients based on the Hessian.\ndiff_in_weights = new_weights_val - old_weights_val\nactual_diff_in_grad = new_grad_val[0] - old_grad_val[0]\nactual_diff_in_grad2 = new_grad2_val[0] - old_grad2_val[0]\n\nfeed_dict[v_placeholder] = diff_in_weights\npredicted_diff_in_grad = sess.run(hessian_vector, feed_dict=feed_dict)\npredicted_diff_in_grad2 = sess.run(hessian_vector2, feed_dict=feed_dict)\n\nprint('Diff in weights:\\n%s' % diff_in_weights)\n\nprint('\\nUsing sparse_softmax_cross_entropy_with_logits:')\nprint('Loss before first step: %s' % old_loss_val)\nprint('Loss after first step : %s' % new_loss_val)\nprint('Actual diff in grad:\\n%s' % actual_diff_in_grad)\nprint('Predicted diff in grad using _hessian_vector_product:\\n%s' % predicted_diff_in_grad)\nprint('Hessian:\\n%s' % hessian_val)\n\nprint('\\nUsing custom loss function:')\nprint('Loss before first step: %s' % old_loss2_val)\nprint('Loss after first step : %s' % new_loss2_val)\nprint('Actual diff in grad:\\n%s' % actual_diff_in_grad2)\nprint('Predicted diff in grad using _hessian_vector_product:\\n%s' % predicted_diff_in_grad2)\nprint('Hessian:\\n%s' % hessian2_val)\n\nsess.close()\n\n\nWhat other attempted solutions have you tried?\nRunning in CPU or GPU makes no difference.\nUsing more complicated networks (i.e., adding some non-linear hidden layers before the linear softmax step) makes the Hessian returned from sparse_softmax_cross_entropy_with_logits() non-zero, but the returned value is still wrong in the sense that it does not match the empirical values. In contrast, using the same custom loss function above returns the correct Hessians.\nThe same problem occurs when using \"real\" data (e.g., MNIST) or with more examples.\nLogs or other output that would be helpful\nFull output when using CPU:\nDiff in weights:\n[ 0.00148226  0.0018035 ]\n\nUsing sparse_softmax_cross_entropy_with_logits:\nLoss before first step: 0.686726\nLoss after first step : 0.686181\nActual diff in grad:\n[ 0.000122    0.00014928]\nPredicted diff in grad using _hessian_vector_product:\n[array([ 0.,  0.], dtype=float32)]\nHessian:\n[array([[ 0.,  0.],\n       [ 0.,  0.]], dtype=float32)]\n\nUsing custom loss function:\nLoss before first step: 0.686726\nLoss after first step : 0.686181\nActual diff in grad:\n[ 0.00012201  0.00014931]\nPredicted diff in grad using _hessian_vector_product:\n[array([ 0.00012199,  0.0001493 ], dtype=float32)]\nHessian:\n[array([[ 0.08229966,  0.        ],\n       [ 0.        ,  0.08278375]], dtype=float32)]", "body": "On a simple model that implements logistic regression, constructing the loss using tf.nn.sparse_softmax_cross_entropy_with_logits() makes both hessians() and _hessian_vector_product() return identically zero vectors, which is incorrect. If I instead write the loss function manually using tf.log, tf.sigmoid, etc., hessians() and _hessian_vector_product return the correct answer. These two versions of the loss function agree on their values and their gradients; however, the Hessian output is different. \r\n\r\nHere is some sample output:\r\n\r\n```\r\nUsing sparse_softmax_cross_entropy_with_logits:\r\nLoss before first step: 0.686726\r\nLoss after first step : 0.686181\r\nActual diff in grad:\r\n[ 0.000122    0.00014928]\r\nPredicted diff in grad using _hessian_vector_product:\r\n[array([ 0.,  0.], dtype=float32)]\r\nHessian:\r\n[array([[ 0.,  0.],\r\n       [ 0.,  0.]], dtype=float32)]\r\n\r\nUsing custom loss function:\r\nLoss before first step: 0.686726\r\nLoss after first step : 0.686181\r\nActual diff in grad:\r\n[ 0.00012201  0.00014931]\r\nPredicted diff in grad using _hessian_vector_product:\r\n[array([ 0.00012199,  0.0001493 ], dtype=float32)]\r\nHessian:\r\n[array([[ 0.08229966,  0.        ],\r\n       [ 0.        ,  0.08278375]], dtype=float32)]\r\n```\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone that I can find. The code below uses hessians() and _hessian_vector_product() from https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/python/ops/gradients_impl.py\r\n\r\nHere is the PR that implemented hessians(): https://github.com/tensorflow/tensorflow/pull/5329\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n-rw-r--r-- 1 root root   558720 Oct  1 00:18 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Oct  1 00:18 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5.0.5\r\n-rw-r--r-- 1 root root 68709594 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\nThe same behavior occurs when running on CPU only.\r\n\r\nInstalled from: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\nv0.11.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\ntf.set_random_seed(0)\r\n\r\n### Setup toy data and weights\r\nimages_placeholder = tf.placeholder(tf.float32, shape=(3, 2))\r\nlabels_placeholder = tf.placeholder(tf.int32, shape=(3))\r\nfeed_dict = {\r\n    images_placeholder: np.array([[0, 0], [0, 1], [1, 0]]),\r\n    labels_placeholder: np.array([0, 1, 1]),\r\n}\r\n  \r\nweights = tf.Variable(\r\n  tf.truncated_normal([2],\r\n                      stddev=1.0 / math.sqrt(float(2))),\r\n  name='weights')\r\n\r\n### Calculate loss using built-in TF function\r\nweights_with_zeros = tf.pack([tf.zeros([2]), weights], axis=1)\r\nlogits = tf.matmul(images_placeholder, weights_with_zeros)\r\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels_placeholder)\r\nloss = tf.reduce_mean(cross_entropy)\r\n\r\n### Calculate loss using manually constructed TF function\r\nlogits2 = tf.matmul(images_placeholder, tf.reshape(weights, [2, 1]))\r\nlabels2 = (tf.to_float(labels_placeholder) * 2) - 1\r\nlogits_mul_labels = tf.mul(tf.reshape(logits2, [-1]), tf.reshape(labels2, [-1]))\r\ncross_entropy2 = - tf.log(tf.sigmoid(logits_mul_labels))\r\nloss2 = tf.reduce_mean(cross_entropy2)\r\n\r\n### Create train_op\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\r\ntrain_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n### Calculate gradients, Hessians, and Hessian-vector products for both versions of loss\r\ngrad = tf.gradients(loss, [weights])\r\ngrad2 = tf.gradients(loss2, [weights])\r\nv_placeholder = tf.placeholder(tf.float32, shape=weights.get_shape())\r\nhessian_vector = _hessian_vector_product(loss, [weights], [v_placeholder])\r\nhessian_vector2 = _hessian_vector_product(loss2, [weights], [v_placeholder])\r\nhessian = hessians(loss, [weights])\r\nhessian2 = hessians(loss2, [weights])\r\n\r\n### Run training for a single step to get the parameters to change.\r\ninit = tf.initialize_all_variables()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\nold_weights_val, old_loss_val, old_grad_val, old_loss2_val, old_grad2_val= sess.run(\r\n  [weights, loss, grad, loss2, grad2], \r\n  feed_dict=feed_dict)\r\n\r\n_ = sess.run(train_op, feed_dict=feed_dict)\r\n\r\nnew_weights_val, new_loss_val, new_grad_val, new_loss2_val, new_grad2_val = sess.run(\r\n  [weights, loss, grad, loss2, grad2], \r\n  feed_dict=feed_dict)\r\n\r\nhessian_val, hessian2_val = sess.run(\r\n  [hessian, hessian2], \r\n  feed_dict=feed_dict)\r\n\r\n### Calculate the actual difference in gradients before and after the train step,\r\n### and compare with the predicted difference in gradients based on the Hessian.\r\ndiff_in_weights = new_weights_val - old_weights_val\r\nactual_diff_in_grad = new_grad_val[0] - old_grad_val[0]\r\nactual_diff_in_grad2 = new_grad2_val[0] - old_grad2_val[0]\r\n\r\nfeed_dict[v_placeholder] = diff_in_weights\r\npredicted_diff_in_grad = sess.run(hessian_vector, feed_dict=feed_dict)\r\npredicted_diff_in_grad2 = sess.run(hessian_vector2, feed_dict=feed_dict)\r\n\r\nprint('Diff in weights:\\n%s' % diff_in_weights)\r\n\r\nprint('\\nUsing sparse_softmax_cross_entropy_with_logits:')\r\nprint('Loss before first step: %s' % old_loss_val)\r\nprint('Loss after first step : %s' % new_loss_val)\r\nprint('Actual diff in grad:\\n%s' % actual_diff_in_grad)\r\nprint('Predicted diff in grad using _hessian_vector_product:\\n%s' % predicted_diff_in_grad)\r\nprint('Hessian:\\n%s' % hessian_val)\r\n\r\nprint('\\nUsing custom loss function:')\r\nprint('Loss before first step: %s' % old_loss2_val)\r\nprint('Loss after first step : %s' % new_loss2_val)\r\nprint('Actual diff in grad:\\n%s' % actual_diff_in_grad2)\r\nprint('Predicted diff in grad using _hessian_vector_product:\\n%s' % predicted_diff_in_grad2)\r\nprint('Hessian:\\n%s' % hessian2_val)\r\n\r\nsess.close()\r\n\r\n```\r\n### What other attempted solutions have you tried?\r\nRunning in CPU or GPU makes no difference.\r\n\r\nUsing more complicated networks (i.e., adding some non-linear hidden layers before the linear softmax step) makes the Hessian returned from sparse_softmax_cross_entropy_with_logits() non-zero, but the returned value is still wrong in the sense that it does not match the empirical values. In contrast, using the same custom loss function above returns the correct Hessians.  \r\n\r\nThe same problem occurs when using \"real\" data (e.g., MNIST) or with more examples.\r\n\r\n### Logs or other output that would be helpful\r\nFull output when using CPU:\r\n```\r\nDiff in weights:\r\n[ 0.00148226  0.0018035 ]\r\n\r\nUsing sparse_softmax_cross_entropy_with_logits:\r\nLoss before first step: 0.686726\r\nLoss after first step : 0.686181\r\nActual diff in grad:\r\n[ 0.000122    0.00014928]\r\nPredicted diff in grad using _hessian_vector_product:\r\n[array([ 0.,  0.], dtype=float32)]\r\nHessian:\r\n[array([[ 0.,  0.],\r\n       [ 0.,  0.]], dtype=float32)]\r\n\r\nUsing custom loss function:\r\nLoss before first step: 0.686726\r\nLoss after first step : 0.686181\r\nActual diff in grad:\r\n[ 0.00012201  0.00014931]\r\nPredicted diff in grad using _hessian_vector_product:\r\n[array([ 0.00012199,  0.0001493 ], dtype=float32)]\r\nHessian:\r\n[array([[ 0.08229966,  0.        ],\r\n       [ 0.        ,  0.08278375]], dtype=float32)]\r\n```"}