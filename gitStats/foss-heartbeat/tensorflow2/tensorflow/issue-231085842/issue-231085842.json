{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10163", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10163/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10163/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10163/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10163", "id": 231085842, "node_id": "MDU6SXNzdWUyMzEwODU4NDI=", "number": 10163, "title": "Custom Poets Models Run Slow on Android", "user": {"login": "jubjamie", "id": 25011496, "node_id": "MDQ6VXNlcjI1MDExNDk2", "avatar_url": "https://avatars3.githubusercontent.com/u/25011496?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jubjamie", "html_url": "https://github.com/jubjamie", "followers_url": "https://api.github.com/users/jubjamie/followers", "following_url": "https://api.github.com/users/jubjamie/following{/other_user}", "gists_url": "https://api.github.com/users/jubjamie/gists{/gist_id}", "starred_url": "https://api.github.com/users/jubjamie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jubjamie/subscriptions", "organizations_url": "https://api.github.com/users/jubjamie/orgs", "repos_url": "https://api.github.com/users/jubjamie/repos", "events_url": "https://api.github.com/users/jubjamie/events{/privacy}", "received_events_url": "https://api.github.com/users/jubjamie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-24T15:45:17Z", "updated_at": "2017-05-25T09:51:31Z", "closed_at": "2017-05-24T23:52:30Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Not really.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Android/Windows</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary (From android nightly</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I've noticed that using a retrained inception model within the demo app, following the guidelines suggested, is awfully slow. Shouldn't the custom models generated in the style of Tensorflow for poets be pretty similar to the inception model that the demo comes with? I have noticed inference times to be around 5 times as slow on two devices. (Nexus 6P and Pixel C) compared to the original demo.<br>\nEven when the graphs are quantized I am getting no apparent performance increase (apart from model size). If anything it's actually slower.<br>\nIs this normal behaviour? I'm aware of the image size is different (224 vs 299) but is that enough to haemorrhage the performance?</p>\n<h3>Source code / logs</h3>\n<p>Avg. ms for Conv2D is 1366ms<br>\nInference time ~1700ms (Pixel C) ~3500 (Nexus 6P)</p>\n<p>Model building steps: Normal Model via Tensorflow for poets etc. --&gt; strip nodes --&gt; quantize --&gt; replace in apk.<br>\nSame performance regardless of quantization.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3376817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewharp\">@andrewharp</a> this was what I referred to in the windows/android thread. Can move to s/o if preferred.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android/Windows\nTensorFlow installed from (source or binary): Binary (From android nightly\nTensorFlow version (use command below): 1.2\n\nDescribe the problem\nI've noticed that using a retrained inception model within the demo app, following the guidelines suggested, is awfully slow. Shouldn't the custom models generated in the style of Tensorflow for poets be pretty similar to the inception model that the demo comes with? I have noticed inference times to be around 5 times as slow on two devices. (Nexus 6P and Pixel C) compared to the original demo.\nEven when the graphs are quantized I am getting no apparent performance increase (apart from model size). If anything it's actually slower.\nIs this normal behaviour? I'm aware of the image size is different (224 vs 299) but is that enough to haemorrhage the performance?\nSource code / logs\nAvg. ms for Conv2D is 1366ms\nInference time ~1700ms (Pixel C) ~3500 (Nexus 6P)\nModel building steps: Normal Model via Tensorflow for poets etc. --> strip nodes --> quantize --> replace in apk.\nSame performance regardless of quantization.\n@andrewharp this was what I referred to in the windows/android thread. Can move to s/o if preferred.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not really.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android/Windows\r\n- **TensorFlow installed from (source or binary)**: Binary (From android nightly\r\n- **TensorFlow version (use command below)**: 1.2\r\n\r\n### Describe the problem\r\nI've noticed that using a retrained inception model within the demo app, following the guidelines suggested, is awfully slow. Shouldn't the custom models generated in the style of Tensorflow for poets be pretty similar to the inception model that the demo comes with? I have noticed inference times to be around 5 times as slow on two devices. (Nexus 6P and Pixel C) compared to the original demo.\r\nEven when the graphs are quantized I am getting no apparent performance increase (apart from model size). If anything it's actually slower.\r\nIs this normal behaviour? I'm aware of the image size is different (224 vs 299) but is that enough to haemorrhage the performance?\r\n\r\n### Source code / logs\r\nAvg. ms for Conv2D is 1366ms \r\nInference time ~1700ms (Pixel C) ~3500 (Nexus 6P)\r\n\r\nModel building steps: Normal Model via Tensorflow for poets etc. --> strip nodes --> quantize --> replace in apk.\r\nSame performance regardless of quantization.\r\n\r\n@andrewharp this was what I referred to in the windows/android thread. Can move to s/o if preferred."}