{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3891", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3891/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3891/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3891/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3891", "id": 171818998, "node_id": "MDU6SXNzdWUxNzE4MTg5OTg=", "number": 3891, "title": "Very low accuracy in the mnist dataset with cnn when running on a GPU using tensorflow", "user": {"login": "vivounicorn", "id": 3362154, "node_id": "MDQ6VXNlcjMzNjIxNTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/3362154?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vivounicorn", "html_url": "https://github.com/vivounicorn", "followers_url": "https://api.github.com/users/vivounicorn/followers", "following_url": "https://api.github.com/users/vivounicorn/following{/other_user}", "gists_url": "https://api.github.com/users/vivounicorn/gists{/gist_id}", "starred_url": "https://api.github.com/users/vivounicorn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vivounicorn/subscriptions", "organizations_url": "https://api.github.com/users/vivounicorn/orgs", "repos_url": "https://api.github.com/users/vivounicorn/repos", "events_url": "https://api.github.com/users/vivounicorn/events{/privacy}", "received_events_url": "https://api.github.com/users/vivounicorn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-08-18T05:10:31Z", "updated_at": "2016-09-05T12:49:28Z", "closed_at": "2016-08-23T02:06:08Z", "author_association": "NONE", "body_html": "<p>hi everyone:<br>\nI tried to run an examples of MNIST with cnn and when i only use cpu the code can work well  <strong>but when i use gpu it is not working well and  it has very low accuracy</strong>.</p>\n<h2>environment like this</h2>\n<p>GPU:Geforce GTX1070<br>\nCuda toolkit version\uff1a7.5<br>\ncuDNN version\uff1a7.0<br>\ntensorflow version\uff1a0.9r</p>\n<h2>code like this:</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.examples.tutorials.mnist <span class=\"pl-k\">import</span> input_data\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">weight_varible</span>(<span class=\"pl-smi\">shape</span>):\n    initial <span class=\"pl-k\">=</span> tf.truncated_normal(shape, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\n    <span class=\"pl-k\">return</span> tf.Variable(initial)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bias_variable</span>(<span class=\"pl-smi\">shape</span>):\n    initial <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape)\n    <span class=\"pl-k\">return</span> tf.Variable(initial)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">conv2d</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">W</span>):\n    <span class=\"pl-k\">return</span> tf.nn.conv2d(x, W, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">max_pool_2x2</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">return</span> tf.nn.max_pool(x, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n\n\nmnist <span class=\"pl-k\">=</span> input_data.read_data_sets(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MNIST_data/<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">one_hot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Download Done!<span class=\"pl-pds\">\"</span></span>)\n\nsess <span class=\"pl-k\">=</span> tf.InteractiveSession()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> paras</span>\nW_conv1 <span class=\"pl-k\">=</span> weight_varible([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>])\nb_conv1 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">32</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> conv layer-1</span>\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">784</span>])\nx_image <span class=\"pl-k\">=</span> tf.reshape(x, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">1</span>])\n\nh_conv1 <span class=\"pl-k\">=</span> tf.nn.relu(conv2d(x_image, W_conv1) <span class=\"pl-k\">+</span> b_conv1)\nh_pool1 <span class=\"pl-k\">=</span> max_pool_2x2(h_conv1)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> conv layer-2</span>\nW_conv2 <span class=\"pl-k\">=</span> weight_varible([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>])\nb_conv2 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">64</span>])\n\nh_conv2 <span class=\"pl-k\">=</span> tf.nn.relu(conv2d(h_pool1, W_conv2) <span class=\"pl-k\">+</span> b_conv2)\nh_pool2 <span class=\"pl-k\">=</span> max_pool_2x2(h_conv2)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> full connection</span>\nW_fc1 <span class=\"pl-k\">=</span> weight_varible([<span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1024</span>])\nb_fc1 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">1024</span>])\n\nh_pool2_flat <span class=\"pl-k\">=</span> tf.reshape(h_pool2, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">64</span>])\nh_fc1 <span class=\"pl-k\">=</span> tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) <span class=\"pl-k\">+</span> b_fc1)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> dropout</span>\nkeep_prob <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\nh_fc1_drop <span class=\"pl-k\">=</span> tf.nn.dropout(h_fc1, keep_prob)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> output layer: softmax</span>\nW_fc2 <span class=\"pl-k\">=</span> weight_varible([<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">10</span>])\nb_fc2 <span class=\"pl-k\">=</span> bias_variable([<span class=\"pl-c1\">10</span>])\n\ny_conv <span class=\"pl-k\">=</span> tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) <span class=\"pl-k\">+</span> b_fc2)\ny_ <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">10</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> model training</span>\ncross_entropy <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>tf.reduce_sum(y_ <span class=\"pl-k\">*</span> tf.log(y_conv))\ntrain_step <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-4</span>).minimize(cross_entropy)\n\ncorrect_prediction <span class=\"pl-k\">=</span> tf.equal(tf.arg_max(y_conv, <span class=\"pl-c1\">1</span>), tf.arg_max(y_, <span class=\"pl-c1\">1</span>))\naccuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsess.run(tf.initialize_all_variables())\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">20000</span>):\n    batch <span class=\"pl-k\">=</span> mnist.train.next_batch(<span class=\"pl-c1\">50</span>)\n\n    <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        train_accuacy <span class=\"pl-k\">=</span> accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch[<span class=\"pl-c1\">0</span>], y_: batch[<span class=\"pl-c1\">1</span>], keep_prob: <span class=\"pl-c1\">1.0</span>})\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>step <span class=\"pl-c1\">%d</span>, training accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">%</span>(i, train_accuacy))\n    train_step.run(<span class=\"pl-v\">feed_dict</span> <span class=\"pl-k\">=</span> {x: batch[<span class=\"pl-c1\">0</span>], y_: batch[<span class=\"pl-c1\">1</span>], keep_prob: <span class=\"pl-c1\">0.5</span>})\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> accuacy on test</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test accuracy <span class=\"pl-c1\">%g</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">%</span>(accuracy.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class=\"pl-c1\">1.0</span>})))</pre></div>\n<h3>on cpu</h3>\n<p>step 0, training accuracy 0.06<br>\nstep 100, training accuracy 0.8<br>\nstep 200, training accuracy 0.94<br>\nstep 300, training accuracy 0.9<br>\nstep 400, training accuracy 0.92<br>\nstep 500, training accuracy 0.84<br>\nstep 600, training accuracy 1<br>\nstep 700, training accuracy 0.96<br>\nstep 800, training accuracy 0.94<br>\nstep 900, training accuracy 1<br>\nstep 1000, training accuracy 0.98<br>\nstep 1100, training accuracy 0.92<br>\nstep 1200, training accuracy 0.96<br>\nstep 1300, training accuracy 1<br>\nstep 1400, training accuracy 1<br>\nstep 1500, training accuracy 1<br>\n......<br>\ntest accuracy 0.9671</p>\n<h3>on gpu</h3>\n<p>step 0, training accuracy 0.1<br>\nstep 100, training accuracy 0.08<br>\nstep 200, training accuracy 0.02<br>\nstep 300, training accuracy 0.12<br>\nstep 400, training accuracy 0.1<br>\nstep 500, training accuracy 0.1<br>\nstep 600, training accuracy 0.12<br>\nstep 700, training accuracy 0.12<br>\nstep 800, training accuracy 0.1<br>\nstep 900, training accuracy 0.04<br>\nstep 1000, training accuracy 0.12<br>\nstep 1100, training accuracy 0.3<br>\nstep 1200, training accuracy 0.1<br>\nstep 1300, training accuracy 0.08<br>\nstep 1400, training accuracy 0.12<br>\nstep 1500, training accuracy 0.2<br>\n......<br>\ntest accuracy 0.1160</p>", "body_text": "hi everyone:\nI tried to run an examples of MNIST with cnn and when i only use cpu the code can work well  but when i use gpu it is not working well and  it has very low accuracy.\nenvironment like this\nGPU:Geforce GTX1070\nCuda toolkit version\uff1a7.5\ncuDNN version\uff1a7.0\ntensorflow version\uff1a0.9r\ncode like this:\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ndef weight_varible(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nprint(\"Download Done!\")\n\nsess = tf.InteractiveSession()\n\n# paras\nW_conv1 = weight_varible([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\n# conv layer-1\nx = tf.placeholder(tf.float32, [None, 784])\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# conv layer-2\nW_conv2 = weight_varible([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n# full connection\nW_fc1 = weight_varible([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# dropout\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# output layer: softmax\nW_fc2 = weight_varible([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\ny_ = tf.placeholder(tf.float32, [None, 10])\n\n# model training\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsess.run(tf.initialize_all_variables())\n\nfor i in range(20000):\n    batch = mnist.train.next_batch(50)\n\n    if i % 100 == 0:\n        train_accuacy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n        print(\"step %d, training accuracy %g\"%(i, train_accuacy))\n    train_step.run(feed_dict = {x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n# accuacy on test\nprint(\"test accuracy %g\"%(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))\non cpu\nstep 0, training accuracy 0.06\nstep 100, training accuracy 0.8\nstep 200, training accuracy 0.94\nstep 300, training accuracy 0.9\nstep 400, training accuracy 0.92\nstep 500, training accuracy 0.84\nstep 600, training accuracy 1\nstep 700, training accuracy 0.96\nstep 800, training accuracy 0.94\nstep 900, training accuracy 1\nstep 1000, training accuracy 0.98\nstep 1100, training accuracy 0.92\nstep 1200, training accuracy 0.96\nstep 1300, training accuracy 1\nstep 1400, training accuracy 1\nstep 1500, training accuracy 1\n......\ntest accuracy 0.9671\non gpu\nstep 0, training accuracy 0.1\nstep 100, training accuracy 0.08\nstep 200, training accuracy 0.02\nstep 300, training accuracy 0.12\nstep 400, training accuracy 0.1\nstep 500, training accuracy 0.1\nstep 600, training accuracy 0.12\nstep 700, training accuracy 0.12\nstep 800, training accuracy 0.1\nstep 900, training accuracy 0.04\nstep 1000, training accuracy 0.12\nstep 1100, training accuracy 0.3\nstep 1200, training accuracy 0.1\nstep 1300, training accuracy 0.08\nstep 1400, training accuracy 0.12\nstep 1500, training accuracy 0.2\n......\ntest accuracy 0.1160", "body": "hi everyone:\nI tried to run an examples of MNIST with cnn and when i only use cpu the code can work well  **but when i use gpu it is not working well and  it has very low accuracy**. \n## environment like this\n\n GPU:Geforce GTX1070\n Cuda toolkit version\uff1a7.5\n cuDNN version\uff1a7.0\ntensorflow version\uff1a0.9r\n## code like this:\n\n``` python\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ndef weight_varible(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nprint(\"Download Done!\")\n\nsess = tf.InteractiveSession()\n\n# paras\nW_conv1 = weight_varible([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\n# conv layer-1\nx = tf.placeholder(tf.float32, [None, 784])\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# conv layer-2\nW_conv2 = weight_varible([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n# full connection\nW_fc1 = weight_varible([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# dropout\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# output layer: softmax\nW_fc2 = weight_varible([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\ny_ = tf.placeholder(tf.float32, [None, 10])\n\n# model training\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsess.run(tf.initialize_all_variables())\n\nfor i in range(20000):\n    batch = mnist.train.next_batch(50)\n\n    if i % 100 == 0:\n        train_accuacy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n        print(\"step %d, training accuracy %g\"%(i, train_accuacy))\n    train_step.run(feed_dict = {x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n# accuacy on test\nprint(\"test accuracy %g\"%(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))\n```\n### on cpu\n\nstep 0, training accuracy 0.06\nstep 100, training accuracy 0.8\nstep 200, training accuracy 0.94\nstep 300, training accuracy 0.9\nstep 400, training accuracy 0.92\nstep 500, training accuracy 0.84\nstep 600, training accuracy 1\nstep 700, training accuracy 0.96\nstep 800, training accuracy 0.94\nstep 900, training accuracy 1\nstep 1000, training accuracy 0.98\nstep 1100, training accuracy 0.92\nstep 1200, training accuracy 0.96\nstep 1300, training accuracy 1\nstep 1400, training accuracy 1\nstep 1500, training accuracy 1\n......\ntest accuracy 0.9671\n### on gpu\n\nstep 0, training accuracy 0.1\nstep 100, training accuracy 0.08\nstep 200, training accuracy 0.02\nstep 300, training accuracy 0.12\nstep 400, training accuracy 0.1\nstep 500, training accuracy 0.1\nstep 600, training accuracy 0.12\nstep 700, training accuracy 0.12\nstep 800, training accuracy 0.1\nstep 900, training accuracy 0.04\nstep 1000, training accuracy 0.12\nstep 1100, training accuracy 0.3\nstep 1200, training accuracy 0.1\nstep 1300, training accuracy 0.08\nstep 1400, training accuracy 0.12\nstep 1500, training accuracy 0.2\n......\ntest accuracy 0.1160\n"}