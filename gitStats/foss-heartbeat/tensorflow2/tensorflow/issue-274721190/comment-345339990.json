{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345339990", "html_url": "https://github.com/tensorflow/tensorflow/issues/14641#issuecomment-345339990", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14641", "id": 345339990, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTMzOTk5MA==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-17T19:19:25Z", "updated_at": "2017-11-17T19:19:25Z", "author_association": "MEMBER", "body_html": "<p>I think this is working as intended. You're differentiating a vector-valued function and we differentiate only scalar-valued functions.</p>\n<p>When we build a scalar function out of a vector function to differentiate we reduce_sum the vector and so you are computing the second gradient of the sum of the components of the first gradient (so the derivative of the divergence of the softmax, if you will) which is 0.</p>", "body_text": "I think this is working as intended. You're differentiating a vector-valued function and we differentiate only scalar-valued functions.\nWhen we build a scalar function out of a vector function to differentiate we reduce_sum the vector and so you are computing the second gradient of the sum of the components of the first gradient (so the derivative of the divergence of the softmax, if you will) which is 0.", "body": "I think this is working as intended. You're differentiating a vector-valued function and we differentiate only scalar-valued functions.\r\n\r\nWhen we build a scalar function out of a vector function to differentiate we reduce_sum the vector and so you are computing the second gradient of the sum of the components of the first gradient (so the derivative of the divergence of the softmax, if you will) which is 0."}