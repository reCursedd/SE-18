{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345773503", "html_url": "https://github.com/tensorflow/tensorflow/issues/14641#issuecomment-345773503", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14641", "id": 345773503, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTc3MzUwMw==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-20T17:48:01Z", "updated_at": "2017-11-20T17:48:01Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4559466\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/armanform\">@armanform</a> what you could do to your code easily is look at the derivative of the first dimension of the gradient of the loss with respect to the logits, which gives you the numbers you were thinking of:</p>\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nimport numpy as np\n\ntfe.enable_eager_execution()\n\nlogits = [0.5, 0.5]\ny = [1, 0]\n\ndef loss_function(x):\n    loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=x)\n    return loss2\n\ngrad_loss = tfe.gradients_function(loss_function)\nprint grad_loss(logits)[0] # prints correct gradient [-0.5 0.5]\n\ngradgrad_loss = tfe.gradients_function(lambda x: grad_loss(x)[0][0])  # this should be +-0.25\n</code></pre>", "body_text": "@armanform what you could do to your code easily is look at the derivative of the first dimension of the gradient of the loss with respect to the logits, which gives you the numbers you were thinking of:\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nimport numpy as np\n\ntfe.enable_eager_execution()\n\nlogits = [0.5, 0.5]\ny = [1, 0]\n\ndef loss_function(x):\n    loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=x)\n    return loss2\n\ngrad_loss = tfe.gradients_function(loss_function)\nprint grad_loss(logits)[0] # prints correct gradient [-0.5 0.5]\n\ngradgrad_loss = tfe.gradients_function(lambda x: grad_loss(x)[0][0])  # this should be +-0.25", "body": "@armanform what you could do to your code easily is look at the derivative of the first dimension of the gradient of the loss with respect to the logits, which gives you the numbers you were thinking of:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nimport numpy as np\r\n\r\ntfe.enable_eager_execution()\r\n\r\nlogits = [0.5, 0.5]\r\ny = [1, 0]\r\n\r\ndef loss_function(x):\r\n    loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=x)\r\n    return loss2\r\n\r\ngrad_loss = tfe.gradients_function(loss_function)\r\nprint grad_loss(logits)[0] # prints correct gradient [-0.5 0.5]\r\n\r\ngradgrad_loss = tfe.gradients_function(lambda x: grad_loss(x)[0][0])  # this should be +-0.25\r\n```"}