{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23696", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23696/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23696/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23696/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23696", "id": 379989764, "node_id": "MDU6SXNzdWUzNzk5ODk3NjQ=", "number": 23696, "title": "GPU crashes when running Keras/tensorflow-gpu, specifically when clock speed goes to idle at 0 MHz", "user": {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097546578, "node_id": "MDU6TGFiZWwxMDk3NTQ2NTc4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:keras", "name": "comp:keras", "color": "0052cc", "default": false}, {"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 1125315826, "node_id": "MDU6TGFiZWwxMTI1MzE1ODI2", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:cross-posted%20from%20Keras", "name": "stat:cross-posted from Keras", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-12T23:12:19Z", "updated_at": "2018-11-12T23:22:37Z", "closed_at": "2018-11-12T23:22:37Z", "author_association": "NONE", "body_html": "<p>Below issue was posted by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=37425003\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/r8drascal\">@r8drascal</a> in TF Keras repo.</p>\n<p>I'm using Jupyter Notebook to run Keras with a Tensorflow GPU backend. I've done some testing with various dummy models while simultaneously monitoring my GPU usage using MSI Afterburner, GPU-Z, nvidia-smi and Task Manager. My GPU is a GeForce GTX 960M, which has no issues running games. The temperatures are also low when running Keras.</p>\n<p>What I've noticed is that the Keras runs fine (e.g. loading or training a model) in the beginning but whenever Keras is not running anything, the GPU naturally wants to idle from 1097 MHz to 0 MHz and as soon as it does that the GPU crashes. I can see that the \"GPU is lost\" on nvidia-nvsmi. I have to then disable and re-enable my GPU in the Device Manager to get it to work.</p>\n<p>Here's a sample script, which I believe was from the official documentation:</p>\n<p>import keras<br>\nfrom keras.models import Sequential<br>\nfrom keras.layers import Dense, Dropout, Activation<br>\nfrom keras.optimizers import SGD</p>\n<p>import numpy as np<br>\nx_train = np.random.random((1000, 20))<br>\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=50)<br>\nx_test = np.random.random((100, 20))<br>\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=50)</p>\n<p>model = Sequential()<br>\nmodel.add(Dense(200, activation='relu', input_dim=20))<br>\nmodel.add(Dropout(0.5))<br>\nmodel.add(Dense(200, activation='relu'))<br>\nmodel.add(Dropout(0.5))<br>\nmodel.add(Dense(50, activation='softmax'))</p>\n<p>sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)<br>\nmodel.compile(loss='categorical_crossentropy',<br>\noptimizer=sgd,<br>\nmetrics=['accuracy'])</p>\n<p>model.fit(x_train, y_train,<br>\nepochs=20,<br>\nbatch_size=128)<br>\nscore = model.evaluate(x_test, y_test, batch_size=128)</p>\n<p>The model runs fine and completes the training but after a few seconds my GPU dies. This even happens if I just load a model. I.e. I import keras modules, then use \"load_model\", and in less than a minute everything crashes as soon as the clock speed drops to 0 MHz.</p>\n<p>Does anyone have any idea why this might be happening?</p>", "body_text": "Below issue was posted by @r8drascal in TF Keras repo.\nI'm using Jupyter Notebook to run Keras with a Tensorflow GPU backend. I've done some testing with various dummy models while simultaneously monitoring my GPU usage using MSI Afterburner, GPU-Z, nvidia-smi and Task Manager. My GPU is a GeForce GTX 960M, which has no issues running games. The temperatures are also low when running Keras.\nWhat I've noticed is that the Keras runs fine (e.g. loading or training a model) in the beginning but whenever Keras is not running anything, the GPU naturally wants to idle from 1097 MHz to 0 MHz and as soon as it does that the GPU crashes. I can see that the \"GPU is lost\" on nvidia-nvsmi. I have to then disable and re-enable my GPU in the Device Manager to get it to work.\nHere's a sample script, which I believe was from the official documentation:\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\nimport numpy as np\nx_train = np.random.random((1000, 20))\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=50)\nx_test = np.random.random((100, 20))\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=50)\nmodel = Sequential()\nmodel.add(Dense(200, activation='relu', input_dim=20))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(50, activation='softmax'))\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy',\noptimizer=sgd,\nmetrics=['accuracy'])\nmodel.fit(x_train, y_train,\nepochs=20,\nbatch_size=128)\nscore = model.evaluate(x_test, y_test, batch_size=128)\nThe model runs fine and completes the training but after a few seconds my GPU dies. This even happens if I just load a model. I.e. I import keras modules, then use \"load_model\", and in less than a minute everything crashes as soon as the clock speed drops to 0 MHz.\nDoes anyone have any idea why this might be happening?", "body": "Below issue was posted by @r8drascal in TF Keras repo.\r\n\r\nI'm using Jupyter Notebook to run Keras with a Tensorflow GPU backend. I've done some testing with various dummy models while simultaneously monitoring my GPU usage using MSI Afterburner, GPU-Z, nvidia-smi and Task Manager. My GPU is a GeForce GTX 960M, which has no issues running games. The temperatures are also low when running Keras.\r\n\r\nWhat I've noticed is that the Keras runs fine (e.g. loading or training a model) in the beginning but whenever Keras is not running anything, the GPU naturally wants to idle from 1097 MHz to 0 MHz and as soon as it does that the GPU crashes. I can see that the \"GPU is lost\" on nvidia-nvsmi. I have to then disable and re-enable my GPU in the Device Manager to get it to work.\r\n\r\nHere's a sample script, which I believe was from the official documentation:\r\n\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation\r\nfrom keras.optimizers import SGD\r\n\r\nimport numpy as np\r\nx_train = np.random.random((1000, 20))\r\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=50)\r\nx_test = np.random.random((100, 20))\r\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=50)\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(200, activation='relu', input_dim=20))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(200, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(50, activation='softmax'))\r\n\r\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=sgd,\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          epochs=20,\r\n          batch_size=128)\r\nscore = model.evaluate(x_test, y_test, batch_size=128)\r\n\r\n\r\nThe model runs fine and completes the training but after a few seconds my GPU dies. This even happens if I just load a model. I.e. I import keras modules, then use \"load_model\", and in less than a minute everything crashes as soon as the clock speed drops to 0 MHz.\r\n\r\nDoes anyone have any idea why this might be happening?"}