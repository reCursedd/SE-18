{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1658", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1658/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1658/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1658/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1658", "id": 143740379, "node_id": "MDU6SXNzdWUxNDM3NDAzNzk=", "number": 1658, "title": "Tensorflow with GPU-support crashes when opening many CPU-only sessions in parallel", "user": {"login": "tcwalther", "id": 2840901, "node_id": "MDQ6VXNlcjI4NDA5MDE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2840901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tcwalther", "html_url": "https://github.com/tcwalther", "followers_url": "https://api.github.com/users/tcwalther/followers", "following_url": "https://api.github.com/users/tcwalther/following{/other_user}", "gists_url": "https://api.github.com/users/tcwalther/gists{/gist_id}", "starred_url": "https://api.github.com/users/tcwalther/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tcwalther/subscriptions", "organizations_url": "https://api.github.com/users/tcwalther/orgs", "repos_url": "https://api.github.com/users/tcwalther/repos", "events_url": "https://api.github.com/users/tcwalther/events{/privacy}", "received_events_url": "https://api.github.com/users/tcwalther/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-03-26T19:08:16Z", "updated_at": "2017-11-25T20:40:22Z", "closed_at": "2016-03-26T20:31:44Z", "author_association": "NONE", "body_html": "<p>The following code works fine in a CPU-only Tensorflow, but crashes on a GPU-enabled Tensorflow installation (0.7.1, with a Titan X, NVidia driver 352.79) when run many times in parallel:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> test_tensorflow_session.py</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> os\nos.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>CUDA_VISIBLE_DEVICES<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>\nsess <span class=\"pl-k\">=</span> tf.Session()</pre></div>\n<p>Bash command to run it in parallel:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">for</span> <span class=\"pl-smi\">i</span> <span class=\"pl-k\">in</span> {1..48}<span class=\"pl-k\">;</span> <span class=\"pl-k\">do</span> <span class=\"pl-s\"><span class=\"pl-pds\">((</span>python .<span class=\"pl-k\">/</span>test_tensorflow_session.py <span class=\"pl-c1\">2</span><span class=\"pl-k\">&gt;</span> output<span class=\"pl-smi\">$i</span>.err <span class=\"pl-c1\">1</span><span class=\"pl-k\">&gt;</span> output<span class=\"pl-smi\">$i</span>.out ) <span class=\"pl-k\">&amp;</span>) ; done</span></pre></div>\n<p>If you then look into the <code>output*.err</code> files, you will see that most of the processes crashed. The output will look like this:</p>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: gpu1\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: gpu1\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:226] kernel version seems to match DSO: 352.79\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: \nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\n</code></pre>\n<p>For the sake of completeness, here is the use case: applying a Tensorflow model is just a small part in processing large amounts of data; processing is parallelised simply by using multiple processes.</p>\n<p>I also tried wrapping the <code>tf.Session()</code> call in <code>with tf.device('/cpu:0'):</code>, but that didn't change anything. I assume that it is trying to get exclusive access to the GPU, and if it can't do that, it crashes. This is a bit annoying, since by setting <code>os.environ['CUDA_VISIBLE_DEVICES'] = ''</code>, I am actively trying to disable the GPU.</p>\n<p>Ideas?</p>", "body_text": "The following code works fine in a CPU-only Tensorflow, but crashes on a GPU-enabled Tensorflow installation (0.7.1, with a Titan X, NVidia driver 352.79) when run many times in parallel:\n# test_tensorflow_session.py\nimport tensorflow as tf\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\nsess = tf.Session()\nBash command to run it in parallel:\nfor i in {1..48}; do ((python ./test_tensorflow_session.py 2> output$i.err 1> output$i.out ) &) ; done\nIf you then look into the output*.err files, you will see that most of the processes crashed. The output will look like this:\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: gpu1\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: gpu1\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:226] kernel version seems to match DSO: 352.79\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: \nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\n\nFor the sake of completeness, here is the use case: applying a Tensorflow model is just a small part in processing large amounts of data; processing is parallelised simply by using multiple processes.\nI also tried wrapping the tf.Session() call in with tf.device('/cpu:0'):, but that didn't change anything. I assume that it is trying to get exclusive access to the GPU, and if it can't do that, it crashes. This is a bit annoying, since by setting os.environ['CUDA_VISIBLE_DEVICES'] = '', I am actively trying to disable the GPU.\nIdeas?", "body": "The following code works fine in a CPU-only Tensorflow, but crashes on a GPU-enabled Tensorflow installation (0.7.1, with a Titan X, NVidia driver 352.79) when run many times in parallel:\n\n``` python\n# test_tensorflow_session.py\nimport tensorflow as tf\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\nsess = tf.Session()\n```\n\nBash command to run it in parallel:\n\n``` bash\nfor i in {1..48}; do ((python ./test_tensorflow_session.py 2> output$i.err 1> output$i.out ) &) ; done\n```\n\nIf you then look into the `output*.err` files, you will see that most of the processes crashed. The output will look like this:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: gpu1\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: gpu1\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:226] kernel version seems to match DSO: 352.79\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: \nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\n```\n\nFor the sake of completeness, here is the use case: applying a Tensorflow model is just a small part in processing large amounts of data; processing is parallelised simply by using multiple processes.\n\nI also tried wrapping the `tf.Session()` call in `with tf.device('/cpu:0'):`, but that didn't change anything. I assume that it is trying to get exclusive access to the GPU, and if it can't do that, it crashes. This is a bit annoying, since by setting `os.environ['CUDA_VISIBLE_DEVICES'] = ''`, I am actively trying to disable the GPU.\n\nIdeas?\n"}