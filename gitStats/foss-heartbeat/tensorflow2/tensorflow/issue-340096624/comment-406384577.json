{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/406384577", "html_url": "https://github.com/tensorflow/tensorflow/issues/20684#issuecomment-406384577", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20684", "id": 406384577, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjM4NDU3Nw==", "user": {"login": "G-mel", "id": 7696807, "node_id": "MDQ6VXNlcjc2OTY4MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7696807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/G-mel", "html_url": "https://github.com/G-mel", "followers_url": "https://api.github.com/users/G-mel/followers", "following_url": "https://api.github.com/users/G-mel/following{/other_user}", "gists_url": "https://api.github.com/users/G-mel/gists{/gist_id}", "starred_url": "https://api.github.com/users/G-mel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/G-mel/subscriptions", "organizations_url": "https://api.github.com/users/G-mel/orgs", "repos_url": "https://api.github.com/users/G-mel/repos", "events_url": "https://api.github.com/users/G-mel/events{/privacy}", "received_events_url": "https://api.github.com/users/G-mel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-19T19:14:31Z", "updated_at": "2018-07-19T19:19:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15260298\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kanul\">@kanul</a> I've looked around at the deeplab source code and the code for training scope already appears on lines 58, 85 and 318 in <a href=\"https://github.com/tensorflow/models/blob/master/research/deeplab/core/feature_extractor.py\">feature_extractor.py</a>.</p>\n<p>It also looks like the training scope in DeepLab is handled properly if <code>is_training=False</code> is being passed into the training scope in <code>feature_extractor.py</code>.</p>\n<p>you can take a look at <a href=\"https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py#L415\">mobilenet.py</a> to see how <code>is_training</code> is handled in the scope.</p>\n<p>If the <code>train.py</code> and the <code>eval.py</code> training scopes are being handled properly, then you have to add <code>tf.contrib.quantize.create_training_graph(quant_delay=quant_delay)</code> into <code>train.py</code> to rewrite its graph with fake quant nodes, and add <code>tf.contrib.quantize.create_eval_graph()</code> into <code>eval.py</code> in order to rewrite the deeplab graph without training nodes into the eval graph with quant nodes.</p>\n<p>Make sure to save the checkpoints from <code>train.py</code>, and save the graph from <code>eval.py</code> (after adding eval quant nodes to graph), then using the training weights and the eval graph, follow the steps I said in my previous comments to generate the quantized tflite file.</p>\n<p>Hopefully it ends up working out with deeplab</p>", "body_text": "@kanul I've looked around at the deeplab source code and the code for training scope already appears on lines 58, 85 and 318 in feature_extractor.py.\nIt also looks like the training scope in DeepLab is handled properly if is_training=False is being passed into the training scope in feature_extractor.py.\nyou can take a look at mobilenet.py to see how is_training is handled in the scope.\nIf the train.py and the eval.py training scopes are being handled properly, then you have to add tf.contrib.quantize.create_training_graph(quant_delay=quant_delay) into train.py to rewrite its graph with fake quant nodes, and add tf.contrib.quantize.create_eval_graph() into eval.py in order to rewrite the deeplab graph without training nodes into the eval graph with quant nodes.\nMake sure to save the checkpoints from train.py, and save the graph from eval.py (after adding eval quant nodes to graph), then using the training weights and the eval graph, follow the steps I said in my previous comments to generate the quantized tflite file.\nHopefully it ends up working out with deeplab", "body": "@kanul I've looked around at the deeplab source code and the code for training scope already appears on lines 58, 85 and 318 in [feature_extractor.py](https://github.com/tensorflow/models/blob/master/research/deeplab/core/feature_extractor.py).\r\n\r\nIt also looks like the training scope in DeepLab is handled properly if `is_training=False` is being passed into the training scope in `feature_extractor.py`.\r\n\r\nyou can take a look at [mobilenet.py](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py#L415) to see how `is_training` is handled in the scope.\r\n\r\nIf the `train.py` and the `eval.py` training scopes are being handled properly, then you have to add `tf.contrib.quantize.create_training_graph(quant_delay=quant_delay)` into `train.py` to rewrite its graph with fake quant nodes, and add `tf.contrib.quantize.create_eval_graph()` into `eval.py` in order to rewrite the deeplab graph without training nodes into the eval graph with quant nodes.\r\n\r\nMake sure to save the checkpoints from `train.py`, and save the graph from `eval.py` (after adding eval quant nodes to graph), then using the training weights and the eval graph, follow the steps I said in my previous comments to generate the quantized tflite file.\r\n\r\nHopefully it ends up working out with deeplab"}