{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/407292491", "html_url": "https://github.com/tensorflow/tensorflow/issues/20684#issuecomment-407292491", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20684", "id": 407292491, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzI5MjQ5MQ==", "user": {"login": "WenguoLi", "id": 31765154, "node_id": "MDQ6VXNlcjMxNzY1MTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/31765154?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WenguoLi", "html_url": "https://github.com/WenguoLi", "followers_url": "https://api.github.com/users/WenguoLi/followers", "following_url": "https://api.github.com/users/WenguoLi/following{/other_user}", "gists_url": "https://api.github.com/users/WenguoLi/gists{/gist_id}", "starred_url": "https://api.github.com/users/WenguoLi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WenguoLi/subscriptions", "organizations_url": "https://api.github.com/users/WenguoLi/orgs", "repos_url": "https://api.github.com/users/WenguoLi/repos", "events_url": "https://api.github.com/users/WenguoLi/events{/privacy}", "received_events_url": "https://api.github.com/users/WenguoLi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-24T06:10:47Z", "updated_at": "2018-07-24T11:44:54Z", "author_association": "NONE", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7696807\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/G-mel\">@G-mel</a> , <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15260298\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kanul\">@kanul</a> ,<br>\nI got the same error.</p>\n<p><code>bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=${MODEL_PATH}/frozen_inference_graph_stripped.pb   --output_file=${MODEL_PATH}/deeplabv3+_quan.lite   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=QUANTIZED_UINT8   --input_shapes=1,513,513,3   --input_arrays=sub_7   --output_arrays=ResizeBilinear_3   --std_values=128   --mean_values=127   --dump_graphviz=${MODEL_PATH}</code></p>\n<pre><code>2018-07-24 13:39:12.271875: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 639 operators, 1220 arrays (0 quantized)\n2018-07-24 13:39:12.293302: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 639 operators, 1220 arrays (0 quantized)\n2018-07-24 13:39:12.537359: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 166 operators, 340 arrays (1 quantized)\n2018-07-24 13:39:12.541454: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 166 operators, 340 arrays (1 quantized)\n2018-07-24 13:39:12.541764: W tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.\n2018-07-24 13:39:12.543031: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 111 operators, 285 arrays (1 quantized)\n2018-07-24 13:39:12.544625: F tensorflow/contrib/lite/toco/tooling_util.cc:1607] **Array MobilenetV2/expanded_conv_7/depthwise/depthwise/SpaceToBatchND, which is an input to the DepthwiseConv operator producing the output array MobilenetV2/expanded_conv_7/depthwise/depthwise, is lacking min/max data, which is necessary for quantization.** Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\nAborted (core dumped)\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1450614\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/suharshs\">@suharshs</a>,<br>\nIt seems that SpaceToBatchND operation is  unsupported  by the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py\">quantization rewrite</a> .</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7696807\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/G-mel\">@G-mel</a> ,<br>\nI add with <code>tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):</code> to <a href=\"https://github.com/tensorflow/models/blob/master/research/deeplab/core/feature_extractor.py#L60\">feature_extractor.py#L60</a>, there are still the same  errors!!</p>\n<pre><code>with tf.variable_scope(scope, 'MobilenetV2', [net], reuse=reuse) as scope:\n    with slim.arg_scope(mobilenet_v2.training_scope(is_training=is_training)):\n        # TODO(b/68150321): Enable fused batch norm once quantization\n        # supports it.\n        with slim.arg_scope([slim.batch_norm], fused=True):#False):  \n            return mobilenet_v2.mobilenet_base(\n                net,\n                conv_defs=mobilenet_v2.V2_DEF,\n                depth_multiplier=depth_multiplier,\n                min_depth=8 if depth_multiplier == 1.0 else 1,\n                divisible_by=8 if depth_multiplier == 1.0 else 1,\n                final_endpoint=final_endpoint or _MOBILENET_V2_FINAL_ENDPOINT,\n                output_stride=output_stride,\n                scope=scope,\n                is_training=is_training)\n</code></pre>\n<p>Thanks!</p>", "body_text": "Hi, @G-mel , @kanul ,\nI got the same error.\nbazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=${MODEL_PATH}/frozen_inference_graph_stripped.pb   --output_file=${MODEL_PATH}/deeplabv3+_quan.lite   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=QUANTIZED_UINT8   --input_shapes=1,513,513,3   --input_arrays=sub_7   --output_arrays=ResizeBilinear_3   --std_values=128   --mean_values=127   --dump_graphviz=${MODEL_PATH}\n2018-07-24 13:39:12.271875: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 639 operators, 1220 arrays (0 quantized)\n2018-07-24 13:39:12.293302: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 639 operators, 1220 arrays (0 quantized)\n2018-07-24 13:39:12.537359: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 166 operators, 340 arrays (1 quantized)\n2018-07-24 13:39:12.541454: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 166 operators, 340 arrays (1 quantized)\n2018-07-24 13:39:12.541764: W tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.\n2018-07-24 13:39:12.543031: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 111 operators, 285 arrays (1 quantized)\n2018-07-24 13:39:12.544625: F tensorflow/contrib/lite/toco/tooling_util.cc:1607] **Array MobilenetV2/expanded_conv_7/depthwise/depthwise/SpaceToBatchND, which is an input to the DepthwiseConv operator producing the output array MobilenetV2/expanded_conv_7/depthwise/depthwise, is lacking min/max data, which is necessary for quantization.** Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\nAborted (core dumped)\n\n@suharshs,\nIt seems that SpaceToBatchND operation is  unsupported  by the quantization rewrite .\n@G-mel ,\nI add with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()): to feature_extractor.py#L60, there are still the same  errors!!\nwith tf.variable_scope(scope, 'MobilenetV2', [net], reuse=reuse) as scope:\n    with slim.arg_scope(mobilenet_v2.training_scope(is_training=is_training)):\n        # TODO(b/68150321): Enable fused batch norm once quantization\n        # supports it.\n        with slim.arg_scope([slim.batch_norm], fused=True):#False):  \n            return mobilenet_v2.mobilenet_base(\n                net,\n                conv_defs=mobilenet_v2.V2_DEF,\n                depth_multiplier=depth_multiplier,\n                min_depth=8 if depth_multiplier == 1.0 else 1,\n                divisible_by=8 if depth_multiplier == 1.0 else 1,\n                final_endpoint=final_endpoint or _MOBILENET_V2_FINAL_ENDPOINT,\n                output_stride=output_stride,\n                scope=scope,\n                is_training=is_training)\n\nThanks!", "body": "Hi, @G-mel , @kanul ,\r\nI got the same error.\r\n\r\n\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=${MODEL_PATH}/frozen_inference_graph_stripped.pb   --output_file=${MODEL_PATH}/deeplabv3+_quan.lite   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=QUANTIZED_UINT8   --input_shapes=1,513,513,3   --input_arrays=sub_7   --output_arrays=ResizeBilinear_3   --std_values=128   --mean_values=127   --dump_graphviz=${MODEL_PATH}`\r\n\r\n```\r\n2018-07-24 13:39:12.271875: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 639 operators, 1220 arrays (0 quantized)\r\n2018-07-24 13:39:12.293302: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 639 operators, 1220 arrays (0 quantized)\r\n2018-07-24 13:39:12.537359: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 166 operators, 340 arrays (1 quantized)\r\n2018-07-24 13:39:12.541454: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 166 operators, 340 arrays (1 quantized)\r\n2018-07-24 13:39:12.541764: W tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.\r\n2018-07-24 13:39:12.543031: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 111 operators, 285 arrays (1 quantized)\r\n2018-07-24 13:39:12.544625: F tensorflow/contrib/lite/toco/tooling_util.cc:1607] **Array MobilenetV2/expanded_conv_7/depthwise/depthwise/SpaceToBatchND, which is an input to the DepthwiseConv operator producing the output array MobilenetV2/expanded_conv_7/depthwise/depthwise, is lacking min/max data, which is necessary for quantization.** Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\nAborted (core dumped)\r\n```\r\n\r\n@suharshs,\r\nIt seems that SpaceToBatchND operation is  unsupported  by the [quantization rewrite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/python/quantize.py) .\r\n\r\n@G-mel ,\r\nI add with `tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):` to [feature_extractor.py#L60](https://github.com/tensorflow/models/blob/master/research/deeplab/core/feature_extractor.py#L60), there are still the same  errors!!\r\n\r\n```\r\nwith tf.variable_scope(scope, 'MobilenetV2', [net], reuse=reuse) as scope:\r\n    with slim.arg_scope(mobilenet_v2.training_scope(is_training=is_training)):\r\n        # TODO(b/68150321): Enable fused batch norm once quantization\r\n        # supports it.\r\n        with slim.arg_scope([slim.batch_norm], fused=True):#False):  \r\n            return mobilenet_v2.mobilenet_base(\r\n                net,\r\n                conv_defs=mobilenet_v2.V2_DEF,\r\n                depth_multiplier=depth_multiplier,\r\n                min_depth=8 if depth_multiplier == 1.0 else 1,\r\n                divisible_by=8 if depth_multiplier == 1.0 else 1,\r\n                final_endpoint=final_endpoint or _MOBILENET_V2_FINAL_ENDPOINT,\r\n                output_stride=output_stride,\r\n                scope=scope,\r\n                is_training=is_training)\r\n```\r\nThanks!\r\n"}