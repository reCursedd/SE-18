{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7755", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7755/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7755/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7755/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7755", "id": 209319588, "node_id": "MDU6SXNzdWUyMDkzMTk1ODg=", "number": 7755, "title": "sess.run(tf.global_variables_initializer()) is slow after installing 1.0.0.", "user": {"login": "chrisranderson", "id": 5461398, "node_id": "MDQ6VXNlcjU0NjEzOTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/5461398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrisranderson", "html_url": "https://github.com/chrisranderson", "followers_url": "https://api.github.com/users/chrisranderson/followers", "following_url": "https://api.github.com/users/chrisranderson/following{/other_user}", "gists_url": "https://api.github.com/users/chrisranderson/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrisranderson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrisranderson/subscriptions", "organizations_url": "https://api.github.com/users/chrisranderson/orgs", "repos_url": "https://api.github.com/users/chrisranderson/repos", "events_url": "https://api.github.com/users/chrisranderson/events{/privacy}", "received_events_url": "https://api.github.com/users/chrisranderson/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 19, "created_at": "2017-02-22T00:51:53Z", "updated_at": "2017-06-16T22:12:39Z", "closed_at": "2017-06-16T22:12:39Z", "author_association": "NONE", "body_html": "<h3>The issue</h3>\n<p><code>sess.run(tf.global_variables_initializer())</code> intermittently takes a long time since installing 1.0.0. I believe this also happened on my Python 2 0.12 install, but only after installing 1.0.0 for Python 3. This particular run was in Python 2, and the initializer took 32 seconds - see bottom of this post (and the attached cprofile output) for more detail.</p>\n<p>Strangely, it seems like the more I run the same file, the faster it runs.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>None.</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 16.04</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<pre><code>ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Oct 27 16:26 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Oct 27 16:26 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn_static.a\n</code></pre>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>A link to the pip package you installed: <code>sudo pip install tensorflow-gpu</code> on 2/21/17</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>. 1.0.0</li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">from</span> collections <span class=\"pl-k\">import</span> deque\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Importing tensorflow<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.examples.tutorials.mnist <span class=\"pl-k\">import</span> input_data\n\nxavier <span class=\"pl-k\">=</span> tf.contrib.layers.xavier_initializer\nbatch_norm <span class=\"pl-k\">=</span> tf.contrib.layers.batch_norm\n\ntf.set_random_seed(<span class=\"pl-c1\">1</span>)\n\nflags <span class=\"pl-k\">=</span> tf.app.flags\n<span class=\"pl-c1\">FLAGS</span> <span class=\"pl-k\">=</span> flags.<span class=\"pl-c1\">FLAGS</span>\nflags.DEFINE_string(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>data_dir<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>data/<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Directory for storing data<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Reading data.<span class=\"pl-pds\">'</span></span>)\nmnist <span class=\"pl-k\">=</span> input_data.read_data_sets(<span class=\"pl-c1\">FLAGS</span>.data_dir, <span class=\"pl-v\">one_hot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ninitialization_variance <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Creating a session.<span class=\"pl-pds\">'</span></span>)\nsess <span class=\"pl-k\">=</span> tf.Session()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">fully_connected_layer</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">output_size</span>, <span class=\"pl-smi\">scope</span>, <span class=\"pl-smi\">nonlinearity</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n  <span class=\"pl-k\">with</span> tf.variable_scope(scope):\n    input_size <span class=\"pl-k\">=</span> inputs.get_shape()[<span class=\"pl-c1\">1</span>].value\n\n    weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>, \n                              [input_size, output_size],\n                              <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>))\n\n    bias <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>b<span class=\"pl-pds\">'</span></span>, \n                           [output_size],\n                           <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>))\n\n    multiplied <span class=\"pl-k\">=</span> tf.matmul(inputs, weights)\n\n    <span class=\"pl-k\">return</span> tf.nn.elu(multiplied <span class=\"pl-k\">+</span> bias) <span class=\"pl-k\">if</span> nonlinearity <span class=\"pl-k\">else</span> multiplied <span class=\"pl-k\">+</span> bias\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Setting up the model.<span class=\"pl-pds\">'</span></span>)\ninput_images <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">784</span>])\nlabels <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">10</span>]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> labels</span>\nlearning_rate_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning-rate<span class=\"pl-pds\">'</span></span>)\n\nlayer_a <span class=\"pl-k\">=</span> fully_connected_layer((input_images), <span class=\"pl-c1\">50</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>a<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">5</span>):\n  layer_a <span class=\"pl-k\">=</span> batch_norm(fully_connected_layer(layer_a, <span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">str</span>(i)))\n\nlayer_b <span class=\"pl-k\">=</span> fully_connected_layer(layer_a, <span class=\"pl-c1\">10</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>b<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">nonlinearity</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\npredictions <span class=\"pl-k\">=</span> tf.nn.softmax(layer_b) \n \ncross_entropy <span class=\"pl-k\">=</span> tf.reduce_mean(<span class=\"pl-k\">-</span>tf.reduce_sum(labels <span class=\"pl-k\">*</span> tf.log(predictions), <span class=\"pl-v\">reduction_indices</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>]))\ntrain_step <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(learning_rate_placeholder).minimize(cross_entropy)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Initializing variables.<span class=\"pl-pds\">'</span></span>)\nsess.run(tf.global_variables_initializer())</pre></div>\n<h3>What other attempted solutions have you tried?</h3>\n<p>None.</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>Some output from <code>python2 -m cProfile -o out.profile 1-mnist-simple.py</code>. This is a call to <code>sess.run(tf.global_variables_initializer())</code>.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/5461398/23191984/132ca252-f85e-11e6-8c55-9b48adc3d40b.png\"><img src=\"https://cloud.githubusercontent.com/assets/5461398/23191984/132ca252-f85e-11e6-8c55-9b48adc3d40b.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/791923/cprofile-output.profile.zip\">cprofile-output.profile.zip</a></p>", "body_text": "The issue\nsess.run(tf.global_variables_initializer()) intermittently takes a long time since installing 1.0.0. I believe this also happened on my Python 2 0.12 install, but only after installing 1.0.0 for Python 3. This particular run was in Python 2, and the initializer took 32 seconds - see bottom of this post (and the attached cprofile output) for more detail.\nStrangely, it seems like the more I run the same file, the faster it runs.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nNone.\nEnvironment info\nOperating System: Ubuntu 16.04\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Oct 27 16:26 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Oct 27 16:26 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n\nA link to the pip package you installed: sudo pip install tensorflow-gpu on 2/21/17\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\". 1.0.0\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport time\nfrom collections import deque\n\nimport numpy as np\nprint('Importing tensorflow')\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nxavier = tf.contrib.layers.xavier_initializer\nbatch_norm = tf.contrib.layers.batch_norm\n\ntf.set_random_seed(1)\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('data_dir', 'data/', 'Directory for storing data')\nprint('Reading data.')\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\ninitialization_variance = 0.1\n\nprint('Creating a session.')\nsess = tf.Session()\n\ndef fully_connected_layer(inputs, output_size, scope, nonlinearity=True):\n  with tf.variable_scope(scope):\n    input_size = inputs.get_shape()[1].value\n\n    weights = tf.get_variable('w', \n                              [input_size, output_size],\n                              initializer=tf.constant_initializer(0.0))\n\n    bias = tf.get_variable('b', \n                           [output_size],\n                           initializer=tf.constant_initializer(0.0))\n\n    multiplied = tf.matmul(inputs, weights)\n\n    return tf.nn.elu(multiplied + bias) if nonlinearity else multiplied + bias\n\nprint('Setting up the model.')\ninput_images = tf.placeholder(tf.float32, [None, 784])\nlabels = tf.placeholder(tf.float32, [None, 10]) # labels\nlearning_rate_placeholder = tf.placeholder(tf.float32, name='learning-rate')\n\nlayer_a = fully_connected_layer((input_images), 50, 'a')\n\nfor i in range(5):\n  layer_a = batch_norm(fully_connected_layer(layer_a, 50, str(i)))\n\nlayer_b = fully_connected_layer(layer_a, 10, 'b', nonlinearity=False)\n\npredictions = tf.nn.softmax(layer_b) \n \ncross_entropy = tf.reduce_mean(-tf.reduce_sum(labels * tf.log(predictions), reduction_indices=[1]))\ntrain_step = tf.train.AdamOptimizer(learning_rate_placeholder).minimize(cross_entropy)\n\nprint('Initializing variables.')\nsess.run(tf.global_variables_initializer())\nWhat other attempted solutions have you tried?\nNone.\nLogs or other output that would be helpful\nSome output from python2 -m cProfile -o out.profile 1-mnist-simple.py. This is a call to sess.run(tf.global_variables_initializer()).\n\ncprofile-output.profile.zip", "body": "### The issue\r\n\r\n`sess.run(tf.global_variables_initializer())` intermittently takes a long time since installing 1.0.0. I believe this also happened on my Python 2 0.12 install, but only after installing 1.0.0 for Python 3. This particular run was in Python 2, and the initializer took 32 seconds - see bottom of this post (and the attached cprofile output) for more detail.\r\n\r\nStrangely, it seems like the more I run the same file, the faster it runs.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root   560184 Oct 27 16:26 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\r\n-rwxr-xr-x 1 root root   394472 Oct 27 16:26 /usr/local/cuda/lib64/libcudart.so.8.0.27\r\n-rw-r--r-- 1 root root   737516 Oct 27 16:26 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 60696704 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn.so.5.1.3\r\n-rw-r--r-- 1 root root 59715990 Oct 27 16:31 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: `sudo pip install tensorflow-gpu` on 2/21/17\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport time\r\nfrom collections import deque\r\n\r\nimport numpy as np\r\nprint('Importing tensorflow')\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nxavier = tf.contrib.layers.xavier_initializer\r\nbatch_norm = tf.contrib.layers.batch_norm\r\n\r\ntf.set_random_seed(1)\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_string('data_dir', 'data/', 'Directory for storing data')\r\nprint('Reading data.')\r\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\r\ninitialization_variance = 0.1\r\n\r\nprint('Creating a session.')\r\nsess = tf.Session()\r\n\r\ndef fully_connected_layer(inputs, output_size, scope, nonlinearity=True):\r\n  with tf.variable_scope(scope):\r\n    input_size = inputs.get_shape()[1].value\r\n\r\n    weights = tf.get_variable('w', \r\n                              [input_size, output_size],\r\n                              initializer=tf.constant_initializer(0.0))\r\n\r\n    bias = tf.get_variable('b', \r\n                           [output_size],\r\n                           initializer=tf.constant_initializer(0.0))\r\n\r\n    multiplied = tf.matmul(inputs, weights)\r\n\r\n    return tf.nn.elu(multiplied + bias) if nonlinearity else multiplied + bias\r\n\r\nprint('Setting up the model.')\r\ninput_images = tf.placeholder(tf.float32, [None, 784])\r\nlabels = tf.placeholder(tf.float32, [None, 10]) # labels\r\nlearning_rate_placeholder = tf.placeholder(tf.float32, name='learning-rate')\r\n\r\nlayer_a = fully_connected_layer((input_images), 50, 'a')\r\n\r\nfor i in range(5):\r\n  layer_a = batch_norm(fully_connected_layer(layer_a, 50, str(i)))\r\n\r\nlayer_b = fully_connected_layer(layer_a, 10, 'b', nonlinearity=False)\r\n\r\npredictions = tf.nn.softmax(layer_b) \r\n \r\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(labels * tf.log(predictions), reduction_indices=[1]))\r\ntrain_step = tf.train.AdamOptimizer(learning_rate_placeholder).minimize(cross_entropy)\r\n\r\nprint('Initializing variables.')\r\nsess.run(tf.global_variables_initializer())\r\n```\r\n### What other attempted solutions have you tried?\r\n\r\nNone.\r\n\r\n### Logs or other output that would be helpful\r\n\r\nSome output from `python2 -m cProfile -o out.profile 1-mnist-simple.py`. This is a call to `sess.run(tf.global_variables_initializer())`.\r\n![image](https://cloud.githubusercontent.com/assets/5461398/23191984/132ca252-f85e-11e6-8c55-9b48adc3d40b.png)\r\n\r\n\r\n[cprofile-output.profile.zip](https://github.com/tensorflow/tensorflow/files/791923/cprofile-output.profile.zip)\r\n\r\n\r\n"}