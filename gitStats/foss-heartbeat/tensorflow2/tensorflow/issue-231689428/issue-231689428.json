{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10226", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10226/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10226/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10226/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10226", "id": 231689428, "node_id": "MDU6SXNzdWUyMzE2ODk0Mjg=", "number": 10226, "title": "Inconsistent Tensor Initialization on Multiple GPUs", "user": {"login": "jthestness", "id": 28744304, "node_id": "MDQ6VXNlcjI4NzQ0MzA0", "avatar_url": "https://avatars0.githubusercontent.com/u/28744304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jthestness", "html_url": "https://github.com/jthestness", "followers_url": "https://api.github.com/users/jthestness/followers", "following_url": "https://api.github.com/users/jthestness/following{/other_user}", "gists_url": "https://api.github.com/users/jthestness/gists{/gist_id}", "starred_url": "https://api.github.com/users/jthestness/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jthestness/subscriptions", "organizations_url": "https://api.github.com/users/jthestness/orgs", "repos_url": "https://api.github.com/users/jthestness/repos", "events_url": "https://api.github.com/users/jthestness/events{/privacy}", "received_events_url": "https://api.github.com/users/jthestness/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-26T17:33:03Z", "updated_at": "2017-05-31T17:30:44Z", "closed_at": "2017-05-31T17:30:44Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>The problem (bug?):</h3>\n<p>I'm having trouble getting consistent initialization of variables across multiple GPUs, and it appears to be a bug. Below is a test that replicates the bug.</p>\n<p>Basically, the test just sets up a tensor on each GPU and an initializer for each. After running initialization and grabbing the initialized tensors, they do not match despite the same initializer configurations. The problem exists across multiple platforms, any number of GPUs &gt; 1, and multiple TF versions.</p>\n<p>Gory details: The inconsistency is non-deterministic, occurring in about 50% of runs with 2 GPUs. Roughly 0.0002% of matrix values do not match. The matrix indices that do not match have significantly different values (i.e. greater than FP rounding errors). Assuming row-major tensor storage, the incorrect indices are in contiguous groups of 4 floats (16B), and the distance - in memory addresses - between these groups is consistent but platform dependent (e.g. 512kB stride between groups on GTX980 vs. 480kB between groups on K40m)</p>\n<ul>\n<li><strong>I have written custom code</strong>:</li>\n</ul>\n<pre><code>import numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.python.platform import test\n\n\nclass AllreduceTest(test.TestCase):\n    def dumpFailure(self, my_rank, num_ranks, first_output, second_output):\n        out_dims = first_output.shape\n        assert(len(out_dims) == 2)\n        for i in range(out_dims[0]):\n            for j in range(out_dims[1]):\n                if first_output[i][j] != second_output[i][j]:\n                    print(\"{}: [{}][{}]: {} {}\"\n                          .format(my_rank, i, j, first_output[i][j],\n                                  second_output[i][j]),\n                          flush=True)\n\n    def test_mpi_allreduce(self):\n        num_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\n        gpu_indices = [index for index in range(num_gpus)]\n\n        mat_dim = 3072\n\n        outputs = []\n        for index in gpu_indices:\n            with tf.device(\"/gpu:{}\".format(index)):\n                initer = tf.random_uniform_initializer(-0.1, 0.1, seed=1234,\n                                                       dtype=tf.float32)\n                outputs.append(tf.get_variable(\"outputs-{}\".format(index),\n                                               shape=(mat_dim, mat_dim),\n                                               dtype=tf.float32,\n                                               initializer=initer))\n\n        # Session to test initialization across multiple GPUs\n        gpu_options = tf.GPUOptions(\n            visible_device_list=','.join(str(idx) for idx in gpu_indices))\n        config = tf.ConfigProto(gpu_options=gpu_options)\n        with tf.Session(config=config) as sess:\n            sess.run(tf.global_variables_initializer())\n            output_result = sess.run(outputs)\n            for index in gpu_indices:\n                if not np.allclose(output_result[0], output_result[index]):\n                    print(\"CRAP: Init outputs 0 and {} do not match\"\n                          .format(index), flush=True)\n                    self.dumpFailure(index, num_gpus, output_result[0],\n                                     output_result[index])\n                    assert(np.allclose(output_result[0],\n                                       output_result[index]))\n\nif __name__ == '__main__':\n    test.main()\n</code></pre>\n<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 14.04.2</li>\n<li><strong>TensorFlow installed from</strong>: source</li>\n<li><strong>TensorFlow version</strong>: 1.0.1 (<code>tf.GIT_VERSION = b'v1.0.1-0-ge895d5c', tf.COMPILER_VERSION = b'v1.0.1-0-ge895d5c', protobuf = 3.1.0</code>) and 1.1.0-rc2 (<code>tf.GIT_VERSION = b'v1.1.0-rc2-1164-g1d993dd', tf.COMPILER_VERSION = b'v1.1.0-rc2-1164-g1d993dd', protobuf = 3.3.0</code>)</li>\n<li><strong>Bazel version</strong>: 0.45</li>\n<li><strong>Numpy version</strong>: 1.12.1</li>\n<li><strong>CUDA/cuDNN version</strong>: cuda-8.0, cudnn-6</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 980, TITAN X Maxwell, Tesla K40m, Tesla M40 24GB</li>\n</ul>", "body_text": "The problem (bug?):\nI'm having trouble getting consistent initialization of variables across multiple GPUs, and it appears to be a bug. Below is a test that replicates the bug.\nBasically, the test just sets up a tensor on each GPU and an initializer for each. After running initialization and grabbing the initialized tensors, they do not match despite the same initializer configurations. The problem exists across multiple platforms, any number of GPUs > 1, and multiple TF versions.\nGory details: The inconsistency is non-deterministic, occurring in about 50% of runs with 2 GPUs. Roughly 0.0002% of matrix values do not match. The matrix indices that do not match have significantly different values (i.e. greater than FP rounding errors). Assuming row-major tensor storage, the incorrect indices are in contiguous groups of 4 floats (16B), and the distance - in memory addresses - between these groups is consistent but platform dependent (e.g. 512kB stride between groups on GTX980 vs. 480kB between groups on K40m)\n\nI have written custom code:\n\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.python.platform import test\n\n\nclass AllreduceTest(test.TestCase):\n    def dumpFailure(self, my_rank, num_ranks, first_output, second_output):\n        out_dims = first_output.shape\n        assert(len(out_dims) == 2)\n        for i in range(out_dims[0]):\n            for j in range(out_dims[1]):\n                if first_output[i][j] != second_output[i][j]:\n                    print(\"{}: [{}][{}]: {} {}\"\n                          .format(my_rank, i, j, first_output[i][j],\n                                  second_output[i][j]),\n                          flush=True)\n\n    def test_mpi_allreduce(self):\n        num_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\n        gpu_indices = [index for index in range(num_gpus)]\n\n        mat_dim = 3072\n\n        outputs = []\n        for index in gpu_indices:\n            with tf.device(\"/gpu:{}\".format(index)):\n                initer = tf.random_uniform_initializer(-0.1, 0.1, seed=1234,\n                                                       dtype=tf.float32)\n                outputs.append(tf.get_variable(\"outputs-{}\".format(index),\n                                               shape=(mat_dim, mat_dim),\n                                               dtype=tf.float32,\n                                               initializer=initer))\n\n        # Session to test initialization across multiple GPUs\n        gpu_options = tf.GPUOptions(\n            visible_device_list=','.join(str(idx) for idx in gpu_indices))\n        config = tf.ConfigProto(gpu_options=gpu_options)\n        with tf.Session(config=config) as sess:\n            sess.run(tf.global_variables_initializer())\n            output_result = sess.run(outputs)\n            for index in gpu_indices:\n                if not np.allclose(output_result[0], output_result[index]):\n                    print(\"CRAP: Init outputs 0 and {} do not match\"\n                          .format(index), flush=True)\n                    self.dumpFailure(index, num_gpus, output_result[0],\n                                     output_result[index])\n                    assert(np.allclose(output_result[0],\n                                       output_result[index]))\n\nif __name__ == '__main__':\n    test.main()\n\nSystem information\n\nOS Platform and Distribution: Linux Ubuntu 14.04.2\nTensorFlow installed from: source\nTensorFlow version: 1.0.1 (tf.GIT_VERSION = b'v1.0.1-0-ge895d5c', tf.COMPILER_VERSION = b'v1.0.1-0-ge895d5c', protobuf = 3.1.0) and 1.1.0-rc2 (tf.GIT_VERSION = b'v1.1.0-rc2-1164-g1d993dd', tf.COMPILER_VERSION = b'v1.1.0-rc2-1164-g1d993dd', protobuf = 3.3.0)\nBazel version: 0.45\nNumpy version: 1.12.1\nCUDA/cuDNN version: cuda-8.0, cudnn-6\nGPU model and memory: GeForce GTX 980, TITAN X Maxwell, Tesla K40m, Tesla M40 24GB", "body": "### The problem (bug?):\r\nI'm having trouble getting consistent initialization of variables across multiple GPUs, and it appears to be a bug. Below is a test that replicates the bug.\r\n\r\nBasically, the test just sets up a tensor on each GPU and an initializer for each. After running initialization and grabbing the initialized tensors, they do not match despite the same initializer configurations. The problem exists across multiple platforms, any number of GPUs > 1, and multiple TF versions.\r\n\r\nGory details: The inconsistency is non-deterministic, occurring in about 50% of runs with 2 GPUs. Roughly 0.0002% of matrix values do not match. The matrix indices that do not match have significantly different values (i.e. greater than FP rounding errors). Assuming row-major tensor storage, the incorrect indices are in contiguous groups of 4 floats (16B), and the distance - in memory addresses - between these groups is consistent but platform dependent (e.g. 512kB stride between groups on GTX980 vs. 480kB between groups on K40m)\r\n\r\n- **I have written custom code**:\r\n```\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import test\r\n\r\n\r\nclass AllreduceTest(test.TestCase):\r\n    def dumpFailure(self, my_rank, num_ranks, first_output, second_output):\r\n        out_dims = first_output.shape\r\n        assert(len(out_dims) == 2)\r\n        for i in range(out_dims[0]):\r\n            for j in range(out_dims[1]):\r\n                if first_output[i][j] != second_output[i][j]:\r\n                    print(\"{}: [{}][{}]: {} {}\"\r\n                          .format(my_rank, i, j, first_output[i][j],\r\n                                  second_output[i][j]),\r\n                          flush=True)\r\n\r\n    def test_mpi_allreduce(self):\r\n        num_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\r\n        gpu_indices = [index for index in range(num_gpus)]\r\n\r\n        mat_dim = 3072\r\n\r\n        outputs = []\r\n        for index in gpu_indices:\r\n            with tf.device(\"/gpu:{}\".format(index)):\r\n                initer = tf.random_uniform_initializer(-0.1, 0.1, seed=1234,\r\n                                                       dtype=tf.float32)\r\n                outputs.append(tf.get_variable(\"outputs-{}\".format(index),\r\n                                               shape=(mat_dim, mat_dim),\r\n                                               dtype=tf.float32,\r\n                                               initializer=initer))\r\n\r\n        # Session to test initialization across multiple GPUs\r\n        gpu_options = tf.GPUOptions(\r\n            visible_device_list=','.join(str(idx) for idx in gpu_indices))\r\n        config = tf.ConfigProto(gpu_options=gpu_options)\r\n        with tf.Session(config=config) as sess:\r\n            sess.run(tf.global_variables_initializer())\r\n            output_result = sess.run(outputs)\r\n            for index in gpu_indices:\r\n                if not np.allclose(output_result[0], output_result[index]):\r\n                    print(\"CRAP: Init outputs 0 and {} do not match\"\r\n                          .format(index), flush=True)\r\n                    self.dumpFailure(index, num_gpus, output_result[0],\r\n                                     output_result[index])\r\n                    assert(np.allclose(output_result[0],\r\n                                       output_result[index]))\r\n\r\nif __name__ == '__main__':\r\n    test.main()\r\n```\r\n\r\n### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 14.04.2\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: 1.0.1 (`tf.GIT_VERSION = b'v1.0.1-0-ge895d5c', tf.COMPILER_VERSION = b'v1.0.1-0-ge895d5c', protobuf = 3.1.0`) and 1.1.0-rc2 (`tf.GIT_VERSION = b'v1.1.0-rc2-1164-g1d993dd', tf.COMPILER_VERSION = b'v1.1.0-rc2-1164-g1d993dd', protobuf = 3.3.0`)\r\n- **Bazel version**: 0.45\r\n- **Numpy version**: 1.12.1\r\n- **CUDA/cuDNN version**: cuda-8.0, cudnn-6\r\n- **GPU model and memory**: GeForce GTX 980, TITAN X Maxwell, Tesla K40m, Tesla M40 24GB\r\n"}