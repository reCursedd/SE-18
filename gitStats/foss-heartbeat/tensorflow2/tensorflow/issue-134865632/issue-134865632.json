{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1195", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1195/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1195/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1195/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1195", "id": 134865632, "node_id": "MDU6SXNzdWUxMzQ4NjU2MzI=", "number": 1195, "title": "Dynamic Partition Gradient", "user": {"login": "altaetran", "id": 6753285, "node_id": "MDQ6VXNlcjY3NTMyODU=", "avatar_url": "https://avatars1.githubusercontent.com/u/6753285?v=4", "gravatar_id": "", "url": "https://api.github.com/users/altaetran", "html_url": "https://github.com/altaetran", "followers_url": "https://api.github.com/users/altaetran/followers", "following_url": "https://api.github.com/users/altaetran/following{/other_user}", "gists_url": "https://api.github.com/users/altaetran/gists{/gist_id}", "starred_url": "https://api.github.com/users/altaetran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/altaetran/subscriptions", "organizations_url": "https://api.github.com/users/altaetran/orgs", "repos_url": "https://api.github.com/users/altaetran/repos", "events_url": "https://api.github.com/users/altaetran/events{/privacy}", "received_events_url": "https://api.github.com/users/altaetran/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-02-19T13:33:16Z", "updated_at": "2016-02-20T02:06:09Z", "closed_at": "2016-02-20T02:06:09Z", "author_association": "NONE", "body_html": "<p>For bugs/issues, please fill in the following.  The more information you<br>\nprovide, the more likely we can help you.</p>\n<h3>Environment info</h3>\n<p>Operating System: Linux (CentOS) cluster, custom build. tensorflow built from source by IT folks managing the cluster.</p>\n<p>If installed from sources, provide the commit hash:</p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>\n<p>The functions of interest contain many dynamic_partitions</p>\n</li>\n<li>\n<p>I then compute the gradient with respect to the variables for optimization</p>\n</li>\n<li>\n<p>I obtain the following error</p>\n<p>train_op = optimizer.minimize(loss_instance, global_step=global_step)<br>\nFile \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py\", line 186, in minimize<br>\naggregation_method=aggregation_method)<br>\nFile \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients<br>\naggregation_method=aggregation_method)<br>\nFile \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/ops/gradients.py\", line 426, in gradients<br>\n(op.name, op.type))<br>\nLookupError: No gradient defined for operation 'DynamicPartition_1' (op type: DynamicPartition)</p>\n</li>\n</ol>\n<h3>What have you tried?</h3>\n<ol>\n<li>I am currently trying to see if I can remove the dynamic partition from my code, but it would be nice if we could keep the dynamic partition inside; however, I understand why the dynamic partition might not be differentiable if variable indices are used. In my case, I just have a constant placeholder that performs that indexing. Could there be a way to compute gradients in this case? Thank you so much!</li>\n</ol>", "body_text": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\nEnvironment info\nOperating System: Linux (CentOS) cluster, custom build. tensorflow built from source by IT folks managing the cluster.\nIf installed from sources, provide the commit hash:\nSteps to reproduce\n\n\nThe functions of interest contain many dynamic_partitions\n\n\nI then compute the gradient with respect to the variables for optimization\n\n\nI obtain the following error\ntrain_op = optimizer.minimize(loss_instance, global_step=global_step)\nFile \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py\", line 186, in minimize\naggregation_method=aggregation_method)\nFile \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\naggregation_method=aggregation_method)\nFile \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/ops/gradients.py\", line 426, in gradients\n(op.name, op.type))\nLookupError: No gradient defined for operation 'DynamicPartition_1' (op type: DynamicPartition)\n\n\nWhat have you tried?\n\nI am currently trying to see if I can remove the dynamic partition from my code, but it would be nice if we could keep the dynamic partition inside; however, I understand why the dynamic partition might not be differentiable if variable indices are used. In my case, I just have a constant placeholder that performs that indexing. Could there be a way to compute gradients in this case? Thank you so much!", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System: Linux (CentOS) cluster, custom build. tensorflow built from source by IT folks managing the cluster. \n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. The functions of interest contain many dynamic_partitions\n2. I then compute the gradient with respect to the variables for optimization\n3. I obtain the following error\n   \n   train_op = optimizer.minimize(loss_instance, global_step=global_step)\n   File \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py\", line 186, in minimize\n     aggregation_method=aggregation_method)\n   File \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\n     aggregation_method=aggregation_method)\n   File \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/ops/gradients.py\", line 426, in gradients\n     (op.name, op.type))\n   LookupError: No gradient defined for operation 'DynamicPartition_1' (op type: DynamicPartition)\n### What have you tried?\n1. I am currently trying to see if I can remove the dynamic partition from my code, but it would be nice if we could keep the dynamic partition inside; however, I understand why the dynamic partition might not be differentiable if variable indices are used. In my case, I just have a constant placeholder that performs that indexing. Could there be a way to compute gradients in this case? Thank you so much!\n"}