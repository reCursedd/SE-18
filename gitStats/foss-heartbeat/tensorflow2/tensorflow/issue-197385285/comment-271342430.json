{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271342430", "html_url": "https://github.com/tensorflow/tensorflow/issues/6478#issuecomment-271342430", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6478", "id": 271342430, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTM0MjQzMA==", "user": {"login": "PhoenixDai", "id": 3335135, "node_id": "MDQ6VXNlcjMzMzUxMzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3335135?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhoenixDai", "html_url": "https://github.com/PhoenixDai", "followers_url": "https://api.github.com/users/PhoenixDai/followers", "following_url": "https://api.github.com/users/PhoenixDai/following{/other_user}", "gists_url": "https://api.github.com/users/PhoenixDai/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhoenixDai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhoenixDai/subscriptions", "organizations_url": "https://api.github.com/users/PhoenixDai/orgs", "repos_url": "https://api.github.com/users/PhoenixDai/repos", "events_url": "https://api.github.com/users/PhoenixDai/events{/privacy}", "received_events_url": "https://api.github.com/users/PhoenixDai/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-09T17:06:12Z", "updated_at": "2017-01-09T17:06:12Z", "author_association": "NONE", "body_html": "<p>I set batch_norm to is_training=False but still have the same issue. Below is the updated evaluation/test graph:</p>\n<pre><code>graph = tf.Graph()\nwith graph.as_default():\n  batch_images = tf.placeholder(tf.float32, (None,32,32,3))\n  batch_labels = tf.placeholder(tf.float32, (None,10))    \n  is_training = tf.placeholder(tf.bool)\n  \n  with slim.arg_scope([slim.batch_norm], is_training=is_training):\n    features = classifier_features(batch_images, is_training)\n    outputs, output_logits = classifier_outputs(features, dataset.num_classes)\n  output_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_logits, batch_labels)) \n  output_loss_reg = output_loss + tf.add_n(slim.losses.get_regularization_losses())\n\n  train_vars = tf.trainable_variables() \n</code></pre>\n<p>Am I doing it in the right way?</p>", "body_text": "I set batch_norm to is_training=False but still have the same issue. Below is the updated evaluation/test graph:\ngraph = tf.Graph()\nwith graph.as_default():\n  batch_images = tf.placeholder(tf.float32, (None,32,32,3))\n  batch_labels = tf.placeholder(tf.float32, (None,10))    \n  is_training = tf.placeholder(tf.bool)\n  \n  with slim.arg_scope([slim.batch_norm], is_training=is_training):\n    features = classifier_features(batch_images, is_training)\n    outputs, output_logits = classifier_outputs(features, dataset.num_classes)\n  output_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_logits, batch_labels)) \n  output_loss_reg = output_loss + tf.add_n(slim.losses.get_regularization_losses())\n\n  train_vars = tf.trainable_variables() \n\nAm I doing it in the right way?", "body": "I set batch_norm to is_training=False but still have the same issue. Below is the updated evaluation/test graph:\r\n\r\n```\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  batch_images = tf.placeholder(tf.float32, (None,32,32,3))\r\n  batch_labels = tf.placeholder(tf.float32, (None,10))    \r\n  is_training = tf.placeholder(tf.bool)\r\n  \r\n  with slim.arg_scope([slim.batch_norm], is_training=is_training):\r\n    features = classifier_features(batch_images, is_training)\r\n    outputs, output_logits = classifier_outputs(features, dataset.num_classes)\r\n  output_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_logits, batch_labels)) \r\n  output_loss_reg = output_loss + tf.add_n(slim.losses.get_regularization_losses())\r\n\r\n  train_vars = tf.trainable_variables() \r\n```\r\n\r\nAm I doing it in the right way?"}