{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/117131764", "pull_request_review_id": 38811689, "id": 117131764, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNzEzMTc2NA==", "diff_hunk": "@@ -38,55 +42,178 @@ class FixedLengthRecordReader : public ReaderBase {\n         hop_bytes_(hop_bytes),\n         env_(env),\n         file_pos_limit_(-1),\n-        record_number_(0) {}\n+        record_number_(0),\n+        encoding_(encoding) {}\n \n   // On success:\n-  // * input_buffer_ != nullptr,\n-  // * input_buffer_->Tell() == header_bytes_\n+  // * buffered_input_stream_ != nullptr,\n+  // * buffered_input_stream_->Tell() == header_bytes_\n   // * file_pos_limit_ == file size - footer_bytes_\n   Status OnWorkStartedLocked() override {\n     record_number_ = 0;\n-    uint64 file_size = 0;\n-    TF_RETURN_IF_ERROR(env_->GetFileSize(current_work(), &file_size));\n-    file_pos_limit_ = file_size - footer_bytes_;\n+    if (encoding_ == \"ZLIB\" || encoding_ == \"GZIP\") {\n+      const io::ZlibCompressionOptions zlib_options =\n+          encoding_ == \"ZLIB\" ?\n+          io::ZlibCompressionOptions::DEFAULT() :\n+          io::ZlibCompressionOptions::GZIP();\n \n-    TF_RETURN_IF_ERROR(env_->NewRandomAccessFile(current_work(), &file_));\n+      file_pos_limit_ = -1;\n+      hop_cache_.clear();\n+      footer_cache_.clear();\n+\n+      TF_RETURN_IF_ERROR(env_->NewRandomAccessFile(current_work(), &file_));\n+      file_stream_.reset(new io::RandomAccessInputStream(file_.get()));\n+      buffered_input_stream_.reset(new io::ZlibInputStream(file_stream_.get(),\n+                                                          (size_t)kBufferSize,\n+                                                          (size_t)kBufferSize,\n+                                                          zlib_options));\n+      TF_RETURN_IF_ERROR(buffered_input_stream_->SkipNBytes(header_bytes_));\n+      // The following is similiar to non encoding case except that we also caches\n+      // footer.\n+      // Basically, each read only advances hop_bytes_. However, we have the data\n+      // that is pieced together by\n+      // hop_cache_ + footer_cache_ + read(hop_bytes_)\n+      // Then we use the following to cut data for use:\n+      // value:                     0 --> record_bytes_\n+      // hop_cache_:       hop_bytes_ --> record_bytes_\n+      // footer_cache_: record_bytes_ --> end(record_bytes_ + footer_bytes_)\n+      // Next time we read, we advance hop_bytes_ and append to the end, and\n+      // repeat the same process.\n+      if (0 < hop_bytes_ && hop_bytes_ < record_bytes_) {", "path": "tensorflow/core/kernels/fixed_length_record_reader_op.cc", "position": null, "original_position": 70, "commit_id": "58c7ad730e58a411864bd5b2d24df0a2d2aabc5b", "original_commit_id": "2028de1edc60cc7de1ed4d58afa6c93981126e6d", "user": {"login": "yongtang", "id": 6932348, "node_id": "MDQ6VXNlcjY5MzIzNDg=", "avatar_url": "https://avatars0.githubusercontent.com/u/6932348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongtang", "html_url": "https://github.com/yongtang", "followers_url": "https://api.github.com/users/yongtang/followers", "following_url": "https://api.github.com/users/yongtang/following{/other_user}", "gists_url": "https://api.github.com/users/yongtang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongtang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongtang/subscriptions", "organizations_url": "https://api.github.com/users/yongtang/orgs", "repos_url": "https://api.github.com/users/yongtang/repos", "events_url": "https://api.github.com/users/yongtang/events{/privacy}", "received_events_url": "https://api.github.com/users/yongtang/received_events", "type": "User", "site_admin": false}, "body": "Thanks @saxenasaurabh. The PR has been updated with footer_cache and hop_cache combined. Also, now the `file_pos_limit_` seems to be not needed so I just removed the `file_pos_limit_` completely.", "created_at": "2017-05-17T23:17:13Z", "updated_at": "2017-06-27T00:11:31Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8901#discussion_r117131764", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8901", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/117131764"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8901#discussion_r117131764"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8901"}}, "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3967488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/saxenasaurabh\">@saxenasaurabh</a>. The PR has been updated with footer_cache and hop_cache combined. Also, now the <code>file_pos_limit_</code> seems to be not needed so I just removed the <code>file_pos_limit_</code> completely.</p>", "body_text": "Thanks @saxenasaurabh. The PR has been updated with footer_cache and hop_cache combined. Also, now the file_pos_limit_ seems to be not needed so I just removed the file_pos_limit_ completely.", "in_reply_to_id": 115320702}