{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14486", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14486/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14486/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14486/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14486", "id": 273153224, "node_id": "MDU6SXNzdWUyNzMxNTMyMjQ=", "number": 14486, "title": "TowerLoss(Multiple GPU) will hurt final accuracy/performance so much when doing image finetune, is it a bug?", "user": {"login": "chenghuige", "id": 6323467, "node_id": "MDQ6VXNlcjYzMjM0Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6323467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenghuige", "html_url": "https://github.com/chenghuige", "followers_url": "https://api.github.com/users/chenghuige/followers", "following_url": "https://api.github.com/users/chenghuige/following{/other_user}", "gists_url": "https://api.github.com/users/chenghuige/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenghuige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenghuige/subscriptions", "organizations_url": "https://api.github.com/users/chenghuige/orgs", "repos_url": "https://api.github.com/users/chenghuige/repos", "events_url": "https://api.github.com/users/chenghuige/events{/privacy}", "received_events_url": "https://api.github.com/users/chenghuige/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-11T15:32:32Z", "updated_at": "2017-11-11T18:08:43Z", "closed_at": "2017-11-11T18:08:43Z", "author_association": "NONE", "body_html": "<p>From the paper \"Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge\"<br>\nWriter say \"Training was<br>\ndone using a single GPU (Nvidia K20), and step time was<br>\nabout 3 seconds. Thus, training took over 3 weeks \u2013 <strong>parallelizing<br>\ntraining yielded somewhat worse results,</strong> though it<br>\nincreased the speed to convergence.\"<br>\nFor my applications, I find tower loss is ok when you do anything without finetune image mode, even<br>\nif you use image model(inception resnet nasnet etc.) is fine.<br>\nBut if you do finetune, either for image caption or image classification, the performance will hurt<br>\na lot(using mulitple gpu wether increase total batch size or keep total batch size the same as single gpu), might not convergent.  Is it a known bug, can we avoid this ?</p>", "body_text": "From the paper \"Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge\"\nWriter say \"Training was\ndone using a single GPU (Nvidia K20), and step time was\nabout 3 seconds. Thus, training took over 3 weeks \u2013 parallelizing\ntraining yielded somewhat worse results, though it\nincreased the speed to convergence.\"\nFor my applications, I find tower loss is ok when you do anything without finetune image mode, even\nif you use image model(inception resnet nasnet etc.) is fine.\nBut if you do finetune, either for image caption or image classification, the performance will hurt\na lot(using mulitple gpu wether increase total batch size or keep total batch size the same as single gpu), might not convergent.  Is it a known bug, can we avoid this ?", "body": "From the paper \"Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge\"\r\nWriter say \"Training was\r\ndone using a single GPU (Nvidia K20), and step time was\r\nabout 3 seconds. Thus, training took over 3 weeks \u2013 **parallelizing\r\ntraining yielded somewhat worse results,** though it\r\nincreased the speed to convergence.\"\r\nFor my applications, I find tower loss is ok when you do anything without finetune image mode, even \r\nif you use image model(inception resnet nasnet etc.) is fine.\r\nBut if you do finetune, either for image caption or image classification, the performance will hurt \r\na lot(using mulitple gpu wether increase total batch size or keep total batch size the same as single gpu), might not convergent.  Is it a known bug, can we avoid this ?\r\n"}