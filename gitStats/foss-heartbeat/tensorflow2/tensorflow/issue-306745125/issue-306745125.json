{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17852", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17852/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17852/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17852/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17852", "id": 306745125, "node_id": "MDU6SXNzdWUzMDY3NDUxMjU=", "number": 17852, "title": "TF1.6/1.7 PS/Worker Distributed Run Failed with \"UnavailableError: OS Error\" when jobs are not running on current machine", "user": {"login": "simpeng", "id": 3860113, "node_id": "MDQ6VXNlcjM4NjAxMTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/3860113?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simpeng", "html_url": "https://github.com/simpeng", "followers_url": "https://api.github.com/users/simpeng/followers", "following_url": "https://api.github.com/users/simpeng/following{/other_user}", "gists_url": "https://api.github.com/users/simpeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/simpeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simpeng/subscriptions", "organizations_url": "https://api.github.com/users/simpeng/orgs", "repos_url": "https://api.github.com/users/simpeng/repos", "events_url": "https://api.github.com/users/simpeng/events{/privacy}", "received_events_url": "https://api.github.com/users/simpeng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2018-03-20T06:37:26Z", "updated_at": "2018-10-19T02:10:01Z", "closed_at": "2018-04-26T17:49:08Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: official 1.6.0 release binary, or build from master branch (with latest commit: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/47407ccb99a61fd5115130020ff8ef5ef9272433/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/47407ccb99a61fd5115130020ff8ef5ef9272433\"><tt>47407cc</tt></a>)</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.6.0 official release or master</li>\n<li><strong>Python version</strong>: python 3.5 or python 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:   0.11.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0/7.0</li>\n<li><strong>GPU model and memory</strong>:  Tesla K80, 12206MiB</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"<br>\n('v1.6.0-rc1-1503-g47407cc', '1.6.0')</p>\n<h3>Describe the problem</h3>\n<h4>The expected behavior</h4>\n<p>The below source code utilized ps/worker mode to do some training, for usage: we need to run</p>\n<blockquote>\n<p>python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'ps'</p>\n<p>python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'worker'</p>\n</blockquote>\n<p>respectively on \"ps job\" machine and \"worker job\" machine.</p>\n<p>If we run the script firstly on ps, normally, it will wait for worker machine ready, before going furthur, the log is as below:</p>\n<blockquote>\n<p>2018-03-20 05:49:40.410488: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:14416}<br>\n2018-03-20 05:49:40.410614: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 10.0.1.4:14417}<br>\n2018-03-20 05:49:40.418149: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:14416<br>\nps 0, create done queue<br>\nps 0, running<br>\n2018-03-20 05:49:50.430531: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0<br>\n2018-03-20 05:50:00.430728: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0<br>\n2018-03-20 05:50:10.430943: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0<br>\n2018-03-20 05:50:20.431080: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0<br>\n2018-03-20 05:50:30.431351: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0<br>\n^C2018-03-20 05:50:40.434895: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0<br>\n2018-03-20 05:50:50.435104: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0<br>\n2018-03-20 05:51:00.435244: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0</p>\n</blockquote>\n<p>Then we run the script on worker machine, the two machines communicated and coordinated to get things done.</p>\n<h4>The problem</h4>\n<p>It works pretty well on tf1.5/1.4 or earlier version, but on latest 1.6.0 release version (and i also tried to build from master source code), it failed for sometimes. I did some investigation and testing, here are the symptoms:</p>\n<ul>\n<li>\n<p>If the specified ps/worker-hosts are having the same ip as current machine running the scripts (e.g. ps/worker are running different ports of current machine), everything is just fine, they works.</p>\n</li>\n<li>\n<p>If the specified ps/worker-hosts are having the same ip (we call it A-IP), but different with current machine, even though current machine can ping successfully the  A-IP, but will failed. The error log after starting ps task (with <strong>python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'ps'</strong>):</p>\n</li>\n</ul>\n<blockquote>\n<p>2018-03-20 05:57:29.228323: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:14416}<br>\n2018-03-20 05:57:29.228478: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; 10.0.1.4:14417}<br>\n2018-03-20 05:57:29.229155: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:14416<br>\nps 0, create done queue<br>\nps 0, running<br>\nI0320 05:57:29.309552659    3803 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525449.309441854\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}<br>\nI0320 05:57:29.309786369    3803 subchannel.cc:484]          Retry in 998 milliseconds<br>\nI0320 05:57:30.307312499    3796 subchannel.cc:437]          Failed to connect to channel, retrying<br>\nI0320 05:57:30.308555551    3804 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525450.308464247\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}<br>\nI0320 05:57:30.308750759    3804 subchannel.cc:484]          Retry in 999 milliseconds<br>\nI0320 05:57:31.307171978    3796 subchannel.cc:437]          Failed to connect to channel, retrying<br>\nI0320 05:57:31.308303225    3802 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525451.308214021\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}<br>\nI0320 05:57:31.308338927    3802 subchannel.cc:484]          Retry in 999 milliseconds<br>\nI0320 05:57:32.307163816    3796 subchannel.cc:437]          Failed to connect to channel, retrying<br>\nI0320 05:57:32.308250261    3801 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525452.308164957\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}<br>\nI0320 05:57:32.308284662    3801 subchannel.cc:484]          Retry in 999 milliseconds<br>\nI0320 05:57:33.307136307    3796 subchannel.cc:437]          Failed to connect to channel, retrying<br>\nI0320 05:57:33.308314356    3806 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525453.308215652\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}<br>\nI0320 05:57:33.308375658    3806 subchannel.cc:484]          Retry in 999 milliseconds<br>\nI0320 05:57:34.307172752    3796 subchannel.cc:437]          Failed to connect to channel, retrying<br>\n2018-03-20 05:57:34.308793: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error<br>\nTraceback (most recent call last):<br>\nFile \"mnist_replica.py\", line 304, in <br>\nmain(args)<br>\nFile \"mnist_replica.py\", line 102, in main<br>\nsess.run(queue.dequeue())<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 905, in run<br>\nrun_metadata_ptr)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1137, in _run<br>\nfeed_dict_tensor, options, run_metadata)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1355, in _do_run<br>\noptions, run_metadata)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1374, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error</p>\n</blockquote>\n<ul>\n<li>\n<p>If the specified ps/worker-hosts are having different ips (in the same LAN, can ping successfully each other), errors on starting on ps worker is similar with the second situation.</p>\n</li>\n<li>\n<p>The exception happens in MasterSession initilization ( i guess there needs some communication via grpc there)</p>\n</li>\n</ul>\n<h4>My personal thinking</h4>\n<p>To be honest, i am wondering whether the gRPC upgrade (that was <a href=\"https://github.com/tensorflow/tensorflow/commit/cb498995bf3499d3dd4a6edad407590af12ac3bd\">introduced</a> since v1.6rc0 ) did the trick, but since I am pretty new to this component, <strong>besides i am not sure whether somebody else have the similar issues (while I think people using tf1.6 and master will suffer from this on distribute run).</strong></p>\n<p>That would be great if any experts can share some insights or thoughts. Thanks in advance!!!</p>\n<h3>Source code / logs</h3>\n<p>source code:</p>\n<p>`from <strong>future</strong> import absolute_import<br>\nfrom <strong>future</strong> import division<br>\nfrom <strong>future</strong> import print_function</p>\n<p>import argparse<br>\nimport math<br>\nimport sys<br>\nimport tempfile<br>\nimport time</p>\n<p>import tensorflow as tf<br>\nfrom tensorflow.examples.tutorials.mnist import input_data</p>\n<p>IMAGE_PIXELS = 28</p>\n<p>def create_done_queue(ps_task_index, worker_count):<br>\n\"\"\"Queue used to signal death for i'th ps shard. Intended to have<br>\nall workers enqueue an item onto it to signal doneness.\"\"\"</p>\n<pre><code>with tf.device(\"/job:ps/task:%d/cpu:0\" % (ps_task_index)):\n    return tf.FIFOQueue(worker_count, tf.int32, shared_name=\"done_queue\" + str(ps_task_index))\n</code></pre>\n<p>def create_done_queues(ps_count, worker_count):<br>\nreturn [create_done_queue(ps_task_index, worker_count) for ps_task_index in range(ps_count)]</p>\n<p>def main(args):<br>\nmnist = input_data.read_data_sets(args.input_training_data_path, one_hot=True)<br>\nif args.download_only:<br>\nsys.exit(0)</p>\n<pre><code>if args.job_name is None or args.job_name == \"\":\n    raise ValueError(\"Must specify an explicit `job_name`\")\nif args.task_index is None or args.task_index == \"\":\n    raise ValueError(\"Must specify an explicit `task_index`\")\n\nprint(\"job name = %s\" % args.job_name)\nprint(\"task index = %d\" % args.task_index)\n\n# Construct the cluster and start the server\nps_spec = args.ps_hosts.split(\",\")\nworker_spec = args.worker_hosts.split(\",\")\n\n# Get the number of workers.\nnum_workers = len(worker_spec)\nnum_pss = len(ps_spec)\n\ncluster = tf.train.ClusterSpec({\n    \"ps\": ps_spec,\n    \"worker\": worker_spec})\n\nif not args.existing_servers:\n    # Not using existing servers. Create an in-process server.\n    server = tf.train.Server(\n        cluster, job_name=args.job_name, task_index=args.task_index, protocol=args.protocol)\n    if args.job_name == \"ps\":\n        config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n        sess = tf.Session(server.target, config=config)\n\n        print(\"ps %d, create done queue\" % args.task_index)\n        queue = create_done_queue(args.task_index, num_workers)\n\n        print(\"ps %d, running\" % args.task_index)\n        for i in range(num_workers):\n            sess.run(queue.dequeue())\n            print(\"ps %d received worker %d done\" % (args.task_index, i))\n\n        print(\"all workers are done, ps %d: exit\" % (args.task_index))\n        sys.exit()\n\nis_chief = (args.task_index == 0)\nif args.num_gpus &gt; 0:\n    # Avoid gpu allocation conflict: now allocate task_num -&gt; #gpu\n    # for each worker in the corresponding machine\n    gpu = (args.task_index % args.num_gpus)\n    worker_device = \"/job:worker/task:%d/gpu:%d\" % (args.task_index, gpu)\nelif args.num_gpus == 0:\n    # Just allocate the CPU to worker server\n    cpu = 0\n    worker_device = \"/job:worker/task:%d/cpu:%d\" % (args.task_index, cpu)\n\nprint(\"worker %d, worker_device=%s\" % (args.task_index, worker_device))\nprint(\"worker %d, create done queue\" % args.task_index)\nqueues = create_done_queues(num_pss, num_workers)\nprint(\"worker %d, done queue created\" % args.task_index)\n\n# The device setter will automatically place Variables ops on separate\n# parameter servers (ps). The non-Variable ops will be placed on the workers.\n# The ps use CPU and workers use corresponding GPU\n\nwith tf.device(\n        tf.train.replica_device_setter(\n            worker_device=worker_device,\n            ps_device=\"/job:ps/cpu:0\",\n            cluster=cluster)):\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n\n    # Variables of the hidden layer\n    hid_w = tf.Variable(\n        tf.truncated_normal(\n            [IMAGE_PIXELS * IMAGE_PIXELS, args.hidden_units],\n            stddev=1.0 / IMAGE_PIXELS),\n        name=\"hid_w\")\n    hid_b = tf.Variable(tf.zeros([args.hidden_units]), name=\"hid_b\")\n\n    # Variables of the softmax layer\n    sm_w = tf.Variable(\n        tf.truncated_normal(\n            [args.hidden_units, 10],\n            stddev=1.0 / math.sqrt(args.hidden_units)),\n        name=\"sm_w\")\n    sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n\n    # Ops: located on the worker specified with args.task_index\n    x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n    y_ = tf.placeholder(tf.float32, [None, 10])\n\n    hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n    hid = tf.nn.relu(hid_lin)\n\n    y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n    cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n\n    opt = tf.train.AdamOptimizer(args.learning_rate)\n\n    if args.sync_replicas:\n        if args.replicas_to_aggregate is None:\n            replicas_to_aggregate = num_workers\n        else:\n            replicas_to_aggregate = args.replicas_to_aggregate\n\n        opt = tf.train.SyncReplicasOptimizer(\n            opt,\n            replicas_to_aggregate=replicas_to_aggregate,\n            total_num_replicas=num_workers,\n            name=\"mnist_sync_replicas\")\n\n    train_step = opt.minimize(cross_entropy, global_step=global_step)\n\n    if args.sync_replicas:\n        local_init_op = opt.local_step_init_op\n        if is_chief:\n            local_init_op = opt.chief_init_op\n\n        ready_for_local_init_op = opt.ready_for_local_init_op\n\n        # Initial token and chief queue runners required by the sync_replicas mode\n        chief_queue_runner = opt.get_chief_queue_runner()\n        sync_init_op = opt.get_init_tokens_op()\n\n    init_op = tf.global_variables_initializer()\n    train_dir = tempfile.mkdtemp()\n\n    enq_ops = []\n    for q in queues:\n        qop = q.enqueue(1)\n        enq_ops.append(qop)\nif args.sync_replicas:\n    sv = tf.train.Supervisor(\n        is_chief=is_chief,\n        logdir=train_dir,\n        init_op=init_op,\n        local_init_op=local_init_op,\n        ready_for_local_init_op=ready_for_local_init_op,\n        recovery_wait_secs=1,\n        global_step=global_step)\nelse:\n    sv = tf.train.Supervisor(\n        is_chief=is_chief,\n        logdir=train_dir,\n        init_op=init_op,\n        recovery_wait_secs=1,\n        global_step=global_step)\n\nsess_config = tf.ConfigProto(\n    allow_soft_placement=True,\n    log_device_placement=False,\n    device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % args.task_index])\nif args.infer_shapes == True:\n    sess_config.graph_options.infer_shapes = args.infer_shapes\n\n# The chief worker (task_index==0) session will prepare the session,\n# while the remaining workers will wait for the preparation to complete.\nif is_chief:\n    print(\"Worker %d: Initializing session...\" % args.task_index)\nelse:\n    print(\"Worker %d: Waiting for session to be initialized...\" %\n          args.task_index)\n\n\t\t  \nif args.existing_servers:\n    server_grpc_url = \"grpc://\" + worker_spec[args.task_index]\n    print(\"Using existing server at: %s\" % server_grpc_url)\n\n    sess = sv.prepare_or_wait_for_session(server_grpc_url,\n                                          config=sess_config)\nelse:\n    sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\n\nprint(\"Worker %d: Session initialization complete.\" % args.task_index)\n\nif args.sync_replicas and is_chief:\n    # Chief worker will start the chief queue runner and call the init op.\n    sess.run(sync_init_op)\n    sv.start_queue_runners(sess, [chief_queue_runner])\n\n# Perform training\ntime_begin = time.time()\nprint(\"Training begins @ %f\" % time_begin)\n\nlocal_step = 0\nwhile True:\n    # Training feed\n    batch_xs, batch_ys = mnist.train.next_batch(args.batch_size)\n    train_feed = {x: batch_xs, y_: batch_ys}\n\n    _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n    local_step += 1\n\n    now = time.time()\n    print(\"%f: Worker %d: training step %d done (global step: %d)\" %\n          (now, args.task_index, local_step, step))\n\n    if step &gt;= args.train_steps:\n        break\n\ntime_end = time.time()\nprint(\"Training ends @ %f\" % time_end)\ntraining_time = time_end - time_begin\nprint(\"Training elapsed time: %f s\" % training_time)\n\n# Validation feed\nval_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\nval_xent = sess.run(cross_entropy, feed_dict=val_feed)\nprint(\"After %d training step(s), validation cross entropy = %g\" %\n      (args.train_steps, val_xent))\n\nfor op in enq_ops:\n    sess.run(op)\n</code></pre>\n<p>if <strong>name</strong> == \"<strong>main</strong>\":<br>\nparser = argparse.ArgumentParser()<br>\nparser.add_argument(\"--input-training-data-path\", default=\"/tmp/mnist-data\")<br>\nparser.add_argument(\"--input_training_data_path\", default=\"/tmp/mnist-data\")<br>\nparser.add_argument(\"--download_only\", type=bool, default=False)<br>\nparser.add_argument(\"--task-index\", type=int)<br>\nparser.add_argument(\"--task_index\", type=int)<br>\nparser.add_argument(\"--num_gpus\", type=int, default=1)<br>\nparser.add_argument(\"--replicas_to_aggregate\", type=int)<br>\nparser.add_argument(\"--hidden_units\", type=int, default=100)<br>\nparser.add_argument(\"--train_steps\", type=int, default=200)<br>\nparser.add_argument(\"--batch_size\", type=int, default=100)<br>\nparser.add_argument(\"--learning_rate\", type=float, default=0.01)<br>\nparser.add_argument(\"--sync_replicas\", type=bool, default=False)<br>\nparser.add_argument(\"--existing_servers\", type=bool, default=False)<br>\nparser.add_argument(\"--ps-hosts\", default=\"localhost:2222\")<br>\nparser.add_argument(\"--ps_hosts\", default=\"localhost:2222\")<br>\nparser.add_argument(\"--worker-hosts\", default=\"localhost:2223,localhost:2224\")<br>\nparser.add_argument(\"--worker_hosts\", default=\"localhost:2223,localhost:2224\")<br>\nparser.add_argument(\"--job-name\")<br>\nparser.add_argument(\"--job_name\")<br>\nparser.add_argument(\"--protocol\", default=\"grpc\")<br>\nparser.add_argument(\"--infer_shapes\", type=bool, default=False)</p>\n<pre><code>(args, unknown) = parser.parse_known_args()\nmain(args)`\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): official 1.6.0 release binary, or build from master branch (with latest commit: 47407cc)\nTensorFlow version (use command below): 1.6.0 official release or master\nPython version: python 3.5 or python 2.7\nBazel version (if compiling from source):   0.11.1\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCUDA/cuDNN version: 9.0/7.0\nGPU model and memory:  Tesla K80, 12206MiB\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\n('v1.6.0-rc1-1503-g47407cc', '1.6.0')\nDescribe the problem\nThe expected behavior\nThe below source code utilized ps/worker mode to do some training, for usage: we need to run\n\npython mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'ps'\npython mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'worker'\n\nrespectively on \"ps job\" machine and \"worker job\" machine.\nIf we run the script firstly on ps, normally, it will wait for worker machine ready, before going furthur, the log is as below:\n\n2018-03-20 05:49:40.410488: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:14416}\n2018-03-20 05:49:40.410614: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.1.4:14417}\n2018-03-20 05:49:40.418149: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:14416\nps 0, create done queue\nps 0, running\n2018-03-20 05:49:50.430531: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n2018-03-20 05:50:00.430728: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n2018-03-20 05:50:10.430943: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n2018-03-20 05:50:20.431080: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n2018-03-20 05:50:30.431351: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n^C2018-03-20 05:50:40.434895: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n2018-03-20 05:50:50.435104: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n2018-03-20 05:51:00.435244: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\n\nThen we run the script on worker machine, the two machines communicated and coordinated to get things done.\nThe problem\nIt works pretty well on tf1.5/1.4 or earlier version, but on latest 1.6.0 release version (and i also tried to build from master source code), it failed for sometimes. I did some investigation and testing, here are the symptoms:\n\n\nIf the specified ps/worker-hosts are having the same ip as current machine running the scripts (e.g. ps/worker are running different ports of current machine), everything is just fine, they works.\n\n\nIf the specified ps/worker-hosts are having the same ip (we call it A-IP), but different with current machine, even though current machine can ping successfully the  A-IP, but will failed. The error log after starting ps task (with python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'ps'):\n\n\n\n2018-03-20 05:57:29.228323: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:14416}\n2018-03-20 05:57:29.228478: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.1.4:14417}\n2018-03-20 05:57:29.229155: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:14416\nps 0, create done queue\nps 0, running\nI0320 05:57:29.309552659    3803 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525449.309441854\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\nI0320 05:57:29.309786369    3803 subchannel.cc:484]          Retry in 998 milliseconds\nI0320 05:57:30.307312499    3796 subchannel.cc:437]          Failed to connect to channel, retrying\nI0320 05:57:30.308555551    3804 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525450.308464247\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\nI0320 05:57:30.308750759    3804 subchannel.cc:484]          Retry in 999 milliseconds\nI0320 05:57:31.307171978    3796 subchannel.cc:437]          Failed to connect to channel, retrying\nI0320 05:57:31.308303225    3802 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525451.308214021\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\nI0320 05:57:31.308338927    3802 subchannel.cc:484]          Retry in 999 milliseconds\nI0320 05:57:32.307163816    3796 subchannel.cc:437]          Failed to connect to channel, retrying\nI0320 05:57:32.308250261    3801 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525452.308164957\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\nI0320 05:57:32.308284662    3801 subchannel.cc:484]          Retry in 999 milliseconds\nI0320 05:57:33.307136307    3796 subchannel.cc:437]          Failed to connect to channel, retrying\nI0320 05:57:33.308314356    3806 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525453.308215652\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\nI0320 05:57:33.308375658    3806 subchannel.cc:484]          Retry in 999 milliseconds\nI0320 05:57:34.307172752    3796 subchannel.cc:437]          Failed to connect to channel, retrying\n2018-03-20 05:57:34.308793: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\nTraceback (most recent call last):\nFile \"mnist_replica.py\", line 304, in \nmain(args)\nFile \"mnist_replica.py\", line 102, in main\nsess.run(queue.dequeue())\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 905, in run\nrun_metadata_ptr)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1137, in _run\nfeed_dict_tensor, options, run_metadata)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\noptions, run_metadata)\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error\n\n\n\nIf the specified ps/worker-hosts are having different ips (in the same LAN, can ping successfully each other), errors on starting on ps worker is similar with the second situation.\n\n\nThe exception happens in MasterSession initilization ( i guess there needs some communication via grpc there)\n\n\nMy personal thinking\nTo be honest, i am wondering whether the gRPC upgrade (that was introduced since v1.6rc0 ) did the trick, but since I am pretty new to this component, besides i am not sure whether somebody else have the similar issues (while I think people using tf1.6 and master will suffer from this on distribute run).\nThat would be great if any experts can share some insights or thoughts. Thanks in advance!!!\nSource code / logs\nsource code:\n`from future import absolute_import\nfrom future import division\nfrom future import print_function\nimport argparse\nimport math\nimport sys\nimport tempfile\nimport time\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nIMAGE_PIXELS = 28\ndef create_done_queue(ps_task_index, worker_count):\n\"\"\"Queue used to signal death for i'th ps shard. Intended to have\nall workers enqueue an item onto it to signal doneness.\"\"\"\nwith tf.device(\"/job:ps/task:%d/cpu:0\" % (ps_task_index)):\n    return tf.FIFOQueue(worker_count, tf.int32, shared_name=\"done_queue\" + str(ps_task_index))\n\ndef create_done_queues(ps_count, worker_count):\nreturn [create_done_queue(ps_task_index, worker_count) for ps_task_index in range(ps_count)]\ndef main(args):\nmnist = input_data.read_data_sets(args.input_training_data_path, one_hot=True)\nif args.download_only:\nsys.exit(0)\nif args.job_name is None or args.job_name == \"\":\n    raise ValueError(\"Must specify an explicit `job_name`\")\nif args.task_index is None or args.task_index == \"\":\n    raise ValueError(\"Must specify an explicit `task_index`\")\n\nprint(\"job name = %s\" % args.job_name)\nprint(\"task index = %d\" % args.task_index)\n\n# Construct the cluster and start the server\nps_spec = args.ps_hosts.split(\",\")\nworker_spec = args.worker_hosts.split(\",\")\n\n# Get the number of workers.\nnum_workers = len(worker_spec)\nnum_pss = len(ps_spec)\n\ncluster = tf.train.ClusterSpec({\n    \"ps\": ps_spec,\n    \"worker\": worker_spec})\n\nif not args.existing_servers:\n    # Not using existing servers. Create an in-process server.\n    server = tf.train.Server(\n        cluster, job_name=args.job_name, task_index=args.task_index, protocol=args.protocol)\n    if args.job_name == \"ps\":\n        config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n        sess = tf.Session(server.target, config=config)\n\n        print(\"ps %d, create done queue\" % args.task_index)\n        queue = create_done_queue(args.task_index, num_workers)\n\n        print(\"ps %d, running\" % args.task_index)\n        for i in range(num_workers):\n            sess.run(queue.dequeue())\n            print(\"ps %d received worker %d done\" % (args.task_index, i))\n\n        print(\"all workers are done, ps %d: exit\" % (args.task_index))\n        sys.exit()\n\nis_chief = (args.task_index == 0)\nif args.num_gpus > 0:\n    # Avoid gpu allocation conflict: now allocate task_num -> #gpu\n    # for each worker in the corresponding machine\n    gpu = (args.task_index % args.num_gpus)\n    worker_device = \"/job:worker/task:%d/gpu:%d\" % (args.task_index, gpu)\nelif args.num_gpus == 0:\n    # Just allocate the CPU to worker server\n    cpu = 0\n    worker_device = \"/job:worker/task:%d/cpu:%d\" % (args.task_index, cpu)\n\nprint(\"worker %d, worker_device=%s\" % (args.task_index, worker_device))\nprint(\"worker %d, create done queue\" % args.task_index)\nqueues = create_done_queues(num_pss, num_workers)\nprint(\"worker %d, done queue created\" % args.task_index)\n\n# The device setter will automatically place Variables ops on separate\n# parameter servers (ps). The non-Variable ops will be placed on the workers.\n# The ps use CPU and workers use corresponding GPU\n\nwith tf.device(\n        tf.train.replica_device_setter(\n            worker_device=worker_device,\n            ps_device=\"/job:ps/cpu:0\",\n            cluster=cluster)):\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n\n    # Variables of the hidden layer\n    hid_w = tf.Variable(\n        tf.truncated_normal(\n            [IMAGE_PIXELS * IMAGE_PIXELS, args.hidden_units],\n            stddev=1.0 / IMAGE_PIXELS),\n        name=\"hid_w\")\n    hid_b = tf.Variable(tf.zeros([args.hidden_units]), name=\"hid_b\")\n\n    # Variables of the softmax layer\n    sm_w = tf.Variable(\n        tf.truncated_normal(\n            [args.hidden_units, 10],\n            stddev=1.0 / math.sqrt(args.hidden_units)),\n        name=\"sm_w\")\n    sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n\n    # Ops: located on the worker specified with args.task_index\n    x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n    y_ = tf.placeholder(tf.float32, [None, 10])\n\n    hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n    hid = tf.nn.relu(hid_lin)\n\n    y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n    cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n\n    opt = tf.train.AdamOptimizer(args.learning_rate)\n\n    if args.sync_replicas:\n        if args.replicas_to_aggregate is None:\n            replicas_to_aggregate = num_workers\n        else:\n            replicas_to_aggregate = args.replicas_to_aggregate\n\n        opt = tf.train.SyncReplicasOptimizer(\n            opt,\n            replicas_to_aggregate=replicas_to_aggregate,\n            total_num_replicas=num_workers,\n            name=\"mnist_sync_replicas\")\n\n    train_step = opt.minimize(cross_entropy, global_step=global_step)\n\n    if args.sync_replicas:\n        local_init_op = opt.local_step_init_op\n        if is_chief:\n            local_init_op = opt.chief_init_op\n\n        ready_for_local_init_op = opt.ready_for_local_init_op\n\n        # Initial token and chief queue runners required by the sync_replicas mode\n        chief_queue_runner = opt.get_chief_queue_runner()\n        sync_init_op = opt.get_init_tokens_op()\n\n    init_op = tf.global_variables_initializer()\n    train_dir = tempfile.mkdtemp()\n\n    enq_ops = []\n    for q in queues:\n        qop = q.enqueue(1)\n        enq_ops.append(qop)\nif args.sync_replicas:\n    sv = tf.train.Supervisor(\n        is_chief=is_chief,\n        logdir=train_dir,\n        init_op=init_op,\n        local_init_op=local_init_op,\n        ready_for_local_init_op=ready_for_local_init_op,\n        recovery_wait_secs=1,\n        global_step=global_step)\nelse:\n    sv = tf.train.Supervisor(\n        is_chief=is_chief,\n        logdir=train_dir,\n        init_op=init_op,\n        recovery_wait_secs=1,\n        global_step=global_step)\n\nsess_config = tf.ConfigProto(\n    allow_soft_placement=True,\n    log_device_placement=False,\n    device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % args.task_index])\nif args.infer_shapes == True:\n    sess_config.graph_options.infer_shapes = args.infer_shapes\n\n# The chief worker (task_index==0) session will prepare the session,\n# while the remaining workers will wait for the preparation to complete.\nif is_chief:\n    print(\"Worker %d: Initializing session...\" % args.task_index)\nelse:\n    print(\"Worker %d: Waiting for session to be initialized...\" %\n          args.task_index)\n\n\t\t  \nif args.existing_servers:\n    server_grpc_url = \"grpc://\" + worker_spec[args.task_index]\n    print(\"Using existing server at: %s\" % server_grpc_url)\n\n    sess = sv.prepare_or_wait_for_session(server_grpc_url,\n                                          config=sess_config)\nelse:\n    sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\n\nprint(\"Worker %d: Session initialization complete.\" % args.task_index)\n\nif args.sync_replicas and is_chief:\n    # Chief worker will start the chief queue runner and call the init op.\n    sess.run(sync_init_op)\n    sv.start_queue_runners(sess, [chief_queue_runner])\n\n# Perform training\ntime_begin = time.time()\nprint(\"Training begins @ %f\" % time_begin)\n\nlocal_step = 0\nwhile True:\n    # Training feed\n    batch_xs, batch_ys = mnist.train.next_batch(args.batch_size)\n    train_feed = {x: batch_xs, y_: batch_ys}\n\n    _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n    local_step += 1\n\n    now = time.time()\n    print(\"%f: Worker %d: training step %d done (global step: %d)\" %\n          (now, args.task_index, local_step, step))\n\n    if step >= args.train_steps:\n        break\n\ntime_end = time.time()\nprint(\"Training ends @ %f\" % time_end)\ntraining_time = time_end - time_begin\nprint(\"Training elapsed time: %f s\" % training_time)\n\n# Validation feed\nval_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\nval_xent = sess.run(cross_entropy, feed_dict=val_feed)\nprint(\"After %d training step(s), validation cross entropy = %g\" %\n      (args.train_steps, val_xent))\n\nfor op in enq_ops:\n    sess.run(op)\n\nif name == \"main\":\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--input-training-data-path\", default=\"/tmp/mnist-data\")\nparser.add_argument(\"--input_training_data_path\", default=\"/tmp/mnist-data\")\nparser.add_argument(\"--download_only\", type=bool, default=False)\nparser.add_argument(\"--task-index\", type=int)\nparser.add_argument(\"--task_index\", type=int)\nparser.add_argument(\"--num_gpus\", type=int, default=1)\nparser.add_argument(\"--replicas_to_aggregate\", type=int)\nparser.add_argument(\"--hidden_units\", type=int, default=100)\nparser.add_argument(\"--train_steps\", type=int, default=200)\nparser.add_argument(\"--batch_size\", type=int, default=100)\nparser.add_argument(\"--learning_rate\", type=float, default=0.01)\nparser.add_argument(\"--sync_replicas\", type=bool, default=False)\nparser.add_argument(\"--existing_servers\", type=bool, default=False)\nparser.add_argument(\"--ps-hosts\", default=\"localhost:2222\")\nparser.add_argument(\"--ps_hosts\", default=\"localhost:2222\")\nparser.add_argument(\"--worker-hosts\", default=\"localhost:2223,localhost:2224\")\nparser.add_argument(\"--worker_hosts\", default=\"localhost:2223,localhost:2224\")\nparser.add_argument(\"--job-name\")\nparser.add_argument(\"--job_name\")\nparser.add_argument(\"--protocol\", default=\"grpc\")\nparser.add_argument(\"--infer_shapes\", type=bool, default=False)\n(args, unknown) = parser.parse_known_args()\nmain(args)`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: official 1.6.0 release binary, or build from master branch (with latest commit: 47407ccb99a61fd5115130020ff8ef5ef9272433)\r\n- **TensorFlow version (use command below)**: 1.6.0 official release or master\r\n- **Python version**: python 3.5 or python 2.7\r\n- **Bazel version (if compiling from source)**:   0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**:  Tesla K80, 12206MiB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.6.0-rc1-1503-g47407cc', '1.6.0')\r\n\r\n### Describe the problem\r\n\r\n#### The expected behavior\r\n\r\nThe below source code utilized ps/worker mode to do some training, for usage: we need to run\r\n\r\n> python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'ps'\r\n>  \r\n>   python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'worker'\r\n\r\nrespectively on \"ps job\" machine and \"worker job\" machine. \r\n\r\nIf we run the script firstly on ps, normally, it will wait for worker machine ready, before going furthur, the log is as below:\r\n\r\n> \r\n>  2018-03-20 05:49:40.410488: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:14416}\r\n> 2018-03-20 05:49:40.410614: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.1.4:14417}\r\n> 2018-03-20 05:49:40.418149: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:14416\r\n> ps 0, create done queue\r\n> ps 0, running\r\n> 2018-03-20 05:49:50.430531: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:00.430728: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:10.430943: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:20.431080: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:30.431351: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> ^C2018-03-20 05:50:40.434895: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:50.435104: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:51:00.435244: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n\r\nThen we run the script on worker machine, the two machines communicated and coordinated to get things done. \r\n \r\n#### The problem\r\n It works pretty well on tf1.5/1.4 or earlier version, but on latest 1.6.0 release version (and i also tried to build from master source code), it failed for sometimes. I did some investigation and testing, here are the symptoms:\r\n\r\n- If the specified ps/worker-hosts are having the same ip as current machine running the scripts (e.g. ps/worker are running different ports of current machine), everything is just fine, they works. \r\n\r\n- If the specified ps/worker-hosts are having the same ip (we call it A-IP), but different with current machine, even though current machine can ping successfully the  A-IP, but will failed. The error log after starting ps task (with **python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'ps'**):\r\n\r\n> 2018-03-20 05:57:29.228323: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:14416}\r\n> 2018-03-20 05:57:29.228478: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.1.4:14417}\r\n> 2018-03-20 05:57:29.229155: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:14416\r\n> ps 0, create done queue\r\n> ps 0, running\r\n> I0320 05:57:29.309552659    3803 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525449.309441854\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:29.309786369    3803 subchannel.cc:484]          Retry in 998 milliseconds\r\n> I0320 05:57:30.307312499    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> I0320 05:57:30.308555551    3804 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525450.308464247\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:30.308750759    3804 subchannel.cc:484]          Retry in 999 milliseconds\r\n> I0320 05:57:31.307171978    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> I0320 05:57:31.308303225    3802 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525451.308214021\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:31.308338927    3802 subchannel.cc:484]          Retry in 999 milliseconds\r\n> I0320 05:57:32.307163816    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> I0320 05:57:32.308250261    3801 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525452.308164957\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:32.308284662    3801 subchannel.cc:484]          Retry in 999 milliseconds\r\n> I0320 05:57:33.307136307    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> I0320 05:57:33.308314356    3806 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525453.308215652\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:33.308375658    3806 subchannel.cc:484]          Retry in 999 milliseconds\r\n> I0320 05:57:34.307172752    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> 2018-03-20 05:57:34.308793: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\r\n> Traceback (most recent call last):\r\n>   File \"mnist_replica.py\", line 304, in <module>\r\n>     main(args)\r\n>   File \"mnist_replica.py\", line 102, in main\r\n>     sess.run(queue.dequeue())\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 905, in run\r\n>     run_metadata_ptr)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n>     feed_dict_tensor, options, run_metadata)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n>     options, run_metadata)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.UnavailableError: OS Error\r\n\r\n- If the specified ps/worker-hosts are having different ips (in the same LAN, can ping successfully each other), errors on starting on ps worker is similar with the second situation.  \r\n\r\n- The exception happens in MasterSession initilization ( i guess there needs some communication via grpc there)\r\n\r\n#### My personal thinking\r\n\r\nTo be honest, i am wondering whether the gRPC upgrade (that was [introduced](https://github.com/tensorflow/tensorflow/commit/cb498995bf3499d3dd4a6edad407590af12ac3bd) since v1.6rc0 ) did the trick, but since I am pretty new to this component, **besides i am not sure whether somebody else have the similar issues (while I think people using tf1.6 and master will suffer from this on distribute run).**\r\n\r\nThat would be great if any experts can share some insights or thoughts. Thanks in advance!!!\r\n\r\n### Source code / logs\r\n\r\nsource code: \r\n\r\n`from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport math\r\nimport sys\r\nimport tempfile\r\nimport time\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nIMAGE_PIXELS = 28\r\n\r\ndef create_done_queue(ps_task_index, worker_count):\r\n    \"\"\"Queue used to signal death for i'th ps shard. Intended to have \r\n    all workers enqueue an item onto it to signal doneness.\"\"\"\r\n\r\n    with tf.device(\"/job:ps/task:%d/cpu:0\" % (ps_task_index)):\r\n        return tf.FIFOQueue(worker_count, tf.int32, shared_name=\"done_queue\" + str(ps_task_index))\r\n\r\n\r\ndef create_done_queues(ps_count, worker_count):\r\n    return [create_done_queue(ps_task_index, worker_count) for ps_task_index in range(ps_count)]\r\n\r\n\r\ndef main(args):\r\n    mnist = input_data.read_data_sets(args.input_training_data_path, one_hot=True)\r\n    if args.download_only:\r\n        sys.exit(0)\r\n\r\n    if args.job_name is None or args.job_name == \"\":\r\n        raise ValueError(\"Must specify an explicit `job_name`\")\r\n    if args.task_index is None or args.task_index == \"\":\r\n        raise ValueError(\"Must specify an explicit `task_index`\")\r\n\r\n    print(\"job name = %s\" % args.job_name)\r\n    print(\"task index = %d\" % args.task_index)\r\n\r\n    # Construct the cluster and start the server\r\n    ps_spec = args.ps_hosts.split(\",\")\r\n    worker_spec = args.worker_hosts.split(\",\")\r\n\r\n    # Get the number of workers.\r\n    num_workers = len(worker_spec)\r\n    num_pss = len(ps_spec)\r\n\r\n    cluster = tf.train.ClusterSpec({\r\n        \"ps\": ps_spec,\r\n        \"worker\": worker_spec})\r\n\r\n    if not args.existing_servers:\r\n        # Not using existing servers. Create an in-process server.\r\n        server = tf.train.Server(\r\n            cluster, job_name=args.job_name, task_index=args.task_index, protocol=args.protocol)\r\n        if args.job_name == \"ps\":\r\n            config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\r\n            sess = tf.Session(server.target, config=config)\r\n\r\n            print(\"ps %d, create done queue\" % args.task_index)\r\n            queue = create_done_queue(args.task_index, num_workers)\r\n\r\n            print(\"ps %d, running\" % args.task_index)\r\n            for i in range(num_workers):\r\n                sess.run(queue.dequeue())\r\n                print(\"ps %d received worker %d done\" % (args.task_index, i))\r\n\r\n            print(\"all workers are done, ps %d: exit\" % (args.task_index))\r\n            sys.exit()\r\n\r\n    is_chief = (args.task_index == 0)\r\n    if args.num_gpus > 0:\r\n        # Avoid gpu allocation conflict: now allocate task_num -> #gpu\r\n        # for each worker in the corresponding machine\r\n        gpu = (args.task_index % args.num_gpus)\r\n        worker_device = \"/job:worker/task:%d/gpu:%d\" % (args.task_index, gpu)\r\n    elif args.num_gpus == 0:\r\n        # Just allocate the CPU to worker server\r\n        cpu = 0\r\n        worker_device = \"/job:worker/task:%d/cpu:%d\" % (args.task_index, cpu)\r\n\r\n    print(\"worker %d, worker_device=%s\" % (args.task_index, worker_device))\r\n    print(\"worker %d, create done queue\" % args.task_index)\r\n    queues = create_done_queues(num_pss, num_workers)\r\n    print(\"worker %d, done queue created\" % args.task_index)\r\n\r\n    # The device setter will automatically place Variables ops on separate\r\n    # parameter servers (ps). The non-Variable ops will be placed on the workers.\r\n    # The ps use CPU and workers use corresponding GPU\r\n\r\n    with tf.device(\r\n            tf.train.replica_device_setter(\r\n                worker_device=worker_device,\r\n                ps_device=\"/job:ps/cpu:0\",\r\n                cluster=cluster)):\r\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n\r\n        # Variables of the hidden layer\r\n        hid_w = tf.Variable(\r\n            tf.truncated_normal(\r\n                [IMAGE_PIXELS * IMAGE_PIXELS, args.hidden_units],\r\n                stddev=1.0 / IMAGE_PIXELS),\r\n            name=\"hid_w\")\r\n        hid_b = tf.Variable(tf.zeros([args.hidden_units]), name=\"hid_b\")\r\n\r\n        # Variables of the softmax layer\r\n        sm_w = tf.Variable(\r\n            tf.truncated_normal(\r\n                [args.hidden_units, 10],\r\n                stddev=1.0 / math.sqrt(args.hidden_units)),\r\n            name=\"sm_w\")\r\n        sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\r\n\r\n        # Ops: located on the worker specified with args.task_index\r\n        x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\r\n        y_ = tf.placeholder(tf.float32, [None, 10])\r\n\r\n        hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\r\n        hid = tf.nn.relu(hid_lin)\r\n\r\n        y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\r\n        cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\r\n\r\n        opt = tf.train.AdamOptimizer(args.learning_rate)\r\n\r\n        if args.sync_replicas:\r\n            if args.replicas_to_aggregate is None:\r\n                replicas_to_aggregate = num_workers\r\n            else:\r\n                replicas_to_aggregate = args.replicas_to_aggregate\r\n\r\n            opt = tf.train.SyncReplicasOptimizer(\r\n                opt,\r\n                replicas_to_aggregate=replicas_to_aggregate,\r\n                total_num_replicas=num_workers,\r\n                name=\"mnist_sync_replicas\")\r\n\r\n        train_step = opt.minimize(cross_entropy, global_step=global_step)\r\n\r\n        if args.sync_replicas:\r\n            local_init_op = opt.local_step_init_op\r\n            if is_chief:\r\n                local_init_op = opt.chief_init_op\r\n\r\n            ready_for_local_init_op = opt.ready_for_local_init_op\r\n\r\n            # Initial token and chief queue runners required by the sync_replicas mode\r\n            chief_queue_runner = opt.get_chief_queue_runner()\r\n            sync_init_op = opt.get_init_tokens_op()\r\n\r\n        init_op = tf.global_variables_initializer()\r\n        train_dir = tempfile.mkdtemp()\r\n\r\n        enq_ops = []\r\n        for q in queues:\r\n            qop = q.enqueue(1)\r\n            enq_ops.append(qop)\r\n    if args.sync_replicas:\r\n        sv = tf.train.Supervisor(\r\n            is_chief=is_chief,\r\n            logdir=train_dir,\r\n            init_op=init_op,\r\n            local_init_op=local_init_op,\r\n            ready_for_local_init_op=ready_for_local_init_op,\r\n            recovery_wait_secs=1,\r\n            global_step=global_step)\r\n    else:\r\n        sv = tf.train.Supervisor(\r\n            is_chief=is_chief,\r\n            logdir=train_dir,\r\n            init_op=init_op,\r\n            recovery_wait_secs=1,\r\n            global_step=global_step)\r\n\r\n    sess_config = tf.ConfigProto(\r\n        allow_soft_placement=True,\r\n        log_device_placement=False,\r\n        device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % args.task_index])\r\n    if args.infer_shapes == True:\r\n        sess_config.graph_options.infer_shapes = args.infer_shapes\r\n\r\n    # The chief worker (task_index==0) session will prepare the session,\r\n    # while the remaining workers will wait for the preparation to complete.\r\n    if is_chief:\r\n        print(\"Worker %d: Initializing session...\" % args.task_index)\r\n    else:\r\n        print(\"Worker %d: Waiting for session to be initialized...\" %\r\n              args.task_index)\r\n\r\n\t\t\t  \r\n    if args.existing_servers:\r\n        server_grpc_url = \"grpc://\" + worker_spec[args.task_index]\r\n        print(\"Using existing server at: %s\" % server_grpc_url)\r\n\r\n        sess = sv.prepare_or_wait_for_session(server_grpc_url,\r\n                                              config=sess_config)\r\n    else:\r\n        sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\r\n\r\n    print(\"Worker %d: Session initialization complete.\" % args.task_index)\r\n\r\n    if args.sync_replicas and is_chief:\r\n        # Chief worker will start the chief queue runner and call the init op.\r\n        sess.run(sync_init_op)\r\n        sv.start_queue_runners(sess, [chief_queue_runner])\r\n\r\n    # Perform training\r\n    time_begin = time.time()\r\n    print(\"Training begins @ %f\" % time_begin)\r\n\r\n    local_step = 0\r\n    while True:\r\n        # Training feed\r\n        batch_xs, batch_ys = mnist.train.next_batch(args.batch_size)\r\n        train_feed = {x: batch_xs, y_: batch_ys}\r\n\r\n        _, step = sess.run([train_step, global_step], feed_dict=train_feed)\r\n        local_step += 1\r\n\r\n        now = time.time()\r\n        print(\"%f: Worker %d: training step %d done (global step: %d)\" %\r\n              (now, args.task_index, local_step, step))\r\n\r\n        if step >= args.train_steps:\r\n            break\r\n\r\n    time_end = time.time()\r\n    print(\"Training ends @ %f\" % time_end)\r\n    training_time = time_end - time_begin\r\n    print(\"Training elapsed time: %f s\" % training_time)\r\n\r\n    # Validation feed\r\n    val_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\r\n    val_xent = sess.run(cross_entropy, feed_dict=val_feed)\r\n    print(\"After %d training step(s), validation cross entropy = %g\" %\r\n          (args.train_steps, val_xent))\r\n\r\n    for op in enq_ops:\r\n        sess.run(op)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--input-training-data-path\", default=\"/tmp/mnist-data\")\r\n    parser.add_argument(\"--input_training_data_path\", default=\"/tmp/mnist-data\")\r\n    parser.add_argument(\"--download_only\", type=bool, default=False)\r\n    parser.add_argument(\"--task-index\", type=int)\r\n    parser.add_argument(\"--task_index\", type=int)\r\n    parser.add_argument(\"--num_gpus\", type=int, default=1)\r\n    parser.add_argument(\"--replicas_to_aggregate\", type=int)\r\n    parser.add_argument(\"--hidden_units\", type=int, default=100)\r\n    parser.add_argument(\"--train_steps\", type=int, default=200)\r\n    parser.add_argument(\"--batch_size\", type=int, default=100)\r\n    parser.add_argument(\"--learning_rate\", type=float, default=0.01)\r\n    parser.add_argument(\"--sync_replicas\", type=bool, default=False)\r\n    parser.add_argument(\"--existing_servers\", type=bool, default=False)\r\n    parser.add_argument(\"--ps-hosts\", default=\"localhost:2222\")\r\n    parser.add_argument(\"--ps_hosts\", default=\"localhost:2222\")\r\n    parser.add_argument(\"--worker-hosts\", default=\"localhost:2223,localhost:2224\")\r\n    parser.add_argument(\"--worker_hosts\", default=\"localhost:2223,localhost:2224\")\r\n    parser.add_argument(\"--job-name\")\r\n    parser.add_argument(\"--job_name\")\r\n    parser.add_argument(\"--protocol\", default=\"grpc\")\r\n    parser.add_argument(\"--infer_shapes\", type=bool, default=False)\r\n\r\n    (args, unknown) = parser.parse_known_args()\r\n    main(args)`\r\n"}