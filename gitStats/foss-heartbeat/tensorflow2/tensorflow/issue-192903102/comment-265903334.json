{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/265903334", "html_url": "https://github.com/tensorflow/tensorflow/pull/6017#issuecomment-265903334", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6017", "id": 265903334, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NTkwMzMzNA==", "user": {"login": "Montmorency", "id": 962924, "node_id": "MDQ6VXNlcjk2MjkyNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/962924?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Montmorency", "html_url": "https://github.com/Montmorency", "followers_url": "https://api.github.com/users/Montmorency/followers", "following_url": "https://api.github.com/users/Montmorency/following{/other_user}", "gists_url": "https://api.github.com/users/Montmorency/gists{/gist_id}", "starred_url": "https://api.github.com/users/Montmorency/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Montmorency/subscriptions", "organizations_url": "https://api.github.com/users/Montmorency/orgs", "repos_url": "https://api.github.com/users/Montmorency/repos", "events_url": "https://api.github.com/users/Montmorency/events{/privacy}", "received_events_url": "https://api.github.com/users/Montmorency/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-09T01:01:50Z", "updated_at": "2016-12-09T01:01:50Z", "author_association": "NONE", "body_html": "<p>Hi All,<br>\nJust thought this might be a useful feature to have in the contrib/learn functionality. It looks like the batch_norm method in /layers can be attached straight forwardly to the layers in the dnn, along with all the batch_parameters that need to be passed along. The only pitfalls I can see are there need to be some clear warning about argument scoping, and making sure the is_training variable is set properly depending on whether the model is being fit or evaluated.</p>", "body_text": "Hi All,\nJust thought this might be a useful feature to have in the contrib/learn functionality. It looks like the batch_norm method in /layers can be attached straight forwardly to the layers in the dnn, along with all the batch_parameters that need to be passed along. The only pitfalls I can see are there need to be some clear warning about argument scoping, and making sure the is_training variable is set properly depending on whether the model is being fit or evaluated.", "body": "Hi All,\r\nJust thought this might be a useful feature to have in the contrib/learn functionality. It looks like the batch_norm method in /layers can be attached straight forwardly to the layers in the dnn, along with all the batch_parameters that need to be passed along. The only pitfalls I can see are there need to be some clear warning about argument scoping, and making sure the is_training variable is set properly depending on whether the model is being fit or evaluated. "}