{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/399624352", "html_url": "https://github.com/tensorflow/tensorflow/issues/19958#issuecomment-399624352", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19958", "id": 399624352, "node_id": "MDEyOklzc3VlQ29tbWVudDM5OTYyNDM1Mg==", "user": {"login": "summersunshine1", "id": 11360140, "node_id": "MDQ6VXNlcjExMzYwMTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/11360140?v=4", "gravatar_id": "", "url": "https://api.github.com/users/summersunshine1", "html_url": "https://github.com/summersunshine1", "followers_url": "https://api.github.com/users/summersunshine1/followers", "following_url": "https://api.github.com/users/summersunshine1/following{/other_user}", "gists_url": "https://api.github.com/users/summersunshine1/gists{/gist_id}", "starred_url": "https://api.github.com/users/summersunshine1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/summersunshine1/subscriptions", "organizations_url": "https://api.github.com/users/summersunshine1/orgs", "repos_url": "https://api.github.com/users/summersunshine1/repos", "events_url": "https://api.github.com/users/summersunshine1/events{/privacy}", "received_events_url": "https://api.github.com/users/summersunshine1/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-23T02:38:47Z", "updated_at": "2018-06-23T02:54:36Z", "author_association": "NONE", "body_html": "<p>I also met the problem, and i solved by self defined train hooks like this:</p>\n<pre lang=\"import\" data-meta=\"tensorflow as tf\"><code>  import os\n  class RestoreCheckpointHook(tf.train.SessionRunHook):\n    def __init__(self,\n                 checkpoint_path,\n                 exclude_scope_patterns = None,\n                 include_scope_patterns = None\n                 ):\n        tf.logging.info(\"Create RestoreCheckpointHook.\")\n        self.checkpoint_path =  checkpoint_path\n        self.exclude_scope_patterns = exclude_scope_patterns\n        self.include_scope_patterns = include_scope_patterns\n    def begin(self):\n      # You can add ops to the graph here.\n      tf.logging.info('Before starting the session.')\n     \n      variables = tf.get_collection(tf.GraphKeys.VARIABLES)\n      \n      for v in variables:\n        if self.exclude_scope_patterns in v.name:\n          continue\n        vars.append(v)\n      #variables_to_restore = tf.contrib.framework.filter_variables(\n      #    variables,\n      #    include_patterns=self.include_scope_patterns,# ['Conv'],\n      #    exclude_patterns=['Adam'], # ['biases', 'Logits'],\n\n      #    # If True (default), performs re.search to find matches\n      #    # (i.e. pattern can match any substring of the variable name).\n      #    # If False, performs re.match (i.e. regexp should match from the beginning of the variable name).\n      #    reg_search = True\n      #)\n      self.saver = tf.train.Saver(vars)\n\n\n    def after_create_session(self, session, coord):\n      # When this is called, the graph is finalized and\n      # ops can no longer be added to the graph.\n\n      tf.logging.info('Session created.')\n\n      tf.logging.info('Fine-tuning from %s' % self.checkpoint_path)\n      self.saver.restore(session, os.path.expanduser(self.checkpoint_path))\n      tf.logging.info('End finetuning from %s' % self.checkpoint_path)\n\n    def before_run(self, run_context):\n      return None #SessionRunArgs(self.your_tensor)\n\n    def after_run(self, run_context, run_values):\n      #print('Done running one step. The value of my tensor: %s', run_values.results)\n      #if you-need-to-stop-loop:\n      #  run_context.request_stop()\n      pass\n\n\n    def end(self, session):\n      #print('Done with the session.')\n      pass\n</code></pre>\n<p>you can using like this:</p>\n<pre><code> train_hooks = RestoreCheckpointHook('checkpointpath',exclude_scope_patterns=\"Adam\")\n estimator.train(\n        dataset.train_input_fn,\n        steps=schedule_manager.single_iteration_train_steps,\n        hooks=train_hooks)\n</code></pre>", "body_text": "I also met the problem, and i solved by self defined train hooks like this:\n  import os\n  class RestoreCheckpointHook(tf.train.SessionRunHook):\n    def __init__(self,\n                 checkpoint_path,\n                 exclude_scope_patterns = None,\n                 include_scope_patterns = None\n                 ):\n        tf.logging.info(\"Create RestoreCheckpointHook.\")\n        self.checkpoint_path =  checkpoint_path\n        self.exclude_scope_patterns = exclude_scope_patterns\n        self.include_scope_patterns = include_scope_patterns\n    def begin(self):\n      # You can add ops to the graph here.\n      tf.logging.info('Before starting the session.')\n     \n      variables = tf.get_collection(tf.GraphKeys.VARIABLES)\n      \n      for v in variables:\n        if self.exclude_scope_patterns in v.name:\n          continue\n        vars.append(v)\n      #variables_to_restore = tf.contrib.framework.filter_variables(\n      #    variables,\n      #    include_patterns=self.include_scope_patterns,# ['Conv'],\n      #    exclude_patterns=['Adam'], # ['biases', 'Logits'],\n\n      #    # If True (default), performs re.search to find matches\n      #    # (i.e. pattern can match any substring of the variable name).\n      #    # If False, performs re.match (i.e. regexp should match from the beginning of the variable name).\n      #    reg_search = True\n      #)\n      self.saver = tf.train.Saver(vars)\n\n\n    def after_create_session(self, session, coord):\n      # When this is called, the graph is finalized and\n      # ops can no longer be added to the graph.\n\n      tf.logging.info('Session created.')\n\n      tf.logging.info('Fine-tuning from %s' % self.checkpoint_path)\n      self.saver.restore(session, os.path.expanduser(self.checkpoint_path))\n      tf.logging.info('End finetuning from %s' % self.checkpoint_path)\n\n    def before_run(self, run_context):\n      return None #SessionRunArgs(self.your_tensor)\n\n    def after_run(self, run_context, run_values):\n      #print('Done running one step. The value of my tensor: %s', run_values.results)\n      #if you-need-to-stop-loop:\n      #  run_context.request_stop()\n      pass\n\n\n    def end(self, session):\n      #print('Done with the session.')\n      pass\n\nyou can using like this:\n train_hooks = RestoreCheckpointHook('checkpointpath',exclude_scope_patterns=\"Adam\")\n estimator.train(\n        dataset.train_input_fn,\n        steps=schedule_manager.single_iteration_train_steps,\n        hooks=train_hooks)", "body": "I also met the problem, and i solved by self defined train hooks like this:\r\n\r\n```import tensorflow as tf \r\n  import os\r\n  class RestoreCheckpointHook(tf.train.SessionRunHook):\r\n    def __init__(self,\r\n                 checkpoint_path,\r\n                 exclude_scope_patterns = None,\r\n                 include_scope_patterns = None\r\n                 ):\r\n        tf.logging.info(\"Create RestoreCheckpointHook.\")\r\n        self.checkpoint_path =  checkpoint_path\r\n        self.exclude_scope_patterns = exclude_scope_patterns\r\n        self.include_scope_patterns = include_scope_patterns\r\n    def begin(self):\r\n      # You can add ops to the graph here.\r\n      tf.logging.info('Before starting the session.')\r\n     \r\n      variables = tf.get_collection(tf.GraphKeys.VARIABLES)\r\n      \r\n      for v in variables:\r\n        if self.exclude_scope_patterns in v.name:\r\n          continue\r\n        vars.append(v)\r\n      #variables_to_restore = tf.contrib.framework.filter_variables(\r\n      #    variables,\r\n      #    include_patterns=self.include_scope_patterns,# ['Conv'],\r\n      #    exclude_patterns=['Adam'], # ['biases', 'Logits'],\r\n\r\n      #    # If True (default), performs re.search to find matches\r\n      #    # (i.e. pattern can match any substring of the variable name).\r\n      #    # If False, performs re.match (i.e. regexp should match from the beginning of the variable name).\r\n      #    reg_search = True\r\n      #)\r\n      self.saver = tf.train.Saver(vars)\r\n\r\n\r\n    def after_create_session(self, session, coord):\r\n      # When this is called, the graph is finalized and\r\n      # ops can no longer be added to the graph.\r\n\r\n      tf.logging.info('Session created.')\r\n\r\n      tf.logging.info('Fine-tuning from %s' % self.checkpoint_path)\r\n      self.saver.restore(session, os.path.expanduser(self.checkpoint_path))\r\n      tf.logging.info('End finetuning from %s' % self.checkpoint_path)\r\n\r\n    def before_run(self, run_context):\r\n      return None #SessionRunArgs(self.your_tensor)\r\n\r\n    def after_run(self, run_context, run_values):\r\n      #print('Done running one step. The value of my tensor: %s', run_values.results)\r\n      #if you-need-to-stop-loop:\r\n      #  run_context.request_stop()\r\n      pass\r\n\r\n\r\n    def end(self, session):\r\n      #print('Done with the session.')\r\n      pass\r\n```\r\nyou can using like this:\r\n```\r\n train_hooks = RestoreCheckpointHook('checkpointpath',exclude_scope_patterns=\"Adam\")\r\n estimator.train(\r\n        dataset.train_input_fn,\r\n        steps=schedule_manager.single_iteration_train_steps,\r\n        hooks=train_hooks)\r\n```\r\n"}