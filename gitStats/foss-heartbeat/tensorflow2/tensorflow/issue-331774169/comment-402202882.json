{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/402202882", "html_url": "https://github.com/tensorflow/tensorflow/issues/19958#issuecomment-402202882", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19958", "id": 402202882, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjIwMjg4Mg==", "user": {"login": "jrabary", "id": 1025387, "node_id": "MDQ6VXNlcjEwMjUzODc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1025387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrabary", "html_url": "https://github.com/jrabary", "followers_url": "https://api.github.com/users/jrabary/followers", "following_url": "https://api.github.com/users/jrabary/following{/other_user}", "gists_url": "https://api.github.com/users/jrabary/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrabary/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrabary/subscriptions", "organizations_url": "https://api.github.com/users/jrabary/orgs", "repos_url": "https://api.github.com/users/jrabary/repos", "events_url": "https://api.github.com/users/jrabary/events{/privacy}", "received_events_url": "https://api.github.com/users/jrabary/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-03T15:44:44Z", "updated_at": "2018-07-03T15:44:44Z", "author_association": "NONE", "body_html": "<p>I tried the workaround using hooks but taking into account the fact that my model and the checkpoint don't have the same scope. For that I slightly modified the <code>RestoreHook</code> from <code>tensor2tensor</code> code base as follow:</p>\n<pre lang=\"class\" data-meta=\"RestoreHook(tf.train.SessionRunHook):\"><code>    \"\"\"Restore variables from a checkpoint path.\"\"\"\n\n    def __init__(self, checkpoint_path=\"\", new_model_scope=\"\", old_model_scope=\"\",\n                 include=None, exclude=None):\n        self._checkpoint_path = checkpoint_path\n        self._new_model_scope = new_model_scope\n        self._old_model_scope = old_model_scope\n        self._include = include\n        self._exclude = exclude\n        self._saver = None\n        self._assignment_map = None\n\n    def begin(self):\n        \"\"\"Load variables from checkpoint.\n\n        New model variables have the following name foramt:\n        new_model_scope/old_model_scope/xxx/xxx:0 To find the map of\n        name to variable, need to strip the new_model_scope and then\n        match the old_model_scope and remove the suffix :0.\n\n        \"\"\"\n        variables_to_restore = tf.contrib.framework.get_variables_to_restore(\n            include=self._include, exclude=self._exclude)\n        # remove new_model_scope from variable name prefix\n        assignment_map = {variable.name[len(self._new_model_scope):]: variable\n                          for variable in variables_to_restore\n                          if variable.name.startswith(self._new_model_scope)}\n        # remove :0 from variable name suffix\n        assignment_map = {name.split(\":\")[0]: variable\n                          for name, variable in six.iteritems(assignment_map)\n                          if name.startswith(self._old_model_scope)}\n\n        self._saver = tf.train.Saver(assignment_map)\n\n    def after_create_session(self, session, coord):\n        # When this is called, the graph is finalized and\n        # ops can no longer be added to the graph.\n\n        tf.logging.info('Session created.')\n\n        tf.logging.info('Fine-tuning from %s' % self._checkpoint_path)\n        self._saver.restore(session, self._checkpoint_path)\n        tf.logging.info('End finetuning from %s' % self._checkpoint_path)\n\n    def before_run(self, run_context):\n        return None  # SessionRunArgs(self.your_tensor)\n</code></pre>\n<p>But this fails at <code>self._saver = tf.train.Saver(assignment_map)</code>  with a mysterious error</p>\n<pre><code>tensorflow/contrib/distribute/python/values.py\", line 242, in _tensor_conversion\n    assert not as_ref\nAssertionError\n</code></pre>\n<p>Any ideas ?</p>", "body_text": "I tried the workaround using hooks but taking into account the fact that my model and the checkpoint don't have the same scope. For that I slightly modified the RestoreHook from tensor2tensor code base as follow:\n    \"\"\"Restore variables from a checkpoint path.\"\"\"\n\n    def __init__(self, checkpoint_path=\"\", new_model_scope=\"\", old_model_scope=\"\",\n                 include=None, exclude=None):\n        self._checkpoint_path = checkpoint_path\n        self._new_model_scope = new_model_scope\n        self._old_model_scope = old_model_scope\n        self._include = include\n        self._exclude = exclude\n        self._saver = None\n        self._assignment_map = None\n\n    def begin(self):\n        \"\"\"Load variables from checkpoint.\n\n        New model variables have the following name foramt:\n        new_model_scope/old_model_scope/xxx/xxx:0 To find the map of\n        name to variable, need to strip the new_model_scope and then\n        match the old_model_scope and remove the suffix :0.\n\n        \"\"\"\n        variables_to_restore = tf.contrib.framework.get_variables_to_restore(\n            include=self._include, exclude=self._exclude)\n        # remove new_model_scope from variable name prefix\n        assignment_map = {variable.name[len(self._new_model_scope):]: variable\n                          for variable in variables_to_restore\n                          if variable.name.startswith(self._new_model_scope)}\n        # remove :0 from variable name suffix\n        assignment_map = {name.split(\":\")[0]: variable\n                          for name, variable in six.iteritems(assignment_map)\n                          if name.startswith(self._old_model_scope)}\n\n        self._saver = tf.train.Saver(assignment_map)\n\n    def after_create_session(self, session, coord):\n        # When this is called, the graph is finalized and\n        # ops can no longer be added to the graph.\n\n        tf.logging.info('Session created.')\n\n        tf.logging.info('Fine-tuning from %s' % self._checkpoint_path)\n        self._saver.restore(session, self._checkpoint_path)\n        tf.logging.info('End finetuning from %s' % self._checkpoint_path)\n\n    def before_run(self, run_context):\n        return None  # SessionRunArgs(self.your_tensor)\n\nBut this fails at self._saver = tf.train.Saver(assignment_map)  with a mysterious error\ntensorflow/contrib/distribute/python/values.py\", line 242, in _tensor_conversion\n    assert not as_ref\nAssertionError\n\nAny ideas ?", "body": "I tried the workaround using hooks but taking into account the fact that my model and the checkpoint don't have the same scope. For that I slightly modified the `RestoreHook` from `tensor2tensor` code base as follow:\r\n\r\n```class RestoreHook(tf.train.SessionRunHook):\r\n    \"\"\"Restore variables from a checkpoint path.\"\"\"\r\n\r\n    def __init__(self, checkpoint_path=\"\", new_model_scope=\"\", old_model_scope=\"\",\r\n                 include=None, exclude=None):\r\n        self._checkpoint_path = checkpoint_path\r\n        self._new_model_scope = new_model_scope\r\n        self._old_model_scope = old_model_scope\r\n        self._include = include\r\n        self._exclude = exclude\r\n        self._saver = None\r\n        self._assignment_map = None\r\n\r\n    def begin(self):\r\n        \"\"\"Load variables from checkpoint.\r\n\r\n        New model variables have the following name foramt:\r\n        new_model_scope/old_model_scope/xxx/xxx:0 To find the map of\r\n        name to variable, need to strip the new_model_scope and then\r\n        match the old_model_scope and remove the suffix :0.\r\n\r\n        \"\"\"\r\n        variables_to_restore = tf.contrib.framework.get_variables_to_restore(\r\n            include=self._include, exclude=self._exclude)\r\n        # remove new_model_scope from variable name prefix\r\n        assignment_map = {variable.name[len(self._new_model_scope):]: variable\r\n                          for variable in variables_to_restore\r\n                          if variable.name.startswith(self._new_model_scope)}\r\n        # remove :0 from variable name suffix\r\n        assignment_map = {name.split(\":\")[0]: variable\r\n                          for name, variable in six.iteritems(assignment_map)\r\n                          if name.startswith(self._old_model_scope)}\r\n\r\n        self._saver = tf.train.Saver(assignment_map)\r\n\r\n    def after_create_session(self, session, coord):\r\n        # When this is called, the graph is finalized and\r\n        # ops can no longer be added to the graph.\r\n\r\n        tf.logging.info('Session created.')\r\n\r\n        tf.logging.info('Fine-tuning from %s' % self._checkpoint_path)\r\n        self._saver.restore(session, self._checkpoint_path)\r\n        tf.logging.info('End finetuning from %s' % self._checkpoint_path)\r\n\r\n    def before_run(self, run_context):\r\n        return None  # SessionRunArgs(self.your_tensor)\r\n```\r\n\r\nBut this fails at `self._saver = tf.train.Saver(assignment_map)`  with a mysterious error \r\n```\r\ntensorflow/contrib/distribute/python/values.py\", line 242, in _tensor_conversion\r\n    assert not as_ref\r\nAssertionError\r\n```\r\n\r\nAny ideas ?"}