{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341763219", "html_url": "https://github.com/tensorflow/tensorflow/issues/6847#issuecomment-341763219", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6847", "id": 341763219, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTc2MzIxOQ==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-03T16:53:50Z", "updated_at": "2017-11-03T16:53:50Z", "author_association": "MEMBER", "body_html": "<p>You can get the equivalent of .backward() now for any tf op by doing something like tfe.gradients_function(tf.extract_image_patches)(arguments_to_image_patches ,..., dy=arguments_to_the_gradient_function) if tf.extract_image_patches has a gradient defined. When executing graphs this will only compute the forward pass if the backward function needs it (like how the gradient of exp or the gradient of relu uses the output of relu).</p>", "body_text": "You can get the equivalent of .backward() now for any tf op by doing something like tfe.gradients_function(tf.extract_image_patches)(arguments_to_image_patches ,..., dy=arguments_to_the_gradient_function) if tf.extract_image_patches has a gradient defined. When executing graphs this will only compute the forward pass if the backward function needs it (like how the gradient of exp or the gradient of relu uses the output of relu).", "body": "You can get the equivalent of .backward() now for any tf op by doing something like tfe.gradients_function(tf.extract_image_patches)(arguments_to_image_patches ,..., dy=arguments_to_the_gradient_function) if tf.extract_image_patches has a gradient defined. When executing graphs this will only compute the forward pass if the backward function needs it (like how the gradient of exp or the gradient of relu uses the output of relu)."}