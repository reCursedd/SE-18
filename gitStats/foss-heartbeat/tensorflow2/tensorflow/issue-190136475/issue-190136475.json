{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5673", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5673/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5673/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5673/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5673", "id": 190136475, "node_id": "MDU6SXNzdWUxOTAxMzY0NzU=", "number": 5673, "title": "Possible typo in docstring of embedding_rnn_seq2seq and embedding_attention_seq2seq?", "user": {"login": "shuvrobiswas", "id": 1322956, "node_id": "MDQ6VXNlcjEzMjI5NTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1322956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shuvrobiswas", "html_url": "https://github.com/shuvrobiswas", "followers_url": "https://api.github.com/users/shuvrobiswas/followers", "following_url": "https://api.github.com/users/shuvrobiswas/following{/other_user}", "gists_url": "https://api.github.com/users/shuvrobiswas/gists{/gist_id}", "starred_url": "https://api.github.com/users/shuvrobiswas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shuvrobiswas/subscriptions", "organizations_url": "https://api.github.com/users/shuvrobiswas/orgs", "repos_url": "https://api.github.com/users/shuvrobiswas/repos", "events_url": "https://api.github.com/users/shuvrobiswas/events{/privacy}", "received_events_url": "https://api.github.com/users/shuvrobiswas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-11-17T19:12:40Z", "updated_at": "2016-11-18T23:09:03Z", "closed_at": "2016-11-18T22:42:53Z", "author_association": "NONE", "body_html": "<p>So in the docstring of embedding_rnn_seq2seq:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L296\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L296</a></p>\n<p>It says:</p>\n<pre><code>  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\n</code></pre>\n<p>Should that instead read:</p>\n<pre><code>  [embedding_size x input_size]). Then it runs an RNN to encode\n</code></pre>\n<p>Since if you are creating an embedding it is to do dimensionality reduction (and so you usually want your embedding size to be smaller than the number of encoder symbols). Also in the code:</p>\n<pre><code>    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n</code></pre>\n<p>It looks like the encoder inputs are mapped to vectors of size embedding_size.</p>\n<p>Looking forward to hearing the community thoughts (this is also the case in the docstring of embedding_attention_seq2seq).</p>", "body_text": "So in the docstring of embedding_rnn_seq2seq:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L296\nIt says:\n  This model first embeds encoder_inputs by a newly created embedding (of shape\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\n\nShould that instead read:\n  [embedding_size x input_size]). Then it runs an RNN to encode\n\nSince if you are creating an embedding it is to do dimensionality reduction (and so you usually want your embedding size to be smaller than the number of encoder symbols). Also in the code:\n    # Encoder.\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell, embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\n\nIt looks like the encoder inputs are mapped to vectors of size embedding_size.\nLooking forward to hearing the community thoughts (this is also the case in the docstring of embedding_attention_seq2seq).", "body": "So in the docstring of embedding_rnn_seq2seq:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L296\r\n\r\nIt says:\r\n\r\n```\r\n  This model first embeds encoder_inputs by a newly created embedding (of shape\r\n  [num_encoder_symbols x input_size]). Then it runs an RNN to encode\r\n```\r\n\r\nShould that instead read:\r\n\r\n```\r\n  [embedding_size x input_size]). Then it runs an RNN to encode\r\n```\r\n\r\nSince if you are creating an embedding it is to do dimensionality reduction (and so you usually want your embedding size to be smaller than the number of encoder symbols). Also in the code:\r\n\r\n```\r\n    # Encoder.\r\n    encoder_cell = rnn_cell.EmbeddingWrapper(\r\n        cell, embedding_classes=num_encoder_symbols,\r\n        embedding_size=embedding_size)\r\n    _, encoder_state = rnn.rnn(encoder_cell, encoder_inputs, dtype=dtype)\r\n```\r\n\r\nIt looks like the encoder inputs are mapped to vectors of size embedding_size.\r\n\r\nLooking forward to hearing the community thoughts (this is also the case in the docstring of embedding_attention_seq2seq)."}