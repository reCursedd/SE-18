{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23214", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23214/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23214/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23214/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23214", "id": 373519298, "node_id": "MDU6SXNzdWUzNzM1MTkyOTg=", "number": 23214, "title": "Try to Run Tensorflow on CUP clusters", "user": {"login": "bastia1989", "id": 8423310, "node_id": "MDQ6VXNlcjg0MjMzMTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/8423310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bastia1989", "html_url": "https://github.com/bastia1989", "followers_url": "https://api.github.com/users/bastia1989/followers", "following_url": "https://api.github.com/users/bastia1989/following{/other_user}", "gists_url": "https://api.github.com/users/bastia1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/bastia1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bastia1989/subscriptions", "organizations_url": "https://api.github.com/users/bastia1989/orgs", "repos_url": "https://api.github.com/users/bastia1989/repos", "events_url": "https://api.github.com/users/bastia1989/events{/privacy}", "received_events_url": "https://api.github.com/users/bastia1989/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-10-24T14:41:44Z", "updated_at": "2018-11-18T01:03:26Z", "closed_at": "2018-11-18T01:03:26Z", "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a build/installation issue. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.9</li>\n<li>TensorFlow version: 1.9.0-rc0</li>\n<li>Python version:3.6.5</li>\n<li>Installed using virtualenv? pip? conda?: pip</li>\n<li>Bazel version (if compiling from source): N/A</li>\n<li>CUDA/cuDNN version: N/A</li>\n<li>GPU model and memory: N/A</li>\n</ul>\n<p>Firstly, I want to run tensorflow on a cluster only with CPU nodes. Therefore no CUDA library is installed and only has the CPU-only version of tensorflow installed. I have many questions on the correct setting. Let's start from the very basic situation:</p>\n<h2>1. node with multiple CPU cores</h2>\n<p>From this <a href=\"https://stackoverflow.com/a/37660913/4468490\" rel=\"nofollow\">answer</a>, it propose to set <code>device_count = {'GPU': 0}</code> in <code>tf.ConfigProto</code> to force the code to be run on CPU only. since my platform has GPUs, so I think this step deosn't work for me. <em>However</em>, I am curious on under such setting, how the resources of CPUs are assigned. Will the code only run on only 1 CPU cores or not?</p>\n<p>So I turned to <a href=\"https://gist.github.com/yaroslavvb/9a5f4a0b613c79152152b35c0bc840b8#file-cpu_device_test-py-L21\">yaroslavvb's example</a>, but I still have some problems:</p>\n<ol>\n<li>\n<p>can I avoid using more statement <code>with tf.device(\"/cpu:0\"):</code>, etc?</p>\n</li>\n<li>\n<p>what's the relation between <code>device_count={\"CPU\": 8}</code> and <code>inter_op_parallelism_threads=3,                    intra_op_parallelism_threads=1</code>? From the <a href=\"https://stackoverflow.com/a/41233901/4468490\" rel=\"nofollow\">answer from mrry</a>, I think <code>inter</code> controls the distribution with threads working on independent blocks in my graph (say, multiple independent matrix operations), and <code>intra</code> works on \"internally parallelable\" tasks (say, a matrix multiplication). Am I correct?  Back to <a href=\"https://gist.github.com/yaroslavvb/9a5f4a0b613c79152152b35c0bc840b8#file-cpu_device_test-py-L21\">yaroslavvb's example</a>, I think the task is assigned to 4 CPU cores, and <code>{\"CPU\": 8}</code> requests 8 cores. If I am correct, why request more than needed?</p>\n</li>\n<li>\n<p>how the task is distributed among each CPU cores, without using <code>with tf.device()</code> statement?</p>\n</li>\n</ol>\n<p>After all these, here is my code, for a simple MNIST example:</p>\n<pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\nimport datetime as dt\nimport urllib.request, json\nimport os, sys\nimport numpy as np\nsys.path.append('./stanford-tensorflow-tutorials/examples')\nimport utils\n# This code has been tested with TensorFlow 1.6\nimport tensorflow as tf\nimage_size = 28\nn_channels = 1\nn_classes = 10\nn_train = 55000\nn_valid = 5000\nn_test = 10000\nn_epochs = 25\nbatch_size = 100\nmnist_folder = './MNIST_data'\nutils.download_mnist(mnist_folder)\ntrain, val, test = utils.read_mnist(mnist_folder, flatten=True)\n#mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n\nconfig = tf.ConfigProto(device_count={\"CPU\": 6}, # limit to num_cpu_core CPU usage  \n                inter_op_parallelism_threads = 6,   \n                intra_op_parallelism_threads = 6,\n                allow_soft_placement=True,                \n                log_device_placement=True)  \n                \n                \n                \n                \ntf.reset_default_graph()\n....\n</code></pre>\n<p>I am not sure if this <code>config</code> setting makes me be able to use 6 CPU cores.</p>\n<p>Then we move further:</p>\n<h2>2. multiple nodes with multiple CPU cores</h2>\n<p>I think this belongs to <strong>distributed tensorflow</strong> concept. Is this senario comparable to multiple GPUs running?</p>", "body_text": "Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template\nSystem information\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.9\nTensorFlow version: 1.9.0-rc0\nPython version:3.6.5\nInstalled using virtualenv? pip? conda?: pip\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\n\nFirstly, I want to run tensorflow on a cluster only with CPU nodes. Therefore no CUDA library is installed and only has the CPU-only version of tensorflow installed. I have many questions on the correct setting. Let's start from the very basic situation:\n1. node with multiple CPU cores\nFrom this answer, it propose to set device_count = {'GPU': 0} in tf.ConfigProto to force the code to be run on CPU only. since my platform has GPUs, so I think this step deosn't work for me. However, I am curious on under such setting, how the resources of CPUs are assigned. Will the code only run on only 1 CPU cores or not?\nSo I turned to yaroslavvb's example, but I still have some problems:\n\n\ncan I avoid using more statement with tf.device(\"/cpu:0\"):, etc?\n\n\nwhat's the relation between device_count={\"CPU\": 8} and inter_op_parallelism_threads=3,                    intra_op_parallelism_threads=1? From the answer from mrry, I think inter controls the distribution with threads working on independent blocks in my graph (say, multiple independent matrix operations), and intra works on \"internally parallelable\" tasks (say, a matrix multiplication). Am I correct?  Back to yaroslavvb's example, I think the task is assigned to 4 CPU cores, and {\"CPU\": 8} requests 8 cores. If I am correct, why request more than needed?\n\n\nhow the task is distributed among each CPU cores, without using with tf.device() statement?\n\n\nAfter all these, here is my code, for a simple MNIST example:\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport datetime as dt\nimport urllib.request, json\nimport os, sys\nimport numpy as np\nsys.path.append('./stanford-tensorflow-tutorials/examples')\nimport utils\n# This code has been tested with TensorFlow 1.6\nimport tensorflow as tf\nimage_size = 28\nn_channels = 1\nn_classes = 10\nn_train = 55000\nn_valid = 5000\nn_test = 10000\nn_epochs = 25\nbatch_size = 100\nmnist_folder = './MNIST_data'\nutils.download_mnist(mnist_folder)\ntrain, val, test = utils.read_mnist(mnist_folder, flatten=True)\n#mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n\nconfig = tf.ConfigProto(device_count={\"CPU\": 6}, # limit to num_cpu_core CPU usage  \n                inter_op_parallelism_threads = 6,   \n                intra_op_parallelism_threads = 6,\n                allow_soft_placement=True,                \n                log_device_placement=True)  \n                \n                \n                \n                \ntf.reset_default_graph()\n....\n\nI am not sure if this config setting makes me be able to use 6 CPU cores.\nThen we move further:\n2. multiple nodes with multiple CPU cores\nI think this belongs to distributed tensorflow concept. Is this senario comparable to multiple GPUs running?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.9 \r\n- TensorFlow version: 1.9.0-rc0\r\n- Python version:3.6.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nFirstly, I want to run tensorflow on a cluster only with CPU nodes. Therefore no CUDA library is installed and only has the CPU-only version of tensorflow installed. I have many questions on the correct setting. Let's start from the very basic situation:\r\n\r\n## 1. node with multiple CPU cores\r\n\r\nFrom this [answer](https://stackoverflow.com/a/37660913/4468490), it propose to set `device_count = {'GPU': 0}` in `tf.ConfigProto` to force the code to be run on CPU only. since my platform has GPUs, so I think this step deosn't work for me. *However*, I am curious on under such setting, how the resources of CPUs are assigned. Will the code only run on only 1 CPU cores or not?\r\n\r\nSo I turned to [yaroslavvb's example](https://gist.github.com/yaroslavvb/9a5f4a0b613c79152152b35c0bc840b8#file-cpu_device_test-py-L21), but I still have some problems:\r\n\r\n1. can I avoid using more statement `with tf.device(\"/cpu:0\"):`, etc?\r\n\r\n2. what's the relation between `device_count={\"CPU\": 8}` and `inter_op_parallelism_threads=3,                    intra_op_parallelism_threads=1`? From the [answer from mrry](https://stackoverflow.com/a/41233901/4468490), I think `inter` controls the distribution with threads working on independent blocks in my graph (say, multiple independent matrix operations), and `intra` works on \"internally parallelable\" tasks (say, a matrix multiplication). Am I correct?  Back to [yaroslavvb's example](https://gist.github.com/yaroslavvb/9a5f4a0b613c79152152b35c0bc840b8#file-cpu_device_test-py-L21), I think the task is assigned to 4 CPU cores, and `{\"CPU\": 8}` requests 8 cores. If I am correct, why request more than needed? \r\n\r\n3. how the task is distributed among each CPU cores, without using `with tf.device()` statement?\r\n\r\n\r\nAfter all these, here is my code, for a simple MNIST example:\r\n```\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nimport datetime as dt\r\nimport urllib.request, json\r\nimport os, sys\r\nimport numpy as np\r\nsys.path.append('./stanford-tensorflow-tutorials/examples')\r\nimport utils\r\n# This code has been tested with TensorFlow 1.6\r\nimport tensorflow as tf\r\nimage_size = 28\r\nn_channels = 1\r\nn_classes = 10\r\nn_train = 55000\r\nn_valid = 5000\r\nn_test = 10000\r\nn_epochs = 25\r\nbatch_size = 100\r\nmnist_folder = './MNIST_data'\r\nutils.download_mnist(mnist_folder)\r\ntrain, val, test = utils.read_mnist(mnist_folder, flatten=True)\r\n#mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\nconfig = tf.ConfigProto(device_count={\"CPU\": 6}, # limit to num_cpu_core CPU usage  \r\n                inter_op_parallelism_threads = 6,   \r\n                intra_op_parallelism_threads = 6,\r\n                allow_soft_placement=True,                \r\n                log_device_placement=True)  \r\n                \r\n                \r\n                \r\n                \r\ntf.reset_default_graph()\r\n....\r\n```\r\nI am not sure if this `config` setting makes me be able to use 6 CPU cores. \r\n\r\nThen we move further:\r\n## 2. multiple nodes with multiple CPU cores\r\n\r\nI think this belongs to **distributed tensorflow** concept. Is this senario comparable to multiple GPUs running?\r\n\r\n"}