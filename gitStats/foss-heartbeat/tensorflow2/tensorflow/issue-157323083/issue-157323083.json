{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2552", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2552/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2552/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2552/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2552", "id": 157323083, "node_id": "MDU6SXNzdWUxNTczMjMwODM=", "number": 2552, "title": "Is it possible to calculate two kinds of gradient separately in tensorflow", "user": {"login": "333caowei", "id": 4569055, "node_id": "MDQ6VXNlcjQ1NjkwNTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/4569055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/333caowei", "html_url": "https://github.com/333caowei", "followers_url": "https://api.github.com/users/333caowei/followers", "following_url": "https://api.github.com/users/333caowei/following{/other_user}", "gists_url": "https://api.github.com/users/333caowei/gists{/gist_id}", "starred_url": "https://api.github.com/users/333caowei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/333caowei/subscriptions", "organizations_url": "https://api.github.com/users/333caowei/orgs", "repos_url": "https://api.github.com/users/333caowei/repos", "events_url": "https://api.github.com/users/333caowei/events{/privacy}", "received_events_url": "https://api.github.com/users/333caowei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-05-28T06:03:09Z", "updated_at": "2016-09-10T05:38:56Z", "closed_at": "2016-06-01T18:14:32Z", "author_association": "NONE", "body_html": "<pre><code>w_1   = tf.get_variable(\"w_1\", shape)   \nw_2   = tf.get_variable(\"w_2\", shape) \noutput = tf.mul(w_1, w_2)\n.....\n.....\noptimizer = tf.train.AdamOptimizer(alpha).minimize(self.cost)\n</code></pre>\n<p>As we know, when we run \"optimizer\", tensorflow will caculate gradient and update w_1 &amp; w_2.</p>\n<p>But what i want to do is, first, I want to <strong>treat w_1 as a constant</strong>, I just want to caculate gradient and update <strong>only w_2</strong>. Second, <strong>treat w_2 as a constant</strong> and caculate gradient and update <strong>only w_1</strong>. I want to <strong>take turns</strong> to do these things.</p>\n<p>Actually, I have seen this  #before: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"128044523\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/834\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/834/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/834\">#834</a>. But I use <strong>BasicLSTMCell</strong> module.  I try this code: <code>print (tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))</code>, it shows there are <strong>four</strong> kinds of parameter in my neural network, <strong>which means besides w_1 and w_2, there are other two parameters in  BasicLSTMCell.</strong><br>\nSo, if I use such as <code>var_list=[w_1]</code>, the other two parameters in BasicLSTMCell  can not be optimized, How can I do it?</p>", "body_text": "w_1   = tf.get_variable(\"w_1\", shape)   \nw_2   = tf.get_variable(\"w_2\", shape) \noutput = tf.mul(w_1, w_2)\n.....\n.....\noptimizer = tf.train.AdamOptimizer(alpha).minimize(self.cost)\n\nAs we know, when we run \"optimizer\", tensorflow will caculate gradient and update w_1 & w_2.\nBut what i want to do is, first, I want to treat w_1 as a constant, I just want to caculate gradient and update only w_2. Second, treat w_2 as a constant and caculate gradient and update only w_1. I want to take turns to do these things.\nActually, I have seen this  #before: #834. But I use BasicLSTMCell module.  I try this code: print (tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)), it shows there are four kinds of parameter in my neural network, which means besides w_1 and w_2, there are other two parameters in  BasicLSTMCell.\nSo, if I use such as var_list=[w_1], the other two parameters in BasicLSTMCell  can not be optimized, How can I do it?", "body": "```\nw_1   = tf.get_variable(\"w_1\", shape)   \nw_2   = tf.get_variable(\"w_2\", shape) \noutput = tf.mul(w_1, w_2)\n.....\n.....\noptimizer = tf.train.AdamOptimizer(alpha).minimize(self.cost)\n```\n\nAs we know, when we run \"optimizer\", tensorflow will caculate gradient and update w_1 & w_2.\n\nBut what i want to do is, first, I want to **treat w_1 as a constant**, I just want to caculate gradient and update **only w_2**. Second, **treat w_2 as a constant** and caculate gradient and update **only w_1**. I want to **take turns** to do these things. \n\nActually, I have seen this  #before: #834. But I use **BasicLSTMCell** module.  I try this code: `print (tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))`, it shows there are **four** kinds of parameter in my neural network, **which means besides w_1 and w_2, there are other two parameters in  BasicLSTMCell.** \nSo, if I use such as `var_list=[w_1]`, the other two parameters in BasicLSTMCell  can not be optimized, How can I do it? \n"}