{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/158575585", "pull_request_review_id": 85415976, "id": 158575585, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODU3NTU4NQ==", "diff_hunk": "@@ -0,0 +1,306 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/common_shape_fns.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/register_types_traits.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/lib/gtl/array_slice.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/util/work_sharder.h\"\n+\n+namespace tensorflow {\n+\n+#define EIGEN_USE_THREADS\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+template <typename T>\n+void DoRoll(OpKernelContext* context, const int64 N, const int D,\n+            const gtl::ArraySlice<int>& dim_size, const T* input, T* output,\n+            const gtl::ArraySlice<int>& threshold,\n+            const gtl::ArraySlice<int64>& dim_range) {\n+  auto work = [input, output, D, &dim_size, &threshold, &dim_range](int64 start,\n+                                                                    int64 end) {\n+    // array of indices for each dimension\n+    gtl::InlinedVector<int, 4> indices(D);\n+    int offset = 0;  // the shift along the flattened tensor for current element\n+    // initialize indices and offset\n+    for (int d = 0; d < D; d++) {\n+      // stride is the number of indices over in the flattened tensor\n+      // you need to skip in order to make it over to an adjacent element\n+      // along a dimension. dim_size[d] != 0 because we set it to max(dim, 1)\n+      const int64 stride = dim_range[d] / dim_size[d];\n+      const int shift = dim_size[d] - threshold[d];\n+      const int indx = (start / stride) % dim_size[d];\n+      indices[d] = indx;\n+      // calculate dimension index after the shift\n+      const int shifted_indx = (indx + shift) % dim_size[d];\n+      offset += (shifted_indx - indx) * stride;\n+    }\n+\n+    for (int64 i = start; i < end; i++) {\n+      output[i + offset] = input[i];\n+      // create next combination of indices\n+      // while at it adjust offset if needed\n+      for (int d = D - 1; d >= 0; d--) {\n+        const int indx = (indices[d] + 1) % dim_size[d];\n+        indices[d] = indx;\n+        if (indx != 0) {\n+          if (indx == threshold[d]) {  // we've reached the threshold\n+            // dim_range[d] = threshold[d] + shift[d]\n+            // offset = shift[d] + ... other offsets\n+            // offset - dim_range[d] = -threshold[d] + ... other offsets\n+            // thus we undo our previous offset as well as add a new offset of\n+            // -threshold[d] in one opperation\n+            offset -= dim_range[d];  // now wraps around\n+          }\n+          break;                         // indx != 0 don't need to carry\n+        } else if (threshold[d] != 0) {  // if threshold is 0 shift is 0\n+          offset += dim_range[d];        // indx became 0 so reverse wrap around\n+        }\n+      }\n+    }\n+  };\n+  // Shard\n+  auto worker_threads = context->device()->tensorflow_cpu_worker_threads();\n+  const int cost_per_unit = 50;  // rough esitmate\n+  Shard(worker_threads->num_threads, worker_threads->workers, N, cost_per_unit,\n+        std::move(work));\n+}\n+\n+template <typename T>\n+// Use memcpy to copy memory in groups when the data type supports memcpy\n+void DoRollV2(OpKernelContext* context, const int64 N, const int D,\n+              const gtl::ArraySlice<int>& dim_size, const T* input, T* output,\n+              const gtl::ArraySlice<int>& threshold,\n+              const gtl::ArraySlice<int64>& dim_range, const int64 isd) {\n+  auto work = [input, output, D, &dim_size, &threshold, &dim_range, isd](\n+                  int64 start, int64 end) {\n+    const T* in_ptr = &input[0];\n+    T* out_ptr = &output[0];\n+    in_ptr += start;\n+    out_ptr += start;\n+\n+    // array of indices for each dimension\n+    // indicies = [i, j, k, l, m, n]\n+    gtl::InlinedVector<int, 4> indicies(D);\n+    // the offset needed to make all inner non-shifting dimensions become 0\n+    int64 remainder_offset = 0;\n+    // initialize indicies\n+    for (int d = 0; d < D; d++) {\n+      // stride is the number of indices over in the flattened tensor\n+      // you need to skip in order to make it over to an adjacent element\n+      // along a dimension. dim_size[d] != 0 because we set it to max(dim, 1)\n+      const int64 stride = dim_range[d] / dim_size[d];\n+      const int shift = dim_size[d] - threshold[d];\n+      const int indx = (start / stride) % dim_size[d];\n+      indicies[d] = indx;\n+      // calculate dimension index after the shift\n+      int out_indx = (indx + shift) % dim_size[d];\n+      if (d > isd) {\n+        // trailing zeroes for indices after the inner shifted dimension\n+        out_indx = 0;\n+        remainder_offset += (out_indx - indx) * stride;\n+      }\n+      out_ptr += (out_indx - indx) * stride;\n+    }\n+    // set trailing zeroes for indices after the inner shifted dimension\n+    for (int d = D - 1; d > isd; d--) indicies[d] = 0;\n+    // the distance along the flattend tensor to the next element in the isd\n+    const int64 isd_stride = dim_range[isd] / dim_size[isd];\n+\n+    // the number of indices in the isd dimension the next group will skip\n+    // to make it to the next threshold or end point\n+    int isd_indx_skip = 0;\n+    // the size of the next group\n+    int64 group_size = 0;\n+    // initialize isd_indx_skip and group_size\n+    if (indicies[isd] < threshold[isd]) {\n+      isd_indx_skip = threshold[isd] - indicies[isd];\n+      group_size = isd_indx_skip * isd_stride + remainder_offset;\n+    } else {\n+      isd_indx_skip = dim_size[isd] - indicies[isd];\n+      group_size = isd_indx_skip * isd_stride + remainder_offset;\n+    }\n+\n+    int64 i = start;\n+    while (i < end) {\n+      // copy group of elements\n+      memcpy(out_ptr, in_ptr, group_size * sizeof(T));\n+\n+      // shift i and the pointers over to the next group position\n+      i += group_size;\n+      out_ptr += group_size;\n+      in_ptr += group_size;\n+\n+      // produce next combination of indices and adjust the out_ptr position\n+      // to fix the offset if necessary\n+      // the isd (inner shift dim) should skip to next threshold or endpoint\n+      // all dimensions to the left increment by 1 when a digit is carried\n+      // all dimensions to the right remain set to 0\n+      //            +1 +1 +1 +isd_indx_skip\n+      // indicies = [i, j, k, l, 0, 0]\n+      //                      ^isd\n+      for (int d = isd; d >= 0; d--) {\n+        int inc = 1;\n+        if (d == isd) inc = isd_indx_skip;\n+        const int indx = (indicies[d] + inc) % dim_size[d];\n+        indicies[d] = indx;\n+        if (indx != 0) {\n+          if (indx == threshold[d]) {\n+            out_ptr -= dim_range[d];  // now wraps around\n+          }\n+          break;                         // indx != 0 don't need to carry\n+        } else if (threshold[d] != 0) {  // if threshold is 0 shift is 0\n+          out_ptr += dim_range[d];       // indx became 0 so reverse wrap around\n+        }\n+      }\n+\n+      // set isd_indx_skip and group_size for next iteration\n+      if (indicies[isd] < threshold[isd]) {\n+        isd_indx_skip = threshold[isd] - indicies[isd];\n+        group_size = isd_indx_skip * isd_stride;\n+      } else {\n+        isd_indx_skip = dim_size[isd] - indicies[isd];\n+        group_size = isd_indx_skip * isd_stride;\n+      }\n+    }\n+  };\n+  // Shard\n+  auto worker_threads = context->device()->tensorflow_cpu_worker_threads();\n+  const int64 ave_group_size = dim_range[isd] / 2;\n+  const int cost_per_unit = 50 / std::max<int>(ave_group_size, 1);  // rough esitmate\n+  Shard(worker_threads->num_threads, worker_threads->workers, N, cost_per_unit,\n+        std::move(work));\n+}\n+\n+template <typename Device, typename T, typename Tshift, typename Taxis>\n+class RollOp : public OpKernel {\n+ public:\n+  explicit RollOp(OpKernelConstruction* context) : OpKernel(context) {}\n+\n+  void Compute(OpKernelContext* context) override {\n+    // Grab the input tensor\n+    const Tensor& input = context->input(0);\n+    const Tensor& shift = context->input(1);\n+    const Tensor& axis = context->input(2);\n+\n+    auto shift_flat = shift.flat<Tshift>();\n+    auto axis_flat = axis.flat<Taxis>();\n+\n+    OP_REQUIRES(context, TensorShapeUtils::IsVectorOrHigher(input.shape()),\n+                errors::InvalidArgument(\"input must be 1-D or higher\"));\n+    OP_REQUIRES(context, shift.shape().dims() <= 1,\n+                errors::InvalidArgument(\n+                    \"shift must be a scalar or a 1-D vector. Found: \",\n+                    shift.shape().DebugString()));\n+    OP_REQUIRES(context, axis.shape().dims() <= 1,\n+                errors::InvalidArgument(\n+                    \"axis must be a scalar or a 1-D vector. Found: \",\n+                    axis.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, shift.shape() == axis.shape(),\n+        errors::InvalidArgument(\"shift and axis must be the same size\"));\n+    const int64 N = input.NumElements();\n+    const int M = static_cast<int>(shift_flat.size());\n+    const int D = input.dims();\n+\n+    // if there are any duplicate axes, shift_mod_sum will have the\n+    // total modulo sum of shifts for each dimension\n+    gtl::InlinedVector<int, 4> shift_mod_sum(D, 0);\n+    for (int m = 0; m < M; m++) {\n+      const int a = axis_flat(m);\n+      OP_REQUIRES(context, a < D,\n+                  errors::InvalidArgument(\"axis \", a, \" is out of range\"));\n+      const int ds = std::max<int>(static_cast<int>(input.dim_size(a)), 1);\n+      const int sum = shift_mod_sum[a] + static_cast<int>(shift_flat(m));\n+      // modulo that works with negatives: ((x % y) + y) % y\n+      shift_mod_sum[a] = (sum % ds + ds) % ds;", "path": "tensorflow/core/kernels/roll_op.cc", "position": null, "original_position": 233, "commit_id": "70fc4c06e907b5578d3345a08967714bfd96f0de", "original_commit_id": "225f72c87a435201e1e2c4c9a8675b3795f68f2b", "user": {"login": "kobejean", "id": 3527868, "node_id": "MDQ6VXNlcjM1Mjc4Njg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3527868?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kobejean", "html_url": "https://github.com/kobejean", "followers_url": "https://api.github.com/users/kobejean/followers", "following_url": "https://api.github.com/users/kobejean/following{/other_user}", "gists_url": "https://api.github.com/users/kobejean/gists{/gist_id}", "starred_url": "https://api.github.com/users/kobejean/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kobejean/subscriptions", "organizations_url": "https://api.github.com/users/kobejean/orgs", "repos_url": "https://api.github.com/users/kobejean/repos", "events_url": "https://api.github.com/users/kobejean/events{/privacy}", "received_events_url": "https://api.github.com/users/kobejean/received_events", "type": "User", "site_admin": false}, "body": "shift_flat can have negative values if the user wants the elements shifted in the opposite direction.", "created_at": "2017-12-23T03:45:33Z", "updated_at": "2018-01-25T23:18:34Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14953#discussion_r158575585", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14953", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/158575585"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14953#discussion_r158575585"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14953"}}, "body_html": "<p>shift_flat can have negative values if the user wants the elements shifted in the opposite direction.</p>", "body_text": "shift_flat can have negative values if the user wants the elements shifted in the opposite direction.", "in_reply_to_id": 158392955}