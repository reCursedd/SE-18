{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435627225", "html_url": "https://github.com/tensorflow/tensorflow/issues/23465#issuecomment-435627225", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23465", "id": 435627225, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTYyNzIyNQ==", "user": {"login": "yashk2810", "id": 14262417, "node_id": "MDQ6VXNlcjE0MjYyNDE3", "avatar_url": "https://avatars1.githubusercontent.com/u/14262417?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yashk2810", "html_url": "https://github.com/yashk2810", "followers_url": "https://api.github.com/users/yashk2810/followers", "following_url": "https://api.github.com/users/yashk2810/following{/other_user}", "gists_url": "https://api.github.com/users/yashk2810/gists{/gist_id}", "starred_url": "https://api.github.com/users/yashk2810/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yashk2810/subscriptions", "organizations_url": "https://api.github.com/users/yashk2810/orgs", "repos_url": "https://api.github.com/users/yashk2810/repos", "events_url": "https://api.github.com/users/yashk2810/events{/privacy}", "received_events_url": "https://api.github.com/users/yashk2810/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-03T22:56:05Z", "updated_at": "2018-11-03T23:58:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi Aakanksha, thanks for reaching out. The Tokenizer code that I updated made it into TF 1.12(<a href=\"https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py\">https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py</a>) takes into account the \"unk\" token and the num_words parameter. As I was trying to work around this issue at the time I wrote this notebook, and the tokenizer code being updated lead to the error. If you try out the following code, it should work as expected.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\ntf.enable_eager_execution()\n<span class=\"pl-k\">import</span> re\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nsent <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;start&gt; Alice in wonderland. &lt;end&gt;<span class=\"pl-pds\">\"</span></span>\nsent2 <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;start&gt; When suddenly Alice saw a White Rabbit. &lt;end&gt;<span class=\"pl-pds\">\"</span></span>\nx_train <span class=\"pl-k\">=</span> [sent, sent2]\n\ntop_k <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\ntokenizer <span class=\"pl-k\">=</span> tf.keras.preprocessing.text.Tokenizer(<span class=\"pl-v\">num_words</span><span class=\"pl-k\">=</span>top_k, \n                                                  <span class=\"pl-v\">oov_token</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;unk&gt;<span class=\"pl-pds\">\"</span></span>, \n                                                  <span class=\"pl-v\">filters</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>!\"#$%&amp;()*+.,-/:;=?@[\\]^_`{|}~ <span class=\"pl-pds\">'</span></span>)\ntokenizer.fit_on_texts(x_train)\ntrain_seqs <span class=\"pl-k\">=</span> tokenizer.texts_to_sequences(x_train)\n<span class=\"pl-c1\">print</span>(tokenizer.word_index)\nidx <span class=\"pl-k\">=</span> tokenizer.word_index[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;unk&gt;<span class=\"pl-pds\">'</span></span>] <span class=\"pl-c\"><span class=\"pl-c\">#</span>saving the original index of '&lt;unk&gt;'</span>\n<span class=\"pl-c1\">print</span> (idx)\nindex_word <span class=\"pl-k\">=</span> {value:key <span class=\"pl-k\">for</span> key, value <span class=\"pl-k\">in</span> tokenizer.word_index.items()}\nindex_word[idx]</pre></div>\n<p>Also, if you want to, you can open a PR and fix this issue. I think you will only have to remove the following lines.</p>\n<pre><code>tokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value &lt;= top_k}\n# putting &lt;unk&gt; token in the word2idx dictionary\ntokenizer.word_index[tokenizer.oov_token] = top_k + 1\n</code></pre>", "body_text": "Hi Aakanksha, thanks for reaching out. The Tokenizer code that I updated made it into TF 1.12(https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py) takes into account the \"unk\" token and the num_words parameter. As I was trying to work around this issue at the time I wrote this notebook, and the tokenizer code being updated lead to the error. If you try out the following code, it should work as expected.\nimport tensorflow as tf\ntf.enable_eager_execution()\nimport re\nimport numpy as np\n\nsent = \"<start> Alice in wonderland. <end>\"\nsent2 = \"<start> When suddenly Alice saw a White Rabbit. <end>\"\nx_train = [sent, sent2]\n\ntop_k = 5\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \n                                                  oov_token=\"<unk>\", \n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(x_train)\ntrain_seqs = tokenizer.texts_to_sequences(x_train)\nprint(tokenizer.word_index)\nidx = tokenizer.word_index['<unk>'] #saving the original index of '<unk>'\nprint (idx)\nindex_word = {value:key for key, value in tokenizer.word_index.items()}\nindex_word[idx]\nAlso, if you want to, you can open a PR and fix this issue. I think you will only have to remove the following lines.\ntokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value <= top_k}\n# putting <unk> token in the word2idx dictionary\ntokenizer.word_index[tokenizer.oov_token] = top_k + 1", "body": "Hi Aakanksha, thanks for reaching out. The Tokenizer code that I updated made it into TF 1.12(https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py) takes into account the \"unk\" token and the num_words parameter. As I was trying to work around this issue at the time I wrote this notebook, and the tokenizer code being updated lead to the error. If you try out the following code, it should work as expected. \r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nimport re\r\nimport numpy as np\r\n\r\nsent = \"<start> Alice in wonderland. <end>\"\r\nsent2 = \"<start> When suddenly Alice saw a White Rabbit. <end>\"\r\nx_train = [sent, sent2]\r\n\r\ntop_k = 5\r\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \r\n                                                  oov_token=\"<unk>\", \r\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\r\ntokenizer.fit_on_texts(x_train)\r\ntrain_seqs = tokenizer.texts_to_sequences(x_train)\r\nprint(tokenizer.word_index)\r\nidx = tokenizer.word_index['<unk>'] #saving the original index of '<unk>'\r\nprint (idx)\r\nindex_word = {value:key for key, value in tokenizer.word_index.items()}\r\nindex_word[idx]\r\n```\r\n\r\nAlso, if you want to, you can open a PR and fix this issue. I think you will only have to remove the following lines.\r\n\r\n```\r\ntokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value <= top_k}\r\n# putting <unk> token in the word2idx dictionary\r\ntokenizer.word_index[tokenizer.oov_token] = top_k + 1\r\n```"}