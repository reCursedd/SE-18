{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23465", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23465/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23465/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23465/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23465", "id": 376950043, "node_id": "MDU6SXNzdWUzNzY5NTAwNDM=", "number": 23465, "title": "Bug in image_captioning_with_attention.ipynb ", "user": {"login": "joaak", "id": 29533036, "node_id": "MDQ6VXNlcjI5NTMzMDM2", "avatar_url": "https://avatars2.githubusercontent.com/u/29533036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joaak", "html_url": "https://github.com/joaak", "followers_url": "https://api.github.com/users/joaak/followers", "following_url": "https://api.github.com/users/joaak/following{/other_user}", "gists_url": "https://api.github.com/users/joaak/gists{/gist_id}", "starred_url": "https://api.github.com/users/joaak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joaak/subscriptions", "organizations_url": "https://api.github.com/users/joaak/orgs", "repos_url": "https://api.github.com/users/joaak/repos", "events_url": "https://api.github.com/users/joaak/events{/privacy}", "received_events_url": "https://api.github.com/users/joaak/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 779195454, "node_id": "MDU6TGFiZWw3NzkxOTU0NTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/pending%20merge%20internally", "name": "pending merge internally", "color": "efad99", "default": false}], "state": "open", "locked": false, "assignee": {"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-11-02T20:04:04Z", "updated_at": "2018-11-22T18:51:07Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information:</strong> <em><strong>Running the notebook on Colab.</strong></em></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA</li>\n<li>TensorFlow installed from (source or binary): Colab</li>\n<li>TensorFlow version (use command below): Colab ('1.12.0-rc2')</li>\n<li>Python version: Colab (3.6)</li>\n<li>Bazel version (if compiling from source): NA</li>\n<li>GCC/Compiler version (if compiling from source): NA</li>\n<li>CUDA/cuDNN version: Colab</li>\n<li>GPU model and memory: Colab</li>\n</ul>\n<p><strong>Describe the current behavior</strong></p>\n<p>Error in Cell 27 (Training):</p>\n<p><code>InvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/</code></p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>If <code>tokenizer.word_index[tokenizer.oov_token] = top_k + 1</code> in Cell 10 is commented out, the notebook runs end-to-end on Colab.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<p>The reason this happens could be because in Cells 9 and 10, Tokenizer is defined with <code>oov_token=\"&lt;unk&gt;\"</code> in Cell 9, which assigns \"&lt;unk&gt; : 1\" in the token dictionary, and then <code>tokenizer.word_index[tokenizer.oov_token] = top_k + 1</code> in Cell 10 assigns \"&lt;unk&gt;: top_k + 1\" which makes the number &lt;unk&gt; was previously assigned to non-existent in the dictionary and causes trouble during lookup.</p>\n<p>The following demonstration explains why we can get an error when we refer to the dictionary later on.</p>\n<pre><code>import tensorflow as tf\ntf.enable_eager_execution()\nimport re\nimport numpy as np\n\nsent = \"&lt;start&gt; Alice in wonderland. &lt;end&gt;\"\nsent2 = \"&lt;start&gt; When suddenly Alice saw a White Rabbit. &lt;end&gt;\"\nx_train = [sent, sent2]\n\ntop_k = 5\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \n                                                  oov_token=\"&lt;unk&gt;\", \n                                                  filters='!\"#$%&amp;()*+.,-/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(x_train)\ntrain_seqs = tokenizer.texts_to_sequences(x_train)\ntokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value &lt;= top_k}\nprint(tokenizer.word_index)\nidx = tokenizer.word_index['&lt;unk&gt;'] #saving the original index of '&lt;unk&gt;'\ntokenizer.word_index[tokenizer.oov_token] = top_k + 1 # the problematic line\nprint(tokenizer.word_index)\nindex_word = {value:key for key, value in tokenizer.word_index.items()}\nindex_word[idx]\n</code></pre>\n<p><code>Output:</code></p>\n<pre><code>{'&lt;unk&gt;': 1, '&lt;start&gt;': 2, 'alice': 3, '&lt;end&gt;': 4, 'in': 5}\n{'&lt;unk&gt;': 6, '&lt;start&gt;': 2, 'alice': 3, '&lt;end&gt;': 4, 'in': 5}\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-11-55bbbd305835&gt; in &lt;module&gt;()\n     11 print(tokenizer.word_index)\n     12 index_word = {value:key for key, value in tokenizer.word_index.items()}\n---&gt; 13 index_word[idx]\n\nKeyError: 1\n</code></pre>\n<p><strong>Other info / logs</strong><br>\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.</p>\n<pre><code>---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n&lt;ipython-input-58-03bc9960ded7&gt; in &lt;module&gt;()\n     19             for i in range(1, target.shape[1]):\n     20                 # passing the features through the decoder\n---&gt; 21                 predictions, hidden, _ = decoder(dec_input, features, hidden)\n     22 \n     23                 loss += loss_function(target[:, i], predictions)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    755       if not in_deferred_mode:\n    756         self._in_call = True\n--&gt; 757         outputs = self.call(inputs, *args, **kwargs)\n    758         self._in_call = False\n    759         if outputs is None:\n\n&lt;ipython-input-54-b844d20e3fc2&gt; in call(self, x, features, hidden)\n     16 \n     17     # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n---&gt; 18     x = self.embedding(x)\n     19 \n     20     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    755       if not in_deferred_mode:\n    756         self._in_call = True\n--&gt; 757         outputs = self.call(inputs, *args, **kwargs)\n    758         self._in_call = False\n    759         if outputs is None:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py in call(self, inputs)\n    175     if dtype != 'int32' and dtype != 'int64':\n    176       inputs = math_ops.cast(inputs, 'int32')\n--&gt; 177     out = embedding_ops.embedding_lookup(self.embeddings, inputs)\n    178     return out\n    179 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in embedding_lookup(params, ids, partition_strategy, name, validate_indices, max_norm)\n    311       name=name,\n    312       max_norm=max_norm,\n--&gt; 313       transform_fn=None)\n    314 \n    315 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in _embedding_lookup_and_transform(params, ids, partition_strategy, name, max_norm, transform_fn)\n    131     if np == 1 and (not transform_fn or ids.get_shape().ndims == 1):\n    132       with ops.colocate_with(params[0]):\n--&gt; 133         result = _clip(array_ops.gather(params[0], ids, name=name),\n    134                        ids, max_norm)\n    135         if transform_fn:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)\n   2671     # TODO(apassos) find a less bad way of detecting resource variables without\n   2672     # introducing a circular dependency.\n-&gt; 2673     return params.sparse_read(indices, name=name)\n   2674   except AttributeError:\n   2675     return gen_array_ops.gather_v2(params, indices, axis, name=name)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in sparse_read(self, indices, name)\n    756         tape.variable_accessed(self)\n    757       value = gen_resource_variable_ops.resource_gather(\n--&gt; 758           self._handle, indices, dtype=self._dtype, name=name)\n    759     return array_ops.identity(value)\n    760 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py in resource_gather(resource, indices, dtype, validate_indices, name)\n    611       else:\n    612         message = e.message\n--&gt; 613       _six.raise_from(_core._status_to_exception(e.code, message), None)\n    614 \n    615 \n\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\n\nInvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/\n</code></pre>", "body_text": "System information: Running the notebook on Colab.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\nTensorFlow installed from (source or binary): Colab\nTensorFlow version (use command below): Colab ('1.12.0-rc2')\nPython version: Colab (3.6)\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: Colab\nGPU model and memory: Colab\n\nDescribe the current behavior\nError in Cell 27 (Training):\nInvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/\nDescribe the expected behavior\nIf tokenizer.word_index[tokenizer.oov_token] = top_k + 1 in Cell 10 is commented out, the notebook runs end-to-end on Colab.\nCode to reproduce the issue\nThe reason this happens could be because in Cells 9 and 10, Tokenizer is defined with oov_token=\"<unk>\" in Cell 9, which assigns \"<unk> : 1\" in the token dictionary, and then tokenizer.word_index[tokenizer.oov_token] = top_k + 1 in Cell 10 assigns \"<unk>: top_k + 1\" which makes the number <unk> was previously assigned to non-existent in the dictionary and causes trouble during lookup.\nThe following demonstration explains why we can get an error when we refer to the dictionary later on.\nimport tensorflow as tf\ntf.enable_eager_execution()\nimport re\nimport numpy as np\n\nsent = \"<start> Alice in wonderland. <end>\"\nsent2 = \"<start> When suddenly Alice saw a White Rabbit. <end>\"\nx_train = [sent, sent2]\n\ntop_k = 5\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \n                                                  oov_token=\"<unk>\", \n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(x_train)\ntrain_seqs = tokenizer.texts_to_sequences(x_train)\ntokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value <= top_k}\nprint(tokenizer.word_index)\nidx = tokenizer.word_index['<unk>'] #saving the original index of '<unk>'\ntokenizer.word_index[tokenizer.oov_token] = top_k + 1 # the problematic line\nprint(tokenizer.word_index)\nindex_word = {value:key for key, value in tokenizer.word_index.items()}\nindex_word[idx]\n\nOutput:\n{'<unk>': 1, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}\n{'<unk>': 6, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-11-55bbbd305835> in <module>()\n     11 print(tokenizer.word_index)\n     12 index_word = {value:key for key, value in tokenizer.word_index.items()}\n---> 13 index_word[idx]\n\nKeyError: 1\n\nOther info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n<ipython-input-58-03bc9960ded7> in <module>()\n     19             for i in range(1, target.shape[1]):\n     20                 # passing the features through the decoder\n---> 21                 predictions, hidden, _ = decoder(dec_input, features, hidden)\n     22 \n     23                 loss += loss_function(target[:, i], predictions)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    755       if not in_deferred_mode:\n    756         self._in_call = True\n--> 757         outputs = self.call(inputs, *args, **kwargs)\n    758         self._in_call = False\n    759         if outputs is None:\n\n<ipython-input-54-b844d20e3fc2> in call(self, x, features, hidden)\n     16 \n     17     # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n---> 18     x = self.embedding(x)\n     19 \n     20     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    755       if not in_deferred_mode:\n    756         self._in_call = True\n--> 757         outputs = self.call(inputs, *args, **kwargs)\n    758         self._in_call = False\n    759         if outputs is None:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py in call(self, inputs)\n    175     if dtype != 'int32' and dtype != 'int64':\n    176       inputs = math_ops.cast(inputs, 'int32')\n--> 177     out = embedding_ops.embedding_lookup(self.embeddings, inputs)\n    178     return out\n    179 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in embedding_lookup(params, ids, partition_strategy, name, validate_indices, max_norm)\n    311       name=name,\n    312       max_norm=max_norm,\n--> 313       transform_fn=None)\n    314 \n    315 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in _embedding_lookup_and_transform(params, ids, partition_strategy, name, max_norm, transform_fn)\n    131     if np == 1 and (not transform_fn or ids.get_shape().ndims == 1):\n    132       with ops.colocate_with(params[0]):\n--> 133         result = _clip(array_ops.gather(params[0], ids, name=name),\n    134                        ids, max_norm)\n    135         if transform_fn:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)\n   2671     # TODO(apassos) find a less bad way of detecting resource variables without\n   2672     # introducing a circular dependency.\n-> 2673     return params.sparse_read(indices, name=name)\n   2674   except AttributeError:\n   2675     return gen_array_ops.gather_v2(params, indices, axis, name=name)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in sparse_read(self, indices, name)\n    756         tape.variable_accessed(self)\n    757       value = gen_resource_variable_ops.resource_gather(\n--> 758           self._handle, indices, dtype=self._dtype, name=name)\n    759     return array_ops.identity(value)\n    760 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py in resource_gather(resource, indices, dtype, validate_indices, name)\n    611       else:\n    612         message = e.message\n--> 613       _six.raise_from(_core._status_to_exception(e.code, message), None)\n    614 \n    615 \n\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\n\nInvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/", "body": "**System information:** ***Running the notebook on Colab.***\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): Colab ('1.12.0-rc2')\r\n- Python version: Colab (3.6)\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: Colab\r\n- GPU model and memory: Colab\r\n\r\n**Describe the current behavior**\r\n\r\nError in Cell 27 (Training): \r\n\r\n`InvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/`\r\n\r\n**Describe the expected behavior**\r\n\r\nIf `tokenizer.word_index[tokenizer.oov_token] = top_k + 1` in Cell 10 is commented out, the notebook runs end-to-end on Colab. \r\n\r\n**Code to reproduce the issue**\r\n\r\nThe reason this happens could be because in Cells 9 and 10, Tokenizer is defined with `oov_token=\"<unk>\"` in Cell 9, which assigns \"\\<unk\\> : 1\" in the token dictionary, and then `tokenizer.word_index[tokenizer.oov_token] = top_k + 1` in Cell 10 assigns \"\\<unk\\>: top_k + 1\" which makes the number \\<unk\\> was previously assigned to non-existent in the dictionary and causes trouble during lookup. \r\n\r\nThe following demonstration explains why we can get an error when we refer to the dictionary later on. \r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nimport re\r\nimport numpy as np\r\n\r\nsent = \"<start> Alice in wonderland. <end>\"\r\nsent2 = \"<start> When suddenly Alice saw a White Rabbit. <end>\"\r\nx_train = [sent, sent2]\r\n\r\ntop_k = 5\r\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \r\n                                                  oov_token=\"<unk>\", \r\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\r\ntokenizer.fit_on_texts(x_train)\r\ntrain_seqs = tokenizer.texts_to_sequences(x_train)\r\ntokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value <= top_k}\r\nprint(tokenizer.word_index)\r\nidx = tokenizer.word_index['<unk>'] #saving the original index of '<unk>'\r\ntokenizer.word_index[tokenizer.oov_token] = top_k + 1 # the problematic line\r\nprint(tokenizer.word_index)\r\nindex_word = {value:key for key, value in tokenizer.word_index.items()}\r\nindex_word[idx]\r\n```\r\n\r\n`Output:`\r\n```\r\n{'<unk>': 1, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}\r\n{'<unk>': 6, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-11-55bbbd305835> in <module>()\r\n     11 print(tokenizer.word_index)\r\n     12 index_word = {value:key for key, value in tokenizer.word_index.items()}\r\n---> 13 index_word[idx]\r\n\r\nKeyError: 1\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-58-03bc9960ded7> in <module>()\r\n     19             for i in range(1, target.shape[1]):\r\n     20                 # passing the features through the decoder\r\n---> 21                 predictions, hidden, _ = decoder(dec_input, features, hidden)\r\n     22 \r\n     23                 loss += loss_function(target[:, i], predictions)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    755       if not in_deferred_mode:\r\n    756         self._in_call = True\r\n--> 757         outputs = self.call(inputs, *args, **kwargs)\r\n    758         self._in_call = False\r\n    759         if outputs is None:\r\n\r\n<ipython-input-54-b844d20e3fc2> in call(self, x, features, hidden)\r\n     16 \r\n     17     # x shape after passing through embedding == (batch_size, 1, embedding_dim)\r\n---> 18     x = self.embedding(x)\r\n     19 \r\n     20     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    755       if not in_deferred_mode:\r\n    756         self._in_call = True\r\n--> 757         outputs = self.call(inputs, *args, **kwargs)\r\n    758         self._in_call = False\r\n    759         if outputs is None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py in call(self, inputs)\r\n    175     if dtype != 'int32' and dtype != 'int64':\r\n    176       inputs = math_ops.cast(inputs, 'int32')\r\n--> 177     out = embedding_ops.embedding_lookup(self.embeddings, inputs)\r\n    178     return out\r\n    179 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in embedding_lookup(params, ids, partition_strategy, name, validate_indices, max_norm)\r\n    311       name=name,\r\n    312       max_norm=max_norm,\r\n--> 313       transform_fn=None)\r\n    314 \r\n    315 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in _embedding_lookup_and_transform(params, ids, partition_strategy, name, max_norm, transform_fn)\r\n    131     if np == 1 and (not transform_fn or ids.get_shape().ndims == 1):\r\n    132       with ops.colocate_with(params[0]):\r\n--> 133         result = _clip(array_ops.gather(params[0], ids, name=name),\r\n    134                        ids, max_norm)\r\n    135         if transform_fn:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)\r\n   2671     # TODO(apassos) find a less bad way of detecting resource variables without\r\n   2672     # introducing a circular dependency.\r\n-> 2673     return params.sparse_read(indices, name=name)\r\n   2674   except AttributeError:\r\n   2675     return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in sparse_read(self, indices, name)\r\n    756         tape.variable_accessed(self)\r\n    757       value = gen_resource_variable_ops.resource_gather(\r\n--> 758           self._handle, indices, dtype=self._dtype, name=name)\r\n    759     return array_ops.identity(value)\r\n    760 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py in resource_gather(resource, indices, dtype, validate_indices, name)\r\n    611       else:\r\n    612         message = e.message\r\n--> 613       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n    614 \r\n    615 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/\r\n```"}