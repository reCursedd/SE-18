{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14188", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14188/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14188/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14188/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14188", "id": 270682023, "node_id": "MDU6SXNzdWUyNzA2ODIwMjM=", "number": 14188, "title": "Retval[0] does not have value in a multithreaded context with FIFOQueue and  tf.scan(...)", "user": {"login": "UlrichBernert68", "id": 33319763, "node_id": "MDQ6VXNlcjMzMzE5NzYz", "avatar_url": "https://avatars1.githubusercontent.com/u/33319763?v=4", "gravatar_id": "", "url": "https://api.github.com/users/UlrichBernert68", "html_url": "https://github.com/UlrichBernert68", "followers_url": "https://api.github.com/users/UlrichBernert68/followers", "following_url": "https://api.github.com/users/UlrichBernert68/following{/other_user}", "gists_url": "https://api.github.com/users/UlrichBernert68/gists{/gist_id}", "starred_url": "https://api.github.com/users/UlrichBernert68/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/UlrichBernert68/subscriptions", "organizations_url": "https://api.github.com/users/UlrichBernert68/orgs", "repos_url": "https://api.github.com/users/UlrichBernert68/repos", "events_url": "https://api.github.com/users/UlrichBernert68/events{/privacy}", "received_events_url": "https://api.github.com/users/UlrichBernert68/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-11-02T14:50:41Z", "updated_at": "2018-02-09T19:33:43Z", "closed_at": "2018-02-09T19:33:43Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: I provide a working script showing the buggy behaviour</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.0</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: none</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 280M 1GB</li>\n</ul>\n<p>Note: I reproduced the buggy behaviour on different platforms too: macOS High Sierra, openSUSE 42.3 and with Tensorflow 1.3</p>\n<h3>problem context</h3>\n<p>I found the buggy behaviour in one of the helper methods the queue loader of the DeepSpeech project (ctc_label_dense_to_sparse). That implementation is partially clumsy in particular when using tf.scan where a scan history context is not used. Nevertheless the algorithm should have worked under any circumstances. I could reduce the problem scenario to a small standalone example. This example uses a number of threads for filling a queue from which batches are requested by dequeu_up_to (the same buggy behaviour if replaced by dequeue_many). Afterwards the batch is postproccessed. For this postprocessing I provided the buggy version (function_buggy, using tf.scan) and an own implementation (function_ok). Both versions produce the same output data in cases there the buggy versions does not fail.</p>\n<h3>buggy behaviour description</h3>\n<p>Very often, but not always a test with a larger batch_size and smaller thread_count leads to the following output: tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value. I include the complete log in the next section. With a larger thread_count and smaller batch_size it works. If the switch-Parameter --use_buggy_version is set to 0 (False) no choice of batch_size and thread_count produces a bug. This proves that the problem is located in the buggy function and the rest of the workflow is OK. I could prove that an implementation without the tf.scan works fine too. In experiments using the context in tf.scan instead of ignoring it didn't change the behaviour. Even as tf.scan doesn't make much sense in the context it was used for in DeepSpeech it  should not have failed and its correct function is essential for many tensorflow based projects. And maybe a similar flaw is hidden in the other \"Higher Order operators\" too: tf.map_fn, tf.foldl, tf.foldr</p>\n<h3>log</h3>\n<p>usage: scan_bug_demo.py [-h] [--batch_size BATCH_SIZE]<br>\n[--thread_count THREAD_COUNT]<br>\n[--use_buggy_version USE_BUGGY_VERSION]<br>\nbatch_size=15<br>\nthread_count=1<br>\nuse_buggy_version=True<br>\nusing buggy implementation: True<br>\nTraceback (most recent call last):<br>\nFile \"scan_bug_demo.py\", line 127, in <br>\nretval = do_it(batch_size, thread_count, use_buggy_version, rnd_seed)<br>\nFile \"scan_bug_demo.py\", line 99, in do_it<br>\ncoord.join(queue_threads)<br>\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join<br>\nsix.reraise(*self._exc_info_to_raise)<br>\nFile \"scan_bug_demo.py\", line 92, in do_it<br>\nres = sess.run(batch)<br>\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run<br>\nrun_metadata_ptr)<br>\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run<br>\nfeed_dict_tensor, options, run_metadata)<br>\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run<br>\noptions, run_metadata)<br>\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value</p>\n<h3>bug demonstration python source</h3>\n<p>import tensorflow as tf<br>\nfrom random import Random<br>\nfrom threading import Thread<br>\nimport numpy as np<br>\nimport sys<br>\nfrom argparse import ArgumentParser</p>\n<p>def function_buggy(value_sequence_lengths):<br>\nmax_len = tf.reduce_max(value_sequence_lengths)</p>\n<pre><code>max_value_seuence_lenght_tns = tf.expand_dims(max_len, 0)\ninit = tf.expand_dims(tf.cast(tf.fill(max_value_seuence_lenght_tns, 0), tf.bool), 0)\n\ndef scan_function(previous_state, current_input):\n    return tf.expand_dims(tf.range(max_len), 0) &lt; current_input\n\nretval = tf.squeeze(tf.scan(scan_function, value_sequence_lengths, initializer=init, parallel_iterations=1),\n                    axis=[1])\n\nreturn retval\n</code></pre>\n<p>def function_ok(value_sequence_lengths):<br>\nmax_len = tf.reduce_max(value_sequence_lengths)<br>\nvalue_sequence_lengths_shape = tf.shape(value_sequence_lengths)</p>\n<pre><code>x_repeated_lenghts = tf.tile(tf.expand_dims(value_sequence_lengths, 1), (1, max_len))\ny_repeated_x_indices = tf.tile(tf.expand_dims(tf.range(max_len), 0), (value_sequence_lengths_shape[0], 1))\n\nretval = y_repeated_x_indices &lt; x_repeated_lenghts\n\nreturn retval\n</code></pre>\n<p>class BatchProvider(object):<br>\ndef <strong>init</strong>(self, batch_size, function, thread_count, rnd_seed):<br>\nself._coord = None<br>\nself.batch_size = batch_size<br>\nself._random = Random(rnd_seed)<br>\nself._capacity = 2 * batch_size<br>\nself._thread_count = thread_count<br>\nself._queue = tf.FIFOQueue(shapes=[[]], dtypes=[tf.int32], capacity=self._capacity)<br>\nself._y_length = tf.placeholder(tf.int32, [])<br>\nself._enqueue_op = self._queue.enqueue(self._y_length)<br>\nself._close_op = self._queue.close(cancel_pending_enqueues=True)<br>\nself._function = function</p>\n<pre><code>def start_queue_threads(self, session, coord):\n    self._coord = coord\n    batch_threads = [Thread(target=self._populate_batch_queue, args=(session,)) for i in range(self._thread_count)]\n    for batch_thread in batch_threads:\n        self._coord.register_thread(batch_thread)\n        batch_thread.daemon = True\n        batch_thread.start()\n    return batch_threads\n\ndef close_queue(self, session):\n    session.run(self._close_op)\n\ndef _populate_batch_queue(self, session):\n    while True:\n        length = self._random.randint(5, 10)\n        target_len = length\n        try:\n            self._enqueue_op.run(session=session, feed_dict={self._y_length: target_len})\n        except tf.errors.CancelledError:\n            return\n\ndef next_batch(self):\n    target_lengths = self._queue.dequeue_up_to(self.batch_size)\n    retval = self._function(target_lengths)\n    return retval\n</code></pre>\n<p>def do_it(batch_size, thread_count, use_buggy_version, rnd_seed):<br>\nfunction = function_buggy if use_buggy_version else function_ok</p>\n<pre><code>batch_provider = BatchProvider(batch_size=batch_size, function=function,\n                               thread_count=thread_count, rnd_seed=rnd_seed)\n\nsess = tf.Session()\n\ncoord = tf.train.Coordinator()\nqueue_threads = batch_provider.start_queue_threads(sess, coord)\nbatch = batch_provider.next_batch()\n\ntry:\n    for _ in range(1):\n        if coord.should_stop():\n            break\n        res = sess.run(batch)\n        print batch\n        print res\nexcept Exception, ex:\n    coord.request_stop(ex)\nfinally:\n    batch_provider.close_queue(sess)\n    coord.join(queue_threads)\n    sess.close()\n\nreturn res\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\nargs = sys.argv[1:]<br>\nargparser = ArgumentParser()<br>\nargparser.add_argument(\"--batch_size\", dest=\"batch_size\", type=int, default=15)<br>\nargparser.add_argument(\"--thread_count\", dest=\"thread_count\", type=int, default=1)<br>\nargparser.add_argument(\"--use_buggy_version\", dest=\"use_buggy_version\", type=int, default=True)<br>\nif len(args) == 0:<br>\nargparser.print_usage()<br>\nparsed = argparser.parse_args(args=args)<br>\nbatch_size = parsed.batch_size<br>\nthread_count = parsed.thread_count<br>\nuse_buggy_version = parsed.use_buggy_version</p>\n<pre><code>print \"batch_size=\" + str(batch_size)\nprint \"thread_count=\" + str(thread_count)\nprint \"use_buggy_version=\" + str(use_buggy_version)\n\nrnd_seed = Random().random()\n\nprint \"using buggy implementation: \" + str(use_buggy_version)\nretval = do_it(batch_size, thread_count, use_buggy_version, rnd_seed)\nprint \"success\"\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): I provide a working script showing the buggy behaviour\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4.0\nPython version: 2.7.12\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: none\nGPU model and memory: GeForce GTX 280M 1GB\n\nNote: I reproduced the buggy behaviour on different platforms too: macOS High Sierra, openSUSE 42.3 and with Tensorflow 1.3\nproblem context\nI found the buggy behaviour in one of the helper methods the queue loader of the DeepSpeech project (ctc_label_dense_to_sparse). That implementation is partially clumsy in particular when using tf.scan where a scan history context is not used. Nevertheless the algorithm should have worked under any circumstances. I could reduce the problem scenario to a small standalone example. This example uses a number of threads for filling a queue from which batches are requested by dequeu_up_to (the same buggy behaviour if replaced by dequeue_many). Afterwards the batch is postproccessed. For this postprocessing I provided the buggy version (function_buggy, using tf.scan) and an own implementation (function_ok). Both versions produce the same output data in cases there the buggy versions does not fail.\nbuggy behaviour description\nVery often, but not always a test with a larger batch_size and smaller thread_count leads to the following output: tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value. I include the complete log in the next section. With a larger thread_count and smaller batch_size it works. If the switch-Parameter --use_buggy_version is set to 0 (False) no choice of batch_size and thread_count produces a bug. This proves that the problem is located in the buggy function and the rest of the workflow is OK. I could prove that an implementation without the tf.scan works fine too. In experiments using the context in tf.scan instead of ignoring it didn't change the behaviour. Even as tf.scan doesn't make much sense in the context it was used for in DeepSpeech it  should not have failed and its correct function is essential for many tensorflow based projects. And maybe a similar flaw is hidden in the other \"Higher Order operators\" too: tf.map_fn, tf.foldl, tf.foldr\nlog\nusage: scan_bug_demo.py [-h] [--batch_size BATCH_SIZE]\n[--thread_count THREAD_COUNT]\n[--use_buggy_version USE_BUGGY_VERSION]\nbatch_size=15\nthread_count=1\nuse_buggy_version=True\nusing buggy implementation: True\nTraceback (most recent call last):\nFile \"scan_bug_demo.py\", line 127, in \nretval = do_it(batch_size, thread_count, use_buggy_version, rnd_seed)\nFile \"scan_bug_demo.py\", line 99, in do_it\ncoord.join(queue_threads)\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\nsix.reraise(*self._exc_info_to_raise)\nFile \"scan_bug_demo.py\", line 92, in do_it\nres = sess.run(batch)\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\nrun_metadata_ptr)\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\nfeed_dict_tensor, options, run_metadata)\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\noptions, run_metadata)\nFile \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\nbug demonstration python source\nimport tensorflow as tf\nfrom random import Random\nfrom threading import Thread\nimport numpy as np\nimport sys\nfrom argparse import ArgumentParser\ndef function_buggy(value_sequence_lengths):\nmax_len = tf.reduce_max(value_sequence_lengths)\nmax_value_seuence_lenght_tns = tf.expand_dims(max_len, 0)\ninit = tf.expand_dims(tf.cast(tf.fill(max_value_seuence_lenght_tns, 0), tf.bool), 0)\n\ndef scan_function(previous_state, current_input):\n    return tf.expand_dims(tf.range(max_len), 0) < current_input\n\nretval = tf.squeeze(tf.scan(scan_function, value_sequence_lengths, initializer=init, parallel_iterations=1),\n                    axis=[1])\n\nreturn retval\n\ndef function_ok(value_sequence_lengths):\nmax_len = tf.reduce_max(value_sequence_lengths)\nvalue_sequence_lengths_shape = tf.shape(value_sequence_lengths)\nx_repeated_lenghts = tf.tile(tf.expand_dims(value_sequence_lengths, 1), (1, max_len))\ny_repeated_x_indices = tf.tile(tf.expand_dims(tf.range(max_len), 0), (value_sequence_lengths_shape[0], 1))\n\nretval = y_repeated_x_indices < x_repeated_lenghts\n\nreturn retval\n\nclass BatchProvider(object):\ndef init(self, batch_size, function, thread_count, rnd_seed):\nself._coord = None\nself.batch_size = batch_size\nself._random = Random(rnd_seed)\nself._capacity = 2 * batch_size\nself._thread_count = thread_count\nself._queue = tf.FIFOQueue(shapes=[[]], dtypes=[tf.int32], capacity=self._capacity)\nself._y_length = tf.placeholder(tf.int32, [])\nself._enqueue_op = self._queue.enqueue(self._y_length)\nself._close_op = self._queue.close(cancel_pending_enqueues=True)\nself._function = function\ndef start_queue_threads(self, session, coord):\n    self._coord = coord\n    batch_threads = [Thread(target=self._populate_batch_queue, args=(session,)) for i in range(self._thread_count)]\n    for batch_thread in batch_threads:\n        self._coord.register_thread(batch_thread)\n        batch_thread.daemon = True\n        batch_thread.start()\n    return batch_threads\n\ndef close_queue(self, session):\n    session.run(self._close_op)\n\ndef _populate_batch_queue(self, session):\n    while True:\n        length = self._random.randint(5, 10)\n        target_len = length\n        try:\n            self._enqueue_op.run(session=session, feed_dict={self._y_length: target_len})\n        except tf.errors.CancelledError:\n            return\n\ndef next_batch(self):\n    target_lengths = self._queue.dequeue_up_to(self.batch_size)\n    retval = self._function(target_lengths)\n    return retval\n\ndef do_it(batch_size, thread_count, use_buggy_version, rnd_seed):\nfunction = function_buggy if use_buggy_version else function_ok\nbatch_provider = BatchProvider(batch_size=batch_size, function=function,\n                               thread_count=thread_count, rnd_seed=rnd_seed)\n\nsess = tf.Session()\n\ncoord = tf.train.Coordinator()\nqueue_threads = batch_provider.start_queue_threads(sess, coord)\nbatch = batch_provider.next_batch()\n\ntry:\n    for _ in range(1):\n        if coord.should_stop():\n            break\n        res = sess.run(batch)\n        print batch\n        print res\nexcept Exception, ex:\n    coord.request_stop(ex)\nfinally:\n    batch_provider.close_queue(sess)\n    coord.join(queue_threads)\n    sess.close()\n\nreturn res\n\nif name == 'main':\nargs = sys.argv[1:]\nargparser = ArgumentParser()\nargparser.add_argument(\"--batch_size\", dest=\"batch_size\", type=int, default=15)\nargparser.add_argument(\"--thread_count\", dest=\"thread_count\", type=int, default=1)\nargparser.add_argument(\"--use_buggy_version\", dest=\"use_buggy_version\", type=int, default=True)\nif len(args) == 0:\nargparser.print_usage()\nparsed = argparser.parse_args(args=args)\nbatch_size = parsed.batch_size\nthread_count = parsed.thread_count\nuse_buggy_version = parsed.use_buggy_version\nprint \"batch_size=\" + str(batch_size)\nprint \"thread_count=\" + str(thread_count)\nprint \"use_buggy_version=\" + str(use_buggy_version)\n\nrnd_seed = Random().random()\n\nprint \"using buggy implementation: \" + str(use_buggy_version)\nretval = do_it(batch_size, thread_count, use_buggy_version, rnd_seed)\nprint \"success\"", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I provide a working script showing the buggy behaviour\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: GeForce GTX 280M 1GB\r\n\r\nNote: I reproduced the buggy behaviour on different platforms too: macOS High Sierra, openSUSE 42.3 and with Tensorflow 1.3\r\n\r\n###  problem context\r\nI found the buggy behaviour in one of the helper methods the queue loader of the DeepSpeech project (ctc_label_dense_to_sparse). That implementation is partially clumsy in particular when using tf.scan where a scan history context is not used. Nevertheless the algorithm should have worked under any circumstances. I could reduce the problem scenario to a small standalone example. This example uses a number of threads for filling a queue from which batches are requested by dequeu_up_to (the same buggy behaviour if replaced by dequeue_many). Afterwards the batch is postproccessed. For this postprocessing I provided the buggy version (function_buggy, using tf.scan) and an own implementation (function_ok). Both versions produce the same output data in cases there the buggy versions does not fail. \r\n\r\n### buggy behaviour description\r\nVery often, but not always a test with a larger batch_size and smaller thread_count leads to the following output: tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value. I include the complete log in the next section. With a larger thread_count and smaller batch_size it works. If the switch-Parameter --use_buggy_version is set to 0 (False) no choice of batch_size and thread_count produces a bug. This proves that the problem is located in the buggy function and the rest of the workflow is OK. I could prove that an implementation without the tf.scan works fine too. In experiments using the context in tf.scan instead of ignoring it didn't change the behaviour. Even as tf.scan doesn't make much sense in the context it was used for in DeepSpeech it  should not have failed and its correct function is essential for many tensorflow based projects. And maybe a similar flaw is hidden in the other \"Higher Order operators\" too: tf.map_fn, tf.foldl, tf.foldr\r\n\r\n### log\r\n\r\nusage: scan_bug_demo.py [-h] [--batch_size BATCH_SIZE]\r\n                        [--thread_count THREAD_COUNT]\r\n                        [--use_buggy_version USE_BUGGY_VERSION]\r\nbatch_size=15\r\nthread_count=1\r\nuse_buggy_version=True\r\nusing buggy implementation: True\r\nTraceback (most recent call last):\r\n  File \"scan_bug_demo.py\", line 127, in <module>\r\n    retval = do_it(batch_size, thread_count, use_buggy_version, rnd_seed)\r\n  File \"scan_bug_demo.py\", line 99, in do_it\r\n    coord.join(queue_threads)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"scan_bug_demo.py\", line 92, in do_it\r\n    res = sess.run(batch)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n\r\n### bug demonstration python source\r\n\r\nimport tensorflow as tf\r\nfrom random import Random\r\nfrom threading import Thread\r\nimport numpy as np\r\nimport sys\r\nfrom argparse import ArgumentParser\r\n\r\n\r\ndef function_buggy(value_sequence_lengths):\r\n    max_len = tf.reduce_max(value_sequence_lengths)\r\n\r\n    max_value_seuence_lenght_tns = tf.expand_dims(max_len, 0)\r\n    init = tf.expand_dims(tf.cast(tf.fill(max_value_seuence_lenght_tns, 0), tf.bool), 0)\r\n\r\n    def scan_function(previous_state, current_input):\r\n        return tf.expand_dims(tf.range(max_len), 0) < current_input\r\n\r\n    retval = tf.squeeze(tf.scan(scan_function, value_sequence_lengths, initializer=init, parallel_iterations=1),\r\n                        axis=[1])\r\n\r\n    return retval\r\n\r\n\r\ndef function_ok(value_sequence_lengths):\r\n    max_len = tf.reduce_max(value_sequence_lengths)\r\n    value_sequence_lengths_shape = tf.shape(value_sequence_lengths)\r\n\r\n    x_repeated_lenghts = tf.tile(tf.expand_dims(value_sequence_lengths, 1), (1, max_len))\r\n    y_repeated_x_indices = tf.tile(tf.expand_dims(tf.range(max_len), 0), (value_sequence_lengths_shape[0], 1))\r\n\r\n    retval = y_repeated_x_indices < x_repeated_lenghts\r\n\r\n    return retval\r\n\r\n\r\nclass BatchProvider(object):\r\n    def __init__(self, batch_size, function, thread_count, rnd_seed):\r\n        self._coord = None\r\n        self.batch_size = batch_size\r\n        self._random = Random(rnd_seed)\r\n        self._capacity = 2 * batch_size\r\n        self._thread_count = thread_count\r\n        self._queue = tf.FIFOQueue(shapes=[[]], dtypes=[tf.int32], capacity=self._capacity)\r\n        self._y_length = tf.placeholder(tf.int32, [])\r\n        self._enqueue_op = self._queue.enqueue(self._y_length)\r\n        self._close_op = self._queue.close(cancel_pending_enqueues=True)\r\n        self._function = function\r\n\r\n    def start_queue_threads(self, session, coord):\r\n        self._coord = coord\r\n        batch_threads = [Thread(target=self._populate_batch_queue, args=(session,)) for i in range(self._thread_count)]\r\n        for batch_thread in batch_threads:\r\n            self._coord.register_thread(batch_thread)\r\n            batch_thread.daemon = True\r\n            batch_thread.start()\r\n        return batch_threads\r\n\r\n    def close_queue(self, session):\r\n        session.run(self._close_op)\r\n\r\n    def _populate_batch_queue(self, session):\r\n        while True:\r\n            length = self._random.randint(5, 10)\r\n            target_len = length\r\n            try:\r\n                self._enqueue_op.run(session=session, feed_dict={self._y_length: target_len})\r\n            except tf.errors.CancelledError:\r\n                return\r\n\r\n    def next_batch(self):\r\n        target_lengths = self._queue.dequeue_up_to(self.batch_size)\r\n        retval = self._function(target_lengths)\r\n        return retval\r\n\r\n\r\ndef do_it(batch_size, thread_count, use_buggy_version, rnd_seed):\r\n    function = function_buggy if use_buggy_version else function_ok\r\n\r\n    batch_provider = BatchProvider(batch_size=batch_size, function=function,\r\n                                   thread_count=thread_count, rnd_seed=rnd_seed)\r\n\r\n    sess = tf.Session()\r\n\r\n    coord = tf.train.Coordinator()\r\n    queue_threads = batch_provider.start_queue_threads(sess, coord)\r\n    batch = batch_provider.next_batch()\r\n\r\n    try:\r\n        for _ in range(1):\r\n            if coord.should_stop():\r\n                break\r\n            res = sess.run(batch)\r\n            print batch\r\n            print res\r\n    except Exception, ex:\r\n        coord.request_stop(ex)\r\n    finally:\r\n        batch_provider.close_queue(sess)\r\n        coord.join(queue_threads)\r\n        sess.close()\r\n\r\n    return res\r\n\r\n\r\nif __name__ == '__main__':\r\n    args = sys.argv[1:]\r\n    argparser = ArgumentParser()\r\n    argparser.add_argument(\"--batch_size\", dest=\"batch_size\", type=int, default=15)\r\n    argparser.add_argument(\"--thread_count\", dest=\"thread_count\", type=int, default=1)\r\n    argparser.add_argument(\"--use_buggy_version\", dest=\"use_buggy_version\", type=int, default=True)\r\n    if len(args) == 0:\r\n        argparser.print_usage()\r\n    parsed = argparser.parse_args(args=args)\r\n    batch_size = parsed.batch_size\r\n    thread_count = parsed.thread_count\r\n    use_buggy_version = parsed.use_buggy_version\r\n\r\n    print \"batch_size=\" + str(batch_size)\r\n    print \"thread_count=\" + str(thread_count)\r\n    print \"use_buggy_version=\" + str(use_buggy_version)\r\n\r\n    rnd_seed = Random().random()\r\n\r\n    print \"using buggy implementation: \" + str(use_buggy_version)\r\n    retval = do_it(batch_size, thread_count, use_buggy_version, rnd_seed)\r\n    print \"success\"\r\n"}