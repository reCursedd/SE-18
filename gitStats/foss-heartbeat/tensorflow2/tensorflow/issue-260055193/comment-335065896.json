{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335065896", "html_url": "https://github.com/tensorflow/tensorflow/pull/13268#issuecomment-335065896", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13268", "id": 335065896, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTA2NTg5Ng==", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-09T04:42:19Z", "updated_at": "2017-10-09T05:16:44Z", "author_association": "MEMBER", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> .  I am a little confusing with xxxGradGrad functions.</p>\n<p>Say, a simple network as below:<br>\nL = h(y),<br>\ny = func(x),</p>\n<h3>first-order gradient</h3>\n<p>As chain ruler, It is easy to get first-order gradient:<br>\ndL / dx = (dy / dx) * (dL / dy)</p>\n<p>And the implementation is straight as well:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">xxxGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n      x <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">0</span>]\n      y <span class=\"pl-k\">=</span> op.outputs[<span class=\"pl-c1\">0</span>]\n      dy_dx <span class=\"pl-k\">=</span>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> math calculation</span>\n      <span class=\"pl-k\">return</span> dy_dx <span class=\"pl-k\">*</span> grad</pre></div>\n<p>Here <code>grad</code> is <code>dL / dy</code>.</p>\n<h3>second-order gradient</h3>\n<p>However, for second-order gradient,  it seems that <code>xxxGradGrad</code> has two inputs.</p>\n<p>Take <code>_SoftplusGradGrad</code> for example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_SoftplusGradGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Let:</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>   y = tf.nn.softplus(x)</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>   dx = gen_nn_ops._softplus_grad(dy, x) = dy / (1 + exp(-x))</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> This op computes (ddy, d2x) from op.inputs == [dy, x] and grad == ddx.</span>\n  dy, x <span class=\"pl-k\">=</span> op.inputs\n  <span class=\"pl-k\">with</span> ops.control_dependencies([grad.op]):\n    ddy <span class=\"pl-k\">=</span> gen_nn_ops._softplus_grad(grad, x)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> pylint: disable=protected-access</span>\n    d2x <span class=\"pl-k\">=</span> grad <span class=\"pl-k\">*</span> dy <span class=\"pl-k\">/</span> (math_ops.exp(<span class=\"pl-k\">-</span>x) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2.0</span> <span class=\"pl-k\">+</span> math_ops.exp(x))\n<span class=\"pl-k\">return</span> (ddy, d2x)</pre></div>\n<p><a href=\"https://github.com/facaiy/tensorflow/blob/ed66e5a9dee770492bfdfe37d1a01c87d3bca3de/tensorflow/python/ops/nn_grad.py#L377\">https://github.com/facaiy/tensorflow/blob/ed66e5a9dee770492bfdfe37d1a01c87d3bca3de/tensorflow/python/ops/nn_grad.py#L377</a></p>\n<p>I don't know how to calculate 2nd gradient with backpropagation, and could you explain what are <code>dy</code>, <code>grad</code>, <code>ddy</code> and <code>d2x</code> here? Thank you very much.</p>", "body_text": "Hi, @alextp .  I am a little confusing with xxxGradGrad functions.\nSay, a simple network as below:\nL = h(y),\ny = func(x),\nfirst-order gradient\nAs chain ruler, It is easy to get first-order gradient:\ndL / dx = (dy / dx) * (dL / dy)\nAnd the implementation is straight as well:\ndef xxxGrad(op, grad):\n      x = op.inputs[0]\n      y = op.outputs[0]\n      dy_dx =  # math calculation\n      return dy_dx * grad\nHere grad is dL / dy.\nsecond-order gradient\nHowever, for second-order gradient,  it seems that xxxGradGrad has two inputs.\nTake _SoftplusGradGrad for example:\ndef _SoftplusGradGrad(op, grad):\n  # Let:\n  #   y = tf.nn.softplus(x)\n  #   dx = gen_nn_ops._softplus_grad(dy, x) = dy / (1 + exp(-x))\n  # This op computes (ddy, d2x) from op.inputs == [dy, x] and grad == ddx.\n  dy, x = op.inputs\n  with ops.control_dependencies([grad.op]):\n    ddy = gen_nn_ops._softplus_grad(grad, x)  # pylint: disable=protected-access\n    d2x = grad * dy / (math_ops.exp(-x) + 2.0 + math_ops.exp(x))\nreturn (ddy, d2x)\nhttps://github.com/facaiy/tensorflow/blob/ed66e5a9dee770492bfdfe37d1a01c87d3bca3de/tensorflow/python/ops/nn_grad.py#L377\nI don't know how to calculate 2nd gradient with backpropagation, and could you explain what are dy, grad, ddy and d2x here? Thank you very much.", "body": "Hi, @alextp .  I am a little confusing with xxxGradGrad functions.\r\n\r\nSay, a simple network as below:\r\nL = h(y), \r\ny = func(x), \r\n\r\n### first-order gradient\r\n\r\nAs chain ruler, It is easy to get first-order gradient:\r\ndL / dx = (dy / dx) * (dL / dy) \r\n\r\nAnd the implementation is straight as well:\r\n```python\r\ndef xxxGrad(op, grad):\r\n      x = op.inputs[0]\r\n      y = op.outputs[0]\r\n      dy_dx =  # math calculation\r\n      return dy_dx * grad\r\n```\r\nHere `grad` is `dL / dy`.\r\n\r\n### second-order gradient\r\n\r\nHowever, for second-order gradient,  it seems that `xxxGradGrad` has two inputs.\r\n\r\nTake `_SoftplusGradGrad` for example:\r\n\r\n```python\r\ndef _SoftplusGradGrad(op, grad):\r\n  # Let:\r\n  #   y = tf.nn.softplus(x)\r\n  #   dx = gen_nn_ops._softplus_grad(dy, x) = dy / (1 + exp(-x))\r\n  # This op computes (ddy, d2x) from op.inputs == [dy, x] and grad == ddx.\r\n  dy, x = op.inputs\r\n  with ops.control_dependencies([grad.op]):\r\n    ddy = gen_nn_ops._softplus_grad(grad, x)  # pylint: disable=protected-access\r\n    d2x = grad * dy / (math_ops.exp(-x) + 2.0 + math_ops.exp(x))\r\nreturn (ddy, d2x)\r\n```\r\nhttps://github.com/facaiy/tensorflow/blob/ed66e5a9dee770492bfdfe37d1a01c87d3bca3de/tensorflow/python/ops/nn_grad.py#L377\r\n\r\nI don't know how to calculate 2nd gradient with backpropagation, and could you explain what are `dy`, `grad`, `ddy` and `d2x` here? Thank you very much."}