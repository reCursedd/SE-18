{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335085280", "html_url": "https://github.com/tensorflow/tensorflow/pull/13268#issuecomment-335085280", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13268", "id": 335085280, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTA4NTI4MA==", "user": {"login": "facaiy", "id": 1112263, "node_id": "MDQ6VXNlcjExMTIyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1112263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/facaiy", "html_url": "https://github.com/facaiy", "followers_url": "https://api.github.com/users/facaiy/followers", "following_url": "https://api.github.com/users/facaiy/following{/other_user}", "gists_url": "https://api.github.com/users/facaiy/gists{/gist_id}", "starred_url": "https://api.github.com/users/facaiy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/facaiy/subscriptions", "organizations_url": "https://api.github.com/users/facaiy/orgs", "repos_url": "https://api.github.com/users/facaiy/repos", "events_url": "https://api.github.com/users/facaiy/events{/privacy}", "received_events_url": "https://api.github.com/users/facaiy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-09T07:43:45Z", "updated_at": "2017-10-09T07:45:19Z", "author_association": "MEMBER", "body_html": "<p>Oh, perhaps I understand it.</p>\n<p>For <code>_SoftplusGradGrad</code>,<br>\nLet:<br>\ny = tf.nn.softplus(x)<br>\ndx = gen_nn_ops._softplus_grad(dy, x)</p>\n<p>here <code>dy</code> is short for <code>dL/dy</code>, <code>dx</code> is short for <code>dL/dx</code>. Since <code>SoftplusGrad</code> is a function with <code>dy</code> and <code>x</code>, so <code>SoftplusGradGrad</code> need to calculate the gradient for <code>dy</code> and <code>x</code> respectively:<br>\n<code>ddy</code> is short for <code>d(dx) / dy</code>,<br>\n<code>d2x</code> is for <code>d(dx) / dx</code>.</p>\n<p>right?</p>", "body_text": "Oh, perhaps I understand it.\nFor _SoftplusGradGrad,\nLet:\ny = tf.nn.softplus(x)\ndx = gen_nn_ops._softplus_grad(dy, x)\nhere dy is short for dL/dy, dx is short for dL/dx. Since SoftplusGrad is a function with dy and x, so SoftplusGradGrad need to calculate the gradient for dy and x respectively:\nddy is short for d(dx) / dy,\nd2x is for d(dx) / dx.\nright?", "body": "Oh, perhaps I understand it.\r\n\r\nFor `_SoftplusGradGrad`,\r\nLet: \r\ny = tf.nn.softplus(x)\r\ndx = gen_nn_ops._softplus_grad(dy, x)\r\n\r\nhere `dy` is short for `dL/dy`, `dx` is short for `dL/dx`. Since `SoftplusGrad` is a function with `dy` and `x`, so `SoftplusGradGrad` need to calculate the gradient for `dy` and `x` respectively:\r\n`ddy` is short for `d(dx) / dy`,\r\n`d2x` is for `d(dx) / dx`.\r\n\r\nright?\r\n"}