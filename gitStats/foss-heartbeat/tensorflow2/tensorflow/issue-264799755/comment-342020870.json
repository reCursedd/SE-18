{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/342020870", "html_url": "https://github.com/tensorflow/tensorflow/issues/13651#issuecomment-342020870", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13651", "id": 342020870, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjAyMDg3MA==", "user": {"login": "selcouthlyBlue", "id": 13268675, "node_id": "MDQ6VXNlcjEzMjY4Njc1", "avatar_url": "https://avatars2.githubusercontent.com/u/13268675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selcouthlyBlue", "html_url": "https://github.com/selcouthlyBlue", "followers_url": "https://api.github.com/users/selcouthlyBlue/followers", "following_url": "https://api.github.com/users/selcouthlyBlue/following{/other_user}", "gists_url": "https://api.github.com/users/selcouthlyBlue/gists{/gist_id}", "starred_url": "https://api.github.com/users/selcouthlyBlue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selcouthlyBlue/subscriptions", "organizations_url": "https://api.github.com/users/selcouthlyBlue/orgs", "repos_url": "https://api.github.com/users/selcouthlyBlue/repos", "events_url": "https://api.github.com/users/selcouthlyBlue/events{/privacy}", "received_events_url": "https://api.github.com/users/selcouthlyBlue/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-06T00:57:35Z", "updated_at": "2017-11-06T00:58:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I did a little poking with the network parameters and it turns out, the number of hidden units influence the dimensions found in \"In[1]\". With that, here's the code building the bidirectional lstm part of the graph:</p>\n<pre><code>#num_layers = 3,\n#num_hidden = 32\n#num_classes = 80 \n\nlstm_fw_cells = [tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0) for _ in range(num_layers)]\nlstm_bw_cells = [tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0) for _ in range(num_layers)]\n\noutputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(lstm_fw_cells, lstm_bw_cells,\n                                                                   self.inputs, dtype=tf.float32)\n\nbatch_size = tf.shape(self.inputs)[0]\n\noutputs = tf.reshape(outputs, [-1, num_hidden])\n\nW = tf.Variable(tf.truncated_normal([num_hidden, num_classes], stddev=0.1, dtype=tf.float32))\nb = tf.Variable(tf.constant(0., shape=[num_classes], dtype=tf.float32))\n\nlogits = tf.matmul(outputs, W) + b\nlogits = tf.reshape(logits, [batch_size, -1, num_classes])\nlogits = tf.transpose(logits, (1, 0, 2))\n</code></pre>", "body_text": "I did a little poking with the network parameters and it turns out, the number of hidden units influence the dimensions found in \"In[1]\". With that, here's the code building the bidirectional lstm part of the graph:\n#num_layers = 3,\n#num_hidden = 32\n#num_classes = 80 \n\nlstm_fw_cells = [tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0) for _ in range(num_layers)]\nlstm_bw_cells = [tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0) for _ in range(num_layers)]\n\noutputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(lstm_fw_cells, lstm_bw_cells,\n                                                                   self.inputs, dtype=tf.float32)\n\nbatch_size = tf.shape(self.inputs)[0]\n\noutputs = tf.reshape(outputs, [-1, num_hidden])\n\nW = tf.Variable(tf.truncated_normal([num_hidden, num_classes], stddev=0.1, dtype=tf.float32))\nb = tf.Variable(tf.constant(0., shape=[num_classes], dtype=tf.float32))\n\nlogits = tf.matmul(outputs, W) + b\nlogits = tf.reshape(logits, [batch_size, -1, num_classes])\nlogits = tf.transpose(logits, (1, 0, 2))", "body": "I did a little poking with the network parameters and it turns out, the number of hidden units influence the dimensions found in \"In[1]\". With that, here's the code building the bidirectional lstm part of the graph:\r\n\r\n```\r\n#num_layers = 3,\r\n#num_hidden = 32\r\n#num_classes = 80 \r\n\r\nlstm_fw_cells = [tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0) for _ in range(num_layers)]\r\nlstm_bw_cells = [tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0) for _ in range(num_layers)]\r\n\r\noutputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(lstm_fw_cells, lstm_bw_cells,\r\n                                                                   self.inputs, dtype=tf.float32)\r\n\r\nbatch_size = tf.shape(self.inputs)[0]\r\n\r\noutputs = tf.reshape(outputs, [-1, num_hidden])\r\n\r\nW = tf.Variable(tf.truncated_normal([num_hidden, num_classes], stddev=0.1, dtype=tf.float32))\r\nb = tf.Variable(tf.constant(0., shape=[num_classes], dtype=tf.float32))\r\n\r\nlogits = tf.matmul(outputs, W) + b\r\nlogits = tf.reshape(logits, [batch_size, -1, num_classes])\r\nlogits = tf.transpose(logits, (1, 0, 2))\r\n```"}