{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/210955658", "pull_request_review_id": 147281571, "id": 210955658, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMDk1NTY1OA==", "diff_hunk": "@@ -0,0 +1,5651 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_CORE_KERNELS_CROP_RESIZE_BILINEAR_CORE_H_\n+#define TENSORFLOW_CORE_KERNELS_CROP_RESIZE_BILINEAR_CORE_H_\n+\n+#include <emmintrin.h>\n+#include <immintrin.h>\n+#include <smmintrin.h>\n+#include <xmmintrin.h>\n+#include <cassert>\n+#include <cstdio>\n+#include <cstdlib>\n+#include <list>\n+#include <string>\n+\n+namespace tensorflow {\n+namespace {\n+\n+template <typename T>\n+std::string sprint_bits(const T& val, bool msb_left = true) {\n+  std::string str;\n+  if (sizeof(T) == 1) {\n+    unsigned char* p = (unsigned char*)&val;\n+    const int num_bits = sizeof(T) * 8;\n+    for (int i = 0; i < num_bits; ++i) {\n+      int bit_idx = msb_left ? num_bits - i - 1 : i;\n+      str = str + std::string((*p) & (1 << bit_idx) ? \"1\" : \"0\");\n+    }\n+  } else if (sizeof(T) == 2) {\n+    unsigned short* p = (unsigned short*)&val;\n+    const int num_bits = sizeof(T) * 8;\n+    for (int i = 0; i < num_bits; ++i) {\n+      int bit_idx = msb_left ? num_bits - i - 1 : i;\n+      str = str + std::string((*p) & (1 << bit_idx) ? \"1\" : \"0\");\n+    }\n+  } else if (sizeof(T) == 4) {\n+    unsigned int* p = (unsigned int*)&val;\n+    const int num_bits = sizeof(T) * 8;\n+    for (int i = 0; i < num_bits; ++i) {\n+      int bit_idx = msb_left ? num_bits - i - 1 : i;\n+      str = str + std::string((*p) & (1 << bit_idx) ? \"1\" : \"0\");\n+    }\n+  } else if (sizeof(T) == 8) {\n+    unsigned long long* p = (unsigned long long*)&val;\n+    const int num_bits = sizeof(T) * 8;\n+    for (int i = 0; i < num_bits; ++i) {\n+      int bit_idx = msb_left ? num_bits - i - 1 : i;\n+      str = str + std::string((*p) & (1 << bit_idx) ? \"1\" : \"0\");\n+    }\n+  } else {\n+    assert(false);\n+  }\n+  return str;\n+}\n+\n+template <typename T>\n+std::string format(const T& val) {\n+  return sprint_bits(val);\n+}\n+template <>\n+std::string format(const Eigen::half& val) {\n+  std::string str = sprint_bits(val);\n+  return str.substr(0, 1) + \" \" + str.substr(1, 1) + \"...\" + str.substr(2, 4) +\n+         \" \" + str.substr(6, 10);\n+}\n+template <>\n+std::string format(const float& val) {\n+  std::string str = sprint_bits(val);\n+  return str.substr(0, 1) + \" \" + str.substr(1, 8) + \" \" + str.substr(9, 23);\n+}\n+\n+//\n+// Useful debugging class\n+//\n+\n+template <class T>\n+class SSEPrinter {\n+ public:\n+  static void print(__m128i* vals, int n) {\n+    const int els_per_vec = 16 / sizeof(T);\n+    printf(\"%s v[%d] = {\", get_typename(), n * els_per_vec);\n+    for (int i = 0; i < n * els_per_vec; ++i) {\n+      if (i == 0)\n+        printf(\"%.2f\", (float)((T*)vals)[i]);\n+      else\n+        printf(\", %.2f\", (float)((T*)vals)[i]);\n+    }\n+    printf(\"}\\n\");\n+  }\n+\n+  static void print_bits(const char* label, __m128i& val) {\n+    printf(\"%s\\n\", label);\n+    const int els_per_vec = 16 / sizeof(T);\n+    for (int i = 0; i < els_per_vec; ++i) {\n+      T* tp = ((T*)&val) + i;\n+      printf(\"  %-2d = %s\\n\", i, format(*tp).c_str());\n+    }\n+  }\n+\n+ private:\n+  static const char* get_typename();\n+};\n+\n+template <>\n+const char* SSEPrinter<uint8>::get_typename() {\n+  return \"uint8\";\n+}\n+template <>\n+const char* SSEPrinter<int8>::get_typename() {\n+  return \"int8\";\n+}\n+template <>\n+const char* SSEPrinter<uint16>::get_typename() {\n+  return \"uint16\";\n+}\n+template <>\n+const char* SSEPrinter<int16>::get_typename() {\n+  return \"int16\";\n+}\n+template <>\n+const char* SSEPrinter<int32>::get_typename() {\n+  return \"int32\";\n+}\n+template <>\n+const char* SSEPrinter<Eigen::half>::get_typename() {\n+  return \"Eigen::half\";\n+}\n+template <>\n+const char* SSEPrinter<bfloat16>::get_typename() {\n+  return \"bfloat16\";\n+}\n+template <>\n+const char* SSEPrinter<float>::get_typename() {\n+  return \"float\";\n+}\n+\n+// Compute the interpolation indices only once.\n+struct CachedInterpolation {\n+  int lower;  // Lower source index used in the interpolation\n+  int upper;  // Upper source index used in the interpolation\n+  // 1-D linear iterpolation scale (see:\n+  // https://en.wikipedia.org/wiki/Bilinear_interpolation)\n+  float lerp;\n+};\n+\n+bool compute_single_interpolation_weight(const int in_size,\n+                                         const float out2in_scale,\n+                                         const float out2in_start,\n+                                         const bool clip, const int i,\n+                                         int* lower, int* upper, float* lerp) {\n+  const float in = i * out2in_scale + out2in_start;\n+  *lower = (int)floor(in);\n+  *upper = (int)ceil(in);\n+  *lerp = (float)(in - (float)*lower);\n+  if (clip) {\n+    if (*lower < 0)\n+      *lower = 0;\n+    else if (*lower >= in_size)\n+      *lower = in_size - 1;\n+    if (*upper < 0)\n+      *upper = 0;\n+    else if (*upper >= in_size)\n+      *upper = in_size - 1;\n+    return true;\n+  } else {\n+    return (*lower >= 0 && *upper < in_size) ? true : false;\n+  }\n+}\n+/**\n+ * Compute interpolation values for output indexes in range\n+ * [out_start,out_start+out_size-1].\n+ * Returns true if all output indexes have lower and upper (input) indexes\n+ * within range [0,in_size-1].\n+ */\n+bool compute_interpolation_weights(const int min_i, const int max_i,\n+                                   const int in_size, const float out2in_scale,\n+                                   const float out2in_start, const bool clip,\n+                                   CachedInterpolation* interpolation) {\n+  bool rval = true;\n+  int num_i = max_i - min_i + 1;\n+  for (int i = 0; i < num_i; ++i) {\n+    if (!compute_single_interpolation_weight(\n+            in_size, out2in_scale, out2in_start, clip, i + min_i,\n+            &interpolation[i].lower, &interpolation[i].upper,\n+            &interpolation[i].lerp)) {\n+      rval = false;\n+    }\n+  }\n+  return rval;\n+}\n+/**\n+ * Compatibility method for resize_bilinear_op.cc\n+ */\n+void compute_interpolation_weights(const int out_size, const int in_size,\n+                                   const float out2in_scale,\n+                                   CachedInterpolation* interpolation) {\n+  interpolation[out_size].lower = 0;\n+  interpolation[out_size].upper = 0;\n+  const bool clip = true;\n+  if (!compute_interpolation_weights(0, out_size - 1, in_size, out2in_scale,\n+                                     0.0f, clip, interpolation)) {\n+    // Should never happen, check for it anyway\n+    printf(\n+        \"Warning! Interpolation values have lower,upper indexes outside of \"\n+        \"range [0,in_size-1]\\n\");\n+  }\n+}\n+/**\n+ * Compute minimum and maximum (output) i where both lower and upper (input) is\n+ * in range [0,in_size-1]\n+ * If no values of i satisfy condition, min_i = in_size, max_i = -1 and method\n+ * returns false.\n+ * Returns true if min_i >= max_i.\n+ */\n+bool compute_minmax_indexes(const int out_size, const int in_size,\n+                            const float out2in_scale, const float out2in_start,\n+                            int& min_i, int& max_i) {\n+  min_i = out_size;\n+  max_i = -1;\n+  int lower, upper;\n+  float lerp;\n+  for (int i = 0; i < out_size; ++i) {\n+    if (compute_single_interpolation_weight(in_size, out2in_scale, out2in_start,\n+                                            false, i, &lower, &upper, &lerp)) {\n+      if (i < min_i) min_i = i;\n+      if (i > max_i) max_i = i;\n+    }\n+  }\n+  return (min_i <= max_i) ? true : false;\n+}\n+/**\n+ * Compute interpolation weights for crop_and_resize_op.cc\n+ * Also computes extrapolation areas.\n+ * Returns true if at least one point requires interpolation, false otherwise.\n+ */\n+bool compute_interpolation_weights(\n+    const int out_size, const int in_size,\n+    const float x1,  // lower bounding box, crop region starts at in_size*x1\n+    const float x2,  // upper bounding box, crop region ends at in_size*x2\n+    int& min_i, int& max_i, CachedInterpolation*& interpolation) {\n+  float out2in_start = out_size > 1\n+                           ? (float)(in_size - 1) * (float)x1\n+                           : (float)(in_size - 1) * (float)(x1 + x2) / 2.0f;\n+  float out2in_scale =\n+      out_size > 1\n+          ? (float)(x2 - x1) * (float)(in_size - 1) / (float)(out_size - 1)\n+          : 0.0f;\n+  if (compute_minmax_indexes(out_size, in_size, out2in_scale, out2in_start,\n+                             min_i, max_i)) {\n+    interpolation = new CachedInterpolation[max_i - min_i + 1];\n+    bool all_inputs_ok =\n+        compute_interpolation_weights(min_i, max_i, in_size, out2in_scale,\n+                                      out2in_start, false, interpolation);\n+    if (!all_inputs_ok) {\n+      // should never happen, purpose of compute_minmax_indexes is to ensure\n+      // that all inputs are ok.\n+      printf(\n+          \"Error! compute_interpolation_weights returned input indexes outside \"\n+          \"valid range - SEGV will likely ensue.\\n\");\n+    }\n+    return true;\n+  } else {\n+    interpolation = 0l;\n+    return false;\n+  }\n+}\n+\n+/**\n+ * Cast float v to type U with range clamping.\n+ *\n+ * If v<min_val, return value is clamped to u_min_val. similarly if v>max_val,\n+ * return value is clamped to u_max_val.\n+ */\n+template <typename U>\n+U cast_to(float v, float min_val, float max_val, U u_min_val, U u_max_val);\n+template <typename U>\n+U cast_to(float v, float min_val, float max_val, U u_min_val, U u_max_val) {\n+  if (v < min_val)\n+    return u_min_val;\n+  else if (v > max_val)\n+    return u_max_val;\n+  else\n+    return static_cast<U>(v);\n+}\n+template <>\n+float cast_to<float>(float v, float min_val, float max_val, float u_min_val,\n+                     float u_max_val) {\n+  return v;\n+}\n+\n+float compute_lerp(const float top_left, const float top_right,\n+                   const float bottom_left, const float bottom_right,\n+                   const float x_lerp, const float y_lerp) {\n+  const float top = top_left + (top_right - top_left) * x_lerp;\n+  const float bottom = bottom_left + (bottom_right - bottom_left) * x_lerp;\n+  return top + (bottom - top) * y_lerp;\n+}\n+\n+/**\n+ * Computes the bilinear interpolation from the appropriate 4 float points\n+ * and the linear interpolation weights.\n+ * Accepts input tensors of type T and produces output tensors of type U.\n+ * Optionally flips horizontal and/or vertical axis.\n+ */\n+template <typename T, typename U>\n+void crop_resize_single_image(const T* image, const int64 in_height,\n+                              const int64 in_width, const int64 out_height,\n+                              const int64 out_width, const int channels,\n+                              const int min_ix, const int max_ix,\n+                              const CachedInterpolation* xs, const int min_iy,\n+                              const int max_iy, const CachedInterpolation* ys,\n+                              const float extrapolated_value, const bool flip_x,\n+                              const bool flip_y,\n+                              U* output) TF_ATTRIBUTE_NOINLINE;\n+template <typename T, typename U>\n+void crop_resize_single_image(const T* image, const int64 in_height,\n+                              const int64 in_width, const int64 out_height,\n+                              const int64 out_width, const int channels,\n+                              const int min_ix, const int max_ix,\n+                              const CachedInterpolation* xs, const int min_iy,\n+                              const int max_iy, const CachedInterpolation* ys,\n+                              const float extrapolated_value, const bool flip_x,\n+                              const bool flip_y, U* output) {\n+  const int64 in_row_size = in_width * channels;\n+  const int64 out_row_size = out_width * channels;\n+  U u_min_val = std::numeric_limits<U>::min();\n+  U u_max_val = std::numeric_limits<U>::max();\n+  float min_val = static_cast<float>(u_min_val);\n+  float max_val = static_cast<float>(u_max_val);\n+  U uEx =\n+      cast_to<U>(extrapolated_value, min_val, max_val, u_min_val, u_max_val);\n+  // low y extrapolation zone\n+  if (min_iy > 0) {\n+    U* p = flip_y ? output + out_row_size * (out_height - min_iy) : output;\n+    int64 nn = out_row_size * (int64)min_iy;\n+    for (int64 i = 0; i < nn; ++i) p[i] = uEx;\n+  }\n+  // high y extrapolation zone\n+  if (max_iy < out_height - 1) {\n+    U* p = flip_y ? output : output + out_row_size * (max_iy + 1);\n+    int64 nn = out_row_size * (int64)(out_height - 1 - max_iy);\n+    for (int64 i = 0; i < nn; ++i) p[i] = uEx;\n+  }\n+  // low x extrapolation zone\n+  if (min_ix > 0) {\n+    for (int iy = min_iy; iy <= max_iy; ++iy) {\n+      int xx0 = flip_x ? (out_width - min_ix) * channels : 0;\n+      int nxx = min_ix * channels;\n+      U* p = output + xx0 +\n+             out_row_size * (int64)(flip_y ? out_height - 1 - iy : iy);\n+      for (int ix = 0; ix < nxx; ++ix) {\n+        p[ix] = uEx;\n+      }\n+    }\n+  }\n+  // high x extrapolation zone\n+  if (max_ix < out_width - 1) {\n+    for (int iy = min_iy; iy <= max_iy; ++iy) {\n+      int xx0 = flip_x ? 0 : (max_ix + 1) * channels;\n+      int nxx = (out_width - 1 - max_ix) * channels;\n+      U* p = output + xx0 +\n+             out_row_size * (int64)(flip_y ? out_height - 1 - iy : iy);\n+      for (int ix = 0; ix < nxx; ++ix) {\n+        p[ix] = uEx;\n+      }\n+    }\n+  }\n+  U* output_y_ptr =\n+      output +\n+      out_row_size * (int64)(flip_y ? out_height - 1 - min_iy : min_iy);\n+  // interpolation zone\n+  if (channels == 1) {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+\n+        // Read channel 0.\n+        const float top_left0(ys_input_lower_ptr[xs_lower]);\n+        const float top_right0(ys_input_lower_ptr[xs_upper]);\n+        const float bottom_left0(ys_input_upper_ptr[xs_lower]);\n+        const float bottom_right0(ys_input_upper_ptr[xs_upper]);\n+\n+        // Compute output.\n+        float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                     bottom_right0, xs_lerp, ys_lerp);\n+        output_y_ptr[x] =\n+            cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  } else if (channels == 2) {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+\n+        // Read channel 0.\n+        const float top_left0(ys_input_lower_ptr[xs_lower + 0]);\n+        const float top_right0(ys_input_lower_ptr[xs_upper + 0]);\n+        const float bottom_left0(ys_input_upper_ptr[xs_lower + 0]);\n+        const float bottom_right0(ys_input_upper_ptr[xs_upper + 0]);\n+\n+        // Read channel 1.\n+        const float top_left1(ys_input_lower_ptr[xs_lower + 1]);\n+        const float top_right1(ys_input_lower_ptr[xs_upper + 1]);\n+        const float bottom_left1(ys_input_upper_ptr[xs_lower + 1]);\n+        const float bottom_right1(ys_input_upper_ptr[xs_upper + 1]);\n+\n+        // Compute output.\n+        float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                     bottom_right0, xs_lerp, ys_lerp);\n+        float result1 = compute_lerp(top_left1, top_right1, bottom_left1,\n+                                     bottom_right1, xs_lerp, ys_lerp);\n+        output_y_ptr[x * 2 + 0] =\n+            cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 2 + 1] =\n+            cast_to<U>(result1, min_val, max_val, u_min_val, u_max_val);\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  } else if (channels == 3) {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+\n+        // Read channel 0.\n+        const float top_left0(ys_input_lower_ptr[xs_lower + 0]);\n+        const float top_right0(ys_input_lower_ptr[xs_upper + 0]);\n+        const float bottom_left0(ys_input_upper_ptr[xs_lower + 0]);\n+        const float bottom_right0(ys_input_upper_ptr[xs_upper + 0]);\n+\n+        // Read channel 1.\n+        const float top_left1(ys_input_lower_ptr[xs_lower + 1]);\n+        const float top_right1(ys_input_lower_ptr[xs_upper + 1]);\n+        const float bottom_left1(ys_input_upper_ptr[xs_lower + 1]);\n+        const float bottom_right1(ys_input_upper_ptr[xs_upper + 1]);\n+\n+        // Read channel 2.\n+        const float top_left2(ys_input_lower_ptr[xs_lower + 2]);\n+        const float top_right2(ys_input_lower_ptr[xs_upper + 2]);\n+        const float bottom_left2(ys_input_upper_ptr[xs_lower + 2]);\n+        const float bottom_right2(ys_input_upper_ptr[xs_upper + 2]);\n+\n+        // Compute output.\n+        float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                     bottom_right0, xs_lerp, ys_lerp);\n+        float result1 = compute_lerp(top_left1, top_right1, bottom_left1,\n+                                     bottom_right1, xs_lerp, ys_lerp);\n+        float result2 = compute_lerp(top_left2, top_right2, bottom_left2,\n+                                     bottom_right2, xs_lerp, ys_lerp);\n+        output_y_ptr[x * 3 + 0] =\n+            cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 3 + 1] =\n+            cast_to<U>(result1, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 3 + 2] =\n+            cast_to<U>(result2, min_val, max_val, u_min_val, u_max_val);\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  } else if (channels == 4) {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+\n+        // Read channel 0.\n+        const float top_left0(ys_input_lower_ptr[xs_lower + 0]);\n+        const float top_right0(ys_input_lower_ptr[xs_upper + 0]);\n+        const float bottom_left0(ys_input_upper_ptr[xs_lower + 0]);\n+        const float bottom_right0(ys_input_upper_ptr[xs_upper + 0]);\n+\n+        // Read channel 1.\n+        const float top_left1(ys_input_lower_ptr[xs_lower + 1]);\n+        const float top_right1(ys_input_lower_ptr[xs_upper + 1]);\n+        const float bottom_left1(ys_input_upper_ptr[xs_lower + 1]);\n+        const float bottom_right1(ys_input_upper_ptr[xs_upper + 1]);\n+\n+        // Read channel 2.\n+        const float top_left2(ys_input_lower_ptr[xs_lower + 2]);\n+        const float top_right2(ys_input_lower_ptr[xs_upper + 2]);\n+        const float bottom_left2(ys_input_upper_ptr[xs_lower + 2]);\n+        const float bottom_right2(ys_input_upper_ptr[xs_upper + 2]);\n+\n+        // Read channel 3.\n+        const float top_left3(ys_input_lower_ptr[xs_lower + 3]);\n+        const float top_right3(ys_input_lower_ptr[xs_upper + 3]);\n+        const float bottom_left3(ys_input_upper_ptr[xs_lower + 3]);\n+        const float bottom_right3(ys_input_upper_ptr[xs_upper + 3]);\n+\n+        // Compute output.\n+        float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                     bottom_right0, xs_lerp, ys_lerp);\n+        float result1 = compute_lerp(top_left1, top_right1, bottom_left1,\n+                                     bottom_right1, xs_lerp, ys_lerp);\n+        float result2 = compute_lerp(top_left2, top_right2, bottom_left2,\n+                                     bottom_right2, xs_lerp, ys_lerp);\n+        float result3 = compute_lerp(top_left3, top_right3, bottom_left3,\n+                                     bottom_right3, xs_lerp, ys_lerp);\n+        output_y_ptr[x * 4 + 0] =\n+            cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 4 + 1] =\n+            cast_to<U>(result1, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 4 + 2] =\n+            cast_to<U>(result2, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 4 + 3] =\n+            cast_to<U>(result3, min_val, max_val, u_min_val, u_max_val);\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  } else {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+        for (int ichan = 0; ichan < channels; ++ichan) {\n+          const float top_left0(ys_input_lower_ptr[xs_lower + ichan]);\n+          const float top_right0(ys_input_lower_ptr[xs_upper + ichan]);\n+          const float bottom_left0(ys_input_upper_ptr[xs_lower + ichan]);\n+          const float bottom_right0(ys_input_upper_ptr[xs_upper + ichan]);\n+          float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                       bottom_right0, xs_lerp, ys_lerp);\n+          output_y_ptr[x * channels + ichan] =\n+              cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+        }\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  }\n+}\n+\n+#ifdef __SSE4_1__\n+\n+//\n+// The remaining code implements explicitly vectorized versions of a bilinear\n+// image resizer.\n+// Images with 1, 2, 3 or 4 channels are supported.\n+// The image resizer reads samples of type T and writes samples of type U.\n+// T and U can be any of the following: uint8, int8, uint16, int16, int32,\n+// Eigen::half, bfloat16 and float.\n+// There are separate codes for SSE4.1 and AVX2. Enabling AVX2 also enables\n+// FP16C instruction set,\n+// which contains instructions that convert between Eigen::half and float. The\n+// SSE4.1 code path emulates\n+// the FP16C instructions in software.\n+//\n+\n+//\n+// This class loads 4 pixels with n channels, converts to fp32 and packs\n+// the result into n SSE vector words.\n+// Input data type T must be one of uint8, int8, uint16, int16, int32,\n+// Eigen::half, bfloat16 or float.\n+//\n+\n+template <class T>\n+class VectorLoader {\n+ public:\n+#ifdef __AVX2__\n+  // convert 8 packed words of type T to fp32.\n+  // T must be one of uint8, int8, uint16, int16, int32, Eigen::half, bfloat16\n+  // or float.\n+  __m256 to_fp32(__m256i raw);\n+#else\n+  // convert 4 packed words of type T to fp32.\n+  // T must be one of uint8, int8, uint16, int16, int32, Eigen::half, bfloat16\n+  // or float.\n+  __m128 to_fp32(__m128i raw);\n+#endif\n+\n+#ifdef __AVX2__\n+  // pack 4 pixels with 1 channel, 2 channels and 3channels respectively in\n+  // separate 128 bit lanes.\n+  // input is stored in lower portion of 4 separate sse words, v0 through v3.\n+  // output is stored in lower portion of v0.\n+  void pack_1ch(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  // output is stored in lower portion of v0 and v1.\n+  void pack_2ch(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  // output is stored in lower portion of v0, v1 and v2.\n+  void pack_3ch(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+#else\n+  // pack 4 pixels with 1 channel, 2 channels and 3channels respectively.\n+  // input is stored in lower portion of 4 separate sse words, v0 through v3.\n+  // output is stored in lower portion of v0.\n+  void pack_1ch(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  // output is stored in lower portion of v0 and v1.\n+  void pack_2ch(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  // output is stored in lower portion of v0, v1 and v2.\n+  void pack_3ch(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+#endif\n+\n+#ifdef __AVX2__\n+  // extract right pixel for load1 and load4 cases.\n+  __m256i extract_right_1ch(const __m256i& left);\n+  __m256i extract_right_2ch(const __m256i& left);\n+  __m256i extract_right_3ch(const __m256i& left);\n+  __m256i extract_right_4ch(const __m256i& left);\n+#else\n+  __m128i extract_right_1ch(const __m128i& left);\n+  __m128i extract_right_2ch(const __m128i& left);\n+  __m128i extract_right_3ch(const __m128i& left);\n+  __m128i extract_right_4ch(const __m128i& left);\n+#endif\n+\n+#ifdef __AVX2__\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 1 channel.\n+  // load1 case, i.e. 4 left and right inputs are loaded with a single unaligned\n+  // SSE load.\n+  void load1_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256& left0, __m256& right0);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 2 channels.\n+  // load1 case, i.e. 4 left and right inputs are loaded with a single unaligned\n+  // SSE load.\n+  void load1_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256& left0, __m256& left1,\n+                 __m256& right0, __m256& right1);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 3 channels.\n+  // load1 case, i.e. 4 left and right inputs are loaded with a single unaligned\n+  // SSE load.\n+  void load1_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256& left0, __m256& left1,\n+                 __m256& left2, __m256& right0, __m256& right1, __m256& right2);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 4 channels.\n+  // load1 case, i.e. 4 left and right inputs are loaded with a single unaligned\n+  // SSE load.\n+  void load1_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256& left0, __m256& left1,\n+                 __m256& left2, __m256& left3, __m256& right0, __m256& right1,\n+                 __m256& right2, __m256& right3);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 1 channel.\n+  // load2 case, i.e. 4 left inputs are loaded with first SSE load and 4 right\n+  // inputs are loaded with second SSE load.\n+  void load2_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256& left0, __m256& right0);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 2 channels.\n+  // load2 case, i.e. 4 left inputs are loaded with first SSE load and 4 right\n+  // inputs are loaded with second SSE load.\n+  void load2_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256& left0, __m256& left1,\n+                 __m256& right0, __m256& right1);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 3 channels.\n+  // load2 case, i.e. 4 left inputs are loaded with first SSE load and 4 right\n+  // inputs are loaded with second SSE load.\n+  void load2_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256& left0, __m256& left1,\n+                 __m256& left2, __m256& right0, __m256& right1, __m256& right2);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 4 channels.\n+  // load2 case, i.e. 4 left inputs are loaded with first SSE load and 4 right\n+  // inputs are loaded with second SSE load.\n+  void load2_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256& left0, __m256& left1,\n+                 __m256& left2, __m256& left3, __m256& right0, __m256& right1,\n+                 __m256& right2, __m256& right3);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 1 channel.\n+  // load4 case, i.e. each pair of left and right inputs are loaded with a\n+  // separate SSE load.\n+  void load4_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256& left0,\n+                 __m256& right0);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 2 channels.\n+  // load4 case, i.e. each pair of left and right inputs are loaded with a\n+  // separate SSE load.\n+  void load4_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256& left0,\n+                 __m256& left1, __m256& right0, __m256& right1);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 3 channels.\n+  // load4 case, i.e. each pair of left and right inputs are loaded with a\n+  // separate SSE load.\n+  void load4_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256& left0,\n+                 __m256& left1, __m256& left2, __m256& right0, __m256& right1,\n+                 __m256& right2);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 4 channels.\n+  // load4 case, i.e. each pair of left and right inputs are loaded with a\n+  // separate SSE load.\n+  void load4_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256& left0,\n+                 __m256& left1, __m256& left2, __m256& left3, __m256& right0,\n+                 __m256& right1, __m256& right2, __m256& right3);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 1 channel.\n+  // load8 case, i.e. each input is loaded with a separate SSE load.\n+  // 4 pixels, each with left and right input necessitates 8 separate SSE loads\n+  // per input row.\n+  void load8_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256& left0,\n+                 __m256& right0);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 2 channels.\n+  // load8 case, i.e. each input is loaded with a separate SSE load.\n+  // 4 pixels, each with left and right input necessitates 8 separate SSE loads\n+  // per input row.\n+  void load8_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256& left0,\n+                 __m256& left1, __m256& right0, __m256& right1);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 3 channels.\n+  // load8 case, i.e. each input is loaded with a separate SSE load.\n+  // 4 pixels, each with left and right input necessitates 8 separate SSE loads\n+  // per input row.\n+  void load8_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256& left0,\n+                 __m256& left1, __m256& left2, __m256& right0, __m256& right1,\n+                 __m256& right2);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 4 channels.\n+  // load8 case, i.e. each input is loaded with a separate SSE load.\n+  // 4 pixels, each with left and right input necessitates 8 separate SSE loads\n+  // per input row.\n+  void load8_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256& left0,\n+                 __m256& left1, __m256& left2, __m256& left3, __m256& right0,\n+                 __m256& right1, __m256& right2, __m256& right3);\n+#else\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 1 channel.\n+  // load1 case, i.e. all inputs for one input row are loaded with a single SSE\n+  // load.\n+  void load1_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128& tl0, __m128& bl0,\n+                 __m128& tr0, __m128& br0);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 2 channels.\n+  // load1 case, i.e. all inputs for one input row are loaded with a single SSE\n+  // load.\n+  void load1_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128& tl0, __m128& tl1,\n+                 __m128& bl0, __m128& bl1, __m128& tr0, __m128& tr1,\n+                 __m128& br0, __m128& br1);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 3 channels.\n+  // load1 case, i.e. all inputs for one input row are loaded with a single SSE\n+  // load.\n+  void load1_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128& tl0, __m128& tl1,\n+                 __m128& tl2, __m128& bl0, __m128& bl1, __m128& bl2,\n+                 __m128& tr0, __m128& tr1, __m128& tr2, __m128& br0,\n+                 __m128& br1, __m128& br2);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 4 channels.\n+  // load1 case, i.e. all inputs for one input row are loaded with a single SSE\n+  // load.\n+  void load1_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128& tl0, __m128& tl1,\n+                 __m128& tl2, __m128& tl3, __m128& bl0, __m128& bl1,\n+                 __m128& bl2, __m128& bl3, __m128& tr0, __m128& tr1,\n+                 __m128& tr2, __m128& tr3, __m128& br0, __m128& br1,\n+                 __m128& br2, __m128& br3);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 1 channel.\n+  // load2 case, i.e. left inputs are loaded with first SSE load, right inputs\n+  // are loaded with second SSE load.\n+  void load2_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128& tl0, __m128& bl0,\n+                 __m128& tr0, __m128& br0);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 2 channels.\n+  // load2 case, i.e. left inputs are loaded with first SSE load, right inputs\n+  // are loaded with second SSE load.\n+  void load2_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128& tl0, __m128& tl1,\n+                 __m128& bl0, __m128& bl1, __m128& tr0, __m128& tr1,\n+                 __m128& br0, __m128& br1);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 3 channels.\n+  // load2 case, i.e. left inputs are loaded with first SSE load, right inputs\n+  // are loaded with second SSE load.\n+  void load2_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128& tl0, __m128& tl1,\n+                 __m128& tl2, __m128& bl0, __m128& bl1, __m128& bl2,\n+                 __m128& tr0, __m128& tr1, __m128& tr2, __m128& br0,\n+                 __m128& br1, __m128& br2);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 4 channels.\n+  // load2 case, i.e. left inputs are loaded with first SSE load, right inputs\n+  // are loaded with second SSE load.\n+  void load2_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128& tl0, __m128& tl1,\n+                 __m128& tl2, __m128& tl3, __m128& bl0, __m128& bl1,\n+                 __m128& bl2, __m128& bl3, __m128& tr0, __m128& tr1,\n+                 __m128& tr2, __m128& tr3, __m128& br0, __m128& br1,\n+                 __m128& br2, __m128& br3);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 1 channel.\n+  // load4 case, i.e. left and right inputs are loaded with a separate SSE load\n+  // for each pixel.\n+  void load4_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128& tl0,\n+                 __m128& bl0, __m128& tr0, __m128& br0);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 2 channels.\n+  // load4 case, i.e. left and right inputs are loaded with a separate SSE load\n+  // for each pixel.\n+  void load4_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128& tl0,\n+                 __m128& tl1, __m128& bl0, __m128& bl1, __m128& tr0,\n+                 __m128& tr1, __m128& br0, __m128& br1);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 3 channels.\n+  // load4 case, i.e. left and right inputs are loaded with a separate SSE load\n+  // for each pixel.\n+  void load4_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128& tl0,\n+                 __m128& tl1, __m128& tl2, __m128& bl0, __m128& bl1,\n+                 __m128& bl2, __m128& tr0, __m128& tr1, __m128& tr2,\n+                 __m128& br0, __m128& br1, __m128& br2);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 4 channels.\n+  // load4 case, i.e. left and right inputs are loaded with a separate SSE load\n+  // for each pixel.\n+  void load4_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128& tl0,\n+                 __m128& tl1, __m128& tl2, __m128& tl3, __m128& bl0,\n+                 __m128& bl1, __m128& bl2, __m128& bl3, __m128& tr0,\n+                 __m128& tr1, __m128& tr2, __m128& tr3, __m128& br0,\n+                 __m128& br1, __m128& br2, __m128& br3);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 1 channel.\n+  // load8 case, i.e. left and right inputs are loaded with separate SSE loads\n+  // for each pixel.\n+  void load8_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128& tl0,\n+                 __m128& bl0, __m128& tr0, __m128& br0);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 2 channels.\n+  // load8 case, i.e. left and right inputs are loaded with separate SSE loads\n+  // for each pixel.\n+  void load8_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128& tl0,\n+                 __m128& tl1, __m128& bl0, __m128& bl1, __m128& tr0,\n+                 __m128& tr1, __m128& br0, __m128& br1);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 3 channels.\n+  // load8 case, i.e. left and right inputs are loaded with separate SSE loads\n+  // for each pixel.\n+  void load8_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128& tl0,\n+                 __m128& tl1, __m128& tl2, __m128& bl0, __m128& bl1,\n+                 __m128& bl2, __m128& tr0, __m128& tr1, __m128& tr2,\n+                 __m128& br0, __m128& br1, __m128& br2);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 4 channels.\n+  // load8 case, i.e. left and right inputs are loaded with separate SSE loads\n+  // for each pixel.\n+  void load8_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128& tl0,\n+                 __m128& tl1, __m128& tl2, __m128& tl3, __m128& bl0,\n+                 __m128& bl1, __m128& bl2, __m128& bl3, __m128& tr0,\n+                 __m128& tr1, __m128& tr2, __m128& tr3, __m128& br0,\n+                 __m128& br1, __m128& br2, __m128& br3);\n+#endif\n+\n+  // there is no method that packs 4 pixels with 4 channel into four sse words.\n+  // nothing to do for this case, everything is already in the right position.\n+\n+ private:\n+// helper methods\n+#ifdef __AVX2__\n+  // pack 4 pixels with 1, 2, 3 or 4 channels into lower portion of SSE vector\n+  // word.\n+  // works within SSE lanes.\n+  // sizeof(sample_data_type) can be 1, 2 or 4 bytes.\n+  void pack4_1b_1ch_(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  void pack4_2b_1ch_(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  void pack4_4b_1ch_(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  void pack4_1b_2ch_(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  void pack4_2b_2ch_(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  void pack4_4b_2ch_(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  void pack4_1b_3ch_(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  void pack4_2b_3ch_(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+  void pack4_4b_3ch_(__m256i& v0, __m256i& v1, __m256i& v2, __m256i& v3);\n+// there is no pack4_xx_4ch functions because none is needed.\n+// all the bytes are loaded in the right spots for this case.\n+#else\n+  // pack 4 pixels with 1, 2, 3 or 4 channels into lower portion of SSE vector\n+  // word.\n+  // sizeof(sample_data_type) can be 1, 2 or 4 bytes.\n+  void pack4_1b_1ch_(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  void pack4_2b_1ch_(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  void pack4_4b_1ch_(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  void pack4_1b_2ch_(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  void pack4_2b_2ch_(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  void pack4_4b_2ch_(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  void pack4_1b_3ch_(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  void pack4_2b_3ch_(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+  void pack4_4b_3ch_(__m128i& v0, __m128i& v1, __m128i& v2, __m128i& v3);\n+#endif\n+#ifdef __AVX2__\n+  __m256i extract_right_1b_(const __m256i& left);\n+  __m256i extract_right_2b_(const __m256i& left);\n+  __m256i extract_right_3b_(const __m256i& left);\n+  __m256i extract_right_4b_(const __m256i& left);\n+  __m256i extract_right_6b_(const __m256i& left);\n+  __m256i extract_right_8b_(const __m256i& left);\n+#else\n+  __m128i extract_right_1b_(const __m128i& left);\n+  __m128i extract_right_2b_(const __m128i& left);\n+  __m128i extract_right_3b_(const __m128i& left);\n+  __m128i extract_right_4b_(const __m128i& left);\n+  __m128i extract_right_6b_(const __m128i& left);\n+  __m128i extract_right_8b_(const __m128i& left);\n+#endif\n+};\n+\n+#ifdef __AVX2__\n+template <class T>\n+void VectorLoader<T>::pack4_1b_1ch_(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  v3 = _mm256_slli_si256(v3, 3);\n+  __m256i and_mask = _mm256_setr_epi32(255, 0, 0, 0, 255, 0, 0, 0);\n+  v2 =\n+      _mm256_or_si256(v3, _mm256_slli_si256(_mm256_and_si256(and_mask, v2), 2));\n+  v1 =\n+      _mm256_or_si256(v2, _mm256_slli_si256(_mm256_and_si256(and_mask, v1), 1));\n+  v0 = _mm256_or_si256(v1, _mm256_and_si256(and_mask, v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_1ch_(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  v3 = _mm256_slli_si256(v3, 6);\n+  __m256i and_mask = _mm256_setr_epi32(65535, 0, 0, 0, 65535, 0, 0, 0);\n+  v2 =\n+      _mm256_or_si256(v3, _mm256_slli_si256(_mm256_and_si256(and_mask, v2), 4));\n+  v1 =\n+      _mm256_or_si256(v2, _mm256_slli_si256(_mm256_and_si256(and_mask, v1), 2));\n+  v0 = _mm256_or_si256(v1, _mm256_and_si256(and_mask, v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_1ch_(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  v3 = _mm256_slli_si256(v3, 12);\n+  __m256i and_mask = _mm256_setr_epi32(-1, 0, 0, 0, -1, 0, 0, 0);\n+  v2 =\n+      _mm256_or_si256(v3, _mm256_slli_si256(_mm256_and_si256(and_mask, v2), 8));\n+  v1 =\n+      _mm256_or_si256(v2, _mm256_slli_si256(_mm256_and_si256(and_mask, v1), 4));\n+  v0 = _mm256_or_si256(v1, _mm256_and_si256(and_mask, v0));\n+}\n+\n+template <class T>\n+void VectorLoader<T>::pack4_1b_2ch_(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  __m256i and_mask = _mm256_setr_epi32(65535, 0, 0, 0, 65535, 0, 0, 0);\n+  v0 =\n+      _mm256_or_si256(_mm256_and_si256(v0, and_mask), _mm256_slli_si256(v1, 2));\n+  v1 =\n+      _mm256_or_si256(_mm256_and_si256(v2, and_mask), _mm256_slli_si256(v3, 2));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_2ch_(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  __m256i and_mask = _mm256_setr_epi32(-1, 0, 0, 0, -1, 0, 0, 0);\n+  v0 =\n+      _mm256_or_si256(_mm256_and_si256(v0, and_mask), _mm256_slli_si256(v1, 4));\n+  v1 =\n+      _mm256_or_si256(_mm256_and_si256(v2, and_mask), _mm256_slli_si256(v3, 4));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_2ch_(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  __m256i and_mask = _mm256_setr_epi32(-1, -1, 0, 0, -1, -1, 0, 0);\n+  v0 =\n+      _mm256_or_si256(_mm256_and_si256(v0, and_mask), _mm256_slli_si256(v1, 8));\n+  v1 =\n+      _mm256_or_si256(_mm256_and_si256(v2, and_mask), _mm256_slli_si256(v3, 8));\n+}\n+\n+template <class T>\n+void VectorLoader<T>::pack4_1b_3ch_(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  __m256i and_mask = _mm256_setr_epi32(16777215, 0, 0, 0, 16777215, 0, 0, 0);\n+  v0 =\n+      _mm256_or_si256(_mm256_and_si256(v0, and_mask), _mm256_slli_si256(v1, 3));\n+  and_mask = _mm256_srli_si256(and_mask, 1);\n+  v1 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(v1, 1), and_mask),\n+                       _mm256_slli_si256(v2, 2));\n+  and_mask = _mm256_srli_si256(and_mask, 1);\n+  v2 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(v2, 2), and_mask),\n+                       _mm256_slli_si256(v3, 1));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_3ch_(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  __m256i and_mask = _mm256_setr_epi32(-1, 65535, 0, 0, -1, 65535, 0, 0);\n+  v0 =\n+      _mm256_or_si256(_mm256_and_si256(v0, and_mask), _mm256_slli_si256(v1, 6));\n+  and_mask = _mm256_srli_si256(and_mask, 2);\n+  v1 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(v1, 2), and_mask),\n+                       _mm256_slli_si256(v2, 4));\n+  and_mask = _mm256_srli_si256(and_mask, 2);\n+  v2 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(v2, 4), and_mask),\n+                       _mm256_slli_si256(v3, 2));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_3ch_(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  __m256i and_mask = _mm256_setr_epi32(-1, -1, -1, 0, -1, -1, -1, 0);\n+  v0 = _mm256_or_si256(_mm256_and_si256(v0, and_mask),\n+                       _mm256_slli_si256(v1, 12));\n+  and_mask = _mm256_srli_si256(and_mask, 4);\n+  v1 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(v1, 4), and_mask),\n+                       _mm256_slli_si256(v2, 8));\n+  and_mask = _mm256_srli_si256(and_mask, 4);\n+  v2 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(v2, 8), and_mask),\n+                       _mm256_slli_si256(v3, 4));\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_1ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_1b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_1ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                  __m256i& v3) {\n+  pack4_1b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_1ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_1ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_1ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_4b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_1ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                         __m256i& v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_1ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                      __m256i& v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_1ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_4b_1ch_(v0, v1, v2, v3);\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_2ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_1b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_2ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                  __m256i& v3) {\n+  pack4_1b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_2ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_2ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_2ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_4b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_2ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                         __m256i& v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_2ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                      __m256i& v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_2ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_4b_2ch_(v0, v1, v2, v3);\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_3ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_1b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_3ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                  __m256i& v3) {\n+  pack4_1b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_3ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                    __m256i& v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_3ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_3ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_4b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_3ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                         __m256i& v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_3ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                      __m256i& v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_3ch(__m256i& v0, __m256i& v1, __m256i& v2,\n+                                   __m256i& v3) {\n+  pack4_4b_3ch_(v0, v1, v2, v3);\n+}\n+#else\n+template <class T>\n+void VectorLoader<T>::pack4_1b_1ch_(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  v3 = _mm_slli_si128(v3, 3);\n+  __m128i and_mask = _mm_setr_epi32(255, 0, 0, 0);\n+  v2 = _mm_or_si128(v3, _mm_slli_si128(_mm_and_si128(and_mask, v2), 2));\n+  v1 = _mm_or_si128(v2, _mm_slli_si128(_mm_and_si128(and_mask, v1), 1));\n+  v0 = _mm_or_si128(v1, _mm_and_si128(and_mask, v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_1ch_(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  v3 = _mm_slli_si128(v3, 6);\n+  __m128i and_mask = _mm_setr_epi32(65535, 0, 0, 0);\n+  v2 = _mm_or_si128(v3, _mm_slli_si128(_mm_and_si128(and_mask, v2), 4));\n+  v1 = _mm_or_si128(v2, _mm_slli_si128(_mm_and_si128(and_mask, v1), 2));\n+  v0 = _mm_or_si128(v1, _mm_and_si128(and_mask, v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_1ch_(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  v3 = _mm_slli_si128(v3, 12);\n+  __m128i and_mask = _mm_setr_epi32(-1, 0, 0, 0);\n+  v2 = _mm_or_si128(v3, _mm_slli_si128(_mm_and_si128(and_mask, v2), 8));\n+  v1 = _mm_or_si128(v2, _mm_slli_si128(_mm_and_si128(and_mask, v1), 4));\n+  v0 = _mm_or_si128(v1, _mm_and_si128(and_mask, v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_1b_2ch_(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  __m128i and_mask = _mm_setr_epi32(65535, 0, 0, 0);\n+  v0 = _mm_or_si128(_mm_and_si128(v0, and_mask), _mm_slli_si128(v1, 2));\n+  v1 = _mm_or_si128(_mm_and_si128(v2, and_mask), _mm_slli_si128(v3, 2));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_2ch_(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  __m128i and_mask = _mm_setr_epi32(-1, 0, 0, 0);\n+  v0 = _mm_or_si128(_mm_and_si128(v0, and_mask), _mm_slli_si128(v1, 4));\n+  v1 = _mm_or_si128(_mm_and_si128(v2, and_mask), _mm_slli_si128(v3, 4));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_2ch_(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  __m128i and_mask = _mm_setr_epi32(-1, -1, 0, 0);\n+  v0 = _mm_or_si128(_mm_and_si128(v0, and_mask), _mm_slli_si128(v1, 8));\n+  v1 = _mm_or_si128(_mm_and_si128(v2, and_mask), _mm_slli_si128(v3, 8));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_1b_3ch_(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  __m128i and_mask = _mm_setr_epi32(16777215, 0, 0, 0);\n+  v0 = _mm_or_si128(_mm_and_si128(v0, and_mask), _mm_slli_si128(v1, 3));\n+  and_mask = _mm_srli_si128(and_mask, 1);\n+  v1 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(v1, 1), and_mask),\n+                    _mm_slli_si128(v2, 2));\n+  and_mask = _mm_srli_si128(and_mask, 1);\n+  v2 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(v2, 2), and_mask),\n+                    _mm_slli_si128(v3, 1));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_3ch_(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  __m128i and_mask = _mm_setr_epi32(-1, 65535, 0, 0);\n+  v0 = _mm_or_si128(_mm_and_si128(v0, and_mask), _mm_slli_si128(v1, 6));\n+  and_mask = _mm_srli_si128(and_mask, 2);\n+  v1 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(v1, 2), and_mask),\n+                    _mm_slli_si128(v2, 4));\n+  and_mask = _mm_srli_si128(and_mask, 2);\n+  v2 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(v2, 4), and_mask),\n+                    _mm_slli_si128(v3, 2));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_3ch_(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  __m128i and_mask = _mm_setr_epi32(-1, -1, -1, 0);\n+  v0 = _mm_or_si128(_mm_and_si128(v0, and_mask), _mm_slli_si128(v1, 12));\n+  and_mask = _mm_srli_si128(and_mask, 4);\n+  v1 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(v1, 4), and_mask),\n+                    _mm_slli_si128(v2, 8));\n+  and_mask = _mm_srli_si128(and_mask, 4);\n+  v2 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(v2, 8), and_mask),\n+                    _mm_slli_si128(v3, 4));\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_1ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                   __m128i& v3) {\n+  pack4_1b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_1ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                  __m128i& v3) {\n+  pack4_1b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_1ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_1ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                   __m128i& v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_1ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                   __m128i& v3) {\n+  pack4_4b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_1ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                         __m128i& v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_1ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                      __m128i& v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_1ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                   __m128i& v3) {\n+  pack4_4b_1ch_(v0, v1, v2, v3);\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_2ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                   __m128i& v3) {\n+  pack4_1b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_2ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                  __m128i& v3) {\n+  pack4_1b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_2ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                    __m128i& v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_2ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                   __m128i& v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_2ch(__m128i& v0, __m128i& v1, __m128i& v2,\n+                                   __m128i& v3) {\n+  pack4_4b_2ch_(v0, v1, v2, v3)", "path": "tensorflow/core/kernels/crop_resize_bilinear_core.h", "position": null, "original_position": 5146, "commit_id": "2bef68f68bb6dbbefee151ea853b92ac17bb5acc", "original_commit_id": "eecaed98a7b2b28cef76f6de55e173f490cb70f3", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "body": "Remove commented-out code", "created_at": "2018-08-17T15:55:51Z", "updated_at": "2018-11-05T16:08:34Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21681#discussion_r210955658", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21681", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/210955658"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21681#discussion_r210955658"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21681"}}, "body_html": "<p>Remove commented-out code</p>", "body_text": "Remove commented-out code"}