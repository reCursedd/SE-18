{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/222399629", "pull_request_review_id": 161277826, "id": 222399629, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMjM5OTYyOQ==", "diff_hunk": "@@ -0,0 +1,5582 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifndef TENSORFLOW_CORE_KERNELS_CROP_RESIZE_BILINEAR_CORE_H_\n+#define TENSORFLOW_CORE_KERNELS_CROP_RESIZE_BILINEAR_CORE_H_\n+\n+// only include intrinsics when the appropriate flags call for it,\n+// since these headers only exists on x86 platforms.\n+#ifdef __SSE4_1__\n+#include <smmintrin.h>\n+#include <tmmintrin.h>\n+#include <xmmintrin.h>\n+#endif\n+#ifdef __AVX2__\n+#include <immintrin.h>\n+#endif\n+#include <cassert>\n+#include <cstdio>\n+#include <cstdlib>\n+#include <list>\n+#include <string>\n+\n+namespace tensorflow {\n+namespace {\n+\n+// Compute the interpolation indices only once.\n+struct CachedInterpolation {\n+  int lower;  // Lower source index used in the interpolation\n+  int upper;  // Upper source index used in the interpolation\n+  // 1-D linear iterpolation scale (see:\n+  // https://en.wikipedia.org/wiki/Bilinear_interpolation)\n+  float lerp;\n+};\n+\n+bool compute_single_interpolation_weight(const int in_size,\n+                                         const float out2in_scale,\n+                                         const float out2in_start,\n+                                         const bool clip, const int i,\n+                                         int* lower, int* upper, float* lerp) {\n+  const float in = i * out2in_scale + out2in_start;\n+  *lower = (int)floor(in);\n+  *upper = (int)ceil(in);\n+  *lerp = (float)(in - (float)*lower);\n+  if (clip) {\n+    if (*lower < 0)\n+      *lower = 0;\n+    else if (*lower >= in_size)\n+      *lower = in_size - 1;\n+    if (*upper < 0)\n+      *upper = 0;\n+    else if (*upper >= in_size)\n+      *upper = in_size - 1;\n+    return true;\n+  } else {\n+    return (*lower >= 0 && *upper < in_size) ? true : false;\n+  }\n+}\n+/**\n+ * Compute interpolation values for output indexes in range\n+ * [out_start,out_start+out_size-1].\n+ * Returns true if all output indexes have lower and upper (input) indexes\n+ * within range [0,in_size-1].\n+ */\n+bool compute_interpolation_weights(const int min_i, const int max_i,\n+                                   const int in_size, const float out2in_scale,\n+                                   const float out2in_start, const bool clip,\n+                                   CachedInterpolation* interpolation) {\n+  bool rval = true;\n+  int num_i = max_i - min_i + 1;\n+  for (int i = 0; i < num_i; ++i) {\n+    if (!compute_single_interpolation_weight(\n+            in_size, out2in_scale, out2in_start, clip, i + min_i,\n+            &interpolation[i].lower, &interpolation[i].upper,\n+            &interpolation[i].lerp)) {\n+      rval = false;\n+    }\n+  }\n+  return rval;\n+}\n+/**\n+ * Compatibility method for resize_bilinear_op.cc\n+ */\n+void compute_interpolation_weights(const int out_size, const int in_size,\n+                                   const float out2in_scale,\n+                                   CachedInterpolation* interpolation) {\n+  interpolation[out_size].lower = 0;\n+  interpolation[out_size].upper = 0;\n+  const bool clip = true;\n+  if (!compute_interpolation_weights(0, out_size - 1, in_size, out2in_scale,\n+                                     0.0f, clip, interpolation)) {\n+    // Should never happen, check for it anyway\n+    printf(\n+        \"Warning! Interpolation values have lower,upper indexes outside of \"\n+        \"range [0,in_size-1]\\n\");\n+  }\n+}\n+/**\n+ * Compute minimum and maximum (output) i where both lower and upper (input) is\n+ * in range [0,in_size-1]\n+ * If no values of i satisfy condition, min_i = in_size, max_i = -1 and method\n+ * returns false.\n+ * Returns true if min_i >= max_i.\n+ */\n+bool compute_minmax_indexes(const int out_size, const int in_size,\n+                            const float out2in_scale, const float out2in_start,\n+                            int* min_i, int* max_i) {\n+  *min_i = out_size;\n+  *max_i = -1;\n+  int lower, upper;\n+  float lerp;\n+  for (int i = 0; i < out_size; ++i) {\n+    if (compute_single_interpolation_weight(in_size, out2in_scale, out2in_start,\n+                                            false, i, &lower, &upper, &lerp)) {\n+      if (i < *min_i) *min_i = i;\n+      if (i > *max_i) *max_i = i;\n+    }\n+  }\n+  return (*min_i <= *max_i) ? true : false;\n+}\n+/**\n+ * Compute interpolation weights for crop_and_resize_op.cc\n+ * Also computes extrapolation areas.\n+ * Returns true if at least one point requires interpolation, false otherwise.\n+ */\n+bool compute_interpolation_weights(\n+    const int out_size, const int in_size,\n+    const float x1,  // lower bounding box, crop region starts at in_size*x1\n+    const float x2,  // upper bounding box, crop region ends at in_size*x2\n+    int* min_i, int* max_i, std::vector<CachedInterpolation>* interpolation) {\n+  float out2in_start = out_size > 1\n+                           ? (float)(in_size - 1) * (float)x1\n+                           : (float)(in_size - 1) * (float)(x1 + x2) / 2.0f;\n+  float out2in_scale =\n+      out_size > 1\n+          ? (float)(x2 - x1) * (float)(in_size - 1) / (float)(out_size - 1)\n+          : 0.0f;\n+  if (compute_minmax_indexes(out_size, in_size, out2in_scale, out2in_start,\n+                             min_i, max_i)) {\n+    interpolation->resize(*max_i - *min_i + 1);\n+    bool all_inputs_ok = compute_interpolation_weights(\n+        *min_i, *max_i, in_size, out2in_scale, out2in_start, false,\n+        interpolation->data());\n+    if (!all_inputs_ok) {\n+      // should never happen, purpose of compute_minmax_indexes is to ensure\n+      // that all inputs are ok.\n+      printf(\n+          \"Error! compute_interpolation_weights returned input indexes outside \"\n+          \"valid range - SEGV will likely ensue.\\n\");\n+    }\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+/**\n+ * Cast float v to type U with range clamping.\n+ *\n+ * If v<min_val, return value is clamped to u_min_val. similarly if v>max_val,\n+ * return value is clamped to u_max_val.\n+ */\n+template <typename U>\n+U cast_to(float v, float min_val, float max_val, U u_min_val, U u_max_val);\n+template <typename U>\n+U cast_to(float v, float min_val, float max_val, U u_min_val, U u_max_val) {\n+  if (v < min_val)\n+    return u_min_val;\n+  else if (v > max_val)\n+    return u_max_val;\n+  else\n+    return static_cast<U>(v);\n+}\n+/**\n+ * no-op cast from float to float.\n+ */\n+template <>\n+float cast_to<float>(float v, float min_val, float max_val, float u_min_val,\n+                     float u_max_val) {\n+  return v;\n+}\n+\n+float compute_lerp(const float top_left, const float top_right,\n+                   const float bottom_left, const float bottom_right,\n+                   const float x_lerp, const float y_lerp) {\n+  const float top = top_left + (top_right - top_left) * x_lerp;\n+  const float bottom = bottom_left + (bottom_right - bottom_left) * x_lerp;\n+  return top + (bottom - top) * y_lerp;\n+}\n+\n+/**\n+ * Computes the bilinear interpolation from the appropriate 4 float points\n+ * and the linear interpolation weights.\n+ * Accepts input tensors of type T and produces output tensors of type U.\n+ * Optionally flips horizontal and/or vertical axis.\n+ */\n+template <typename T, typename U>\n+void crop_resize_single_image(const T* image, const int64 in_height,\n+                              const int64 in_width, const int64 out_height,\n+                              const int64 out_width, const int channels,\n+                              const int min_ix, const int max_ix,\n+                              const CachedInterpolation* xs, const int min_iy,\n+                              const int max_iy, const CachedInterpolation* ys,\n+                              const float extrapolated_value, const bool flip_x,\n+                              const bool flip_y,\n+                              U* output) TF_ATTRIBUTE_NOINLINE;\n+template <typename T, typename U>\n+void crop_resize_single_image(const T* image, const int64 in_height,\n+                              const int64 in_width, const int64 out_height,\n+                              const int64 out_width, const int channels,\n+                              const int min_ix, const int max_ix,\n+                              const CachedInterpolation* xs, const int min_iy,\n+                              const int max_iy, const CachedInterpolation* ys,\n+                              const float extrapolated_value, const bool flip_x,\n+                              const bool flip_y, U* output) {\n+  const int64 in_row_size = in_width * channels;\n+  const int64 out_row_size = out_width * channels;\n+  U u_min_val = std::numeric_limits<U>::min();\n+  U u_max_val = std::numeric_limits<U>::max();\n+  float min_val = static_cast<float>(u_min_val);\n+  float max_val = static_cast<float>(u_max_val);\n+  U uEx =\n+      cast_to<U>(extrapolated_value, min_val, max_val, u_min_val, u_max_val);\n+  // low y extrapolation zone\n+  if (min_iy > 0) {\n+    U* p = flip_y ? output + out_row_size * (out_height - min_iy) : output;\n+    int64 nn = out_row_size * (int64)min_iy;\n+    for (int64 i = 0; i < nn; ++i) p[i] = uEx;\n+  }\n+  // high y extrapolation zone\n+  if (max_iy < out_height - 1) {\n+    U* p = flip_y ? output : output + out_row_size * (max_iy + 1);\n+    int64 nn = out_row_size * (int64)(out_height - 1 - max_iy);\n+    for (int64 i = 0; i < nn; ++i) p[i] = uEx;\n+  }\n+  // low x extrapolation zone\n+  if (min_ix > 0) {\n+    for (int iy = min_iy; iy <= max_iy; ++iy) {\n+      int xx0 = flip_x ? (out_width - min_ix) * channels : 0;\n+      int nxx = min_ix * channels;\n+      U* p = output + xx0 +\n+             out_row_size * (int64)(flip_y ? out_height - 1 - iy : iy);\n+      for (int ix = 0; ix < nxx; ++ix) {\n+        p[ix] = uEx;\n+      }\n+    }\n+  }\n+  // high x extrapolation zone\n+  if (max_ix < out_width - 1) {\n+    for (int iy = min_iy; iy <= max_iy; ++iy) {\n+      int xx0 = flip_x ? 0 : (max_ix + 1) * channels;\n+      int nxx = (out_width - 1 - max_ix) * channels;\n+      U* p = output + xx0 +\n+             out_row_size * (int64)(flip_y ? out_height - 1 - iy : iy);\n+      for (int ix = 0; ix < nxx; ++ix) {\n+        p[ix] = uEx;\n+      }\n+    }\n+  }\n+  U* output_y_ptr =\n+      output +\n+      out_row_size * (int64)(flip_y ? out_height - 1 - min_iy : min_iy);\n+  // interpolation zone\n+  if (channels == 1) {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+\n+        // Read channel 0.\n+        const float top_left0(ys_input_lower_ptr[xs_lower]);\n+        const float top_right0(ys_input_lower_ptr[xs_upper]);\n+        const float bottom_left0(ys_input_upper_ptr[xs_lower]);\n+        const float bottom_right0(ys_input_upper_ptr[xs_upper]);\n+\n+        // Compute output.\n+        float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                     bottom_right0, xs_lerp, ys_lerp);\n+        output_y_ptr[x] =\n+            cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  } else if (channels == 2) {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+\n+        // Read channel 0.\n+        const float top_left0(ys_input_lower_ptr[xs_lower + 0]);\n+        const float top_right0(ys_input_lower_ptr[xs_upper + 0]);\n+        const float bottom_left0(ys_input_upper_ptr[xs_lower + 0]);\n+        const float bottom_right0(ys_input_upper_ptr[xs_upper + 0]);\n+\n+        // Read channel 1.\n+        const float top_left1(ys_input_lower_ptr[xs_lower + 1]);\n+        const float top_right1(ys_input_lower_ptr[xs_upper + 1]);\n+        const float bottom_left1(ys_input_upper_ptr[xs_lower + 1]);\n+        const float bottom_right1(ys_input_upper_ptr[xs_upper + 1]);\n+\n+        // Compute output.\n+        float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                     bottom_right0, xs_lerp, ys_lerp);\n+        float result1 = compute_lerp(top_left1, top_right1, bottom_left1,\n+                                     bottom_right1, xs_lerp, ys_lerp);\n+        output_y_ptr[x * 2 + 0] =\n+            cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 2 + 1] =\n+            cast_to<U>(result1, min_val, max_val, u_min_val, u_max_val);\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  } else if (channels == 3) {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+\n+        // Read channel 0.\n+        const float top_left0(ys_input_lower_ptr[xs_lower + 0]);\n+        const float top_right0(ys_input_lower_ptr[xs_upper + 0]);\n+        const float bottom_left0(ys_input_upper_ptr[xs_lower + 0]);\n+        const float bottom_right0(ys_input_upper_ptr[xs_upper + 0]);\n+\n+        // Read channel 1.\n+        const float top_left1(ys_input_lower_ptr[xs_lower + 1]);\n+        const float top_right1(ys_input_lower_ptr[xs_upper + 1]);\n+        const float bottom_left1(ys_input_upper_ptr[xs_lower + 1]);\n+        const float bottom_right1(ys_input_upper_ptr[xs_upper + 1]);\n+\n+        // Read channel 2.\n+        const float top_left2(ys_input_lower_ptr[xs_lower + 2]);\n+        const float top_right2(ys_input_lower_ptr[xs_upper + 2]);\n+        const float bottom_left2(ys_input_upper_ptr[xs_lower + 2]);\n+        const float bottom_right2(ys_input_upper_ptr[xs_upper + 2]);\n+\n+        // Compute output.\n+        float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                     bottom_right0, xs_lerp, ys_lerp);\n+        float result1 = compute_lerp(top_left1, top_right1, bottom_left1,\n+                                     bottom_right1, xs_lerp, ys_lerp);\n+        float result2 = compute_lerp(top_left2, top_right2, bottom_left2,\n+                                     bottom_right2, xs_lerp, ys_lerp);\n+        output_y_ptr[x * 3 + 0] =\n+            cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 3 + 1] =\n+            cast_to<U>(result1, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 3 + 2] =\n+            cast_to<U>(result2, min_val, max_val, u_min_val, u_max_val);\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  } else if (channels == 4) {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+\n+        // Read channel 0.\n+        const float top_left0(ys_input_lower_ptr[xs_lower + 0]);\n+        const float top_right0(ys_input_lower_ptr[xs_upper + 0]);\n+        const float bottom_left0(ys_input_upper_ptr[xs_lower + 0]);\n+        const float bottom_right0(ys_input_upper_ptr[xs_upper + 0]);\n+\n+        // Read channel 1.\n+        const float top_left1(ys_input_lower_ptr[xs_lower + 1]);\n+        const float top_right1(ys_input_lower_ptr[xs_upper + 1]);\n+        const float bottom_left1(ys_input_upper_ptr[xs_lower + 1]);\n+        const float bottom_right1(ys_input_upper_ptr[xs_upper + 1]);\n+\n+        // Read channel 2.\n+        const float top_left2(ys_input_lower_ptr[xs_lower + 2]);\n+        const float top_right2(ys_input_lower_ptr[xs_upper + 2]);\n+        const float bottom_left2(ys_input_upper_ptr[xs_lower + 2]);\n+        const float bottom_right2(ys_input_upper_ptr[xs_upper + 2]);\n+\n+        // Read channel 3.\n+        const float top_left3(ys_input_lower_ptr[xs_lower + 3]);\n+        const float top_right3(ys_input_lower_ptr[xs_upper + 3]);\n+        const float bottom_left3(ys_input_upper_ptr[xs_lower + 3]);\n+        const float bottom_right3(ys_input_upper_ptr[xs_upper + 3]);\n+\n+        // Compute output.\n+        float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                     bottom_right0, xs_lerp, ys_lerp);\n+        float result1 = compute_lerp(top_left1, top_right1, bottom_left1,\n+                                     bottom_right1, xs_lerp, ys_lerp);\n+        float result2 = compute_lerp(top_left2, top_right2, bottom_left2,\n+                                     bottom_right2, xs_lerp, ys_lerp);\n+        float result3 = compute_lerp(top_left3, top_right3, bottom_left3,\n+                                     bottom_right3, xs_lerp, ys_lerp);\n+        output_y_ptr[x * 4 + 0] =\n+            cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 4 + 1] =\n+            cast_to<U>(result1, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 4 + 2] =\n+            cast_to<U>(result2, min_val, max_val, u_min_val, u_max_val);\n+        output_y_ptr[x * 4 + 3] =\n+            cast_to<U>(result3, min_val, max_val, u_min_val, u_max_val);\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  } else {\n+    for (int y = min_iy; y <= max_iy; ++y) {\n+      const int iy = y - min_iy;\n+      const T* ys_input_lower_ptr = image + ys[iy].lower * in_row_size;\n+      const T* ys_input_upper_ptr = image + ys[iy].upper * in_row_size;\n+      const float ys_lerp = ys[iy].lerp;\n+      const int x0 = flip_x ? out_width - 1 - max_ix : min_ix;\n+      const int x1 = flip_x ? out_width - 1 - min_ix : max_ix;\n+      for (int x = x0; x <= x1; ++x) {\n+        const int ix = flip_x ? out_width - 1 - min_ix - x : x - min_ix;\n+        const int64 xs_lower = xs[ix].lower;\n+        const int64 xs_upper = xs[ix].upper;\n+        const float xs_lerp = xs[ix].lerp;\n+        for (int ichan = 0; ichan < channels; ++ichan) {\n+          const float top_left0(ys_input_lower_ptr[xs_lower + ichan]);\n+          const float top_right0(ys_input_lower_ptr[xs_upper + ichan]);\n+          const float bottom_left0(ys_input_upper_ptr[xs_lower + ichan]);\n+          const float bottom_right0(ys_input_upper_ptr[xs_upper + ichan]);\n+          float result0 = compute_lerp(top_left0, top_right0, bottom_left0,\n+                                       bottom_right0, xs_lerp, ys_lerp);\n+          output_y_ptr[x * channels + ichan] =\n+              cast_to<U>(result0, min_val, max_val, u_min_val, u_max_val);\n+        }\n+      }\n+      output_y_ptr =\n+          flip_y ? output_y_ptr - out_row_size : output_y_ptr + out_row_size;\n+    }\n+  }\n+}\n+\n+// template for method that calls either explicitly vectorized method\n+// or the fallback method, depending on what is appropriate for the\n+// machine you are running on\n+template <typename T, typename U>\n+void crop_resize_single_image_common(\n+    const T* image, const int64 in_height, const int64 in_width,\n+    const int64 out_height, const int64 out_width, const int channels,\n+    const int min_ix, const int max_ix, const CachedInterpolation* xs,\n+    const int min_iy, const int max_iy, const CachedInterpolation* ys,\n+    const float extrapolated_value, const bool flip_x, const bool flip_y,\n+    U* output) TF_ATTRIBUTE_NOINLINE;\n+\n+// For now, only compile vectorized code on LINUX systems.\n+// to-do: Test vectorized code on other platforms (MacOS and Windows).\n+#if defined(__linux__) && defined(__SSE4_1__)\n+\n+//\n+// The remaining code implements explicitly vectorized versions of a bilinear\n+// image resizer.\n+// Images with 1, 2, 3 or 4 channels are supported.\n+// The image resizer reads samples of type T and writes samples of type U.\n+// T and U can be any of the following: uint8, int8, uint16, int16, int32,\n+// Eigen::half, bfloat16 and float.\n+// There are separate codes for SSE4.1 and AVX2. Enabling AVX2 also enables\n+// FP16C instruction set,\n+// which contains instructions that convert between Eigen::half and float. The\n+// SSE4.1 code path emulates\n+// the FP16C instructions in software.\n+//\n+\n+//\n+// This class loads 4 pixels with n channels, converts to fp32 and packs\n+// the result into n SSE vector words.\n+// Input data type T must be one of uint8, int8, uint16, int16, int32,\n+// Eigen::half, bfloat16 or float.\n+//\n+\n+template <class T>\n+class VectorLoader {\n+ public:\n+#ifdef __AVX2__\n+  // convert 8 packed words of type T to fp32.\n+  // T must be one of uint8, int8, uint16, int16, int32, Eigen::half, bfloat16\n+  // or float.\n+  __m256 to_fp32(__m256i raw);\n+#else\n+  // convert 4 packed words of type T to fp32.\n+  // T must be one of uint8, int8, uint16, int16, int32, Eigen::half, bfloat16\n+  // or float.\n+  __m128 to_fp32(__m128i raw);\n+#endif\n+\n+#ifdef __AVX2__\n+  // pack 4 pixels with 1 channel, 2 channels and 3channels respectively in\n+  // separate 128 bit lanes.\n+  // input is stored in lower portion of 4 separate sse words, v0 through v3.\n+  // output is stored in lower portion of v0.\n+  void pack_1ch(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  // output is stored in lower portion of v0 and v1.\n+  void pack_2ch(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  // output is stored in lower portion of v0, v1 and v2.\n+  void pack_3ch(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+#else\n+  // pack 4 pixels with 1 channel, 2 channels and 3channels respectively.\n+  // input is stored in lower portion of 4 separate sse words, v0 through v3.\n+  // output is stored in lower portion of v0.\n+  void pack_1ch(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  // output is stored in lower portion of v0 and v1.\n+  void pack_2ch(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  // output is stored in lower portion of v0, v1 and v2.\n+  void pack_3ch(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+#endif\n+\n+#ifdef __AVX2__\n+  // extract right pixel for load1 and load4 cases.\n+  __m256i extract_right_1ch(const __m256i left);\n+  __m256i extract_right_2ch(const __m256i left);\n+  __m256i extract_right_3ch(const __m256i left);\n+  __m256i extract_right_4ch(const __m256i left);\n+#else\n+  __m128i extract_right_1ch(const __m128i left);\n+  __m128i extract_right_2ch(const __m128i left);\n+  __m128i extract_right_3ch(const __m128i left);\n+  __m128i extract_right_4ch(const __m128i left);\n+#endif\n+\n+#ifdef __AVX2__\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 1 channel.\n+  // load1 case, i.e. 4 left and right inputs are loaded with a single unaligned\n+  // SSE load.\n+  void load1_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256* left0, __m256* right0);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 2 channels.\n+  // load1 case, i.e. 4 left and right inputs are loaded with a single unaligned\n+  // SSE load.\n+  void load1_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256* left0, __m256* left1,\n+                 __m256* right0, __m256* right1);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 3 channels.\n+  // load1 case, i.e. 4 left and right inputs are loaded with a single unaligned\n+  // SSE load.\n+  void load1_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256* left0, __m256* left1,\n+                 __m256* left2, __m256* right0, __m256* right1, __m256* right2);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 4 channels.\n+  // load1 case, i.e. 4 left and right inputs are loaded with a single unaligned\n+  // SSE load.\n+  void load1_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256* left0, __m256* left1,\n+                 __m256* left2, __m256* left3, __m256* right0, __m256* right1,\n+                 __m256* right2, __m256* right3);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 1 channel.\n+  // load2 case, i.e. 4 left inputs are loaded with first SSE load and 4 right\n+  // inputs are loaded with second SSE load.\n+  void load2_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256* left0, __m256* right0);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 2 channels.\n+  // load2 case, i.e. 4 left inputs are loaded with first SSE load and 4 right\n+  // inputs are loaded with second SSE load.\n+  void load2_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256* left0, __m256* left1,\n+                 __m256* right0, __m256* right1);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 3 channels.\n+  // load2 case, i.e. 4 left inputs are loaded with first SSE load and 4 right\n+  // inputs are loaded with second SSE load.\n+  void load2_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256* left0, __m256* left1,\n+                 __m256* left2, __m256* right0, __m256* right1, __m256* right2);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 4 channels.\n+  // load2 case, i.e. 4 left inputs are loaded with first SSE load and 4 right\n+  // inputs are loaded with second SSE load.\n+  void load2_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m256* left0, __m256* left1,\n+                 __m256* left2, __m256* left3, __m256* right0, __m256* right1,\n+                 __m256* right2, __m256* right3);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 1 channel.\n+  // load4 case, i.e. each pair of left and right inputs are loaded with a\n+  // separate SSE load.\n+  void load4_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256* left0,\n+                 __m256* right0);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 2 channels.\n+  // load4 case, i.e. each pair of left and right inputs are loaded with a\n+  // separate SSE load.\n+  void load4_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256* left0,\n+                 __m256* left1, __m256* right0, __m256* right1);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 3 channels.\n+  // load4 case, i.e. each pair of left and right inputs are loaded with a\n+  // separate SSE load.\n+  void load4_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256* left0,\n+                 __m256* left1, __m256* left2, __m256* right0, __m256* right1,\n+                 __m256* right2);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 4 channels.\n+  // load4 case, i.e. each pair of left and right inputs are loaded with a\n+  // separate SSE load.\n+  void load4_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256* left0,\n+                 __m256* left1, __m256* left2, __m256* left3, __m256* right0,\n+                 __m256* right1, __m256* right2, __m256* right3);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 1 channel.\n+  // load8 case, i.e. each input is loaded with a separate SSE load.\n+  // 4 pixels, each with left and right input necessitates 8 separate SSE loads\n+  // per input row.\n+  void load8_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256* left0,\n+                 __m256* right0);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 2 channels.\n+  // load8 case, i.e. each input is loaded with a separate SSE load.\n+  // 4 pixels, each with left and right input necessitates 8 separate SSE loads\n+  // per input row.\n+  void load8_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256* left0,\n+                 __m256* left1, __m256* right0, __m256* right1);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 3 channels.\n+  // load8 case, i.e. each input is loaded with a separate SSE load.\n+  // 4 pixels, each with left and right input necessitates 8 separate SSE loads\n+  // per input row.\n+  void load8_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256* left0,\n+                 __m256* left1, __m256* left2, __m256* right0, __m256* right1,\n+                 __m256* right2);\n+  // load top left and bottom left interpolation inputs into output argument\n+  // left.\n+  // load top right and bottom right interpolation inputs into output argument\n+  // right.\n+  // pixels have 4 channels.\n+  // load8 case, i.e. each input is loaded with a separate SSE load.\n+  // 4 pixels, each with left and right input necessitates 8 separate SSE loads\n+  // per input row.\n+  void load8_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m256* left0,\n+                 __m256* left1, __m256* left2, __m256* left3, __m256* right0,\n+                 __m256* right1, __m256* right2, __m256* right3);\n+#else\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 1 channel.\n+  // load1 case, i.e. all inputs for one input row are loaded with a single SSE\n+  // load.\n+  void load1_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128* tl0, __m128* bl0,\n+                 __m128* tr0, __m128* br0);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 2 channels.\n+  // load1 case, i.e. all inputs for one input row are loaded with a single SSE\n+  // load.\n+  void load1_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128* tl0, __m128* tl1,\n+                 __m128* bl0, __m128* bl1, __m128* tr0, __m128* tr1,\n+                 __m128* br0, __m128* br1);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 3 channels.\n+  // load1 case, i.e. all inputs for one input row are loaded with a single SSE\n+  // load.\n+  void load1_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128* tl0, __m128* tl1,\n+                 __m128* tl2, __m128* bl0, __m128* bl1, __m128* bl2,\n+                 __m128* tr0, __m128* tr1, __m128* tr2, __m128* br0,\n+                 __m128* br1, __m128* br2);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 4 channels.\n+  // load1 case, i.e. all inputs for one input row are loaded with a single SSE\n+  // load.\n+  void load1_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128* tl0, __m128* tl1,\n+                 __m128* tl2, __m128* tl3, __m128* bl0, __m128* bl1,\n+                 __m128* bl2, __m128* bl3, __m128* tr0, __m128* tr1,\n+                 __m128* tr2, __m128* tr3, __m128* br0, __m128* br1,\n+                 __m128* br2, __m128* br3);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 1 channel.\n+  // load2 case, i.e. left inputs are loaded with first SSE load, right inputs\n+  // are loaded with second SSE load.\n+  void load2_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128* tl0, __m128* bl0,\n+                 __m128* tr0, __m128* br0);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 2 channels.\n+  // load2 case, i.e. left inputs are loaded with first SSE load, right inputs\n+  // are loaded with second SSE load.\n+  void load2_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128* tl0, __m128* tl1,\n+                 __m128* bl0, __m128* bl1, __m128* tr0, __m128* tr1,\n+                 __m128* br0, __m128* br1);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 3 channels.\n+  // load2 case, i.e. left inputs are loaded with first SSE load, right inputs\n+  // are loaded with second SSE load.\n+  void load2_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128* tl0, __m128* tl1,\n+                 __m128* tl2, __m128* bl0, __m128* bl1, __m128* bl2,\n+                 __m128* tr0, __m128* tr1, __m128* tr2, __m128* br0,\n+                 __m128* br1, __m128* br2);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 4 channels.\n+  // load2 case, i.e. left inputs are loaded with first SSE load, right inputs\n+  // are loaded with second SSE load.\n+  void load2_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 const __m128i* shuffle_masks, __m128* tl0, __m128* tl1,\n+                 __m128* tl2, __m128* tl3, __m128* bl0, __m128* bl1,\n+                 __m128* bl2, __m128* bl3, __m128* tr0, __m128* tr1,\n+                 __m128* tr2, __m128* tr3, __m128* br0, __m128* br1,\n+                 __m128* br2, __m128* br3);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 1 channel.\n+  // load4 case, i.e. left and right inputs are loaded with a separate SSE load\n+  // for each pixel.\n+  void load4_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128* tl0,\n+                 __m128* bl0, __m128* tr0, __m128* br0);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 2 channels.\n+  // load4 case, i.e. left and right inputs are loaded with a separate SSE load\n+  // for each pixel.\n+  void load4_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128* tl0,\n+                 __m128* tl1, __m128* bl0, __m128* bl1, __m128* tr0,\n+                 __m128* tr1, __m128* br0, __m128* br1);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 3 channels.\n+  // load4 case, i.e. left and right inputs are loaded with a separate SSE load\n+  // for each pixel.\n+  void load4_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128* tl0,\n+                 __m128* tl1, __m128* tl2, __m128* bl0, __m128* bl1,\n+                 __m128* bl2, __m128* tr0, __m128* tr1, __m128* tr2,\n+                 __m128* br0, __m128* br1, __m128* br2);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 4 channels.\n+  // load4 case, i.e. left and right inputs are loaded with a separate SSE load\n+  // for each pixel.\n+  void load4_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128* tl0,\n+                 __m128* tl1, __m128* tl2, __m128* tl3, __m128* bl0,\n+                 __m128* bl1, __m128* bl2, __m128* bl3, __m128* tr0,\n+                 __m128* tr1, __m128* tr2, __m128* tr3, __m128* br0,\n+                 __m128* br1, __m128* br2, __m128* br3);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 1 channel.\n+  // load8 case, i.e. left and right inputs are loaded with separate SSE loads\n+  // for each pixel.\n+  void load8_1ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128* tl0,\n+                 __m128* bl0, __m128* tr0, __m128* br0);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 2 channels.\n+  // load8 case, i.e. left and right inputs are loaded with separate SSE loads\n+  // for each pixel.\n+  void load8_2ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128* tl0,\n+                 __m128* tl1, __m128* bl0, __m128* bl1, __m128* tr0,\n+                 __m128* tr1, __m128* br0, __m128* br1);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 3 channels.\n+  // load8 case, i.e. left and right inputs are loaded with separate SSE loads\n+  // for each pixel.\n+  void load8_3ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128* tl0,\n+                 __m128* tl1, __m128* tl2, __m128* bl0, __m128* bl1,\n+                 __m128* bl2, __m128* tr0, __m128* tr1, __m128* tr2,\n+                 __m128* br0, __m128* br1, __m128* br2);\n+  // load top left interpolation inputs into output argument tl.\n+  // load bottom left interpolation inputs into output argument bl.\n+  // load top right interpolation inputs into output argument tr.\n+  // load bottom right interpolation inputs into output argument br.\n+  // pixels have 4 channels.\n+  // load8 case, i.e. left and right inputs are loaded with separate SSE loads\n+  // for each pixel.\n+  void load8_4ch(const T* lower_ptr, const T* upper_ptr, int offset0,\n+                 int offset1, int offset2, int offset3, __m128* tl0,\n+                 __m128* tl1, __m128* tl2, __m128* tl3, __m128* bl0,\n+                 __m128* bl1, __m128* bl2, __m128* bl3, __m128* tr0,\n+                 __m128* tr1, __m128* tr2, __m128* tr3, __m128* br0,\n+                 __m128* br1, __m128* br2, __m128* br3);\n+#endif\n+\n+  // there is no method that packs 4 pixels with 4 channel into four sse words.\n+  // nothing to do for this case, everything is already in the right position.\n+\n+ private:\n+// helper methods\n+#ifdef __AVX2__\n+  // pack 4 pixels with 1, 2, 3 or 4 channels into lower portion of SSE vector\n+  // word.\n+  // works within SSE lanes.\n+  // sizeof(sample_data_type) can be 1, 2 or 4 bytes.\n+  void pack4_1b_1ch_(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  void pack4_2b_1ch_(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  void pack4_4b_1ch_(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  void pack4_1b_2ch_(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  void pack4_2b_2ch_(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  void pack4_4b_2ch_(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  void pack4_1b_3ch_(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  void pack4_2b_3ch_(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+  void pack4_4b_3ch_(__m256i* v0, __m256i* v1, __m256i* v2, __m256i* v3);\n+// there is no pack4_xx_4ch functions because none is needed.\n+// all the bytes are loaded in the right spots for this case.\n+#else\n+  // pack 4 pixels with 1, 2, 3 or 4 channels into lower portion of SSE vector\n+  // word.\n+  // sizeof(sample_data_type) can be 1, 2 or 4 bytes.\n+  void pack4_1b_1ch_(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  void pack4_2b_1ch_(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  void pack4_4b_1ch_(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  void pack4_1b_2ch_(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  void pack4_2b_2ch_(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  void pack4_4b_2ch_(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  void pack4_1b_3ch_(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  void pack4_2b_3ch_(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+  void pack4_4b_3ch_(__m128i* v0, __m128i* v1, __m128i* v2, __m128i* v3);\n+#endif\n+#ifdef __AVX2__\n+  __m256i extract_right_1b_(const __m256i left);\n+  __m256i extract_right_2b_(const __m256i left);\n+  __m256i extract_right_3b_(const __m256i left);\n+  __m256i extract_right_4b_(const __m256i left);\n+  __m256i extract_right_6b_(const __m256i left);\n+  __m256i extract_right_8b_(const __m256i left);\n+#else\n+  __m128i extract_right_1b_(const __m128i left);\n+  __m128i extract_right_2b_(const __m128i left);\n+  __m128i extract_right_3b_(const __m128i left);\n+  __m128i extract_right_4b_(const __m128i left);\n+  __m128i extract_right_6b_(const __m128i left);\n+  __m128i extract_right_8b_(const __m128i left);\n+#endif\n+};\n+\n+#ifdef __AVX2__\n+template <class T>\n+void VectorLoader<T>::pack4_1b_1ch_(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  *v3 = _mm256_slli_si256(*v3, 3);\n+  __m256i and_mask = _mm256_setr_epi32(255, 0, 0, 0, 255, 0, 0, 0);\n+  *v2 = _mm256_or_si256(*v3,\n+                        _mm256_slli_si256(_mm256_and_si256(and_mask, *v2), 2));\n+  *v1 = _mm256_or_si256(*v2,\n+                        _mm256_slli_si256(_mm256_and_si256(and_mask, *v1), 1));\n+  *v0 = _mm256_or_si256(*v1, _mm256_and_si256(and_mask, *v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_1ch_(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  *v3 = _mm256_slli_si256(*v3, 6);\n+  __m256i and_mask = _mm256_setr_epi32(65535, 0, 0, 0, 65535, 0, 0, 0);\n+  *v2 = _mm256_or_si256(*v3,\n+                        _mm256_slli_si256(_mm256_and_si256(and_mask, *v2), 4));\n+  *v1 = _mm256_or_si256(*v2,\n+                        _mm256_slli_si256(_mm256_and_si256(and_mask, *v1), 2));\n+  *v0 = _mm256_or_si256(*v1, _mm256_and_si256(and_mask, *v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_1ch_(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  *v3 = _mm256_slli_si256(*v3, 12);\n+  __m256i and_mask = _mm256_setr_epi32(-1, 0, 0, 0, -1, 0, 0, 0);\n+  *v2 = _mm256_or_si256(*v3,\n+                        _mm256_slli_si256(_mm256_and_si256(and_mask, *v2), 8));\n+  *v1 = _mm256_or_si256(*v2,\n+                        _mm256_slli_si256(_mm256_and_si256(and_mask, *v1), 4));\n+  *v0 = _mm256_or_si256(*v1, _mm256_and_si256(and_mask, *v0));\n+}\n+\n+template <class T>\n+void VectorLoader<T>::pack4_1b_2ch_(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  __m256i and_mask = _mm256_setr_epi32(65535, 0, 0, 0, 65535, 0, 0, 0);\n+  *v0 = _mm256_or_si256(_mm256_and_si256(*v0, and_mask),\n+                        _mm256_slli_si256(*v1, 2));\n+  *v1 = _mm256_or_si256(_mm256_and_si256(*v2, and_mask),\n+                        _mm256_slli_si256(*v3, 2));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_2ch_(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  __m256i and_mask = _mm256_setr_epi32(-1, 0, 0, 0, -1, 0, 0, 0);\n+  *v0 = _mm256_or_si256(_mm256_and_si256(*v0, and_mask),\n+                        _mm256_slli_si256(*v1, 4));\n+  *v1 = _mm256_or_si256(_mm256_and_si256(*v2, and_mask),\n+                        _mm256_slli_si256(*v3, 4));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_2ch_(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  __m256i and_mask = _mm256_setr_epi32(-1, -1, 0, 0, -1, -1, 0, 0);\n+  *v0 = _mm256_or_si256(_mm256_and_si256(*v0, and_mask),\n+                        _mm256_slli_si256(*v1, 8));\n+  *v1 = _mm256_or_si256(_mm256_and_si256(*v2, and_mask),\n+                        _mm256_slli_si256(*v3, 8));\n+}\n+\n+template <class T>\n+void VectorLoader<T>::pack4_1b_3ch_(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  __m256i and_mask = _mm256_setr_epi32(16777215, 0, 0, 0, 16777215, 0, 0, 0);\n+  *v0 = _mm256_or_si256(_mm256_and_si256(*v0, and_mask),\n+                        _mm256_slli_si256(*v1, 3));\n+  and_mask = _mm256_srli_si256(and_mask, 1);\n+  *v1 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(*v1, 1), and_mask),\n+                        _mm256_slli_si256(*v2, 2));\n+  and_mask = _mm256_srli_si256(and_mask, 1);\n+  *v2 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(*v2, 2), and_mask),\n+                        _mm256_slli_si256(*v3, 1));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_3ch_(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  __m256i and_mask = _mm256_setr_epi32(-1, 65535, 0, 0, -1, 65535, 0, 0);\n+  *v0 = _mm256_or_si256(_mm256_and_si256(*v0, and_mask),\n+                        _mm256_slli_si256(*v1, 6));\n+  and_mask = _mm256_srli_si256(and_mask, 2);\n+  *v1 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(*v1, 2), and_mask),\n+                        _mm256_slli_si256(*v2, 4));\n+  and_mask = _mm256_srli_si256(and_mask, 2);\n+  *v2 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(*v2, 4), and_mask),\n+                        _mm256_slli_si256(*v3, 2));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_3ch_(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  __m256i and_mask = _mm256_setr_epi32(-1, -1, -1, 0, -1, -1, -1, 0);\n+  *v0 = _mm256_or_si256(_mm256_and_si256(*v0, and_mask),\n+                        _mm256_slli_si256(*v1, 12));\n+  and_mask = _mm256_srli_si256(and_mask, 4);\n+  *v1 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(*v1, 4), and_mask),\n+                        _mm256_slli_si256(*v2, 8));\n+  and_mask = _mm256_srli_si256(and_mask, 4);\n+  *v2 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_si256(*v2, 8), and_mask),\n+                        _mm256_slli_si256(*v3, 4));\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_1ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_1b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_1ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                  __m256i* v3) {\n+  pack4_1b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_1ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_1ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_1ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_4b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_1ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                         __m256i* v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_1ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                      __m256i* v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_1ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_4b_1ch_(v0, v1, v2, v3);\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_2ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_1b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_2ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                  __m256i* v3) {\n+  pack4_1b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_2ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_2ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_2ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_4b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_2ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                         __m256i* v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_2ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                      __m256i* v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_2ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_4b_2ch_(v0, v1, v2, v3);\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_3ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_1b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_3ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                  __m256i* v3) {\n+  pack4_1b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_3ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                    __m256i* v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_3ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_3ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_4b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_3ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                         __m256i* v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_3ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                      __m256i* v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_3ch(__m256i* v0, __m256i* v1, __m256i* v2,\n+                                   __m256i* v3) {\n+  pack4_4b_3ch_(v0, v1, v2, v3);\n+}\n+#else\n+template <class T>\n+void VectorLoader<T>::pack4_1b_1ch_(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  *v3 = _mm_slli_si128(*v3, 3);\n+  __m128i and_mask = _mm_setr_epi32(255, 0, 0, 0);\n+  *v2 = _mm_or_si128(*v3, _mm_slli_si128(_mm_and_si128(and_mask, *v2), 2));\n+  *v1 = _mm_or_si128(*v2, _mm_slli_si128(_mm_and_si128(and_mask, *v1), 1));\n+  *v0 = _mm_or_si128(*v1, _mm_and_si128(and_mask, *v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_1ch_(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  *v3 = _mm_slli_si128(*v3, 6);\n+  __m128i and_mask = _mm_setr_epi32(65535, 0, 0, 0);\n+  *v2 = _mm_or_si128(*v3, _mm_slli_si128(_mm_and_si128(and_mask, *v2), 4));\n+  *v1 = _mm_or_si128(*v2, _mm_slli_si128(_mm_and_si128(and_mask, *v1), 2));\n+  *v0 = _mm_or_si128(*v1, _mm_and_si128(and_mask, *v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_1ch_(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  *v3 = _mm_slli_si128(*v3, 12);\n+  __m128i and_mask = _mm_setr_epi32(-1, 0, 0, 0);\n+  *v2 = _mm_or_si128(*v3, _mm_slli_si128(_mm_and_si128(and_mask, *v2), 8));\n+  *v1 = _mm_or_si128(*v2, _mm_slli_si128(_mm_and_si128(and_mask, *v1), 4));\n+  *v0 = _mm_or_si128(*v1, _mm_and_si128(and_mask, *v0));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_1b_2ch_(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  __m128i and_mask = _mm_setr_epi32(65535, 0, 0, 0);\n+  *v0 = _mm_or_si128(_mm_and_si128(*v0, and_mask), _mm_slli_si128(*v1, 2));\n+  *v1 = _mm_or_si128(_mm_and_si128(*v2, and_mask), _mm_slli_si128(*v3, 2));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_2ch_(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  __m128i and_mask = _mm_setr_epi32(-1, 0, 0, 0);\n+  *v0 = _mm_or_si128(_mm_and_si128(*v0, and_mask), _mm_slli_si128(*v1, 4));\n+  *v1 = _mm_or_si128(_mm_and_si128(*v2, and_mask), _mm_slli_si128(*v3, 4));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_2ch_(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  __m128i and_mask = _mm_setr_epi32(-1, -1, 0, 0);\n+  *v0 = _mm_or_si128(_mm_and_si128(*v0, and_mask), _mm_slli_si128(*v1, 8));\n+  *v1 = _mm_or_si128(_mm_and_si128(*v2, and_mask), _mm_slli_si128(*v3, 8));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_1b_3ch_(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  __m128i and_mask = _mm_setr_epi32(16777215, 0, 0, 0);\n+  *v0 = _mm_or_si128(_mm_and_si128(*v0, and_mask), _mm_slli_si128(*v1, 3));\n+  and_mask = _mm_srli_si128(and_mask, 1);\n+  *v1 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(*v1, 1), and_mask),\n+                     _mm_slli_si128(*v2, 2));\n+  and_mask = _mm_srli_si128(and_mask, 1);\n+  *v2 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(*v2, 2), and_mask),\n+                     _mm_slli_si128(*v3, 1));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_2b_3ch_(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  __m128i and_mask = _mm_setr_epi32(-1, 65535, 0, 0);\n+  *v0 = _mm_or_si128(_mm_and_si128(*v0, and_mask), _mm_slli_si128(*v1, 6));\n+  and_mask = _mm_srli_si128(and_mask, 2);\n+  *v1 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(*v1, 2), and_mask),\n+                     _mm_slli_si128(*v2, 4));\n+  and_mask = _mm_srli_si128(and_mask, 2);\n+  *v2 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(*v2, 4), and_mask),\n+                     _mm_slli_si128(*v3, 2));\n+}\n+template <class T>\n+void VectorLoader<T>::pack4_4b_3ch_(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  __m128i and_mask = _mm_setr_epi32(-1, -1, -1, 0);\n+  *v0 = _mm_or_si128(_mm_and_si128(*v0, and_mask), _mm_slli_si128(*v1, 12));\n+  and_mask = _mm_srli_si128(and_mask, 4);\n+  *v1 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(*v1, 4), and_mask),\n+                     _mm_slli_si128(*v2, 8));\n+  and_mask = _mm_srli_si128(and_mask, 4);\n+  *v2 = _mm_or_si128(_mm_and_si128(_mm_srli_si128(*v2, 8), and_mask),\n+                     _mm_slli_si128(*v3, 4));\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_1ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_1b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_1ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                  __m128i* v3) {\n+  pack4_1b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_1ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_1ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_1ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_4b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_1ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                         __m128i* v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_1ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                      __m128i* v3) {\n+  pack4_2b_1ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_1ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_4b_1ch_(v0, v1, v2, v3);\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_2ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_1b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_2ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                  __m128i* v3) {\n+  pack4_1b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_2ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_2ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_2ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_4b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_2ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                         __m128i* v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_2ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                      __m128i* v3) {\n+  pack4_2b_2ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_2ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_4b_2ch_(v0, v1, v2, v3);\n+}\n+\n+template <>\n+void VectorLoader<uint8>::pack_3ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_1b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int8>::pack_3ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                  __m128i* v3) {\n+  pack4_1b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<uint16>::pack_3ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                    __m128i* v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int16>::pack_3ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<int32>::pack_3ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                   __m128i* v3) {\n+  pack4_4b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<Eigen::half>::pack_3ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                         __m128i* v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<bfloat16>::pack_3ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+                                      __m128i* v3) {\n+  pack4_2b_3ch_(v0, v1, v2, v3);\n+}\n+template <>\n+void VectorLoader<float>::pack_3ch(__m128i* v0, __m128i* v1, __m128i* v2,\n+     ", "path": "tensorflow/core/kernels/crop_resize_bilinear_core.h", "position": null, "original_position": 3622, "commit_id": "2bef68f68bb6dbbefee151ea853b92ac17bb5acc", "original_commit_id": "848a72432d2093893bc3a70ed41d528876ffa324", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "body": "This assertion is failing in one of the tests:\r\n\r\nF1002 18:47:12.186167    5518 logging.cc:80] assert.h assertion failed at ./third_party/tensorflow/core/kernels/crop_resize_bilinear_core.h:3621 in tensorflow::(anonymous namespace)::CropResizeCastImage<float, float>::CropResizeCastImage(const int, const int, const int, const int, const int, const int, const int, const tensorflow::(anonymous namespace)::CachedInterpolation *, const int, const int, const tensorflow::(anonymous namespace)::CachedInterpolation *, const float, const bool, const bool, const bool, const int) [T = float, U = float]: min_ix_ <= max_ix_\r\n*** Check failure stack trace: ***\r\n    @     0x559e7befccd2  base_logging::LogMessage::SendToLog()\r\n    @     0x559e7befd4d3  base_logging::LogMessage::Flush()\r\n    @     0x559e7beff3d5  base_logging::LogMessageFatal::~LogMessageFatal()\r\n    @     0x559e7befa590  __assert_fail\r\n    @     0x559e72986c68  tensorflow::(anonymous namespace)::CropResizeCastImage<>::CropResizeCastImage()\r\n    @     0x559e729868f0  tensorflow::(anonymous namespace)::crop_resize_single_image_common<>()\r\n    @     0x559e729b99eb  tensorflow::functor::CropAndResize<>::operator()()::{lambda()#1}::operator()()\r\n    @     0x559e729b947e  std::_Function_handler<>::_M_invoke()\r\n    @     0x559e7747d502  std::function<>::operator()()\r\n    @     0x559e7a064168  tensorflow::thread::ThreadPool::Impl::ParallelFor()::{lambda()#1}::operator()()\r\n    @     0x559e7a063fda  std::_Function_handler<>::_M_invoke()\r\n    @     0x559e709b87f2  std::function<>::operator()()\r\n    @     0x559e709b7149  Eigen::ThreadPoolDevice::parallelFor()\r\n    @     0x559e709b80a5  Eigen::ThreadPoolDevice::parallelFor()\r\n    @     0x559e7a05bfc7  tensorflow::thread::ThreadPool::Impl::ParallelFor()\r\n    @     0x559e7a05acef  tensorflow::thread::ThreadPool::ParallelFor()\r\n    @     0x559e79ee3380  tensorflow::Shard()\r\n    @     0x559e729b9314  tensorflow::functor::CropAndResize<>::operator()()\r\n    @     0x559e729b8eb9  tensorflow::CropAndResizeOp<>::ComputeAsync()::{lambda()#1}::operator()()\r\n    @     0x559e729b8bdd  std::_Function_handler<>::_M_invoke()\r\n    @     0x559e6e44ba3d  std::function<>::operator()()\r\n    @     0x559e72911ab3  tensorflow::(anonymous namespace)::RunIfBoxIndexIsValid<>()\r\n    @     0x559e729b8ab9  tensorflow::CropAndResizeOp<>::ComputeAsync()\r\n    @     0x559e76c285ed  tensorflow::Device::ComputeAsync()\r\n    @     0x559e79c168b6  tensorflow::(anonymous namespace)::ExecutorState::Process()\r\n    @     0x559e79c17395  tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady()::$_1::operator()()\r\n    @     0x559e79c171bd  std::_Function_handler<>::_M_invoke()\r\n    @     0x559e6e44ba3d  std::function<>::operator()()\r\n    @     0x559e7a05ed85  tensorflow::thread::EigenEnvironment::ExecuteTask()\r\n    @     0x559e7a05e0d9  Eigen::ThreadPoolTempl<>::WorkerLoop()\r\n    @     0x559e7a05dd1e  Eigen::ThreadPoolTempl<>::ThreadPoolTempl()::{lambda()#1}::operator()()\r\n    @     0x559e7a05db8d  std::_Function_handler<>::_M_invoke()\r\n*", "created_at": "2018-10-03T17:34:07Z", "updated_at": "2018-11-05T16:08:34Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21681#discussion_r222399629", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21681", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/222399629"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21681#discussion_r222399629"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21681"}}, "body_html": "<p>This assertion is failing in one of the tests:</p>\n<p>F1002 18:47:12.186167    5518 logging.cc:80] assert.h assertion failed at ./third_party/tensorflow/core/kernels/crop_resize_bilinear_core.h:3621 in tensorflow::(anonymous namespace)::CropResizeCastImage&lt;float, float&gt;::CropResizeCastImage(const int, const int, const int, const int, const int, const int, const int, const tensorflow::(anonymous namespace)::CachedInterpolation *, const int, const int, const tensorflow::(anonymous namespace)::CachedInterpolation *, const float, const bool, const bool, const bool, const int) [T = float, U = float]: min_ix_ &lt;= max_ix_<br>\n*** Check failure stack trace: ***<br>\n@     0x559e7befccd2  base_logging::LogMessage::SendToLog()<br>\n@     0x559e7befd4d3  base_logging::LogMessage::Flush()<br>\n@     0x559e7beff3d5  base_logging::LogMessageFatal::~LogMessageFatal()<br>\n@     0x559e7befa590  __assert_fail<br>\n@     0x559e72986c68  tensorflow::(anonymous namespace)::CropResizeCastImage&lt;&gt;::CropResizeCastImage()<br>\n@     0x559e729868f0  tensorflow::(anonymous namespace)::crop_resize_single_image_common&lt;&gt;()<br>\n@     0x559e729b99eb  tensorflow::functor::CropAndResize&lt;&gt;::operator()()::{lambda()<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a>}::operator()()<br>\n@     0x559e729b947e  std::_Function_handler&lt;&gt;::_M_invoke()<br>\n@     0x559e7747d502  std::function&lt;&gt;::operator()()<br>\n@     0x559e7a064168  tensorflow::thread::ThreadPool::Impl::ParallelFor()::{lambda()<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a>}::operator()()<br>\n@     0x559e7a063fda  std::_Function_handler&lt;&gt;::_M_invoke()<br>\n@     0x559e709b87f2  std::function&lt;&gt;::operator()()<br>\n@     0x559e709b7149  Eigen::ThreadPoolDevice::parallelFor()<br>\n@     0x559e709b80a5  Eigen::ThreadPoolDevice::parallelFor()<br>\n@     0x559e7a05bfc7  tensorflow::thread::ThreadPool::Impl::ParallelFor()<br>\n@     0x559e7a05acef  tensorflow::thread::ThreadPool::ParallelFor()<br>\n@     0x559e79ee3380  tensorflow::Shard()<br>\n@     0x559e729b9314  tensorflow::functor::CropAndResize&lt;&gt;::operator()()<br>\n@     0x559e729b8eb9  tensorflow::CropAndResizeOp&lt;&gt;::ComputeAsync()::{lambda()<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a>}::operator()()<br>\n@     0x559e729b8bdd  std::_Function_handler&lt;&gt;::_M_invoke()<br>\n@     0x559e6e44ba3d  std::function&lt;&gt;::operator()()<br>\n@     0x559e72911ab3  tensorflow::(anonymous namespace)::RunIfBoxIndexIsValid&lt;&gt;()<br>\n@     0x559e729b8ab9  tensorflow::CropAndResizeOp&lt;&gt;::ComputeAsync()<br>\n@     0x559e76c285ed  tensorflow::Device::ComputeAsync()<br>\n@     0x559e79c168b6  tensorflow::(anonymous namespace)::ExecutorState::Process()<br>\n@     0x559e79c17395  tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady()::$_1::operator()()<br>\n@     0x559e79c171bd  std::_Function_handler&lt;&gt;::_M_invoke()<br>\n@     0x559e6e44ba3d  std::function&lt;&gt;::operator()()<br>\n@     0x559e7a05ed85  tensorflow::thread::EigenEnvironment::ExecuteTask()<br>\n@     0x559e7a05e0d9  Eigen::ThreadPoolTempl&lt;&gt;::WorkerLoop()<br>\n@     0x559e7a05dd1e  Eigen::ThreadPoolTempl&lt;&gt;::ThreadPoolTempl()::{lambda()<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a>}::operator()()<br>\n@     0x559e7a05db8d  std::_Function_handler&lt;&gt;::_M_invoke()<br>\n*</p>", "body_text": "This assertion is failing in one of the tests:\nF1002 18:47:12.186167    5518 logging.cc:80] assert.h assertion failed at ./third_party/tensorflow/core/kernels/crop_resize_bilinear_core.h:3621 in tensorflow::(anonymous namespace)::CropResizeCastImage<float, float>::CropResizeCastImage(const int, const int, const int, const int, const int, const int, const int, const tensorflow::(anonymous namespace)::CachedInterpolation *, const int, const int, const tensorflow::(anonymous namespace)::CachedInterpolation *, const float, const bool, const bool, const bool, const int) [T = float, U = float]: min_ix_ <= max_ix_\n*** Check failure stack trace: ***\n@     0x559e7befccd2  base_logging::LogMessage::SendToLog()\n@     0x559e7befd4d3  base_logging::LogMessage::Flush()\n@     0x559e7beff3d5  base_logging::LogMessageFatal::~LogMessageFatal()\n@     0x559e7befa590  __assert_fail\n@     0x559e72986c68  tensorflow::(anonymous namespace)::CropResizeCastImage<>::CropResizeCastImage()\n@     0x559e729868f0  tensorflow::(anonymous namespace)::crop_resize_single_image_common<>()\n@     0x559e729b99eb  tensorflow::functor::CropAndResize<>::operator()()::{lambda()#1}::operator()()\n@     0x559e729b947e  std::_Function_handler<>::_M_invoke()\n@     0x559e7747d502  std::function<>::operator()()\n@     0x559e7a064168  tensorflow::thread::ThreadPool::Impl::ParallelFor()::{lambda()#1}::operator()()\n@     0x559e7a063fda  std::_Function_handler<>::_M_invoke()\n@     0x559e709b87f2  std::function<>::operator()()\n@     0x559e709b7149  Eigen::ThreadPoolDevice::parallelFor()\n@     0x559e709b80a5  Eigen::ThreadPoolDevice::parallelFor()\n@     0x559e7a05bfc7  tensorflow::thread::ThreadPool::Impl::ParallelFor()\n@     0x559e7a05acef  tensorflow::thread::ThreadPool::ParallelFor()\n@     0x559e79ee3380  tensorflow::Shard()\n@     0x559e729b9314  tensorflow::functor::CropAndResize<>::operator()()\n@     0x559e729b8eb9  tensorflow::CropAndResizeOp<>::ComputeAsync()::{lambda()#1}::operator()()\n@     0x559e729b8bdd  std::_Function_handler<>::_M_invoke()\n@     0x559e6e44ba3d  std::function<>::operator()()\n@     0x559e72911ab3  tensorflow::(anonymous namespace)::RunIfBoxIndexIsValid<>()\n@     0x559e729b8ab9  tensorflow::CropAndResizeOp<>::ComputeAsync()\n@     0x559e76c285ed  tensorflow::Device::ComputeAsync()\n@     0x559e79c168b6  tensorflow::(anonymous namespace)::ExecutorState::Process()\n@     0x559e79c17395  tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady()::$_1::operator()()\n@     0x559e79c171bd  std::_Function_handler<>::_M_invoke()\n@     0x559e6e44ba3d  std::function<>::operator()()\n@     0x559e7a05ed85  tensorflow::thread::EigenEnvironment::ExecuteTask()\n@     0x559e7a05e0d9  Eigen::ThreadPoolTempl<>::WorkerLoop()\n@     0x559e7a05dd1e  Eigen::ThreadPoolTempl<>::ThreadPoolTempl()::{lambda()#1}::operator()()\n@     0x559e7a05db8d  std::_Function_handler<>::_M_invoke()\n*"}