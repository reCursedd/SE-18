{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/233125331", "html_url": "https://github.com/tensorflow/tensorflow/issues/2995#issuecomment-233125331", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2995", "id": 233125331, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMzEyNTMzMQ==", "user": {"login": "rajkumarcm", "id": 9053016, "node_id": "MDQ6VXNlcjkwNTMwMTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9053016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rajkumarcm", "html_url": "https://github.com/rajkumarcm", "followers_url": "https://api.github.com/users/rajkumarcm/followers", "following_url": "https://api.github.com/users/rajkumarcm/following{/other_user}", "gists_url": "https://api.github.com/users/rajkumarcm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rajkumarcm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rajkumarcm/subscriptions", "organizations_url": "https://api.github.com/users/rajkumarcm/orgs", "repos_url": "https://api.github.com/users/rajkumarcm/repos", "events_url": "https://api.github.com/users/rajkumarcm/events{/privacy}", "received_events_url": "https://api.github.com/users/rajkumarcm/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-16T11:05:11Z", "updated_at": "2016-07-16T11:05:11Z", "author_association": "NONE", "body_html": "<p>@ScartleRoy Could you please show here how you compute the loss with the concatenated version of logits, and what was the outcome ?</p>\n<p>I have been training a Bidirectional LSTM network, aiming to reproduce result of network described in Alex Grave's \"Towards End-End Speech Recognition using Recurrent Neural Networks\", but the predicted transcription is nowhere near that of actual. After 4000 iterations (not epochs, mini-batch size:100 with total data: 47,000) I get the CTC loss reducing from 650 to roughly 90. I am finding it really hard to determine the hyperparameters for training such a network.</p>\n<p>Any suggestions would be greatly appreciated.</p>", "body_text": "@ScartleRoy Could you please show here how you compute the loss with the concatenated version of logits, and what was the outcome ?\nI have been training a Bidirectional LSTM network, aiming to reproduce result of network described in Alex Grave's \"Towards End-End Speech Recognition using Recurrent Neural Networks\", but the predicted transcription is nowhere near that of actual. After 4000 iterations (not epochs, mini-batch size:100 with total data: 47,000) I get the CTC loss reducing from 650 to roughly 90. I am finding it really hard to determine the hyperparameters for training such a network.\nAny suggestions would be greatly appreciated.", "body": "@ScartleRoy Could you please show here how you compute the loss with the concatenated version of logits, and what was the outcome ? \n\nI have been training a Bidirectional LSTM network, aiming to reproduce result of network described in Alex Grave's \"Towards End-End Speech Recognition using Recurrent Neural Networks\", but the predicted transcription is nowhere near that of actual. After 4000 iterations (not epochs, mini-batch size:100 with total data: 47,000) I get the CTC loss reducing from 650 to roughly 90. I am finding it really hard to determine the hyperparameters for training such a network.\n\nAny suggestions would be greatly appreciated.\n"}