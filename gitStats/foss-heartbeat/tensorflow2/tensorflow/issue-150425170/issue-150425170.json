{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2066", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2066/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2066/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2066/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2066", "id": 150425170, "node_id": "MDU6SXNzdWUxNTA0MjUxNzA=", "number": 2066, "title": "segmentation fault when stride > ksize in conv2d", "user": {"login": "kshmelkov", "id": 10819534, "node_id": "MDQ6VXNlcjEwODE5NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/10819534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kshmelkov", "html_url": "https://github.com/kshmelkov", "followers_url": "https://api.github.com/users/kshmelkov/followers", "following_url": "https://api.github.com/users/kshmelkov/following{/other_user}", "gists_url": "https://api.github.com/users/kshmelkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/kshmelkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kshmelkov/subscriptions", "organizations_url": "https://api.github.com/users/kshmelkov/orgs", "repos_url": "https://api.github.com/users/kshmelkov/repos", "events_url": "https://api.github.com/users/kshmelkov/events{/privacy}", "received_events_url": "https://api.github.com/users/kshmelkov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2016-04-22T17:16:08Z", "updated_at": "2016-05-01T18:53:15Z", "closed_at": "2016-04-30T19:07:57Z", "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>I experience segfault trying to use recently merged modification of conv2d to support stride &gt; ksize (yes, residual networks). Somehow it only arises in a complicated graph. I wrote minimal example, but it is still pretty big.</p>\n<h3>Environment info</h3>\n<p>Operating System: Fedora 21<br>\nPython 3.4, GPU Titan X and Titan Z</p>\n<p>Installed version of CUDA and cuDNN: Cuda 7.5, Cudnn 4.<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<pre><code>$ ls -1 $CUDA_HOME/lib/libcud*\n$CUDA_HOME/lib/libcudadevrt.a\n$CUDA_HOME/lib/libcudart.so\n$CUDA_HOME/lib/libcudart.so.7.5\n$CUDA_HOME/lib/libcudart.so.7.5.18\n$CUDA_HOME/lib/libcudart_static.a\n\n$ ls -1 $CUDA_HOME/lib64/libcud*\n$CUDA_HOME/lib64/libcudadevrt.a\n$CUDA_HOME/lib64/libcudart.so\n$CUDA_HOME/lib64/libcudart.so.7.5\n$CUDA_HOME/lib64/libcudart.so.7.5.18\n$CUDA_HOME/lib64/libcudart_static.a\n$CUDA_HOME/lib64/libcudnn.so\n$CUDA_HOME/lib64/libcudnn.so.4\n$CUDA_HOME/lib64/libcudnn.so.4.0.7\n$CUDA_HOME/lib64/libcudnn_static.a\n</code></pre>\n<p>commit hash (nightly): <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/7b536cd5cfdbfd0fbc80496a38624557fb977783/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/7b536cd5cfdbfd0fbc80496a38624557fb977783\"><tt>7b536cd</tt></a></p>\n<h3>Steps to reproduce</h3>\n<p>Run the following code.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_integer('res_n', 18,\n                            \"\"\"Residual network parameter.\"\"\")\ntf.app.flags.DEFINE_integer('batch_size', 128,\n                            \"\"\"Batch size.\"\"\")\ntf.app.flags.DEFINE_integer('proj_ksize', 1,\n                            \"\"\"Kernel size of identity projection.\"\"\")\n\n\ndef weight_variable(shape):\n    init = tf.uniform_unit_scaling_initializer()\n    var = tf.get_variable('weights', shape, initializer=init)\n    return var\n\n\ndef batch_norm(x, conv=True, scope='bn'):\n    phase_train = tf.get_collection('is_train')[0]\n    n_out = int(x.get_shape()[-1])\n    with tf.variable_scope(scope):\n        beta = tf.get_variable('beta', shape=[n_out],\n                initializer=tf.constant_initializer(0.0), trainable=True)\n        gamma = tf.get_variable('gamma', shape=[n_out],\n                initializer=tf.constant_initializer(1.0), trainable=True)\n\n        axes = [0, 1, 2] if conv else [0]\n        batch_mean, batch_var = tf.nn.moments(x, axes, name='moments')\n        ema = tf.train.ExponentialMovingAverage(decay=0.9)\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n        def mean_var_with_update():\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n        mean, var = tf.python.control_flow_ops.cond(phase_train,\n            mean_var_with_update,\n            lambda: (ema_mean, ema_var))\n\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n\n\ndef create_conv_layer(src, name, ksize, out_filters,\n                      stride=1, padding='SAME'):\n    in_filters = int(src.get_shape()[-1])\n    with tf.variable_scope(name):\n        W = weight_variable([ksize, ksize, in_filters, out_filters])\n        conv = tf.nn.conv2d(src, W, strides=[1, stride, stride, 1],\n                            padding=padding)\n    return conv\n\n\ndef res_block(layer, i, lvl, num_filters, preact=True):\n    in_filters = int(layer.get_shape()[-1])\n    name = 'conv%i_%i' % (lvl, i)\n    if preact:\n        layer_act = tf.nn.relu(batch_norm(layer,\n                                          scope='bn_%i_%i_0' % (lvl, i)))\n    else:\n        layer_act = layer\n    if num_filters != in_filters:\n        stride = 2\n    else:\n        stride = 1\n    layer2 = create_conv_layer(layer_act, name+'_1', 3, num_filters, stride=stride)\n    layer2_act = tf.nn.relu(batch_norm(layer2, scope='bn_%i_%i_1' % (lvl, i)))\n    layer3 = create_conv_layer(layer2_act, name+'_2', 3, num_filters)\n    if stride &gt; 1:\n        layer_proj = create_conv_layer(layer, name+'_proj', FLAGS.proj_ksize, num_filters,\n                                       stride=stride)\n        res = layer_proj + layer3\n    else:\n        res = layer + layer3\n    return res\n\n\ndef inference(layer, n):\n    layer = create_conv_layer(layer, 'conv1_0', 3, 16)\n    layer = tf.nn.relu(batch_norm(layer, scope='conv1_bn'))\n    for i in range(n):\n        layer = res_block(layer, i, 1, 16, preact=(i &gt; 0))\n    for i in range(n):\n        layer = res_block(layer, i, 2, 32)\n    for i in range(n):\n        layer = res_block(layer, i, 3, 64)\n    layer = tf.nn.relu(batch_norm(layer, scope='bn_post'))\n    layer = tf.reduce_mean(layer, [1, 2], keep_dims=True)\n    layer = create_conv_layer(layer, 'conv5', 1, 10)\n    layer = tf.squeeze(layer)\n    return layer\n\n\nif __name__ == '__main__':\n    np.random.seed(42)\n    a = np.random.randn(FLAGS.batch_size, 32, 32, 3)\n    b = np.random.randint(0, 10, size=FLAGS.batch_size)\n\n    sess = tf.Session()\n\n    phase_train = tf.Variable(True, trainable=False, name='phase_train')\n    tf.add_to_collection('is_train', phase_train)\n\n    x = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, 32, 32, 3))\n    y = tf.placeholder(tf.int64, shape=(FLAGS.batch_size))\n    logits = inference(x, FLAGS.res_n)\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n\n    opt = tf.train.GradientDescentOptimizer(0.01)\n    train_op = opt.minimize(loss)\n\n    sess.run(tf.initialize_all_variables())\n    sess.run(train_op, feed_dict={x: a, y: b})\n</code></pre>\n<p>It fails with an error cited below (if run on gpu with cudnn).</p>\n<h3>What have you tried?</h3>\n<ol>\n<li>It doesn't look like a memory issue, it still fails with tiny batch size.</li>\n<li>It works on CPU though.</li>\n<li>It works if <code>proj_ksize=3</code> which again shows it isn't OOM issue and points to recently introduced feature.</li>\n<li>It doesn't fail on a trivial example (just isolated conv2d with stride=2, ksize=1). While writing it, I realize I forgot to check the trivial example with backward pass.</li>\n<li>It works for forward pass even on gpu.</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment).</p>\n<pre><code>$ python3 conv_bug.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN Z\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.8755\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN Z, pci bus id: 0000:05:00.0)\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:904] failed to enqueue convolution on stream: CUDNN_STATUS_BAD_PARAM\n[1]    10160 abort (core dumped)  python3 conv_bug.py\n</code></pre>", "body_text": "Hello,\nI experience segfault trying to use recently merged modification of conv2d to support stride > ksize (yes, residual networks). Somehow it only arises in a complicated graph. I wrote minimal example, but it is still pretty big.\nEnvironment info\nOperating System: Fedora 21\nPython 3.4, GPU Titan X and Titan Z\nInstalled version of CUDA and cuDNN: Cuda 7.5, Cudnn 4.\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n$ ls -1 $CUDA_HOME/lib/libcud*\n$CUDA_HOME/lib/libcudadevrt.a\n$CUDA_HOME/lib/libcudart.so\n$CUDA_HOME/lib/libcudart.so.7.5\n$CUDA_HOME/lib/libcudart.so.7.5.18\n$CUDA_HOME/lib/libcudart_static.a\n\n$ ls -1 $CUDA_HOME/lib64/libcud*\n$CUDA_HOME/lib64/libcudadevrt.a\n$CUDA_HOME/lib64/libcudart.so\n$CUDA_HOME/lib64/libcudart.so.7.5\n$CUDA_HOME/lib64/libcudart.so.7.5.18\n$CUDA_HOME/lib64/libcudart_static.a\n$CUDA_HOME/lib64/libcudnn.so\n$CUDA_HOME/lib64/libcudnn.so.4\n$CUDA_HOME/lib64/libcudnn.so.4.0.7\n$CUDA_HOME/lib64/libcudnn_static.a\n\ncommit hash (nightly): 7b536cd\nSteps to reproduce\nRun the following code.\nimport tensorflow as tf\nimport numpy as np\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_integer('res_n', 18,\n                            \"\"\"Residual network parameter.\"\"\")\ntf.app.flags.DEFINE_integer('batch_size', 128,\n                            \"\"\"Batch size.\"\"\")\ntf.app.flags.DEFINE_integer('proj_ksize', 1,\n                            \"\"\"Kernel size of identity projection.\"\"\")\n\n\ndef weight_variable(shape):\n    init = tf.uniform_unit_scaling_initializer()\n    var = tf.get_variable('weights', shape, initializer=init)\n    return var\n\n\ndef batch_norm(x, conv=True, scope='bn'):\n    phase_train = tf.get_collection('is_train')[0]\n    n_out = int(x.get_shape()[-1])\n    with tf.variable_scope(scope):\n        beta = tf.get_variable('beta', shape=[n_out],\n                initializer=tf.constant_initializer(0.0), trainable=True)\n        gamma = tf.get_variable('gamma', shape=[n_out],\n                initializer=tf.constant_initializer(1.0), trainable=True)\n\n        axes = [0, 1, 2] if conv else [0]\n        batch_mean, batch_var = tf.nn.moments(x, axes, name='moments')\n        ema = tf.train.ExponentialMovingAverage(decay=0.9)\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n        def mean_var_with_update():\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n        mean, var = tf.python.control_flow_ops.cond(phase_train,\n            mean_var_with_update,\n            lambda: (ema_mean, ema_var))\n\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n\n\ndef create_conv_layer(src, name, ksize, out_filters,\n                      stride=1, padding='SAME'):\n    in_filters = int(src.get_shape()[-1])\n    with tf.variable_scope(name):\n        W = weight_variable([ksize, ksize, in_filters, out_filters])\n        conv = tf.nn.conv2d(src, W, strides=[1, stride, stride, 1],\n                            padding=padding)\n    return conv\n\n\ndef res_block(layer, i, lvl, num_filters, preact=True):\n    in_filters = int(layer.get_shape()[-1])\n    name = 'conv%i_%i' % (lvl, i)\n    if preact:\n        layer_act = tf.nn.relu(batch_norm(layer,\n                                          scope='bn_%i_%i_0' % (lvl, i)))\n    else:\n        layer_act = layer\n    if num_filters != in_filters:\n        stride = 2\n    else:\n        stride = 1\n    layer2 = create_conv_layer(layer_act, name+'_1', 3, num_filters, stride=stride)\n    layer2_act = tf.nn.relu(batch_norm(layer2, scope='bn_%i_%i_1' % (lvl, i)))\n    layer3 = create_conv_layer(layer2_act, name+'_2', 3, num_filters)\n    if stride > 1:\n        layer_proj = create_conv_layer(layer, name+'_proj', FLAGS.proj_ksize, num_filters,\n                                       stride=stride)\n        res = layer_proj + layer3\n    else:\n        res = layer + layer3\n    return res\n\n\ndef inference(layer, n):\n    layer = create_conv_layer(layer, 'conv1_0', 3, 16)\n    layer = tf.nn.relu(batch_norm(layer, scope='conv1_bn'))\n    for i in range(n):\n        layer = res_block(layer, i, 1, 16, preact=(i > 0))\n    for i in range(n):\n        layer = res_block(layer, i, 2, 32)\n    for i in range(n):\n        layer = res_block(layer, i, 3, 64)\n    layer = tf.nn.relu(batch_norm(layer, scope='bn_post'))\n    layer = tf.reduce_mean(layer, [1, 2], keep_dims=True)\n    layer = create_conv_layer(layer, 'conv5', 1, 10)\n    layer = tf.squeeze(layer)\n    return layer\n\n\nif __name__ == '__main__':\n    np.random.seed(42)\n    a = np.random.randn(FLAGS.batch_size, 32, 32, 3)\n    b = np.random.randint(0, 10, size=FLAGS.batch_size)\n\n    sess = tf.Session()\n\n    phase_train = tf.Variable(True, trainable=False, name='phase_train')\n    tf.add_to_collection('is_train', phase_train)\n\n    x = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, 32, 32, 3))\n    y = tf.placeholder(tf.int64, shape=(FLAGS.batch_size))\n    logits = inference(x, FLAGS.res_n)\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n\n    opt = tf.train.GradientDescentOptimizer(0.01)\n    train_op = opt.minimize(loss)\n\n    sess.run(tf.initialize_all_variables())\n    sess.run(train_op, feed_dict={x: a, y: b})\n\nIt fails with an error cited below (if run on gpu with cudnn).\nWhat have you tried?\n\nIt doesn't look like a memory issue, it still fails with tiny batch size.\nIt works on CPU though.\nIt works if proj_ksize=3 which again shows it isn't OOM issue and points to recently introduced feature.\nIt doesn't fail on a trivial example (just isolated conv2d with stride=2, ksize=1). While writing it, I realize I forgot to check the trivial example with backward pass.\nIt works for forward pass even on gpu.\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).\n$ python3 conv_bug.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN Z\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.8755\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Z, pci bus id: 0000:05:00.0)\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:904] failed to enqueue convolution on stream: CUDNN_STATUS_BAD_PARAM\n[1]    10160 abort (core dumped)  python3 conv_bug.py", "body": "Hello,\n\nI experience segfault trying to use recently merged modification of conv2d to support stride > ksize (yes, residual networks). Somehow it only arises in a complicated graph. I wrote minimal example, but it is still pretty big.\n### Environment info\n\nOperating System: Fedora 21\nPython 3.4, GPU Titan X and Titan Z\n\nInstalled version of CUDA and cuDNN: Cuda 7.5, Cudnn 4.\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n$ ls -1 $CUDA_HOME/lib/libcud*\n$CUDA_HOME/lib/libcudadevrt.a\n$CUDA_HOME/lib/libcudart.so\n$CUDA_HOME/lib/libcudart.so.7.5\n$CUDA_HOME/lib/libcudart.so.7.5.18\n$CUDA_HOME/lib/libcudart_static.a\n\n$ ls -1 $CUDA_HOME/lib64/libcud*\n$CUDA_HOME/lib64/libcudadevrt.a\n$CUDA_HOME/lib64/libcudart.so\n$CUDA_HOME/lib64/libcudart.so.7.5\n$CUDA_HOME/lib64/libcudart.so.7.5.18\n$CUDA_HOME/lib64/libcudart_static.a\n$CUDA_HOME/lib64/libcudnn.so\n$CUDA_HOME/lib64/libcudnn.so.4\n$CUDA_HOME/lib64/libcudnn.so.4.0.7\n$CUDA_HOME/lib64/libcudnn_static.a\n```\n\ncommit hash (nightly): 7b536cd5cfdbfd0fbc80496a38624557fb977783\n### Steps to reproduce\n\nRun the following code.\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_integer('res_n', 18,\n                            \"\"\"Residual network parameter.\"\"\")\ntf.app.flags.DEFINE_integer('batch_size', 128,\n                            \"\"\"Batch size.\"\"\")\ntf.app.flags.DEFINE_integer('proj_ksize', 1,\n                            \"\"\"Kernel size of identity projection.\"\"\")\n\n\ndef weight_variable(shape):\n    init = tf.uniform_unit_scaling_initializer()\n    var = tf.get_variable('weights', shape, initializer=init)\n    return var\n\n\ndef batch_norm(x, conv=True, scope='bn'):\n    phase_train = tf.get_collection('is_train')[0]\n    n_out = int(x.get_shape()[-1])\n    with tf.variable_scope(scope):\n        beta = tf.get_variable('beta', shape=[n_out],\n                initializer=tf.constant_initializer(0.0), trainable=True)\n        gamma = tf.get_variable('gamma', shape=[n_out],\n                initializer=tf.constant_initializer(1.0), trainable=True)\n\n        axes = [0, 1, 2] if conv else [0]\n        batch_mean, batch_var = tf.nn.moments(x, axes, name='moments')\n        ema = tf.train.ExponentialMovingAverage(decay=0.9)\n        ema_apply_op = ema.apply([batch_mean, batch_var])\n        ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n        def mean_var_with_update():\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n        mean, var = tf.python.control_flow_ops.cond(phase_train,\n            mean_var_with_update,\n            lambda: (ema_mean, ema_var))\n\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n\n\ndef create_conv_layer(src, name, ksize, out_filters,\n                      stride=1, padding='SAME'):\n    in_filters = int(src.get_shape()[-1])\n    with tf.variable_scope(name):\n        W = weight_variable([ksize, ksize, in_filters, out_filters])\n        conv = tf.nn.conv2d(src, W, strides=[1, stride, stride, 1],\n                            padding=padding)\n    return conv\n\n\ndef res_block(layer, i, lvl, num_filters, preact=True):\n    in_filters = int(layer.get_shape()[-1])\n    name = 'conv%i_%i' % (lvl, i)\n    if preact:\n        layer_act = tf.nn.relu(batch_norm(layer,\n                                          scope='bn_%i_%i_0' % (lvl, i)))\n    else:\n        layer_act = layer\n    if num_filters != in_filters:\n        stride = 2\n    else:\n        stride = 1\n    layer2 = create_conv_layer(layer_act, name+'_1', 3, num_filters, stride=stride)\n    layer2_act = tf.nn.relu(batch_norm(layer2, scope='bn_%i_%i_1' % (lvl, i)))\n    layer3 = create_conv_layer(layer2_act, name+'_2', 3, num_filters)\n    if stride > 1:\n        layer_proj = create_conv_layer(layer, name+'_proj', FLAGS.proj_ksize, num_filters,\n                                       stride=stride)\n        res = layer_proj + layer3\n    else:\n        res = layer + layer3\n    return res\n\n\ndef inference(layer, n):\n    layer = create_conv_layer(layer, 'conv1_0', 3, 16)\n    layer = tf.nn.relu(batch_norm(layer, scope='conv1_bn'))\n    for i in range(n):\n        layer = res_block(layer, i, 1, 16, preact=(i > 0))\n    for i in range(n):\n        layer = res_block(layer, i, 2, 32)\n    for i in range(n):\n        layer = res_block(layer, i, 3, 64)\n    layer = tf.nn.relu(batch_norm(layer, scope='bn_post'))\n    layer = tf.reduce_mean(layer, [1, 2], keep_dims=True)\n    layer = create_conv_layer(layer, 'conv5', 1, 10)\n    layer = tf.squeeze(layer)\n    return layer\n\n\nif __name__ == '__main__':\n    np.random.seed(42)\n    a = np.random.randn(FLAGS.batch_size, 32, 32, 3)\n    b = np.random.randint(0, 10, size=FLAGS.batch_size)\n\n    sess = tf.Session()\n\n    phase_train = tf.Variable(True, trainable=False, name='phase_train')\n    tf.add_to_collection('is_train', phase_train)\n\n    x = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, 32, 32, 3))\n    y = tf.placeholder(tf.int64, shape=(FLAGS.batch_size))\n    logits = inference(x, FLAGS.res_n)\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n\n    opt = tf.train.GradientDescentOptimizer(0.01)\n    train_op = opt.minimize(loss)\n\n    sess.run(tf.initialize_all_variables())\n    sess.run(train_op, feed_dict={x: a, y: b})\n```\n\nIt fails with an error cited below (if run on gpu with cudnn).\n### What have you tried?\n1. It doesn't look like a memory issue, it still fails with tiny batch size.\n2. It works on CPU though.\n3. It works if `proj_ksize=3` which again shows it isn't OOM issue and points to recently introduced feature.\n4. It doesn't fail on a trivial example (just isolated conv2d with stride=2, ksize=1). While writing it, I realize I forgot to check the trivial example with backward pass.\n5. It works for forward pass even on gpu.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n```\n$ python3 conv_bug.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN Z\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.8755\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN Z, pci bus id: 0000:05:00.0)\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:904] failed to enqueue convolution on stream: CUDNN_STATUS_BAD_PARAM\n[1]    10160 abort (core dumped)  python3 conv_bug.py\n```\n"}