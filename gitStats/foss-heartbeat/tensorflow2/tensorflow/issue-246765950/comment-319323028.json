{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/319323028", "html_url": "https://github.com/tensorflow/tensorflow/issues/11909#issuecomment-319323028", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11909", "id": 319323028, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTMyMzAyOA==", "user": {"login": "emsi", "id": 433383, "node_id": "MDQ6VXNlcjQzMzM4Mw==", "avatar_url": "https://avatars3.githubusercontent.com/u/433383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emsi", "html_url": "https://github.com/emsi", "followers_url": "https://api.github.com/users/emsi/followers", "following_url": "https://api.github.com/users/emsi/following{/other_user}", "gists_url": "https://api.github.com/users/emsi/gists{/gist_id}", "starred_url": "https://api.github.com/users/emsi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emsi/subscriptions", "organizations_url": "https://api.github.com/users/emsi/orgs", "repos_url": "https://api.github.com/users/emsi/repos", "events_url": "https://api.github.com/users/emsi/events{/privacy}", "received_events_url": "https://api.github.com/users/emsi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-01T09:45:37Z", "updated_at": "2017-08-01T09:45:37Z", "author_association": "NONE", "body_html": "<p>The case is slightly different. I already have NaNs in the model (in the Conv layer) and the model produces correct results on GPU. What's interesting is that if I change the NaNs to minus infinity it produces the same results on CPU.</p>\n<p>Take a look at the weights:</p>\n<pre><code>input\n[[ -3.76313413e-03   3.56433578e-02  -2.13286784e-02 ...,   4.60807569e-02\n    1.64003521e-02   4.84493654e-03]\n [ -1.49039514e+03   1.54171045e+03   3.97881046e-02 ...,  -5.78388453e+00\n    2.43417993e-02  -1.80144477e+00]\n [ -7.30040884e+00  -6.69775963e-01  -2.57102852e+01 ...,   1.29456043e+01\n    2.43919420e+00  -2.05847025e-02]\n ..., \n [ -2.07939720e+00   1.65184236e+00   5.03200665e-02 ...,  -9.66950320e-03\n   -3.28505486e-02  -1.60967112e-01]\n [  2.00684814e+01  -4.48382616e+00   9.10888481e+00 ...,   5.54858351e+00\n    1.92856118e-01  -3.27318764e+00]\n [  1.32376838e+00  -8.82686138e+00  -5.80457896e-02 ...,   4.60517597e+00\n    1.85110033e-01  -7.66239548e+00]]\nembedding_1\n[[[ -1.99184626e-01  -6.24039583e-02  -5.06775379e-02 ...,   1.50278226e-01\n     1.72856063e-01   9.54210311e-02]\n  [  8.40011835e-02  -1.04346856e-01  -1.50010496e-01 ...,  -6.32669926e-02\n     3.96224223e-02  -4.31677438e-02]\n  [ -8.77222940e-02   5.68446703e-02   5.21141998e-02 ...,  -4.07455601e-02\n    -1.41425105e-02  -1.18918315e-01]\n  ..., \n  [  3.44693642e+01   9.11418945e-02   1.19048271e+01 ...,   1.09344691e-01\n     1.87920347e-01   1.70073420e-01]\n  [  1.95824668e-01   7.89859332e-03   9.49118510e-02 ...,  -3.29354890e-02\n     9.31546241e-02  -1.15318425e-01]\n  [  5.89678064e-02   1.41023742e-02   1.11247830e-01 ...,  -9.51733999e-03\n    -1.24835804e-01  -1.12498775e-01]]]\nconv1d_1\n[  1.22442953e-02  -1.09238578e-02  -2.47095129e-04   1.17717348e-02\n  -3.92556889e-03  -5.25355758e-03   6.99595024e-04  -1.10572961e-03\n  -1.23395994e-02              nan  -1.07570309e-02              nan\n  -4.08349838e-03              nan  -4.71794903e-02              nan\n  -8.35025460e-02   4.59762150e-03  -1.97616145e-02  -4.07275278e-03\n              nan  -2.22547930e-02  -1.18769081e-02  -4.17554192e-03\n  -1.11441838e-03  -4.26767347e-03  -7.31970591e-04   1.48863369e-03\n  -8.20796192e-03  -5.81618951e-05  -9.59698856e-03   1.10042130e-03\n  -2.86204537e-04  -1.64017070e-03  -3.14154982e-04   9.65116825e-03\n  -3.80394212e-03  -1.14975907e-02  -1.14118000e-02  -6.63896417e-03\n  -1.02947457e-02  -9.60258301e-03  -1.01221241e-02  -3.38621135e-03\n  -9.19509027e-03              nan  -2.48736516e-03  -2.78100204e-02\n  -6.50373427e-03  -1.12824216e-02  -1.99352647e-03  -2.70526763e-03\n  -3.97901144e-03  -8.55945144e-03  -2.47867964e-03  -7.23447651e-03\n  -1.89489545e-03              nan   2.34743766e-03  -1.76552832e-02\n  -1.13650626e-02  -8.72352626e-03  -1.39924623e-02  -9.96223418e-04\n   1.19819669e-02  -1.65980589e-02  -4.43597836e-03  -1.32924877e-03\n   5.39703481e-03  -5.20827901e-03   2.09201197e-03  -9.91921872e-04\n  -4.76511335e-03  -2.82827131e-02  -3.15303504e-02   7.75449863e-03\n  -2.50613503e-03   5.26739890e-03  -9.09337029e-03  -4.06189971e-02\n  -2.45879288e-03  -6.14755554e-03  -4.51790220e-05  -1.96866468e-02\n  -6.05669746e-04  -1.08566531e-03  -2.80132494e-03   1.35421744e-02\n  -1.56419240e-02  -9.22969636e-03  -1.79088686e-03  -6.13477267e-03\n   3.58441076e-03  -7.55022466e-03   7.20449630e-03   9.64386389e-03\n  -2.01407354e-02   3.64139187e-03  -8.29497911e-03  -9.83765721e-03\n  -6.11743657e-03   1.19033596e-03   3.14918067e-03   9.48947109e-03\n  -1.80235994e-03  -3.43534909e-03  -9.94156580e-03  -1.02966810e-02\n  -1.11212442e-02  -1.83315631e-02   1.77474413e-02  -5.63624455e-03\n   1.24310488e-02  -7.89506827e-03   5.76251280e-03   3.04201362e-03\n  -9.81117040e-03  -2.30816868e-03  -1.17079932e-02  -3.04500442e-02\n   1.04938364e-02  -1.00002484e-02  -5.61673800e-03  -1.49540678e-02\n  -4.57247645e-02  -2.97049657e-02  -1.49060250e-03  -4.22246335e-03]\n</code></pre>\n<p>The model is defined as follows hence the NaNs are in the first layer after Embeddings:</p>\n<pre><code>def _model():\n    max_length = 500\n    top_words = 5000\n    embedding_vecor_length = 60\n    classes = 24\n    \n    _input = Input(shape=(max_length,), name='input')\n    embedding=Embedding(top_words, embedding_vecor_length, input_length=max_length)(_input)\n    \n    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n    \n    conv1 = conv1(embedding)\n    glob1 = GlobalAveragePooling1D()(conv1)\n    conv2 = conv2(embedding)\n    glob2 = GlobalAveragePooling1D()(conv2)\n    conv3 = conv3(embedding)\n    glob3 = GlobalAveragePooling1D()(conv3)\n    conv4 = conv4(embedding)\n    glob4 = GlobalAveragePooling1D()(conv4)\n    conv5 = conv5(embedding)\n    glob5 = GlobalAveragePooling1D()(conv5)\n    conv6 = conv6(embedding)\n    glob6 = GlobalAveragePooling1D()(conv6)\n    \n    merge = concatenate([glob1, glob2, glob3, glob4, glob5, glob6])\n    x = Dropout(0.2)(merge)\n    x = BatchNormalization()(x)\n    x = Dense(300, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    x = BatchNormalization()(x)\n    pred = Dense(classes, activation='softmax')(x)\n    \n    model = Model(inputs=[_input], outputs=pred)\n    \n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam', \n                  metrics=['accuracy']\n                 )\n    \n    return model  \n</code></pre>", "body_text": "The case is slightly different. I already have NaNs in the model (in the Conv layer) and the model produces correct results on GPU. What's interesting is that if I change the NaNs to minus infinity it produces the same results on CPU.\nTake a look at the weights:\ninput\n[[ -3.76313413e-03   3.56433578e-02  -2.13286784e-02 ...,   4.60807569e-02\n    1.64003521e-02   4.84493654e-03]\n [ -1.49039514e+03   1.54171045e+03   3.97881046e-02 ...,  -5.78388453e+00\n    2.43417993e-02  -1.80144477e+00]\n [ -7.30040884e+00  -6.69775963e-01  -2.57102852e+01 ...,   1.29456043e+01\n    2.43919420e+00  -2.05847025e-02]\n ..., \n [ -2.07939720e+00   1.65184236e+00   5.03200665e-02 ...,  -9.66950320e-03\n   -3.28505486e-02  -1.60967112e-01]\n [  2.00684814e+01  -4.48382616e+00   9.10888481e+00 ...,   5.54858351e+00\n    1.92856118e-01  -3.27318764e+00]\n [  1.32376838e+00  -8.82686138e+00  -5.80457896e-02 ...,   4.60517597e+00\n    1.85110033e-01  -7.66239548e+00]]\nembedding_1\n[[[ -1.99184626e-01  -6.24039583e-02  -5.06775379e-02 ...,   1.50278226e-01\n     1.72856063e-01   9.54210311e-02]\n  [  8.40011835e-02  -1.04346856e-01  -1.50010496e-01 ...,  -6.32669926e-02\n     3.96224223e-02  -4.31677438e-02]\n  [ -8.77222940e-02   5.68446703e-02   5.21141998e-02 ...,  -4.07455601e-02\n    -1.41425105e-02  -1.18918315e-01]\n  ..., \n  [  3.44693642e+01   9.11418945e-02   1.19048271e+01 ...,   1.09344691e-01\n     1.87920347e-01   1.70073420e-01]\n  [  1.95824668e-01   7.89859332e-03   9.49118510e-02 ...,  -3.29354890e-02\n     9.31546241e-02  -1.15318425e-01]\n  [  5.89678064e-02   1.41023742e-02   1.11247830e-01 ...,  -9.51733999e-03\n    -1.24835804e-01  -1.12498775e-01]]]\nconv1d_1\n[  1.22442953e-02  -1.09238578e-02  -2.47095129e-04   1.17717348e-02\n  -3.92556889e-03  -5.25355758e-03   6.99595024e-04  -1.10572961e-03\n  -1.23395994e-02              nan  -1.07570309e-02              nan\n  -4.08349838e-03              nan  -4.71794903e-02              nan\n  -8.35025460e-02   4.59762150e-03  -1.97616145e-02  -4.07275278e-03\n              nan  -2.22547930e-02  -1.18769081e-02  -4.17554192e-03\n  -1.11441838e-03  -4.26767347e-03  -7.31970591e-04   1.48863369e-03\n  -8.20796192e-03  -5.81618951e-05  -9.59698856e-03   1.10042130e-03\n  -2.86204537e-04  -1.64017070e-03  -3.14154982e-04   9.65116825e-03\n  -3.80394212e-03  -1.14975907e-02  -1.14118000e-02  -6.63896417e-03\n  -1.02947457e-02  -9.60258301e-03  -1.01221241e-02  -3.38621135e-03\n  -9.19509027e-03              nan  -2.48736516e-03  -2.78100204e-02\n  -6.50373427e-03  -1.12824216e-02  -1.99352647e-03  -2.70526763e-03\n  -3.97901144e-03  -8.55945144e-03  -2.47867964e-03  -7.23447651e-03\n  -1.89489545e-03              nan   2.34743766e-03  -1.76552832e-02\n  -1.13650626e-02  -8.72352626e-03  -1.39924623e-02  -9.96223418e-04\n   1.19819669e-02  -1.65980589e-02  -4.43597836e-03  -1.32924877e-03\n   5.39703481e-03  -5.20827901e-03   2.09201197e-03  -9.91921872e-04\n  -4.76511335e-03  -2.82827131e-02  -3.15303504e-02   7.75449863e-03\n  -2.50613503e-03   5.26739890e-03  -9.09337029e-03  -4.06189971e-02\n  -2.45879288e-03  -6.14755554e-03  -4.51790220e-05  -1.96866468e-02\n  -6.05669746e-04  -1.08566531e-03  -2.80132494e-03   1.35421744e-02\n  -1.56419240e-02  -9.22969636e-03  -1.79088686e-03  -6.13477267e-03\n   3.58441076e-03  -7.55022466e-03   7.20449630e-03   9.64386389e-03\n  -2.01407354e-02   3.64139187e-03  -8.29497911e-03  -9.83765721e-03\n  -6.11743657e-03   1.19033596e-03   3.14918067e-03   9.48947109e-03\n  -1.80235994e-03  -3.43534909e-03  -9.94156580e-03  -1.02966810e-02\n  -1.11212442e-02  -1.83315631e-02   1.77474413e-02  -5.63624455e-03\n   1.24310488e-02  -7.89506827e-03   5.76251280e-03   3.04201362e-03\n  -9.81117040e-03  -2.30816868e-03  -1.17079932e-02  -3.04500442e-02\n   1.04938364e-02  -1.00002484e-02  -5.61673800e-03  -1.49540678e-02\n  -4.57247645e-02  -2.97049657e-02  -1.49060250e-03  -4.22246335e-03]\n\nThe model is defined as follows hence the NaNs are in the first layer after Embeddings:\ndef _model():\n    max_length = 500\n    top_words = 5000\n    embedding_vecor_length = 60\n    classes = 24\n    \n    _input = Input(shape=(max_length,), name='input')\n    embedding=Embedding(top_words, embedding_vecor_length, input_length=max_length)(_input)\n    \n    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n    \n    conv1 = conv1(embedding)\n    glob1 = GlobalAveragePooling1D()(conv1)\n    conv2 = conv2(embedding)\n    glob2 = GlobalAveragePooling1D()(conv2)\n    conv3 = conv3(embedding)\n    glob3 = GlobalAveragePooling1D()(conv3)\n    conv4 = conv4(embedding)\n    glob4 = GlobalAveragePooling1D()(conv4)\n    conv5 = conv5(embedding)\n    glob5 = GlobalAveragePooling1D()(conv5)\n    conv6 = conv6(embedding)\n    glob6 = GlobalAveragePooling1D()(conv6)\n    \n    merge = concatenate([glob1, glob2, glob3, glob4, glob5, glob6])\n    x = Dropout(0.2)(merge)\n    x = BatchNormalization()(x)\n    x = Dense(300, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    x = BatchNormalization()(x)\n    pred = Dense(classes, activation='softmax')(x)\n    \n    model = Model(inputs=[_input], outputs=pred)\n    \n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam', \n                  metrics=['accuracy']\n                 )\n    \n    return model", "body": "The case is slightly different. I already have NaNs in the model (in the Conv layer) and the model produces correct results on GPU. What's interesting is that if I change the NaNs to minus infinity it produces the same results on CPU.\r\n\r\nTake a look at the weights:\r\n```\r\ninput\r\n[[ -3.76313413e-03   3.56433578e-02  -2.13286784e-02 ...,   4.60807569e-02\r\n    1.64003521e-02   4.84493654e-03]\r\n [ -1.49039514e+03   1.54171045e+03   3.97881046e-02 ...,  -5.78388453e+00\r\n    2.43417993e-02  -1.80144477e+00]\r\n [ -7.30040884e+00  -6.69775963e-01  -2.57102852e+01 ...,   1.29456043e+01\r\n    2.43919420e+00  -2.05847025e-02]\r\n ..., \r\n [ -2.07939720e+00   1.65184236e+00   5.03200665e-02 ...,  -9.66950320e-03\r\n   -3.28505486e-02  -1.60967112e-01]\r\n [  2.00684814e+01  -4.48382616e+00   9.10888481e+00 ...,   5.54858351e+00\r\n    1.92856118e-01  -3.27318764e+00]\r\n [  1.32376838e+00  -8.82686138e+00  -5.80457896e-02 ...,   4.60517597e+00\r\n    1.85110033e-01  -7.66239548e+00]]\r\nembedding_1\r\n[[[ -1.99184626e-01  -6.24039583e-02  -5.06775379e-02 ...,   1.50278226e-01\r\n     1.72856063e-01   9.54210311e-02]\r\n  [  8.40011835e-02  -1.04346856e-01  -1.50010496e-01 ...,  -6.32669926e-02\r\n     3.96224223e-02  -4.31677438e-02]\r\n  [ -8.77222940e-02   5.68446703e-02   5.21141998e-02 ...,  -4.07455601e-02\r\n    -1.41425105e-02  -1.18918315e-01]\r\n  ..., \r\n  [  3.44693642e+01   9.11418945e-02   1.19048271e+01 ...,   1.09344691e-01\r\n     1.87920347e-01   1.70073420e-01]\r\n  [  1.95824668e-01   7.89859332e-03   9.49118510e-02 ...,  -3.29354890e-02\r\n     9.31546241e-02  -1.15318425e-01]\r\n  [  5.89678064e-02   1.41023742e-02   1.11247830e-01 ...,  -9.51733999e-03\r\n    -1.24835804e-01  -1.12498775e-01]]]\r\nconv1d_1\r\n[  1.22442953e-02  -1.09238578e-02  -2.47095129e-04   1.17717348e-02\r\n  -3.92556889e-03  -5.25355758e-03   6.99595024e-04  -1.10572961e-03\r\n  -1.23395994e-02              nan  -1.07570309e-02              nan\r\n  -4.08349838e-03              nan  -4.71794903e-02              nan\r\n  -8.35025460e-02   4.59762150e-03  -1.97616145e-02  -4.07275278e-03\r\n              nan  -2.22547930e-02  -1.18769081e-02  -4.17554192e-03\r\n  -1.11441838e-03  -4.26767347e-03  -7.31970591e-04   1.48863369e-03\r\n  -8.20796192e-03  -5.81618951e-05  -9.59698856e-03   1.10042130e-03\r\n  -2.86204537e-04  -1.64017070e-03  -3.14154982e-04   9.65116825e-03\r\n  -3.80394212e-03  -1.14975907e-02  -1.14118000e-02  -6.63896417e-03\r\n  -1.02947457e-02  -9.60258301e-03  -1.01221241e-02  -3.38621135e-03\r\n  -9.19509027e-03              nan  -2.48736516e-03  -2.78100204e-02\r\n  -6.50373427e-03  -1.12824216e-02  -1.99352647e-03  -2.70526763e-03\r\n  -3.97901144e-03  -8.55945144e-03  -2.47867964e-03  -7.23447651e-03\r\n  -1.89489545e-03              nan   2.34743766e-03  -1.76552832e-02\r\n  -1.13650626e-02  -8.72352626e-03  -1.39924623e-02  -9.96223418e-04\r\n   1.19819669e-02  -1.65980589e-02  -4.43597836e-03  -1.32924877e-03\r\n   5.39703481e-03  -5.20827901e-03   2.09201197e-03  -9.91921872e-04\r\n  -4.76511335e-03  -2.82827131e-02  -3.15303504e-02   7.75449863e-03\r\n  -2.50613503e-03   5.26739890e-03  -9.09337029e-03  -4.06189971e-02\r\n  -2.45879288e-03  -6.14755554e-03  -4.51790220e-05  -1.96866468e-02\r\n  -6.05669746e-04  -1.08566531e-03  -2.80132494e-03   1.35421744e-02\r\n  -1.56419240e-02  -9.22969636e-03  -1.79088686e-03  -6.13477267e-03\r\n   3.58441076e-03  -7.55022466e-03   7.20449630e-03   9.64386389e-03\r\n  -2.01407354e-02   3.64139187e-03  -8.29497911e-03  -9.83765721e-03\r\n  -6.11743657e-03   1.19033596e-03   3.14918067e-03   9.48947109e-03\r\n  -1.80235994e-03  -3.43534909e-03  -9.94156580e-03  -1.02966810e-02\r\n  -1.11212442e-02  -1.83315631e-02   1.77474413e-02  -5.63624455e-03\r\n   1.24310488e-02  -7.89506827e-03   5.76251280e-03   3.04201362e-03\r\n  -9.81117040e-03  -2.30816868e-03  -1.17079932e-02  -3.04500442e-02\r\n   1.04938364e-02  -1.00002484e-02  -5.61673800e-03  -1.49540678e-02\r\n  -4.57247645e-02  -2.97049657e-02  -1.49060250e-03  -4.22246335e-03]\r\n```\r\n\r\nThe model is defined as follows hence the NaNs are in the first layer after Embeddings:\r\n\r\n```\r\ndef _model():\r\n    max_length = 500\r\n    top_words = 5000\r\n    embedding_vecor_length = 60\r\n    classes = 24\r\n    \r\n    _input = Input(shape=(max_length,), name='input')\r\n    embedding=Embedding(top_words, embedding_vecor_length, input_length=max_length)(_input)\r\n    \r\n    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\r\n    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\r\n    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\r\n    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\r\n    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\r\n    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\r\n    \r\n    conv1 = conv1(embedding)\r\n    glob1 = GlobalAveragePooling1D()(conv1)\r\n    conv2 = conv2(embedding)\r\n    glob2 = GlobalAveragePooling1D()(conv2)\r\n    conv3 = conv3(embedding)\r\n    glob3 = GlobalAveragePooling1D()(conv3)\r\n    conv4 = conv4(embedding)\r\n    glob4 = GlobalAveragePooling1D()(conv4)\r\n    conv5 = conv5(embedding)\r\n    glob5 = GlobalAveragePooling1D()(conv5)\r\n    conv6 = conv6(embedding)\r\n    glob6 = GlobalAveragePooling1D()(conv6)\r\n    \r\n    merge = concatenate([glob1, glob2, glob3, glob4, glob5, glob6])\r\n    x = Dropout(0.2)(merge)\r\n    x = BatchNormalization()(x)\r\n    x = Dense(300, activation='relu')(x)\r\n    x = Dropout(0.2)(x)\r\n    x = BatchNormalization()(x)\r\n    pred = Dense(classes, activation='softmax')(x)\r\n    \r\n    model = Model(inputs=[_input], outputs=pred)\r\n    \r\n    model.compile(loss='categorical_crossentropy', \r\n                  optimizer='adam', \r\n                  metrics=['accuracy']\r\n                 )\r\n    \r\n    return model  \r\n```"}