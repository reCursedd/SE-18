{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22788", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22788/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22788/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22788/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/22788", "id": 367414795, "node_id": "MDExOlB1bGxSZXF1ZXN0MjIwODU5MTUx", "number": 22788, "title": "TFTRT User provided INT8 quantization scales", "user": {"login": "trevor-m", "id": 12981474, "node_id": "MDQ6VXNlcjEyOTgxNDc0", "avatar_url": "https://avatars1.githubusercontent.com/u/12981474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trevor-m", "html_url": "https://github.com/trevor-m", "followers_url": "https://api.github.com/users/trevor-m/followers", "following_url": "https://api.github.com/users/trevor-m/following{/other_user}", "gists_url": "https://api.github.com/users/trevor-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/trevor-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trevor-m/subscriptions", "organizations_url": "https://api.github.com/users/trevor-m/orgs", "repos_url": "https://api.github.com/users/trevor-m/repos", "events_url": "https://api.github.com/users/trevor-m/events{/privacy}", "received_events_url": "https://api.github.com/users/trevor-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 987666414, "node_id": "MDU6TGFiZWw5ODc2NjY0MTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/ready%20to%20pull", "name": "ready to pull", "color": "2cd643", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 17, "created_at": "2018-10-06T02:16:41Z", "updated_at": "2018-11-22T09:16:25Z", "closed_at": "2018-11-22T09:16:25Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22788", "html_url": "https://github.com/tensorflow/tensorflow/pull/22788", "diff_url": "https://github.com/tensorflow/tensorflow/pull/22788.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/22788.patch"}, "body_html": "<p>TF-TRT now supports the following quantization nodes:</p>\n<ul>\n<li>QuantizeAndDequantizeV2</li>\n<li>QuantizeAndDequantizeV3</li>\n<li>FakeQuantWithMinMaxVars</li>\n<li>FakeQuantWithMinMaxArgs</li>\n</ul>\n<p>When these nodes are converted:</p>\n<ol>\n<li>Their quantization ranges are extracted and stored</li>\n<li>The nodes are removed</li>\n<li>The ranges are applied to the relevant tensors</li>\n</ol>\n<p>This enables a path for TF-TRT to deploy models trained with quantization in the loop, for example those trained with tf.contrib.quantize.</p>\n<p>trt.create_inference_graph() has a new boolean argument, <code>use_calibration</code>.<br>\nIf we are in INT8 mode and use_calibration=True, create_inference_graph will return a calibration graph just like it did previously.<br>\nThe calibrator will not override ranges provided via quantization nodes.<br>\nIf we are in INT8 mode and use_calibration=False, a warning will be issued for every tensor which does not have a calibration range. Since TRT may fuse some operations, users may not always need to provide a value for these tensors. If a tensor that TRT needs is missing, the conversion will fail.</p>\n<p>This PR also adds support for Relu6 nodes, and fixes a bug with not renaming tensors.</p>\n<p>Current Issues/Considerations:</p>\n<ul>\n<li>If a model was trained with quantization nodes in places where TRT will not quantize (i.e. due to op fusion), then accuracy may drop dramatically. The solution is to figure out which ops will be fused by TRT and avoid placing quantization nodes between those ops. Another option is to place clip ops with each quantization node, so that if the tensor is not quantized it will still be clipped to be in the expected range - this will impact performace negatively.</li>\n<li>TRT does not have documentation for op fusion yet.</li>\n<li>TRT only supports symmetric quantization. Ranges are converted to symmetric ranges using <code>max(abs(min_range), abs(max_range))</code>.</li>\n<li>Since the tensors which are inputs and outputs to the TRTEngine are renamed to \"TensorRTInputPH_{X}\", \"TensorRTOutputPH_{X}\", if these tensors are missing ranges the warnings will not make sense to users.</li>\n<li>MatMul and BiasAdd are not fused, requiring users to provide a range between these ops.</li>\n</ul>", "body_text": "TF-TRT now supports the following quantization nodes:\n\nQuantizeAndDequantizeV2\nQuantizeAndDequantizeV3\nFakeQuantWithMinMaxVars\nFakeQuantWithMinMaxArgs\n\nWhen these nodes are converted:\n\nTheir quantization ranges are extracted and stored\nThe nodes are removed\nThe ranges are applied to the relevant tensors\n\nThis enables a path for TF-TRT to deploy models trained with quantization in the loop, for example those trained with tf.contrib.quantize.\ntrt.create_inference_graph() has a new boolean argument, use_calibration.\nIf we are in INT8 mode and use_calibration=True, create_inference_graph will return a calibration graph just like it did previously.\nThe calibrator will not override ranges provided via quantization nodes.\nIf we are in INT8 mode and use_calibration=False, a warning will be issued for every tensor which does not have a calibration range. Since TRT may fuse some operations, users may not always need to provide a value for these tensors. If a tensor that TRT needs is missing, the conversion will fail.\nThis PR also adds support for Relu6 nodes, and fixes a bug with not renaming tensors.\nCurrent Issues/Considerations:\n\nIf a model was trained with quantization nodes in places where TRT will not quantize (i.e. due to op fusion), then accuracy may drop dramatically. The solution is to figure out which ops will be fused by TRT and avoid placing quantization nodes between those ops. Another option is to place clip ops with each quantization node, so that if the tensor is not quantized it will still be clipped to be in the expected range - this will impact performace negatively.\nTRT does not have documentation for op fusion yet.\nTRT only supports symmetric quantization. Ranges are converted to symmetric ranges using max(abs(min_range), abs(max_range)).\nSince the tensors which are inputs and outputs to the TRTEngine are renamed to \"TensorRTInputPH_{X}\", \"TensorRTOutputPH_{X}\", if these tensors are missing ranges the warnings will not make sense to users.\nMatMul and BiasAdd are not fused, requiring users to provide a range between these ops.", "body": "TF-TRT now supports the following quantization nodes:\r\n- QuantizeAndDequantizeV2\r\n- QuantizeAndDequantizeV3\r\n- FakeQuantWithMinMaxVars\r\n- FakeQuantWithMinMaxArgs\r\n\r\nWhen these nodes are converted:\r\n1. Their quantization ranges are extracted and stored\r\n2. The nodes are removed\r\n3. The ranges are applied to the relevant tensors\r\n\r\nThis enables a path for TF-TRT to deploy models trained with quantization in the loop, for example those trained with tf.contrib.quantize.\r\n\r\ntrt.create_inference_graph() has a new boolean argument, `use_calibration`.\r\nIf we are in INT8 mode and use_calibration=True, create_inference_graph will return a calibration graph just like it did previously.\r\nThe calibrator will not override ranges provided via quantization nodes.\r\nIf we are in INT8 mode and use_calibration=False, a warning will be issued for every tensor which does not have a calibration range. Since TRT may fuse some operations, users may not always need to provide a value for these tensors. If a tensor that TRT needs is missing, the conversion will fail.\r\n\r\nThis PR also adds support for Relu6 nodes, and fixes a bug with not renaming tensors.\r\n\r\nCurrent Issues/Considerations:\r\n- If a model was trained with quantization nodes in places where TRT will not quantize (i.e. due to op fusion), then accuracy may drop dramatically. The solution is to figure out which ops will be fused by TRT and avoid placing quantization nodes between those ops. Another option is to place clip ops with each quantization node, so that if the tensor is not quantized it will still be clipped to be in the expected range - this will impact performace negatively.\r\n- TRT does not have documentation for op fusion yet.\r\n- TRT only supports symmetric quantization. Ranges are converted to symmetric ranges using `max(abs(min_range), abs(max_range))`.\r\n- Since the tensors which are inputs and outputs to the TRTEngine are renamed to \"TensorRTInputPH_{X}\", \"TensorRTOutputPH_{X}\", if these tensors are missing ranges the warnings will not make sense to users.\r\n- MatMul and BiasAdd are not fused, requiring users to provide a range between these ops."}