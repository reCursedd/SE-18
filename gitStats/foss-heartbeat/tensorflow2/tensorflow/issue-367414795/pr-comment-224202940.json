{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/224202940", "pull_request_review_id": 163509561, "id": 224202940, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNDIwMjk0MA==", "diff_hunk": "@@ -1577,6 +1712,104 @@ tensorflow::Status ConvertActivation(\n   return tensorflow::Status::OK();\n }\n \n+tensorflow::Status ConvertQuantize(\n+    Converter& ctx, const tensorflow::NodeDef& node_def,\n+    const std::vector<TRT_TensorOrWeights>& inputs,\n+    std::vector<TRT_TensorOrWeights>* outputs) {\n+  if (inputs.at(0).is_weights()) {\n+    outputs->push_back(inputs.at(0));\n+    return tensorflow::Status::OK();\n+  }\n+  // ***************************************************************************\n+  // Fake quantization nodes are replaced with no-ops and their min/max values\n+  // are applied as the quantization scale for the input and output\n+  // ***************************************************************************\n+  nvinfer1::ITensor* tensor = const_cast<nvinfer1::ITensor*>(inputs.at(0).tensor());\n+  // Min\n+  TRT_ShapedWeights weights_min = inputs.at(1).weights();\n+  auto weights_min_ptr = static_cast<float*>(const_cast<void*>(weights_min.GetValues()));\n+  float min_range = weights_min_ptr[0];\n+  // Max\n+  TRT_ShapedWeights weights_max = inputs.at(2).weights();\n+  auto weights_max_ptr = static_cast<float*>(const_cast<void*>(weights_max.GetValues()));\n+  float max_range = weights_max_ptr[0];\n+  \n+  // The default shuffle layer serves as a dummy layer which will be optimized\n+  // away. Identity layer does not get optimized away as of TRT 5.0, but\n+  // once we know that it does it would be nice to use that instead.\n+  nvinfer1::IShuffleLayer* layer;\n+  layer = ctx.network()->addShuffle(*tensor);\n+  TFTRT_RETURN_ERROR_IF_NULLPTR(layer, node_def.name());\n+  nvinfer1::ITensor* output_tensor = layer->getOutput(0);\n+\n+  // Store ranges for input and output of dummy op\n+  ctx.ProvideQuantizationRange(tensor, min_range, max_range);", "path": "tensorflow/contrib/tensorrt/convert/convert_nodes.cc", "position": null, "original_position": 233, "commit_id": "c9774910f1e24b964f0a31fdbf98df472d92501b", "original_commit_id": "57f6ba3434a9f47ffac4957555096c7cc0930f4a", "user": {"login": "wujingyue", "id": 2772612, "node_id": "MDQ6VXNlcjI3NzI2MTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2772612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wujingyue", "html_url": "https://github.com/wujingyue", "followers_url": "https://api.github.com/users/wujingyue/followers", "following_url": "https://api.github.com/users/wujingyue/following{/other_user}", "gists_url": "https://api.github.com/users/wujingyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/wujingyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wujingyue/subscriptions", "organizations_url": "https://api.github.com/users/wujingyue/orgs", "repos_url": "https://api.github.com/users/wujingyue/repos", "events_url": "https://api.github.com/users/wujingyue/events{/privacy}", "received_events_url": "https://api.github.com/users/wujingyue/received_events", "type": "User", "site_admin": false}, "body": "You would need mixed-precision to solve these cases. One potential solution is to keep the tensor in fp and quantize it on one path bot not the other, or quantize it differently on both paths. \r\n\r\nGiven TensorRT doesn't plan to support mixed-precision in short term, I'd emit a fatal error if this happens. We can make sure quantized training doesn't produce graphs like that at the first place. ", "created_at": "2018-10-10T19:00:35Z", "updated_at": "2018-11-21T23:48:52Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22788#discussion_r224202940", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22788", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/224202940"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22788#discussion_r224202940"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22788"}}, "body_html": "<p>You would need mixed-precision to solve these cases. One potential solution is to keep the tensor in fp and quantize it on one path bot not the other, or quantize it differently on both paths.</p>\n<p>Given TensorRT doesn't plan to support mixed-precision in short term, I'd emit a fatal error if this happens. We can make sure quantized training doesn't produce graphs like that at the first place.</p>", "body_text": "You would need mixed-precision to solve these cases. One potential solution is to keep the tensor in fp and quantize it on one path bot not the other, or quantize it differently on both paths.\nGiven TensorRT doesn't plan to support mixed-precision in short term, I'd emit a fatal error if this happens. We can make sure quantized training doesn't produce graphs like that at the first place.", "in_reply_to_id": 224151450}