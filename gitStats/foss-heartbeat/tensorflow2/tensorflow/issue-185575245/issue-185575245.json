{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5228", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5228/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5228/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5228/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5228", "id": 185575245, "node_id": "MDU6SXNzdWUxODU1NzUyNDU=", "number": 5228, "title": "Poor VGG performance in slim.", "user": {"login": "warmspringwinds", "id": 2501383, "node_id": "MDQ6VXNlcjI1MDEzODM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2501383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/warmspringwinds", "html_url": "https://github.com/warmspringwinds", "followers_url": "https://api.github.com/users/warmspringwinds/followers", "following_url": "https://api.github.com/users/warmspringwinds/following{/other_user}", "gists_url": "https://api.github.com/users/warmspringwinds/gists{/gist_id}", "starred_url": "https://api.github.com/users/warmspringwinds/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/warmspringwinds/subscriptions", "organizations_url": "https://api.github.com/users/warmspringwinds/orgs", "repos_url": "https://api.github.com/users/warmspringwinds/repos", "events_url": "https://api.github.com/users/warmspringwinds/events{/privacy}", "received_events_url": "https://api.github.com/users/warmspringwinds/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-10-27T05:16:58Z", "updated_at": "2017-02-09T22:36:18Z", "closed_at": "2016-10-27T17:31:04Z", "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>I have been following the slim tutorial:<br>\n<a href=\"https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb\">https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb</a></p>\n<p>And decided to replace the Inception model v1 that is used in the example with VGG model<br>\nfrom tensorflow. I took it from here:<br>\n<a href=\"https://github.com/tensorflow/models/tree/master/slim#pre-trained-models\">https://github.com/tensorflow/models/tree/master/slim#pre-trained-models</a></p>\n<p>I have changed the code to work with VGG, but I got a considerably worse accuracy<br>\neven on simple images for VGG, while it works perfectly for Inception v1. Even though the<br>\nreported accuracy for VGG and Inception v1 are more or less equal in the imagenet.</p>\n<p>I don't know if there is a bug in my code or the model is bad.</p>\n<p>Here is the code that I have used:</p>\n<pre><code>%matplotlib inline\n\nfrom matplotlib import pyplot as plt\n\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport urllib2\n\nfrom datasets import imagenet\nfrom nets import vgg\nfrom preprocessing import vgg_preprocessing\n\nslim = tf.contrib.slim\n\nimage_size = vgg.vgg_19.default_image_size\n\ncheckpoints_dir = '/tmp/checkpoints'\n\n\n\nwith tf.Graph().as_default():\n    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'\n    url = 'https://upload.wikimedia.org/wikipedia/commons/d/d9/First_Student_IC_school_bus_202076.jpg'\n    image_string = urllib2.urlopen(url).read()\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    processed_image = vgg_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)\n    processed_images  = tf.expand_dims(processed_image, 0)\n\n    # Create the model, use the default arg scope to configure the batch norm parameters.\n    with slim.arg_scope(vgg.vgg_arg_scope()):\n        logits, _ = vgg.vgg_19(processed_images, num_classes=1000, is_training=False)\n    probabilities = tf.nn.softmax(logits)\n\n    init_fn = slim.assign_from_checkpoint_fn(\n        os.path.join(checkpoints_dir, 'vgg_19.ckpt'),\n        slim.get_model_variables('vgg_19'))\n\n    with tf.Session() as sess:\n        init_fn(sess)\n        np_image, probabilities = sess.run([image, probabilities])\n        probabilities = probabilities[0, 0:]\n        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])]\n\n    plt.figure()\n    plt.imshow(np_image.astype(np.uint8))\n    plt.axis('off')\n    plt.show()\n\n\n    names = imagenet.create_readable_names_for_imagenet_labels()\n    for i in range(5):\n        index = sorted_inds[i]\n        print('Probability %0.2f%% =&gt; [%s]' % (probabilities[index], names[index]))\n\n</code></pre>", "body_text": "Hello,\nI have been following the slim tutorial:\nhttps://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb\nAnd decided to replace the Inception model v1 that is used in the example with VGG model\nfrom tensorflow. I took it from here:\nhttps://github.com/tensorflow/models/tree/master/slim#pre-trained-models\nI have changed the code to work with VGG, but I got a considerably worse accuracy\neven on simple images for VGG, while it works perfectly for Inception v1. Even though the\nreported accuracy for VGG and Inception v1 are more or less equal in the imagenet.\nI don't know if there is a bug in my code or the model is bad.\nHere is the code that I have used:\n%matplotlib inline\n\nfrom matplotlib import pyplot as plt\n\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport urllib2\n\nfrom datasets import imagenet\nfrom nets import vgg\nfrom preprocessing import vgg_preprocessing\n\nslim = tf.contrib.slim\n\nimage_size = vgg.vgg_19.default_image_size\n\ncheckpoints_dir = '/tmp/checkpoints'\n\n\n\nwith tf.Graph().as_default():\n    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'\n    url = 'https://upload.wikimedia.org/wikipedia/commons/d/d9/First_Student_IC_school_bus_202076.jpg'\n    image_string = urllib2.urlopen(url).read()\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    processed_image = vgg_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)\n    processed_images  = tf.expand_dims(processed_image, 0)\n\n    # Create the model, use the default arg scope to configure the batch norm parameters.\n    with slim.arg_scope(vgg.vgg_arg_scope()):\n        logits, _ = vgg.vgg_19(processed_images, num_classes=1000, is_training=False)\n    probabilities = tf.nn.softmax(logits)\n\n    init_fn = slim.assign_from_checkpoint_fn(\n        os.path.join(checkpoints_dir, 'vgg_19.ckpt'),\n        slim.get_model_variables('vgg_19'))\n\n    with tf.Session() as sess:\n        init_fn(sess)\n        np_image, probabilities = sess.run([image, probabilities])\n        probabilities = probabilities[0, 0:]\n        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])]\n\n    plt.figure()\n    plt.imshow(np_image.astype(np.uint8))\n    plt.axis('off')\n    plt.show()\n\n\n    names = imagenet.create_readable_names_for_imagenet_labels()\n    for i in range(5):\n        index = sorted_inds[i]\n        print('Probability %0.2f%% => [%s]' % (probabilities[index], names[index]))", "body": "Hello,\n\nI have been following the slim tutorial:\nhttps://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb\n\nAnd decided to replace the Inception model v1 that is used in the example with VGG model\nfrom tensorflow. I took it from here:\nhttps://github.com/tensorflow/models/tree/master/slim#pre-trained-models\n\nI have changed the code to work with VGG, but I got a considerably worse accuracy\neven on simple images for VGG, while it works perfectly for Inception v1. Even though the\nreported accuracy for VGG and Inception v1 are more or less equal in the imagenet.\n\nI don't know if there is a bug in my code or the model is bad.\n\nHere is the code that I have used:\n\n```\n%matplotlib inline\n\nfrom matplotlib import pyplot as plt\n\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport urllib2\n\nfrom datasets import imagenet\nfrom nets import vgg\nfrom preprocessing import vgg_preprocessing\n\nslim = tf.contrib.slim\n\nimage_size = vgg.vgg_19.default_image_size\n\ncheckpoints_dir = '/tmp/checkpoints'\n\n\n\nwith tf.Graph().as_default():\n    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'\n    url = 'https://upload.wikimedia.org/wikipedia/commons/d/d9/First_Student_IC_school_bus_202076.jpg'\n    image_string = urllib2.urlopen(url).read()\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    processed_image = vgg_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)\n    processed_images  = tf.expand_dims(processed_image, 0)\n\n    # Create the model, use the default arg scope to configure the batch norm parameters.\n    with slim.arg_scope(vgg.vgg_arg_scope()):\n        logits, _ = vgg.vgg_19(processed_images, num_classes=1000, is_training=False)\n    probabilities = tf.nn.softmax(logits)\n\n    init_fn = slim.assign_from_checkpoint_fn(\n        os.path.join(checkpoints_dir, 'vgg_19.ckpt'),\n        slim.get_model_variables('vgg_19'))\n\n    with tf.Session() as sess:\n        init_fn(sess)\n        np_image, probabilities = sess.run([image, probabilities])\n        probabilities = probabilities[0, 0:]\n        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])]\n\n    plt.figure()\n    plt.imshow(np_image.astype(np.uint8))\n    plt.axis('off')\n    plt.show()\n\n\n    names = imagenet.create_readable_names_for_imagenet_labels()\n    for i in range(5):\n        index = sorted_inds[i]\n        print('Probability %0.2f%% => [%s]' % (probabilities[index], names[index]))\n\n```\n"}