{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/769", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/769/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/769/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/769/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/769", "id": 126573831, "node_id": "MDU6SXNzdWUxMjY1NzM4MzE=", "number": 769, "title": "\"Deep MNIST for Experts\" too complex for second tutorial.", "user": {"login": "NHDaly", "id": 1582097, "node_id": "MDQ6VXNlcjE1ODIwOTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1582097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NHDaly", "html_url": "https://github.com/NHDaly", "followers_url": "https://api.github.com/users/NHDaly/followers", "following_url": "https://api.github.com/users/NHDaly/following{/other_user}", "gists_url": "https://api.github.com/users/NHDaly/gists{/gist_id}", "starred_url": "https://api.github.com/users/NHDaly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NHDaly/subscriptions", "organizations_url": "https://api.github.com/users/NHDaly/orgs", "repos_url": "https://api.github.com/users/NHDaly/repos", "events_url": "https://api.github.com/users/NHDaly/events{/privacy}", "received_events_url": "https://api.github.com/users/NHDaly/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-01-14T04:16:00Z", "updated_at": "2016-06-29T09:38:06Z", "closed_at": "2016-01-14T16:56:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The end of the first tutorial, \"MNIST for ML Beginners\", has this paragraph:</p>\n<blockquote>\n<p>What matters is that we learned from this model. Still, if you're feeling a bit down about these results, check out <a href=\"https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts\" rel=\"nofollow\">the next tutorial</a> where we do a lot better, and learn how to build more sophisticated models using TensorFlow!</p>\n</blockquote>\n<p>But the next tutorial is \"Deep MNIST for Experts\", and it does not explain things nearly as well as the first tutorial. For example, the second tutorial starts differing at the <code>Build a Multilayer Convolutional Network</code> section, but the very first paragraph under <code>Weight Initialization</code> does not explain the concepts it is using:</p>\n<blockquote>\n<p>To create this model, we're going to need to create a lot of weights and biases. One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons.\" Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us.</p>\n</blockquote>\n<p>What are \"ReLU neurons\"? Why are we using them? What <em>is</em> a convolutional network even?</p>\n<p>I would say, either</p>\n<ol>\n<li>This tutorial should be expanded to clarify/explain things better for beginners, or</li>\n<li>This tutorial should be moved to later in the list of tutorials, and</li>\n<li>The first tutorial, \"MNIST for Beginners,\" should not say <code>the next tutorial</code> but instead say <code>a later tutorial on MNIST for Experts</code>.</li>\n</ol>\n<p>Thanks!</p>", "body_text": "The end of the first tutorial, \"MNIST for ML Beginners\", has this paragraph:\n\nWhat matters is that we learned from this model. Still, if you're feeling a bit down about these results, check out the next tutorial where we do a lot better, and learn how to build more sophisticated models using TensorFlow!\n\nBut the next tutorial is \"Deep MNIST for Experts\", and it does not explain things nearly as well as the first tutorial. For example, the second tutorial starts differing at the Build a Multilayer Convolutional Network section, but the very first paragraph under Weight Initialization does not explain the concepts it is using:\n\nTo create this model, we're going to need to create a lot of weights and biases. One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons.\" Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us.\n\nWhat are \"ReLU neurons\"? Why are we using them? What is a convolutional network even?\nI would say, either\n\nThis tutorial should be expanded to clarify/explain things better for beginners, or\nThis tutorial should be moved to later in the list of tutorials, and\nThe first tutorial, \"MNIST for Beginners,\" should not say the next tutorial but instead say a later tutorial on MNIST for Experts.\n\nThanks!", "body": "The end of the first tutorial, \"MNIST for ML Beginners\", has this paragraph:\n\n> What matters is that we learned from this model. Still, if you're feeling a bit down about these results, check out [the next tutorial](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts) where we do a lot better, and learn how to build more sophisticated models using TensorFlow!\n\nBut the next tutorial is \"Deep MNIST for Experts\", and it does not explain things nearly as well as the first tutorial. For example, the second tutorial starts differing at the `Build a Multilayer Convolutional Network` section, but the very first paragraph under `Weight Initialization` does not explain the concepts it is using:\n\n> To create this model, we're going to need to create a lot of weights and biases. One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons.\" Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us.\n\nWhat are \"ReLU neurons\"? Why are we using them? What _is_ a convolutional network even?\n\nI would say, either\n1. This tutorial should be expanded to clarify/explain things better for beginners, or\n2. This tutorial should be moved to later in the list of tutorials, and\n3. The first tutorial, \"MNIST for Beginners,\" should not say `the next tutorial` but instead say `a later tutorial on MNIST for Experts`.\n\nThanks!\n"}