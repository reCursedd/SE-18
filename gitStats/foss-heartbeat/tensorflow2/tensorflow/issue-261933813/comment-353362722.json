{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/353362722", "html_url": "https://github.com/tensorflow/tensorflow/issues/13433#issuecomment-353362722", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13433", "id": 353362722, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzM2MjcyMg==", "user": {"login": "marek-krcal", "id": 26710141, "node_id": "MDQ6VXNlcjI2NzEwMTQx", "avatar_url": "https://avatars0.githubusercontent.com/u/26710141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marek-krcal", "html_url": "https://github.com/marek-krcal", "followers_url": "https://api.github.com/users/marek-krcal/followers", "following_url": "https://api.github.com/users/marek-krcal/following{/other_user}", "gists_url": "https://api.github.com/users/marek-krcal/gists{/gist_id}", "starred_url": "https://api.github.com/users/marek-krcal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marek-krcal/subscriptions", "organizations_url": "https://api.github.com/users/marek-krcal/orgs", "repos_url": "https://api.github.com/users/marek-krcal/repos", "events_url": "https://api.github.com/users/marek-krcal/events{/privacy}", "received_events_url": "https://api.github.com/users/marek-krcal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-21T14:22:00Z", "updated_at": "2017-12-21T16:15:38Z", "author_association": "NONE", "body_html": "<p>A different issue on a very related note:<br>\nMy features are good enough when rounded to half-precision (float16) - actually I get even slightly better outcomes probably due to a regularization effect.<br>\nI would like to enjoy the half memory footprint of such dataset. However, once my variable slightly exceeds some number around 4GB, I get an error while assigning at that the offset=number:</p>\n<pre><code>dtype = 'float16'\nfeatures = np.ones((4000000,538),dtype)\nshape = features.shape\nprint(features.nbytes) #prints 4304000000\nfeatures_var = tf.Variable(tf.ones(shape,dtype=dtype),trainable=False,dtype=dtype)\nefficient_init = tf.contrib.framework.zero_initializer(features_var)\nprint('initializing data variable features_var by zeros')\nsess.run(efficient_init)\nsess.run(tf.assign(features_var[:1000,:],features[:1000,:])) #this is OK\nsess.run(tf.assign(features_var[3900000:3901000,:],features[:1000,:])) #up to this point still OK\nsess.run(tf.assign(features_var[3990000:3991000,:],features[-1000:,:]))#this throws an error\nsess.run(tf.assign(features_var[-1000:,:],features[-1000:,:])) #this throws an error\n</code></pre>\n<p>My Jupyter kernel fails with no error printed and some lines containing <code>Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS</code> in my jupyter logfile.</p>\n<p>With features having the shape=(3900000,538), everything goes fine. I.e., afterall the limit on the number of datapoints turns out to be (almost) the same as if I used single precision (float32).</p>\n<p>If for any of the above problems the \"memory allocation timeline\" from \"mem_utils\" of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> would help, I could try to produce that as well.</p>", "body_text": "A different issue on a very related note:\nMy features are good enough when rounded to half-precision (float16) - actually I get even slightly better outcomes probably due to a regularization effect.\nI would like to enjoy the half memory footprint of such dataset. However, once my variable slightly exceeds some number around 4GB, I get an error while assigning at that the offset=number:\ndtype = 'float16'\nfeatures = np.ones((4000000,538),dtype)\nshape = features.shape\nprint(features.nbytes) #prints 4304000000\nfeatures_var = tf.Variable(tf.ones(shape,dtype=dtype),trainable=False,dtype=dtype)\nefficient_init = tf.contrib.framework.zero_initializer(features_var)\nprint('initializing data variable features_var by zeros')\nsess.run(efficient_init)\nsess.run(tf.assign(features_var[:1000,:],features[:1000,:])) #this is OK\nsess.run(tf.assign(features_var[3900000:3901000,:],features[:1000,:])) #up to this point still OK\nsess.run(tf.assign(features_var[3990000:3991000,:],features[-1000:,:]))#this throws an error\nsess.run(tf.assign(features_var[-1000:,:],features[-1000:,:])) #this throws an error\n\nMy Jupyter kernel fails with no error printed and some lines containing Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS in my jupyter logfile.\nWith features having the shape=(3900000,538), everything goes fine. I.e., afterall the limit on the number of datapoints turns out to be (almost) the same as if I used single precision (float32).\nIf for any of the above problems the \"memory allocation timeline\" from \"mem_utils\" of @yaroslavvb would help, I could try to produce that as well.", "body": "A different issue on a very related note:\r\nMy features are good enough when rounded to half-precision (float16) - actually I get even slightly better outcomes probably due to a regularization effect.\r\nI would like to enjoy the half memory footprint of such dataset. However, once my variable slightly exceeds some number around 4GB, I get an error while assigning at that the offset=number:\r\n```\r\ndtype = 'float16'\r\nfeatures = np.ones((4000000,538),dtype)\r\nshape = features.shape\r\nprint(features.nbytes) #prints 4304000000\r\nfeatures_var = tf.Variable(tf.ones(shape,dtype=dtype),trainable=False,dtype=dtype)\r\nefficient_init = tf.contrib.framework.zero_initializer(features_var)\r\nprint('initializing data variable features_var by zeros')\r\nsess.run(efficient_init)\r\nsess.run(tf.assign(features_var[:1000,:],features[:1000,:])) #this is OK\r\nsess.run(tf.assign(features_var[3900000:3901000,:],features[:1000,:])) #up to this point still OK\r\nsess.run(tf.assign(features_var[3990000:3991000,:],features[-1000:,:]))#this throws an error\r\nsess.run(tf.assign(features_var[-1000:,:],features[-1000:,:])) #this throws an error\r\n```\r\nMy Jupyter kernel fails with no error printed and some lines containing `Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS` in my jupyter logfile.\r\n\r\nWith features having the shape=(3900000,538), everything goes fine. I.e., afterall the limit on the number of datapoints turns out to be (almost) the same as if I used single precision (float32).\r\n\r\nIf for any of the above problems the \"memory allocation timeline\" from \"mem_utils\" of @yaroslavvb would help, I could try to produce that as well."}