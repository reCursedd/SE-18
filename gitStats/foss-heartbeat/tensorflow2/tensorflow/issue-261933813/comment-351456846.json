{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/351456846", "html_url": "https://github.com/tensorflow/tensorflow/issues/13433#issuecomment-351456846", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13433", "id": 351456846, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTQ1Njg0Ng==", "user": {"login": "marek-krcal", "id": 26710141, "node_id": "MDQ6VXNlcjI2NzEwMTQx", "avatar_url": "https://avatars0.githubusercontent.com/u/26710141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marek-krcal", "html_url": "https://github.com/marek-krcal", "followers_url": "https://api.github.com/users/marek-krcal/followers", "following_url": "https://api.github.com/users/marek-krcal/following{/other_user}", "gists_url": "https://api.github.com/users/marek-krcal/gists{/gist_id}", "starred_url": "https://api.github.com/users/marek-krcal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marek-krcal/subscriptions", "organizations_url": "https://api.github.com/users/marek-krcal/orgs", "repos_url": "https://api.github.com/users/marek-krcal/repos", "events_url": "https://api.github.com/users/marek-krcal/events{/privacy}", "received_events_url": "https://api.github.com/users/marek-krcal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-13T17:07:42Z", "updated_at": "2017-12-13T17:07:42Z", "author_association": "NONE", "body_html": "<p>I agree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1831252\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/georgh\">@georgh</a>:<br>\nI believe my deep learning work is pretty down-to-earth but already two times in past several months I came across the urgent need of sending the training (and testing) dataset to the gpu memory.<br>\nA simple minded way in Keras is seting up a large fixed embedding layer, but then a \"protobuf error\" appears when the size exceeds 2GB (the limitation seem artificial to me, but I admit, I don't see into the core of tensorflow at all).<br>\nThen I tried to work in pure tensorflow in the sense of this post but it turns out that unless I use some hardly documented hack, I can use only half of the gpu memory (I don't understand why other dozens of people are not complaining here:)).</p>", "body_text": "I agree with @georgh:\nI believe my deep learning work is pretty down-to-earth but already two times in past several months I came across the urgent need of sending the training (and testing) dataset to the gpu memory.\nA simple minded way in Keras is seting up a large fixed embedding layer, but then a \"protobuf error\" appears when the size exceeds 2GB (the limitation seem artificial to me, but I admit, I don't see into the core of tensorflow at all).\nThen I tried to work in pure tensorflow in the sense of this post but it turns out that unless I use some hardly documented hack, I can use only half of the gpu memory (I don't understand why other dozens of people are not complaining here:)).", "body": "I agree with @georgh:\r\nI believe my deep learning work is pretty down-to-earth but already two times in past several months I came across the urgent need of sending the training (and testing) dataset to the gpu memory.\r\nA simple minded way in Keras is seting up a large fixed embedding layer, but then a \"protobuf error\" appears when the size exceeds 2GB (the limitation seem artificial to me, but I admit, I don't see into the core of tensorflow at all). \r\nThen I tried to work in pure tensorflow in the sense of this post but it turns out that unless I use some hardly documented hack, I can use only half of the gpu memory (I don't understand why other dozens of people are not complaining here:))."}