{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11589", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11589/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11589/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11589/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11589", "id": 243855899, "node_id": "MDU6SXNzdWUyNDM4NTU4OTk=", "number": 11589, "title": "[beginner][bug] TensorFlow doesn't seem to be decomposing GradientDescent into XLA ops", "user": {"login": "singam-sanjay", "id": 6310523, "node_id": "MDQ6VXNlcjYzMTA1MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6310523?v=4", "gravatar_id": "", "url": "https://api.github.com/users/singam-sanjay", "html_url": "https://github.com/singam-sanjay", "followers_url": "https://api.github.com/users/singam-sanjay/followers", "following_url": "https://api.github.com/users/singam-sanjay/following{/other_user}", "gists_url": "https://api.github.com/users/singam-sanjay/gists{/gist_id}", "starred_url": "https://api.github.com/users/singam-sanjay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/singam-sanjay/subscriptions", "organizations_url": "https://api.github.com/users/singam-sanjay/orgs", "repos_url": "https://api.github.com/users/singam-sanjay/repos", "events_url": "https://api.github.com/users/singam-sanjay/events{/privacy}", "received_events_url": "https://api.github.com/users/singam-sanjay/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-07-18T21:23:51Z", "updated_at": "2018-01-09T10:14:15Z", "closed_at": "2018-01-09T10:14:15Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Tensorflow compiled on a branch of a fork</strong>: <a href=\"https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops\">https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops</a>. Contains extra LOG(INFO) and std::cout statements to notify the names of the functions being called.</li>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.2.0-1878-ga5066f6', '1.2.1')</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: release 0.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 8</li>\n<li><strong>GPU model and memory</strong>: NVIDIA Quadro K1200,  4 GB</li>\n<li><strong>Exact command to reproduce</strong>: python &lt;tflow_parent&gt;/tensorflow/tensorflow/example/tutorials/mnist/mnist/mnist_softmax_xla.py<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1157069/tf_env.txt\">tf_env.txt</a></li>\n</ul>\n<h3>Context</h3>\n<p>My work involves securing Tensorflow's computations. The approach I've planned to take is to add XLA ops during the conversion of TF ops to XLA ops,  which are lowered into functions calls (in LLVM-IR) that send data to and receive data from a secure environment.</p>\n<p>I'm currently verifying if both training and inference TF Ops are lowered to XLA ops.</p>\n<h3>The Problem</h3>\n<p>I've made some <a href=\"https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops\">changes to the code</a> to highlight when some OpKernels and XlaOpKernels are being invoked / lowered. I've used the mnist_softmax_xla.py example to observe tensorflow's behaviour.</p>\n<p>I found that \"ApplyGradientDescentGPU\" was being called 2000 times (2 layer network and 1000 iterations, clearly not JIT) instead of ResourceApplyGradientDescent, yet other Ops including MatMul and subtraction seem to have been optimized by XLA.</p>\n<p>Why is gradient descent not optimized by XLA ?</p>\n<h3>Source code / logs</h3>\n<p>The output text lost its structure when I pasted it. So, here are the screenshots.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/6310523/28340192-80abeec2-6c2c-11e7-8fb3-d3ec6ae1676e.PNG\"><img src=\"https://user-images.githubusercontent.com/6310523/28340192-80abeec2-6c2c-11e7-8fb3-d3ec6ae1676e.PNG\" alt=\"github 1\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/6310523/28340198-8593c450-6c2c-11e7-9c62-dd06e93e2b5a.PNG\"><img src=\"https://user-images.githubusercontent.com/6310523/28340198-8593c450-6c2c-11e7-9c62-dd06e93e2b5a.PNG\" alt=\"github 2\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/6310523/28340204-8909b69e-6c2c-11e7-9e16-afe089ccde8e.PNG\"><img src=\"https://user-images.githubusercontent.com/6310523/28340204-8909b69e-6c2c-11e7-9e16-afe089ccde8e.PNG\" alt=\"github 3\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/6310523/28340210-8c3c4a02-6c2c-11e7-93b1-a416035b3ed8.PNG\"><img src=\"https://user-images.githubusercontent.com/6310523/28340210-8c3c4a02-6c2c-11e7-93b1-a416035b3ed8.PNG\" alt=\"github 4\" style=\"max-width:100%;\"></a> Note that the first 2 kernels have been called a 100 times each.</p>\n<h3>Related</h3>\n<p>Also, it would be helpful if someone could answer <a href=\"https://stackoverflow.com/questions/45146444/tensorflow-xla-how-are-tf-ops-lowered-to-xla-for-training\" rel=\"nofollow\">Tensow - XLA | Passing tensors to external functions at runtime</a> , as it is <strong>the</strong> important assumption of my approach that Tensorflow can allow such XLA ops. I appreciate any information on its feasibility as I could start on a different approach as soon as possible.</p>\n<p>Thanks,<br>\nSanjay</p>", "body_text": "System information\n\nTensorflow compiled on a branch of a fork: https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops. Contains extra LOG(INFO) and std::cout statements to notify the names of the functions being called.\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): ('v1.2.0-1878-ga5066f6', '1.2.1')\nPython version: 2.7.12\nBazel version (if compiling from source): release 0.5.2\nCUDA/cuDNN version: CUDA 8\nGPU model and memory: NVIDIA Quadro K1200,  4 GB\nExact command to reproduce: python <tflow_parent>/tensorflow/tensorflow/example/tutorials/mnist/mnist/mnist_softmax_xla.py\ntf_env.txt\n\nContext\nMy work involves securing Tensorflow's computations. The approach I've planned to take is to add XLA ops during the conversion of TF ops to XLA ops,  which are lowered into functions calls (in LLVM-IR) that send data to and receive data from a secure environment.\nI'm currently verifying if both training and inference TF Ops are lowered to XLA ops.\nThe Problem\nI've made some changes to the code to highlight when some OpKernels and XlaOpKernels are being invoked / lowered. I've used the mnist_softmax_xla.py example to observe tensorflow's behaviour.\nI found that \"ApplyGradientDescentGPU\" was being called 2000 times (2 layer network and 1000 iterations, clearly not JIT) instead of ResourceApplyGradientDescent, yet other Ops including MatMul and subtraction seem to have been optimized by XLA.\nWhy is gradient descent not optimized by XLA ?\nSource code / logs\nThe output text lost its structure when I pasted it. So, here are the screenshots.\n\n\n\n Note that the first 2 kernels have been called a 100 times each.\nRelated\nAlso, it would be helpful if someone could answer Tensow - XLA | Passing tensors to external functions at runtime , as it is the important assumption of my approach that Tensorflow can allow such XLA ops. I appreciate any information on its feasibility as I could start on a different approach as soon as possible.\nThanks,\nSanjay", "body": "### System information\r\n- **Tensorflow compiled on a branch of a fork**: https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops. Contains extra LOG(INFO) and std::cout statements to notify the names of the functions being called.\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.2.0-1878-ga5066f6', '1.2.1')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: release 0.5.2\r\n- **CUDA/cuDNN version**: CUDA 8\r\n- **GPU model and memory**: NVIDIA Quadro K1200,  4 GB\r\n- **Exact command to reproduce**: python <tflow_parent>/tensorflow/tensorflow/example/tutorials/mnist/mnist/mnist_softmax_xla.py\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1157069/tf_env.txt)\r\n\r\n### Context\r\nMy work involves securing Tensorflow's computations. The approach I've planned to take is to add XLA ops during the conversion of TF ops to XLA ops,  which are lowered into functions calls (in LLVM-IR) that send data to and receive data from a secure environment.\r\n\r\nI'm currently verifying if both training and inference TF Ops are lowered to XLA ops.\r\n\r\n### The Problem\r\nI've made some [changes to the code](https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops) to highlight when some OpKernels and XlaOpKernels are being invoked / lowered. I've used the mnist_softmax_xla.py example to observe tensorflow's behaviour.\r\n\r\nI found that \"ApplyGradientDescentGPU\" was being called 2000 times (2 layer network and 1000 iterations, clearly not JIT) instead of ResourceApplyGradientDescent, yet other Ops including MatMul and subtraction seem to have been optimized by XLA.\r\n\r\nWhy is gradient descent not optimized by XLA ?\r\n\r\n### Source code / logs\r\nThe output text lost its structure when I pasted it. So, here are the screenshots.\r\n![github 1](https://user-images.githubusercontent.com/6310523/28340192-80abeec2-6c2c-11e7-8fb3-d3ec6ae1676e.PNG)\r\n![github 2](https://user-images.githubusercontent.com/6310523/28340198-8593c450-6c2c-11e7-9c62-dd06e93e2b5a.PNG)\r\n![github 3](https://user-images.githubusercontent.com/6310523/28340204-8909b69e-6c2c-11e7-9e16-afe089ccde8e.PNG)\r\n![github 4](https://user-images.githubusercontent.com/6310523/28340210-8c3c4a02-6c2c-11e7-93b1-a416035b3ed8.PNG) Note that the first 2 kernels have been called a 100 times each.\r\n\r\n### Related\r\n Also, it would be helpful if someone could answer [Tensow - XLA | Passing tensors to external functions at runtime](https://stackoverflow.com/questions/45146444/tensorflow-xla-how-are-tf-ops-lowered-to-xla-for-training) , as it is **the** important assumption of my approach that Tensorflow can allow such XLA ops. I appreciate any information on its feasibility as I could start on a different approach as soon as possible.\r\n\r\nThanks,\r\nSanjay"}