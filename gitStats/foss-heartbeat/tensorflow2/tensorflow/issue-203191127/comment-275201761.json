{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275201761", "html_url": "https://github.com/tensorflow/tensorflow/issues/7067#issuecomment-275201761", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7067", "id": 275201761, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTIwMTc2MQ==", "user": {"login": "seragENTp", "id": 13450600, "node_id": "MDQ6VXNlcjEzNDUwNjAw", "avatar_url": "https://avatars1.githubusercontent.com/u/13450600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seragENTp", "html_url": "https://github.com/seragENTp", "followers_url": "https://api.github.com/users/seragENTp/followers", "following_url": "https://api.github.com/users/seragENTp/following{/other_user}", "gists_url": "https://api.github.com/users/seragENTp/gists{/gist_id}", "starred_url": "https://api.github.com/users/seragENTp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seragENTp/subscriptions", "organizations_url": "https://api.github.com/users/seragENTp/orgs", "repos_url": "https://api.github.com/users/seragENTp/repos", "events_url": "https://api.github.com/users/seragENTp/events{/privacy}", "received_events_url": "https://api.github.com/users/seragENTp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-25T19:06:23Z", "updated_at": "2017-01-25T19:10:02Z", "author_association": "NONE", "body_html": "<p>import tensorflow as tf<br>\nfrom spatial_transformer import transformer<br>\nfrom tensorflow.python.ops import rnn,rnn_cell<br>\nimport numpy as np<br>\nfrom tf_utils import weight_variable, bias_variable,  dense_to_one_hot</p>\n<h1>%% load data</h1>\n<p>mnist_cluttered = np.load('data/mnist_sequence3_sample_8distortions_9x9.npz')</p>\n<p>X_train = mnist_cluttered['X_train']<br>\ny_train = mnist_cluttered['y_train']<br>\nX_valid = mnist_cluttered['X_valid']<br>\ny_valid = mnist_cluttered['y_valid']<br>\nX_test = mnist_cluttered['X_test']<br>\ny_test = mnist_cluttered['y_test']</p>\n<p>y_train = np.reshape(y_train,[y_train.size,1])<br>\ny_valid = np.reshape(y_valid,[y_valid.size,1])<br>\ny_test = np.reshape(y_test,[y_test.size,1])</p>\n<h1>% turn from dense to one hot representation</h1>\n<p>Y_train = dense_to_one_hot(y_train, n_classes=10)<br>\nY_valid = dense_to_one_hot(y_valid, n_classes=10)<br>\nY_test = dense_to_one_hot(y_test, n_classes=10)</p>\n<p>Y_train = np.reshape(Y_train,[y_train.size/3,3,10])<br>\nY_valid = np.reshape(Y_valid,[y_valid.size/3,3,10])<br>\nY_test = np.reshape(Y_test,[y_test.size/3,3,10])</p>\n<h1>%% Placeholders for 100x100 resolution</h1>\n<p>x = tf.placeholder(tf.float32, [None, 10000])<br>\ny = tf.placeholder(tf.float32, [None,3, 10])</p>\n<p>x_tensor = tf.reshape(x, [-1, 100, 100, 1])</p>\n<p>y_tensor = tf.reshape(y,[-1 ,10])</p>\n<p>#%% localizaton network</p>\n<p>keep_prob = tf.placeholder(tf.float32)</p>\n<p>l_pool0_loc = tf.nn.max_pool(x_tensor,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')</p>\n<p>W_conv0_loc = weight_variable([3,3,1,20],'W_conv0_loc')</p>\n<p>b_conv0_loc = bias_variable([20],'b_conv0_loc')</p>\n<p>l_conv0_loc = tf.nn.relu(tf.nn.conv2d(l_pool0_loc,W_conv0_loc,strides=[1,1,1,1],padding='VALID')+b_conv0_loc)</p>\n<p>l_pool1_loc = tf.nn.max_pool(l_conv0_loc,ksize=[1,2,2,1],strides =[1,2,2,1],padding='VALID')</p>\n<p>W_conv1_loc = weight_variable([3,3,20,20],'W_conv1_loc')</p>\n<p>b_conv1_loc = bias_variable([20],'b_conv1_loc')</p>\n<p>l_conv1_loc =  tf.nn.relu(tf.nn.conv2d(l_pool1_loc,W_conv1_loc,strides=[1,1,1,1],padding='VALID')+b_conv1_loc)</p>\n<p>l_pool2_loc = tf.nn.max_pool(l_conv1_loc,ksize=[1,2,2,1],strides =[1,2,2,1],padding='VALID')</p>\n<p>W_conv2_loc = weight_variable([3,3,20,20],'W_conv2_loc')</p>\n<p>b_conv2_loc = bias_variable([20],'b_conv2_loc')</p>\n<p>l_conv2_loc = tf.nn.relu(tf.nn.conv2d(l_pool2_loc,W_conv2_loc,strides=[1,1,1,1],padding='VALID')+b_conv2_loc )</p>\n<p>l_conv2_loc = tf.reshape(l_conv2_loc,[-1 ,9<em>9</em>20 ])</p>\n<h1>Replicate input for Gated Recurrent Unit</h1>\n<p>l_conv2_loc = tf.tile(l_conv2_loc,[1,3])</p>\n<p>l_conv2_loc = tf.split(1,3,l_conv2_loc)</p>\n<h1>Gated Recurrent Unit</h1>\n<p>gru_cell = rnn_cell.GRUCell(num_units=256)</p>\n<p>output, state = rnn.rnn(gru_cell,inputs=l_conv2_loc,dtype=tf.float32)</p>\n<p>output = tf.reshape(output,[-1,256])</p>\n<p>initial = tf.zeros([256,6])</p>\n<p>W_fc1_loc = tf.Variable(initial_value=initial,name='W_fc1_loc')</p>\n<h1>Use identity transformation as starting point</h1>\n<p>initial = np.array([[1., 0, 0], [0, 1., 0]])<br>\ninitial = initial.astype('float32')<br>\ninitial = initial.flatten()<br>\nb_fc1_loc = tf.Variable(initial_value=initial,name='b_fc1_loc')</p>\n<p>l_fc1_loc = tf.add(tf.matmul(output,W_fc1_loc), b_fc1_loc)</p>\n<h1>%% We'll create a spatial transformer module to identify discriminative patches</h1>\n<p>downsample = 3</p>\n<p>out_size = (100/downsample, 100/downsample)</p>\n<p>l_transform = transformer(tf.tile(x_tensor,[3,1,1,1]), l_fc1_loc, out_size)</p>\n<h1>%% Classification Network</h1>\n<p>W_conv0_out = weight_variable([3,3,1,32],'W_conv0_out')</p>\n<p>b_conv0_out = bias_variable([32],'b_conv0_out')</p>\n<p>l_conv0_out = tf.nn.relu(tf.nn.conv2d(l_transform,W_conv0_out,strides=[1,1,1,1],padding='VALID')+b_conv0_out)</p>\n<p>l_pool1_out = tf.nn.max_pool(l_conv0_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')</p>\n<p>#l_drp1_out = tf.nn.dropout(l_pool1_out,keep_prob)</p>\n<p>W_conv1_out = weight_variable([3,3,32,32],'W_conv1_out')</p>\n<p>b_conv1_out = bias_variable([32],'b_conv1_out')</p>\n<p>l_conv1_out = tf.nn.relu(tf.nn.conv2d(l_pool1_out,W_conv1_out,strides=[1,1,1,1],padding='VALID')+b_conv1_out)</p>\n<p>l_pool2_out = tf.nn.max_pool(l_conv1_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')</p>\n<p>#l_drp2_out = tf.nn.dropout(l_pool2_out,keep_prob)</p>\n<p>W_conv2_out = weight_variable([3,3,32,32],'W_conv2_out')</p>\n<p>b_conv2_out = bias_variable([32],'b_conv2_out')</p>\n<p>l_conv2_out = tf.nn.relu(tf.nn.conv2d(l_pool2_out,W_conv2_out,strides=[1,1,1,1],padding='VALID')+b_conv2_out)</p>\n<h1>%% We'll now reshape so we can connect to a fully-connected layer:</h1>\n<p>l_conv2_out_flat = tf.reshape(l_conv2_out, [-1, 4<em>4</em>32])</p>\n<h1>%% Create a fully-connected layer:</h1>\n<p>n_fc = 400</p>\n<p>W_fc1 = tf.get_variable('W_fc1',shape=[4<em>4</em>32,n_fc],initializer=tf.contrib.layers.xavier_initializer())</p>\n<p>#W_fc1 = weight_variable([4<em>4</em>32,n_fc],'W_fc1')</p>\n<p>b_fc1=bias_variable([n_fc],'b_fc1')</p>\n<p>h_fc1 = tf.nn.relu(tf.add(tf.matmul(l_conv2_out_flat, W_fc1) , b_fc1))</p>\n<h1>%% And finally our softmax layer:</h1>\n<p>W_fc2 = tf.get_variable('W_fc2',shape=[n_fc, 10],initializer=tf.contrib.layers.xavier_initializer())</p>\n<p>#W_fc2 = weight_variable([n_fc,10],'W_fc2')</p>\n<p>b_fc2=bias_variable([10],'b_fc2')</p>\n<p>y_logits = tf.add(tf.matmul(h_fc1, W_fc2) , b_fc2)</p>\n<h1>%% Monitor accuracy</h1>\n<p>correct_prediction = tf.equal(tf.argmax(y_logits, 1), tf.argmax(y_tensor, 1))<br>\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))</p>\n<h1>%% Define loss/eval/training functions</h1>\n<p>cross_entropy = tf.reduce_mean(<br>\ntf.nn.softmax_cross_entropy_with_logits(y_logits,y_tensor))</p>\n<p>opt = tf.train.RMSPropOptimizer(0.0005,epsilon=1e-6)</p>\n<p>#opt = tf.train.AdagradOptimizer(0.01)<br>\n#optimizer = opt.minimize(cross_entropy)</p>\n<p>gvs = opt.compute_gradients(cross_entropy)</p>\n<p>capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]</p>\n<p>optimizer = opt.apply_gradients(capped_gvs )</p>\n<h1>%% We'll now train in minibatches and report accuracy, loss:</h1>\n<p>num_batches = 600<br>\nn_epochs = 300<br>\nbatch_size = 100</p>\n<p>with tf.Session( ) as sess:</p>\n<pre><code> sess.run(tf.initialize_all_variables())\n\n for epoch_i in range(n_epochs):\n\n#print ('epoch: ' + str(epoch_i))\n     shuffle = np.random.permutation(X_train.shape[0])\n     avg_cost = 0.\n     for iter_i in range(num_batches - 1):\n         idx = shuffle[iter_i*batch_size:(iter_i+1)*batch_size]\n         batch_xs = X_train[idx]\n         batch_ys = Y_train[idx]\n   \n\n         _,c=sess.run([optimizer,cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\n\n         avg_cost += c / num_batches\n         print('iter: ' + str(iter_i) +' &gt;&gt; ' +' MiniBatch Cost: ' +str(c)) \n   \n #   gr_print= sess.run([grads for grads,_  in gvs], feed_dict={x : batch_xs, y : batch_ys}) \n #   print ('iter: '+str(iter_i))\n #   for t in gr_print:\n  #      print np.linalg.norm(t)\n</code></pre>\n<p>saver = tf.train.Saver()</p>\n<p>saver.save(sess,\"save/my-model\")</p>\n<p>`</p>", "body_text": "import tensorflow as tf\nfrom spatial_transformer import transformer\nfrom tensorflow.python.ops import rnn,rnn_cell\nimport numpy as np\nfrom tf_utils import weight_variable, bias_variable,  dense_to_one_hot\n%% load data\nmnist_cluttered = np.load('data/mnist_sequence3_sample_8distortions_9x9.npz')\nX_train = mnist_cluttered['X_train']\ny_train = mnist_cluttered['y_train']\nX_valid = mnist_cluttered['X_valid']\ny_valid = mnist_cluttered['y_valid']\nX_test = mnist_cluttered['X_test']\ny_test = mnist_cluttered['y_test']\ny_train = np.reshape(y_train,[y_train.size,1])\ny_valid = np.reshape(y_valid,[y_valid.size,1])\ny_test = np.reshape(y_test,[y_test.size,1])\n% turn from dense to one hot representation\nY_train = dense_to_one_hot(y_train, n_classes=10)\nY_valid = dense_to_one_hot(y_valid, n_classes=10)\nY_test = dense_to_one_hot(y_test, n_classes=10)\nY_train = np.reshape(Y_train,[y_train.size/3,3,10])\nY_valid = np.reshape(Y_valid,[y_valid.size/3,3,10])\nY_test = np.reshape(Y_test,[y_test.size/3,3,10])\n%% Placeholders for 100x100 resolution\nx = tf.placeholder(tf.float32, [None, 10000])\ny = tf.placeholder(tf.float32, [None,3, 10])\nx_tensor = tf.reshape(x, [-1, 100, 100, 1])\ny_tensor = tf.reshape(y,[-1 ,10])\n#%% localizaton network\nkeep_prob = tf.placeholder(tf.float32)\nl_pool0_loc = tf.nn.max_pool(x_tensor,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\nW_conv0_loc = weight_variable([3,3,1,20],'W_conv0_loc')\nb_conv0_loc = bias_variable([20],'b_conv0_loc')\nl_conv0_loc = tf.nn.relu(tf.nn.conv2d(l_pool0_loc,W_conv0_loc,strides=[1,1,1,1],padding='VALID')+b_conv0_loc)\nl_pool1_loc = tf.nn.max_pool(l_conv0_loc,ksize=[1,2,2,1],strides =[1,2,2,1],padding='VALID')\nW_conv1_loc = weight_variable([3,3,20,20],'W_conv1_loc')\nb_conv1_loc = bias_variable([20],'b_conv1_loc')\nl_conv1_loc =  tf.nn.relu(tf.nn.conv2d(l_pool1_loc,W_conv1_loc,strides=[1,1,1,1],padding='VALID')+b_conv1_loc)\nl_pool2_loc = tf.nn.max_pool(l_conv1_loc,ksize=[1,2,2,1],strides =[1,2,2,1],padding='VALID')\nW_conv2_loc = weight_variable([3,3,20,20],'W_conv2_loc')\nb_conv2_loc = bias_variable([20],'b_conv2_loc')\nl_conv2_loc = tf.nn.relu(tf.nn.conv2d(l_pool2_loc,W_conv2_loc,strides=[1,1,1,1],padding='VALID')+b_conv2_loc )\nl_conv2_loc = tf.reshape(l_conv2_loc,[-1 ,9920 ])\nReplicate input for Gated Recurrent Unit\nl_conv2_loc = tf.tile(l_conv2_loc,[1,3])\nl_conv2_loc = tf.split(1,3,l_conv2_loc)\nGated Recurrent Unit\ngru_cell = rnn_cell.GRUCell(num_units=256)\noutput, state = rnn.rnn(gru_cell,inputs=l_conv2_loc,dtype=tf.float32)\noutput = tf.reshape(output,[-1,256])\ninitial = tf.zeros([256,6])\nW_fc1_loc = tf.Variable(initial_value=initial,name='W_fc1_loc')\nUse identity transformation as starting point\ninitial = np.array([[1., 0, 0], [0, 1., 0]])\ninitial = initial.astype('float32')\ninitial = initial.flatten()\nb_fc1_loc = tf.Variable(initial_value=initial,name='b_fc1_loc')\nl_fc1_loc = tf.add(tf.matmul(output,W_fc1_loc), b_fc1_loc)\n%% We'll create a spatial transformer module to identify discriminative patches\ndownsample = 3\nout_size = (100/downsample, 100/downsample)\nl_transform = transformer(tf.tile(x_tensor,[3,1,1,1]), l_fc1_loc, out_size)\n%% Classification Network\nW_conv0_out = weight_variable([3,3,1,32],'W_conv0_out')\nb_conv0_out = bias_variable([32],'b_conv0_out')\nl_conv0_out = tf.nn.relu(tf.nn.conv2d(l_transform,W_conv0_out,strides=[1,1,1,1],padding='VALID')+b_conv0_out)\nl_pool1_out = tf.nn.max_pool(l_conv0_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\n#l_drp1_out = tf.nn.dropout(l_pool1_out,keep_prob)\nW_conv1_out = weight_variable([3,3,32,32],'W_conv1_out')\nb_conv1_out = bias_variable([32],'b_conv1_out')\nl_conv1_out = tf.nn.relu(tf.nn.conv2d(l_pool1_out,W_conv1_out,strides=[1,1,1,1],padding='VALID')+b_conv1_out)\nl_pool2_out = tf.nn.max_pool(l_conv1_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\n#l_drp2_out = tf.nn.dropout(l_pool2_out,keep_prob)\nW_conv2_out = weight_variable([3,3,32,32],'W_conv2_out')\nb_conv2_out = bias_variable([32],'b_conv2_out')\nl_conv2_out = tf.nn.relu(tf.nn.conv2d(l_pool2_out,W_conv2_out,strides=[1,1,1,1],padding='VALID')+b_conv2_out)\n%% We'll now reshape so we can connect to a fully-connected layer:\nl_conv2_out_flat = tf.reshape(l_conv2_out, [-1, 4432])\n%% Create a fully-connected layer:\nn_fc = 400\nW_fc1 = tf.get_variable('W_fc1',shape=[4432,n_fc],initializer=tf.contrib.layers.xavier_initializer())\n#W_fc1 = weight_variable([4432,n_fc],'W_fc1')\nb_fc1=bias_variable([n_fc],'b_fc1')\nh_fc1 = tf.nn.relu(tf.add(tf.matmul(l_conv2_out_flat, W_fc1) , b_fc1))\n%% And finally our softmax layer:\nW_fc2 = tf.get_variable('W_fc2',shape=[n_fc, 10],initializer=tf.contrib.layers.xavier_initializer())\n#W_fc2 = weight_variable([n_fc,10],'W_fc2')\nb_fc2=bias_variable([10],'b_fc2')\ny_logits = tf.add(tf.matmul(h_fc1, W_fc2) , b_fc2)\n%% Monitor accuracy\ncorrect_prediction = tf.equal(tf.argmax(y_logits, 1), tf.argmax(y_tensor, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n%% Define loss/eval/training functions\ncross_entropy = tf.reduce_mean(\ntf.nn.softmax_cross_entropy_with_logits(y_logits,y_tensor))\nopt = tf.train.RMSPropOptimizer(0.0005,epsilon=1e-6)\n#opt = tf.train.AdagradOptimizer(0.01)\n#optimizer = opt.minimize(cross_entropy)\ngvs = opt.compute_gradients(cross_entropy)\ncapped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\noptimizer = opt.apply_gradients(capped_gvs )\n%% We'll now train in minibatches and report accuracy, loss:\nnum_batches = 600\nn_epochs = 300\nbatch_size = 100\nwith tf.Session( ) as sess:\n sess.run(tf.initialize_all_variables())\n\n for epoch_i in range(n_epochs):\n\n#print ('epoch: ' + str(epoch_i))\n     shuffle = np.random.permutation(X_train.shape[0])\n     avg_cost = 0.\n     for iter_i in range(num_batches - 1):\n         idx = shuffle[iter_i*batch_size:(iter_i+1)*batch_size]\n         batch_xs = X_train[idx]\n         batch_ys = Y_train[idx]\n   \n\n         _,c=sess.run([optimizer,cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\n\n         avg_cost += c / num_batches\n         print('iter: ' + str(iter_i) +' >> ' +' MiniBatch Cost: ' +str(c)) \n   \n #   gr_print= sess.run([grads for grads,_  in gvs], feed_dict={x : batch_xs, y : batch_ys}) \n #   print ('iter: '+str(iter_i))\n #   for t in gr_print:\n  #      print np.linalg.norm(t)\n\nsaver = tf.train.Saver()\nsaver.save(sess,\"save/my-model\")\n`", "body": "import tensorflow as tf\r\nfrom spatial_transformer import transformer\r\nfrom tensorflow.python.ops import rnn,rnn_cell\r\nimport numpy as np\r\nfrom tf_utils import weight_variable, bias_variable,  dense_to_one_hot\r\n\r\n# %% load data\r\nmnist_cluttered = np.load('data/mnist_sequence3_sample_8distortions_9x9.npz')\r\n\r\nX_train = mnist_cluttered['X_train']\r\ny_train = mnist_cluttered['y_train']\r\nX_valid = mnist_cluttered['X_valid']\r\ny_valid = mnist_cluttered['y_valid']\r\nX_test = mnist_cluttered['X_test']\r\ny_test = mnist_cluttered['y_test']\r\n\r\ny_train = np.reshape(y_train,[y_train.size,1])\r\ny_valid = np.reshape(y_valid,[y_valid.size,1])\r\ny_test = np.reshape(y_test,[y_test.size,1])\r\n\r\n# % turn from dense to one hot representation\r\nY_train = dense_to_one_hot(y_train, n_classes=10)\r\nY_valid = dense_to_one_hot(y_valid, n_classes=10)\r\nY_test = dense_to_one_hot(y_test, n_classes=10)\r\n\r\n\r\nY_train = np.reshape(Y_train,[y_train.size/3,3,10])\r\nY_valid = np.reshape(Y_valid,[y_valid.size/3,3,10])\r\nY_test = np.reshape(Y_test,[y_test.size/3,3,10])\r\n\r\n# %% Placeholders for 100x100 resolution\r\nx = tf.placeholder(tf.float32, [None, 10000])\r\ny = tf.placeholder(tf.float32, [None,3, 10])\r\n\r\n\r\nx_tensor = tf.reshape(x, [-1, 100, 100, 1])\r\n\r\ny_tensor = tf.reshape(y,[-1 ,10])\r\n\r\n#%% localizaton network\r\n\r\nkeep_prob = tf.placeholder(tf.float32)\r\n\r\nl_pool0_loc = tf.nn.max_pool(x_tensor,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\r\n\r\nW_conv0_loc = weight_variable([3,3,1,20],'W_conv0_loc')\r\n\r\nb_conv0_loc = bias_variable([20],'b_conv0_loc')\r\n\r\nl_conv0_loc = tf.nn.relu(tf.nn.conv2d(l_pool0_loc,W_conv0_loc,strides=[1,1,1,1],padding='VALID')+b_conv0_loc)\r\n\r\nl_pool1_loc = tf.nn.max_pool(l_conv0_loc,ksize=[1,2,2,1],strides =[1,2,2,1],padding='VALID')\r\n\r\nW_conv1_loc = weight_variable([3,3,20,20],'W_conv1_loc')   \r\n\r\nb_conv1_loc = bias_variable([20],'b_conv1_loc')\r\n       \r\nl_conv1_loc =  tf.nn.relu(tf.nn.conv2d(l_pool1_loc,W_conv1_loc,strides=[1,1,1,1],padding='VALID')+b_conv1_loc)\r\n\r\nl_pool2_loc = tf.nn.max_pool(l_conv1_loc,ksize=[1,2,2,1],strides =[1,2,2,1],padding='VALID')\r\n\r\nW_conv2_loc = weight_variable([3,3,20,20],'W_conv2_loc')\r\n\r\nb_conv2_loc = bias_variable([20],'b_conv2_loc')\r\n\r\nl_conv2_loc = tf.nn.relu(tf.nn.conv2d(l_pool2_loc,W_conv2_loc,strides=[1,1,1,1],padding='VALID')+b_conv2_loc )\r\n\r\nl_conv2_loc = tf.reshape(l_conv2_loc,[-1 ,9*9*20 ])\r\n\r\n# Replicate input for Gated Recurrent Unit\r\nl_conv2_loc = tf.tile(l_conv2_loc,[1,3])\r\n\r\nl_conv2_loc = tf.split(1,3,l_conv2_loc)\r\n\r\n# Gated Recurrent Unit\r\n\r\ngru_cell = rnn_cell.GRUCell(num_units=256)\r\n\r\noutput, state = rnn.rnn(gru_cell,inputs=l_conv2_loc,dtype=tf.float32)\r\n\r\noutput = tf.reshape(output,[-1,256])\r\n\r\ninitial = tf.zeros([256,6]) \r\n\r\n\r\nW_fc1_loc = tf.Variable(initial_value=initial,name='W_fc1_loc')\r\n\r\n# Use identity transformation as starting point\r\ninitial = np.array([[1., 0, 0], [0, 1., 0]])\r\ninitial = initial.astype('float32')\r\ninitial = initial.flatten()\r\nb_fc1_loc = tf.Variable(initial_value=initial,name='b_fc1_loc')\r\n\r\n\r\nl_fc1_loc = tf.add(tf.matmul(output,W_fc1_loc), b_fc1_loc)\r\n\r\n\r\n# %% We'll create a spatial transformer module to identify discriminative patches\r\n\r\ndownsample = 3\r\n\r\nout_size = (100/downsample, 100/downsample)\r\n\r\n\r\nl_transform = transformer(tf.tile(x_tensor,[3,1,1,1]), l_fc1_loc, out_size)\r\n\r\n# %% Classification Network\r\n\r\n\r\nW_conv0_out = weight_variable([3,3,1,32],'W_conv0_out')                   \r\n\r\nb_conv0_out = bias_variable([32],'b_conv0_out')\r\n\r\nl_conv0_out = tf.nn.relu(tf.nn.conv2d(l_transform,W_conv0_out,strides=[1,1,1,1],padding='VALID')+b_conv0_out)\r\n\r\nl_pool1_out = tf.nn.max_pool(l_conv0_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\r\n                   \r\n#l_drp1_out = tf.nn.dropout(l_pool1_out,keep_prob)\r\n\r\nW_conv1_out = weight_variable([3,3,32,32],'W_conv1_out')  \r\n\r\nb_conv1_out = bias_variable([32],'b_conv1_out')\r\n               \r\nl_conv1_out = tf.nn.relu(tf.nn.conv2d(l_pool1_out,W_conv1_out,strides=[1,1,1,1],padding='VALID')+b_conv1_out)\r\n\r\nl_pool2_out = tf.nn.max_pool(l_conv1_out,ksize=[1,2,2,1], strides=[1,2,2,1],padding='VALID')\r\n\r\n#l_drp2_out = tf.nn.dropout(l_pool2_out,keep_prob)\r\n\r\nW_conv2_out = weight_variable([3,3,32,32],'W_conv2_out')     \r\n\r\nb_conv2_out = bias_variable([32],'b_conv2_out')\r\n            \r\nl_conv2_out = tf.nn.relu(tf.nn.conv2d(l_pool2_out,W_conv2_out,strides=[1,1,1,1],padding='VALID')+b_conv2_out)\r\n\r\n\r\n\r\n# %% We'll now reshape so we can connect to a fully-connected layer:\r\nl_conv2_out_flat = tf.reshape(l_conv2_out, [-1, 4*4*32])\r\n\r\n# %% Create a fully-connected layer:\r\nn_fc = 400\r\n\r\nW_fc1 = tf.get_variable('W_fc1',shape=[4*4*32,n_fc],initializer=tf.contrib.layers.xavier_initializer())\r\n\r\n#W_fc1 = weight_variable([4*4*32,n_fc],'W_fc1')\r\n\r\nb_fc1=bias_variable([n_fc],'b_fc1')\r\n\r\n\r\nh_fc1 = tf.nn.relu(tf.add(tf.matmul(l_conv2_out_flat, W_fc1) , b_fc1))\r\n\r\n# %% And finally our softmax layer:\r\n\r\nW_fc2 = tf.get_variable('W_fc2',shape=[n_fc, 10],initializer=tf.contrib.layers.xavier_initializer())\r\n\r\n#W_fc2 = weight_variable([n_fc,10],'W_fc2')\r\n\r\nb_fc2=bias_variable([10],'b_fc2')\r\n\r\ny_logits = tf.add(tf.matmul(h_fc1, W_fc2) , b_fc2)\r\n\r\n\r\n\r\n# %% Monitor accuracy\r\n\r\n\r\n\r\ncorrect_prediction = tf.equal(tf.argmax(y_logits, 1), tf.argmax(y_tensor, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\r\n\r\n\r\n# %% Define loss/eval/training functions\r\ncross_entropy = tf.reduce_mean(\r\n    tf.nn.softmax_cross_entropy_with_logits(y_logits,y_tensor))\r\n\r\nopt = tf.train.RMSPropOptimizer(0.0005,epsilon=1e-6)\r\n\r\n#opt = tf.train.AdagradOptimizer(0.01)\r\n#optimizer = opt.minimize(cross_entropy)\r\n\r\n\r\n\r\n\r\ngvs = opt.compute_gradients(cross_entropy)\r\n\r\ncapped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\r\n\r\noptimizer = opt.apply_gradients(capped_gvs )\r\n\r\n\r\n\r\n\r\n# %% We'll now train in minibatches and report accuracy, loss:\r\n\r\nnum_batches = 600\r\nn_epochs = 300\r\nbatch_size = 100\r\n\r\n\r\nwith tf.Session( ) as sess:\r\n\r\n     sess.run(tf.initialize_all_variables())\r\n    \r\n     for epoch_i in range(n_epochs):\r\n    \r\n    #print ('epoch: ' + str(epoch_i))\r\n         shuffle = np.random.permutation(X_train.shape[0])\r\n         avg_cost = 0.\r\n         for iter_i in range(num_batches - 1):\r\n             idx = shuffle[iter_i*batch_size:(iter_i+1)*batch_size]\r\n             batch_xs = X_train[idx]\r\n             batch_ys = Y_train[idx]\r\n       \r\n    \r\n             _,c=sess.run([optimizer,cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\r\n    \r\n             avg_cost += c / num_batches\r\n             print('iter: ' + str(iter_i) +' >> ' +' MiniBatch Cost: ' +str(c)) \r\n       \r\n     #   gr_print= sess.run([grads for grads,_  in gvs], feed_dict={x : batch_xs, y : batch_ys}) \r\n     #   print ('iter: '+str(iter_i))\r\n     #   for t in gr_print:\r\n      #      print np.linalg.norm(t)\r\n    \r\n       \t \r\n\r\nsaver = tf.train.Saver()\r\n\r\nsaver.save(sess,\"save/my-model\")\r\n\r\n`"}