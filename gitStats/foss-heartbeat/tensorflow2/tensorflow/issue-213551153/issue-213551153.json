{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8309", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8309/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8309/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8309/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8309", "id": 213551153, "node_id": "MDU6SXNzdWUyMTM1NTExNTM=", "number": 8309, "title": "Kernel Died Issue", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-11T18:59:35Z", "updated_at": "2017-04-05T10:20:24Z", "closed_at": "2017-04-05T10:20:24Z", "author_association": "NONE", "body_html": "<p>Can anybody know why kernel is always died? When I excute this cell, a part of Lenet CNN.</p>\n<ol>\n<li>Code :</li>\n</ol>\n<pre><code>with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    num_examples = len(X_train)\n    \n    print(\"Training...\")\n    print()\n    for i in range(EPOCHS):\n        X_train, y_train = shuffle(X_train, y_train)\n        for offset in range(0, num_examples, BATCH_SIZE):\n            end = offset + BATCH_SIZE\n            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n            \n        validation_accuracy = evaluate(X_validation, y_validation)\n        print(\"EPOCH {} ...\".format(i+1))\n        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n        print()\n    try:\n        saver\n    except NameError:\n        saver=tf.train.Saver()       \n    saver.save(sess, './lenet')\n    print(\"Model saved\")\n</code></pre>\n<ol start=\"2\">\n<li>Error Message :</li>\n</ol>\n<pre><code>E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:532] Check failed: stream-&gt;parent()-&gt;GetConvolveAlgorithms(&amp;algorithms)\n</code></pre>\n<p>Then, the kernel is died....it is crazy. Please let me know the solution</p>\n<ol start=\"3\">\n<li>Environment<br>\nWindow 10 (No Ubantu)<br>\nAnaconda w/Jupyter Notebook<br>\nTensorflow w/GPU (Cuda 8.0, cuDNN 5.1)</li>\n</ol>\n<p>Thank you!!</p>", "body_text": "Can anybody know why kernel is always died? When I excute this cell, a part of Lenet CNN.\n\nCode :\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    num_examples = len(X_train)\n    \n    print(\"Training...\")\n    print()\n    for i in range(EPOCHS):\n        X_train, y_train = shuffle(X_train, y_train)\n        for offset in range(0, num_examples, BATCH_SIZE):\n            end = offset + BATCH_SIZE\n            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n            \n        validation_accuracy = evaluate(X_validation, y_validation)\n        print(\"EPOCH {} ...\".format(i+1))\n        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n        print()\n    try:\n        saver\n    except NameError:\n        saver=tf.train.Saver()       \n    saver.save(sess, './lenet')\n    print(\"Model saved\")\n\n\nError Message :\n\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\n\nThen, the kernel is died....it is crazy. Please let me know the solution\n\nEnvironment\nWindow 10 (No Ubantu)\nAnaconda w/Jupyter Notebook\nTensorflow w/GPU (Cuda 8.0, cuDNN 5.1)\n\nThank you!!", "body": "Can anybody know why kernel is always died? When I excute this cell, a part of Lenet CNN.\r\n\r\n1. Code : \r\n```\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    num_examples = len(X_train)\r\n    \r\n    print(\"Training...\")\r\n    print()\r\n    for i in range(EPOCHS):\r\n        X_train, y_train = shuffle(X_train, y_train)\r\n        for offset in range(0, num_examples, BATCH_SIZE):\r\n            end = offset + BATCH_SIZE\r\n            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\r\n            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\r\n            \r\n        validation_accuracy = evaluate(X_validation, y_validation)\r\n        print(\"EPOCH {} ...\".format(i+1))\r\n        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\r\n        print()\r\n    try:\r\n        saver\r\n    except NameError:\r\n        saver=tf.train.Saver()       \r\n    saver.save(sess, './lenet')\r\n    print(\"Model saved\")\r\n```\r\n2. Error Message :\r\n``` \r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\nThen, the kernel is died....it is crazy. Please let me know the solution \r\n\r\n3. Environment \r\nWindow 10 (No Ubantu)\r\nAnaconda w/Jupyter Notebook \r\nTensorflow w/GPU (Cuda 8.0, cuDNN 5.1)\r\n\r\nThank you!!\r\n"}