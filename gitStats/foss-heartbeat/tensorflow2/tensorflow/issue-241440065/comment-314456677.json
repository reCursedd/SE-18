{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/314456677", "html_url": "https://github.com/tensorflow/tensorflow/issues/11371#issuecomment-314456677", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11371", "id": 314456677, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNDQ1NjY3Nw==", "user": {"login": "oborchers", "id": 26734737, "node_id": "MDQ6VXNlcjI2NzM0NzM3", "avatar_url": "https://avatars2.githubusercontent.com/u/26734737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oborchers", "html_url": "https://github.com/oborchers", "followers_url": "https://api.github.com/users/oborchers/followers", "following_url": "https://api.github.com/users/oborchers/following{/other_user}", "gists_url": "https://api.github.com/users/oborchers/gists{/gist_id}", "starred_url": "https://api.github.com/users/oborchers/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oborchers/subscriptions", "organizations_url": "https://api.github.com/users/oborchers/orgs", "repos_url": "https://api.github.com/users/oborchers/repos", "events_url": "https://api.github.com/users/oborchers/events{/privacy}", "received_events_url": "https://api.github.com/users/oborchers/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-11T14:12:03Z", "updated_at": "2017-07-11T14:14:57Z", "author_association": "NONE", "body_html": "<p>I am having a very similar problem to this one. Tf cond does not seem to affect my input placeholder to which I want to ad some salt and pepper noise. Consider the following code:</p>\n<pre><code>def noise_mask(input, keep_prob):    \n    return tf.zeros_like(input)\ndef binary_noise(input, keep_prob=0.05, is_training=True):\n    is_training = tf.Variable(is_training)\n    return tf.cond(is_training, lambda: noise_mask(input, keep_prob), lambda: tf.identity(input))\n</code></pre>\n<p>This simple version sets all tensor values to zero if desired, i.e. if is training == true.<br>\nThen I apply this to the output layer:</p>\n<p><code> self.out = utils.binary_noise(tf.add(tf.matmul(self.hidden, self.weights['w2']), self.weights['b2']), is_training=True)</code></p>\n<p>As expected, the loss stagnates as the output layer is not able to pass the gradient through.<br>\nEpoch:   1      Cost:0.693194389        Loss:0.692861199<br>\nEpoch:   2      Cost:0.693194389        Loss:0.692861199<br>\nEpoch:   3      Cost:0.693194389        Loss:0.692861199</p>\n<p>Now, I would like to apply this function to the input to add binary noise.<br>\nConsider:</p>\n<pre><code>self.x_input = tf.placeholder(tf.float64, [None, self.n_input])\nself.x = utils.binary_noise(self.x_input, keep_prob=0.05, is_training=False)`\n</code></pre>\n<p>However the loss changes as well, when is_training == True. I have no idea why this is the case.</p>", "body_text": "I am having a very similar problem to this one. Tf cond does not seem to affect my input placeholder to which I want to ad some salt and pepper noise. Consider the following code:\ndef noise_mask(input, keep_prob):    \n    return tf.zeros_like(input)\ndef binary_noise(input, keep_prob=0.05, is_training=True):\n    is_training = tf.Variable(is_training)\n    return tf.cond(is_training, lambda: noise_mask(input, keep_prob), lambda: tf.identity(input))\n\nThis simple version sets all tensor values to zero if desired, i.e. if is training == true.\nThen I apply this to the output layer:\n self.out = utils.binary_noise(tf.add(tf.matmul(self.hidden, self.weights['w2']), self.weights['b2']), is_training=True)\nAs expected, the loss stagnates as the output layer is not able to pass the gradient through.\nEpoch:   1      Cost:0.693194389        Loss:0.692861199\nEpoch:   2      Cost:0.693194389        Loss:0.692861199\nEpoch:   3      Cost:0.693194389        Loss:0.692861199\nNow, I would like to apply this function to the input to add binary noise.\nConsider:\nself.x_input = tf.placeholder(tf.float64, [None, self.n_input])\nself.x = utils.binary_noise(self.x_input, keep_prob=0.05, is_training=False)`\n\nHowever the loss changes as well, when is_training == True. I have no idea why this is the case.", "body": "I am having a very similar problem to this one. Tf cond does not seem to affect my input placeholder to which I want to ad some salt and pepper noise. Consider the following code:\r\n\r\n    def noise_mask(input, keep_prob):    \r\n        return tf.zeros_like(input)\r\n    def binary_noise(input, keep_prob=0.05, is_training=True):\r\n        is_training = tf.Variable(is_training)\r\n        return tf.cond(is_training, lambda: noise_mask(input, keep_prob), lambda: tf.identity(input))\r\n\r\nThis simple version sets all tensor values to zero if desired, i.e. if is training == true.\r\nThen I apply this to the output layer:\r\n\r\n`\r\nself.out = utils.binary_noise(tf.add(tf.matmul(self.hidden, self.weights['w2']), self.weights['b2']), is_training=True)`\r\n\r\nAs expected, the loss stagnates as the output layer is not able to pass the gradient through.\r\nEpoch:   1      Cost:0.693194389        Loss:0.692861199\r\nEpoch:   2      Cost:0.693194389        Loss:0.692861199\r\nEpoch:   3      Cost:0.693194389        Loss:0.692861199\r\n\r\nNow, I would like to apply this function to the input to add binary noise.\r\nConsider:\r\n\r\n    self.x_input = tf.placeholder(tf.float64, [None, self.n_input])\r\n    self.x = utils.binary_noise(self.x_input, keep_prob=0.05, is_training=False)`\r\n\r\nHowever the loss changes as well, when is_training == True. I have no idea why this is the case."}