{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/244119850", "html_url": "https://github.com/tensorflow/tensorflow/issues/4093#issuecomment-244119850", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4093", "id": 244119850, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NDExOTg1MA==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-01T15:39:41Z", "updated_at": "2016-09-01T15:39:41Z", "author_association": "MEMBER", "body_html": "<p>OK.  So it may be possible to reduce your max memory consumption a bit without down-scaling.  Unfortunately I don't know of any specific documentation on this.  We have some future improvements planned which will hopefully make it less necessary and/or easier to do.</p>\n<p>The fact that your program doesn't run out of memory in the first couple of steps, but runs for some thousands first, suggests that either the OOM condition is the result of an unlucky concurrent execution that exceeds memory limits, or you have variable size Op inputs somewhere, and eventually you hit a minibatch which raises the aggregate live tensor size above your limit.</p>\n<p>You'll need to figure out exactly what your computation graph looks like, i.e. what are the types and sizes of every Op input and output, and what edges connect them together.  There may be a *.pbtxt file generated as a side effect of execution which describes this graph.  Your python program will determine which subgraphs execute by session.run() statements that specify a set of input and output nodes that induce a subgraph.  Wherever two Ops in that subgraph don't have a serial dependency, i.e. one of them does not take as input an edge which is descendant from the other, they are potentially concurrent and may overlap in execution.  When that happens, all of their inputs and outputs are allocated simultaneously.  Knowing the types and sizes, you should be able to count up fairly precisely the memory requirement to execute each Op.  Keep in mind that Vars are always live, and if you're training, some values from the forward computation phase need to live until they're used (again) in the backprop phase.  This should all be explicit in the graph.</p>\n<p>If you have some potential parallelism in the graph that may be causing OOM with an unlucky execution overlap, you can eliminate it by introducing a control edge that prevents one of the potentially overlapping Ops from beginning execution until the other has finished.  See <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html#control_dependencies\" rel=\"nofollow\">control dependencies</a>.</p>\n<p>If your OOM problem is due to variable sized inputs, maybe you can change your input minibatch selection somehow to stay within your limits.</p>", "body_text": "OK.  So it may be possible to reduce your max memory consumption a bit without down-scaling.  Unfortunately I don't know of any specific documentation on this.  We have some future improvements planned which will hopefully make it less necessary and/or easier to do.\nThe fact that your program doesn't run out of memory in the first couple of steps, but runs for some thousands first, suggests that either the OOM condition is the result of an unlucky concurrent execution that exceeds memory limits, or you have variable size Op inputs somewhere, and eventually you hit a minibatch which raises the aggregate live tensor size above your limit.\nYou'll need to figure out exactly what your computation graph looks like, i.e. what are the types and sizes of every Op input and output, and what edges connect them together.  There may be a *.pbtxt file generated as a side effect of execution which describes this graph.  Your python program will determine which subgraphs execute by session.run() statements that specify a set of input and output nodes that induce a subgraph.  Wherever two Ops in that subgraph don't have a serial dependency, i.e. one of them does not take as input an edge which is descendant from the other, they are potentially concurrent and may overlap in execution.  When that happens, all of their inputs and outputs are allocated simultaneously.  Knowing the types and sizes, you should be able to count up fairly precisely the memory requirement to execute each Op.  Keep in mind that Vars are always live, and if you're training, some values from the forward computation phase need to live until they're used (again) in the backprop phase.  This should all be explicit in the graph.\nIf you have some potential parallelism in the graph that may be causing OOM with an unlucky execution overlap, you can eliminate it by introducing a control edge that prevents one of the potentially overlapping Ops from beginning execution until the other has finished.  See control dependencies.\nIf your OOM problem is due to variable sized inputs, maybe you can change your input minibatch selection somehow to stay within your limits.", "body": "OK.  So it may be possible to reduce your max memory consumption a bit without down-scaling.  Unfortunately I don't know of any specific documentation on this.  We have some future improvements planned which will hopefully make it less necessary and/or easier to do.   \n\nThe fact that your program doesn't run out of memory in the first couple of steps, but runs for some thousands first, suggests that either the OOM condition is the result of an unlucky concurrent execution that exceeds memory limits, or you have variable size Op inputs somewhere, and eventually you hit a minibatch which raises the aggregate live tensor size above your limit.\n\nYou'll need to figure out exactly what your computation graph looks like, i.e. what are the types and sizes of every Op input and output, and what edges connect them together.  There may be a *.pbtxt file generated as a side effect of execution which describes this graph.  Your python program will determine which subgraphs execute by session.run() statements that specify a set of input and output nodes that induce a subgraph.  Wherever two Ops in that subgraph don't have a serial dependency, i.e. one of them does not take as input an edge which is descendant from the other, they are potentially concurrent and may overlap in execution.  When that happens, all of their inputs and outputs are allocated simultaneously.  Knowing the types and sizes, you should be able to count up fairly precisely the memory requirement to execute each Op.  Keep in mind that Vars are always live, and if you're training, some values from the forward computation phase need to live until they're used (again) in the backprop phase.  This should all be explicit in the graph.\n\nIf you have some potential parallelism in the graph that may be causing OOM with an unlucky execution overlap, you can eliminate it by introducing a control edge that prevents one of the potentially overlapping Ops from beginning execution until the other has finished.  See [control dependencies](https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html#control_dependencies).\n\nIf your OOM problem is due to variable sized inputs, maybe you can change your input minibatch selection somehow to stay within your limits.\n"}