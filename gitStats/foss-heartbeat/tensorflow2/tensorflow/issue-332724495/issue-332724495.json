{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20055", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20055/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20055/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20055/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20055", "id": 332724495, "node_id": "MDU6SXNzdWUzMzI3MjQ0OTU=", "number": 20055, "title": "Speech_commands conv model depthwise_conv operation slow in tensorflow lite", "user": {"login": "nickwhsu", "id": 25834973, "node_id": "MDQ6VXNlcjI1ODM0OTcz", "avatar_url": "https://avatars1.githubusercontent.com/u/25834973?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nickwhsu", "html_url": "https://github.com/nickwhsu", "followers_url": "https://api.github.com/users/nickwhsu/followers", "following_url": "https://api.github.com/users/nickwhsu/following{/other_user}", "gists_url": "https://api.github.com/users/nickwhsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/nickwhsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nickwhsu/subscriptions", "organizations_url": "https://api.github.com/users/nickwhsu/orgs", "repos_url": "https://api.github.com/users/nickwhsu/repos", "events_url": "https://api.github.com/users/nickwhsu/events{/privacy}", "received_events_url": "https://api.github.com/users/nickwhsu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-06-15T10:30:24Z", "updated_at": "2018-07-26T01:20:00Z", "closed_at": "2018-07-26T01:20:00Z", "author_association": "NONE", "body_html": "<p>System information<br>\nHave I written custom code: Yes<br>\nOS Platform and Distribution: Linux Ubuntu 16.04<br>\nTensorFlow installed from (source): tensorflow v1.8.0<br>\nPython version: 3.6<br>\nBazel version (if compiling from source): 0.11.1<br>\nGCC/Compiler version (if compiling from source): 5.4<br>\nCUDA/cuDNN version: N/A<br>\nGPU model and memory: N/A<br>\nExact command to reproduce: N/A</p>\n<p>#<strong>Describe the problem</strong></p>\n<p>I am trying to use Speech_commands conv model (conv_actions_frozen.pb from \"<a href=\"http://download.tensorflow.org/models/speech_commands_v0.01.zip\" rel=\"nofollow\">http://download.tensorflow.org/models/speech_commands_v0.01.zip</a>\")<br>\nto inference on tensorflow lite.<br>\nI use TOCO to convert conv_actions_frozen.pb to conv_actions_frozen.tflite, and it convert the conv to<br>\ndepthwise_conv  + conv operation.</p>\n<p>Below is inference log (depthwise_conv  + conv ) :</p>\n<p>Loaded model conv_actions_frozen.tflite<br>\nresolved reporter<br>\nResizing input tensor<br>\nAUDIO_SPECTROGRAM computing time: 1.42 ms<br>\nMfcc computing time: 2.143 ms<br>\nReshape computing time: 0.003 ms<br>\nDepthWise_Conv computing time: 68.609 ms<br>\nMaxPool computing time: 0.484 ms<br>\nEvalFloat<br>\nEvalFloat kMultithreadOptimized<br>\nConv computing time: 21.343 ms<br>\ninvoked<br>\naverage time: 102.171 ms</p>\n<p>I found  that DepthWise_Conv operation took so long to compute , so i modify TOCO (toco_tooling.cc) to<br>\nlet converting process without ConvToDepthwise transformation.<br>\n//transformations.Add(new ConvertPureConvToDepthwise);</p>\n<p>Here is the inference log (conv + conv ) :</p>\n<p>Loaded model noDSconv.tflite<br>\nresolved reporter<br>\nResizing input tensor<br>\nAUDIO_SPECTROGRAM computing time: 1.083 ms<br>\nMfcc computing time: 1.551 ms<br>\nReshape computing time: 0.002 ms<br>\nEvalFloat<br>\nEvalFloat kMultithreadOptimized<br>\nConv computing time: 10.334 ms<br>\nMaxPool computing time: 0.454 ms<br>\nEvalFloat<br>\nEvalFloat kMultithreadOptimized<br>\nConv computing time: 18.243 ms<br>\ninvoked<br>\naverage time: 32.628 ms</p>\n<p>I would expect the inference on tflite should be fast after TOCO converting the model.<br>\nIs there any reason or clue can check this?<br>\nThanks.</p>", "body_text": "System information\nHave I written custom code: Yes\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from (source): tensorflow v1.8.0\nPython version: 3.6\nBazel version (if compiling from source): 0.11.1\nGCC/Compiler version (if compiling from source): 5.4\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n#Describe the problem\nI am trying to use Speech_commands conv model (conv_actions_frozen.pb from \"http://download.tensorflow.org/models/speech_commands_v0.01.zip\")\nto inference on tensorflow lite.\nI use TOCO to convert conv_actions_frozen.pb to conv_actions_frozen.tflite, and it convert the conv to\ndepthwise_conv  + conv operation.\nBelow is inference log (depthwise_conv  + conv ) :\nLoaded model conv_actions_frozen.tflite\nresolved reporter\nResizing input tensor\nAUDIO_SPECTROGRAM computing time: 1.42 ms\nMfcc computing time: 2.143 ms\nReshape computing time: 0.003 ms\nDepthWise_Conv computing time: 68.609 ms\nMaxPool computing time: 0.484 ms\nEvalFloat\nEvalFloat kMultithreadOptimized\nConv computing time: 21.343 ms\ninvoked\naverage time: 102.171 ms\nI found  that DepthWise_Conv operation took so long to compute , so i modify TOCO (toco_tooling.cc) to\nlet converting process without ConvToDepthwise transformation.\n//transformations.Add(new ConvertPureConvToDepthwise);\nHere is the inference log (conv + conv ) :\nLoaded model noDSconv.tflite\nresolved reporter\nResizing input tensor\nAUDIO_SPECTROGRAM computing time: 1.083 ms\nMfcc computing time: 1.551 ms\nReshape computing time: 0.002 ms\nEvalFloat\nEvalFloat kMultithreadOptimized\nConv computing time: 10.334 ms\nMaxPool computing time: 0.454 ms\nEvalFloat\nEvalFloat kMultithreadOptimized\nConv computing time: 18.243 ms\ninvoked\naverage time: 32.628 ms\nI would expect the inference on tflite should be fast after TOCO converting the model.\nIs there any reason or clue can check this?\nThanks.", "body": "System information\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nTensorFlow installed from (source): tensorflow v1.8.0\r\nPython version: 3.6\r\nBazel version (if compiling from source): 0.11.1\r\nGCC/Compiler version (if compiling from source): 5.4\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\n#**Describe the problem**\r\n\r\nI am trying to use Speech_commands conv model (conv_actions_frozen.pb from \"http://download.tensorflow.org/models/speech_commands_v0.01.zip\")\r\nto inference on tensorflow lite. \r\nI use TOCO to convert conv_actions_frozen.pb to conv_actions_frozen.tflite, and it convert the conv to \r\ndepthwise_conv  + conv operation.\r\n\r\nBelow is inference log (depthwise_conv  + conv ) :\r\n\r\nLoaded model conv_actions_frozen.tflite\r\nresolved reporter\r\nResizing input tensor\r\nAUDIO_SPECTROGRAM computing time: 1.42 ms \r\nMfcc computing time: 2.143 ms \r\nReshape computing time: 0.003 ms \r\nDepthWise_Conv computing time: 68.609 ms \r\nMaxPool computing time: 0.484 ms \r\nEvalFloat \r\nEvalFloat kMultithreadOptimized \r\nConv computing time: 21.343 ms \r\ninvoked \r\naverage time: 102.171 ms \r\n\r\nI found  that DepthWise_Conv operation took so long to compute , so i modify TOCO (toco_tooling.cc) to \r\nlet converting process without ConvToDepthwise transformation. \r\n//transformations.Add(new ConvertPureConvToDepthwise);\r\n\r\nHere is the inference log (conv + conv ) :\r\n\r\nLoaded model noDSconv.tflite\r\nresolved reporter\r\nResizing input tensor\r\nAUDIO_SPECTROGRAM computing time: 1.083 ms \r\nMfcc computing time: 1.551 ms \r\nReshape computing time: 0.002 ms \r\nEvalFloat \r\nEvalFloat kMultithreadOptimized \r\nConv computing time: 10.334 ms \r\nMaxPool computing time: 0.454 ms \r\nEvalFloat \r\nEvalFloat kMultithreadOptimized \r\nConv computing time: 18.243 ms \r\ninvoked \r\naverage time: 32.628 ms \r\n\r\n\r\nI would expect the inference on tflite should be fast after TOCO converting the model.\r\nIs there any reason or clue can check this?\r\nThanks.\r\n\r\n"}