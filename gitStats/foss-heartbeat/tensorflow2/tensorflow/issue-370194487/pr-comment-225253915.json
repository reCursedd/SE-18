{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/225253915", "pull_request_review_id": 164816377, "id": 225253915, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNTI1MzkxNQ==", "diff_hunk": "@@ -276,8 +275,8 @@ Status GdrMemoryManager::Init() {\n     };\n     GPUProcessState::singleton()->AddGPUAllocVisitor(bus_id,\n                                                      cuda_alloc_visitor);\n-    GPUProcessState::singleton()->AddCUDAHostAllocVisitor(bus_id,\n-                                                          alloc_visitor);\n+    GPUProcessState::singleton()->AddCUDAHostAllocVisitor(0, alloc_visitor);\n+    GPUProcessState::singleton()->AddCUDAHostAllocVisitor(1, alloc_visitor);", "path": "tensorflow/contrib/gdr/gdr_memory_manager.cc", "position": null, "original_position": 19, "commit_id": "5c750c20a43148dc02b1677885d950e7e005a0a2", "original_commit_id": "9a0bc94a86e090d1eafaf5b05afd48f6bc0f753c", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "body": "This seems fine for now.  NUMA specific allocation has not yet been turned on, pending bringing TF's eigen version up to date.   It might be better to add a visitor for every available NUMA node by using port::NUMANumNodes.   ", "created_at": "2018-10-15T17:35:26Z", "updated_at": "2018-10-22T17:56:52Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22989#discussion_r225253915", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22989", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/225253915"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22989#discussion_r225253915"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22989"}}, "body_html": "<p>This seems fine for now.  NUMA specific allocation has not yet been turned on, pending bringing TF's eigen version up to date.   It might be better to add a visitor for every available NUMA node by using port::NUMANumNodes.</p>", "body_text": "This seems fine for now.  NUMA specific allocation has not yet been turned on, pending bringing TF's eigen version up to date.   It might be better to add a visitor for every available NUMA node by using port::NUMANumNodes."}