{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20915", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20915/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20915/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20915/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20915", "id": 342257922, "node_id": "MDU6SXNzdWUzNDIyNTc5MjI=", "number": 20915, "title": "Heavily increased memory consumption for optimizing batch_norm in tf versions > 1.3.0 ", "user": {"login": "moboehle", "id": 26657721, "node_id": "MDQ6VXNlcjI2NjU3NzIx", "avatar_url": "https://avatars0.githubusercontent.com/u/26657721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/moboehle", "html_url": "https://github.com/moboehle", "followers_url": "https://api.github.com/users/moboehle/followers", "following_url": "https://api.github.com/users/moboehle/following{/other_user}", "gists_url": "https://api.github.com/users/moboehle/gists{/gist_id}", "starred_url": "https://api.github.com/users/moboehle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/moboehle/subscriptions", "organizations_url": "https://api.github.com/users/moboehle/orgs", "repos_url": "https://api.github.com/users/moboehle/repos", "events_url": "https://api.github.com/users/moboehle/events{/privacy}", "received_events_url": "https://api.github.com/users/moboehle/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "smit-hinsu", "id": 1990079, "node_id": "MDQ6VXNlcjE5OTAwNzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1990079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smit-hinsu", "html_url": "https://github.com/smit-hinsu", "followers_url": "https://api.github.com/users/smit-hinsu/followers", "following_url": "https://api.github.com/users/smit-hinsu/following{/other_user}", "gists_url": "https://api.github.com/users/smit-hinsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/smit-hinsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smit-hinsu/subscriptions", "organizations_url": "https://api.github.com/users/smit-hinsu/orgs", "repos_url": "https://api.github.com/users/smit-hinsu/repos", "events_url": "https://api.github.com/users/smit-hinsu/events{/privacy}", "received_events_url": "https://api.github.com/users/smit-hinsu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "smit-hinsu", "id": 1990079, "node_id": "MDQ6VXNlcjE5OTAwNzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1990079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smit-hinsu", "html_url": "https://github.com/smit-hinsu", "followers_url": "https://api.github.com/users/smit-hinsu/followers", "following_url": "https://api.github.com/users/smit-hinsu/following{/other_user}", "gists_url": "https://api.github.com/users/smit-hinsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/smit-hinsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smit-hinsu/subscriptions", "organizations_url": "https://api.github.com/users/smit-hinsu/orgs", "repos_url": "https://api.github.com/users/smit-hinsu/repos", "events_url": "https://api.github.com/users/smit-hinsu/events{/privacy}", "received_events_url": "https://api.github.com/users/smit-hinsu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2018-07-18T09:54:42Z", "updated_at": "2018-11-14T00:58:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</p>\n<p>I attach a code example that is intended to profile memory consumption for different layers of a network under different tensorflow versions. This should in principle work out of the box.</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:</p>\n<p>VERSION=\"16.04.3 LTS (Xenial Xerus)\"<br>\nVERSION_ID=\"16.04\"<br>\nVERSION_CODENAME=xenial</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:</p>\n<p>pip install tensorflow-gpu==1.3.0<br>\nand<br>\npip install tensorflow-gpu==1.5.0<br>\nand<br>\npip install tensorflow-gpu==1.8.0<br>\nand<br>\npip install tensorflow-gpu==1.9.0</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\nThe following tf versions were used to recreate the issue<br>\ntf.GIT_VERSION = b'unknown'<br>\ntf.VERSION = 1.3.0<br>\nand<br>\ntf.GIT_VERSION = v1.5.0-0-g37aa430d84<br>\ntf.VERSION = 1.5.0<br>\nand<br>\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072<br>\ntf.VERSION = 1.8.0<br>\nand<br>\ntf.GIT_VERSION = v1.9.0-0-g25c197e023<br>\ntf.VERSION = 1.9.0</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\nPython 3.5.5 :: Anaconda, Inc.</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nFor tf 1.3.0:<br>\nnvcc: NVIDIA (R) Cuda compiler driver<br>\nCopyright (c) 2005-2016 NVIDIA Corporation<br>\nBuilt on Tue_Jan_10_13:22:03_CST_2017<br>\nCuda compilation tools, release 8.0, V8.0.61</p>\n<p>For tf 1.5.0, 1.8.0, 1.9.0<br>\nnvcc: NVIDIA (R) Cuda compiler driver<br>\nCopyright (c) 2005-2017 NVIDIA Corporation<br>\nBuilt on Fri_Sep__1_21:08:03_CDT_2017<br>\nCuda compilation tools, release 9.0, V9.0.176</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nGeForce GTX 1080 Ti<br>\ntotal memory shown as 10.91GiB</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:<br>\npython 'script shown below'</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When trying to update from tensorflow 1.3.0 to a newer version, we noticed that memory consumption increased significantly for our networks.<br>\nWe tried to find out what was causing this issue and realized that there is a large discrepancy for memory consumption in our batch normalization layers when optimizing the network in training.<br>\nI attach the code necessary to reproduce the results.<br>\nIn the code the following happens:<br>\nFirst, a rather useless network of 10 batch normalization layers is created. In the examples shown below, this network expects 1D input of width 500 with 32 channels and a batch size of 16.<br>\nSecond, a GPU profiler is started, repeatedly calling nvidia-smi to check the current memory consumption. While the memory usage is measured, the network is evaluated 2500 times, under many different conditions. Lastly, the timings and the memory consumption over time are saved and plotted for comparison of the tf versions.</p>\n<h3>What is the problem?</h3>\n<p>The results from running the code under 4 different conditions is shown in the plots below.</p>\n<ol>\n<li>Fused batch norm is used if possible, <em>with gradient update</em>, version specific batch norm</li>\n<li>Fused batch norm is used if possible, <em>without gradient update</em>, version specific batch norm</li>\n<li>No fused batch norm, <em>without gradient update</em>, all use batch normalization as in<br>\n<a href=\"https://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py\" rel=\"nofollow\">https://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py</a></li>\n<li>No fused batch norm,<em>with gradient update</em>, all use batch normalization as in<br>\n<a href=\"https://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py\" rel=\"nofollow\">https://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py</a></li>\n</ol>\n<p>For 3 and 4, we still run the different tf versions, but we copied the mentioned file directly to our repo and call the batch_normalization from that file.</p>\n<p>Furthermore, currently the layout optimizer in the session config is turned off, but we also tried this with the layout optimizer turned on and this has no effect on this issue.</p>\n<h4>1.</h4>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2205279/batch_norm_fused_optimizing_step.pdf\">batch_norm_fused_optimizing_step.pdf</a></p>\n<h4>2.</h4>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2205281/batch_norm_fused_without_optimizing_step.pdf\">batch_norm_fused_without_optimizing_step.pdf</a></p>\n<h4>3.</h4>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2205284/batch_norm_unfused_1.3_implementaion.pdf\">batch_norm_unfused_1.3_implementaion.pdf</a></p>\n<h4>4.</h4>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2205285/batch_norm_unfused_1.3_implementaion_with_update.pdf\">batch_norm_unfused_1.3_implementaion_with_update.pdf</a></p>\n<p>From these plots, it seems that versions &gt; 1.3.0 need a significant amount of extra memory when doing a gradient update step. For example, memory requirements for Batch norm in 1.9 increased by 50% compared to 1.3. Why is that?</p>\n<h3>Request</h3>\n<p>Is there a way to circumvent the increased memory consumption? I assume some optimizing is happening in the background in newer versions, which leads to an increase in memory requirements. Can this be turned off?</p>\n<h3>Source code / logs</h3>\n<p>Source code necessary to reproduce the problem is attached below.</p>\n<p>In principle, however, I repeatedly run nvidia-smi for memory profiling and measure timing and memory for the following</p>\n<pre><code>for i in range(self.steps):\n    start = time.time()\n    sess.run([net, train_op])\n    durations[i] = time.time() - start\n</code></pre>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2205292/code.zip\">code.zip</a></p>", "body_text": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\nI attach a code example that is intended to profile memory consumption for different layers of a network under different tensorflow versions. This should in principle work out of the box.\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n\nTensorFlow installed from (source or binary):\npip install tensorflow-gpu==1.3.0\nand\npip install tensorflow-gpu==1.5.0\nand\npip install tensorflow-gpu==1.8.0\nand\npip install tensorflow-gpu==1.9.0\n\n\nTensorFlow version (use command below):\nThe following tf versions were used to recreate the issue\ntf.GIT_VERSION = b'unknown'\ntf.VERSION = 1.3.0\nand\ntf.GIT_VERSION = v1.5.0-0-g37aa430d84\ntf.VERSION = 1.5.0\nand\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\ntf.VERSION = 1.8.0\nand\ntf.GIT_VERSION = v1.9.0-0-g25c197e023\ntf.VERSION = 1.9.0\n\n\nPython version:\nPython 3.5.5 :: Anaconda, Inc.\n\n\nCUDA/cuDNN version:\nFor tf 1.3.0:\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2016 NVIDIA Corporation\nBuilt on Tue_Jan_10_13:22:03_CST_2017\nCuda compilation tools, release 8.0, V8.0.61\nFor tf 1.5.0, 1.8.0, 1.9.0\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2017 NVIDIA Corporation\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\nCuda compilation tools, release 9.0, V9.0.176\n\n\nBazel version (if compiling from source):\nN/A\n\n\nGPU model and memory:\nGeForce GTX 1080 Ti\ntotal memory shown as 10.91GiB\n\n\nExact command to reproduce:\npython 'script shown below'\n\n\nDescribe the problem\nWhen trying to update from tensorflow 1.3.0 to a newer version, we noticed that memory consumption increased significantly for our networks.\nWe tried to find out what was causing this issue and realized that there is a large discrepancy for memory consumption in our batch normalization layers when optimizing the network in training.\nI attach the code necessary to reproduce the results.\nIn the code the following happens:\nFirst, a rather useless network of 10 batch normalization layers is created. In the examples shown below, this network expects 1D input of width 500 with 32 channels and a batch size of 16.\nSecond, a GPU profiler is started, repeatedly calling nvidia-smi to check the current memory consumption. While the memory usage is measured, the network is evaluated 2500 times, under many different conditions. Lastly, the timings and the memory consumption over time are saved and plotted for comparison of the tf versions.\nWhat is the problem?\nThe results from running the code under 4 different conditions is shown in the plots below.\n\nFused batch norm is used if possible, with gradient update, version specific batch norm\nFused batch norm is used if possible, without gradient update, version specific batch norm\nNo fused batch norm, without gradient update, all use batch normalization as in\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py\nNo fused batch norm,with gradient update, all use batch normalization as in\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py\n\nFor 3 and 4, we still run the different tf versions, but we copied the mentioned file directly to our repo and call the batch_normalization from that file.\nFurthermore, currently the layout optimizer in the session config is turned off, but we also tried this with the layout optimizer turned on and this has no effect on this issue.\n1.\nbatch_norm_fused_optimizing_step.pdf\n2.\nbatch_norm_fused_without_optimizing_step.pdf\n3.\nbatch_norm_unfused_1.3_implementaion.pdf\n4.\nbatch_norm_unfused_1.3_implementaion_with_update.pdf\nFrom these plots, it seems that versions > 1.3.0 need a significant amount of extra memory when doing a gradient update step. For example, memory requirements for Batch norm in 1.9 increased by 50% compared to 1.3. Why is that?\nRequest\nIs there a way to circumvent the increased memory consumption? I assume some optimizing is happening in the background in newer versions, which leads to an increase in memory requirements. Can this be turned off?\nSource code / logs\nSource code necessary to reproduce the problem is attached below.\nIn principle, however, I repeatedly run nvidia-smi for memory profiling and measure timing and memory for the following\nfor i in range(self.steps):\n    start = time.time()\n    sess.run([net, train_op])\n    durations[i] = time.time() - start\n\ncode.zip", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\n    I attach a code example that is intended to profile memory consumption for different layers of a network under different tensorflow versions. This should in principle work out of the box.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\n    VERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\n    VERSION_ID=\"16.04\"\r\n    VERSION_CODENAME=xenial\r\n\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\n    pip install tensorflow-gpu==1.3.0\r\n    and\r\n    pip install tensorflow-gpu==1.5.0\r\n    and\r\n    pip install tensorflow-gpu==1.8.0\r\n    and\r\n    pip install tensorflow-gpu==1.9.0\r\n\r\n- **TensorFlow version (use command below)**:\r\nThe following tf versions were used to recreate the issue\r\n    tf.GIT_VERSION = b'unknown' \r\n    tf.VERSION = 1.3.0\r\n    and\r\n    tf.GIT_VERSION = v1.5.0-0-g37aa430d84\r\n    tf.VERSION = 1.5.0\r\n    and\r\n    tf.GIT_VERSION = v1.8.0-0-g93bc2e2072 \r\n    tf.VERSION = 1.8.0\r\n    and\r\n    tf.GIT_VERSION = v1.9.0-0-g25c197e023\r\n    tf.VERSION = 1.9.0\r\n  \r\n- **Python version**:\r\nPython 3.5.5 :: Anaconda, Inc.\r\n- **CUDA/cuDNN version**:\r\n    For tf 1.3.0:\r\n    nvcc: NVIDIA (R) Cuda compiler driver\r\n    Copyright (c) 2005-2016 NVIDIA Corporation\r\n    Built on Tue_Jan_10_13:22:03_CST_2017\r\n    Cuda compilation tools, release 8.0, V8.0.61\r\n    \r\n    For tf 1.5.0, 1.8.0, 1.9.0\r\n    nvcc: NVIDIA (R) Cuda compiler driver\r\n    Copyright (c) 2005-2017 NVIDIA Corporation\r\n    Built on Fri_Sep__1_21:08:03_CDT_2017\r\n    Cuda compilation tools, release 9.0, V9.0.176\r\n- **Bazel version (if compiling from source)**:\r\n    N/A\r\n- **GPU model and memory**:\r\n    GeForce GTX 1080 Ti\r\n    total memory shown as 10.91GiB\r\n- **Exact command to reproduce**:\r\n    python 'script shown below'\r\n\r\n### Describe the problem\r\nWhen trying to update from tensorflow 1.3.0 to a newer version, we noticed that memory consumption increased significantly for our networks.  \r\nWe tried to find out what was causing this issue and realized that there is a large discrepancy for memory consumption in our batch normalization layers when optimizing the network in training.\r\nI attach the code necessary to reproduce the results. \r\nIn the code the following happens:\r\n   First, a rather useless network of 10 batch normalization layers is created. In the examples shown below, this network expects 1D input of width 500 with 32 channels and a batch size of 16. \r\n   Second, a GPU profiler is started, repeatedly calling nvidia-smi to check the current memory consumption. While the memory usage is measured, the network is evaluated 2500 times, under many different conditions. Lastly, the timings and the memory consumption over time are saved and plotted for comparison of the tf versions. \r\n\r\n### What is the problem? \r\n\r\nThe results from running the code under 4 different conditions is shown in the plots below.\r\n1. Fused batch norm is used if possible, *with gradient update*, version specific batch norm\r\n2. Fused batch norm is used if possible, *without gradient update*, version specific batch norm\r\n3. No fused batch norm, *without gradient update*, all use batch normalization as in \r\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py\r\n4. No fused batch norm,*with gradient update*, all use batch normalization as in \r\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/r1.3/tensorflow/python/layers/normalization.py\r\n\r\nFor 3 and 4, we still run the different tf versions, but we copied the mentioned file directly to our repo and call the batch_normalization from that file.\r\n\r\nFurthermore, currently the layout optimizer in the session config is turned off, but we also tried this with the layout optimizer turned on and this has no effect on this issue.\r\n\r\n#### 1.\r\n[batch_norm_fused_optimizing_step.pdf](https://github.com/tensorflow/tensorflow/files/2205279/batch_norm_fused_optimizing_step.pdf)\r\n#### 2.\r\n[batch_norm_fused_without_optimizing_step.pdf](https://github.com/tensorflow/tensorflow/files/2205281/batch_norm_fused_without_optimizing_step.pdf)\r\n#### 3.\r\n[batch_norm_unfused_1.3_implementaion.pdf](https://github.com/tensorflow/tensorflow/files/2205284/batch_norm_unfused_1.3_implementaion.pdf)\r\n#### 4.\r\n[batch_norm_unfused_1.3_implementaion_with_update.pdf](https://github.com/tensorflow/tensorflow/files/2205285/batch_norm_unfused_1.3_implementaion_with_update.pdf)\r\n\r\nFrom these plots, it seems that versions > 1.3.0 need a significant amount of extra memory when doing a gradient update step. For example, memory requirements for Batch norm in 1.9 increased by 50% compared to 1.3. Why is that?\r\n\r\n### Request\r\nIs there a way to circumvent the increased memory consumption? I assume some optimizing is happening in the background in newer versions, which leads to an increase in memory requirements. Can this be turned off?\r\n\r\n\r\n### Source code / logs\r\nSource code necessary to reproduce the problem is attached below.\r\n\r\nIn principle, however, I repeatedly run nvidia-smi for memory profiling and measure timing and memory for the following \r\n```\r\nfor i in range(self.steps):\r\n    start = time.time()\r\n    sess.run([net, train_op])\r\n    durations[i] = time.time() - start\r\n```\r\n[code.zip](https://github.com/tensorflow/tensorflow/files/2205292/code.zip)\r\n"}