{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/416685756", "html_url": "https://github.com/tensorflow/tensorflow/issues/20915#issuecomment-416685756", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20915", "id": 416685756, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjY4NTc1Ng==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-28T18:07:53Z", "updated_at": "2018-08-28T18:07:53Z", "author_association": "MEMBER", "body_html": "<p>Briefly skimming your code, I noticed you ran</p>\n<pre><code>nvidia-smi --query-gpu=utilization.memory --format=csv\n</code></pre>\n<p>The name \"utilization.memory\" is very confusing. According to <code>nvidia-smi --help-query-gpu</code>, <code>utilization.memory</code> is the \"Percent of time over the past sample period during which global (device) memory was being read or written.\" This is not the amount of memory used.</p>\n<p>The best way to check for memory regression is to use binary search to find the largest batch size you can run with. If the largest batch size decreases, there is a regression. Alternatively, you can run with a fixed batch size and check memory usage with the <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/memory_stats/MaxBytesInUse\" rel=\"nofollow\"><code>tf.contrib.memory_stats.MaxBytesInUse</code></a> op</p>", "body_text": "Briefly skimming your code, I noticed you ran\nnvidia-smi --query-gpu=utilization.memory --format=csv\n\nThe name \"utilization.memory\" is very confusing. According to nvidia-smi --help-query-gpu, utilization.memory is the \"Percent of time over the past sample period during which global (device) memory was being read or written.\" This is not the amount of memory used.\nThe best way to check for memory regression is to use binary search to find the largest batch size you can run with. If the largest batch size decreases, there is a regression. Alternatively, you can run with a fixed batch size and check memory usage with the tf.contrib.memory_stats.MaxBytesInUse op", "body": "Briefly skimming your code, I noticed you ran \r\n```\r\nnvidia-smi --query-gpu=utilization.memory --format=csv\r\n```\r\nThe name \"utilization.memory\" is very confusing. According to `nvidia-smi --help-query-gpu`, `utilization.memory` is the \"Percent of time over the past sample period during which global (device) memory was being read or written.\" This is not the amount of memory used.\r\n\r\nThe best way to check for memory regression is to use binary search to find the largest batch size you can run with. If the largest batch size decreases, there is a regression. Alternatively, you can run with a fixed batch size and check memory usage with the [`tf.contrib.memory_stats.MaxBytesInUse`](https://www.tensorflow.org/api_docs/python/tf/contrib/memory_stats/MaxBytesInUse) op"}