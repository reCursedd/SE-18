{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/252763368", "html_url": "https://github.com/tensorflow/tensorflow/issues/4883#issuecomment-252763368", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4883", "id": 252763368, "node_id": "MDEyOklzc3VlQ29tbWVudDI1Mjc2MzM2OA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-10T22:30:01Z", "updated_at": "2016-10-10T22:30:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>You can use <code>stop_gradient</code> trick to replace gradient of any node with<br>\nsomething else<br>\n<a href=\"http://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182#36480182\" rel=\"nofollow\">http://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182#36480182</a></p>\n<p>On Mon, Oct 10, 2016 at 3:03 PM, zergylord <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p>Imagine a tiny network defined as follows, where linear is a typical<br>\nhelper function defining TensorFlow variables for a weight matrix and<br>\nactivation function:</p>\n<p>final_layer = linear(linear(_input,10,tf.nn.tanh),20)</p>\n<p>Normally this would be optimized via gradient descent on a loss:</p>\n<p>loss = tf.reduce_sum(tf.square(final_layer - _target))<br>\ntrain_step = tf.train.AdamOptimizer().minimmize(loss)</p>\n<p>But assume I'm getting the derivatives of the loss w.r.t. final_layer from<br>\nan external source (e.g. a tf.placeholder named _deriv). I would like to be<br>\nable to use this gradient information with one of the builtin optimizers to<br>\nbackpropagate and update the network parameters, but this appears to be<br>\ncurrently impossible.</p>\n<p>The workaround I'm currently using is to construct an artificial loss<br>\nconsisting of the inner product between _deriv and final_layer (since the<br>\nderivatives of this loss w.r.t. final_layer will be equal to _deriv).</p>\n<p>loss = tf.reduce_sum(final_layer*_deriv)<br>\ntrain_step = tf.train.AdamOptimizer().minimmize(loss)</p>\n<p>This is very wasteful though, as it needs to do this unnecessary inner<br>\nproduct and calculate its derivative for every training step even though I<br>\nalready know this information.</p>\n<p>For those thinking this an odd thing to need to do, it is necessary for<br>\nimplementing synthetic gradients <a href=\"https://arxiv.org/abs/1608.05343\" rel=\"nofollow\">https://arxiv.org/abs/1608.05343</a>.</p>\n<p>\u2014<br>\nYou are receiving this because you are subscribed to this thread.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"182126107\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4883\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4883/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4883\">#4883</a>, or mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/AABaHISF59xfOeRfoUw6a33uH1Yk_I-7ks5qyrZEgaJpZM4KTCSQ\">https://github.com/notifications/unsubscribe-auth/AABaHISF59xfOeRfoUw6a33uH1Yk_I-7ks5qyrZEgaJpZM4KTCSQ</a><br>\n.</p>\n</blockquote>", "body_text": "You can use stop_gradient trick to replace gradient of any node with\nsomething else\nhttp://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182#36480182\nOn Mon, Oct 10, 2016 at 3:03 PM, zergylord notifications@github.com wrote:\n\nImagine a tiny network defined as follows, where linear is a typical\nhelper function defining TensorFlow variables for a weight matrix and\nactivation function:\nfinal_layer = linear(linear(_input,10,tf.nn.tanh),20)\nNormally this would be optimized via gradient descent on a loss:\nloss = tf.reduce_sum(tf.square(final_layer - _target))\ntrain_step = tf.train.AdamOptimizer().minimmize(loss)\nBut assume I'm getting the derivatives of the loss w.r.t. final_layer from\nan external source (e.g. a tf.placeholder named _deriv). I would like to be\nable to use this gradient information with one of the builtin optimizers to\nbackpropagate and update the network parameters, but this appears to be\ncurrently impossible.\nThe workaround I'm currently using is to construct an artificial loss\nconsisting of the inner product between _deriv and final_layer (since the\nderivatives of this loss w.r.t. final_layer will be equal to _deriv).\nloss = tf.reduce_sum(final_layer*_deriv)\ntrain_step = tf.train.AdamOptimizer().minimmize(loss)\nThis is very wasteful though, as it needs to do this unnecessary inner\nproduct and calculate its derivative for every training step even though I\nalready know this information.\nFor those thinking this an odd thing to need to do, it is necessary for\nimplementing synthetic gradients https://arxiv.org/abs/1608.05343.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n#4883, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABaHISF59xfOeRfoUw6a33uH1Yk_I-7ks5qyrZEgaJpZM4KTCSQ\n.", "body": "You can use `stop_gradient` trick to replace gradient of any node with\nsomething else\nhttp://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182#36480182\n\nOn Mon, Oct 10, 2016 at 3:03 PM, zergylord notifications@github.com wrote:\n\n> Imagine a tiny network defined as follows, where linear is a typical\n> helper function defining TensorFlow variables for a weight matrix and\n> activation function:\n> \n> final_layer = linear(linear(_input,10,tf.nn.tanh),20)\n> \n> Normally this would be optimized via gradient descent on a loss:\n> \n> loss = tf.reduce_sum(tf.square(final_layer - _target))\n> train_step = tf.train.AdamOptimizer().minimmize(loss)\n> \n> But assume I'm getting the derivatives of the loss w.r.t. final_layer from\n> an external source (e.g. a tf.placeholder named _deriv). I would like to be\n> able to use this gradient information with one of the builtin optimizers to\n> backpropagate and update the network parameters, but this appears to be\n> currently impossible.\n> \n> The workaround I'm currently using is to construct an artificial loss\n> consisting of the inner product between _deriv and final_layer (since the\n> derivatives of this loss w.r.t. final_layer will be equal to _deriv).\n> \n> loss = tf.reduce_sum(final_layer*_deriv)\n> train_step = tf.train.AdamOptimizer().minimmize(loss)\n> \n> This is very wasteful though, as it needs to do this unnecessary inner\n> product and calculate its derivative for every training step even though I\n> already know this information.\n> \n> For those thinking this an odd thing to need to do, it is necessary for\n> implementing synthetic gradients https://arxiv.org/abs/1608.05343.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4883, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHISF59xfOeRfoUw6a33uH1Yk_I-7ks5qyrZEgaJpZM4KTCSQ\n> .\n"}