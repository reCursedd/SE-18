{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19890", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19890/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19890/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19890/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19890", "id": 330969616, "node_id": "MDU6SXNzdWUzMzA5Njk2MTY=", "number": 19890, "title": "Logits all reduced to very small value when training multi-label image classification", "user": {"login": "Lancerchiang", "id": 35952525, "node_id": "MDQ6VXNlcjM1OTUyNTI1", "avatar_url": "https://avatars2.githubusercontent.com/u/35952525?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lancerchiang", "html_url": "https://github.com/Lancerchiang", "followers_url": "https://api.github.com/users/Lancerchiang/followers", "following_url": "https://api.github.com/users/Lancerchiang/following{/other_user}", "gists_url": "https://api.github.com/users/Lancerchiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lancerchiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lancerchiang/subscriptions", "organizations_url": "https://api.github.com/users/Lancerchiang/orgs", "repos_url": "https://api.github.com/users/Lancerchiang/repos", "events_url": "https://api.github.com/users/Lancerchiang/events{/privacy}", "received_events_url": "https://api.github.com/users/Lancerchiang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-06-10T13:42:19Z", "updated_at": "2018-06-11T01:31:30Z", "closed_at": "2018-06-11T01:14:59Z", "author_association": "NONE", "body_html": "<pre><code>Tensorflow version: 1.7-gpu\nOperating system: ubuntu 16.04\n</code></pre>\n<p>I was training a multi-label image classification model (an image could have multiple labels, and the label is like <code>[1,0,0,0,1,0,0,1]</code> with variable number of 1s).</p>\n<p>I have switched to the <code>sigmoid_cross_entropy_with_logits</code> like below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> slim.arg_scope(inception_resnet_v2_arg_scope()):\n            logits, end_points <span class=\"pl-k\">=</span> inception_resnet_v2(images, <span class=\"pl-v\">num_classes</span><span class=\"pl-k\">=</span>dataset.num_classes, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n         \n        exclude <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>InceptionResnetV2/Logits<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>InceptionResnetV2/AuxLogits<span class=\"pl-pds\">'</span></span>]\n        variables_to_restore <span class=\"pl-k\">=</span> slim.get_variables_to_restore(<span class=\"pl-v\">exclude</span><span class=\"pl-k\">=</span>exclude)\n\n        fp_labels <span class=\"pl-k\">=</span> tf.cast(labels, tf.float32)\n        loss <span class=\"pl-k\">=</span> tf.nn.sigmoid_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>fp_labels, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\n        \n        total_loss <span class=\"pl-k\">=</span> tf.losses.get_total_loss()  \n        \n        global_step <span class=\"pl-k\">=</span> get_or_create_global_step()\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define your exponentially decaying learning rate</span>\n        lr <span class=\"pl-k\">=</span> tf.train.exponential_decay(\n            <span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span>initial_learning_rate,\n            <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step,\n            <span class=\"pl-v\">decay_steps</span><span class=\"pl-k\">=</span>decay_steps,\n            <span class=\"pl-v\">decay_rate</span><span class=\"pl-k\">=</span>learning_rate_decay_factor,\n            <span class=\"pl-v\">staircase</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Now we can define the optimizer that takes on the learning rate</span>\n        optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span>lr)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the train_op.</span>\n        train_op <span class=\"pl-k\">=</span> slim.learning.create_train_op(total_loss, optimizer)\n        \n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">train_step</span>(<span class=\"pl-smi\">sess</span>, <span class=\"pl-smi\">train_op</span>, <span class=\"pl-smi\">global_step</span>):\n   \n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Check the time for each sess run</span>\n            start_time <span class=\"pl-k\">=</span> time.time()\n            total_loss, global_step_count, _ <span class=\"pl-k\">=</span> sess.run([train_op, global_step, metrics_op])\n            time_elapsed <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> start_time\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the logging to print some results</span>\n            logging.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>global step <span class=\"pl-c1\">%s</span>: loss: <span class=\"pl-c1\">%.4f</span> (<span class=\"pl-c1\">%.2f</span> sec/step)<span class=\"pl-pds\">'</span></span>, global_step_count, total_loss, \n            time_elapsed)\n\n            <span class=\"pl-k\">return</span> total_loss, global_step_count\n</pre></div>\n<p>But when I started training the loss is decreasing very fast (from 0.5 to almost 0 in several minites), and all the logits value became almost zero for every image.</p>\n<p>I tried manual loss function implementation but also failed.</p>\n<p>Any advice would be greatly appreciated!</p>", "body_text": "Tensorflow version: 1.7-gpu\nOperating system: ubuntu 16.04\n\nI was training a multi-label image classification model (an image could have multiple labels, and the label is like [1,0,0,0,1,0,0,1] with variable number of 1s).\nI have switched to the sigmoid_cross_entropy_with_logits like below:\nwith slim.arg_scope(inception_resnet_v2_arg_scope()):\n            logits, end_points = inception_resnet_v2(images, num_classes=dataset.num_classes, is_training=True)\n         \n        exclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\n        variables_to_restore = slim.get_variables_to_restore(exclude=exclude)\n\n        fp_labels = tf.cast(labels, tf.float32)\n        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=fp_labels, logits=logits)\n        \n        total_loss = tf.losses.get_total_loss()  \n        \n        global_step = get_or_create_global_step()\n\n        # Define your exponentially decaying learning rate\n        lr = tf.train.exponential_decay(\n            learning_rate=initial_learning_rate,\n            global_step=global_step,\n            decay_steps=decay_steps,\n            decay_rate=learning_rate_decay_factor,\n            staircase=True)\n\n        # Now we can define the optimizer that takes on the learning rate\n        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n\n        # Create the train_op.\n        train_op = slim.learning.create_train_op(total_loss, optimizer)\n        \n        def train_step(sess, train_op, global_step):\n   \n            # Check the time for each sess run\n            start_time = time.time()\n            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n            time_elapsed = time.time() - start_time\n\n            # Run the logging to print some results\n            logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, \n            time_elapsed)\n\n            return total_loss, global_step_count\n\nBut when I started training the loss is decreasing very fast (from 0.5 to almost 0 in several minites), and all the logits value became almost zero for every image.\nI tried manual loss function implementation but also failed.\nAny advice would be greatly appreciated!", "body": "```\r\nTensorflow version: 1.7-gpu\r\nOperating system: ubuntu 16.04\r\n```\r\n\r\nI was training a multi-label image classification model (an image could have multiple labels, and the label is like `[1,0,0,0,1,0,0,1]` with variable number of 1s).\r\n\r\nI have switched to the `sigmoid_cross_entropy_with_logits` like below:\r\n\r\n```python\r\nwith slim.arg_scope(inception_resnet_v2_arg_scope()):\r\n            logits, end_points = inception_resnet_v2(images, num_classes=dataset.num_classes, is_training=True)\r\n         \r\n        exclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\r\n        variables_to_restore = slim.get_variables_to_restore(exclude=exclude)\r\n\r\n        fp_labels = tf.cast(labels, tf.float32)\r\n        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=fp_labels, logits=logits)\r\n        \r\n        total_loss = tf.losses.get_total_loss()  \r\n        \r\n        global_step = get_or_create_global_step()\r\n\r\n        # Define your exponentially decaying learning rate\r\n        lr = tf.train.exponential_decay(\r\n            learning_rate=initial_learning_rate,\r\n            global_step=global_step,\r\n            decay_steps=decay_steps,\r\n            decay_rate=learning_rate_decay_factor,\r\n            staircase=True)\r\n\r\n        # Now we can define the optimizer that takes on the learning rate\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\r\n\r\n        # Create the train_op.\r\n        train_op = slim.learning.create_train_op(total_loss, optimizer)\r\n        \r\n        def train_step(sess, train_op, global_step):\r\n   \r\n            # Check the time for each sess run\r\n            start_time = time.time()\r\n            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\r\n            time_elapsed = time.time() - start_time\r\n\r\n            # Run the logging to print some results\r\n            logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, \r\n            time_elapsed)\r\n\r\n            return total_loss, global_step_count\r\n\r\n```\r\n\r\nBut when I started training the loss is decreasing very fast (from 0.5 to almost 0 in several minites), and all the logits value became almost zero for every image. \r\n\r\nI tried manual loss function implementation but also failed.\r\n\r\nAny advice would be greatly appreciated! "}