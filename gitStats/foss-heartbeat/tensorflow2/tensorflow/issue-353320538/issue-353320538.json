{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21824", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21824/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21824/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21824/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/21824", "id": 353320538, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEwNDAyMDI2", "number": 21824, "title": "Improve correctness of auto_parallel", "user": {"login": "sj6077", "id": 2465713, "node_id": "MDQ6VXNlcjI0NjU3MTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/2465713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sj6077", "html_url": "https://github.com/sj6077", "followers_url": "https://api.github.com/users/sj6077/followers", "following_url": "https://api.github.com/users/sj6077/following{/other_user}", "gists_url": "https://api.github.com/users/sj6077/gists{/gist_id}", "starred_url": "https://api.github.com/users/sj6077/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sj6077/subscriptions", "organizations_url": "https://api.github.com/users/sj6077/orgs", "repos_url": "https://api.github.com/users/sj6077/repos", "events_url": "https://api.github.com/users/sj6077/events{/privacy}", "received_events_url": "https://api.github.com/users/sj6077/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 390482148, "node_id": "MDU6TGFiZWwzOTA0ODIxNDg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20review", "name": "awaiting review", "color": "fef2c0", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "open", "locked": false, "assignee": {"login": "akshaym", "id": 122911, "node_id": "MDQ6VXNlcjEyMjkxMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/122911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akshaym", "html_url": "https://github.com/akshaym", "followers_url": "https://api.github.com/users/akshaym/followers", "following_url": "https://api.github.com/users/akshaym/following{/other_user}", "gists_url": "https://api.github.com/users/akshaym/gists{/gist_id}", "starred_url": "https://api.github.com/users/akshaym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akshaym/subscriptions", "organizations_url": "https://api.github.com/users/akshaym/orgs", "repos_url": "https://api.github.com/users/akshaym/repos", "events_url": "https://api.github.com/users/akshaym/events{/privacy}", "received_events_url": "https://api.github.com/users/akshaym/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akshaym", "id": 122911, "node_id": "MDQ6VXNlcjEyMjkxMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/122911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akshaym", "html_url": "https://github.com/akshaym", "followers_url": "https://api.github.com/users/akshaym/followers", "following_url": "https://api.github.com/users/akshaym/following{/other_user}", "gists_url": "https://api.github.com/users/akshaym/gists{/gist_id}", "starred_url": "https://api.github.com/users/akshaym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akshaym/subscriptions", "organizations_url": "https://api.github.com/users/akshaym/orgs", "repos_url": "https://api.github.com/users/akshaym/repos", "events_url": "https://api.github.com/users/akshaym/events{/privacy}", "received_events_url": "https://api.github.com/users/akshaym/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-08-23T10:30:00Z", "updated_at": "2018-11-21T19:03:34Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21824", "html_url": "https://github.com/tensorflow/tensorflow/pull/21824", "diff_url": "https://github.com/tensorflow/tensorflow/pull/21824.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/21824.patch"}, "body_html": "<p>This PR improves the correctness of current auto_parallel in grappler.</p>\n<p>The problem of current auto_parallel can be summarized as below.<br>\n<strong>First, each replica updates a variable independently.</strong> This computation result could be different with a single device, which computes gradients for large batch size and updates a variable once if you use complex optimizers like momentum optimizer.<br>\n<strong>Second, the result of gradient clipping is changed.</strong> In the current version, auto_parallel aggregates the input of variable update operators(e.g., ApplyGradientDescent) so \"clipping-&gt;aggregation\" is the current order if clipping is used. However, \u201caggregation-&gt;clipping\u201d is the right order to make the same result as a single-device job.</p>\n<p>This PR changes to aggregate gradients from auto differentiation of trainable variables.<br>\nThe aggregations are defined explicitly instead of implicit aggregation through update operators.</p>\n<p><strong>[Current auto_parallel]</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2465713/40906054-a946024c-681a-11e8-85af-b3d2db7803e5.jpg\"><img src=\"https://user-images.githubusercontent.com/2465713/40906054-a946024c-681a-11e8-85af-b3d2db7803e5.jpg\" alt=\"current_auto_parallel\" style=\"max-width:100%;\"></a></p>\n<p><strong>[auto_parallel in this PR]</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2465713/40886079-ac008a9c-676c-11e8-9397-bcf7f96513f0.jpg\"><img src=\"https://user-images.githubusercontent.com/2465713/40886079-ac008a9c-676c-11e8-9397-bcf7f96513f0.jpg\" alt=\"auto_parallel_after_pr\" style=\"max-width:100%;\"></a></p>\n<p><strong>Changes in this PR</strong></p>\n<ol>\n<li>\n<p>Assumptions<br>\nCurrent: variables are updated by optimizer operations<br>\nThis PR : all the gradients are made by tf.gradients, the gradients of trainable variables are the target of aggregation in synchronous training</p>\n</li>\n<li>\n<p>Correct gradient clipping(e.g. tf.clip_by_global_norm, tf.clip_by_value)<br>\nRefer figures above.</p>\n</li>\n<li>\n<p>The number of updates<br>\nCurrent : as many as the number of replicas(refer above)<br>\nThis PR : one (it can also improve performance on multi-machine environment)</p>\n</li>\n</ol>", "body_text": "This PR improves the correctness of current auto_parallel in grappler.\nThe problem of current auto_parallel can be summarized as below.\nFirst, each replica updates a variable independently. This computation result could be different with a single device, which computes gradients for large batch size and updates a variable once if you use complex optimizers like momentum optimizer.\nSecond, the result of gradient clipping is changed. In the current version, auto_parallel aggregates the input of variable update operators(e.g., ApplyGradientDescent) so \"clipping->aggregation\" is the current order if clipping is used. However, \u201caggregation->clipping\u201d is the right order to make the same result as a single-device job.\nThis PR changes to aggregate gradients from auto differentiation of trainable variables.\nThe aggregations are defined explicitly instead of implicit aggregation through update operators.\n[Current auto_parallel]\n\n[auto_parallel in this PR]\n\nChanges in this PR\n\n\nAssumptions\nCurrent: variables are updated by optimizer operations\nThis PR : all the gradients are made by tf.gradients, the gradients of trainable variables are the target of aggregation in synchronous training\n\n\nCorrect gradient clipping(e.g. tf.clip_by_global_norm, tf.clip_by_value)\nRefer figures above.\n\n\nThe number of updates\nCurrent : as many as the number of replicas(refer above)\nThis PR : one (it can also improve performance on multi-machine environment)", "body": "This PR improves the correctness of current auto_parallel in grappler.\r\n\r\nThe problem of current auto_parallel can be summarized as below.\r\n**First, each replica updates a variable independently.** This computation result could be different with a single device, which computes gradients for large batch size and updates a variable once if you use complex optimizers like momentum optimizer.  \r\n**Second, the result of gradient clipping is changed.** In the current version, auto_parallel aggregates the input of variable update operators(e.g., ApplyGradientDescent) so \"clipping->aggregation\" is the current order if clipping is used. However, \u201caggregation->clipping\u201d is the right order to make the same result as a single-device job.\r\n\r\nThis PR changes to aggregate gradients from auto differentiation of trainable variables.\r\nThe aggregations are defined explicitly instead of implicit aggregation through update operators. \r\n\r\n**[Current auto_parallel]**\r\n![current_auto_parallel](https://user-images.githubusercontent.com/2465713/40906054-a946024c-681a-11e8-85af-b3d2db7803e5.jpg)\r\n\r\n**[auto_parallel in this PR]**\r\n![auto_parallel_after_pr](https://user-images.githubusercontent.com/2465713/40886079-ac008a9c-676c-11e8-9397-bcf7f96513f0.jpg)\r\n\r\n**Changes in this PR**\r\n1. Assumptions\r\nCurrent: variables are updated by optimizer operations\r\nThis PR : all the gradients are made by tf.gradients, the gradients of trainable variables are the target of aggregation in synchronous training\r\n\r\n2. Correct gradient clipping(e.g. tf.clip_by_global_norm, tf.clip_by_value)\r\nRefer figures above.\r\n\r\n3. The number of updates\r\nCurrent : as many as the number of replicas(refer above)\r\nThis PR : one (it can also improve performance on multi-machine environment)"}