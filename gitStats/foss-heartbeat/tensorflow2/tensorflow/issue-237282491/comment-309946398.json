{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309946398", "html_url": "https://github.com/tensorflow/tensorflow/issues/10857#issuecomment-309946398", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10857", "id": 309946398, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTk0NjM5OA==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-21T02:39:12Z", "updated_at": "2017-06-21T02:39:12Z", "author_association": "NONE", "body_html": "<p>Yes, fine tune is done with a batch size of 1 (on each GPU). I am training on a 4x 1080 machine, and fine tuning would take around ~20 hours. Non-fused batch norm does work (gives correct gradients), but it is way too slow and will push the fine tuning time beyond 100 hours. It would be faster to simply retrain the original network in NHWC and fine tune there with non-fused batch norm, which is ironic, considering NCHW is the CuDNN canonic format.</p>", "body_text": "Yes, fine tune is done with a batch size of 1 (on each GPU). I am training on a 4x 1080 machine, and fine tuning would take around ~20 hours. Non-fused batch norm does work (gives correct gradients), but it is way too slow and will push the fine tuning time beyond 100 hours. It would be faster to simply retrain the original network in NHWC and fine tune there with non-fused batch norm, which is ironic, considering NCHW is the CuDNN canonic format.", "body": "Yes, fine tune is done with a batch size of 1 (on each GPU). I am training on a 4x 1080 machine, and fine tuning would take around ~20 hours. Non-fused batch norm does work (gives correct gradients), but it is way too slow and will push the fine tuning time beyond 100 hours. It would be faster to simply retrain the original network in NHWC and fine tune there with non-fused batch norm, which is ironic, considering NCHW is the CuDNN canonic format."}