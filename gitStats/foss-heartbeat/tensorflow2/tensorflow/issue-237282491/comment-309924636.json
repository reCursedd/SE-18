{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309924636", "html_url": "https://github.com/tensorflow/tensorflow/issues/10857#issuecomment-309924636", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10857", "id": 309924636, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTkyNDYzNg==", "user": {"login": "zhangyaobit", "id": 1034716, "node_id": "MDQ6VXNlcjEwMzQ3MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1034716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangyaobit", "html_url": "https://github.com/zhangyaobit", "followers_url": "https://api.github.com/users/zhangyaobit/followers", "following_url": "https://api.github.com/users/zhangyaobit/following{/other_user}", "gists_url": "https://api.github.com/users/zhangyaobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangyaobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangyaobit/subscriptions", "organizations_url": "https://api.github.com/users/zhangyaobit/orgs", "repos_url": "https://api.github.com/users/zhangyaobit/repos", "events_url": "https://api.github.com/users/zhangyaobit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangyaobit/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-20T23:59:54Z", "updated_at": "2017-06-21T00:01:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Setting is_training to False and at the same time doing gradient computation is expected to get erroneous gradients, because the second and third outputs of FusedBatchNorm (batch mean and batch variance) are only applicable/valid when is_training is True.</p>\n<p>Could you try train with fused batch norm, and fine-tune with non-fused batch norm?</p>", "body_text": "Setting is_training to False and at the same time doing gradient computation is expected to get erroneous gradients, because the second and third outputs of FusedBatchNorm (batch mean and batch variance) are only applicable/valid when is_training is True.\nCould you try train with fused batch norm, and fine-tune with non-fused batch norm?", "body": "Setting is_training to False and at the same time doing gradient computation is expected to get erroneous gradients, because the second and third outputs of FusedBatchNorm (batch mean and batch variance) are only applicable/valid when is_training is True. \r\n\r\nCould you try train with fused batch norm, and fine-tune with non-fused batch norm?"}