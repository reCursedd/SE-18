{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324741670", "html_url": "https://github.com/tensorflow/tensorflow/issues/10857#issuecomment-324741670", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10857", "id": 324741670, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDc0MTY3MA==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-24T20:02:43Z", "updated_at": "2017-08-24T20:02:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I used the kernel <code>BatchNormWithGlobalNormalizationGrad</code> for the backward implementation when is_training=False. This improves the speed a lot (my overall training time is then closed to the fused case when is_training=True).<br>\nTo really use this op there are still some issues:</p>\n<ol>\n<li>The op <code>BatchNormWithGlobalNormalizationGrad</code> was marked deprecated. I unmarked it to test but not sure if that's desired. It's a helpful op in this case.</li>\n<li>The op doesn't support backward. This will break \"grad of grad\" unit tests. I think \"grad of grad\" is less important but it could be implemented if really needed.</li>\n</ol>", "body_text": "I used the kernel BatchNormWithGlobalNormalizationGrad for the backward implementation when is_training=False. This improves the speed a lot (my overall training time is then closed to the fused case when is_training=True).\nTo really use this op there are still some issues:\n\nThe op BatchNormWithGlobalNormalizationGrad was marked deprecated. I unmarked it to test but not sure if that's desired. It's a helpful op in this case.\nThe op doesn't support backward. This will break \"grad of grad\" unit tests. I think \"grad of grad\" is less important but it could be implemented if really needed.", "body": "I used the kernel `BatchNormWithGlobalNormalizationGrad` for the backward implementation when is_training=False. This improves the speed a lot (my overall training time is then closed to the fused case when is_training=True).\r\nTo really use this op there are still some issues:\r\n1. The op `BatchNormWithGlobalNormalizationGrad` was marked deprecated. I unmarked it to test but not sure if that's desired. It's a helpful op in this case.\r\n2. The op doesn't support backward. This will break \"grad of grad\" unit tests. I think \"grad of grad\" is less important but it could be implemented if really needed."}