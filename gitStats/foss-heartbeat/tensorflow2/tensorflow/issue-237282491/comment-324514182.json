{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/324514182", "html_url": "https://github.com/tensorflow/tensorflow/issues/10857#issuecomment-324514182", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10857", "id": 324514182, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDUxNDE4Mg==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-24T02:22:20Z", "updated_at": "2017-08-24T02:22:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1034716\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zhangyaobit\">@zhangyaobit</a> Thanks for providing the implementation for backward. However, I noticed that the performance of FusedBatchNormGrad when is_training=False is very pool.<br>\nI didn't benchmark the op alone, but switching is_training on and off can affect my total training speed by 2.5x. My estimate is that the op itself would have at least 5x difference in speed.<br>\nI guess the reason is that the implementation now is not based on fused kernels. This makes it as slow as the non-fused kernels.</p>", "body_text": "@zhangyaobit Thanks for providing the implementation for backward. However, I noticed that the performance of FusedBatchNormGrad when is_training=False is very pool.\nI didn't benchmark the op alone, but switching is_training on and off can affect my total training speed by 2.5x. My estimate is that the op itself would have at least 5x difference in speed.\nI guess the reason is that the implementation now is not based on fused kernels. This makes it as slow as the non-fused kernels.", "body": "@zhangyaobit Thanks for providing the implementation for backward. However, I noticed that the performance of FusedBatchNormGrad when is_training=False is very pool. \r\nI didn't benchmark the op alone, but switching is_training on and off can affect my total training speed by 2.5x. My estimate is that the op itself would have at least 5x difference in speed.\r\nI guess the reason is that the implementation now is not based on fused kernels. This makes it as slow as the non-fused kernels."}