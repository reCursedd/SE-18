{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/410512055", "html_url": "https://github.com/tensorflow/tensorflow/issues/10857#issuecomment-410512055", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10857", "id": 410512055, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMDUxMjA1NQ==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-05T10:53:55Z", "updated_at": "2018-08-05T10:53:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>A quick test shows that fused &amp; non-fused BN have similar backward speed when freezed:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">from</span> tensorflow.python.client <span class=\"pl-k\">import</span> device_lib\nlocal_device_protos <span class=\"pl-k\">=</span> device_lib.list_local_devices()\n<span class=\"pl-c1\">GPU_MODE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>([x.name <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> local_device_protos <span class=\"pl-k\">if</span> x.device_type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>GPU<span class=\"pl-pds\">'</span></span>])\n\nN <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\nC <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\nH, W <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>N, C, H, W:<span class=\"pl-pds\">\"</span></span>, [N, C, H, W])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">benchmark_all</span>(<span class=\"pl-smi\">fuse</span>, <span class=\"pl-smi\">format</span>):\n    shape4d <span class=\"pl-k\">=</span> [N, C, H, W] <span class=\"pl-k\">if</span> <span class=\"pl-c1\">format</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>NCHW<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">else</span> [N, H, W, C]\n\n    tf.reset_default_graph()\n    <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape4d, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n    scale <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>scale<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[C])\n    offset <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>offset<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[C])\n    mean <span class=\"pl-k\">=</span> tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[C])\n    var <span class=\"pl-k\">=</span> tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[C]) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>.\n\n    <span class=\"pl-k\">if</span> fuse:\n        output, _, _ <span class=\"pl-k\">=</span> tf.nn.fused_batch_norm(\n            <span class=\"pl-c1\">input</span>, scale, offset, mean, var, <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-5</span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">format</span>, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">format</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>NCHW<span class=\"pl-pds\">'</span></span>:\n            newshape <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span>, C, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>]\n            scale <span class=\"pl-k\">=</span> tf.reshape(scale, newshape)\n            offset <span class=\"pl-k\">=</span> tf.reshape(offset, newshape)\n            mean <span class=\"pl-k\">=</span> tf.reshape(mean, newshape)\n            var <span class=\"pl-k\">=</span> tf.reshape(var, newshape)\n        output <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">input</span> <span class=\"pl-k\">-</span> mean) <span class=\"pl-k\">*</span> (tf.rsqrt(var <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1e-5</span>) <span class=\"pl-k\">*</span> scale) <span class=\"pl-k\">+</span> offset\n\n\n    forward_op <span class=\"pl-k\">=</span> output.op\n    cost <span class=\"pl-k\">=</span> tf.reduce_sum(output)\n    backward_op <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.1</span>).minimize(cost)\n\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">benchmark</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">nr_iter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>, <span class=\"pl-smi\">nr_warmup</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">GPU_MODE</span>:\n            nr_iter <span class=\"pl-k\">=</span> nr_iter <span class=\"pl-k\">//</span> <span class=\"pl-c1\">10</span>\n            nr_warmup <span class=\"pl-k\">=</span> nr_warmup <span class=\"pl-k\">//</span> <span class=\"pl-c1\">5</span>\n        <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(nr_warmup):\n            op.run()\n        start <span class=\"pl-k\">=</span> time.perf_counter()\n        <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(nr_iter):\n            op.run()\n        end <span class=\"pl-k\">=</span> time.perf_counter()\n        itr_per_sec <span class=\"pl-k\">=</span> nr_iter <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1</span>. <span class=\"pl-k\">/</span> (end <span class=\"pl-k\">-</span> start)\n        <span class=\"pl-k\">return</span> itr_per_sec\n\n    sess <span class=\"pl-k\">=</span> tf.Session()\n    <span class=\"pl-k\">with</span> sess.as_default():\n        sess.run(tf.global_variables_initializer())\n\n        spd_forward <span class=\"pl-k\">=</span> benchmark(forward_op)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Fuse=<span class=\"pl-c1\">{}</span>, Format=<span class=\"pl-c1\">{}</span>, Forward: <span class=\"pl-c1\">{}</span> itr/s<span class=\"pl-pds\">\"</span></span>.format(fuse, <span class=\"pl-c1\">format</span>, spd_forward))\n        spd_backward <span class=\"pl-k\">=</span> benchmark(backward_op)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Fuse=<span class=\"pl-c1\">{}</span>, Format=<span class=\"pl-c1\">{}</span>, Backward: <span class=\"pl-c1\">{}</span> itr/s<span class=\"pl-pds\">\"</span></span>.format(fuse, <span class=\"pl-c1\">format</span>, spd_backward))\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">GPU_MODE</span>:\n    formats <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>NHWC<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>NCHW<span class=\"pl-pds\">'</span></span>]\n<span class=\"pl-k\">else</span>:\n    formats <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>NHWC<span class=\"pl-pds\">'</span></span>]\n<span class=\"pl-k\">for</span> <span class=\"pl-c1\">format</span> <span class=\"pl-k\">in</span> formats:\n    <span class=\"pl-k\">for</span> fuse <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">True</span>, <span class=\"pl-c1\">False</span>]:\n        benchmark_all(fuse, <span class=\"pl-c1\">format</span>)</pre></div>\n<p>Outputs on GTX1080Ti:</p>\n<pre><code>N, C, H, W: [64, 128, 128, 128]\nFuse=True, Format=NHWC, Forward: 100.62081027587591 itr/s\nFuse=True, Format=NHWC, Backward: 61.392495723794156 itr/s\nFuse=False, Format=NHWC, Forward: 100.51494045143419 itr/s\nFuse=False, Format=NHWC, Backward: 60.875319893753236 itr/s\nFuse=True, Format=NCHW, Forward: 261.46341501158184 itr/s\nFuse=True, Format=NCHW, Backward: 44.5398623696861 itr/s\nFuse=False, Format=NCHW, Forward: 71.64600813406135 itr/s\nFuse=False, Format=NCHW, Backward: 48.042794214933586 itr/s\n</code></pre>", "body_text": "A quick test shows that fused & non-fused BN have similar backward speed when freezed:\n#!/usr/bin/env python\nimport tensorflow as tf\nimport time\nimport os\nfrom tensorflow.python.client import device_lib\nlocal_device_protos = device_lib.list_local_devices()\nGPU_MODE = len([x.name for x in local_device_protos if x.device_type == 'GPU'])\n\nN = 64\nC = 128\nH, W = 128, 128\nprint(\"N, C, H, W:\", [N, C, H, W])\n\ndef benchmark_all(fuse, format):\n    shape4d = [N, C, H, W] if format == 'NCHW' else [N, H, W, C]\n\n    tf.reset_default_graph()\n    input = tf.get_variable('input', shape=shape4d, dtype=tf.float32)\n\n    scale = tf.get_variable('scale', shape=[C])\n    offset = tf.get_variable('offset', shape=[C])\n    mean = tf.random_normal(shape=[C])\n    var = tf.random_normal(shape=[C]) + 1.\n\n    if fuse:\n        output, _, _ = tf.nn.fused_batch_norm(\n            input, scale, offset, mean, var, epsilon=1e-5, data_format=format, is_training=False)\n    else:\n        if format == 'NCHW':\n            newshape = [1, C, 1, 1]\n            scale = tf.reshape(scale, newshape)\n            offset = tf.reshape(offset, newshape)\n            mean = tf.reshape(mean, newshape)\n            var = tf.reshape(var, newshape)\n        output = (input - mean) * (tf.rsqrt(var + 1e-5) * scale) + offset\n\n\n    forward_op = output.op\n    cost = tf.reduce_sum(output)\n    backward_op = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n\n\n    def benchmark(op, nr_iter=200, nr_warmup=10):\n        if not GPU_MODE:\n            nr_iter = nr_iter // 10\n            nr_warmup = nr_warmup // 5\n        for k in range(nr_warmup):\n            op.run()\n        start = time.perf_counter()\n        for k in range(nr_iter):\n            op.run()\n        end = time.perf_counter()\n        itr_per_sec = nr_iter * 1. / (end - start)\n        return itr_per_sec\n\n    sess = tf.Session()\n    with sess.as_default():\n        sess.run(tf.global_variables_initializer())\n\n        spd_forward = benchmark(forward_op)\n        print(\"Fuse={}, Format={}, Forward: {} itr/s\".format(fuse, format, spd_forward))\n        spd_backward = benchmark(backward_op)\n        print(\"Fuse={}, Format={}, Backward: {} itr/s\".format(fuse, format, spd_backward))\n\n\nif GPU_MODE:\n    formats = ['NHWC', 'NCHW']\nelse:\n    formats = ['NHWC']\nfor format in formats:\n    for fuse in [True, False]:\n        benchmark_all(fuse, format)\nOutputs on GTX1080Ti:\nN, C, H, W: [64, 128, 128, 128]\nFuse=True, Format=NHWC, Forward: 100.62081027587591 itr/s\nFuse=True, Format=NHWC, Backward: 61.392495723794156 itr/s\nFuse=False, Format=NHWC, Forward: 100.51494045143419 itr/s\nFuse=False, Format=NHWC, Backward: 60.875319893753236 itr/s\nFuse=True, Format=NCHW, Forward: 261.46341501158184 itr/s\nFuse=True, Format=NCHW, Backward: 44.5398623696861 itr/s\nFuse=False, Format=NCHW, Forward: 71.64600813406135 itr/s\nFuse=False, Format=NCHW, Backward: 48.042794214933586 itr/s", "body": "A quick test shows that fused & non-fused BN have similar backward speed when freezed:\r\n```python\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\nimport time\r\nimport os\r\nfrom tensorflow.python.client import device_lib\r\nlocal_device_protos = device_lib.list_local_devices()\r\nGPU_MODE = len([x.name for x in local_device_protos if x.device_type == 'GPU'])\r\n\r\nN = 64\r\nC = 128\r\nH, W = 128, 128\r\nprint(\"N, C, H, W:\", [N, C, H, W])\r\n\r\ndef benchmark_all(fuse, format):\r\n    shape4d = [N, C, H, W] if format == 'NCHW' else [N, H, W, C]\r\n\r\n    tf.reset_default_graph()\r\n    input = tf.get_variable('input', shape=shape4d, dtype=tf.float32)\r\n\r\n    scale = tf.get_variable('scale', shape=[C])\r\n    offset = tf.get_variable('offset', shape=[C])\r\n    mean = tf.random_normal(shape=[C])\r\n    var = tf.random_normal(shape=[C]) + 1.\r\n\r\n    if fuse:\r\n        output, _, _ = tf.nn.fused_batch_norm(\r\n            input, scale, offset, mean, var, epsilon=1e-5, data_format=format, is_training=False)\r\n    else:\r\n        if format == 'NCHW':\r\n            newshape = [1, C, 1, 1]\r\n            scale = tf.reshape(scale, newshape)\r\n            offset = tf.reshape(offset, newshape)\r\n            mean = tf.reshape(mean, newshape)\r\n            var = tf.reshape(var, newshape)\r\n        output = (input - mean) * (tf.rsqrt(var + 1e-5) * scale) + offset\r\n\r\n\r\n    forward_op = output.op\r\n    cost = tf.reduce_sum(output)\r\n    backward_op = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\r\n\r\n\r\n    def benchmark(op, nr_iter=200, nr_warmup=10):\r\n        if not GPU_MODE:\r\n            nr_iter = nr_iter // 10\r\n            nr_warmup = nr_warmup // 5\r\n        for k in range(nr_warmup):\r\n            op.run()\r\n        start = time.perf_counter()\r\n        for k in range(nr_iter):\r\n            op.run()\r\n        end = time.perf_counter()\r\n        itr_per_sec = nr_iter * 1. / (end - start)\r\n        return itr_per_sec\r\n\r\n    sess = tf.Session()\r\n    with sess.as_default():\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        spd_forward = benchmark(forward_op)\r\n        print(\"Fuse={}, Format={}, Forward: {} itr/s\".format(fuse, format, spd_forward))\r\n        spd_backward = benchmark(backward_op)\r\n        print(\"Fuse={}, Format={}, Backward: {} itr/s\".format(fuse, format, spd_backward))\r\n\r\n\r\nif GPU_MODE:\r\n    formats = ['NHWC', 'NCHW']\r\nelse:\r\n    formats = ['NHWC']\r\nfor format in formats:\r\n    for fuse in [True, False]:\r\n        benchmark_all(fuse, format)\r\n```\r\n\r\nOutputs on GTX1080Ti:\r\n```\r\nN, C, H, W: [64, 128, 128, 128]\r\nFuse=True, Format=NHWC, Forward: 100.62081027587591 itr/s\r\nFuse=True, Format=NHWC, Backward: 61.392495723794156 itr/s\r\nFuse=False, Format=NHWC, Forward: 100.51494045143419 itr/s\r\nFuse=False, Format=NHWC, Backward: 60.875319893753236 itr/s\r\nFuse=True, Format=NCHW, Forward: 261.46341501158184 itr/s\r\nFuse=True, Format=NCHW, Backward: 44.5398623696861 itr/s\r\nFuse=False, Format=NCHW, Forward: 71.64600813406135 itr/s\r\nFuse=False, Format=NCHW, Backward: 48.042794214933586 itr/s\r\n```"}