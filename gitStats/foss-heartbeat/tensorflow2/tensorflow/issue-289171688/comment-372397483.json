{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/372397483", "html_url": "https://github.com/tensorflow/tensorflow/issues/16182#issuecomment-372397483", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16182", "id": 372397483, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjM5NzQ4Mw==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-12T17:37:42Z", "updated_at": "2018-03-12T17:37:42Z", "author_association": "MEMBER", "body_html": "<p>No idea why anyone thinks I could answer this but I can try to be helpful.  thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1710528\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bhack\">@bhack</a></p>\n<p>tf.slim is deprecated and other than a few people using it the core TensorFlow team does not support it.  I realize this is frustrating because there are a lot of good examples in SLIM.  If you look at SLIM it extends what is tf.layers which is core and supported.  The multi-GPU support in SLIM is not very good, it works and is computationally correct but all the parameters are put on GPU:0 which is not efficient and in some setups without GPUDirect peer-to-peer it is awful for scaling.</p>\n<p>Going forward there are two options for high performance code for multi-GPU and otherwise:</p>\n<ol>\n<li>\n<p>This is the path forward:  use tf.layers to create the models and run with with tf.estimator or as part of tf.eager.  Keep in mind if using tf.eager the imperative execution can be made into a graph with a single call I believe.  Multi-GPU for Estimator is in alpha and a unified method for taking a single-GPU model going multi-GPU and then distributed is in development now.</p>\n</li>\n<li>\n<p>Use the bleeding edge code in tf_cnn_benchmarks to roll your own solution.  This means make your model in tf.layers and then use tf_cnn_benchmarks to create a multi-GPU or multi-node solution.  I would not do this as it is time consuming and prone to error.  The go forward solution is very close and I am monitoring the progress personally as I am also excited to bring multi-GPU to everyone.  I also own the performance section of tensorflow.org and have advocated for a high level API over the past year.</p>\n</li>\n</ol>\n<p>I apologize for grammar errors, I just wrote this down between meetings as I doubted I would come back to it.  I hope this is helpful.  I am marking this closed but I will try to answer other questions if you have them in the thread.  I also have insider information there might be some clarity on this as the TensorFlow Summit in a few weeks at the end of March.</p>", "body_text": "No idea why anyone thinks I could answer this but I can try to be helpful.  thanks @bhack\ntf.slim is deprecated and other than a few people using it the core TensorFlow team does not support it.  I realize this is frustrating because there are a lot of good examples in SLIM.  If you look at SLIM it extends what is tf.layers which is core and supported.  The multi-GPU support in SLIM is not very good, it works and is computationally correct but all the parameters are put on GPU:0 which is not efficient and in some setups without GPUDirect peer-to-peer it is awful for scaling.\nGoing forward there are two options for high performance code for multi-GPU and otherwise:\n\n\nThis is the path forward:  use tf.layers to create the models and run with with tf.estimator or as part of tf.eager.  Keep in mind if using tf.eager the imperative execution can be made into a graph with a single call I believe.  Multi-GPU for Estimator is in alpha and a unified method for taking a single-GPU model going multi-GPU and then distributed is in development now.\n\n\nUse the bleeding edge code in tf_cnn_benchmarks to roll your own solution.  This means make your model in tf.layers and then use tf_cnn_benchmarks to create a multi-GPU or multi-node solution.  I would not do this as it is time consuming and prone to error.  The go forward solution is very close and I am monitoring the progress personally as I am also excited to bring multi-GPU to everyone.  I also own the performance section of tensorflow.org and have advocated for a high level API over the past year.\n\n\nI apologize for grammar errors, I just wrote this down between meetings as I doubted I would come back to it.  I hope this is helpful.  I am marking this closed but I will try to answer other questions if you have them in the thread.  I also have insider information there might be some clarity on this as the TensorFlow Summit in a few weeks at the end of March.", "body": "No idea why anyone thinks I could answer this but I can try to be helpful.  thanks @bhack \r\n\r\ntf.slim is deprecated and other than a few people using it the core TensorFlow team does not support it.  I realize this is frustrating because there are a lot of good examples in SLIM.  If you look at SLIM it extends what is tf.layers which is core and supported.  The multi-GPU support in SLIM is not very good, it works and is computationally correct but all the parameters are put on GPU:0 which is not efficient and in some setups without GPUDirect peer-to-peer it is awful for scaling.  \r\n\r\nGoing forward there are two options for high performance code for multi-GPU and otherwise:\r\n1)  This is the path forward:  use tf.layers to create the models and run with with tf.estimator or as part of tf.eager.  Keep in mind if using tf.eager the imperative execution can be made into a graph with a single call I believe.  Multi-GPU for Estimator is in alpha and a unified method for taking a single-GPU model going multi-GPU and then distributed is in development now.  \r\n\r\n2) Use the bleeding edge code in tf_cnn_benchmarks to roll your own solution.  This means make your model in tf.layers and then use tf_cnn_benchmarks to create a multi-GPU or multi-node solution.  I would not do this as it is time consuming and prone to error.  The go forward solution is very close and I am monitoring the progress personally as I am also excited to bring multi-GPU to everyone.  I also own the performance section of tensorflow.org and have advocated for a high level API over the past year.  \r\n\r\nI apologize for grammar errors, I just wrote this down between meetings as I doubted I would come back to it.  I hope this is helpful.  I am marking this closed but I will try to answer other questions if you have them in the thread.  I also have insider information there might be some clarity on this as the TensorFlow Summit in a few weeks at the end of March.  \r\n\r\n\r\n"}