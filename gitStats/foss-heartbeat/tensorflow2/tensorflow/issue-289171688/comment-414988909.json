{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/414988909", "html_url": "https://github.com/tensorflow/tensorflow/issues/16182#issuecomment-414988909", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16182", "id": 414988909, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNDk4ODkwOQ==", "user": {"login": "gabrieldemarmiesse", "id": 12891691, "node_id": "MDQ6VXNlcjEyODkxNjkx", "avatar_url": "https://avatars1.githubusercontent.com/u/12891691?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gabrieldemarmiesse", "html_url": "https://github.com/gabrieldemarmiesse", "followers_url": "https://api.github.com/users/gabrieldemarmiesse/followers", "following_url": "https://api.github.com/users/gabrieldemarmiesse/following{/other_user}", "gists_url": "https://api.github.com/users/gabrieldemarmiesse/gists{/gist_id}", "starred_url": "https://api.github.com/users/gabrieldemarmiesse/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gabrieldemarmiesse/subscriptions", "organizations_url": "https://api.github.com/users/gabrieldemarmiesse/orgs", "repos_url": "https://api.github.com/users/gabrieldemarmiesse/repos", "events_url": "https://api.github.com/users/gabrieldemarmiesse/events{/privacy}", "received_events_url": "https://api.github.com/users/gabrieldemarmiesse/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-22T10:36:46Z", "updated_at": "2018-08-22T10:36:46Z", "author_association": "NONE", "body_html": "<p>You can use keras layers as drop-in replacement for tensorflow <code>tf.layers</code>. No need to change the rest of your code. There is a common misconception, which is that as soon as you start using a keras layer, you should use keras functions from start to end, which is simply not true. See this dummy example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> keras.layers <span class=\"pl-k\">import</span> Dropout, Dense\n<span class=\"pl-k\">from</span> keras <span class=\"pl-k\">import</span> backend <span class=\"pl-k\">as</span> K\n\nimg <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">784</span>))\nlabels <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">10</span>))\n\nx <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">128</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(img)\nx <span class=\"pl-k\">*=</span> <span class=\"pl-c1\">4</span>\nx <span class=\"pl-k\">=</span> tf.clip_by_value(x, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">7</span>)\nx <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">128</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(x)\nx <span class=\"pl-k\">=</span> Dropout(<span class=\"pl-c1\">0.5</span>)(x)\npreds <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>softmax<span class=\"pl-pds\">'</span></span>)(x)\n\nloss <span class=\"pl-k\">=</span> tf.reduce_mean(categorical_crossentropy(labels, preds))\n\ntrain_step <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.5</span>).minimize(loss)\n<span class=\"pl-k\">with</span> sess.as_default():\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n        batch <span class=\"pl-k\">=</span> mnist_data.train.next_batch(<span class=\"pl-c1\">50</span>)\n        train_step.run(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{img: batch[<span class=\"pl-c1\">0</span>],\n                                  labels: batch[<span class=\"pl-c1\">1</span>],\n                                  K.learning_phase(): <span class=\"pl-c1\">1</span>})\n\nacc_value <span class=\"pl-k\">=</span> accuracy(labels, preds)\n<span class=\"pl-k\">with</span> sess.as_default():\n    <span class=\"pl-c1\">print</span> acc_value.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{img: mnist_data.test.images,\n                                    labels: mnist_data.test.labels,\n                                    K.learning_phase(): <span class=\"pl-c1\">0</span>})</pre></div>\n<p>See <a href=\"https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\" rel=\"nofollow\">https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html</a></p>", "body_text": "You can use keras layers as drop-in replacement for tensorflow tf.layers. No need to change the rest of your code. There is a common misconception, which is that as soon as you start using a keras layer, you should use keras functions from start to end, which is simply not true. See this dummy example:\nfrom keras.layers import Dropout, Dense\nfrom keras import backend as K\n\nimg = tf.placeholder(tf.float32, shape=(None, 784))\nlabels = tf.placeholder(tf.float32, shape=(None, 10))\n\nx = Dense(128, activation='relu')(img)\nx *= 4\nx = tf.clip_by_value(x, 4, 7)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\npreds = Dense(10, activation='softmax')(x)\n\nloss = tf.reduce_mean(categorical_crossentropy(labels, preds))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\nwith sess.as_default():\n    for i in range(100):\n        batch = mnist_data.train.next_batch(50)\n        train_step.run(feed_dict={img: batch[0],\n                                  labels: batch[1],\n                                  K.learning_phase(): 1})\n\nacc_value = accuracy(labels, preds)\nwith sess.as_default():\n    print acc_value.eval(feed_dict={img: mnist_data.test.images,\n                                    labels: mnist_data.test.labels,\n                                    K.learning_phase(): 0})\nSee https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html", "body": "You can use keras layers as drop-in replacement for tensorflow `tf.layers`. No need to change the rest of your code. There is a common misconception, which is that as soon as you start using a keras layer, you should use keras functions from start to end, which is simply not true. See this dummy example:\r\n\r\n```python\r\nfrom keras.layers import Dropout, Dense\r\nfrom keras import backend as K\r\n\r\nimg = tf.placeholder(tf.float32, shape=(None, 784))\r\nlabels = tf.placeholder(tf.float32, shape=(None, 10))\r\n\r\nx = Dense(128, activation='relu')(img)\r\nx *= 4\r\nx = tf.clip_by_value(x, 4, 7)\r\nx = Dense(128, activation='relu')(x)\r\nx = Dropout(0.5)(x)\r\npreds = Dense(10, activation='softmax')(x)\r\n\r\nloss = tf.reduce_mean(categorical_crossentropy(labels, preds))\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\nwith sess.as_default():\r\n    for i in range(100):\r\n        batch = mnist_data.train.next_batch(50)\r\n        train_step.run(feed_dict={img: batch[0],\r\n                                  labels: batch[1],\r\n                                  K.learning_phase(): 1})\r\n\r\nacc_value = accuracy(labels, preds)\r\nwith sess.as_default():\r\n    print acc_value.eval(feed_dict={img: mnist_data.test.images,\r\n                                    labels: mnist_data.test.labels,\r\n                                    K.learning_phase(): 0})\r\n```\r\n\r\nSee https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\r\n"}