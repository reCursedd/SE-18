{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3888", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3888/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3888/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3888/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3888", "id": 171774451, "node_id": "MDU6SXNzdWUxNzE3NzQ0NTE=", "number": 3888, "title": "Tensorflow loss not changing and also computed gradients and applied batch norm but still loss is not changing?", "user": {"login": "inderpreetsganda", "id": 10248819, "node_id": "MDQ6VXNlcjEwMjQ4ODE5", "avatar_url": "https://avatars0.githubusercontent.com/u/10248819?v=4", "gravatar_id": "", "url": "https://api.github.com/users/inderpreetsganda", "html_url": "https://github.com/inderpreetsganda", "followers_url": "https://api.github.com/users/inderpreetsganda/followers", "following_url": "https://api.github.com/users/inderpreetsganda/following{/other_user}", "gists_url": "https://api.github.com/users/inderpreetsganda/gists{/gist_id}", "starred_url": "https://api.github.com/users/inderpreetsganda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/inderpreetsganda/subscriptions", "organizations_url": "https://api.github.com/users/inderpreetsganda/orgs", "repos_url": "https://api.github.com/users/inderpreetsganda/repos", "events_url": "https://api.github.com/users/inderpreetsganda/events{/privacy}", "received_events_url": "https://api.github.com/users/inderpreetsganda/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-08-17T22:18:49Z", "updated_at": "2016-08-17T22:25:26Z", "closed_at": "2016-08-17T22:25:26Z", "author_association": "NONE", "body_html": "<p>Tensorflow loss is not changing. This is my code.</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nimport tflearn\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.01\nnum_classes = 8\npath1 = \"/home/indy/Downloads/aclImdb/train/pos\"\npath2 = \"/home/indy/Downloads/aclImdb/train/neg\"\npath3 = \"/home/indy/Downloads/aclImdb/test/pos\"\npath4 = \"/home/indy/Downloads/aclImdb/test/neg\"\ntime_steps = 300\nembedding = 50\nstep = 1\n\n\ndef get_embedding():\n    gfile_path = os.path.join(\"/home/indy/Downloads/glove.6B\", \"glove.6B.50d.txt\")\n    f = open(gfile_path,'r')\n    embeddings = {}\n    for line in f:\n        sp_value = line.split()\n        word = sp_value[0]\n        embedding = [float(value) for value in sp_value[1:]]\n        assert len(embedding) == 50\n        embeddings[word] = embedding\n    return embeddings\n\nebd = get_embedding()\n\ndef get_y(file_path):\n    y_value = file_path.split('_')\n    y_value = y_value[1].split('.')\n    if y_value[0] == '1':\n       return 0\n    elif y_value[0] == '2':\n         return 1\n    elif y_value[0] == '3':\n          return 2\n    elif y_value[0] == '4':\n          return 3\n    elif y_value[0] == '7':\n          return 4\n    elif y_value[0] == '8':\n          return 5\n    elif y_value[0] == '9':\n          return 6\n    elif y_value[0] == '10':\n          return 7 \n\ndef get_x(file_path):\n    x_value = open(file_path,'r')\n    for line in x_value:\n        x_value = line.replace(\"&lt;br /&gt;&lt;br /&gt;\",\"\") \n        x_value = x_value.lower()\n    x_value = nltk.word_tokenize(x_value.decode('utf-8'))\n    padding = 300 - len(x_value)\n    if padding &gt; 0:\n       p_value = ['pad' for i in range(padding)]\n       x_value = np.concatenate((x_value,p_value))\n    if padding &lt; 0:\n       x_value = x_value[:300]\n    for i in x_value:\n        if ebd.get(i) == None:\n           ebd[i] = [float(np.random.normal(0.0,1.0)) for j in range(50)]\n    x_value = [ebd[value] for value in x_value]\n    assert len(x_value) == 300\n    return x_value\n\n\ndef get_total_files(path1,path2,path3,path4):\n    directory1 = os.listdir(path1)\n    file_path1 = [os.path.join(path1,file) for file in directory1]\n    directory2 = os.listdir(path2)\n    file_path2 = [os.path.join(path2,file) for file in directory2]\n    directory3 = os.listdir(path3)\n    file_path3 = [os.path.join(path3,file) for file in directory3]\n    directory4 = os.listdir(path4)\n    file_path4 = [os.path.join(path4,file) for file in directory4]\n    total_files_train = np.concatenate((file_path1,file_path2))\n    total_files_test = np.concatenate((file_path3,file_path4))\n    random.shuffle(total_files_train)\n    random.shuffle(total_files_test)    \n    x1 = [get_x(file) for file in total_files_train]\n    y1 = [get_y(file) for file in total_files_train]\n    x2 = [get_x(file) for file in total_files_test]\n    y2 = [get_y(file) for file in total_files_test]\n    return x1 , y1 , x2 , y2\n\ntotal_files_train_x, total_files_train_y, total_files_test_x, total_files_test_y = get_total_files(path1,path2,path3,path4)\n\n\ntrain_set_x = total_files_train_x[:10000]\nvalidate_set_x = total_files_train_x[10000:15000]\ntest_set_x = total_files_test_x[0:5000]\ntrain_set_y = total_files_train_y[:10000]\nvalidate_set_y = total_files_train_y[10000:15000]\ntest_set_y = total_files_test_y[0:5000]\n\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\ndef build_nlp_model(x, _units,num_classes,num_of_filters):\n    x = tf.expand_dims(x,3)\n    with tf.variable_scope(\"one\"):      \n         filter_shape = [1, embedding, 1, num_of_filters]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\n         conv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"two\"):         \n         filter_shape = [1, 1, 1, 1000]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[1000]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"three\"):        \n         filter_shape = [1, 1, 1, 1000]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[1000]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"four\"):         \n         filter_shape = [1, 1, 1, num_of_filters]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"five\"):         \n         filter_shape = [1, 1, 1, num_of_filters]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    x = tf.squeeze(outputs_fed_lstm, [2])     \n    x = tf.transpose(x, [1, 0, 2])\n    x = tf.reshape(x, [-1, 1])\n    x = tf.split(0, time_steps, x)\n\n    lstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n     # multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\n    outputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\n    weights = tf.Variable(tf.random_normal([_units,num_classes]))\n    biases  = tf.Variable(tf.random_normal([num_classes]))\n\n    logits = tf.matmul(outputs[-1], weights) + biases\n    return logits\n\nlogits = build_nlp_model(X,500,num_classes,1000)\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \nwith tf.variable_scope(\"four\", reuse = True):\n     weights = tf.get_variable(\"conv_weights\") \n     grads_and_vars = optimizer.compute_gradients(loss,[weights]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(10):\n         for j in range(100):\n             x = train_set_x[start:end]\n             y = train_set_y[start:end]\n             start = end\n             end += batch_size\n             if start &gt;= 10000:\n                start = 0\n                end = batch_size  \n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             print (gr_print)\n         print (\"One Epoch Finished\")\n         cost = sess.run(loss,feed_dict = {X: x,Y: y})\n         accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n         print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n         q = validate_set_x[:100]\n         w = validate_set_y[:100]\n         cost = sess.run(loss,feed_dict = {X: q,Y: w})\n         accu = sess.run(accuracy,feed_dict = {X: q, Y: w})\n</code></pre>\n<p>My loss remains the same after many Epochs. So I think that I'm having vanishing gradient problem and so I applied batch normalization but I got no difference in results.I also tried overfitting the model, but I'm getting same results. I'm using <code>optimizer.compute_gradients</code> for computing gradients. Below are the results of gradients of loss with respect to different conv layers, and how they look like. Here is how my gradients look like with respect to first conv layers and with respect to 4th conv layer.</p>\n<p>Code for gradients with respect to first conv layer:</p>\n<pre><code>with tf.variable_scope(\"one\", reuse = True):\n     weights = tf.get_variable(\"conv_weights\") \n     grads_and_vars = optimizer.compute_gradients(loss,[weights])\n\n\ngr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n           print (gr_print)\n</code></pre>\n<p>And this is what I get after one iteration:</p>\n<pre><code>[array([[[[  2.38197345e-06,  -1.04135906e-04,   2.60035231e-05, ...,\n           -1.01550373e-04,   0.00000000e+00,   1.01060732e-06]],\n\n        [[ -1.98007251e-06,   8.13827137e-05,  -8.14055747e-05, ...,\n           -6.40711369e-05,   0.00000000e+00,   1.05516607e-04]],\n\n        [[  4.51127789e-06,   2.21654373e-05,  -4.99439229e-05, ...,\n            9.87191743e-05,   0.00000000e+00,   1.70595697e-04]],\n\n        ..., \n        [[ -4.70160239e-06,  -8.67914496e-05,   2.50699850e-05, ...,\n            1.18909593e-04,   0.00000000e+00,   2.43308150e-05]],\n\n        [[ -1.18101923e-06,  -7.71943451e-05,  -3.41630148e-05, ...,\n           -3.28040805e-05,   0.00000000e+00,  -6.01144784e-05]],\n\n        [[ -1.98778321e-06,  -3.23160748e-05,  -5.44797731e-05, ...,\n            2.23019324e-05,   0.00000000e+00,  -3.29296927e-05]]]], dtype=float32)]\n</code></pre>\n<p>Code for gradients with respect to 4th conv layer:</p>\n<pre><code>with tf.variable_scope(\"four\", reuse = True):\n     weights = tf.get_variable(\"conv_weights\") \n     grads_and_vars = optimizer.compute_gradients(loss,[weights])\ngr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n           print (gr_print)\n</code></pre>\n<p>And this what I get after one iteration:</p>\n<pre><code>[array([[[[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        , -6.21198082,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.                ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.              ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]]], dtype=float32)]\n</code></pre>\n<p>After first layer, gradients with respect to 2nd,3rd,4th,5th conv layers all look like above. But there's one thing common among all the gradients with respect to conv layers which are after first conv layer, they all have one number in the entire gradient array,that is not zero as shown above in the output. And I also applied batch norm and I'm still getting the above results.</p>\n<p>I'm totally confused, I don't know where the problem is?</p>", "body_text": "Tensorflow loss is not changing. This is my code.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nimport tflearn\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.01\nnum_classes = 8\npath1 = \"/home/indy/Downloads/aclImdb/train/pos\"\npath2 = \"/home/indy/Downloads/aclImdb/train/neg\"\npath3 = \"/home/indy/Downloads/aclImdb/test/pos\"\npath4 = \"/home/indy/Downloads/aclImdb/test/neg\"\ntime_steps = 300\nembedding = 50\nstep = 1\n\n\ndef get_embedding():\n    gfile_path = os.path.join(\"/home/indy/Downloads/glove.6B\", \"glove.6B.50d.txt\")\n    f = open(gfile_path,'r')\n    embeddings = {}\n    for line in f:\n        sp_value = line.split()\n        word = sp_value[0]\n        embedding = [float(value) for value in sp_value[1:]]\n        assert len(embedding) == 50\n        embeddings[word] = embedding\n    return embeddings\n\nebd = get_embedding()\n\ndef get_y(file_path):\n    y_value = file_path.split('_')\n    y_value = y_value[1].split('.')\n    if y_value[0] == '1':\n       return 0\n    elif y_value[0] == '2':\n         return 1\n    elif y_value[0] == '3':\n          return 2\n    elif y_value[0] == '4':\n          return 3\n    elif y_value[0] == '7':\n          return 4\n    elif y_value[0] == '8':\n          return 5\n    elif y_value[0] == '9':\n          return 6\n    elif y_value[0] == '10':\n          return 7 \n\ndef get_x(file_path):\n    x_value = open(file_path,'r')\n    for line in x_value:\n        x_value = line.replace(\"<br /><br />\",\"\") \n        x_value = x_value.lower()\n    x_value = nltk.word_tokenize(x_value.decode('utf-8'))\n    padding = 300 - len(x_value)\n    if padding > 0:\n       p_value = ['pad' for i in range(padding)]\n       x_value = np.concatenate((x_value,p_value))\n    if padding < 0:\n       x_value = x_value[:300]\n    for i in x_value:\n        if ebd.get(i) == None:\n           ebd[i] = [float(np.random.normal(0.0,1.0)) for j in range(50)]\n    x_value = [ebd[value] for value in x_value]\n    assert len(x_value) == 300\n    return x_value\n\n\ndef get_total_files(path1,path2,path3,path4):\n    directory1 = os.listdir(path1)\n    file_path1 = [os.path.join(path1,file) for file in directory1]\n    directory2 = os.listdir(path2)\n    file_path2 = [os.path.join(path2,file) for file in directory2]\n    directory3 = os.listdir(path3)\n    file_path3 = [os.path.join(path3,file) for file in directory3]\n    directory4 = os.listdir(path4)\n    file_path4 = [os.path.join(path4,file) for file in directory4]\n    total_files_train = np.concatenate((file_path1,file_path2))\n    total_files_test = np.concatenate((file_path3,file_path4))\n    random.shuffle(total_files_train)\n    random.shuffle(total_files_test)    \n    x1 = [get_x(file) for file in total_files_train]\n    y1 = [get_y(file) for file in total_files_train]\n    x2 = [get_x(file) for file in total_files_test]\n    y2 = [get_y(file) for file in total_files_test]\n    return x1 , y1 , x2 , y2\n\ntotal_files_train_x, total_files_train_y, total_files_test_x, total_files_test_y = get_total_files(path1,path2,path3,path4)\n\n\ntrain_set_x = total_files_train_x[:10000]\nvalidate_set_x = total_files_train_x[10000:15000]\ntest_set_x = total_files_test_x[0:5000]\ntrain_set_y = total_files_train_y[:10000]\nvalidate_set_y = total_files_train_y[10000:15000]\ntest_set_y = total_files_test_y[0:5000]\n\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\ndef build_nlp_model(x, _units,num_classes,num_of_filters):\n    x = tf.expand_dims(x,3)\n    with tf.variable_scope(\"one\"):      \n         filter_shape = [1, embedding, 1, num_of_filters]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\n         conv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"two\"):         \n         filter_shape = [1, 1, 1, 1000]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[1000]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"three\"):        \n         filter_shape = [1, 1, 1, 1000]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[1000]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"four\"):         \n         filter_shape = [1, 1, 1, num_of_filters]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"five\"):         \n         filter_shape = [1, 1, 1, num_of_filters]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    x = tf.squeeze(outputs_fed_lstm, [2])     \n    x = tf.transpose(x, [1, 0, 2])\n    x = tf.reshape(x, [-1, 1])\n    x = tf.split(0, time_steps, x)\n\n    lstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n     # multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\n    outputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\n    weights = tf.Variable(tf.random_normal([_units,num_classes]))\n    biases  = tf.Variable(tf.random_normal([num_classes]))\n\n    logits = tf.matmul(outputs[-1], weights) + biases\n    return logits\n\nlogits = build_nlp_model(X,500,num_classes,1000)\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \nwith tf.variable_scope(\"four\", reuse = True):\n     weights = tf.get_variable(\"conv_weights\") \n     grads_and_vars = optimizer.compute_gradients(loss,[weights]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(10):\n         for j in range(100):\n             x = train_set_x[start:end]\n             y = train_set_y[start:end]\n             start = end\n             end += batch_size\n             if start >= 10000:\n                start = 0\n                end = batch_size  \n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             print (gr_print)\n         print (\"One Epoch Finished\")\n         cost = sess.run(loss,feed_dict = {X: x,Y: y})\n         accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n         print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n         q = validate_set_x[:100]\n         w = validate_set_y[:100]\n         cost = sess.run(loss,feed_dict = {X: q,Y: w})\n         accu = sess.run(accuracy,feed_dict = {X: q, Y: w})\n\nMy loss remains the same after many Epochs. So I think that I'm having vanishing gradient problem and so I applied batch normalization but I got no difference in results.I also tried overfitting the model, but I'm getting same results. I'm using optimizer.compute_gradients for computing gradients. Below are the results of gradients of loss with respect to different conv layers, and how they look like. Here is how my gradients look like with respect to first conv layers and with respect to 4th conv layer.\nCode for gradients with respect to first conv layer:\nwith tf.variable_scope(\"one\", reuse = True):\n     weights = tf.get_variable(\"conv_weights\") \n     grads_and_vars = optimizer.compute_gradients(loss,[weights])\n\n\ngr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n           print (gr_print)\n\nAnd this is what I get after one iteration:\n[array([[[[  2.38197345e-06,  -1.04135906e-04,   2.60035231e-05, ...,\n           -1.01550373e-04,   0.00000000e+00,   1.01060732e-06]],\n\n        [[ -1.98007251e-06,   8.13827137e-05,  -8.14055747e-05, ...,\n           -6.40711369e-05,   0.00000000e+00,   1.05516607e-04]],\n\n        [[  4.51127789e-06,   2.21654373e-05,  -4.99439229e-05, ...,\n            9.87191743e-05,   0.00000000e+00,   1.70595697e-04]],\n\n        ..., \n        [[ -4.70160239e-06,  -8.67914496e-05,   2.50699850e-05, ...,\n            1.18909593e-04,   0.00000000e+00,   2.43308150e-05]],\n\n        [[ -1.18101923e-06,  -7.71943451e-05,  -3.41630148e-05, ...,\n           -3.28040805e-05,   0.00000000e+00,  -6.01144784e-05]],\n\n        [[ -1.98778321e-06,  -3.23160748e-05,  -5.44797731e-05, ...,\n            2.23019324e-05,   0.00000000e+00,  -3.29296927e-05]]]], dtype=float32)]\n\nCode for gradients with respect to 4th conv layer:\nwith tf.variable_scope(\"four\", reuse = True):\n     weights = tf.get_variable(\"conv_weights\") \n     grads_and_vars = optimizer.compute_gradients(loss,[weights])\ngr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n           print (gr_print)\n\nAnd this what I get after one iteration:\n[array([[[[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        , -6.21198082,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.                ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.              ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]]], dtype=float32)]\n\nAfter first layer, gradients with respect to 2nd,3rd,4th,5th conv layers all look like above. But there's one thing common among all the gradients with respect to conv layers which are after first conv layer, they all have one number in the entire gradient array,that is not zero as shown above in the output. And I also applied batch norm and I'm still getting the above results.\nI'm totally confused, I don't know where the problem is?", "body": "Tensorflow loss is not changing. This is my code.\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nimport tflearn\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.01\nnum_classes = 8\npath1 = \"/home/indy/Downloads/aclImdb/train/pos\"\npath2 = \"/home/indy/Downloads/aclImdb/train/neg\"\npath3 = \"/home/indy/Downloads/aclImdb/test/pos\"\npath4 = \"/home/indy/Downloads/aclImdb/test/neg\"\ntime_steps = 300\nembedding = 50\nstep = 1\n\n\ndef get_embedding():\n    gfile_path = os.path.join(\"/home/indy/Downloads/glove.6B\", \"glove.6B.50d.txt\")\n    f = open(gfile_path,'r')\n    embeddings = {}\n    for line in f:\n        sp_value = line.split()\n        word = sp_value[0]\n        embedding = [float(value) for value in sp_value[1:]]\n        assert len(embedding) == 50\n        embeddings[word] = embedding\n    return embeddings\n\nebd = get_embedding()\n\ndef get_y(file_path):\n    y_value = file_path.split('_')\n    y_value = y_value[1].split('.')\n    if y_value[0] == '1':\n       return 0\n    elif y_value[0] == '2':\n         return 1\n    elif y_value[0] == '3':\n          return 2\n    elif y_value[0] == '4':\n          return 3\n    elif y_value[0] == '7':\n          return 4\n    elif y_value[0] == '8':\n          return 5\n    elif y_value[0] == '9':\n          return 6\n    elif y_value[0] == '10':\n          return 7 \n\ndef get_x(file_path):\n    x_value = open(file_path,'r')\n    for line in x_value:\n        x_value = line.replace(\"<br /><br />\",\"\") \n        x_value = x_value.lower()\n    x_value = nltk.word_tokenize(x_value.decode('utf-8'))\n    padding = 300 - len(x_value)\n    if padding > 0:\n       p_value = ['pad' for i in range(padding)]\n       x_value = np.concatenate((x_value,p_value))\n    if padding < 0:\n       x_value = x_value[:300]\n    for i in x_value:\n        if ebd.get(i) == None:\n           ebd[i] = [float(np.random.normal(0.0,1.0)) for j in range(50)]\n    x_value = [ebd[value] for value in x_value]\n    assert len(x_value) == 300\n    return x_value\n\n\ndef get_total_files(path1,path2,path3,path4):\n    directory1 = os.listdir(path1)\n    file_path1 = [os.path.join(path1,file) for file in directory1]\n    directory2 = os.listdir(path2)\n    file_path2 = [os.path.join(path2,file) for file in directory2]\n    directory3 = os.listdir(path3)\n    file_path3 = [os.path.join(path3,file) for file in directory3]\n    directory4 = os.listdir(path4)\n    file_path4 = [os.path.join(path4,file) for file in directory4]\n    total_files_train = np.concatenate((file_path1,file_path2))\n    total_files_test = np.concatenate((file_path3,file_path4))\n    random.shuffle(total_files_train)\n    random.shuffle(total_files_test)    \n    x1 = [get_x(file) for file in total_files_train]\n    y1 = [get_y(file) for file in total_files_train]\n    x2 = [get_x(file) for file in total_files_test]\n    y2 = [get_y(file) for file in total_files_test]\n    return x1 , y1 , x2 , y2\n\ntotal_files_train_x, total_files_train_y, total_files_test_x, total_files_test_y = get_total_files(path1,path2,path3,path4)\n\n\ntrain_set_x = total_files_train_x[:10000]\nvalidate_set_x = total_files_train_x[10000:15000]\ntest_set_x = total_files_test_x[0:5000]\ntrain_set_y = total_files_train_y[:10000]\nvalidate_set_y = total_files_train_y[10000:15000]\ntest_set_y = total_files_test_y[0:5000]\n\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\ndef build_nlp_model(x, _units,num_classes,num_of_filters):\n    x = tf.expand_dims(x,3)\n    with tf.variable_scope(\"one\"):      \n         filter_shape = [1, embedding, 1, num_of_filters]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\n         conv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"two\"):         \n         filter_shape = [1, 1, 1, 1000]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[1000]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"three\"):        \n         filter_shape = [1, 1, 1, 1000]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[1000]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"four\"):         \n         filter_shape = [1, 1, 1, num_of_filters]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    with tf.variable_scope(\"five\"):         \n         filter_shape = [1, 1, 1, num_of_filters]\n         conv_weights = tf.get_variable(\"conv_weights\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\n         conv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\n         conv = tf.nn.conv2d(outputs_fed_lstm, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\n         normalize = conv + conv_biases\n         tf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\n         relu = tf.nn.elu(tf_normalize)\n         pooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\n         outputs_fed_lstm = pooling\n\n    x = tf.squeeze(outputs_fed_lstm, [2])     \n    x = tf.transpose(x, [1, 0, 2])\n    x = tf.reshape(x, [-1, 1])\n    x = tf.split(0, time_steps, x)\n\n    lstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n     # multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\n    outputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\n    weights = tf.Variable(tf.random_normal([_units,num_classes]))\n    biases  = tf.Variable(tf.random_normal([num_classes]))\n\n    logits = tf.matmul(outputs[-1], weights) + biases\n    return logits\n\nlogits = build_nlp_model(X,500,num_classes,1000)\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \nwith tf.variable_scope(\"four\", reuse = True):\n     weights = tf.get_variable(\"conv_weights\") \n     grads_and_vars = optimizer.compute_gradients(loss,[weights]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(10):\n         for j in range(100):\n             x = train_set_x[start:end]\n             y = train_set_y[start:end]\n             start = end\n             end += batch_size\n             if start >= 10000:\n                start = 0\n                end = batch_size  \n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             print (gr_print)\n         print (\"One Epoch Finished\")\n         cost = sess.run(loss,feed_dict = {X: x,Y: y})\n         accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n         print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n         q = validate_set_x[:100]\n         w = validate_set_y[:100]\n         cost = sess.run(loss,feed_dict = {X: q,Y: w})\n         accu = sess.run(accuracy,feed_dict = {X: q, Y: w})\n```\n\nMy loss remains the same after many Epochs. So I think that I'm having vanishing gradient problem and so I applied batch normalization but I got no difference in results.I also tried overfitting the model, but I'm getting same results. I'm using `optimizer.compute_gradients` for computing gradients. Below are the results of gradients of loss with respect to different conv layers, and how they look like. Here is how my gradients look like with respect to first conv layers and with respect to 4th conv layer.\n\nCode for gradients with respect to first conv layer:\n\n```\nwith tf.variable_scope(\"one\", reuse = True):\n     weights = tf.get_variable(\"conv_weights\") \n     grads_and_vars = optimizer.compute_gradients(loss,[weights])\n\n\ngr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n           print (gr_print)\n```\n\n And this is what I get after one iteration:\n\n```\n[array([[[[  2.38197345e-06,  -1.04135906e-04,   2.60035231e-05, ...,\n           -1.01550373e-04,   0.00000000e+00,   1.01060732e-06]],\n\n        [[ -1.98007251e-06,   8.13827137e-05,  -8.14055747e-05, ...,\n           -6.40711369e-05,   0.00000000e+00,   1.05516607e-04]],\n\n        [[  4.51127789e-06,   2.21654373e-05,  -4.99439229e-05, ...,\n            9.87191743e-05,   0.00000000e+00,   1.70595697e-04]],\n\n        ..., \n        [[ -4.70160239e-06,  -8.67914496e-05,   2.50699850e-05, ...,\n            1.18909593e-04,   0.00000000e+00,   2.43308150e-05]],\n\n        [[ -1.18101923e-06,  -7.71943451e-05,  -3.41630148e-05, ...,\n           -3.28040805e-05,   0.00000000e+00,  -6.01144784e-05]],\n\n        [[ -1.98778321e-06,  -3.23160748e-05,  -5.44797731e-05, ...,\n            2.23019324e-05,   0.00000000e+00,  -3.29296927e-05]]]], dtype=float32)]\n```\n\nCode for gradients with respect to 4th conv layer:\n\n```\nwith tf.variable_scope(\"four\", reuse = True):\n     weights = tf.get_variable(\"conv_weights\") \n     grads_and_vars = optimizer.compute_gradients(loss,[weights])\ngr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n           print (gr_print)\n```\n\nAnd this what I get after one iteration:\n\n```\n[array([[[[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        , -6.21198082,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.                ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.              ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]]], dtype=float32)]\n```\n\nAfter first layer, gradients with respect to 2nd,3rd,4th,5th conv layers all look like above. But there's one thing common among all the gradients with respect to conv layers which are after first conv layer, they all have one number in the entire gradient array,that is not zero as shown above in the output. And I also applied batch norm and I'm still getting the above results.\n\nI'm totally confused, I don't know where the problem is?\n"}