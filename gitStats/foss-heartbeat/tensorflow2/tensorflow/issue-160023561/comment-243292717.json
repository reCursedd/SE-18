{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243292717", "html_url": "https://github.com/tensorflow/tensorflow/issues/2838#issuecomment-243292717", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2838", "id": 243292717, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzI5MjcxNw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-29T23:52:57Z", "updated_at": "2016-08-29T23:52:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2111293\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/danijar\">@danijar</a>: I recommend against using a non-trainable variable because this is not thread-safe (you can't run multiple inference threads against the same graph).  However, it's not too hard to create some placeholder tensors and wrap them in the necessary tuple type.  Similarly when calling a session run, one can pull out the \"next state\" tuple and store it, feeding it as an input to the next session.run.  This is decidedly more thread-safe than the variable solution (and in fact is zero-copy if you're doing this in a C++ client; though sadly not zero-copy in python since TF runtime must copy feed_dict inputs from python since python does its own memory management)</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6223213\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nitishgupta\">@nitishgupta</a> you can now fetch an arbitrary tuple type in python.  don't think you can feed one though (but that may have changed recently?)  since usually per-step RNN inference is meant done in a C++ client, i don't have any plans to add python sugar for this.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=141456\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wpm\">@wpm</a>  an example of \"my_parser\" is something that reads a serialized <code>SequenceExample</code> via a reader and deserializes it using <code>parse_single_sequence_example</code>.  The <code>parse_single_sequence_example</code> call returns <code>context</code> and <code>sequences</code> dictionaries that exactly match some of the inputs of <code>batch_sequences_with_states</code>.</p>", "body_text": "@danijar: I recommend against using a non-trainable variable because this is not thread-safe (you can't run multiple inference threads against the same graph).  However, it's not too hard to create some placeholder tensors and wrap them in the necessary tuple type.  Similarly when calling a session run, one can pull out the \"next state\" tuple and store it, feeding it as an input to the next session.run.  This is decidedly more thread-safe than the variable solution (and in fact is zero-copy if you're doing this in a C++ client; though sadly not zero-copy in python since TF runtime must copy feed_dict inputs from python since python does its own memory management)\n@nitishgupta you can now fetch an arbitrary tuple type in python.  don't think you can feed one though (but that may have changed recently?)  since usually per-step RNN inference is meant done in a C++ client, i don't have any plans to add python sugar for this.\n@wpm  an example of \"my_parser\" is something that reads a serialized SequenceExample via a reader and deserializes it using parse_single_sequence_example.  The parse_single_sequence_example call returns context and sequences dictionaries that exactly match some of the inputs of batch_sequences_with_states.", "body": "@danijar: I recommend against using a non-trainable variable because this is not thread-safe (you can't run multiple inference threads against the same graph).  However, it's not too hard to create some placeholder tensors and wrap them in the necessary tuple type.  Similarly when calling a session run, one can pull out the \"next state\" tuple and store it, feeding it as an input to the next session.run.  This is decidedly more thread-safe than the variable solution (and in fact is zero-copy if you're doing this in a C++ client; though sadly not zero-copy in python since TF runtime must copy feed_dict inputs from python since python does its own memory management)\n\n@nitishgupta you can now fetch an arbitrary tuple type in python.  don't think you can feed one though (but that may have changed recently?)  since usually per-step RNN inference is meant done in a C++ client, i don't have any plans to add python sugar for this.\n\n@wpm  an example of \"my_parser\" is something that reads a serialized `SequenceExample` via a reader and deserializes it using `parse_single_sequence_example`.  The `parse_single_sequence_example` call returns `context` and `sequences` dictionaries that exactly match some of the inputs of `batch_sequences_with_states`.\n"}