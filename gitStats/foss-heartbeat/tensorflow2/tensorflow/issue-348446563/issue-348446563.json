{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21454", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21454/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21454/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21454/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21454", "id": 348446563, "node_id": "MDU6SXNzdWUzNDg0NDY1NjM=", "number": 21454, "title": "Turning off Teacher Forcing in decoders of Seq2Seq models ", "user": {"login": "malsulaimi", "id": 31324944, "node_id": "MDQ6VXNlcjMxMzI0OTQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/31324944?v=4", "gravatar_id": "", "url": "https://api.github.com/users/malsulaimi", "html_url": "https://github.com/malsulaimi", "followers_url": "https://api.github.com/users/malsulaimi/followers", "following_url": "https://api.github.com/users/malsulaimi/following{/other_user}", "gists_url": "https://api.github.com/users/malsulaimi/gists{/gist_id}", "starred_url": "https://api.github.com/users/malsulaimi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/malsulaimi/subscriptions", "organizations_url": "https://api.github.com/users/malsulaimi/orgs", "repos_url": "https://api.github.com/users/malsulaimi/repos", "events_url": "https://api.github.com/users/malsulaimi/events{/privacy}", "received_events_url": "https://api.github.com/users/malsulaimi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-08-07T18:58:00Z", "updated_at": "2018-11-14T19:24:47Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Have I written custom code:N/A<br>\nOS Platform and Distribution :macOS Sierra 10.12.6<br>\nTensorFlow installed from : tensforflow.org<br>\nTensorFlow version: TensorFlow Version: 1.3.0<br>\nBazel version: NA<br>\nCUDA/cuDNN version:NA<br>\nGPU model and memory:NA<br>\nExact command to reproduce:NA<br>\nMobile device:NA</p>\n<p>Hello ,</p>\n<p>I'm posting this request here however I'm not sure if this is a new feature request or it is something already doable in tensorflow but I can not find find an example or any documentation about it  .</p>\n<p>Geting to the Point . I'm experimenting with  seq2seq models . I have followed all the examples available and all is good. Now my model uses Teacher forcing ( passing the true output to the decoder network during training ) and nI  would like to turn it off to see how the model performs without it . unfortunately I do not see any possible way to do it without making my own Helper . I tried to use the inferenceHelper during the Training instead of the TraningHelper. Since the inferenceHelper does not require the true output but the model gives a run time error after some epochs where inferenceHelper is returning predictions with shape other than the expected. my guess is that I inferenceHelper is not meant to be used during training ( as the name suggests) .</p>\n<p>is there a way to turn off the Teacher Forcing , if not are there any plans to incorporate this in Tensorflow ?</p>\n<p>my original decoder network code :</p>\n<pre><code>`def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n                     target_sequence_length, max_summary_length, \n                     output_layer, keep_prob):\n\"\"\"\nCreate a decoding layer for training\n:param encoder_state: Encoder State\n:param dec_cell: Decoder RNN Cell\n:param dec_embed_input: Decoder embedded input\n:param target_sequence_length: The lengths of each sequence in the target batch\n:param max_summary_length: The length of the longest sequence in the batch\n:param output_layer: Function to apply the output layer\n:param keep_prob: Dropout keep probability\n:return: BasicDecoderOutput containing training logits and sample_id\n\"\"\"\n\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n                                                    sequence_length=target_sequence_length,\n                                                    time_major=False)\n\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, encoder_state, output_layer)\n\ntraining_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n                                                            impute_finished=True,\n                                                            maximum_iterations=max_summary_length)[0]\nreturn training_decoder_output\n</code></pre>\n<p>thank you</p>", "body_text": "Have I written custom code:N/A\nOS Platform and Distribution :macOS Sierra 10.12.6\nTensorFlow installed from : tensforflow.org\nTensorFlow version: TensorFlow Version: 1.3.0\nBazel version: NA\nCUDA/cuDNN version:NA\nGPU model and memory:NA\nExact command to reproduce:NA\nMobile device:NA\nHello ,\nI'm posting this request here however I'm not sure if this is a new feature request or it is something already doable in tensorflow but I can not find find an example or any documentation about it  .\nGeting to the Point . I'm experimenting with  seq2seq models . I have followed all the examples available and all is good. Now my model uses Teacher forcing ( passing the true output to the decoder network during training ) and nI  would like to turn it off to see how the model performs without it . unfortunately I do not see any possible way to do it without making my own Helper . I tried to use the inferenceHelper during the Training instead of the TraningHelper. Since the inferenceHelper does not require the true output but the model gives a run time error after some epochs where inferenceHelper is returning predictions with shape other than the expected. my guess is that I inferenceHelper is not meant to be used during training ( as the name suggests) .\nis there a way to turn off the Teacher Forcing , if not are there any plans to incorporate this in Tensorflow ?\nmy original decoder network code :\n`def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n                     target_sequence_length, max_summary_length, \n                     output_layer, keep_prob):\n\"\"\"\nCreate a decoding layer for training\n:param encoder_state: Encoder State\n:param dec_cell: Decoder RNN Cell\n:param dec_embed_input: Decoder embedded input\n:param target_sequence_length: The lengths of each sequence in the target batch\n:param max_summary_length: The length of the longest sequence in the batch\n:param output_layer: Function to apply the output layer\n:param keep_prob: Dropout keep probability\n:return: BasicDecoderOutput containing training logits and sample_id\n\"\"\"\n\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n                                                    sequence_length=target_sequence_length,\n                                                    time_major=False)\n\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, encoder_state, output_layer)\n\ntraining_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n                                                            impute_finished=True,\n                                                            maximum_iterations=max_summary_length)[0]\nreturn training_decoder_output\n\nthank you", "body": "Have I written custom code:N/A\r\nOS Platform and Distribution :macOS Sierra 10.12.6\r\nTensorFlow installed from : tensforflow.org\r\nTensorFlow version: TensorFlow Version: 1.3.0\r\nBazel version: NA\r\nCUDA/cuDNN version:NA\r\nGPU model and memory:NA\r\nExact command to reproduce:NA\r\nMobile device:NA\r\n\r\n\r\nHello , \r\n\r\nI'm posting this request here however I'm not sure if this is a new feature request or it is something already doable in tensorflow but I can not find find an example or any documentation about it  . \r\n\r\nGeting to the Point . I'm experimenting with  seq2seq models . I have followed all the examples available and all is good. Now my model uses Teacher forcing ( passing the true output to the decoder network during training ) and nI  would like to turn it off to see how the model performs without it . unfortunately I do not see any possible way to do it without making my own Helper . I tried to use the inferenceHelper during the Training instead of the TraningHelper. Since the inferenceHelper does not require the true output but the model gives a run time error after some epochs where inferenceHelper is returning predictions with shape other than the expected. my guess is that I inferenceHelper is not meant to be used during training ( as the name suggests) . \r\n\r\nis there a way to turn off the Teacher Forcing , if not are there any plans to incorporate this in Tensorflow ? \r\n\r\nmy original decoder network code : \r\n\r\n```\r\n`def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \r\n                     target_sequence_length, max_summary_length, \r\n                     output_layer, keep_prob):\r\n\"\"\"\r\nCreate a decoding layer for training\r\n:param encoder_state: Encoder State\r\n:param dec_cell: Decoder RNN Cell\r\n:param dec_embed_input: Decoder embedded input\r\n:param target_sequence_length: The lengths of each sequence in the target batch\r\n:param max_summary_length: The length of the longest sequence in the batch\r\n:param output_layer: Function to apply the output layer\r\n:param keep_prob: Dropout keep probability\r\n:return: BasicDecoderOutput containing training logits and sample_id\r\n\"\"\"\r\n\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\r\n                                                    sequence_length=target_sequence_length,\r\n                                                    time_major=False)\r\n\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, encoder_state, output_layer)\r\n\r\ntraining_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\r\n                                                            impute_finished=True,\r\n                                                            maximum_iterations=max_summary_length)[0]\r\nreturn training_decoder_output\r\n````\r\n\r\n\r\n\r\nthank you "}