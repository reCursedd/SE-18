{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11092", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11092/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11092/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11092/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11092", "id": 238999866, "node_id": "MDU6SXNzdWUyMzg5OTk4NjY=", "number": 11092, "title": "conv2d on CPU does not pass numerical gradient check, possibly because the forward has an offset but the backward not when padding values are negative.", "user": {"login": "zhisbug", "id": 1654062, "node_id": "MDQ6VXNlcjE2NTQwNjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1654062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhisbug", "html_url": "https://github.com/zhisbug", "followers_url": "https://api.github.com/users/zhisbug/followers", "following_url": "https://api.github.com/users/zhisbug/following{/other_user}", "gists_url": "https://api.github.com/users/zhisbug/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhisbug/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhisbug/subscriptions", "organizations_url": "https://api.github.com/users/zhisbug/orgs", "repos_url": "https://api.github.com/users/zhisbug/repos", "events_url": "https://api.github.com/users/zhisbug/events{/privacy}", "received_events_url": "https://api.github.com/users/zhisbug/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2017-06-27T22:19:39Z", "updated_at": "2018-09-24T16:24:00Z", "closed_at": "2018-09-24T16:24:00Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.2.0-rc2-21-g12f033d', '1.2.0')</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>conv2d (CPU version) gradient function does not pass my gradient check tests when performing 'SAME' convolution in some special cases.<br>\nSee my code below for details.</p>\n<p>TensorFlow uses eigen_spatial_convolution.h and eigen_backward_spatial_convolution.h for performaing conv2d on CPU.<br>\nThe possible reason is that in this case the padding will be negative. During forward the eigen function SpatialConvolution will apply this negative padding as an offset. However, in backward, it does not apply this offset -- the forward and backward are inconsistent.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\nbH <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span> \nbW <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span> \nH <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> \nW <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>    \nin_c <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> \nout_c <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span> \n\nstride <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span> \nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> \nbatch_data  <span class=\"pl-k\">=</span> np.ones([batch_size, bH, bW, in_c])\n<span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_size):\n  <span class=\"pl-k\">for</span> c <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(in_c):\n    <span class=\"pl-k\">for</span> h <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(bH):\n      <span class=\"pl-k\">for</span> w <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(bW):\n        batch_data[n, h, w, c] <span class=\"pl-k\">=</span> n<span class=\"pl-k\">*</span><span class=\"pl-c1\">0.001</span> <span class=\"pl-k\">+</span> c<span class=\"pl-k\">*</span><span class=\"pl-c1\">0.002</span> <span class=\"pl-k\">+</span> h<span class=\"pl-k\">*</span><span class=\"pl-c1\">0.003</span> <span class=\"pl-k\">+</span> w<span class=\"pl-k\">*</span><span class=\"pl-c1\">0.004</span>   \n    \nbatch <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">2</span>, bH, bW, <span class=\"pl-c1\">2</span>]) \nf <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [H, W, in_c, out_c])\noutput <span class=\"pl-k\">=</span> tf.nn.conv2d(batch, f, <span class=\"pl-v\">strides</span> <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span>, stride, stride, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\ns <span class=\"pl-k\">=</span> tf.reduce_sum(output)\ngrad_y <span class=\"pl-k\">=</span> tf.gradients(s, f)\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n\nalpha <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5e-4</span> \n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n  sess.run(init)\n  filters <span class=\"pl-k\">=</span> np.ones([H, W, in_c, out_c], <span class=\"pl-v\">dtype</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">float</span>)\n  result, grads <span class=\"pl-k\">=</span> sess.run([s, grad_y], <span class=\"pl-v\">feed_dict</span> <span class=\"pl-k\">=</span> {batch: batch_data, f: filters})\n  <span class=\"pl-c1\">print</span>(result)\n  <span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(out_c):\n    <span class=\"pl-k\">for</span> c <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(in_c):\n      <span class=\"pl-k\">for</span> h <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(H):\n        <span class=\"pl-k\">for</span> w <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(W):\n          old <span class=\"pl-k\">=</span> filters[h, w, c, n]\n          filters[h, w, c, n] <span class=\"pl-k\">=</span> old <span class=\"pl-k\">-</span> alpha\n          [result_left] <span class=\"pl-k\">=</span> sess.run([s], <span class=\"pl-v\">feed_dict</span> <span class=\"pl-k\">=</span> {batch: batch_data, f: filters}) \n          filters[h, w, c, n] <span class=\"pl-k\">=</span> old <span class=\"pl-k\">+</span> alpha\n          [result_right] <span class=\"pl-k\">=</span> sess.run([s], <span class=\"pl-v\">feed_dict</span> <span class=\"pl-k\">=</span> {batch: batch_data, f: filters})\n          filters[h, w, c, n] <span class=\"pl-k\">=</span> old \n          grad_est <span class=\"pl-k\">=</span> (result_right <span class=\"pl-k\">-</span> result_left) <span class=\"pl-k\">/</span> (<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> alpha)\n          grad_act <span class=\"pl-k\">=</span> grads[<span class=\"pl-c1\">0</span>][h, w, c, n]\n          <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>(<span class=\"pl-c1\">%d</span>,<span class=\"pl-c1\">%d</span>,<span class=\"pl-c1\">%d</span>,<span class=\"pl-c1\">%d</span>): <span class=\"pl-c1\">%f</span>, <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (n, c, h, w, grad_act, grad_est))</pre></div>\n<pre><code>2017-06-27 18:16:36.858424: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-27 18:16:36.858460: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-27 18:16:36.858464: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-27 18:16:36.858468: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-27 18:16:36.858472: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n0.576\n(0,0,0,0): 0.001000, 0.014961\n(0,0,0,1): 0.009000, 0.023007\n(0,0,1,0): 0.007000, 0.020981\n(0,0,1,1): 0.015000, 0.028968\n(0,1,0,0): 0.005000, 0.018954\n(0,1,0,1): 0.013000, 0.027061\n(0,1,1,0): 0.011000, 0.024974\n(0,1,1,1): 0.019000, 0.033021\n(1,0,0,0): 0.001000, 0.014961\n(1,0,0,1): 0.009000, 0.023007\n(1,0,1,0): 0.007000, 0.020981\n(1,0,1,1): 0.015000, 0.028968\n(1,1,0,0): 0.005000, 0.018954\n(1,1,0,1): 0.013000, 0.027001\n(1,1,1,0): 0.011000, 0.024974\n(1,1,1,1): 0.019000, 0.033021\n(2,0,0,0): 0.001000, 0.014961\n(2,0,0,1): 0.009000, 0.023007\n(2,0,1,0): 0.007000, 0.020981\n(2,0,1,1): 0.015000, 0.028968\n(2,1,0,0): 0.005000, 0.018954\n(2,1,0,1): 0.013000, 0.027001\n(2,1,1,0): 0.011000, 0.024974\n(2,1,1,1): 0.019000, 0.033021\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): ('v1.2.0-rc2-21-g12f033d', '1.2.0')\nBazel version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nconv2d (CPU version) gradient function does not pass my gradient check tests when performing 'SAME' convolution in some special cases.\nSee my code below for details.\nTensorFlow uses eigen_spatial_convolution.h and eigen_backward_spatial_convolution.h for performaing conv2d on CPU.\nThe possible reason is that in this case the padding will be negative. During forward the eigen function SpatialConvolution will apply this negative padding as an offset. However, in backward, it does not apply this offset -- the forward and backward are inconsistent.\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\nbH = 4 \nbW = 4 \nH = 2 \nW = 2    \nin_c = 2 \nout_c = 3 \n\nstride = 4 \nbatch_size = 2 \nbatch_data  = np.ones([batch_size, bH, bW, in_c])\nfor n in range(batch_size):\n  for c in range(in_c):\n    for h in range(bH):\n      for w in range(bW):\n        batch_data[n, h, w, c] = n*0.001 + c*0.002 + h*0.003 + w*0.004   \n    \nbatch = tf.placeholder(tf.float32, [2, bH, bW, 2]) \nf = tf.placeholder(tf.float32, [H, W, in_c, out_c])\noutput = tf.nn.conv2d(batch, f, strides = [1, stride, stride, 1], padding = 'SAME')\ns = tf.reduce_sum(output)\ngrad_y = tf.gradients(s, f)\ninit = tf.global_variables_initializer()\n\nalpha = 5e-4 \nwith tf.Session() as sess:\n  sess.run(init)\n  filters = np.ones([H, W, in_c, out_c], dtype = float)\n  result, grads = sess.run([s, grad_y], feed_dict = {batch: batch_data, f: filters})\n  print(result)\n  for n in range(out_c):\n    for c in range(in_c):\n      for h in range(H):\n        for w in range(W):\n          old = filters[h, w, c, n]\n          filters[h, w, c, n] = old - alpha\n          [result_left] = sess.run([s], feed_dict = {batch: batch_data, f: filters}) \n          filters[h, w, c, n] = old + alpha\n          [result_right] = sess.run([s], feed_dict = {batch: batch_data, f: filters})\n          filters[h, w, c, n] = old \n          grad_est = (result_right - result_left) / (2 * alpha)\n          grad_act = grads[0][h, w, c, n]\n          print(\"(%d,%d,%d,%d): %f, %f\" % (n, c, h, w, grad_act, grad_est))\n2017-06-27 18:16:36.858424: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-27 18:16:36.858460: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-27 18:16:36.858464: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-27 18:16:36.858468: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-27 18:16:36.858472: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n0.576\n(0,0,0,0): 0.001000, 0.014961\n(0,0,0,1): 0.009000, 0.023007\n(0,0,1,0): 0.007000, 0.020981\n(0,0,1,1): 0.015000, 0.028968\n(0,1,0,0): 0.005000, 0.018954\n(0,1,0,1): 0.013000, 0.027061\n(0,1,1,0): 0.011000, 0.024974\n(0,1,1,1): 0.019000, 0.033021\n(1,0,0,0): 0.001000, 0.014961\n(1,0,0,1): 0.009000, 0.023007\n(1,0,1,0): 0.007000, 0.020981\n(1,0,1,1): 0.015000, 0.028968\n(1,1,0,0): 0.005000, 0.018954\n(1,1,0,1): 0.013000, 0.027001\n(1,1,1,0): 0.011000, 0.024974\n(1,1,1,1): 0.019000, 0.033021\n(2,0,0,0): 0.001000, 0.014961\n(2,0,0,1): 0.009000, 0.023007\n(2,0,1,0): 0.007000, 0.020981\n(2,0,1,1): 0.015000, 0.028968\n(2,1,0,0): 0.005000, 0.018954\n(2,1,0,1): 0.013000, 0.027001\n(2,1,1,0): 0.011000, 0.024974\n(2,1,1,1): 0.019000, 0.033021", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nconv2d (CPU version) gradient function does not pass my gradient check tests when performing 'SAME' convolution in some special cases.\r\nSee my code below for details.\r\n\r\nTensorFlow uses eigen_spatial_convolution.h and eigen_backward_spatial_convolution.h for performaing conv2d on CPU.\r\nThe possible reason is that in this case the padding will be negative. During forward the eigen function SpatialConvolution will apply this negative padding as an offset. However, in backward, it does not apply this offset -- the forward and backward are inconsistent. \r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nbH = 4 \r\nbW = 4 \r\nH = 2 \r\nW = 2    \r\nin_c = 2 \r\nout_c = 3 \r\n\r\nstride = 4 \r\nbatch_size = 2 \r\nbatch_data  = np.ones([batch_size, bH, bW, in_c])\r\nfor n in range(batch_size):\r\n  for c in range(in_c):\r\n    for h in range(bH):\r\n      for w in range(bW):\r\n        batch_data[n, h, w, c] = n*0.001 + c*0.002 + h*0.003 + w*0.004   \r\n    \r\nbatch = tf.placeholder(tf.float32, [2, bH, bW, 2]) \r\nf = tf.placeholder(tf.float32, [H, W, in_c, out_c])\r\noutput = tf.nn.conv2d(batch, f, strides = [1, stride, stride, 1], padding = 'SAME')\r\ns = tf.reduce_sum(output)\r\ngrad_y = tf.gradients(s, f)\r\ninit = tf.global_variables_initializer()\r\n\r\nalpha = 5e-4 \r\nwith tf.Session() as sess:\r\n  sess.run(init)\r\n  filters = np.ones([H, W, in_c, out_c], dtype = float)\r\n  result, grads = sess.run([s, grad_y], feed_dict = {batch: batch_data, f: filters})\r\n  print(result)\r\n  for n in range(out_c):\r\n    for c in range(in_c):\r\n      for h in range(H):\r\n        for w in range(W):\r\n          old = filters[h, w, c, n]\r\n          filters[h, w, c, n] = old - alpha\r\n          [result_left] = sess.run([s], feed_dict = {batch: batch_data, f: filters}) \r\n          filters[h, w, c, n] = old + alpha\r\n          [result_right] = sess.run([s], feed_dict = {batch: batch_data, f: filters})\r\n          filters[h, w, c, n] = old \r\n          grad_est = (result_right - result_left) / (2 * alpha)\r\n          grad_act = grads[0][h, w, c, n]\r\n          print(\"(%d,%d,%d,%d): %f, %f\" % (n, c, h, w, grad_act, grad_est))\r\n```\r\n```\r\n2017-06-27 18:16:36.858424: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-27 18:16:36.858460: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-27 18:16:36.858464: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-27 18:16:36.858468: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-27 18:16:36.858472: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n0.576\r\n(0,0,0,0): 0.001000, 0.014961\r\n(0,0,0,1): 0.009000, 0.023007\r\n(0,0,1,0): 0.007000, 0.020981\r\n(0,0,1,1): 0.015000, 0.028968\r\n(0,1,0,0): 0.005000, 0.018954\r\n(0,1,0,1): 0.013000, 0.027061\r\n(0,1,1,0): 0.011000, 0.024974\r\n(0,1,1,1): 0.019000, 0.033021\r\n(1,0,0,0): 0.001000, 0.014961\r\n(1,0,0,1): 0.009000, 0.023007\r\n(1,0,1,0): 0.007000, 0.020981\r\n(1,0,1,1): 0.015000, 0.028968\r\n(1,1,0,0): 0.005000, 0.018954\r\n(1,1,0,1): 0.013000, 0.027001\r\n(1,1,1,0): 0.011000, 0.024974\r\n(1,1,1,1): 0.019000, 0.033021\r\n(2,0,0,0): 0.001000, 0.014961\r\n(2,0,0,1): 0.009000, 0.023007\r\n(2,0,1,0): 0.007000, 0.020981\r\n(2,0,1,1): 0.015000, 0.028968\r\n(2,1,0,0): 0.005000, 0.018954\r\n(2,1,0,1): 0.013000, 0.027001\r\n(2,1,1,0): 0.011000, 0.024974\r\n(2,1,1,1): 0.019000, 0.033021\r\n```\r\n\r\n"}