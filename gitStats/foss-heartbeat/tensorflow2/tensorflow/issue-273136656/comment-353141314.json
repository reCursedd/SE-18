{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/353141314", "html_url": "https://github.com/tensorflow/tensorflow/issues/14480#issuecomment-353141314", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14480", "id": 353141314, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzE0MTMxNA==", "user": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-20T18:15:08Z", "updated_at": "2017-12-20T18:17:21Z", "author_association": "MEMBER", "body_html": "<p>As per my comment on <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"258314920\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13101\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/13101/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/13101\">#13101</a>, executing <code>tf.data</code> pipelines comes with overhead (moving data between Python and C++, scheduling TensorFlow ops, ...). I agree that the difference in throughput is surprising and will take a closer look at this. Having said that, the <code>tf.data</code> runtime performance is not optimized for this type of pipeline; it is optimized for pipelines that do non-trivial I/O and data transformations which can additionally benefit from parallel processing.</p>\n<p>For instance, if the generator in your example would do work that requires one second of CPU time per element, then executing the non-<code>tf.data</code> pipeline for <code>1000</code> elements would take ~<code>1000</code> seconds irrespective of your hardware. In contrast, you can write a <code>tf.data</code> pipeline that does the work inside of <code>tf.data.Dataset.map</code>, which can be parallelized and on a machine with <code>n</code> CPU cores, the pipeline would take ~<code>1000/n</code> seconds to execute.</p>\n<p>In other words, the objective of <code>tf.data</code> runtime is not to match the performance of sequential execution of Python code on trivial pipelines. Its objective is to efficiently utilize multicore architectures for non-trivial pipelines.</p>", "body_text": "As per my comment on #13101, executing tf.data pipelines comes with overhead (moving data between Python and C++, scheduling TensorFlow ops, ...). I agree that the difference in throughput is surprising and will take a closer look at this. Having said that, the tf.data runtime performance is not optimized for this type of pipeline; it is optimized for pipelines that do non-trivial I/O and data transformations which can additionally benefit from parallel processing.\nFor instance, if the generator in your example would do work that requires one second of CPU time per element, then executing the non-tf.data pipeline for 1000 elements would take ~1000 seconds irrespective of your hardware. In contrast, you can write a tf.data pipeline that does the work inside of tf.data.Dataset.map, which can be parallelized and on a machine with n CPU cores, the pipeline would take ~1000/n seconds to execute.\nIn other words, the objective of tf.data runtime is not to match the performance of sequential execution of Python code on trivial pipelines. Its objective is to efficiently utilize multicore architectures for non-trivial pipelines.", "body": "As per my comment on #13101, executing `tf.data` pipelines comes with overhead (moving data between Python and C++, scheduling TensorFlow ops, ...). I agree that the difference in throughput is surprising and will take a closer look at this. Having said that, the `tf.data` runtime performance is not optimized for this type of pipeline; it is optimized for pipelines that do non-trivial I/O and data transformations which can additionally benefit from parallel processing. \r\n\r\nFor instance, if the generator in your example would do work that requires one second of CPU time per element, then executing the non-`tf.data` pipeline for `1000` elements would take ~`1000` seconds irrespective of your hardware. In contrast, you can write a `tf.data` pipeline that does the work inside of `tf.data.Dataset.map`, which can be parallelized and on a machine with `n` CPU cores, the pipeline would take ~`1000/n` seconds to execute.\r\n\r\nIn other words, the objective of `tf.data` runtime is not to match the performance of sequential execution of Python code on trivial pipelines. Its objective is to efficiently utilize multicore architectures for non-trivial pipelines."}