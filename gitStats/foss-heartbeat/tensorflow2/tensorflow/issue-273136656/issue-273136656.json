{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14480", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14480/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14480/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14480/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14480", "id": 273136656, "node_id": "MDU6SXNzdWUyNzMxMzY2NTY=", "number": 14480, "title": "Dataset API batching is slow for numpy arrays", "user": {"login": "t-fi", "id": 16362866, "node_id": "MDQ6VXNlcjE2MzYyODY2", "avatar_url": "https://avatars2.githubusercontent.com/u/16362866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-fi", "html_url": "https://github.com/t-fi", "followers_url": "https://api.github.com/users/t-fi/followers", "following_url": "https://api.github.com/users/t-fi/following{/other_user}", "gists_url": "https://api.github.com/users/t-fi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-fi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-fi/subscriptions", "organizations_url": "https://api.github.com/users/t-fi/orgs", "repos_url": "https://api.github.com/users/t-fi/repos", "events_url": "https://api.github.com/users/t-fi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-fi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-11-11T10:42:22Z", "updated_at": "2018-01-05T18:01:38Z", "closed_at": "2018-01-05T18:01:38Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.4rc1</li>\n<li><strong>Python version</strong>:<br>\n3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nNot sure, think 0.7?</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\n5.4</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n8/6</li>\n<li><strong>GPU model and memory</strong>:<br>\ngtx980</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nsee attached</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The <code>tf.data.Dataset.batch()</code> functions seems to be slow when concatenating numpy arrays into batches.</p>\n<p>The attached code basically pushes dummy data ([0, 1, 2, ... , NUM_ITEMS] resembling the output of <code>range()</code>) through tensorflow by different approaches.</p>\n<p>There are basically two ways to go about this:</p>\n<ol>\n<li>Use the <code>tf.data.Dataset.range()</code></li>\n<li>Use custom generators and <code>tf.data.Dataset.from_generator()</code></li>\n</ol>\n<p>Also the data can be batched to increase throughput as it is usually done in deep learning. This can be done either in the generator itself or via <code>tf.data.Dataset.batch()</code>. The latter appears to be ~30x slower for numpy arrays. Somehow this is not the case for 'native' tensorflow generation.</p>\n<p>I attached a benchmark to reproduce this. Please note that the behavior for larger amounts of data e.g. images is similar, resulting in only tens of images per second instead of hundreds.</p>\n<h3>Comments to the source code:</h3>\n<p><code>gen()</code> is a python generator resembling <code>range()</code>.<br>\n<code>gen_batch()</code> is also a python generator, but yields batches of 100 numbers instead of single numbers.</p>\n<p>These two generators are benchmarked first, yielding millions of elements per second. They should not be a bottleneck.</p>\n<p>Then we define a few <code>tf.data.Dataset</code>:<br>\n<code>ds_range_single</code> is similar to <code>gen()</code>, using <code>tf.data.Dataset.range()</code><br>\n<code>ds_range_batch</code> is similar to <code>gen_batch()</code>, using <code>tf.data.Dataset.range().batch()</code></p>\n<p><code>ds_npy_single</code> uses <code>gen()</code> with <code>tf.data.Dataset.from_generator()</code><br>\n<code>ds_npy_batch</code> uses <code>gen()</code> with <code>tf.data.Dataset.from_generator().batch()</code><br>\n<code>ds_npy_single_batch</code> uses uses <code>gen_batch()</code> and will perform much better than <code>ds_npy_batch</code>. In fact it will be close to the 'native' <code>ds_range_batch</code>.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> time <span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c1\">NUM_ITEMS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100000</span>\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">gen</span>():\n    <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> np.arange(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">NUM_ITEMS</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int64):\n        <span class=\"pl-k\">yield</span> x\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">gen_batch</span>():\n    <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> np.arange(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">NUM_ITEMS</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int64).reshape(<span class=\"pl-c1\">NUM_ITEMS</span><span class=\"pl-k\">//</span><span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">BATCH_SIZE</span>):\n        <span class=\"pl-k\">yield</span> x\n\nstart <span class=\"pl-k\">=</span> time()\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> gen():\n    <span class=\"pl-k\">pass</span>\npy_single_generator_time <span class=\"pl-k\">=</span> time() <span class=\"pl-k\">-</span> start\n\nstart <span class=\"pl-k\">=</span> time()\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> gen_batch():\n    <span class=\"pl-k\">pass</span>\npy_batch_generator_time <span class=\"pl-k\">=</span> time() <span class=\"pl-k\">-</span> start\n\nds_range_single <span class=\"pl-k\">=</span> tf.data.Dataset.range(<span class=\"pl-c1\">NUM_ITEMS</span>)\\\n    .make_one_shot_iterator().get_next()\n\nds_range_batch <span class=\"pl-k\">=</span> tf.data.Dataset.range(<span class=\"pl-c1\">NUM_ITEMS</span>)\\\n    .batch(<span class=\"pl-c1\">BATCH_SIZE</span>).make_one_shot_iterator().get_next()\n\nds_npy_single <span class=\"pl-k\">=</span> tf.data.Dataset.from_generator(\n    gen, <span class=\"pl-v\">output_types</span><span class=\"pl-k\">=</span>tf.int64, <span class=\"pl-v\">output_shapes</span><span class=\"pl-k\">=</span>tf.TensorShape([]))\\\n    .make_one_shot_iterator().get_next()\n\nds_npy_batch <span class=\"pl-k\">=</span> tf.data.Dataset.from_generator(\n    gen, <span class=\"pl-v\">output_types</span><span class=\"pl-k\">=</span>tf.int64, <span class=\"pl-v\">output_shapes</span><span class=\"pl-k\">=</span>tf.TensorShape([]))\\\n    .batch(<span class=\"pl-c1\">BATCH_SIZE</span>).make_one_shot_iterator().get_next()\n\nds_npy_single_batch <span class=\"pl-k\">=</span> tf.data.Dataset.from_generator(\n    gen_batch, <span class=\"pl-v\">output_types</span><span class=\"pl-k\">=</span>tf.int64, <span class=\"pl-v\">output_shapes</span><span class=\"pl-k\">=</span>tf.TensorShape([<span class=\"pl-c1\">BATCH_SIZE</span>]))\\\n    .make_one_shot_iterator().get_next()\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    start <span class=\"pl-k\">=</span> time()\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">NUM_ITEMS</span>):\n        sess.run(ds_npy_single)\n    npy_single_time <span class=\"pl-k\">=</span> time() <span class=\"pl-k\">-</span> start\n\n    start <span class=\"pl-k\">=</span> time()\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">NUM_ITEMS</span> <span class=\"pl-k\">//</span> <span class=\"pl-c1\">BATCH_SIZE</span>):\n        sess.run(ds_npy_batch)\n    npy_batch_time <span class=\"pl-k\">=</span> time() <span class=\"pl-k\">-</span> start\n\n    start <span class=\"pl-k\">=</span> time()\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">NUM_ITEMS</span> <span class=\"pl-k\">//</span> <span class=\"pl-c1\">BATCH_SIZE</span>):\n        sess.run(ds_npy_single_batch)\n    npy_single_batch_time <span class=\"pl-k\">=</span> time() <span class=\"pl-k\">-</span> start\n\n    start <span class=\"pl-k\">=</span> time()\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">NUM_ITEMS</span>):\n        sess.run(ds_range_single)\n    range_single_time <span class=\"pl-k\">=</span> time() <span class=\"pl-k\">-</span> start\n\n    start <span class=\"pl-k\">=</span> time()\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">NUM_ITEMS</span> <span class=\"pl-k\">//</span> <span class=\"pl-c1\">BATCH_SIZE</span>):\n        sess.run(ds_range_batch)\n    range_batch_time <span class=\"pl-k\">=</span> time() <span class=\"pl-k\">-</span> start\n\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Python single generator examples/s :<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">NUM_ITEMS</span><span class=\"pl-k\">/</span>py_single_generator_time)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Python batch generator examples/s :<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">NUM_ITEMS</span><span class=\"pl-k\">/</span>py_batch_generator_time)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tf npy single examples/s :<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">NUM_ITEMS</span><span class=\"pl-k\">/</span>npy_single_time)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tf npy batch examples/s :<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">NUM_ITEMS</span><span class=\"pl-k\">/</span>npy_batch_time)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tf npy single batch examples/s<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">NUM_ITEMS</span><span class=\"pl-k\">/</span>npy_single_batch_time)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tf range single examples/s :<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">NUM_ITEMS</span><span class=\"pl-k\">/</span>range_single_time)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tf range batch examples/s :<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">NUM_ITEMS</span><span class=\"pl-k\">/</span>range_batch_time)</pre></div>\n<h3>Prints the following on my machine:</h3>\n<p>Python single generator examples/s : 3779128.899140432<br>\nPython batch generator examples/s : 137113566.52500817<br>\ntf npy single examples/s : 2623.528591111384<br>\ntf npy batch examples/s : 10184.987886595052<br>\ntf npy single batch examples/s 308841.62594729604<br>\ntf range single examples/s : 4673.512357184766<br>\ntf range batch examples/s : 352137.31184393575</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 16.04\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\n1.4rc1\nPython version:\n3.5.2\nBazel version (if compiling from source):\nNot sure, think 0.7?\nGCC/Compiler version (if compiling from source):\n5.4\nCUDA/cuDNN version:\n8/6\nGPU model and memory:\ngtx980\nExact command to reproduce:\nsee attached\n\nDescribe the problem\nThe tf.data.Dataset.batch() functions seems to be slow when concatenating numpy arrays into batches.\nThe attached code basically pushes dummy data ([0, 1, 2, ... , NUM_ITEMS] resembling the output of range()) through tensorflow by different approaches.\nThere are basically two ways to go about this:\n\nUse the tf.data.Dataset.range()\nUse custom generators and tf.data.Dataset.from_generator()\n\nAlso the data can be batched to increase throughput as it is usually done in deep learning. This can be done either in the generator itself or via tf.data.Dataset.batch(). The latter appears to be ~30x slower for numpy arrays. Somehow this is not the case for 'native' tensorflow generation.\nI attached a benchmark to reproduce this. Please note that the behavior for larger amounts of data e.g. images is similar, resulting in only tens of images per second instead of hundreds.\nComments to the source code:\ngen() is a python generator resembling range().\ngen_batch() is also a python generator, but yields batches of 100 numbers instead of single numbers.\nThese two generators are benchmarked first, yielding millions of elements per second. They should not be a bottleneck.\nThen we define a few tf.data.Dataset:\nds_range_single is similar to gen(), using tf.data.Dataset.range()\nds_range_batch is similar to gen_batch(), using tf.data.Dataset.range().batch()\nds_npy_single uses gen() with tf.data.Dataset.from_generator()\nds_npy_batch uses gen() with tf.data.Dataset.from_generator().batch()\nds_npy_single_batch uses uses gen_batch() and will perform much better than ds_npy_batch. In fact it will be close to the 'native' ds_range_batch.\nSource code / logs\nfrom time import time\nimport numpy as np\nimport tensorflow as tf\n\nNUM_ITEMS = 100000\nBATCH_SIZE = 100\n\n\ndef gen():\n    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64):\n        yield x\n\n\ndef gen_batch():\n    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64).reshape(NUM_ITEMS//BATCH_SIZE, BATCH_SIZE):\n        yield x\n\nstart = time()\nfor _ in gen():\n    pass\npy_single_generator_time = time() - start\n\nstart = time()\nfor _ in gen_batch():\n    pass\npy_batch_generator_time = time() - start\n\nds_range_single = tf.data.Dataset.range(NUM_ITEMS)\\\n    .make_one_shot_iterator().get_next()\n\nds_range_batch = tf.data.Dataset.range(NUM_ITEMS)\\\n    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()\n\nds_npy_single = tf.data.Dataset.from_generator(\n    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\\\n    .make_one_shot_iterator().get_next()\n\nds_npy_batch = tf.data.Dataset.from_generator(\n    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\\\n    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()\n\nds_npy_single_batch = tf.data.Dataset.from_generator(\n    gen_batch, output_types=tf.int64, output_shapes=tf.TensorShape([BATCH_SIZE]))\\\n    .make_one_shot_iterator().get_next()\n\nwith tf.Session() as sess:\n    start = time()\n    for _ in range(NUM_ITEMS):\n        sess.run(ds_npy_single)\n    npy_single_time = time() - start\n\n    start = time()\n    for _ in range(NUM_ITEMS // BATCH_SIZE):\n        sess.run(ds_npy_batch)\n    npy_batch_time = time() - start\n\n    start = time()\n    for _ in range(NUM_ITEMS // BATCH_SIZE):\n        sess.run(ds_npy_single_batch)\n    npy_single_batch_time = time() - start\n\n    start = time()\n    for _ in range(NUM_ITEMS):\n        sess.run(ds_range_single)\n    range_single_time = time() - start\n\n    start = time()\n    for _ in range(NUM_ITEMS // BATCH_SIZE):\n        sess.run(ds_range_batch)\n    range_batch_time = time() - start\n\n\nprint('Python single generator examples/s :', NUM_ITEMS/py_single_generator_time)\nprint('Python batch generator examples/s :', NUM_ITEMS/py_batch_generator_time)\nprint('tf npy single examples/s :', NUM_ITEMS/npy_single_time)\nprint('tf npy batch examples/s :', NUM_ITEMS/npy_batch_time)\nprint('tf npy single batch examples/s', NUM_ITEMS/npy_single_batch_time)\nprint('tf range single examples/s :', NUM_ITEMS/range_single_time)\nprint('tf range batch examples/s :', NUM_ITEMS/range_batch_time)\nPrints the following on my machine:\nPython single generator examples/s : 3779128.899140432\nPython batch generator examples/s : 137113566.52500817\ntf npy single examples/s : 2623.528591111384\ntf npy batch examples/s : 10184.987886595052\ntf npy single batch examples/s 308841.62594729604\ntf range single examples/s : 4673.512357184766\ntf range batch examples/s : 352137.31184393575", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.4rc1\r\n- **Python version**:\r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nNot sure, think 0.7?\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4\r\n- **CUDA/cuDNN version**:\r\n8/6\r\n- **GPU model and memory**:\r\ngtx980\r\n- **Exact command to reproduce**:\r\nsee attached\r\n\r\n### Describe the problem\r\nThe ```tf.data.Dataset.batch()``` functions seems to be slow when concatenating numpy arrays into batches.\r\n\r\nThe attached code basically pushes dummy data ([0, 1, 2, ... , NUM_ITEMS] resembling the output of ```range()```) through tensorflow by different approaches.\r\n\r\nThere are basically two ways to go about this:\r\n1. Use the ```tf.data.Dataset.range()```\r\n2. Use custom generators and ```tf.data.Dataset.from_generator()```\r\n\r\nAlso the data can be batched to increase throughput as it is usually done in deep learning. This can be done either in the generator itself or via ```tf.data.Dataset.batch()```. The latter appears to be ~30x slower for numpy arrays. Somehow this is not the case for 'native' tensorflow generation.\r\n\r\nI attached a benchmark to reproduce this. Please note that the behavior for larger amounts of data e.g. images is similar, resulting in only tens of images per second instead of hundreds.\r\n\r\n### Comments to the source code:\r\n```gen()``` is a python generator resembling ```range()```.\r\n```gen_batch()``` is also a python generator, but yields batches of 100 numbers instead of single numbers.\r\n\r\nThese two generators are benchmarked first, yielding millions of elements per second. They should not be a bottleneck.\r\n\r\nThen we define a few ```tf.data.Dataset```:\r\n```ds_range_single``` is similar to ```gen()```, using ```tf.data.Dataset.range()```\r\n```ds_range_batch``` is similar to ```gen_batch()```, using ```tf.data.Dataset.range().batch()```\r\n\r\n```ds_npy_single``` uses ```gen()``` with ```tf.data.Dataset.from_generator()```\r\n```ds_npy_batch``` uses ```gen()``` with ```tf.data.Dataset.from_generator().batch()```\r\n```ds_npy_single_batch``` uses uses ```gen_batch()``` and will perform much better than ```ds_npy_batch```. In fact it will be close to the 'native' ```ds_range_batch```.\r\n\r\n### Source code / logs\r\n```python\r\nfrom time import time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nNUM_ITEMS = 100000\r\nBATCH_SIZE = 100\r\n\r\n\r\ndef gen():\r\n    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64):\r\n        yield x\r\n\r\n\r\ndef gen_batch():\r\n    for x in np.arange(0, NUM_ITEMS, 1, dtype=np.int64).reshape(NUM_ITEMS//BATCH_SIZE, BATCH_SIZE):\r\n        yield x\r\n\r\nstart = time()\r\nfor _ in gen():\r\n    pass\r\npy_single_generator_time = time() - start\r\n\r\nstart = time()\r\nfor _ in gen_batch():\r\n    pass\r\npy_batch_generator_time = time() - start\r\n\r\nds_range_single = tf.data.Dataset.range(NUM_ITEMS)\\\r\n    .make_one_shot_iterator().get_next()\r\n\r\nds_range_batch = tf.data.Dataset.range(NUM_ITEMS)\\\r\n    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()\r\n\r\nds_npy_single = tf.data.Dataset.from_generator(\r\n    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\\\r\n    .make_one_shot_iterator().get_next()\r\n\r\nds_npy_batch = tf.data.Dataset.from_generator(\r\n    gen, output_types=tf.int64, output_shapes=tf.TensorShape([]))\\\r\n    .batch(BATCH_SIZE).make_one_shot_iterator().get_next()\r\n\r\nds_npy_single_batch = tf.data.Dataset.from_generator(\r\n    gen_batch, output_types=tf.int64, output_shapes=tf.TensorShape([BATCH_SIZE]))\\\r\n    .make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n    start = time()\r\n    for _ in range(NUM_ITEMS):\r\n        sess.run(ds_npy_single)\r\n    npy_single_time = time() - start\r\n\r\n    start = time()\r\n    for _ in range(NUM_ITEMS // BATCH_SIZE):\r\n        sess.run(ds_npy_batch)\r\n    npy_batch_time = time() - start\r\n\r\n    start = time()\r\n    for _ in range(NUM_ITEMS // BATCH_SIZE):\r\n        sess.run(ds_npy_single_batch)\r\n    npy_single_batch_time = time() - start\r\n\r\n    start = time()\r\n    for _ in range(NUM_ITEMS):\r\n        sess.run(ds_range_single)\r\n    range_single_time = time() - start\r\n\r\n    start = time()\r\n    for _ in range(NUM_ITEMS // BATCH_SIZE):\r\n        sess.run(ds_range_batch)\r\n    range_batch_time = time() - start\r\n\r\n\r\nprint('Python single generator examples/s :', NUM_ITEMS/py_single_generator_time)\r\nprint('Python batch generator examples/s :', NUM_ITEMS/py_batch_generator_time)\r\nprint('tf npy single examples/s :', NUM_ITEMS/npy_single_time)\r\nprint('tf npy batch examples/s :', NUM_ITEMS/npy_batch_time)\r\nprint('tf npy single batch examples/s', NUM_ITEMS/npy_single_batch_time)\r\nprint('tf range single examples/s :', NUM_ITEMS/range_single_time)\r\nprint('tf range batch examples/s :', NUM_ITEMS/range_batch_time)\r\n```\r\n### Prints the following on my machine:\r\nPython single generator examples/s : 3779128.899140432\r\nPython batch generator examples/s : 137113566.52500817\r\ntf npy single examples/s : 2623.528591111384\r\ntf npy batch examples/s : 10184.987886595052\r\ntf npy single batch examples/s 308841.62594729604\r\ntf range single examples/s : 4673.512357184766\r\ntf range batch examples/s : 352137.31184393575"}