{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299915873", "html_url": "https://github.com/tensorflow/tensorflow/issues/8310#issuecomment-299915873", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8310", "id": 299915873, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTkxNTg3Mw==", "user": {"login": "leandro-gracia-gil", "id": 8785797, "node_id": "MDQ6VXNlcjg3ODU3OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8785797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leandro-gracia-gil", "html_url": "https://github.com/leandro-gracia-gil", "followers_url": "https://api.github.com/users/leandro-gracia-gil/followers", "following_url": "https://api.github.com/users/leandro-gracia-gil/following{/other_user}", "gists_url": "https://api.github.com/users/leandro-gracia-gil/gists{/gist_id}", "starred_url": "https://api.github.com/users/leandro-gracia-gil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leandro-gracia-gil/subscriptions", "organizations_url": "https://api.github.com/users/leandro-gracia-gil/orgs", "repos_url": "https://api.github.com/users/leandro-gracia-gil/repos", "events_url": "https://api.github.com/users/leandro-gracia-gil/events{/privacy}", "received_events_url": "https://api.github.com/users/leandro-gracia-gil/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-08T16:21:15Z", "updated_at": "2017-05-09T04:30:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>By the way, I've also noticed that there are a lot of kernels not implemented for the DEVICE_CPU_XLA_JIT device, which limits a lot what you can actually do with the AoT compiler. This seems to include multiple control flow operations (enter, exit, merge, loopcond, switch, nextiteration), TensorArray ops (size, read, gather, scatter) and some ops like argmin (although there's an argmax XLA kernel). You can hit most of these by simply using tf.map_fn in your graph. Kernels for any quantized data types are missing too.</p>\n<p>Are there there any plans about implementing these kernels for the AoT compiler? If I wanted to try implementing some of these myself, is there something I should keep in mind?</p>\n<p>Thanks.</p>", "body_text": "By the way, I've also noticed that there are a lot of kernels not implemented for the DEVICE_CPU_XLA_JIT device, which limits a lot what you can actually do with the AoT compiler. This seems to include multiple control flow operations (enter, exit, merge, loopcond, switch, nextiteration), TensorArray ops (size, read, gather, scatter) and some ops like argmin (although there's an argmax XLA kernel). You can hit most of these by simply using tf.map_fn in your graph. Kernels for any quantized data types are missing too.\nAre there there any plans about implementing these kernels for the AoT compiler? If I wanted to try implementing some of these myself, is there something I should keep in mind?\nThanks.", "body": "By the way, I've also noticed that there are a lot of kernels not implemented for the DEVICE_CPU_XLA_JIT device, which limits a lot what you can actually do with the AoT compiler. This seems to include multiple control flow operations (enter, exit, merge, loopcond, switch, nextiteration), TensorArray ops (size, read, gather, scatter) and some ops like argmin (although there's an argmax XLA kernel). You can hit most of these by simply using tf.map_fn in your graph. Kernels for any quantized data types are missing too.\r\n\r\nAre there there any plans about implementing these kernels for the AoT compiler? If I wanted to try implementing some of these myself, is there something I should keep in mind?\r\n\r\nThanks."}