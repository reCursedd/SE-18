{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/301976439", "html_url": "https://github.com/tensorflow/tensorflow/issues/8310#issuecomment-301976439", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8310", "id": 301976439, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTk3NjQzOQ==", "user": {"login": "leandro-gracia-gil", "id": 8785797, "node_id": "MDQ6VXNlcjg3ODU3OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8785797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leandro-gracia-gil", "html_url": "https://github.com/leandro-gracia-gil", "followers_url": "https://api.github.com/users/leandro-gracia-gil/followers", "following_url": "https://api.github.com/users/leandro-gracia-gil/following{/other_user}", "gists_url": "https://api.github.com/users/leandro-gracia-gil/gists{/gist_id}", "starred_url": "https://api.github.com/users/leandro-gracia-gil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leandro-gracia-gil/subscriptions", "organizations_url": "https://api.github.com/users/leandro-gracia-gil/orgs", "repos_url": "https://api.github.com/users/leandro-gracia-gil/repos", "events_url": "https://api.github.com/users/leandro-gracia-gil/events{/privacy}", "received_events_url": "https://api.github.com/users/leandro-gracia-gil/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-17T03:37:47Z", "updated_at": "2017-05-17T03:37:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for merging the fix! Here are detailed instructions on how to build and run a static library generated by tfcompile in MSVC.</p>\n<ol>\n<li>In a Linux machine (tfcompile doesn't work on Windows yet) checkout tensorflow and build tfcompile. You will need to <a href=\"https://bazel.build/versions/master/docs/install.html\" rel=\"nofollow\">install Bazel</a> if you haven't done so already.</li>\n</ol>\n<blockquote>\n<p>git clone <a href=\"https://github.com/tensorflow/tensorflow.git\">https://github.com/tensorflow/tensorflow.git</a><br>\ncd tensorflow<br>\nbazel build -c opt //tensorflow/compiler/aot:tfcompile</p>\n</blockquote>\n<ol start=\"2\">\n<li>Run tfcompile to generate the static library for your graph and config.</li>\n</ol>\n<blockquote>\n<p>bazel-bin/tensorflow/compiler/aot/tfcompile --target_triple=\"<strong>x86_64-pc-windows-msvc</strong>\" --graph=\"<em>your_graph.pb</em>\" --config=\"<em>your_graph_config.pbtxt</em>\" --entry_point=\"<em>your_C_entry_point_name</em>\" --cpp_class=\"<em>YourNamespace::YourCppClass</em>\" --out_object=\"output.lib\" --out_header=\"output.h\"</p>\n</blockquote>\n<p>You might want to add some other arguments to tfcompile, like for example:</p>\n<blockquote>\n<p>--xla_cpu_llvm_opt_level=3 --target_features=\"+avx,+avx2,+sse4.2\"</p>\n</blockquote>\n<ol start=\"3\">\n<li>\n<p>Copy your generated .lib and .h files to the Windows machine where you run MSVC.</p>\n</li>\n<li>\n<p>In the MSVC solution you want to use the static library, create a new empty Win32 Static Library project.</p>\n</li>\n<li>\n<p>Checkout the Tensorflow source again in Windows (e.g., your_project\\third_party\\tensorflow)</p>\n</li>\n<li>\n<p>Open tensorflow\\workspace.bzl in your tensorflow checkout and look for \"eigen_archive\". Download one of the URLs there and extract the contents into a folder (e.g., your_project\\third_party\\eigen).</p>\n</li>\n<li>\n<p>Open tensorflow\\contrib\\cmake\\CMakeLists.txt in your tensorflow checkout and look for \"add_definitions\". Take a look to the preprocessor macros defined there (starting with -D) and add them manually in MSVC to your Tensorflow project properties -&gt; C/C++ -&gt; Preprocessor. You will need to define at least NOMINMAX, COMPILER_MSVC and TF_COMPILE_LIBRARY. Similarly, you might want to add some other compiler options from CMakeLists.txt into C/C++ -&gt; Command Line -&gt; Additional Options. None of these should be a strong requirement AFAIK.</p>\n</li>\n<li>\n<p>In your Tensorflow project properties -&gt; C/C++ -&gt; General -&gt; Additional Include Directories add the following entries in this order:</p>\n</li>\n</ol>\n<ul>\n<li>If needed, the path where your generated .h file is.</li>\n<li>The path where you extracted eigen (e.g., your_project\\third_party\\eigen).</li>\n<li>The path where you extracted tensorflow (e.g., your_project\\third_party\\tensorflow).</li>\n</ul>\n<ol start=\"9\">\n<li>\n<p>In your Tensorflow project properties -&gt; Librarian, add your generated .lib to Additional Dependencies, and the path to it in Additional Library Directories.</p>\n</li>\n<li>\n<p>Manually add the following files to the \"Source Files\" section of your Tensorflow project:</p>\n</li>\n</ol>\n<ul>\n<li>tensorflow/compiler/aot/runtime.cc</li>\n<li>tensorflow/compiler/xla/executable_run_options.cc</li>\n<li>tensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int32.cc</li>\n<li>tensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int64.cc</li>\n<li>tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_1d.cc</li>\n<li>tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_2d.cc</li>\n<li>tensorflow/compiler/xla/service/cpu/runtime_matmul.cc</li>\n<li>tensorflow/compiler/xla/service/cpu/runtime_single_threaded_matmul.cc</li>\n<li>tensorflow/compiler/xla/service/cpu/runtime_conv2d.cc</li>\n<li>tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc</li>\n<li>tensorflow/compiler/xla/service/cpu/cpu_runtime.cc</li>\n</ul>\n<ol start=\"11\">\n<li>\n<p>Set your project dependencies in your solution so that the projects you want to use the static library in depend on your Tensorflow project. (Right-click in your project -&gt; Build Dependencies -&gt; Project dependencies).</p>\n</li>\n<li>\n<p>You're ready to use the generated static library in your code. Here's an example snippet:</p>\n</li>\n</ol>\n<pre><code>#define EIGEN_USE_THREADS\n#define EIGEN_USE_CUSTOM_THREAD_POOL\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"  // From your Tensorflow checkout.\n#include \"your_generated_header.h\"\n\neigen::ThreadPool thread_pool(2);  // Number of threads in your pool. Set as you want.\neigen::ThreadPoolDevice thread_pool_device(&amp;thread_pool, thread_pool.NumThreads());\nYourNamespace::YourCppClass helper_object;\n\nhelper_object.set_thread_pool(&amp;thread_pool_device);\nhelper_object.set_arg0_data(your_arg_data);\nhelper_object.Run();\nauto your_result = helper_object.result0();\n</code></pre>\n<p>Keep in mind that if you enable vector instructions (by passing args like +sse4.2, +avx or +avx2 to target_features when running tfcompile) you should ensure that any pointers passed to set_arg*_data(...) have the appropriate memory alignment. Otherwise you are likely to experience crashes when calling Run().</p>\n<p>Enjoy!</p>", "body_text": "Thanks for merging the fix! Here are detailed instructions on how to build and run a static library generated by tfcompile in MSVC.\n\nIn a Linux machine (tfcompile doesn't work on Windows yet) checkout tensorflow and build tfcompile. You will need to install Bazel if you haven't done so already.\n\n\ngit clone https://github.com/tensorflow/tensorflow.git\ncd tensorflow\nbazel build -c opt //tensorflow/compiler/aot:tfcompile\n\n\nRun tfcompile to generate the static library for your graph and config.\n\n\nbazel-bin/tensorflow/compiler/aot/tfcompile --target_triple=\"x86_64-pc-windows-msvc\" --graph=\"your_graph.pb\" --config=\"your_graph_config.pbtxt\" --entry_point=\"your_C_entry_point_name\" --cpp_class=\"YourNamespace::YourCppClass\" --out_object=\"output.lib\" --out_header=\"output.h\"\n\nYou might want to add some other arguments to tfcompile, like for example:\n\n--xla_cpu_llvm_opt_level=3 --target_features=\"+avx,+avx2,+sse4.2\"\n\n\n\nCopy your generated .lib and .h files to the Windows machine where you run MSVC.\n\n\nIn the MSVC solution you want to use the static library, create a new empty Win32 Static Library project.\n\n\nCheckout the Tensorflow source again in Windows (e.g., your_project\\third_party\\tensorflow)\n\n\nOpen tensorflow\\workspace.bzl in your tensorflow checkout and look for \"eigen_archive\". Download one of the URLs there and extract the contents into a folder (e.g., your_project\\third_party\\eigen).\n\n\nOpen tensorflow\\contrib\\cmake\\CMakeLists.txt in your tensorflow checkout and look for \"add_definitions\". Take a look to the preprocessor macros defined there (starting with -D) and add them manually in MSVC to your Tensorflow project properties -> C/C++ -> Preprocessor. You will need to define at least NOMINMAX, COMPILER_MSVC and TF_COMPILE_LIBRARY. Similarly, you might want to add some other compiler options from CMakeLists.txt into C/C++ -> Command Line -> Additional Options. None of these should be a strong requirement AFAIK.\n\n\nIn your Tensorflow project properties -> C/C++ -> General -> Additional Include Directories add the following entries in this order:\n\n\n\nIf needed, the path where your generated .h file is.\nThe path where you extracted eigen (e.g., your_project\\third_party\\eigen).\nThe path where you extracted tensorflow (e.g., your_project\\third_party\\tensorflow).\n\n\n\nIn your Tensorflow project properties -> Librarian, add your generated .lib to Additional Dependencies, and the path to it in Additional Library Directories.\n\n\nManually add the following files to the \"Source Files\" section of your Tensorflow project:\n\n\n\ntensorflow/compiler/aot/runtime.cc\ntensorflow/compiler/xla/executable_run_options.cc\ntensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int32.cc\ntensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int64.cc\ntensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_1d.cc\ntensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_2d.cc\ntensorflow/compiler/xla/service/cpu/runtime_matmul.cc\ntensorflow/compiler/xla/service/cpu/runtime_single_threaded_matmul.cc\ntensorflow/compiler/xla/service/cpu/runtime_conv2d.cc\ntensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc\ntensorflow/compiler/xla/service/cpu/cpu_runtime.cc\n\n\n\nSet your project dependencies in your solution so that the projects you want to use the static library in depend on your Tensorflow project. (Right-click in your project -> Build Dependencies -> Project dependencies).\n\n\nYou're ready to use the generated static library in your code. Here's an example snippet:\n\n\n#define EIGEN_USE_THREADS\n#define EIGEN_USE_CUSTOM_THREAD_POOL\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"  // From your Tensorflow checkout.\n#include \"your_generated_header.h\"\n\neigen::ThreadPool thread_pool(2);  // Number of threads in your pool. Set as you want.\neigen::ThreadPoolDevice thread_pool_device(&thread_pool, thread_pool.NumThreads());\nYourNamespace::YourCppClass helper_object;\n\nhelper_object.set_thread_pool(&thread_pool_device);\nhelper_object.set_arg0_data(your_arg_data);\nhelper_object.Run();\nauto your_result = helper_object.result0();\n\nKeep in mind that if you enable vector instructions (by passing args like +sse4.2, +avx or +avx2 to target_features when running tfcompile) you should ensure that any pointers passed to set_arg*_data(...) have the appropriate memory alignment. Otherwise you are likely to experience crashes when calling Run().\nEnjoy!", "body": "Thanks for merging the fix! Here are detailed instructions on how to build and run a static library generated by tfcompile in MSVC.\r\n\r\n1. In a Linux machine (tfcompile doesn't work on Windows yet) checkout tensorflow and build tfcompile. You will need to [install Bazel](https://bazel.build/versions/master/docs/install.html) if you haven't done so already.\r\n> git clone https://github.com/tensorflow/tensorflow.git\r\n> cd tensorflow\r\n> bazel build -c opt //tensorflow/compiler/aot:tfcompile\r\n\r\n2. Run tfcompile to generate the static library for your graph and config.\r\n> bazel-bin/tensorflow/compiler/aot/tfcompile --target_triple=\"**x86_64-pc-windows-msvc**\" --graph=\"_your_graph.pb_\" --config=\"_your_graph_config.pbtxt_\" --entry_point=\"_your_C_entry_point_name_\" --cpp_class=\"_YourNamespace::YourCppClass_\" --out_object=\"output.lib\" --out_header=\"output.h\"\r\n\r\nYou might want to add some other arguments to tfcompile, like for example:\r\n> --xla_cpu_llvm_opt_level=3 --target_features=\"+avx,+avx2,+sse4.2\"\r\n\r\n3. Copy your generated .lib and .h files to the Windows machine where you run MSVC.\r\n\r\n4. In the MSVC solution you want to use the static library, create a new empty Win32 Static Library project.\r\n\r\n5. Checkout the Tensorflow source again in Windows (e.g., your_project\\third_party\\tensorflow)\r\n\r\n6. Open tensorflow\\workspace.bzl in your tensorflow checkout and look for \"eigen_archive\". Download one of the URLs there and extract the contents into a folder (e.g., your_project\\third_party\\eigen).\r\n\r\n7. Open tensorflow\\contrib\\cmake\\CMakeLists.txt in your tensorflow checkout and look for \"add_definitions\". Take a look to the preprocessor macros defined there (starting with -D) and add them manually in MSVC to your Tensorflow project properties -> C/C++ -> Preprocessor. You will need to define at least NOMINMAX, COMPILER_MSVC and TF_COMPILE_LIBRARY. Similarly, you might want to add some other compiler options from CMakeLists.txt into C/C++ -> Command Line -> Additional Options. None of these should be a strong requirement AFAIK.\r\n\r\n8. In your Tensorflow project properties -> C/C++ -> General -> Additional Include Directories add the following entries in this order:\r\n- If needed, the path where your generated .h file is.\r\n- The path where you extracted eigen (e.g., your_project\\third_party\\eigen).\r\n- The path where you extracted tensorflow (e.g., your_project\\third_party\\tensorflow).\r\n\r\n9. In your Tensorflow project properties -> Librarian, add your generated .lib to Additional Dependencies, and the path to it in Additional Library Directories.\r\n\r\n10. Manually add the following files to the \"Source Files\" section of your Tensorflow project:\r\n- tensorflow/compiler/aot/runtime.cc\r\n- tensorflow/compiler/xla/executable_run_options.cc\r\n- tensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int32.cc\r\n- tensorflow/compiler/tf2xla/kernels/gather_op_kernel_float_int64.cc\r\n- tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_1d.cc\r\n- tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_2d.cc\r\n- tensorflow/compiler/xla/service/cpu/runtime_matmul.cc\r\n- tensorflow/compiler/xla/service/cpu/runtime_single_threaded_matmul.cc\r\n- tensorflow/compiler/xla/service/cpu/runtime_conv2d.cc\r\n- tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc\r\n- tensorflow/compiler/xla/service/cpu/cpu_runtime.cc\r\n\r\n11. Set your project dependencies in your solution so that the projects you want to use the static library in depend on your Tensorflow project. (Right-click in your project -> Build Dependencies -> Project dependencies).\r\n\r\n12. You're ready to use the generated static library in your code. Here's an example snippet:\r\n```\r\n#define EIGEN_USE_THREADS\r\n#define EIGEN_USE_CUSTOM_THREAD_POOL\r\n\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"  // From your Tensorflow checkout.\r\n#include \"your_generated_header.h\"\r\n\r\neigen::ThreadPool thread_pool(2);  // Number of threads in your pool. Set as you want.\r\neigen::ThreadPoolDevice thread_pool_device(&thread_pool, thread_pool.NumThreads());\r\nYourNamespace::YourCppClass helper_object;\r\n\r\nhelper_object.set_thread_pool(&thread_pool_device);\r\nhelper_object.set_arg0_data(your_arg_data);\r\nhelper_object.Run();\r\nauto your_result = helper_object.result0();\r\n```\r\n\r\nKeep in mind that if you enable vector instructions (by passing args like +sse4.2, +avx or +avx2 to target_features when running tfcompile) you should ensure that any pointers passed to set_arg*_data(...) have the appropriate memory alignment. Otherwise you are likely to experience crashes when calling Run().\r\n\r\nEnjoy!"}