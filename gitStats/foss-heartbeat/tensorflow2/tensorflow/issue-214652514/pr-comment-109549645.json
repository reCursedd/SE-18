{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/109549645", "pull_request_review_id": 30659273, "id": 109549645, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTU0OTY0NQ==", "diff_hunk": "@@ -1200,16 +1201,238 @@ def conv2d_transpose(inputs,\n   return layer.apply(inputs)\n \n \n+class Conv3DTranspose(Conv3D):\n+  \"\"\"Transposed 3D convolution layer (sometimes called Deconvolution).\n+\n+  The need for transposed convolutions generally arises\n+  from the desire to use a transformation going in the opposite direction\n+  of a normal convolution, i.e., from something that has the shape of the\n+  output of some convolution to something that has the shape of its input\n+  while maintaining a connectivity pattern that is compatible with\n+  said convolution.\n+\n+  Arguments:\n+    filters: Integer, the dimensionality of the output space (i.e. the number\n+      of filters in the convolution).\n+    kernel_size: An integer or tuple/list of 3 integers, specifying the\n+      depth, height and width of the 3D convolution window.\n+      Can be a single integer to specify the same value for all spatial\n+      dimensions.\n+    strides: An integer or tuple/list of 3 integers, specifying the strides\n+      of the convolution along the depth, height and width.\n+      Can be a single integer to specify the same value for all spatial\n+      dimensions.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, depth, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, channels, depth, height, width)`.\n+    activation: Activation function. Set it to None to maintain a\n+      linear activation.\n+    use_bias: Boolean, whether the layer uses a bias.\n+    kernel_initializer: An initializer for the convolution kernel.\n+    bias_initializer: An initializer for the bias vector. If None, no bias will\n+      be applied.\n+    kernel_regularizer: Optional regularizer for the convolution kernel.\n+    bias_regularizer: Optional regularizer for the bias vector.\n+    activity_regularizer: Regularizer function for the output.\n+    trainable: Boolean, if `True` also add variables to the graph collection\n+      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n+    name: A string, the name of the layer.\n+  \"\"\"\n+\n+  def __init__(self, filters,\n+               kernel_size,\n+               strides=(1, 1, 1),\n+               padding='valid',\n+               data_format='channels_last',\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer=None,\n+               bias_initializer=init_ops.zeros_initializer(),\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               trainable=True,\n+               name=None,\n+               **kwargs):\n+    super(Conv3DTranspose, self).__init__(\n+        rank=3,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        activation=activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        bias_initializer=bias_initializer,\n+        kernel_regularizer=kernel_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        trainable=trainable,\n+        name=name, **kwargs)\n+\n+  def call(self, inputs):\n+    inputs_shape = array_ops.shape(inputs)\n+    batch_size = inputs_shape[0]\n+    if self.data_format == 'channels_first':\n+      c_axis, d_axis, h_axis, w_axis = 1, 2, 3, 4\n+    else:\n+      c_axis, d_axis, h_axis, w_axis = 4, 1, 2, 3\n+\n+    depth, height, width = inputs_shape[d_axis], inputs_shape[h_axis], \\\n+        inputs_shape[w_axis]\n+    kernel_d, kernel_h, kernel_w = self.kernel_size\n+    stride_d, stride_h, stride_w = self.strides\n+\n+    def get_deconv_dim(dim_size, stride_size, kernel_size, padding):\n+      if isinstance(dim_size, ops.Tensor):\n+        dim_size = math_ops.multiply(dim_size, stride_size)\n+      elif dim_size is not None:\n+        dim_size *= stride_size\n+\n+      if padding == 'valid' and dim_size is not None:\n+        dim_size += max(kernel_size - stride_size, 0)\n+      return dim_size\n+\n+    # Infer the dynamic output shape:\n+    out_depth = get_deconv_dim(depth, stride_d, kernel_d, self.padding)\n+    out_height = get_deconv_dim(height, stride_h, kernel_h, self.padding)\n+    out_width = get_deconv_dim(width, stride_w, kernel_w, self.padding)\n+\n+    if self.data_format == 'channels_first':\n+      output_shape = (batch_size, self.filters, out_depth, out_height,\n+                      out_width)\n+      strides = (1, 1, stride_d, stride_h, stride_w)\n+    else:\n+      output_shape = (batch_size, out_depth, out_height, out_width,\n+                      self.filters)\n+      strides = (1, stride_d, stride_h, stride_w, 1)\n+\n+    output_shape_tensor = array_ops.stack(output_shape)\n+    outputs = nn.conv3d_transpose(\n+        inputs,\n+        self.kernel,\n+        output_shape_tensor,\n+        strides,\n+        padding=self.padding.upper())\n+\n+    # Infer the static output shape:\n+    out_shape = inputs.get_shape().as_list()\n+    out_shape[c_axis] = self.filters\n+    out_shape[d_axis] = get_deconv_dim(\n+        out_shape[d_axis], stride_d, kernel_d, self.padding)\n+    out_shape[h_axis] = get_deconv_dim(\n+        out_shape[h_axis], stride_h, kernel_h, self.padding)\n+    out_shape[w_axis] = get_deconv_dim(\n+        out_shape[w_axis], stride_w, kernel_w, self.padding)\n+    outputs.set_shape(out_shape)\n+\n+    if self.bias:\n+      outputs = nn.bias_add(", "path": "tensorflow/python/layers/convolutional.py", "position": null, "original_position": 154, "commit_id": "a2006b0284e870da65c5f3a0ffafd7d4268e3c0e", "original_commit_id": "b9e72541348e2d0663cb63716435aaaef4f27930", "user": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "body": "I believe this would fail in data_format `channels_first` (to be verified). See what we do for `Conv3D` bias.", "created_at": "2017-04-03T23:40:49Z", "updated_at": "2017-04-26T14:21:13Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8461#discussion_r109549645", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8461", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/109549645"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8461#discussion_r109549645"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8461"}}, "body_html": "<p>I believe this would fail in data_format <code>channels_first</code> (to be verified). See what we do for <code>Conv3D</code> bias.</p>", "body_text": "I believe this would fail in data_format channels_first (to be verified). See what we do for Conv3D bias."}