{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/110753457", "pull_request_review_id": 31948382, "id": 110753457, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMDc1MzQ1Nw==", "diff_hunk": "@@ -1212,16 +1203,260 @@ def conv2d_transpose(inputs,\n   return layer.apply(inputs)\n \n \n+class Conv3DTranspose(Conv3D):\n+  \"\"\"Transposed 3D convolution layer (sometimes called 3D Deconvolution).\n+\n+  Arguments:\n+    filters: Integer, the dimensionality of the output space (i.e. the number\n+      of filters in the convolution).\n+    kernel_size: An integer or tuple/list of 3 integers, specifying the\n+      depth, height and width of the 3D convolution window.\n+      Can be a single integer to specify the same value for all spatial\n+      dimensions.\n+    strides: An integer or tuple/list of 3 integers, specifying the strides\n+      of the convolution along the depth, height and width.\n+      Can be a single integer to specify the same value for all spatial\n+      dimensions.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, depth, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, channels, depth, height, width)`.\n+    activation: Activation function. Set it to None to maintain a\n+      linear activation.\n+    use_bias: Boolean, whether the layer uses a bias.\n+    kernel_initializer: An initializer for the convolution kernel.\n+    bias_initializer: An initializer for the bias vector. If None, no bias will\n+      be applied.\n+    kernel_regularizer: Optional regularizer for the convolution kernel.\n+    bias_regularizer: Optional regularizer for the bias vector.\n+    activity_regularizer: Regularizer function for the output.\n+    trainable: Boolean, if `True` also add variables to the graph collection\n+      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n+    name: A string, the name of the layer.\n+  \"\"\"\n+\n+  def __init__(self, filters,\n+               kernel_size,\n+               strides=(1, 1, 1),\n+               padding='valid',\n+               data_format='channels_last',\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer=None,\n+               bias_initializer=init_ops.zeros_initializer(),\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               trainable=True,\n+               name=None,\n+               **kwargs):\n+    super(Conv3DTranspose, self).__init__(\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        activation=activation,\n+        use_bias=use_bias,\n+        kernel_initializer=kernel_initializer,\n+        bias_initializer=bias_initializer,\n+        kernel_regularizer=kernel_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        trainable=trainable,\n+        name=name, **kwargs)\n+\n+  def build(self, input_shape):\n+    if len(input_shape) != 5:\n+      raise ValueError('Inputs should have rank ' +\n+                       str(5) +\n+                       'Received input shape:', str(input_shape))\n+    if self.data_format == 'channels_first':\n+      channel_axis = 1\n+    else:\n+      channel_axis = -1\n+    if input_shape[channel_axis] is None:\n+      raise ValueError('The channel dimension of the inputs '\n+                       'should be defined. Found `None`.')\n+    input_dim = input_shape[channel_axis]\n+    kernel_shape = self.kernel_size + (self.filters, input_dim)\n+\n+    self.kernel = vs.get_variable('kernel',\n+                                  shape=kernel_shape,\n+                                  initializer=self.kernel_initializer,\n+                                  regularizer=self.kernel_regularizer,\n+                                  trainable=True,\n+                                  dtype=self.dtype)\n+    if self.use_bias:\n+      self.bias = vs.get_variable('bias',\n+                                  shape=(self.filters,),\n+                                  initializer=self.bias_initializer,\n+                                  regularizer=self.bias_regularizer,\n+                                  trainable=True,\n+                                  dtype=self.dtype)\n+    else:\n+      self.bias = None\n+\n+  def call(self, inputs):\n+    inputs_shape = array_ops.shape(inputs)\n+    batch_size = inputs_shape[0]\n+    if self.data_format == 'channels_first':\n+      c_axis, d_axis, h_axis, w_axis = 1, 2, 3, 4\n+    else:\n+      c_axis, d_axis, h_axis, w_axis = 4, 1, 2, 3\n+\n+    depth = inputs_shape[d_axis]\n+    height = inputs_shape[h_axis]\n+    width = inputs_shape[w_axis]\n+\n+    kernel_d, kernel_h, kernel_w = self.kernel_size\n+    stride_d, stride_h, stride_w = self.strides\n+\n+    # Infer the dynamic output shape:\n+    out_depth = utils.get_deconv_dim(depth, stride_d, kernel_d, self.padding)\n+    out_height = utils.get_deconv_dim(height, stride_h, kernel_h, self.padding)\n+    out_width = utils.get_deconv_dim(width, stride_w, kernel_w, self.padding)\n+\n+    if self.data_format == 'channels_first':\n+      output_shape = (batch_size, self.filters, out_depth, out_height,\n+                      out_width)\n+      strides = (1, 1, stride_d, stride_h, stride_w)\n+    else:\n+      output_shape = (batch_size, out_depth, out_height, out_width,\n+                      self.filters)\n+      strides = (1, stride_d, stride_h, stride_w, 1)\n+\n+    output_shape_tensor = array_ops.stack(output_shape)\n+    outputs = nn.conv3d_transpose(\n+        inputs,\n+        self.kernel,\n+        output_shape_tensor,\n+        strides,\n+        data_format=utils.convert_data_format(self.data_format, ndim=5),\n+        padding=self.padding.upper())\n+\n+    # Infer the static output shape:\n+    out_shape = inputs.get_shape().as_list()\n+    out_shape[c_axis] = self.filters\n+    out_shape[d_axis] = utils.get_deconv_dim(\n+        out_shape[d_axis], stride_d, kernel_d, self.padding)\n+    out_shape[h_axis] = utils.get_deconv_dim(\n+        out_shape[h_axis], stride_h, kernel_h, self.padding)\n+    out_shape[w_axis] = utils.get_deconv_dim(\n+        out_shape[w_axis], stride_w, kernel_w, self.padding)\n+    outputs.set_shape(out_shape)\n+\n+    if self.bias:\n+      outputs_shape = outputs.shape.as_list()\n+      if self.data_format == 'channels_first':\n+        outputs_4d = array_ops.reshape(outputs,\n+                                       [outputs_shape[0], outputs_shape[1],\n+                                        outputs_shape[2] * outputs_shape[3],\n+                                        outputs_shape[4]])\n+      else:\n+        outputs_4d = array_ops.reshape(outputs,\n+                                       [outputs_shape[0],\n+                                        outputs_shape[1] * outputs_shape[2],\n+                                        outputs_shape[3], outputs_shape[4]])\n+      outputs_4d = nn.bias_add(\n+          outputs_4d,\n+          self.bias,\n+          data_format=utils.convert_data_format(self.data_format, ndim=4))\n+      outputs = array_ops.reshape(outputs_4d, outputs_shape)\n+\n+    if self.activation is not None:\n+      return self.activation(outputs)\n+    return outputs\n+\n+\n+def conv3d_transpose(inputs,\n+                     filters,\n+                     kernel_size,\n+                     strides=(1, 1, 1),\n+                     padding='valid',\n+                     data_format='channels_last',\n+                     activation=None,\n+                     use_bias=True,\n+                     kernel_initializer=None,\n+                     bias_initializer=init_ops.zeros_initializer(),\n+                     kernel_regularizer=None,\n+                     bias_regularizer=None,\n+                     activity_regularizer=None,\n+                     trainable=True,\n+                     name=None,\n+                     reuse=None):\n+  \"\"\"Functional interface for transposed 3D convolution layer (sometimes\n+  called 3D Deconvolution).", "path": "tensorflow/python/layers/convolutional.py", "position": null, "original_position": 243, "commit_id": "a2006b0284e870da65c5f3a0ffafd7d4268e3c0e", "original_commit_id": "5b8c4ee1ee6eec2a788237d0483d7587502b59e5", "user": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "body": "Make it fit on one line", "created_at": "2017-04-10T20:20:11Z", "updated_at": "2017-04-26T14:21:13Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8461#discussion_r110753457", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8461", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/110753457"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8461#discussion_r110753457"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8461"}}, "body_html": "<p>Make it fit on one line</p>", "body_text": "Make it fit on one line"}