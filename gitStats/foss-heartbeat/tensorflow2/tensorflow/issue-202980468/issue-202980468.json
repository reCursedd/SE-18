{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7051", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7051/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7051/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7051/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7051", "id": 202980468, "node_id": "MDU6SXNzdWUyMDI5ODA0Njg=", "number": 7051, "title": "Bug - tfdbg + multi-gpu gives ValueError: Duplicate node name: 'n/_0'", "user": {"login": "raphtown", "id": 177576, "node_id": "MDQ6VXNlcjE3NzU3Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/177576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raphtown", "html_url": "https://github.com/raphtown", "followers_url": "https://api.github.com/users/raphtown/followers", "following_url": "https://api.github.com/users/raphtown/following{/other_user}", "gists_url": "https://api.github.com/users/raphtown/gists{/gist_id}", "starred_url": "https://api.github.com/users/raphtown/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raphtown/subscriptions", "organizations_url": "https://api.github.com/users/raphtown/orgs", "repos_url": "https://api.github.com/users/raphtown/repos", "events_url": "https://api.github.com/users/raphtown/events{/privacy}", "received_events_url": "https://api.github.com/users/raphtown/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "caisq", "id": 16824702, "node_id": "MDQ6VXNlcjE2ODI0NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16824702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caisq", "html_url": "https://github.com/caisq", "followers_url": "https://api.github.com/users/caisq/followers", "following_url": "https://api.github.com/users/caisq/following{/other_user}", "gists_url": "https://api.github.com/users/caisq/gists{/gist_id}", "starred_url": "https://api.github.com/users/caisq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caisq/subscriptions", "organizations_url": "https://api.github.com/users/caisq/orgs", "repos_url": "https://api.github.com/users/caisq/repos", "events_url": "https://api.github.com/users/caisq/events{/privacy}", "received_events_url": "https://api.github.com/users/caisq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "caisq", "id": 16824702, "node_id": "MDQ6VXNlcjE2ODI0NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16824702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caisq", "html_url": "https://github.com/caisq", "followers_url": "https://api.github.com/users/caisq/followers", "following_url": "https://api.github.com/users/caisq/following{/other_user}", "gists_url": "https://api.github.com/users/caisq/gists{/gist_id}", "starred_url": "https://api.github.com/users/caisq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caisq/subscriptions", "organizations_url": "https://api.github.com/users/caisq/orgs", "repos_url": "https://api.github.com/users/caisq/repos", "events_url": "https://api.github.com/users/caisq/events{/privacy}", "received_events_url": "https://api.github.com/users/caisq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2017-01-25T00:26:45Z", "updated_at": "2018-08-29T02:17:17Z", "closed_at": "2017-06-06T00:05:10Z", "author_association": "NONE", "body_html": "<p>Hello tensorflow team,</p>\n<p>I have been starting to use your tensorflow debugger but have run into the issue that when I try and use it on a multi-gpu model I get <code>ValueError: Duplicate node name: 'n/_0'</code>.</p>\n<p>Inspecting things closer, I saw that the issue originated from the run_metadata, whose partition graphs have many _Send and _HostRecv ops with names like 'n/_0'.  These ops are replicated with identical names across my towers which is what is causing the issue.</p>\n<p>Looking through the tensorflow code, I believe I tracked where this name is set down to <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/graph_partition.cc#L195\">graph_partition.cc:195</a> where the edge's source name is used as the prefix 'n'.  Unfortunately, I have not been able to figure out why the source's name is only 'n', but that seems to be the root of the issue here.</p>\n<p>I should add that I never set any tensor name to 'n' anywhere in my own code.  Plus, I see certain tests in your codebase rely on names such as 'n/_0' which indicates to me the name is being set somewhere internally in the tensorflow code.</p>\n<p>Any help you can provide would be much appreciated!</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>I didn't find any related issues.</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04.5 LTS (running in a <a href=\"http://singularity.lbl.gov\" rel=\"nofollow\">singularity</a> container on a CentOS 6.7 host).</p>\n<p>Installed version of CUDA and cuDNN:<br>\nI am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 .<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<pre><code>libOpenCL.so\nlibOpenCL.so.1\nlibOpenCL.so.1.0\nlibOpenCL.so.1.0.0\nlibcublas.so\nlibcublas.so.8.0\nlibcublas.so.8.0.45\nlibcublas_device.a\nlibcublas_static.a\nlibcudadevrt.a\nlibcudart.so\nlibcudart.so.8.0\nlibcudart.so.8.0.44\nlibcudart_static.a\nlibcudnn.so\nlibcudnn.so.5\nlibcudnn.so.5.1.5\nlibcudnn_static.a\nlibcufft.so\nlibcufft.so.8.0\nlibcufft.so.8.0.44\nlibcufft_static.a\nlibcufftw.so\nlibcufftw.so.8.0\nlibcufftw.so.8.0.44\nlibcufftw_static.a\nlibcuinj64.so\nlibcuinj64.so.8.0\nlibcuinj64.so.8.0.44\nlibculibos.a\nlibcurand.so\nlibcurand.so.8.0\nlibcurand.so.8.0.44\nlibcurand_static.a\nlibcusolver.so\nlibcusolver.so.8.0\nlibcusolver.so.8.0.44\nlibcusolver_static.a\nlibcusparse.so\nlibcusparse.so.8.0\nlibcusparse.so.8.0.44\nlibcusparse_static.a\nlibnppc.so\nlibnppc.so.8.0\nlibnppc.so.8.0.44\nlibnppc_static.a\nlibnppi.so\nlibnppi.so.8.0\nlibnppi.so.8.0.44\nlibnppi_static.a\nlibnppial.so\nlibnppial.so.8.0\nlibnppial.so.8.0.44\nlibnppicc.so\nlibnppicc.so.8.0\nlibnppicc.so.8.0.44\nlibnppicom.so\nlibnppicom.so.8.0\nlibnppicom.so.8.0.44\nlibnppidei.so\nlibnppidei.so.8.0\nlibnppidei.so.8.0.44\nlibnppif.so\nlibnppif.so.8.0\nlibnppif.so.8.0.44\nlibnppig.so\nlibnppig.so.8.0\nlibnppig.so.8.0.44\nlibnppim.so\nlibnppim.so.8.0\nlibnppim.so.8.0.44\nlibnppist.so\nlibnppist.so.8.0\nlibnppist.so.8.0.44\nlibnppisu.so\nlibnppisu.so.8.0\nlibnppisu.so.8.0.44\nlibnppitc.so\nlibnppitc.so.8.0\nlibnppitc.so.8.0.44\nlibnpps.so\nlibnpps.so.8.0\nlibnpps.so.8.0.44\nlibnpps_static.a\nlibnvToolsExt.so\nlibnvToolsExt.so.1\nlibnvToolsExt.so.1.0.0\nlibnvblas.so\nlibnvblas.so.8.0\nlibnvblas.so.8.0.44\nlibnvgraph.so\nlibnvgraph.so.8.0\nlibnvgraph.so.8.0.44\nlibnvgraph_static.a\nlibnvrtc-builtins.so\nlibnvrtc-builtins.so.8.0\nlibnvrtc-builtins.so.8.0.44\nlibnvrtc.so\nlibnvrtc.so.8.0\nlibnvrtc.so.8.0.44\nstubs\n</code></pre>\n<ol>\n<li>A link to the pip package you installed:<br>\nI installed tensorflow using <code>pip install tensorflow-gpu==0.12.1</code></li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally \nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally \nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\n0.12.1 \n</code></pre>\n<h3>What other attempted solutions have you tried?</h3>\n<p>The single GPU case works fine.</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>Here is the dump of some of the problematic nodes.</p>\n<pre><code>node {\n  name: \"n/_0\"\n  op: \"_Send\"\n  input: \"__copy_TOWER0/Const_0\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: \"client_terminated\"\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: \"recv_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device_incarnation\"\n    value {\n      i: 0\n    }\n  }\n  attr {\n    key: \"tensor_name\"\n    value {\n      s: \"edge_545___copy_TOWER0/Const_0\"\n    }\n  }\n}\nnode {\n  name: \"n/_1\"\n  op: \"_HostRecv\"\n  input: \"^n/_0\"\n  attr {\n    key: \"client_terminated\"\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: \"recv_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device_incarnation\"\n    value {\n      i: 0\n    }\n  }\n  attr {\n    key: \"tensor_name\"\n    value {\n      s: \"edge_545___copy_TOWER0/Const_0\"\n    }\n  }\n  attr {\n    key: \"tensor_type\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: \"n/_2\"\n  op: \"_Send\"\n  input: \"__copy_TOWER0/Sub_0\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: \"client_terminated\"\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: \"recv_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device_incarnation\"\n    value {\n      i: 0\n    }\n  }\n  attr {\n    key: \"tensor_name\"\n    value {\n      s: \"edge_551___copy_TOWER0/Sub_0\"\n    }\n  }\n}\n\n</code></pre>\n<p>End of backtrace at crash point</p>\n<pre><code>  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py(419)run()                 \n-&gt; run_end_resp = self.on_run_end(run_end_req)                                                                              \n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py(262)on_run_end()  \n-&gt; self._dump_root, partition_graphs=partition_graphs)                                                                      \n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(407)__init__()                    \n-&gt; self._load_partition_graphs(partition_graphs)                                                                            \n&gt; /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(493)_load_partition_graphs()      \n-&gt; raise ValueError(\"Duplicate node name: '%s'\" % node.name)                                                                \n</code></pre>", "body_text": "Hello tensorflow team,\nI have been starting to use your tensorflow debugger but have run into the issue that when I try and use it on a multi-gpu model I get ValueError: Duplicate node name: 'n/_0'.\nInspecting things closer, I saw that the issue originated from the run_metadata, whose partition graphs have many _Send and _HostRecv ops with names like 'n/_0'.  These ops are replicated with identical names across my towers which is what is causing the issue.\nLooking through the tensorflow code, I believe I tracked where this name is set down to graph_partition.cc:195 where the edge's source name is used as the prefix 'n'.  Unfortunately, I have not been able to figure out why the source's name is only 'n', but that seems to be the root of the issue here.\nI should add that I never set any tensor name to 'n' anywhere in my own code.  Plus, I see certain tests in your codebase rely on names such as 'n/_0' which indicates to me the name is being set somewhere internally in the tensorflow code.\nAny help you can provide would be much appreciated!\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nI didn't find any related issues.\nEnvironment info\nOperating System: Ubuntu 14.04.5 LTS (running in a singularity container on a CentOS 6.7 host).\nInstalled version of CUDA and cuDNN:\nI am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 .\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nlibOpenCL.so\nlibOpenCL.so.1\nlibOpenCL.so.1.0\nlibOpenCL.so.1.0.0\nlibcublas.so\nlibcublas.so.8.0\nlibcublas.so.8.0.45\nlibcublas_device.a\nlibcublas_static.a\nlibcudadevrt.a\nlibcudart.so\nlibcudart.so.8.0\nlibcudart.so.8.0.44\nlibcudart_static.a\nlibcudnn.so\nlibcudnn.so.5\nlibcudnn.so.5.1.5\nlibcudnn_static.a\nlibcufft.so\nlibcufft.so.8.0\nlibcufft.so.8.0.44\nlibcufft_static.a\nlibcufftw.so\nlibcufftw.so.8.0\nlibcufftw.so.8.0.44\nlibcufftw_static.a\nlibcuinj64.so\nlibcuinj64.so.8.0\nlibcuinj64.so.8.0.44\nlibculibos.a\nlibcurand.so\nlibcurand.so.8.0\nlibcurand.so.8.0.44\nlibcurand_static.a\nlibcusolver.so\nlibcusolver.so.8.0\nlibcusolver.so.8.0.44\nlibcusolver_static.a\nlibcusparse.so\nlibcusparse.so.8.0\nlibcusparse.so.8.0.44\nlibcusparse_static.a\nlibnppc.so\nlibnppc.so.8.0\nlibnppc.so.8.0.44\nlibnppc_static.a\nlibnppi.so\nlibnppi.so.8.0\nlibnppi.so.8.0.44\nlibnppi_static.a\nlibnppial.so\nlibnppial.so.8.0\nlibnppial.so.8.0.44\nlibnppicc.so\nlibnppicc.so.8.0\nlibnppicc.so.8.0.44\nlibnppicom.so\nlibnppicom.so.8.0\nlibnppicom.so.8.0.44\nlibnppidei.so\nlibnppidei.so.8.0\nlibnppidei.so.8.0.44\nlibnppif.so\nlibnppif.so.8.0\nlibnppif.so.8.0.44\nlibnppig.so\nlibnppig.so.8.0\nlibnppig.so.8.0.44\nlibnppim.so\nlibnppim.so.8.0\nlibnppim.so.8.0.44\nlibnppist.so\nlibnppist.so.8.0\nlibnppist.so.8.0.44\nlibnppisu.so\nlibnppisu.so.8.0\nlibnppisu.so.8.0.44\nlibnppitc.so\nlibnppitc.so.8.0\nlibnppitc.so.8.0.44\nlibnpps.so\nlibnpps.so.8.0\nlibnpps.so.8.0.44\nlibnpps_static.a\nlibnvToolsExt.so\nlibnvToolsExt.so.1\nlibnvToolsExt.so.1.0.0\nlibnvblas.so\nlibnvblas.so.8.0\nlibnvblas.so.8.0.44\nlibnvgraph.so\nlibnvgraph.so.8.0\nlibnvgraph.so.8.0.44\nlibnvgraph_static.a\nlibnvrtc-builtins.so\nlibnvrtc-builtins.so.8.0\nlibnvrtc-builtins.so.8.0.44\nlibnvrtc.so\nlibnvrtc.so.8.0\nlibnvrtc.so.8.0.44\nstubs\n\n\nA link to the pip package you installed:\nI installed tensorflow using pip install tensorflow-gpu==0.12.1\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally \nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally \nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\n0.12.1 \n\nWhat other attempted solutions have you tried?\nThe single GPU case works fine.\nLogs or other output that would be helpful\nHere is the dump of some of the problematic nodes.\nnode {\n  name: \"n/_0\"\n  op: \"_Send\"\n  input: \"__copy_TOWER0/Const_0\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: \"client_terminated\"\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: \"recv_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device_incarnation\"\n    value {\n      i: 0\n    }\n  }\n  attr {\n    key: \"tensor_name\"\n    value {\n      s: \"edge_545___copy_TOWER0/Const_0\"\n    }\n  }\n}\nnode {\n  name: \"n/_1\"\n  op: \"_HostRecv\"\n  input: \"^n/_0\"\n  attr {\n    key: \"client_terminated\"\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: \"recv_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device_incarnation\"\n    value {\n      i: 0\n    }\n  }\n  attr {\n    key: \"tensor_name\"\n    value {\n      s: \"edge_545___copy_TOWER0/Const_0\"\n    }\n  }\n  attr {\n    key: \"tensor_type\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n}\nnode {\n  name: \"n/_2\"\n  op: \"_Send\"\n  input: \"__copy_TOWER0/Sub_0\"\n  attr {\n    key: \"T\"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: \"client_terminated\"\n    value {\n      b: false\n    }\n  }\n  attr {\n    key: \"recv_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device\"\n    value {\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\n    }\n  }\n  attr {\n    key: \"send_device_incarnation\"\n    value {\n      i: 0\n    }\n  }\n  attr {\n    key: \"tensor_name\"\n    value {\n      s: \"edge_551___copy_TOWER0/Sub_0\"\n    }\n  }\n}\n\n\nEnd of backtrace at crash point\n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py(419)run()                 \n-> run_end_resp = self.on_run_end(run_end_req)                                                                              \n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py(262)on_run_end()  \n-> self._dump_root, partition_graphs=partition_graphs)                                                                      \n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(407)__init__()                    \n-> self._load_partition_graphs(partition_graphs)                                                                            \n> /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(493)_load_partition_graphs()      \n-> raise ValueError(\"Duplicate node name: '%s'\" % node.name)", "body": "Hello tensorflow team,\r\n\r\nI have been starting to use your tensorflow debugger but have run into the issue that when I try and use it on a multi-gpu model I get `ValueError: Duplicate node name: 'n/_0'`.\r\n\r\nInspecting things closer, I saw that the issue originated from the run_metadata, whose partition graphs have many _Send and _HostRecv ops with names like 'n/_0'.  These ops are replicated with identical names across my towers which is what is causing the issue.\r\n\r\nLooking through the tensorflow code, I believe I tracked where this name is set down to [graph_partition.cc:195](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/graph_partition.cc#L195) where the edge's source name is used as the prefix 'n'.  Unfortunately, I have not been able to figure out why the source's name is only 'n', but that seems to be the root of the issue here.\r\n\r\nI should add that I never set any tensor name to 'n' anywhere in my own code.  Plus, I see certain tests in your codebase rely on names such as 'n/_0' which indicates to me the name is being set somewhere internally in the tensorflow code.\r\n\r\nAny help you can provide would be much appreciated!\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI didn't find any related issues.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.5 LTS (running in a [singularity](http://singularity.lbl.gov) container on a CentOS 6.7 host). \r\n\r\nInstalled version of CUDA and cuDNN: \r\nI am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 . \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nlibOpenCL.so\r\nlibOpenCL.so.1\r\nlibOpenCL.so.1.0\r\nlibOpenCL.so.1.0.0\r\nlibcublas.so\r\nlibcublas.so.8.0\r\nlibcublas.so.8.0.45\r\nlibcublas_device.a\r\nlibcublas_static.a\r\nlibcudadevrt.a\r\nlibcudart.so\r\nlibcudart.so.8.0\r\nlibcudart.so.8.0.44\r\nlibcudart_static.a\r\nlibcudnn.so\r\nlibcudnn.so.5\r\nlibcudnn.so.5.1.5\r\nlibcudnn_static.a\r\nlibcufft.so\r\nlibcufft.so.8.0\r\nlibcufft.so.8.0.44\r\nlibcufft_static.a\r\nlibcufftw.so\r\nlibcufftw.so.8.0\r\nlibcufftw.so.8.0.44\r\nlibcufftw_static.a\r\nlibcuinj64.so\r\nlibcuinj64.so.8.0\r\nlibcuinj64.so.8.0.44\r\nlibculibos.a\r\nlibcurand.so\r\nlibcurand.so.8.0\r\nlibcurand.so.8.0.44\r\nlibcurand_static.a\r\nlibcusolver.so\r\nlibcusolver.so.8.0\r\nlibcusolver.so.8.0.44\r\nlibcusolver_static.a\r\nlibcusparse.so\r\nlibcusparse.so.8.0\r\nlibcusparse.so.8.0.44\r\nlibcusparse_static.a\r\nlibnppc.so\r\nlibnppc.so.8.0\r\nlibnppc.so.8.0.44\r\nlibnppc_static.a\r\nlibnppi.so\r\nlibnppi.so.8.0\r\nlibnppi.so.8.0.44\r\nlibnppi_static.a\r\nlibnppial.so\r\nlibnppial.so.8.0\r\nlibnppial.so.8.0.44\r\nlibnppicc.so\r\nlibnppicc.so.8.0\r\nlibnppicc.so.8.0.44\r\nlibnppicom.so\r\nlibnppicom.so.8.0\r\nlibnppicom.so.8.0.44\r\nlibnppidei.so\r\nlibnppidei.so.8.0\r\nlibnppidei.so.8.0.44\r\nlibnppif.so\r\nlibnppif.so.8.0\r\nlibnppif.so.8.0.44\r\nlibnppig.so\r\nlibnppig.so.8.0\r\nlibnppig.so.8.0.44\r\nlibnppim.so\r\nlibnppim.so.8.0\r\nlibnppim.so.8.0.44\r\nlibnppist.so\r\nlibnppist.so.8.0\r\nlibnppist.so.8.0.44\r\nlibnppisu.so\r\nlibnppisu.so.8.0\r\nlibnppisu.so.8.0.44\r\nlibnppitc.so\r\nlibnppitc.so.8.0\r\nlibnppitc.so.8.0.44\r\nlibnpps.so\r\nlibnpps.so.8.0\r\nlibnpps.so.8.0.44\r\nlibnpps_static.a\r\nlibnvToolsExt.so\r\nlibnvToolsExt.so.1\r\nlibnvToolsExt.so.1.0.0\r\nlibnvblas.so\r\nlibnvblas.so.8.0\r\nlibnvblas.so.8.0.44\r\nlibnvgraph.so\r\nlibnvgraph.so.8.0\r\nlibnvgraph.so.8.0.44\r\nlibnvgraph_static.a\r\nlibnvrtc-builtins.so\r\nlibnvrtc-builtins.so.8.0\r\nlibnvrtc-builtins.so.8.0.44\r\nlibnvrtc.so\r\nlibnvrtc.so.8.0\r\nlibnvrtc.so.8.0.44\r\nstubs\r\n```\r\n\r\n\r\n\r\n1. A link to the pip package you installed:\r\nI installed tensorflow using `pip install tensorflow-gpu==0.12.1`\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally \r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally \r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n0.12.1 \r\n```                                                                                     \r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nThe single GPU case works fine.\r\n\r\n### Logs or other output that would be helpful\r\n\r\nHere is the dump of some of the problematic nodes.\r\n\r\n```\r\nnode {\r\n  name: \"n/_0\"\r\n  op: \"_Send\"\r\n  input: \"__copy_TOWER0/Const_0\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"client_terminated\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"recv_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device_incarnation\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_name\"\r\n    value {\r\n      s: \"edge_545___copy_TOWER0/Const_0\"\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"n/_1\"\r\n  op: \"_HostRecv\"\r\n  input: \"^n/_0\"\r\n  attr {\r\n    key: \"client_terminated\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"recv_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device_incarnation\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_name\"\r\n    value {\r\n      s: \"edge_545___copy_TOWER0/Const_0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_type\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"n/_2\"\r\n  op: \"_Send\"\r\n  input: \"__copy_TOWER0/Sub_0\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"client_terminated\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"recv_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device_incarnation\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_name\"\r\n    value {\r\n      s: \"edge_551___copy_TOWER0/Sub_0\"\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\nEnd of backtrace at crash point\r\n```\r\n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py(419)run()                 \r\n-> run_end_resp = self.on_run_end(run_end_req)                                                                              \r\n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py(262)on_run_end()  \r\n-> self._dump_root, partition_graphs=partition_graphs)                                                                      \r\n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(407)__init__()                    \r\n-> self._load_partition_graphs(partition_graphs)                                                                            \r\n> /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(493)_load_partition_graphs()      \r\n-> raise ValueError(\"Duplicate node name: '%s'\" % node.name)                                                                \r\n```"}