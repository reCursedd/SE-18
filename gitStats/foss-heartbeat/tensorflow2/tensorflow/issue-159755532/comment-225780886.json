{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225780886", "html_url": "https://github.com/tensorflow/tensorflow/pull/2798#issuecomment-225780886", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2798", "id": 225780886, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTc4MDg4Ng==", "user": {"login": "suiyuan2009", "id": 5105569, "node_id": "MDQ6VXNlcjUxMDU1Njk=", "avatar_url": "https://avatars0.githubusercontent.com/u/5105569?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suiyuan2009", "html_url": "https://github.com/suiyuan2009", "followers_url": "https://api.github.com/users/suiyuan2009/followers", "following_url": "https://api.github.com/users/suiyuan2009/following{/other_user}", "gists_url": "https://api.github.com/users/suiyuan2009/gists{/gist_id}", "starred_url": "https://api.github.com/users/suiyuan2009/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suiyuan2009/subscriptions", "organizations_url": "https://api.github.com/users/suiyuan2009/orgs", "repos_url": "https://api.github.com/users/suiyuan2009/repos", "events_url": "https://api.github.com/users/suiyuan2009/events{/privacy}", "received_events_url": "https://api.github.com/users/suiyuan2009/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-14T05:05:41Z", "updated_at": "2016-06-14T05:11:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15904123\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/georgedahl\">@georgedahl</a><br>\nthe obvious implementation of nesterov momentum is as follow</p>\n<pre><code>1. var' = var - lr_ * momentum_ * accum\n2. grad = some_function(var')\n3. accum = momentum_ * accum + grad\n4. var = var - lr_ * accum\n</code></pre>\n<p>my implementation is</p>\n<pre><code>1. grad = some_function(var)\n2. var = var + lr_ * momentum_ * accum\n3. accum = momentum_ * accum + grad\n4. var = var - lr_ * accum\n5. var = var - lr_ * momentum_ * accum\n</code></pre>\n<p>In tensorflow, a minimal step contains two parts, one is <code>compute_gradients</code>, another is <code>apply_gradients</code>. In my implenmentation, after <code>apply_gradients</code>, the <code>var</code> stored is actually <code>var - lr_ * momentum_ * accum</code>, so the value of <code>var'</code> will pass to next <code>compute_gradients</code>. In the next <code>apply_gradients</code> step, we should first recover <code>var</code> by <code>var = var + lr_ * momentum_ * accum</code>. I combine formulas to simplify codes.</p>", "body_text": "@georgedahl\nthe obvious implementation of nesterov momentum is as follow\n1. var' = var - lr_ * momentum_ * accum\n2. grad = some_function(var')\n3. accum = momentum_ * accum + grad\n4. var = var - lr_ * accum\n\nmy implementation is\n1. grad = some_function(var)\n2. var = var + lr_ * momentum_ * accum\n3. accum = momentum_ * accum + grad\n4. var = var - lr_ * accum\n5. var = var - lr_ * momentum_ * accum\n\nIn tensorflow, a minimal step contains two parts, one is compute_gradients, another is apply_gradients. In my implenmentation, after apply_gradients, the var stored is actually var - lr_ * momentum_ * accum, so the value of var' will pass to next compute_gradients. In the next apply_gradients step, we should first recover var by var = var + lr_ * momentum_ * accum. I combine formulas to simplify codes.", "body": "@georgedahl \nthe obvious implementation of nesterov momentum is as follow\n\n```\n1. var' = var - lr_ * momentum_ * accum\n2. grad = some_function(var')\n3. accum = momentum_ * accum + grad\n4. var = var - lr_ * accum\n```\n\nmy implementation is\n\n```\n1. grad = some_function(var)\n2. var = var + lr_ * momentum_ * accum\n3. accum = momentum_ * accum + grad\n4. var = var - lr_ * accum\n5. var = var - lr_ * momentum_ * accum\n```\n\nIn tensorflow, a minimal step contains two parts, one is `compute_gradients`, another is `apply_gradients`. In my implenmentation, after `apply_gradients`, the `var` stored is actually `var - lr_ * momentum_ * accum`, so the value of `var'` will pass to next `compute_gradients`. In the next `apply_gradients` step, we should first recover `var` by `var = var + lr_ * momentum_ * accum`. I combine formulas to simplify codes.\n"}