{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/156107388", "pull_request_review_id": 82531423, "id": 156107388, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NjEwNzM4OA==", "diff_hunk": "@@ -0,0 +1,239 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#define EIGEN_USE_THREADS\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+template <typename T>\n+class BroadcastToOp : public OpKernel {\n+ public:\n+  explicit BroadcastToOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor& input_tensor = ctx->input(0);\n+    const TensorShape& input_shape = input_tensor.shape();\n+\n+    const Tensor& shape_tensor = ctx->input(1);\n+\n+    TensorShape output_shape;\n+    OP_REQUIRES_OK(ctx,\n+                   ctx->op_kernel().MakeShape(shape_tensor, &output_shape));\n+\n+    Tensor* output_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output_tensor));\n+\n+#define BROADCAST_SHAPE(broadcast, reshape, NDIMS, input_shape, output_shape) \\\n+  for (int i = 0; i < NDIMS; i++) {                                           \\\n+    OP_REQUIRES(ctx, (broadcast[i] % reshape[i] == 0),                        \\\n+                errors::InvalidArgument(\"invalid shape to broadcast from \",   \\\n+                                        input_shape.DebugString(), \" to \",    \\\n+                                        output_shape.DebugString()));         \\\n+    broadcast[i] = broadcast[i] / reshape[i];                                 \\\n+  }\n+\n+    switch (output_shape.dims()) {\n+      case 1: {\n+        auto reshape = AsEigenDSizesWithPrefix<1>(input_shape);\n+        auto broadcast = output_shape.AsEigenDSizes<1>();\n+\n+        BROADCAST_SHAPE(broadcast, reshape, 1, input_shape, output_shape);\n+\n+        auto output = output_tensor->tensor<T, 1>();\n+        switch (input_shape.dims()) {\n+          case 0: {\n+            output.setConstant(input_tensor.scalar<T>()());\n+          } break;\n+          case 1: {\n+            auto input = input_tensor.tensor<T, 1>();\n+            output = input.broadcast(broadcast);\n+          } break;\n+          default:\n+            ctx->CtxFailure(errors::InvalidArgument(\n+                \"invalid shape to broadcast from \", input_shape.DebugString(),\n+                \" to \", output_shape.DebugString()));\n+            break;\n+        }\n+      } break;\n+      case 2: {\n+        auto reshape = AsEigenDSizesWithPrefix<2>(input_shape);\n+        auto broadcast = output_shape.AsEigenDSizes<2>();\n+\n+        BROADCAST_SHAPE(broadcast, reshape, 2, input_shape, output_shape);\n+\n+        auto output = output_tensor->tensor<T, 2>();\n+        switch (input_shape.dims()) {\n+          case 0: {\n+            output.setConstant(input_tensor.scalar<T>()());\n+          } break;\n+          case 1: {\n+            auto input = input_tensor.tensor<T, 1>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 2: {\n+            auto input = input_tensor.tensor<T, 2>();\n+            output = input.broadcast(broadcast);\n+          } break;\n+          default:\n+            ctx->CtxFailure(errors::InvalidArgument(\n+                \"invalid shape to broadcast from \", input_shape.DebugString(),\n+                \" to \", output_shape.DebugString()));\n+            break;\n+        }\n+      } break;\n+      case 3: {\n+        auto reshape = AsEigenDSizesWithPrefix<3>(input_shape);\n+        auto broadcast = output_shape.AsEigenDSizes<3>();\n+\n+        BROADCAST_SHAPE(broadcast, reshape, 3, input_shape, output_shape);\n+\n+        auto output = output_tensor->tensor<T, 3>();\n+        switch (input_shape.dims()) {\n+          case 0: {\n+            output.setConstant(input_tensor.scalar<T>()());\n+          } break;\n+          case 1: {\n+            auto input = input_tensor.tensor<T, 1>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 2: {\n+            auto input = input_tensor.tensor<T, 2>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 3: {\n+            auto input = input_tensor.tensor<T, 3>();\n+            output = input.broadcast(broadcast);\n+          } break;\n+          default:\n+            ctx->CtxFailure(errors::InvalidArgument(\n+                \"invalid shape to broadcast from \", input_shape.DebugString(),\n+                \" to \", output_shape.DebugString()));\n+            break;\n+        }\n+      } break;\n+      case 4: {\n+        auto reshape = AsEigenDSizesWithPrefix<4>(input_shape);\n+        auto broadcast = output_shape.AsEigenDSizes<4>();\n+\n+        BROADCAST_SHAPE(broadcast, reshape, 4, input_shape, output_shape);\n+\n+        auto output = output_tensor->tensor<T, 4>();\n+        switch (input_shape.dims()) {\n+          case 0: {\n+            output.setConstant(input_tensor.scalar<T>()());\n+          } break;\n+          case 1: {\n+            auto input = input_tensor.tensor<T, 1>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 2: {\n+            auto input = input_tensor.tensor<T, 2>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 3: {\n+            auto input = input_tensor.tensor<T, 3>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 4: {\n+            auto input = input_tensor.tensor<T, 4>();\n+            output = input.broadcast(broadcast);\n+          } break;\n+          default:\n+            ctx->CtxFailure(errors::InvalidArgument(\n+                \"invalid shape to broadcast from \", input_shape.DebugString(),\n+                \" to \", output_shape.DebugString()));\n+            break;\n+        }\n+      } break;\n+      case 5: {\n+        auto reshape = AsEigenDSizesWithPrefix<5>(input_shape);\n+        auto broadcast = output_shape.AsEigenDSizes<5>();\n+\n+        BROADCAST_SHAPE(broadcast, reshape, 5, input_shape, output_shape);\n+\n+        auto output = output_tensor->tensor<T, 5>();\n+        switch (input_shape.dims()) {\n+          case 0: {\n+            output.setConstant(input_tensor.scalar<T>()());\n+          } break;\n+          case 1: {\n+            auto input = input_tensor.tensor<T, 1>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 2: {\n+            auto input = input_tensor.tensor<T, 2>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 3: {\n+            auto input = input_tensor.tensor<T, 3>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 4: {\n+            auto input = input_tensor.tensor<T, 4>();\n+            output = input.reshape(reshape).broadcast(broadcast);\n+          } break;\n+          case 5: {\n+            auto input = input_tensor.tensor<T, 5>();\n+            output = input.broadcast(broadcast);\n+          } break;\n+          default:\n+            ctx->CtxFailure(errors::InvalidArgument(\n+                \"invalid shape to broadcast from \", input_shape.DebugString(),\n+                \" to \", output_shape.DebugString()));\n+            break;\n+        }\n+      } break;\n+      default:\n+        ctx->CtxFailure(errors::InvalidArgument(\n+            \"invalid shape to broadcast from \", input_shape.DebugString(),\n+            \" to \", output_shape.DebugString()));\n+        break;\n+    }\n+  }\n+\n+ private:\n+  template <int NDIMS>\n+  Eigen::DSizes<Eigen::DenseIndex, NDIMS> AsEigenDSizesWithPrefix(\n+      const TensorShape& shape) const {\n+    Eigen::DSizes<Eigen::DenseIndex, NDIMS> dsizes;\n+    for (int d = 0; d < NDIMS - shape.dims(); d++) {\n+      dsizes[d] = 1;\n+    }\n+    for (int d = NDIMS - shape.dims(); d < NDIMS; d++) {\n+      dsizes[d] = shape.dim_size(d - (NDIMS - shape.dims()));\n+    }\n+    return dsizes;\n+  }\n+};\n+\n+// As MakeShape is able to handle both DT_INT32 and DT_INT64,\n+// no need to have TypeConstraint for `Tidx`\n+#define REGISTER_KERNEL(type)                                           \\", "path": "tensorflow/core/kernels/broadcast_to_op.cc", "position": 57, "original_position": 231, "commit_id": "9ec1539c35a0b94afdc89b2fb96a6cd856b962f8", "original_commit_id": "eb357a10acd255414067568e18ed5b52c5615e11", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "body": "Register a GPU kernel as well? That might require adding a .device() before the eigen call to .broadcast but otherwise should be easy.", "created_at": "2017-12-11T15:36:40Z", "updated_at": "2018-04-12T01:49:10Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/15243#discussion_r156107388", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15243", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/156107388"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/15243#discussion_r156107388"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15243"}}, "body_html": "<p>Register a GPU kernel as well? That might require adding a .device() before the eigen call to .broadcast but otherwise should be easy.</p>", "body_text": "Register a GPU kernel as well? That might require adding a .device() before the eigen call to .broadcast but otherwise should be easy."}