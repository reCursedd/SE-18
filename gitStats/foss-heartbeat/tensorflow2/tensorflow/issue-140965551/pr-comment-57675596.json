{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/57675596", "pull_request_review_id": null, "id": 57675596, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3Njc1NTk2", "diff_hunk": "@@ -0,0 +1,190 @@\n+# Copyright 2015 Google Inc. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"Various learning rate decay functions.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import constant_op\n+\n+\n+def exponential_time_decay(initial_lr, epoch, decay_rate, name=None):\n+  \"\"\"Applies exponential time decay to the initial learning rate.\n+\n+  When training a model, it is often recommended to lower the learning rate as\n+  the training progresses.  This function applies an exponential decay function\n+  to a provided initial learning rate.  It requires an `epoch` value to compute\n+  the decayed learning rate.  You can just pass a TensorFlow variable that you\n+  increment at each training step.\n+\n+  The function returns the decayed learning rate.  It is computed as:\n+\n+  ```python\n+  decayed_learning_rate = initial_lr * exp(-decay_rate * epoch)\n+  ```\n+\n+  Example: decay exponetially with a base of 0.96:\n+\n+  ```python\n+  ...\n+  epoch = tf.Variable(0, trainable=False)\n+  initial_lr = 0.1\n+  k = 0.5\n+  learning_rate = tf.train.exponential_time_decay(initial_lr, epoch, k)\n+\n+  # Passing epoch to minimize() will increment it at each step.\n+  learning_step = (\n+      tf.GradientDescentOptimizer(learning_rate)\n+      .minimize(...my loss..., global_step=epoch)\n+  )\n+  ```\n+\n+  Args:\n+    initial_lr: A scalar `float32` or `float64` `Tensor` or a\n+      Python number.  The initial learning rate.\n+    epoch: A Python number.\n+      Global step to use for the decay computation.  Must not be negative.\n+    decay_rate: A Python number.  The decay rate.\n+    name: String.  Optional name of the operation.  Defaults to\n+      'ExponentialTimeDecay'\n+\n+  Returns:\n+    A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n+    learning rate.\n+  \"\"\"\n+  with ops.op_scope([initial_lr, epoch, decay_rate],\n+                    name, \"ExponentialTimeDecay\") as name:\n+    initial_lr = ops.convert_to_tensor(initial_lr, name=\"learning_rate\")\n+    decay_rate = math_ops.cast(decay_rate, initial_lr.dtype)\n+    exponent = math_ops.exp(math_ops.mul(math_ops.neg(decay_rate), epoch))", "path": "tensorflow/python/training/initial_learning_rate_decay.py", "position": 75, "original_position": 75, "commit_id": "d3baec908a620e6a1b1af20e63af329815d4e260", "original_commit_id": "d3baec908a620e6a1b1af20e63af329815d4e260", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "body": "after more closely looking at the differences, I realize that all of these functions have a 'staircase' effect: they only drop after each epoch.\n\nI think it might be worth looking at how it is done in exponential_decay:\n- You always pass the global_step and the 'decay steps' (essentially the number of steps in an epoch)\n- If staircase is True, then you take the floor of global_step / decay_steps when computing the equivalent of 'epoch' here\n- If staircase is False, then you use global_step / decay_steps as a floating point value, so you have smooth behavior.  That way all of the interfaces to the learning rate functions are basically identical, they each take an initial learning rate, an global step value, and a decay_steps. \n\nIn fact, where possible, it might be preferable to keep the signature (the names of the variables) the same as in exponential_decay.  Does this seem reasonable?\n", "created_at": "2016-03-29T06:06:28Z", "updated_at": "2016-03-29T06:06:28Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/1512#discussion_r57675596", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1512", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/57675596"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/1512#discussion_r57675596"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1512"}}, "body_html": "<p>after more closely looking at the differences, I realize that all of these functions have a 'staircase' effect: they only drop after each epoch.</p>\n<p>I think it might be worth looking at how it is done in exponential_decay:</p>\n<ul>\n<li>You always pass the global_step and the 'decay steps' (essentially the number of steps in an epoch)</li>\n<li>If staircase is True, then you take the floor of global_step / decay_steps when computing the equivalent of 'epoch' here</li>\n<li>If staircase is False, then you use global_step / decay_steps as a floating point value, so you have smooth behavior.  That way all of the interfaces to the learning rate functions are basically identical, they each take an initial learning rate, an global step value, and a decay_steps.</li>\n</ul>\n<p>In fact, where possible, it might be preferable to keep the signature (the names of the variables) the same as in exponential_decay.  Does this seem reasonable?</p>", "body_text": "after more closely looking at the differences, I realize that all of these functions have a 'staircase' effect: they only drop after each epoch.\nI think it might be worth looking at how it is done in exponential_decay:\n\nYou always pass the global_step and the 'decay steps' (essentially the number of steps in an epoch)\nIf staircase is True, then you take the floor of global_step / decay_steps when computing the equivalent of 'epoch' here\nIf staircase is False, then you use global_step / decay_steps as a floating point value, so you have smooth behavior.  That way all of the interfaces to the learning rate functions are basically identical, they each take an initial learning rate, an global step value, and a decay_steps.\n\nIn fact, where possible, it might be preferable to keep the signature (the names of the variables) the same as in exponential_decay.  Does this seem reasonable?"}