{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/57676025", "pull_request_review_id": null, "id": 57676025, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3Njc2MDI1", "diff_hunk": "@@ -0,0 +1,190 @@\n+# Copyright 2015 Google Inc. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"Various learning rate decay functions.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import constant_op\n+\n+\n+def exponential_time_decay(initial_lr, epoch, decay_rate, name=None):\n+  \"\"\"Applies exponential time decay to the initial learning rate.\n+\n+  When training a model, it is often recommended to lower the learning rate as\n+  the training progresses.  This function applies an exponential decay function\n+  to a provided initial learning rate.  It requires an `epoch` value to compute\n+  the decayed learning rate.  You can just pass a TensorFlow variable that you\n+  increment at each training step.\n+\n+  The function returns the decayed learning rate.  It is computed as:\n+\n+  ```python\n+  decayed_learning_rate = initial_lr * exp(-decay_rate * epoch)\n+  ```\n+\n+  Example: decay exponetially with a base of 0.96:\n+\n+  ```python\n+  ...\n+  epoch = tf.Variable(0, trainable=False)\n+  initial_lr = 0.1\n+  k = 0.5\n+  learning_rate = tf.train.exponential_time_decay(initial_lr, epoch, k)\n+\n+  # Passing epoch to minimize() will increment it at each step.\n+  learning_step = (\n+      tf.GradientDescentOptimizer(learning_rate)\n+      .minimize(...my loss..., global_step=epoch)\n+  )\n+  ```\n+\n+  Args:\n+    initial_lr: A scalar `float32` or `float64` `Tensor` or a\n+      Python number.  The initial learning rate.\n+    epoch: A Python number.\n+      Global step to use for the decay computation.  Must not be negative.\n+    decay_rate: A Python number.  The decay rate.\n+    name: String.  Optional name of the operation.  Defaults to\n+      'ExponentialTimeDecay'\n+\n+  Returns:\n+    A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n+    learning rate.\n+  \"\"\"\n+  with ops.op_scope([initial_lr, epoch, decay_rate],\n+                    name, \"ExponentialTimeDecay\") as name:\n+    initial_lr = ops.convert_to_tensor(initial_lr, name=\"learning_rate\")\n+    decay_rate = math_ops.cast(decay_rate, initial_lr.dtype)\n+    exponent = math_ops.exp(math_ops.mul(math_ops.neg(decay_rate), epoch))", "path": "tensorflow/python/training/initial_learning_rate_decay.py", "position": 75, "original_position": 75, "commit_id": "d3baec908a620e6a1b1af20e63af329815d4e260", "original_commit_id": "d3baec908a620e6a1b1af20e63af329815d4e260", "user": {"login": "chemelnucfin", "id": 3982092, "node_id": "MDQ6VXNlcjM5ODIwOTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3982092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chemelnucfin", "html_url": "https://github.com/chemelnucfin", "followers_url": "https://api.github.com/users/chemelnucfin/followers", "following_url": "https://api.github.com/users/chemelnucfin/following{/other_user}", "gists_url": "https://api.github.com/users/chemelnucfin/gists{/gist_id}", "starred_url": "https://api.github.com/users/chemelnucfin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chemelnucfin/subscriptions", "organizations_url": "https://api.github.com/users/chemelnucfin/orgs", "repos_url": "https://api.github.com/users/chemelnucfin/repos", "events_url": "https://api.github.com/users/chemelnucfin/events{/privacy}", "received_events_url": "https://api.github.com/users/chemelnucfin/received_events", "type": "User", "site_admin": false}, "body": "Yes.  I was just on my way to naming them consistently.  Thanks for your\ntime in reviewing.\n\nOn Mon, Mar 28, 2016, 11:08 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> In tensorflow/python/training/initial_learning_rate_decay.py\n> https://github.com/tensorflow/tensorflow/pull/1512#discussion_r57675596:\n> \n> > -      Python number.  The initial learning rate.\n> > -    epoch: A Python number.\n> > -      Global step to use for the decay computation.  Must not be negative.\n> > -    decay_rate: A Python number.  The decay rate.\n> > -    name: String.  Optional name of the operation.  Defaults to\n> > -      'ExponentialTimeDecay'\n> >   +\n> > -  Returns:\n> > -    A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n> > -    learning rate.\n> > -  \"\"\"\n> > -  with ops.op_scope([initial_lr, epoch, decay_rate],\n> > -                    name, \"ExponentialTimeDecay\") as name:\n> > -    initial_lr = ops.convert_to_tensor(initial_lr, name=\"learning_rate\")\n> > -    decay_rate = math_ops.cast(decay_rate, initial_lr.dtype)\n> > -    exponent = math_ops.exp(math_ops.mul(math_ops.neg(decay_rate), epoch))\n> \n> after more closely looking at the differences, I realize that all of these\n> functions have a 'staircase' effect: they only drop after each epoch.\n> \n> I think it might be worth looking at how it is done in exponential_decay:\n> - You always pass the global_step and the 'decay steps' (essentially\n>   the number of steps in an epoch)\n> - If staircase is True, then you take the floor of global_step /\n>   decay_steps when computing the equivalent of 'epoch' here\n> - If staircase is False, then you use global_step / decay_steps as a\n>   floating point value, so you have smooth behavior. That way all of the\n>   interfaces to the learning rate functions are basically identical, they\n>   each take an initial learning rate, an global step value, and a\n>   decay_steps.\n> \n> In fact, where possible, it might be preferable to keep the signature (the\n> names of the variables) the same as in exponential_decay. Does this seem\n> reasonable?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1512/files/d3baec908a620e6a1b1af20e63af329815d4e260#r57675596\n", "created_at": "2016-03-29T06:12:35Z", "updated_at": "2016-03-29T06:12:35Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/1512#discussion_r57676025", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1512", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/57676025"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/1512#discussion_r57676025"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1512"}}, "body_html": "<p>Yes.  I was just on my way to naming them consistently.  Thanks for your<br>\ntime in reviewing.</p>\n<p>On Mon, Mar 28, 2016, 11:08 PM Vijay Vasudevan <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>In tensorflow/python/training/initial_learning_rate_decay.py<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"140965551\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1512\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/1512/hovercard?comment_id=57675596&amp;comment_type=review_comment\" href=\"https://github.com/tensorflow/tensorflow/pull/1512#discussion_r57675596\">#1512 (comment)</a>:</p>\n<blockquote>\n<ul>\n<li>\n<pre><code> Python number.  The initial learning rate.\n</code></pre>\n</li>\n<li>epoch: A Python number.</li>\n<li>\n<pre><code> Global step to use for the decay computation.  Must not be negative.\n</code></pre>\n</li>\n<li>decay_rate: A Python number.  The decay rate.</li>\n<li>name: String.  Optional name of the operation.  Defaults to</li>\n<li>\n<pre><code> 'ExponentialTimeDecay'\n</code></pre>\n<ul>\n<li></li>\n</ul>\n</li>\n<li>Returns:</li>\n<li>A scalar <code>Tensor</code> of the same type as <code>learning_rate</code>.  The decayed</li>\n<li>learning rate.</li>\n<li>\"\"\"</li>\n<li>with ops.op_scope([initial_lr, epoch, decay_rate],</li>\n<li>\n<pre><code>               name, \"ExponentialTimeDecay\") as name:\n</code></pre>\n</li>\n<li>initial_lr = ops.convert_to_tensor(initial_lr, name=\"learning_rate\")</li>\n<li>decay_rate = math_ops.cast(decay_rate, initial_lr.dtype)</li>\n<li>exponent = math_ops.exp(math_ops.mul(math_ops.neg(decay_rate), epoch))</li>\n</ul>\n</blockquote>\n<p>after more closely looking at the differences, I realize that all of these<br>\nfunctions have a 'staircase' effect: they only drop after each epoch.</p>\n<p>I think it might be worth looking at how it is done in exponential_decay:</p>\n<ul>\n<li>You always pass the global_step and the 'decay steps' (essentially<br>\nthe number of steps in an epoch)</li>\n<li>If staircase is True, then you take the floor of global_step /<br>\ndecay_steps when computing the equivalent of 'epoch' here</li>\n<li>If staircase is False, then you use global_step / decay_steps as a<br>\nfloating point value, so you have smooth behavior. That way all of the<br>\ninterfaces to the learning rate functions are basically identical, they<br>\neach take an initial learning rate, an global step value, and a<br>\ndecay_steps.</li>\n</ul>\n<p>In fact, where possible, it might be preferable to keep the signature (the<br>\nnames of the variables) the same as in exponential_decay. Does this seem<br>\nreasonable?</p>\n<p>\u2014<br>\nYou are receiving this because you authored the thread.<br>\nReply to this email directly or view it on GitHub<br>\n<a href=\"https://github.com/tensorflow/tensorflow/pull/1512/files/d3baec908a620e6a1b1af20e63af329815d4e260#r57675596\">https://github.com/tensorflow/tensorflow/pull/1512/files/d3baec908a620e6a1b1af20e63af329815d4e260#r57675596</a></p>\n</blockquote>", "body_text": "Yes.  I was just on my way to naming them consistently.  Thanks for your\ntime in reviewing.\nOn Mon, Mar 28, 2016, 11:08 PM Vijay Vasudevan notifications@github.com\nwrote:\n\nIn tensorflow/python/training/initial_learning_rate_decay.py\n#1512 (comment):\n\n\n\n Python number.  The initial learning rate.\n\n\nepoch: A Python number.\n\n Global step to use for the decay computation.  Must not be negative.\n\n\ndecay_rate: A Python number.  The decay rate.\nname: String.  Optional name of the operation.  Defaults to\n\n 'ExponentialTimeDecay'\n\n\n\n\n\nReturns:\nA scalar Tensor of the same type as learning_rate.  The decayed\nlearning rate.\n\"\"\"\nwith ops.op_scope([initial_lr, epoch, decay_rate],\n\n               name, \"ExponentialTimeDecay\") as name:\n\n\ninitial_lr = ops.convert_to_tensor(initial_lr, name=\"learning_rate\")\ndecay_rate = math_ops.cast(decay_rate, initial_lr.dtype)\nexponent = math_ops.exp(math_ops.mul(math_ops.neg(decay_rate), epoch))\n\n\nafter more closely looking at the differences, I realize that all of these\nfunctions have a 'staircase' effect: they only drop after each epoch.\nI think it might be worth looking at how it is done in exponential_decay:\n\nYou always pass the global_step and the 'decay steps' (essentially\nthe number of steps in an epoch)\nIf staircase is True, then you take the floor of global_step /\ndecay_steps when computing the equivalent of 'epoch' here\nIf staircase is False, then you use global_step / decay_steps as a\nfloating point value, so you have smooth behavior. That way all of the\ninterfaces to the learning rate functions are basically identical, they\neach take an initial learning rate, an global step value, and a\ndecay_steps.\n\nIn fact, where possible, it might be preferable to keep the signature (the\nnames of the variables) the same as in exponential_decay. Does this seem\nreasonable?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/tensorflow/tensorflow/pull/1512/files/d3baec908a620e6a1b1af20e63af329815d4e260#r57675596"}