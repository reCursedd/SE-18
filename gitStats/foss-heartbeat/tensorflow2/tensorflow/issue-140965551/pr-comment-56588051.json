{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/56588051", "pull_request_review_id": null, "id": 56588051, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NTg4MDUx", "diff_hunk": "@@ -77,10 +78,173 @@ def exponential_decay(learning_rate, global_step, decay_steps, decay_rate,\n                    name, \"ExponentialDecay\") as name:\n     learning_rate = ops.convert_to_tensor(learning_rate, name=\"learning_rate\")\n     dtype = learning_rate.dtype\n-    global_step = math_ops.cast(global_step, dtype)\n-    decay_steps = math_ops.cast(decay_steps, dtype)\n-    decay_rate = math_ops.cast(decay_rate, dtype)\n+    global_step = cast(global_step, dtype)\n+    decay_steps = cast(decay_steps, dtype)\n+    decay_rate = cast(decay_rate, dtype)\n     p = global_step / decay_steps\n     if staircase:\n-      p = math_ops.floor(p)\n-    return math_ops.mul(learning_rate, math_ops.pow(decay_rate, p), name=name)\n+      p = floor(p)\n+    return mul(learning_rate, pow(decay_rate, p), name=name)\n+\n+def exponential_time_decay(initial_lr, epoch, decay_rate, name=None):\n+  \"\"\"Applies exponential decay to the learning rate.\n+\n+  When training a model, it is often recommended to lower the learning rate as\n+  the training progresses.  This function applies an exponential decay function\n+  to a provided initial learning rate.  It requires an `epoch` to  compute the\n+  decayed learning rate.  You can just pass a TensorFlow variable that you\n+  increment at each training step.\n+\n+  The function returns the decayed learning rate.  It is computed as:\n+\n+  ```python\n+  decayed_learning_rate = initial_lr * exp(-decay_rate * epoch)\n+  ```\n+\n+  Example: decay exponetially with a base of 0.96:\n+\n+  ```python\n+  ...\n+  epoch = tf.Variable(0, trainable=False)\n+  initial_lr = 0.1\n+  decay_rate = 0.5\n+  learning_rate = tf.train.exponential_time_decay(initial_lr, epoch, k)\n+\n+  # Passing epoch to minimize() will increment it at each step.\n+  learning_step = (\n+      tf.GradientDescentOptimizer(learning_rate)\n+      .minimize(...my loss..., global_step=epoch)\n+  )\n+  ```\n+\n+  Args:\n+    initial_lr: A scalar `float32` or `float64` `Tensor` or a\n+      Python number.  The initial learning rate.\n+    epoch: A Python number.\n+      Global step to use for the decay computation.  Must not be negative.\n+    decay_rate: A Python number.  The decay rate.\n+    name: String.  Optional name of the operation.  Defaults to\n+      'ExponentialTimeDecay'\n+\n+  Returns:\n+    A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n+    learning rate.\n+  \"\"\"\n+  with ops.op_scope([initial_lr, epoch, decay_rate],\n+                   name, \"ExponentialTimeDecay\") as name:\n+    initial_lr = ops.convert_to_tensor(initial_lr, name=\"learning_rate\")\n+    decay_rate = cast(decay_rate, initial_lr.dtype)\n+    exponent = exp(mul(neg(decay_rate), epoch))\n+    return mul(initial_lr, exponent, name=name)\n+\n+def inverse_time_decay(initial_lr, epoch, decay_rate, name=None):\n+  \"\"\"Applies inverse time decay to the learning rate.\n+\n+  When training a model, it is often recommended to lower the learning rate as\n+  the training progresses.  This function applies an inverse decay function\n+  to a provided initial learning rate.  It requires an `epoch` value to\n+  compute the decayed learning rate.  You can just pass a TensorFlow variable\n+  that you increment at each training step.\n+\n+  The function returns the decayed learning rate.  It is computed as:\n+\n+  ```python\n+  decayed_learning_rate = learning_rate / (1 + decay_rate * t)\n+  ```\n+\n+  Example: decay 1/t with a rate of 0.5:\n+\n+  ```python\n+  ...\n+  epoch = tf.Variable(0, trainable=False)\n+  initial_lr = 0.1\n+  decay_rate = 0.5\n+  learning_rate = tf.train.inverse_time_decay(initial_lr, epoch, k)\n+\n+  # Passing epoch to minimize() will increment it at each step.\n+  learning_step = (\n+      tf.GradientDescentOptimizer(learning_rate)\n+      .minimize(...my loss..., global_step=epoch)\n+  )\n+  ```\n+\n+  Args:\n+    initial_lr: A scalar `float32` or `float64` `Tensor` or a\n+      Python number.  The initial learning rate.\n+    epoch: A Python number.\n+      Global step to use for the decay computation.  Must not be negative.\n+    decay_rate: A Python number.  The decay rate.\n+    name: String.  Optional name of the operation.  Defaults to\n+      'ExponentialTimeDecay'\n+\n+  Returns:\n+    A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n+    learning rate.\n+  \"\"\"\n+\n+  with ops.op_scope([initial_lr, epoch, decay_rate],\n+                   name, \"InverseTimeDecay\") as name:\n+    initial_lr = ops.convert_to_tensor(initial_lr, name=\"learning_rate\")\n+    decay_rate = cast(decay_rate, initial_lr.dtype)\n+    const = cast(constant(1), initial_lr.dtype)\n+    denom = add(const, mul(decay_rate, epoch))\n+    return div(initial_lr, denom, name=name)\n+\n+def step_time_decay(learning_rate, epoch, num_steps_for_decay, decay_rate=0.5,\n+               name=None):\n+  \"\"\"Applies step decay to the learning rate.\n+\n+  When training a model, it is often recommended to lower the learning rate as\n+  the training progresses.  This function applies a step decay function\n+  to a provided initial learning rate.  It requires a `global_step` value to\n+  compute the decayed learning rate.  You can just pass a python number\n+  that you increment at each training step.\n+\n+  The function returns the decayed learning rate.  It is computed as:\n+\n+  ```python\n+  if global_step and global_step % num_steps_for_decay == 0:\n+    decayed_learning_rate = learning_rate * decay_rate\n+  ```\n+  ```\n+\n+  Example: decay every 1000 steps with a base of 0.5:\n+\n+  ```python\n+  ...\n+  global_step = tf.Variable(0, trainable=False)\n+  starter_learning_rate = 0.1\n+  learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n+                                             1000, 0.5)\n+  # Passing global_step to minimize() will increment it at each step.\n+  learning_step = (\n+      tf.GradientDescentOptimizer(learning_rate)\n+      .minimize(...my loss..., global_step=global_step)\n+  )\n+  ```\n+\n+  Args:\n+    learning_rate: A scalar `float32` or `float64` `Tensor` or a\n+      Python number.  The initial learning rate.\n+    global_step: A Python number.\n+      Global step to use for the decay computation.  Must not be negative.\n+    num_steps_for_decay: A scalar `int32` or `int64` `Tensor` or a Python\n+      number.\n+      Must be positive.  See the decay computation above.\n+    decay_rate: A scalar `float32` or `float64` `Tensor` or a\n+      Python number.  The decay rate.\n+    name: String.  Optional name of the operation.  Defaults to 'StepDecay'\n+\n+  Returns:\n+    A scalar `Tensor` of the same type as `learning_rate`.  The decayed\n+    learning rate.\n+  \"\"\"\n+  with ops.op_scope([learning_rate, epoch, num_steps_for_decay, decay_rate],\n+                   name, \"StepTimeDecay\") as name:\n+    learning_rate = ops.convert_to_tensor(learning_rate, name=\"learning_rate\")\n+    dtype = learning_rate.dtype\n+    decay_rate = cast(decay_rate, dtype)\n+    if mod(epoch, num_steps_for_decay).eval():", "path": "tensorflow/python/training/learning_rate_decay.py", "position": null, "original_position": 185, "commit_id": "d3baec908a620e6a1b1af20e63af329815d4e260", "original_commit_id": "6d04864e44a892fb109da29654a226fe8900a7fa", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "body": "Functions shouldn't require an eval in them to work -- can you express this symbolically?\n", "created_at": "2016-03-17T22:26:23Z", "updated_at": "2016-03-28T19:57:51Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/1512#discussion_r56588051", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1512", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/56588051"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/1512#discussion_r56588051"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1512"}}, "body_html": "<p>Functions shouldn't require an eval in them to work -- can you express this symbolically?</p>", "body_text": "Functions shouldn't require an eval in them to work -- can you express this symbolically?"}