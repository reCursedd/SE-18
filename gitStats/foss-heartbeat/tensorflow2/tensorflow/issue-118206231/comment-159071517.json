{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/159071517", "html_url": "https://github.com/tensorflow/tensorflow/issues/317#issuecomment-159071517", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/317", "id": 159071517, "node_id": "MDEyOklzc3VlQ29tbWVudDE1OTA3MTUxNw==", "user": {"login": "dave-andersen", "id": 827870, "node_id": "MDQ6VXNlcjgyNzg3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/827870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dave-andersen", "html_url": "https://github.com/dave-andersen", "followers_url": "https://api.github.com/users/dave-andersen/followers", "following_url": "https://api.github.com/users/dave-andersen/following{/other_user}", "gists_url": "https://api.github.com/users/dave-andersen/gists{/gist_id}", "starred_url": "https://api.github.com/users/dave-andersen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dave-andersen/subscriptions", "organizations_url": "https://api.github.com/users/dave-andersen/orgs", "repos_url": "https://api.github.com/users/dave-andersen/repos", "events_url": "https://api.github.com/users/dave-andersen/events{/privacy}", "received_events_url": "https://api.github.com/users/dave-andersen/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-23T21:30:20Z", "updated_at": "2015-11-23T21:36:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I propose this as a revised \"first view of tensorflow\" - given that it's just supposed to introduce some very high-level comments and not be a tutorial of any sort:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3                                                                                                                                      \n\nx_data = np.random.rand(100).astype(\"float32\")\ny_data = x_data * 0.1 + 0.3\n\n# Construct a linear model with weight W and bias b, to predict y given x.                                                                                                                           \nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nb = tf.Variable(tf.zeros([1]))\ny = W * x_data + b\n\n# Minimize the mean squared errors.                                                                                                                                                                  \nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss)\n\n# Before starting, initialize the variables.  We will 'run' this first.                                                                                                                              \ninit = tf.initialize_all_variables()\n\n# Launch the graph.                                                                                                                                                                                  \nsess = tf.Session()\nsess.run(init)\n\n# Fit the line.                                                                                                                                                                                      \nfor step in xrange(201):\n    sess.run(train)\n    if step % 20 == 0:\n        print step, sess.run(W), sess.run(b)\n\n# Learns best fit is W: [0.1], b: [0.3]                    \n</code></pre>\n<p>Suggestions before I make a CL for this?</p>\n<p>(Note that I'm specifically not making a neural network here because it introduces even more syntax and complexity -- if someone is feeling confused by W and b, they're going to boggle about seeing tf.conv2d and its associated parameters.  This gets people off the ground with training a trivial model, and they can hopefully then move on to the for-newbies MNIST example for a convnet)</p>", "body_text": "I propose this as a revised \"first view of tensorflow\" - given that it's just supposed to introduce some very high-level comments and not be a tutorial of any sort:\nimport tensorflow as tf\nimport numpy as np\n\n# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3                                                                                                                                      \n\nx_data = np.random.rand(100).astype(\"float32\")\ny_data = x_data * 0.1 + 0.3\n\n# Construct a linear model with weight W and bias b, to predict y given x.                                                                                                                           \nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nb = tf.Variable(tf.zeros([1]))\ny = W * x_data + b\n\n# Minimize the mean squared errors.                                                                                                                                                                  \nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss)\n\n# Before starting, initialize the variables.  We will 'run' this first.                                                                                                                              \ninit = tf.initialize_all_variables()\n\n# Launch the graph.                                                                                                                                                                                  \nsess = tf.Session()\nsess.run(init)\n\n# Fit the line.                                                                                                                                                                                      \nfor step in xrange(201):\n    sess.run(train)\n    if step % 20 == 0:\n        print step, sess.run(W), sess.run(b)\n\n# Learns best fit is W: [0.1], b: [0.3]                    \n\nSuggestions before I make a CL for this?\n(Note that I'm specifically not making a neural network here because it introduces even more syntax and complexity -- if someone is feeling confused by W and b, they're going to boggle about seeing tf.conv2d and its associated parameters.  This gets people off the ground with training a trivial model, and they can hopefully then move on to the for-newbies MNIST example for a convnet)", "body": "I propose this as a revised \"first view of tensorflow\" - given that it's just supposed to introduce some very high-level comments and not be a tutorial of any sort:\n\n```\nimport tensorflow as tf\nimport numpy as np\n\n# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3                                                                                                                                      \n\nx_data = np.random.rand(100).astype(\"float32\")\ny_data = x_data * 0.1 + 0.3\n\n# Construct a linear model with weight W and bias b, to predict y given x.                                                                                                                           \nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nb = tf.Variable(tf.zeros([1]))\ny = W * x_data + b\n\n# Minimize the mean squared errors.                                                                                                                                                                  \nloss = tf.reduce_mean(tf.square(y - y_data))\noptimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = optimizer.minimize(loss)\n\n# Before starting, initialize the variables.  We will 'run' this first.                                                                                                                              \ninit = tf.initialize_all_variables()\n\n# Launch the graph.                                                                                                                                                                                  \nsess = tf.Session()\nsess.run(init)\n\n# Fit the line.                                                                                                                                                                                      \nfor step in xrange(201):\n    sess.run(train)\n    if step % 20 == 0:\n        print step, sess.run(W), sess.run(b)\n\n# Learns best fit is W: [0.1], b: [0.3]                    \n```\n\nSuggestions before I make a CL for this?\n\n(Note that I'm specifically not making a neural network here because it introduces even more syntax and complexity -- if someone is feeling confused by W and b, they're going to boggle about seeing tf.conv2d and its associated parameters.  This gets people off the ground with training a trivial model, and they can hopefully then move on to the for-newbies MNIST example for a convnet)\n"}