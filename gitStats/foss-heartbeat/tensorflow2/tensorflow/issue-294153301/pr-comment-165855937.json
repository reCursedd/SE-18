{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/165855937", "pull_request_review_id": 93856307, "id": 165855937, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NTg1NTkzNw==", "diff_hunk": "@@ -3017,3 +3018,170 @@ def call(self, inputs, state):\n \n       new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)\n       return new_h, new_state\n+\n+\n+class NLSTMCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Nested LSTM Cell. Adapted from `rnn_cell_impl.LSTMCell`\n+\n+  The implementation is based on:\n+    https://arxiv.org/abs/1801.10308\n+    JRA. Moniz, D. Krueger.\n+    \"Nested LSTMs\"\n+    ACML, PMLR 77:530-544, 2017\n+  \"\"\"\n+\n+  def __init__(self, num_units, depth, forget_bias=1.0,\n+               state_is_tuple=True, activation=None, reuse=None, name=None):\n+    \"\"\"Initialize the basic NLSTM cell.\n+\n+    Args:\n+      num_units: int, The number of hidden units of each cell state\n+        and hidden state.\n+      depth: int, The number of layers in the nest\n+      forget_bias: float, The bias added to forget gates.\n+      state_is_tuple: If True, accepted and returned states are tuples of\n+        the `h_state` and `c_state`s.  If False, they are concatenated\n+        along the column axis.  The latter behavior will soon be deprecated.\n+      activation: Activation function of the inner states.  Default: `tanh`.\n+      reuse: (optional) Python boolean describing whether to reuse variables\n+        in an existing scope.  If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n+      name: String, the name of the layer. Layers with the same name will\n+        share weights, but to avoid mistakes we require reuse=True in such\n+        cases.\n+    \"\"\"\n+    super(NLSTMCell, self).__init__(_reuse=reuse, name=name)\n+    if not state_is_tuple:\n+      logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n+                   \"deprecated.  Use state_is_tuple=True.\", self)\n+\n+    # Inputs must be 2-dimensional.\n+    self.input_spec = base_layer.InputSpec(ndim=2)\n+    self._num_units = num_units\n+    self._forget_bias = forget_bias\n+    self._state_is_tuple = state_is_tuple\n+    self._depth = depth\n+    self._activation = activation or math_ops.tanh\n+    self._kernels = None\n+    self._biases = None\n+    self.built = False\n+\n+  @property\n+  def state_size(self):\n+    if self._state_is_tuple:\n+      return tuple([self._num_units] * (self.depth + 1))\n+    else:\n+      return self._num_units * (self.depth + 1)\n+\n+  @property\n+  def output_size(self):\n+    return self._num_units\n+\n+  @property\n+  def depth(self):\n+    return self._depth\n+\n+  def build(self, inputs_shape):\n+    if inputs_shape[1].value is None:\n+      raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\n+                       % inputs_shape)\n+\n+    input_depth = inputs_shape[1].value\n+    h_depth = self._num_units\n+    self._kernels = []\n+    self._biases = []\n+    for i in range(self.depth):\n+      if i == 0:\n+        self._kernels.append(\n+          self.add_variable(\n+            \"kernel_{}\".format(i),\n+            shape=[input_depth + h_depth, 4 * self._num_units]))\n+      else:\n+        self._kernels.append(\n+          self.add_variable(\n+            \"kernel_{}\".format(i),\n+            shape=[2 * h_depth, 4 * self._num_units]))\n+      self._biases.append(\n+        self.add_variable(\n+          \"bias_{}\".format(i),\n+          shape=[4 * self._num_units],\n+          initializer=init_ops.zeros_initializer(dtype=self.dtype)))\n+\n+    self.built = True\n+\n+  def recurrence(self, inputs, hidden_state, cell_states, depth):\n+    \"\"\"use recurrence to traverse the nested structure\n+\n+    Args:\n+      inputs: a 2D `Tensor` of [batch_size x input_size] shape\n+      hidden_state: a 2D `Tensor` of [batch_size x num_units] shape\n+      cell_states: a `list` of 2D `Tensor` of [batch_size x num_units] shape\n+      depth: `int`\n+        the current depth in the nested structure, begins at 0.\n+\n+    Return:\n+      new_h: a 2D `Tensor` of [batch_size x num_units] shape\n+        the latest hidden state for current step\n+      new_cs: a `list` of 2D `Tensor` of [batch_size x num_units] shape\n+        the accumulated cell states for current step\n+    \"\"\"\n+    sigmoid = math_ops.sigmoid", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 119, "commit_id": "492c8c708017e279e449bc0857ec3e28452c287c", "original_commit_id": "17aab7396efb34c3b0daf6ffee3df1af9953566b", "user": {"login": "hannw", "id": 4380470, "node_id": "MDQ6VXNlcjQzODA0NzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/4380470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hannw", "html_url": "https://github.com/hannw", "followers_url": "https://api.github.com/users/hannw/followers", "following_url": "https://api.github.com/users/hannw/following{/other_user}", "gists_url": "https://api.github.com/users/hannw/gists{/gist_id}", "starred_url": "https://api.github.com/users/hannw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hannw/subscriptions", "organizations_url": "https://api.github.com/users/hannw/orgs", "repos_url": "https://api.github.com/users/hannw/repos", "events_url": "https://api.github.com/users/hannw/events{/privacy}", "received_events_url": "https://api.github.com/users/hannw/received_events", "type": "User", "site_admin": false}, "body": "I think this is a judgement call between speed and readability. This pattern appeared in the original BasicLSTM class. The \".\" notation is basically dictionary access, so this pattern is a few less dictionary access and slightly faster. Can change to \".\" pattern if you prefer.", "created_at": "2018-02-04T20:10:45Z", "updated_at": "2018-02-12T00:40:47Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16736#discussion_r165855937", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16736", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/165855937"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16736#discussion_r165855937"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16736"}}, "body_html": "<p>I think this is a judgement call between speed and readability. This pattern appeared in the original BasicLSTM class. The \".\" notation is basically dictionary access, so this pattern is a few less dictionary access and slightly faster. Can change to \".\" pattern if you prefer.</p>", "body_text": "I think this is a judgement call between speed and readability. This pattern appeared in the original BasicLSTM class. The \".\" notation is basically dictionary access, so this pattern is a few less dictionary access and slightly faster. Can change to \".\" pattern if you prefer.", "in_reply_to_id": 165853638}