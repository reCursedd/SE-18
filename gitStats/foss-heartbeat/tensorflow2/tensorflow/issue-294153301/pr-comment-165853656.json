{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/165853656", "pull_request_review_id": 93853790, "id": 165853656, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NTg1MzY1Ng==", "diff_hunk": "@@ -3017,3 +3018,170 @@ def call(self, inputs, state):\n \n       new_state = rnn_cell_impl.LSTMStateTuple(new_c, new_h)\n       return new_h, new_state\n+\n+\n+class NLSTMCell(rnn_cell_impl.RNNCell):\n+  \"\"\"Nested LSTM Cell. Adapted from `rnn_cell_impl.LSTMCell`\n+\n+  The implementation is based on:\n+    https://arxiv.org/abs/1801.10308\n+    JRA. Moniz, D. Krueger.\n+    \"Nested LSTMs\"\n+    ACML, PMLR 77:530-544, 2017\n+  \"\"\"\n+\n+  def __init__(self, num_units, depth, forget_bias=1.0,\n+               state_is_tuple=True, activation=None, reuse=None, name=None):\n+    \"\"\"Initialize the basic NLSTM cell.\n+\n+    Args:\n+      num_units: int, The number of hidden units of each cell state\n+        and hidden state.\n+      depth: int, The number of layers in the nest\n+      forget_bias: float, The bias added to forget gates.\n+      state_is_tuple: If True, accepted and returned states are tuples of\n+        the `h_state` and `c_state`s.  If False, they are concatenated\n+        along the column axis.  The latter behavior will soon be deprecated.\n+      activation: Activation function of the inner states.  Default: `tanh`.\n+      reuse: (optional) Python boolean describing whether to reuse variables\n+        in an existing scope.  If not `True`, and the existing scope already has\n+        the given variables, an error is raised.\n+      name: String, the name of the layer. Layers with the same name will\n+        share weights, but to avoid mistakes we require reuse=True in such\n+        cases.\n+    \"\"\"\n+    super(NLSTMCell, self).__init__(_reuse=reuse, name=name)\n+    if not state_is_tuple:\n+      logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n+                   \"deprecated.  Use state_is_tuple=True.\", self)\n+\n+    # Inputs must be 2-dimensional.\n+    self.input_spec = base_layer.InputSpec(ndim=2)\n+    self._num_units = num_units\n+    self._forget_bias = forget_bias\n+    self._state_is_tuple = state_is_tuple\n+    self._depth = depth\n+    self._activation = activation or math_ops.tanh\n+    self._kernels = None\n+    self._biases = None\n+    self.built = False\n+\n+  @property\n+  def state_size(self):\n+    if self._state_is_tuple:\n+      return tuple([self._num_units] * (self.depth + 1))\n+    else:\n+      return self._num_units * (self.depth + 1)\n+\n+  @property\n+  def output_size(self):\n+    return self._num_units\n+\n+  @property\n+  def depth(self):\n+    return self._depth\n+\n+  def build(self, inputs_shape):\n+    if inputs_shape[1].value is None:\n+      raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\n+                       % inputs_shape)\n+\n+    input_depth = inputs_shape[1].value\n+    h_depth = self._num_units\n+    self._kernels = []\n+    self._biases = []\n+    for i in range(self.depth):\n+      if i == 0:\n+        self._kernels.append(\n+          self.add_variable(\n+            \"kernel_{}\".format(i),\n+            shape=[input_depth + h_depth, 4 * self._num_units]))\n+      else:\n+        self._kernels.append(\n+          self.add_variable(\n+            \"kernel_{}\".format(i),\n+            shape=[2 * h_depth, 4 * self._num_units]))\n+      self._biases.append(\n+        self.add_variable(\n+          \"bias_{}\".format(i),\n+          shape=[4 * self._num_units],\n+          initializer=init_ops.zeros_initializer(dtype=self.dtype)))\n+\n+    self.built = True\n+\n+  def recurrence(self, inputs, hidden_state, cell_states, depth):\n+    \"\"\"use recurrence to traverse the nested structure\n+\n+    Args:\n+      inputs: a 2D `Tensor` of [batch_size x input_size] shape\n+      hidden_state: a 2D `Tensor` of [batch_size x num_units] shape\n+      cell_states: a `list` of 2D `Tensor` of [batch_size x num_units] shape\n+      depth: `int`\n+        the current depth in the nested structure, begins at 0.\n+\n+    Return:\n+      new_h: a 2D `Tensor` of [batch_size x num_units] shape\n+        the latest hidden state for current step\n+      new_cs: a `list` of 2D `Tensor` of [batch_size x num_units] shape\n+        the accumulated cell states for current step\n+    \"\"\"\n+    sigmoid = math_ops.sigmoid\n+    one = constant_op.constant(1, dtype=dtypes.int32)\n+    # Parameters of gates are concatenated into one multiply for efficiency.\n+    c = cell_states[depth]\n+    h = hidden_state\n+\n+    gate_inputs = math_ops.matmul(\n+        array_ops.concat([inputs, h], 1), self._kernels[depth])\n+    gate_inputs = nn_ops.bias_add(gate_inputs, self._biases[depth])\n+\n+    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n+    i, j, f, o = array_ops.split(\n+        value=gate_inputs, num_or_size_splits=4, axis=one)\n+\n+    forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype)\n+    # Note that using `add` and `multiply` instead of `+` and `*` gives a\n+    # performance improvement. So using those at the cost of readability.\n+    add = math_ops.add\n+    multiply = math_ops.multiply\n+\n+    inner_hidden = multiply(c, sigmoid(add(f, forget_bias_tensor)))\n+    inner_input = multiply(sigmoid(i), self._activation(j))\n+    if depth == (self.depth - 1):\n+      new_c = add(inner_hidden, inner_input)\n+      new_cs = [new_c]\n+    else:\n+      new_c, new_cs = self.recurrence(\n+        inputs=inner_input,\n+        hidden_state=inner_hidden,\n+        cell_states=cell_states,\n+        depth=depth + 1)\n+    new_h = multiply(self._activation(new_c), sigmoid(o))\n+    new_cs = [new_h] + new_cs\n+    return new_h, new_cs\n+\n+  def call(self, inputs, state):\n+    \"\"\"forward propagation of the cell\n+\n+    Args:\n+      inputs: a 2D `Tensor` of [batch_size x input_size] shape\n+      state: a tuple of 2D `Tensor` of [batch_size x num_units] shape\n+        or a `Tensor` of [batch_size x (num_units * (self.depth + 1))] shape\n+\n+    Return:", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 162, "commit_id": "492c8c708017e279e449bc0857ec3e28452c287c", "original_commit_id": "17aab7396efb34c3b0daf6ffee3df1af9953566b", "user": {"login": "caisq", "id": 16824702, "node_id": "MDQ6VXNlcjE2ODI0NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16824702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caisq", "html_url": "https://github.com/caisq", "followers_url": "https://api.github.com/users/caisq/followers", "following_url": "https://api.github.com/users/caisq/following{/other_user}", "gists_url": "https://api.github.com/users/caisq/gists{/gist_id}", "starred_url": "https://api.github.com/users/caisq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caisq/subscriptions", "organizations_url": "https://api.github.com/users/caisq/orgs", "repos_url": "https://api.github.com/users/caisq/repos", "events_url": "https://api.github.com/users/caisq/events{/privacy}", "received_events_url": "https://api.github.com/users/caisq/received_events", "type": "User", "site_admin": false}, "body": "Nit: \"Return\" --> \"Returns\"", "created_at": "2018-02-04T18:47:16Z", "updated_at": "2018-02-12T00:40:47Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16736#discussion_r165853656", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16736", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/165853656"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16736#discussion_r165853656"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16736"}}, "body_html": "<p>Nit: \"Return\" --&gt; \"Returns\"</p>", "body_text": "Nit: \"Return\" --> \"Returns\""}