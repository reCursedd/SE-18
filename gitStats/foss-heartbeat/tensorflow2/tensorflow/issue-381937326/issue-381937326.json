{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23834", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23834/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23834/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23834/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23834", "id": 381937326, "node_id": "MDU6SXNzdWUzODE5MzczMjY=", "number": 23834, "title": "Relative Device Placement", "user": {"login": "SumNeuron", "id": 22868585, "node_id": "MDQ6VXNlcjIyODY4NTg1", "avatar_url": "https://avatars3.githubusercontent.com/u/22868585?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SumNeuron", "html_url": "https://github.com/SumNeuron", "followers_url": "https://api.github.com/users/SumNeuron/followers", "following_url": "https://api.github.com/users/SumNeuron/following{/other_user}", "gists_url": "https://api.github.com/users/SumNeuron/gists{/gist_id}", "starred_url": "https://api.github.com/users/SumNeuron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SumNeuron/subscriptions", "organizations_url": "https://api.github.com/users/SumNeuron/orgs", "repos_url": "https://api.github.com/users/SumNeuron/repos", "events_url": "https://api.github.com/users/SumNeuron/events{/privacy}", "received_events_url": "https://api.github.com/users/SumNeuron/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-18T07:48:25Z", "updated_at": "2018-11-18T07:48:25Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>TensorFlow version (you are using): 1.10+</li>\n<li>Are you willing to contribute it (Yes/No): Yes</li>\n</ul>\n<p><strong>Describe the feature and the current behavior/state.</strong></p>\n<p>From the <a href=\"https://www.tensorflow.org/guide/using_gpu#manual_device_placement\" rel=\"nofollow\">Using GPUs</a> Guide (under <a href=\"https://www.tensorflow.org/guide/using_gpu#using_multiple_gpus\" rel=\"nofollow\">multiple gpus</a>)  there is a part about using multiple GPUs in a \"multi-tower fashion\":</p>\n<pre><code>for d in ['/device:GPU:2', '/device:GPU:3']:\n  with tf.device(d):\n</code></pre>\n<p>Seeing this, one might be tempted to leverage this style for multiple GPU training in a custom Estimator to indicate to the model that it can be distributed across multiple GPUs efficiently.</p>\n<p>To my knowledge, if manual device placement is absent TensorFlow does not have some form of optimal device mapping (expect perhaps if you have the GPU version installed and a GPU is available, using it over the CPU). So what other choice do you have?</p>\n<p>Anyway, you carry on with training your estimator and export it to a <code>SavedModel</code> via <code>estimator.export_savedmodel(...)</code> and wish to use this <code>SavedModel</code> later... perhaps on a different machine, one which may not have as many GPUs as the device on which the model was trained (or maybe no GPUs)</p>\n<p>so when you run</p>\n<pre><code>from tensorflow.contrib import predictor\npredict_fn = predictor.from_saved_model(model_dir)\n</code></pre>\n<p>you get</p>\n<pre><code>Cannot assign a device for operation &lt;OP-NAME&gt;. Operation was \nexplicitly assigned to &lt;DEVICE-NAME&gt; but available devices are \n[&lt;AVAILABLE-DEVICE-0&gt;,...]\n</code></pre>\n<p>Now if you share a GPU cluster and are specifying TensorFlow to run your model on <code>GPU:3</code> and <code>GPU:6</code>, then go to run your model on your local machine, which may have 2 GPUs, you get the same problem.</p>\n<h2>Putative Solution</h2>\n<p>I propose either:</p>\n<ol>\n<li>\n<p>machine device mapping</p>\n</li>\n<li>\n<p>introducing relative device placement with automatic fallback.</p>\n</li>\n</ol>\n<p>For the former, this means that I can tell TensorFlow that at the beginning to treat the manual placement of devices in the graph to another via a dictionary:</p>\n<pre><code># map all GPUs to single core CPU\nmachine_device_map = {\"device:GPU:{}\".format(i): \"device/CPU:0\" for i in range(8)}\n\n# map odd GPUs to GPU 0 and even GPUs to GPU 1 on smaller GPU machine\nmachine_device_map = {\n    \"device/GPU:{}\".format(i): \"device:GPU:{}\".format(i % 2)\n    for i in range(8)\n}\n\n\n# then something like this goes after importing TensorFlow\ntf.devices.map_device(machine_device_map)\n</code></pre>\n<p>For the latter if I train my model on GPUs 1,4,6,7  and my machine only has GPUs 0 and 1 then  device placement 1,4 falls back to 0 and 6,7 falls back to 1. If my device only has a single CPU</p>\n<p>This would be done with something like:</p>\n<pre><code>with tf.relative_device('GPU:0'):\n</code></pre>\n<p>where, given 'device/relative/GPU:0' relative device calls</p>\n<pre><code>from TensorFlow's.python.client import device_lib\ndevices = device_lib.list_local_devices()\n\nrelative_device_num = 0\nrelative_device_type = 'GPU'\n\ncpus = [d for d in devices if d.device_type == 'CPU']\ngpus = [d for d in devices if d.device_type == 'GPU']\n\nif relative_device_type=='GPU':\n    if gpus:\n        return gpus[relative_device_num % len(gpus)] \n\nreturn cpus[relative_device_num % len(cpus)]\n\n</code></pre>\n<p><strong>Will this change the current api? How?</strong></p>\n<p>No. It would add the ability to change device placement for distributed models after training</p>\n<p><strong>Who will benefit with this feature?</strong></p>\n<p>All TF users</p>\n<p><strong>Any Other info.</strong></p>", "body_text": "System information\n\nTensorFlow version (you are using): 1.10+\nAre you willing to contribute it (Yes/No): Yes\n\nDescribe the feature and the current behavior/state.\nFrom the Using GPUs Guide (under multiple gpus)  there is a part about using multiple GPUs in a \"multi-tower fashion\":\nfor d in ['/device:GPU:2', '/device:GPU:3']:\n  with tf.device(d):\n\nSeeing this, one might be tempted to leverage this style for multiple GPU training in a custom Estimator to indicate to the model that it can be distributed across multiple GPUs efficiently.\nTo my knowledge, if manual device placement is absent TensorFlow does not have some form of optimal device mapping (expect perhaps if you have the GPU version installed and a GPU is available, using it over the CPU). So what other choice do you have?\nAnyway, you carry on with training your estimator and export it to a SavedModel via estimator.export_savedmodel(...) and wish to use this SavedModel later... perhaps on a different machine, one which may not have as many GPUs as the device on which the model was trained (or maybe no GPUs)\nso when you run\nfrom tensorflow.contrib import predictor\npredict_fn = predictor.from_saved_model(model_dir)\n\nyou get\nCannot assign a device for operation <OP-NAME>. Operation was \nexplicitly assigned to <DEVICE-NAME> but available devices are \n[<AVAILABLE-DEVICE-0>,...]\n\nNow if you share a GPU cluster and are specifying TensorFlow to run your model on GPU:3 and GPU:6, then go to run your model on your local machine, which may have 2 GPUs, you get the same problem.\nPutative Solution\nI propose either:\n\n\nmachine device mapping\n\n\nintroducing relative device placement with automatic fallback.\n\n\nFor the former, this means that I can tell TensorFlow that at the beginning to treat the manual placement of devices in the graph to another via a dictionary:\n# map all GPUs to single core CPU\nmachine_device_map = {\"device:GPU:{}\".format(i): \"device/CPU:0\" for i in range(8)}\n\n# map odd GPUs to GPU 0 and even GPUs to GPU 1 on smaller GPU machine\nmachine_device_map = {\n    \"device/GPU:{}\".format(i): \"device:GPU:{}\".format(i % 2)\n    for i in range(8)\n}\n\n\n# then something like this goes after importing TensorFlow\ntf.devices.map_device(machine_device_map)\n\nFor the latter if I train my model on GPUs 1,4,6,7  and my machine only has GPUs 0 and 1 then  device placement 1,4 falls back to 0 and 6,7 falls back to 1. If my device only has a single CPU\nThis would be done with something like:\nwith tf.relative_device('GPU:0'):\n\nwhere, given 'device/relative/GPU:0' relative device calls\nfrom TensorFlow's.python.client import device_lib\ndevices = device_lib.list_local_devices()\n\nrelative_device_num = 0\nrelative_device_type = 'GPU'\n\ncpus = [d for d in devices if d.device_type == 'CPU']\ngpus = [d for d in devices if d.device_type == 'GPU']\n\nif relative_device_type=='GPU':\n    if gpus:\n        return gpus[relative_device_num % len(gpus)] \n\nreturn cpus[relative_device_num % len(cpus)]\n\n\nWill this change the current api? How?\nNo. It would add the ability to change device placement for distributed models after training\nWho will benefit with this feature?\nAll TF users\nAny Other info.", "body": "**System information**\r\n- TensorFlow version (you are using): 1.10+\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nFrom the [Using GPUs] Guide (under [multiple gpus][Using Multiple GPUs])  there is a part about using multiple GPUs in a \"multi-tower fashion\":\r\n\r\n```\r\nfor d in ['/device:GPU:2', '/device:GPU:3']:\r\n  with tf.device(d):\r\n```\r\n\r\nSeeing this, one might be tempted to leverage this style for multiple GPU training in a custom Estimator to indicate to the model that it can be distributed across multiple GPUs efficiently.\r\n\r\nTo my knowledge, if manual device placement is absent TensorFlow does not have some form of optimal device mapping (expect perhaps if you have the GPU version installed and a GPU is available, using it over the CPU). So what other choice do you have?\r\n\r\nAnyway, you carry on with training your estimator and export it to a `SavedModel` via `estimator.export_savedmodel(...)` and wish to use this `SavedModel` later... perhaps on a different machine, one which may not have as many GPUs as the device on which the model was trained (or maybe no GPUs)\r\n\r\nso when you run\r\n\r\n```\r\nfrom tensorflow.contrib import predictor\r\npredict_fn = predictor.from_saved_model(model_dir)\r\n```\r\n\r\nyou get\r\n\r\n```\r\nCannot assign a device for operation <OP-NAME>. Operation was \r\nexplicitly assigned to <DEVICE-NAME> but available devices are \r\n[<AVAILABLE-DEVICE-0>,...]\r\n```\r\n\r\nNow if you share a GPU cluster and are specifying TensorFlow to run your model on `GPU:3` and `GPU:6`, then go to run your model on your local machine, which may have 2 GPUs, you get the same problem.\r\n\r\n## Putative Solution\r\n\r\nI propose either:\r\n\r\n1. machine device mapping\r\n\r\n2. introducing relative device placement with automatic fallback. \r\n\r\n\r\nFor the former, this means that I can tell TensorFlow that at the beginning to treat the manual placement of devices in the graph to another via a dictionary:\r\n\r\n```\r\n# map all GPUs to single core CPU\r\nmachine_device_map = {\"device:GPU:{}\".format(i): \"device/CPU:0\" for i in range(8)}\r\n\r\n# map odd GPUs to GPU 0 and even GPUs to GPU 1 on smaller GPU machine\r\nmachine_device_map = {\r\n    \"device/GPU:{}\".format(i): \"device:GPU:{}\".format(i % 2)\r\n    for i in range(8)\r\n}\r\n\r\n\r\n# then something like this goes after importing TensorFlow\r\ntf.devices.map_device(machine_device_map)\r\n```\r\n\r\nFor the latter if I train my model on GPUs 1,4,6,7  and my machine only has GPUs 0 and 1 then  device placement 1,4 falls back to 0 and 6,7 falls back to 1. If my device only has a single CPU\r\n\r\nThis would be done with something like:\r\n\r\n```\r\nwith tf.relative_device('GPU:0'):\r\n```\r\n\r\nwhere, given 'device/relative/GPU:0' relative device calls\r\n\r\n```\r\nfrom TensorFlow's.python.client import device_lib\r\ndevices = device_lib.list_local_devices()\r\n\r\nrelative_device_num = 0\r\nrelative_device_type = 'GPU'\r\n\r\ncpus = [d for d in devices if d.device_type == 'CPU']\r\ngpus = [d for d in devices if d.device_type == 'GPU']\r\n\r\nif relative_device_type=='GPU':\r\n    if gpus:\r\n        return gpus[relative_device_num % len(gpus)] \r\n\r\nreturn cpus[relative_device_num % len(cpus)]\r\n\r\n```\r\n\r\n\r\n\r\n[Using GPUs]: https://www.tensorflow.org/guide/using_gpu#manual_device_placement\r\n[Using Multiple GPUs]: https://www.tensorflow.org/guide/using_gpu#using_multiple_gpus\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo. It would add the ability to change device placement for distributed models after training\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAll TF users\r\n\r\n**Any Other info.**\r\n"}