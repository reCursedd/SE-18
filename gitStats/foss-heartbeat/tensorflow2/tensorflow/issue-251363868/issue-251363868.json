{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12406", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12406/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12406/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12406/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12406", "id": 251363868, "node_id": "MDU6SXNzdWUyNTEzNjM4Njg=", "number": 12406, "title": "compilation error ", "user": {"login": "Goddard", "id": 231351, "node_id": "MDQ6VXNlcjIzMTM1MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/231351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Goddard", "html_url": "https://github.com/Goddard", "followers_url": "https://api.github.com/users/Goddard/followers", "following_url": "https://api.github.com/users/Goddard/following{/other_user}", "gists_url": "https://api.github.com/users/Goddard/gists{/gist_id}", "starred_url": "https://api.github.com/users/Goddard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Goddard/subscriptions", "organizations_url": "https://api.github.com/users/Goddard/orgs", "repos_url": "https://api.github.com/users/Goddard/repos", "events_url": "https://api.github.com/users/Goddard/events{/privacy}", "received_events_url": "https://api.github.com/users/Goddard/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-08-18T21:31:40Z", "updated_at": "2017-08-22T04:24:06Z", "closed_at": "2017-08-18T22:01:32Z", "author_association": "NONE", "body_html": "<p>On Ubuntu 16.04 with gcc5</p>\n<p>./configure<br>\nYou have bazel 0.5.3 installed.<br>\nPlease specify the location of python. [Default is /usr/bin/python]:<br>\nFound possible Python library paths:<br>\n/usr/local/lib/python2.7/dist-packages<br>\n/usr/lib/python2.7/dist-packages<br>\nPlease input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages]</p>\n<p>Using python library path: /usr/local/lib/python2.7/dist-packages<br>\nDo you wish to build TensorFlow with MKL support? [y/N]<br>\nNo MKL support will be enabled for TensorFlow<br>\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:<br>\nDo you wish to use jemalloc as the malloc implementation? [Y/n]<br>\njemalloc enabled<br>\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]<br>\nNo Google Cloud Platform support will be enabled for TensorFlow<br>\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]<br>\nNo Hadoop File System support will be enabled for TensorFlow<br>\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]<br>\nNo XLA support will be enabled for TensorFlow<br>\nDo you wish to build TensorFlow with VERBS support? [y/N]<br>\nNo VERBS support will be enabled for TensorFlow<br>\nDo you wish to build TensorFlow with OpenCL support? [y/N]<br>\nNo OpenCL support will be enabled for TensorFlow<br>\nDo you wish to build TensorFlow with CUDA support? [y/N] y<br>\nCUDA support will be enabled for TensorFlow<br>\nDo you want to use clang as CUDA compiler? [y/N]<br>\nnvcc will be used as CUDA compiler<br>\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8<br>\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:<br>\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found<br>\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8<br>\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:<br>\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found<br>\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0<br>\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:<br>\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:<br>\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.0.1<br>\nPlease specify the location where cuDNN 7.0.1 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu/<br>\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.<br>\nYou can find the compute capability of your device at: <a href=\"https://developer.nvidia.com/cuda-gpus\" rel=\"nofollow\">https://developer.nvidia.com/cuda-gpus</a>.<br>\nPlease note that each additional compute capability significantly increases your build time and binary size.<br>\n[Default is: \"6.1\"]:<br>\nDo you wish to build TensorFlow with MPI support? [y/N]<br>\nMPI support will not be enabled for TensorFlow<br>\nConfiguration finished</p>\n<p>BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1)<br>\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':<br>\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1021:50: required from here<br>\ntensorflow/stream_executor/cuda/cuda_dnn.cc:140:38: error: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescriptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnRNNAlgo_t, cudnnDataType_t)'<br>\ncudnnStatus_t retval = ::__name(args...);<br>\n^<br>\ntensorflow/stream_executor/cuda/cuda_dnn.cc:234:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'<br>\n__macro(cudnnSetRNNDescriptor)</p>", "body_text": "On Ubuntu 16.04 with gcc5\n./configure\nYou have bazel 0.5.3 installed.\nPlease specify the location of python. [Default is /usr/bin/python]:\nFound possible Python library paths:\n/usr/local/lib/python2.7/dist-packages\n/usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages]\nUsing python library path: /usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with MKL support? [y/N]\nNo MKL support will be enabled for TensorFlow\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\nDo you wish to use jemalloc as the malloc implementation? [Y/n]\njemalloc enabled\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\nNo Hadoop File System support will be enabled for TensorFlow\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\nNo XLA support will be enabled for TensorFlow\nDo you wish to build TensorFlow with VERBS support? [y/N]\nNo VERBS support will be enabled for TensorFlow\nDo you wish to build TensorFlow with OpenCL support? [y/N]\nNo OpenCL support will be enabled for TensorFlow\nDo you wish to build TensorFlow with CUDA support? [y/N] y\nCUDA support will be enabled for TensorFlow\nDo you want to use clang as CUDA compiler? [y/N]\nnvcc will be used as CUDA compiler\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.0.1\nPlease specify the location where cuDNN 7.0.1 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu/\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"6.1\"]:\nDo you wish to build TensorFlow with MPI support? [y/N]\nMPI support will not be enabled for TensorFlow\nConfiguration finished\nBUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1)\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1021:50: required from here\ntensorflow/stream_executor/cuda/cuda_dnn.cc:140:38: error: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescriptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnRNNAlgo_t, cudnnDataType_t)'\ncudnnStatus_t retval = ::__name(args...);\n^\ntensorflow/stream_executor/cuda/cuda_dnn.cc:234:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\n__macro(cudnnSetRNNDescriptor)", "body": "On Ubuntu 16.04 with gcc5\r\n\r\n./configure\r\nYou have bazel 0.5.3 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nFound possible Python library paths:\r\n/usr/local/lib/python2.7/dist-packages\r\n/usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N]\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n]\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N]\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N]\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8\r\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8\r\nPlease specify the location where CUDA 8 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nInvalid path to CUDA 8 toolkit. /usr/local/cuda/lib64/libcudart.so.8 cannot be found\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.0.1\r\nPlease specify the location where cuDNN 7.0.1 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu/\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"6.1\"]:\r\nDo you wish to build TensorFlow with MPI support? [y/N]\r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n\r\nBUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1)\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In instantiation of 'cudnnStatus_t perftools::gputools::cuda::wrap::WrapperShim__cudnnSetRNNDescriptor::operator()(perftools::gputools::cuda::CUDAExecutor*, Args ...) [with Args = {cudnnRNNStruct*, int, int, cudnnDropoutStruct*, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnDataType_t}]':\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:1021:50: required from here\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:140:38: error: cannot convert 'cudnnRNNStruct*' to 'cudnnHandle_t {aka cudnnContext*}' for argument '1' to 'cudnnStatus_t cudnnSetRNNDescriptor(cudnnHandle_t, cudnnRNNDescriptor_t, int, int, cudnnDropoutDescriptor_t, cudnnRNNInputMode_t, cudnnDirectionMode_t, cudnnRNNMode_t, cudnnRNNAlgo_t, cudnnDataType_t)'\r\ncudnnStatus_t retval = ::__name(args...); \r\n^\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:234:3: note: in expansion of macro 'PERFTOOLS_GPUTOOLS_CUDNN_WRAP'\r\n__macro(cudnnSetRNNDescriptor)"}