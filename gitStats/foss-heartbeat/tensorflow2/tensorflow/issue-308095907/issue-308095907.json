{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17955", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17955/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17955/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17955/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17955", "id": 308095907, "node_id": "MDU6SXNzdWUzMDgwOTU5MDc=", "number": 17955, "title": "Results inconsistent between each freeze graph", "user": {"login": "joelteply", "id": 347104, "node_id": "MDQ6VXNlcjM0NzEwNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/347104?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joelteply", "html_url": "https://github.com/joelteply", "followers_url": "https://api.github.com/users/joelteply/followers", "following_url": "https://api.github.com/users/joelteply/following{/other_user}", "gists_url": "https://api.github.com/users/joelteply/gists{/gist_id}", "starred_url": "https://api.github.com/users/joelteply/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joelteply/subscriptions", "organizations_url": "https://api.github.com/users/joelteply/orgs", "repos_url": "https://api.github.com/users/joelteply/repos", "events_url": "https://api.github.com/users/joelteply/events{/privacy}", "received_events_url": "https://api.github.com/users/joelteply/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-23T16:24:35Z", "updated_at": "2018-11-21T18:39:44Z", "closed_at": "2018-03-24T16:31:04Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>I apologize if this is the wrong forum, but it may be a bug regarding certain operations. When freezing a graph and then running it elsewhere (mobile device), the output is of low quality compared to the inference on the server on my semantic segmentation model. It is basically a messy version of what would run on the server. It is executing successfully, but it appears as though something was not initialized prior to freezing, even though the method to load the model between the export script and inference scripts is nearly identical.</p>\n<p>The exported model can be run on the same images over and over and produce the same results for a given set of images, as expected.</p>\n<p><strong>Here is the really strange part:</strong><br>\nHowever, each time the model is frozen, using exactly the same script and same checkpoint, it creates a different output for a given set of images.</p>\n<p>I went ahead and posted to stackoverflow in case this is the wrong place.<br>\n<a href=\"https://stackoverflow.com/questions/49454430/tensorflow-results-inconsistent-between-each-freeze-graph\" rel=\"nofollow\">https://stackoverflow.com/questions/49454430/tensorflow-results-inconsistent-between-each-freeze-graph</a></p>\n<h3>Source code / logs</h3>\n<pre><code>def main():\n    args = get_arguments()\n    \n    if args.dataset == 'cityscapes':\n        num_classes = cityscapes_class\n    else:\n        num_classes = ADE20k_class\n\n    shape = [320, 320]\n\n    x = tf.placeholder(dtype=tf.float32, shape=(shape[0], shape[1], 3), name=\"input\")\n    img_tf = preprocess(x)\n\n    model = model_config[args.model]\n    net = model({'data': img_tf}, num_classes=num_classes, filter_scale=args.filter_scale)\n\n    raw_output = net.layers['conv6_cls']\n    raw_output_up = tf.image.resize_bilinear(raw_output, size=shape, align_corners=True)\n    raw_output_maxed = tf.argmax(raw_output_up, axis=3, name=\"output\")\n        \n    # Init tf Session\n    config = tf.ConfigProto()\n    sess = tf.Session(config=config)\n    init = tf.global_variables_initializer()\n\n    sess.run(init)\n    \n    model_path = model_paths[args.model]\n    ckpt = tf.train.get_checkpoint_state(model_path)\n    if ckpt and ckpt.model_checkpoint_path:\n        input_checkpoint = ckpt.model_checkpoint_path\n        loader = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)\n        load(loader, sess, ckpt.model_checkpoint_path)     \n    else:\n        print('No checkpoint file found at %s.' % model_path)\n        exit()\n\n    print(\"Loaded Model\")\n\n    # We retrieve the protobuf graph definition\n    graph = tf.get_default_graph()\n    input_graph_def = graph.as_graph_def()\n\n    # We use a built-in TF helper to export variables to constants\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess, # The session is used to retrieve the weights\n        input_graph_def, # The graph_def is used to retrieve the nodes\n        output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n    )\n\n    # Finally we serialize and dump the output graph to the filesystem\n    with tf.gfile.GFile(\"model/output_graph.pb\", \"wb\") as f:\n        f.write(output_graph_def.SerializeToString())\n    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n</code></pre>", "body_text": "Describe the problem\nI apologize if this is the wrong forum, but it may be a bug regarding certain operations. When freezing a graph and then running it elsewhere (mobile device), the output is of low quality compared to the inference on the server on my semantic segmentation model. It is basically a messy version of what would run on the server. It is executing successfully, but it appears as though something was not initialized prior to freezing, even though the method to load the model between the export script and inference scripts is nearly identical.\nThe exported model can be run on the same images over and over and produce the same results for a given set of images, as expected.\nHere is the really strange part:\nHowever, each time the model is frozen, using exactly the same script and same checkpoint, it creates a different output for a given set of images.\nI went ahead and posted to stackoverflow in case this is the wrong place.\nhttps://stackoverflow.com/questions/49454430/tensorflow-results-inconsistent-between-each-freeze-graph\nSource code / logs\ndef main():\n    args = get_arguments()\n    \n    if args.dataset == 'cityscapes':\n        num_classes = cityscapes_class\n    else:\n        num_classes = ADE20k_class\n\n    shape = [320, 320]\n\n    x = tf.placeholder(dtype=tf.float32, shape=(shape[0], shape[1], 3), name=\"input\")\n    img_tf = preprocess(x)\n\n    model = model_config[args.model]\n    net = model({'data': img_tf}, num_classes=num_classes, filter_scale=args.filter_scale)\n\n    raw_output = net.layers['conv6_cls']\n    raw_output_up = tf.image.resize_bilinear(raw_output, size=shape, align_corners=True)\n    raw_output_maxed = tf.argmax(raw_output_up, axis=3, name=\"output\")\n        \n    # Init tf Session\n    config = tf.ConfigProto()\n    sess = tf.Session(config=config)\n    init = tf.global_variables_initializer()\n\n    sess.run(init)\n    \n    model_path = model_paths[args.model]\n    ckpt = tf.train.get_checkpoint_state(model_path)\n    if ckpt and ckpt.model_checkpoint_path:\n        input_checkpoint = ckpt.model_checkpoint_path\n        loader = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)\n        load(loader, sess, ckpt.model_checkpoint_path)     \n    else:\n        print('No checkpoint file found at %s.' % model_path)\n        exit()\n\n    print(\"Loaded Model\")\n\n    # We retrieve the protobuf graph definition\n    graph = tf.get_default_graph()\n    input_graph_def = graph.as_graph_def()\n\n    # We use a built-in TF helper to export variables to constants\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess, # The session is used to retrieve the weights\n        input_graph_def, # The graph_def is used to retrieve the nodes\n        output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n    )\n\n    # Finally we serialize and dump the output graph to the filesystem\n    with tf.gfile.GFile(\"model/output_graph.pb\", \"wb\") as f:\n        f.write(output_graph_def.SerializeToString())\n    print(\"%d ops in the final graph.\" % len(output_graph_def.node))", "body": "### Describe the problem\r\n\r\nI apologize if this is the wrong forum, but it may be a bug regarding certain operations. When freezing a graph and then running it elsewhere (mobile device), the output is of low quality compared to the inference on the server on my semantic segmentation model. It is basically a messy version of what would run on the server. It is executing successfully, but it appears as though something was not initialized prior to freezing, even though the method to load the model between the export script and inference scripts is nearly identical. \r\n\r\nThe exported model can be run on the same images over and over and produce the same results for a given set of images, as expected. \r\n\r\n**Here is the really strange part:**\r\nHowever, each time the model is frozen, using exactly the same script and same checkpoint, it creates a different output for a given set of images.\r\n\r\nI went ahead and posted to stackoverflow in case this is the wrong place.\r\n[https://stackoverflow.com/questions/49454430/tensorflow-results-inconsistent-between-each-freeze-graph](https://stackoverflow.com/questions/49454430/tensorflow-results-inconsistent-between-each-freeze-graph)\r\n\r\n### Source code / logs\r\n\r\n```\r\ndef main():\r\n    args = get_arguments()\r\n    \r\n    if args.dataset == 'cityscapes':\r\n        num_classes = cityscapes_class\r\n    else:\r\n        num_classes = ADE20k_class\r\n\r\n    shape = [320, 320]\r\n\r\n    x = tf.placeholder(dtype=tf.float32, shape=(shape[0], shape[1], 3), name=\"input\")\r\n    img_tf = preprocess(x)\r\n\r\n    model = model_config[args.model]\r\n    net = model({'data': img_tf}, num_classes=num_classes, filter_scale=args.filter_scale)\r\n\r\n    raw_output = net.layers['conv6_cls']\r\n    raw_output_up = tf.image.resize_bilinear(raw_output, size=shape, align_corners=True)\r\n    raw_output_maxed = tf.argmax(raw_output_up, axis=3, name=\"output\")\r\n        \r\n    # Init tf Session\r\n    config = tf.ConfigProto()\r\n    sess = tf.Session(config=config)\r\n    init = tf.global_variables_initializer()\r\n\r\n    sess.run(init)\r\n    \r\n    model_path = model_paths[args.model]\r\n    ckpt = tf.train.get_checkpoint_state(model_path)\r\n    if ckpt and ckpt.model_checkpoint_path:\r\n        input_checkpoint = ckpt.model_checkpoint_path\r\n        loader = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)\r\n        load(loader, sess, ckpt.model_checkpoint_path)     \r\n    else:\r\n        print('No checkpoint file found at %s.' % model_path)\r\n        exit()\r\n\r\n    print(\"Loaded Model\")\r\n\r\n    # We retrieve the protobuf graph definition\r\n    graph = tf.get_default_graph()\r\n    input_graph_def = graph.as_graph_def()\r\n\r\n    # We use a built-in TF helper to export variables to constants\r\n    output_graph_def = graph_util.convert_variables_to_constants(\r\n        sess, # The session is used to retrieve the weights\r\n        input_graph_def, # The graph_def is used to retrieve the nodes\r\n        output_node_names.split(\",\") # The output node names are used to select the usefull nodes\r\n    )\r\n\r\n    # Finally we serialize and dump the output graph to the filesystem\r\n    with tf.gfile.GFile(\"model/output_graph.pb\", \"wb\") as f:\r\n        f.write(output_graph_def.SerializeToString())\r\n    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n```"}