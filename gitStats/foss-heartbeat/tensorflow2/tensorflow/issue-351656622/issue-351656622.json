{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21684", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21684/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21684/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21684/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21684", "id": 351656622, "node_id": "MDU6SXNzdWUzNTE2NTY2MjI=", "number": 21684, "title": "Two optimizers, with two assign ops on all .get_variable variables when using biderctional_dynamic_rnn", "user": {"login": "nazariyv", "id": 13678461, "node_id": "MDQ6VXNlcjEzNjc4NDYx", "avatar_url": "https://avatars3.githubusercontent.com/u/13678461?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nazariyv", "html_url": "https://github.com/nazariyv", "followers_url": "https://api.github.com/users/nazariyv/followers", "following_url": "https://api.github.com/users/nazariyv/following{/other_user}", "gists_url": "https://api.github.com/users/nazariyv/gists{/gist_id}", "starred_url": "https://api.github.com/users/nazariyv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nazariyv/subscriptions", "organizations_url": "https://api.github.com/users/nazariyv/orgs", "repos_url": "https://api.github.com/users/nazariyv/repos", "events_url": "https://api.github.com/users/nazariyv/events{/privacy}", "received_events_url": "https://api.github.com/users/nazariyv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-08-17T16:21:28Z", "updated_at": "2018-08-24T16:20:33Z", "closed_at": "2018-08-24T16:20:33Z", "author_association": "NONE", "body_html": "<h3>Environment Information</h3>\n<pre><code>\n== cat /etc/issue ===============================================\nLinux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy                              1.14.3     \nnumpydoc                           0.8.0      \nprotobuf                           3.5.2.post1\ntensorflow                         1.8.0      \n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.8.0\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\nSanity check: array([1], dtype=int32)\n/home/nazariy/anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/lib:/usr/lib/lp_solve/lp_solve_5.5\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\ntf_env_collect.sh: 105: tf_env_collect.sh: nvidia-smi: not found\n\n== cuda libs  ===================================================\n/usr/local/MATLAB/R2017b/bin/glnxa64/libcudart.so.8.0.44\n</code></pre>\n<h3>Additional System Questions</h3>\n<p>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No<br>\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1<br>\nTensorFlow installed from (source or binary): pip<br>\nTensorFlow version (use command below): 1.8.0<br>\nPython version: 3.6.5<br>\nBazel version (if compiling from source): none<br>\nGCC/Compiler version (if compiling from source): none<br>\nCUDA/cuDNN version: none<br>\nGPU model and memory: none<br>\nExact command to reproduce: none<br>\nMobile device: No</p>\n<h3>Problem outline</h3>\n<p>I have written an architecture whereby I have attention layers and in the middle I have a <code>bidirectional_dynamic_rnn</code> with forward and backward GRU cells like so:</p>\n<pre><code>class TensorGRU(object):\n\n    def __init__(self, inputs, hidden_size, dtype=tf.float64):\n        with tf.name_scope(\"GRULayer\"):\n            self.GRU_forward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=\"GRU_Forward\")\n            self.GRU_backward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=\"GRU_Backward\")\n\n            self.GRU_bi, _ = tf.nn.bidirectional_dynamic_rnn(self.GRU_forward, self.GRU_backward, inputs=inputs,\n                                                             dtype=dtype)\n            self.h = tf.concat([self.GRU_bi[0], self.GRU_bi[1]], axis=2, name='h')\n            tf.summary.histogram('GRU_Output', self.h)\n</code></pre>\n<p>As soon as I add the <code>tf.nn.bidirectional_dymaic_rnn</code> line I get two optimizers (Adam in my case) \"attached\" to every <code>.get_variable()</code> variable in my code. For example, see the screenshot provided from Tensorboard Debugger:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13678461/44277083-6f60f800-a241-11e8-8e20-fe0b7ac85fe2.PNG\"><img width=\"204\" alt=\"issue_1\" src=\"https://user-images.githubusercontent.com/13678461/44277083-6f60f800-a241-11e8-8e20-fe0b7ac85fe2.PNG\" style=\"max-width:100%;\"></a><br>\nand from graph:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13678461/44277095-77b93300-a241-11e8-9c04-f378b8409e22.PNG\"><img width=\"548\" alt=\"issue_2\" src=\"https://user-images.githubusercontent.com/13678461/44277095-77b93300-a241-11e8-9c04-f378b8409e22.PNG\" style=\"max-width:100%;\"></a></p>\n<p>Here is the MLP code for your convenience, but there is nothing special there:</p>\n<pre><code>class MLP(object):\n\n    # incoming is V, which is [batch, 2*hidden]\n    def __init__(self, n_input, n_hidden_1, n_hidden_2, n_classes, dtype=tf.float64,\n                 input_layer=None, activation_func=None):\n\n        if input_layer is None:\n            self.input_layer = tf.placeholder(dtype=dtype, shape=(None, n_input),\n                                              name=\"PL_Output\")\n        else:\n            if isinstance(input_layer, tf.Tensor):\n                self.input_layer = input_layer\n            else:\n                self.input_layer = tf.convert_to_tensor(input_layer, name=\"PL_Output\")\n\n        if activation_func is None:\n            self.activation_func = tf.tanh\n        else:\n            self.activation_func = activation_func\n\n        with tf.variable_scope(\"MLP_Variables\"):\n            self.weights = {\n                'l1': tf.get_variable(\"MLP_Layer_1_Weights\", dtype=dtype, shape=(n_input, n_hidden_1),\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\n                'l2': tf.get_variable(\"MLP_Layer_2_Weights\", dtype=dtype, shape=(n_hidden_1, n_hidden_2),\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\n                'out': tf.get_variable(\"MLP_Layer_Out_Weights\", dtype=dtype, shape=(n_hidden_2, n_classes),\n                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))\n            }\n\n            self.biases = {\n                'l1': tf.get_variable(\"MLP_Layer_1_Biases\", dtype=dtype, shape=(n_hidden_1,),\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\n                'l2': tf.get_variable(\"MLP_Layer_2_Biases\", dtype=dtype, shape=(n_hidden_2,),\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\n                'out': tf.get_variable(\"MLP_Layer_Out_Biases\", dtype=dtype, shape=(n_classes,),\n                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))\n            }\n\n        self.layer_1 = None\n        self.layer_2 = None\n        self.layer_out = None\n        self.expected_output = None\n\n        self.multilayer_perceptron()\n\n    def multilayer_perceptron(self):\n        self.layer_1 = tf.add(tf.matmul(self.input_layer, self.weights['l1']), self.biases['l1'])\n        self.layer_1 = self.activation_func(self.layer_1)\n\n        self.layer_2 = tf.add(tf.matmul(self.layer_1, self.weights['l2']), self.biases['l2'])\n        self.layer_2 = self.activation_func(self.layer_2)\n\n        self.layer_out = tf.add(tf.matmul(self.layer_2, self.weights['out']), self.biases['out'])\n        self.layer_out = self.activation_func(self.layer_out)\n\n        self.expected_output = tf.nn.softmax(self.layer_out, axis=1)\n\n        return self.expected_output\n</code></pre>\n<p><strong>I would expect that each variable only has one optimizer</strong>.</p>", "body_text": "Environment Information\n\n== cat /etc/issue ===============================================\nLinux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== are we in docker =============================================\nNo\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy                              1.14.3     \nnumpydoc                           0.8.0      \nprotobuf                           3.5.2.post1\ntensorflow                         1.8.0      \n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.8.0\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\nSanity check: array([1], dtype=int32)\n/home/nazariy/anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/lib:/usr/lib/lp_solve/lp_solve_5.5\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\ntf_env_collect.sh: 105: tf_env_collect.sh: nvidia-smi: not found\n\n== cuda libs  ===================================================\n/usr/local/MATLAB/R2017b/bin/glnxa64/libcudart.so.8.0.44\n\nAdditional System Questions\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1\nTensorFlow installed from (source or binary): pip\nTensorFlow version (use command below): 1.8.0\nPython version: 3.6.5\nBazel version (if compiling from source): none\nGCC/Compiler version (if compiling from source): none\nCUDA/cuDNN version: none\nGPU model and memory: none\nExact command to reproduce: none\nMobile device: No\nProblem outline\nI have written an architecture whereby I have attention layers and in the middle I have a bidirectional_dynamic_rnn with forward and backward GRU cells like so:\nclass TensorGRU(object):\n\n    def __init__(self, inputs, hidden_size, dtype=tf.float64):\n        with tf.name_scope(\"GRULayer\"):\n            self.GRU_forward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=\"GRU_Forward\")\n            self.GRU_backward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=\"GRU_Backward\")\n\n            self.GRU_bi, _ = tf.nn.bidirectional_dynamic_rnn(self.GRU_forward, self.GRU_backward, inputs=inputs,\n                                                             dtype=dtype)\n            self.h = tf.concat([self.GRU_bi[0], self.GRU_bi[1]], axis=2, name='h')\n            tf.summary.histogram('GRU_Output', self.h)\n\nAs soon as I add the tf.nn.bidirectional_dymaic_rnn line I get two optimizers (Adam in my case) \"attached\" to every .get_variable() variable in my code. For example, see the screenshot provided from Tensorboard Debugger:\n\nand from graph:\n\nHere is the MLP code for your convenience, but there is nothing special there:\nclass MLP(object):\n\n    # incoming is V, which is [batch, 2*hidden]\n    def __init__(self, n_input, n_hidden_1, n_hidden_2, n_classes, dtype=tf.float64,\n                 input_layer=None, activation_func=None):\n\n        if input_layer is None:\n            self.input_layer = tf.placeholder(dtype=dtype, shape=(None, n_input),\n                                              name=\"PL_Output\")\n        else:\n            if isinstance(input_layer, tf.Tensor):\n                self.input_layer = input_layer\n            else:\n                self.input_layer = tf.convert_to_tensor(input_layer, name=\"PL_Output\")\n\n        if activation_func is None:\n            self.activation_func = tf.tanh\n        else:\n            self.activation_func = activation_func\n\n        with tf.variable_scope(\"MLP_Variables\"):\n            self.weights = {\n                'l1': tf.get_variable(\"MLP_Layer_1_Weights\", dtype=dtype, shape=(n_input, n_hidden_1),\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\n                'l2': tf.get_variable(\"MLP_Layer_2_Weights\", dtype=dtype, shape=(n_hidden_1, n_hidden_2),\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\n                'out': tf.get_variable(\"MLP_Layer_Out_Weights\", dtype=dtype, shape=(n_hidden_2, n_classes),\n                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))\n            }\n\n            self.biases = {\n                'l1': tf.get_variable(\"MLP_Layer_1_Biases\", dtype=dtype, shape=(n_hidden_1,),\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\n                'l2': tf.get_variable(\"MLP_Layer_2_Biases\", dtype=dtype, shape=(n_hidden_2,),\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\n                'out': tf.get_variable(\"MLP_Layer_Out_Biases\", dtype=dtype, shape=(n_classes,),\n                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))\n            }\n\n        self.layer_1 = None\n        self.layer_2 = None\n        self.layer_out = None\n        self.expected_output = None\n\n        self.multilayer_perceptron()\n\n    def multilayer_perceptron(self):\n        self.layer_1 = tf.add(tf.matmul(self.input_layer, self.weights['l1']), self.biases['l1'])\n        self.layer_1 = self.activation_func(self.layer_1)\n\n        self.layer_2 = tf.add(tf.matmul(self.layer_1, self.weights['l2']), self.biases['l2'])\n        self.layer_2 = self.activation_func(self.layer_2)\n\n        self.layer_out = tf.add(tf.matmul(self.layer_2, self.weights['out']), self.biases['out'])\n        self.layer_out = self.activation_func(self.layer_out)\n\n        self.expected_output = tf.nn.softmax(self.layer_out, axis=1)\n\n        return self.expected_output\n\nI would expect that each variable only has one optimizer.", "body": "### Environment Information\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux nazariy-box 4.15.0-32-generic #35~16.04.1-Ubuntu SMP Fri Aug 10 21:54:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.3     \r\nnumpydoc                           0.8.0      \r\nprotobuf                           3.5.2.post1\r\ntensorflow                         1.8.0      \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/home/nazariy/anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/lib:/usr/lib/lp_solve/lp_solve_5.5\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: 105: tf_env_collect.sh: nvidia-smi: not found\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/MATLAB/R2017b/bin/glnxa64/libcudart.so.8.0.44\r\n```\r\n### Additional System Questions\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1\r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (use command below): 1.8.0\r\nPython version: 3.6.5\r\nBazel version (if compiling from source): none\r\nGCC/Compiler version (if compiling from source): none\r\nCUDA/cuDNN version: none\r\nGPU model and memory: none\r\nExact command to reproduce: none\r\nMobile device: No\r\n\r\n### Problem outline\r\nI have written an architecture whereby I have attention layers and in the middle I have a `bidirectional_dynamic_rnn` with forward and backward GRU cells like so:\r\n\r\n```\r\nclass TensorGRU(object):\r\n\r\n    def __init__(self, inputs, hidden_size, dtype=tf.float64):\r\n        with tf.name_scope(\"GRULayer\"):\r\n            self.GRU_forward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=\"GRU_Forward\")\r\n            self.GRU_backward = tf.nn.rnn_cell.GRUCell(num_units=hidden_size, activation=tf.nn.tanh, name=\"GRU_Backward\")\r\n\r\n            self.GRU_bi, _ = tf.nn.bidirectional_dynamic_rnn(self.GRU_forward, self.GRU_backward, inputs=inputs,\r\n                                                             dtype=dtype)\r\n            self.h = tf.concat([self.GRU_bi[0], self.GRU_bi[1]], axis=2, name='h')\r\n            tf.summary.histogram('GRU_Output', self.h)\r\n```\r\nAs soon as I add the `tf.nn.bidirectional_dymaic_rnn` line I get two optimizers (Adam in my case) \"attached\" to every `.get_variable()` variable in my code. For example, see the screenshot provided from Tensorboard Debugger:\r\n<img width=\"204\" alt=\"issue_1\" src=\"https://user-images.githubusercontent.com/13678461/44277083-6f60f800-a241-11e8-8e20-fe0b7ac85fe2.PNG\">\r\nand from graph:\r\n<img width=\"548\" alt=\"issue_2\" src=\"https://user-images.githubusercontent.com/13678461/44277095-77b93300-a241-11e8-9c04-f378b8409e22.PNG\">\r\n\r\nHere is the MLP code for your convenience, but there is nothing special there:\r\n\r\n```\r\nclass MLP(object):\r\n\r\n    # incoming is V, which is [batch, 2*hidden]\r\n    def __init__(self, n_input, n_hidden_1, n_hidden_2, n_classes, dtype=tf.float64,\r\n                 input_layer=None, activation_func=None):\r\n\r\n        if input_layer is None:\r\n            self.input_layer = tf.placeholder(dtype=dtype, shape=(None, n_input),\r\n                                              name=\"PL_Output\")\r\n        else:\r\n            if isinstance(input_layer, tf.Tensor):\r\n                self.input_layer = input_layer\r\n            else:\r\n                self.input_layer = tf.convert_to_tensor(input_layer, name=\"PL_Output\")\r\n\r\n        if activation_func is None:\r\n            self.activation_func = tf.tanh\r\n        else:\r\n            self.activation_func = activation_func\r\n\r\n        with tf.variable_scope(\"MLP_Variables\"):\r\n            self.weights = {\r\n                'l1': tf.get_variable(\"MLP_Layer_1_Weights\", dtype=dtype, shape=(n_input, n_hidden_1),\r\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\r\n                'l2': tf.get_variable(\"MLP_Layer_2_Weights\", dtype=dtype, shape=(n_hidden_1, n_hidden_2),\r\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\r\n                'out': tf.get_variable(\"MLP_Layer_Out_Weights\", dtype=dtype, shape=(n_hidden_2, n_classes),\r\n                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))\r\n            }\r\n\r\n            self.biases = {\r\n                'l1': tf.get_variable(\"MLP_Layer_1_Biases\", dtype=dtype, shape=(n_hidden_1,),\r\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\r\n                'l2': tf.get_variable(\"MLP_Layer_2_Biases\", dtype=dtype, shape=(n_hidden_2,),\r\n                                      initializer=tf.contrib.layers.xavier_initializer(dtype=dtype)),\r\n                'out': tf.get_variable(\"MLP_Layer_Out_Biases\", dtype=dtype, shape=(n_classes,),\r\n                                       initializer=tf.contrib.layers.xavier_initializer(dtype=dtype))\r\n            }\r\n\r\n        self.layer_1 = None\r\n        self.layer_2 = None\r\n        self.layer_out = None\r\n        self.expected_output = None\r\n\r\n        self.multilayer_perceptron()\r\n\r\n    def multilayer_perceptron(self):\r\n        self.layer_1 = tf.add(tf.matmul(self.input_layer, self.weights['l1']), self.biases['l1'])\r\n        self.layer_1 = self.activation_func(self.layer_1)\r\n\r\n        self.layer_2 = tf.add(tf.matmul(self.layer_1, self.weights['l2']), self.biases['l2'])\r\n        self.layer_2 = self.activation_func(self.layer_2)\r\n\r\n        self.layer_out = tf.add(tf.matmul(self.layer_2, self.weights['out']), self.biases['out'])\r\n        self.layer_out = self.activation_func(self.layer_out)\r\n\r\n        self.expected_output = tf.nn.softmax(self.layer_out, axis=1)\r\n\r\n        return self.expected_output\r\n```\r\n\r\n**I would expect that each variable only has one optimizer**."}