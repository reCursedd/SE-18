{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/353506160", "html_url": "https://github.com/tensorflow/tensorflow/issues/7116#issuecomment-353506160", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7116", "id": 353506160, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzUwNjE2MA==", "user": {"login": "fenrus75", "id": 1302037, "node_id": "MDQ6VXNlcjEzMDIwMzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1302037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fenrus75", "html_url": "https://github.com/fenrus75", "followers_url": "https://api.github.com/users/fenrus75/followers", "following_url": "https://api.github.com/users/fenrus75/following{/other_user}", "gists_url": "https://api.github.com/users/fenrus75/gists{/gist_id}", "starred_url": "https://api.github.com/users/fenrus75/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fenrus75/subscriptions", "organizations_url": "https://api.github.com/users/fenrus75/orgs", "repos_url": "https://api.github.com/users/fenrus75/repos", "events_url": "https://api.github.com/users/fenrus75/events{/privacy}", "received_events_url": "https://api.github.com/users/fenrus75/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-22T02:20:36Z", "updated_at": "2017-12-22T02:20:36Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">(more notes, apologies if this is undesired)\n\nthe 32 byte alignment comes from\n\ntensorflow/core/framework/allocator.h\n\nclass Allocator {\n public:\n#ifdef EIGEN_VECTORIZE_AVX512\n  // Align to 64 byte boundary.\n  static constexpr size_t kAllocatorAlignment = 64;\n#else\n  // Align to 32 byte boundary.\n  static c\n\nwhere provably, EIGEN_VECTORIZE_AVX512 is not actually set.\n\nOn Thu, Dec 21, 2017 at 6:13 PM, Arjan van de Ven &lt;arjanvandeven@gmail.com&gt;\nwrote:</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\"> (putting some data here to keep notes)\n\n  python3 ./tensorflow/examples/tutorials/layers/cnn_mnist.py\n\n\n  &lt;Eigen::internal::gemm_pack_lhs&lt;float, long, Eigen::internal::\n TensorContractionSubMapper&lt;float, long, 1, Eigen::TensorEvaluator&lt;Eigen::\n TensorReshapingOp&lt;Eigen::DSizes&lt;long, 2&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float\n const, 4, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const,\n Eigen::ThreadPoolDevice&gt;, Eigen::array&lt;long, 1ul&gt;, Eigen::array&lt;long, 1ul&gt;,\n 16, true, false, 0, Eigen::MakePointer&gt;, 48, 16, 0, false,\n false&gt;::operator()(float*, Eigen::internal::TensorContractionSubMapper&lt;float,\n long, 1, Eigen::TensorEvaluator&lt;Eigen::TensorReshapingOp&lt;Eigen::DSizes&lt;long,\n 2&gt; const, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 4, 1, long&gt;, 16,\n Eigen::MakePointer&gt; const&gt; const, Eigen::ThreadPoolDevice&gt;,\n Eigen::array&lt;long, 1ul&gt;, Eigen::array&lt;long, 1ul&gt;, 16, true, false, 0,\n Eigen::MakePointer&gt; const&amp;, long, long, long, long)+414&gt;\n    0x00007fffcc11eea5 &lt;+341&gt;:   lea    0x10(%r9),%rbx\n    0x00007fffcc11eea9 &lt;+345&gt;:   lea    (%rsi,%r12,4),%r10\n    0x00007fffcc11eead &lt;+349&gt;:   xor    %edx,%edx\n    0x00007fffcc11eeaf &lt;+351&gt;:   nop\n    0x00007fffcc11eeb0 &lt;+352&gt;:   mov    0x18(%rdi),%rax\n    0x00007fffcc11eeb4 &lt;+356&gt;:   mov    (%rdi),%r11\n    0x00007fffcc11eeb7 &lt;+359&gt;:   sub    $0xffffffffffffff80,%r10\n    0x00007fffcc11eebb &lt;+363&gt;:   imul   %rdx,%rax\n    0x00007fffcc11eebf &lt;+367&gt;:   add    $0x1,%rdx\n    0x00007fffcc11eec3 &lt;+371&gt;:   lea    (%rax,%r9,1),%r15\n    0x00007fffcc11eec7 &lt;+375&gt;:   add    %rbx,%rax\n    0x00007fffcc11eeca &lt;+378&gt;:   vmovups (%r11,%r15,4),%zmm1\n    0x00007fffcc11eed1 &lt;+385&gt;:   vmovups (%r11,%rax,4),%zmm0\n =&gt; 0x00007fffcc11eed8 &lt;+392&gt;:   vmovaps %zmm1,-0x80(%r10)\n    0x00007fffcc11eedf &lt;+399&gt;:   vmovaps %zmm0,-0x40(%r10)\n\n\n (gdb) display /x $r10\n 3: /x $r10 = 0x7fff3c009720\n\n\n the memory is not aligned to 64 bytes, but only to 32 bytes.. doing an\n avx512 vmovaps then will fault on this misalignment.\n\n next step, tracing down where that memory got allocated\n\n On Wed, Dec 20, 2017 at 5:39 PM, Arjan van de Ven ***@***.***\n &gt; wrote:\n\n&gt; I have not had enough time to dig through all the math to find where it\n&gt; miscomputes... it's on my (long) todo list though\n&gt;\n&gt; On Wed, Dec 20, 2017 at 1:49 PM, drpngx ***@***.***&gt; wrote:\n&gt;\n&gt;&gt; That's not good. Are you in a position to send out a fix?\n&gt;&gt;\n&gt;&gt; \u2014\n&gt;&gt; You are receiving this because you were mentioned.\n&gt;&gt; Reply to this email directly, view it on GitHub\n&gt;&gt; &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"203742014\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7116\" href=\"https://github.com/tensorflow/tensorflow/issues/7116#issuecomment-353191729\">#7116 (comment)</a>&gt;,\n&gt;&gt; or mute the thread\n&gt;&gt; &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABPeFfxeEqVJuVbKZvtOk3JZMERsgJK6ks5tCYEAgaJpZM4LwOOY\">https://github.com/notifications/unsubscribe-auth/ABPeFfxeEqVJuVbKZvtOk3JZMERsgJK6ks5tCYEAgaJpZM4LwOOY</a>&gt;\n&gt;&gt; .\n&gt;&gt;\n&gt;\n&gt;\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "(more notes, apologies if this is undesired)\n\nthe 32 byte alignment comes from\n\ntensorflow/core/framework/allocator.h\n\nclass Allocator {\n public:\n#ifdef EIGEN_VECTORIZE_AVX512\n  // Align to 64 byte boundary.\n  static constexpr size_t kAllocatorAlignment = 64;\n#else\n  // Align to 32 byte boundary.\n  static c\n\nwhere provably, EIGEN_VECTORIZE_AVX512 is not actually set.\n\nOn Thu, Dec 21, 2017 at 6:13 PM, Arjan van de Ven <arjanvandeven@gmail.com>\nwrote:\n\u2026\n (putting some data here to keep notes)\n\n  python3 ./tensorflow/examples/tutorials/layers/cnn_mnist.py\n\n\n  <Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::\n TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::\n TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float\n const, 4, 1, long>, 16, Eigen::MakePointer> const> const,\n Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>,\n 16, true, false, 0, Eigen::MakePointer>, 48, 16, 0, false,\n false>::operator()(float*, Eigen::internal::TensorContractionSubMapper<float,\n long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long,\n 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16,\n Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>,\n Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0,\n Eigen::MakePointer> const&, long, long, long, long)+414>\n    0x00007fffcc11eea5 <+341>:   lea    0x10(%r9),%rbx\n    0x00007fffcc11eea9 <+345>:   lea    (%rsi,%r12,4),%r10\n    0x00007fffcc11eead <+349>:   xor    %edx,%edx\n    0x00007fffcc11eeaf <+351>:   nop\n    0x00007fffcc11eeb0 <+352>:   mov    0x18(%rdi),%rax\n    0x00007fffcc11eeb4 <+356>:   mov    (%rdi),%r11\n    0x00007fffcc11eeb7 <+359>:   sub    $0xffffffffffffff80,%r10\n    0x00007fffcc11eebb <+363>:   imul   %rdx,%rax\n    0x00007fffcc11eebf <+367>:   add    $0x1,%rdx\n    0x00007fffcc11eec3 <+371>:   lea    (%rax,%r9,1),%r15\n    0x00007fffcc11eec7 <+375>:   add    %rbx,%rax\n    0x00007fffcc11eeca <+378>:   vmovups (%r11,%r15,4),%zmm1\n    0x00007fffcc11eed1 <+385>:   vmovups (%r11,%rax,4),%zmm0\n => 0x00007fffcc11eed8 <+392>:   vmovaps %zmm1,-0x80(%r10)\n    0x00007fffcc11eedf <+399>:   vmovaps %zmm0,-0x40(%r10)\n\n\n (gdb) display /x $r10\n 3: /x $r10 = 0x7fff3c009720\n\n\n the memory is not aligned to 64 bytes, but only to 32 bytes.. doing an\n avx512 vmovaps then will fault on this misalignment.\n\n next step, tracing down where that memory got allocated\n\n On Wed, Dec 20, 2017 at 5:39 PM, Arjan van de Ven ***@***.***\n > wrote:\n\n> I have not had enough time to dig through all the math to find where it\n> miscomputes... it's on my (long) todo list though\n>\n> On Wed, Dec 20, 2017 at 1:49 PM, drpngx ***@***.***> wrote:\n>\n>> That's not good. Are you in a position to send out a fix?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <#7116 (comment)>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABPeFfxeEqVJuVbKZvtOk3JZMERsgJK6ks5tCYEAgaJpZM4LwOOY>\n>> .\n>>\n>\n>", "body": "(more notes, apologies if this is undesired)\n\nthe 32 byte alignment comes from\n\ntensorflow/core/framework/allocator.h\n\nclass Allocator {\n public:\n#ifdef EIGEN_VECTORIZE_AVX512\n  // Align to 64 byte boundary.\n  static constexpr size_t kAllocatorAlignment = 64;\n#else\n  // Align to 32 byte boundary.\n  static c\n\nwhere provably, EIGEN_VECTORIZE_AVX512 is not actually set.\n\nOn Thu, Dec 21, 2017 at 6:13 PM, Arjan van de Ven <arjanvandeven@gmail.com>\nwrote:\n\n> (putting some data here to keep notes)\n>\n>  python3 ./tensorflow/examples/tutorials/layers/cnn_mnist.py\n>\n>\n>  <Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::\n> TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::\n> TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float\n> const, 4, 1, long>, 16, Eigen::MakePointer> const> const,\n> Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>,\n> 16, true, false, 0, Eigen::MakePointer>, 48, 16, 0, false,\n> false>::operator()(float*, Eigen::internal::TensorContractionSubMapper<float,\n> long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long,\n> 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16,\n> Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>,\n> Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0,\n> Eigen::MakePointer> const&, long, long, long, long)+414>\n>    0x00007fffcc11eea5 <+341>:   lea    0x10(%r9),%rbx\n>    0x00007fffcc11eea9 <+345>:   lea    (%rsi,%r12,4),%r10\n>    0x00007fffcc11eead <+349>:   xor    %edx,%edx\n>    0x00007fffcc11eeaf <+351>:   nop\n>    0x00007fffcc11eeb0 <+352>:   mov    0x18(%rdi),%rax\n>    0x00007fffcc11eeb4 <+356>:   mov    (%rdi),%r11\n>    0x00007fffcc11eeb7 <+359>:   sub    $0xffffffffffffff80,%r10\n>    0x00007fffcc11eebb <+363>:   imul   %rdx,%rax\n>    0x00007fffcc11eebf <+367>:   add    $0x1,%rdx\n>    0x00007fffcc11eec3 <+371>:   lea    (%rax,%r9,1),%r15\n>    0x00007fffcc11eec7 <+375>:   add    %rbx,%rax\n>    0x00007fffcc11eeca <+378>:   vmovups (%r11,%r15,4),%zmm1\n>    0x00007fffcc11eed1 <+385>:   vmovups (%r11,%rax,4),%zmm0\n> => 0x00007fffcc11eed8 <+392>:   vmovaps %zmm1,-0x80(%r10)\n>    0x00007fffcc11eedf <+399>:   vmovaps %zmm0,-0x40(%r10)\n>\n>\n> (gdb) display /x $r10\n> 3: /x $r10 = 0x7fff3c009720\n>\n>\n> the memory is not aligned to 64 bytes, but only to 32 bytes.. doing an\n> avx512 vmovaps then will fault on this misalignment.\n>\n> next step, tracing down where that memory got allocated\n>\n> On Wed, Dec 20, 2017 at 5:39 PM, Arjan van de Ven <arjanvandeven@gmail.com\n> > wrote:\n>\n>> I have not had enough time to dig through all the math to find where it\n>> miscomputes... it's on my (long) todo list though\n>>\n>> On Wed, Dec 20, 2017 at 1:49 PM, drpngx <notifications@github.com> wrote:\n>>\n>>> That's not good. Are you in a position to send out a fix?\n>>>\n>>> \u2014\n>>> You are receiving this because you were mentioned.\n>>> Reply to this email directly, view it on GitHub\n>>> <https://github.com/tensorflow/tensorflow/issues/7116#issuecomment-353191729>,\n>>> or mute the thread\n>>> <https://github.com/notifications/unsubscribe-auth/ABPeFfxeEqVJuVbKZvtOk3JZMERsgJK6ks5tCYEAgaJpZM4LwOOY>\n>>> .\n>>>\n>>\n>>\n>\n"}