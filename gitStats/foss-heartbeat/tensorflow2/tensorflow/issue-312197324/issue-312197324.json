{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18310", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18310/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18310/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18310/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18310", "id": 312197324, "node_id": "MDU6SXNzdWUzMTIxOTczMjQ=", "number": 18310, "title": "Feature Request: Locally Connected Conv2d", "user": {"login": "Windaway", "id": 4530735, "node_id": "MDQ6VXNlcjQ1MzA3MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4530735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Windaway", "html_url": "https://github.com/Windaway", "followers_url": "https://api.github.com/users/Windaway/followers", "following_url": "https://api.github.com/users/Windaway/following{/other_user}", "gists_url": "https://api.github.com/users/Windaway/gists{/gist_id}", "starred_url": "https://api.github.com/users/Windaway/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Windaway/subscriptions", "organizations_url": "https://api.github.com/users/Windaway/orgs", "repos_url": "https://api.github.com/users/Windaway/repos", "events_url": "https://api.github.com/users/Windaway/events{/privacy}", "received_events_url": "https://api.github.com/users/Windaway/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-04-07T10:58:34Z", "updated_at": "2018-05-27T05:13:30Z", "closed_at": "2018-05-27T05:13:30Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>Need new features, the local conv is avaiable in Keras.<br>\n<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D</a><br>\nBut tensorflow does not have this feature. I have writen this layer with tensorlayer, it can work, but the initializing is too slow. So I want offical to release a better implementation. Thanks</p>\n<h3>Source code / logs</h3>\n<pre><code>class LocalConnectedConv(Layer):\n    def __init__(\n        self,\n        layer = None,\n        filters=0,\n        size=3,\n        multiplexH=1,\n        multiplexW=1,\n        stride=1,\n        overlap=True,\n        act = tf.identity,\n        name ='lcconv2d',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        channels = int(self.inputs.get_shape()[-1])\n        inputH=int(self.inputs.get_shape()[1])\n        inputW = int(self.inputs.get_shape()[2])\n        if inputH%multiplexH==0 and inputW%multiplexW==0:\n            print('ok')\n        else:\n            return 0\n        CellH= int(np.floor(inputH/multiplexH))\n        CellW=int(np.floor(inputW/multiplexW))\n        CellHm= int(np.floor(inputH/multiplexH))\n        CellWm=int(np.floor(inputW/multiplexW))\n        if overlap:\n            CellH = np.floor(inputH / multiplexH*2)\n            CellW = np.floor(inputW / multiplexW*2)\n            CellH=int(CellH)\n            CellW=int(CellW)\n        Wd=False\n        Hd=False\n        if CellH%2==0:\n            Hd=True\n        if CellW%2==0:\n            Wd=True\n        with tf.variable_scope(name) as vs:\n            Welist=[]\n            Bilist=[]\n            for i in range(multiplexH):\n                for j in range(multiplexW):\n\n                    We = tf.get_variable(name='weights%d-%d'%(i,j), shape=[size, size, channels, filters],\n                                               initializer=tf.truncated_normal_initializer(stddev=0.03),\n                                               dtype=tf.float32, trainable=True)\n                    bi = tf.get_variable(name='biases%d-%d'%(i,j), shape=[filters, ],\n                                              initializer=tf.constant_initializer(value=0.1),\n                                              dtype=tf.float32, trainable=True)\n                    Welist.append(We)\n                    Bilist.append(bi)\n        Convij=[]\n        for i in range(multiplexH):\n            for j in range(multiplexW):\n                ci=np.floor((i+0.5)*CellHm-0.01)+1\n                cj=np.floor((j+0.5)*CellWm-0.01)+1\n                if not overlap:\n                    if i==0:\n                        hcs=0\n                        hce=hcs+CellH+size-1\n                    elif i==multiplexH-1:\n                        hce=inputH\n                        hcs = hce-CellH-size+1\n                    elif Hd:\n                        hcs=ci-(CellH+size-1)/2\n                        hce=ci+(CellH+size-1)/2\n                    else:\n                        hcs=ci-np.floor((CellH+size-1)*0.5-0.01)-1\n                        hce=ci+np.floor((CellH+size-1)*0.5-0.01)\n                    if j==0:\n                        wcs=0\n                        wce=wcs+CellW+size-1\n                    elif j==multiplexW-1:\n                        wce=inputW\n                        wcs = wce-CellW-size+1\n                    elif Wd:\n                        wcs=cj-(CellW+size-1)/2\n                        wce=cj+(CellW+size-1)/2\n                    else:\n                        wcs=cj-np.floor((CellW+size-1)*0.5-0.01)-1\n                        wce=cj+np.floor((CellW+size-1)*0.5-0.01)\n                else:\n                    if i == 0:\n                        hcs = 0\n                        hce = hcs + CellH\n                    elif i == multiplexH - 1:\n                        hce = inputH\n                        hcs = hce - CellH\n                    elif Hd:\n                        hcs = ci - (CellH) / 2\n                        hce = ci + (CellH) / 2\n                    else:\n                        hcs = ci - np.floor((CellH ) * 0.5 - 0.01)-1\n                        hce = ci + np.floor((CellH ) * 0.5 - 0.01)\n                    if j == 0:\n                        wcs = 0\n                        wce = wcs + CellW\n                    elif j == multiplexW - 1:\n                        wce = inputW\n                        wcs = wce - CellW\n                    elif Wd:\n                        wcs = cj - (CellW ) / 2\n                        wce = cj + (CellW ) / 2\n                    else:\n                        wcs = cj - np.floor((CellW ) * 0.5 - 0.01)-1\n                        wce = cj + np.floor((CellW) * 0.5 - 0.01)\n                hcs=int(hcs)\n                wcs=int(wcs)\n                hce=int(hce)\n                wce=int(wce)\n                it=self.inputs[:,hcs:hce,wcs:wce,:]\n                convtemp=tf.nn.conv2d(it,Welist[multiplexW*i+j], strides=[1, stride, stride, 1], padding='VALID')\n                convtemp=tf.add(convtemp, Bilist[multiplexW*i+j])\n                Convij.append(convtemp)\n        convli=[]\n        for i in range(multiplexH):\n            convlii=[]\n            for j in range(multiplexW):\n                convlii.append(Convij[multiplexW * i + j])\n            convli.append(convlii)\n        convt=[]\n        for i in range(multiplexH):\n            convt.append(tf.concat(convli[i],axis=2))\n        convfin=tf.concat(convt,axis=1)\n        self.outputs =act(convfin)\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        self.all_params.extend(Welist)\n        self.all_params.extend(Bilist)\n</code></pre>", "body_text": "Describe the problem\nNeed new features, the local conv is avaiable in Keras.\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D\nBut tensorflow does not have this feature. I have writen this layer with tensorlayer, it can work, but the initializing is too slow. So I want offical to release a better implementation. Thanks\nSource code / logs\nclass LocalConnectedConv(Layer):\n    def __init__(\n        self,\n        layer = None,\n        filters=0,\n        size=3,\n        multiplexH=1,\n        multiplexW=1,\n        stride=1,\n        overlap=True,\n        act = tf.identity,\n        name ='lcconv2d',\n    ):\n        Layer.__init__(self, name=name)\n        self.inputs = layer.outputs\n        channels = int(self.inputs.get_shape()[-1])\n        inputH=int(self.inputs.get_shape()[1])\n        inputW = int(self.inputs.get_shape()[2])\n        if inputH%multiplexH==0 and inputW%multiplexW==0:\n            print('ok')\n        else:\n            return 0\n        CellH= int(np.floor(inputH/multiplexH))\n        CellW=int(np.floor(inputW/multiplexW))\n        CellHm= int(np.floor(inputH/multiplexH))\n        CellWm=int(np.floor(inputW/multiplexW))\n        if overlap:\n            CellH = np.floor(inputH / multiplexH*2)\n            CellW = np.floor(inputW / multiplexW*2)\n            CellH=int(CellH)\n            CellW=int(CellW)\n        Wd=False\n        Hd=False\n        if CellH%2==0:\n            Hd=True\n        if CellW%2==0:\n            Wd=True\n        with tf.variable_scope(name) as vs:\n            Welist=[]\n            Bilist=[]\n            for i in range(multiplexH):\n                for j in range(multiplexW):\n\n                    We = tf.get_variable(name='weights%d-%d'%(i,j), shape=[size, size, channels, filters],\n                                               initializer=tf.truncated_normal_initializer(stddev=0.03),\n                                               dtype=tf.float32, trainable=True)\n                    bi = tf.get_variable(name='biases%d-%d'%(i,j), shape=[filters, ],\n                                              initializer=tf.constant_initializer(value=0.1),\n                                              dtype=tf.float32, trainable=True)\n                    Welist.append(We)\n                    Bilist.append(bi)\n        Convij=[]\n        for i in range(multiplexH):\n            for j in range(multiplexW):\n                ci=np.floor((i+0.5)*CellHm-0.01)+1\n                cj=np.floor((j+0.5)*CellWm-0.01)+1\n                if not overlap:\n                    if i==0:\n                        hcs=0\n                        hce=hcs+CellH+size-1\n                    elif i==multiplexH-1:\n                        hce=inputH\n                        hcs = hce-CellH-size+1\n                    elif Hd:\n                        hcs=ci-(CellH+size-1)/2\n                        hce=ci+(CellH+size-1)/2\n                    else:\n                        hcs=ci-np.floor((CellH+size-1)*0.5-0.01)-1\n                        hce=ci+np.floor((CellH+size-1)*0.5-0.01)\n                    if j==0:\n                        wcs=0\n                        wce=wcs+CellW+size-1\n                    elif j==multiplexW-1:\n                        wce=inputW\n                        wcs = wce-CellW-size+1\n                    elif Wd:\n                        wcs=cj-(CellW+size-1)/2\n                        wce=cj+(CellW+size-1)/2\n                    else:\n                        wcs=cj-np.floor((CellW+size-1)*0.5-0.01)-1\n                        wce=cj+np.floor((CellW+size-1)*0.5-0.01)\n                else:\n                    if i == 0:\n                        hcs = 0\n                        hce = hcs + CellH\n                    elif i == multiplexH - 1:\n                        hce = inputH\n                        hcs = hce - CellH\n                    elif Hd:\n                        hcs = ci - (CellH) / 2\n                        hce = ci + (CellH) / 2\n                    else:\n                        hcs = ci - np.floor((CellH ) * 0.5 - 0.01)-1\n                        hce = ci + np.floor((CellH ) * 0.5 - 0.01)\n                    if j == 0:\n                        wcs = 0\n                        wce = wcs + CellW\n                    elif j == multiplexW - 1:\n                        wce = inputW\n                        wcs = wce - CellW\n                    elif Wd:\n                        wcs = cj - (CellW ) / 2\n                        wce = cj + (CellW ) / 2\n                    else:\n                        wcs = cj - np.floor((CellW ) * 0.5 - 0.01)-1\n                        wce = cj + np.floor((CellW) * 0.5 - 0.01)\n                hcs=int(hcs)\n                wcs=int(wcs)\n                hce=int(hce)\n                wce=int(wce)\n                it=self.inputs[:,hcs:hce,wcs:wce,:]\n                convtemp=tf.nn.conv2d(it,Welist[multiplexW*i+j], strides=[1, stride, stride, 1], padding='VALID')\n                convtemp=tf.add(convtemp, Bilist[multiplexW*i+j])\n                Convij.append(convtemp)\n        convli=[]\n        for i in range(multiplexH):\n            convlii=[]\n            for j in range(multiplexW):\n                convlii.append(Convij[multiplexW * i + j])\n            convli.append(convlii)\n        convt=[]\n        for i in range(multiplexH):\n            convt.append(tf.concat(convli[i],axis=2))\n        convfin=tf.concat(convt,axis=1)\n        self.outputs =act(convfin)\n        self.all_layers = list(layer.all_layers)\n        self.all_params = list(layer.all_params)\n        self.all_drop = dict(layer.all_drop)\n        self.all_layers.extend([self.outputs])\n        self.all_params.extend(Welist)\n        self.all_params.extend(Bilist)", "body": "### Describe the problem\r\nNeed new features, the local conv is avaiable in Keras.\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D\r\nBut tensorflow does not have this feature. I have writen this layer with tensorlayer, it can work, but the initializing is too slow. So I want offical to release a better implementation. Thanks\r\n### Source code / logs\r\n```\r\nclass LocalConnectedConv(Layer):\r\n    def __init__(\r\n        self,\r\n        layer = None,\r\n        filters=0,\r\n        size=3,\r\n        multiplexH=1,\r\n        multiplexW=1,\r\n        stride=1,\r\n        overlap=True,\r\n        act = tf.identity,\r\n        name ='lcconv2d',\r\n    ):\r\n        Layer.__init__(self, name=name)\r\n        self.inputs = layer.outputs\r\n        channels = int(self.inputs.get_shape()[-1])\r\n        inputH=int(self.inputs.get_shape()[1])\r\n        inputW = int(self.inputs.get_shape()[2])\r\n        if inputH%multiplexH==0 and inputW%multiplexW==0:\r\n            print('ok')\r\n        else:\r\n            return 0\r\n        CellH= int(np.floor(inputH/multiplexH))\r\n        CellW=int(np.floor(inputW/multiplexW))\r\n        CellHm= int(np.floor(inputH/multiplexH))\r\n        CellWm=int(np.floor(inputW/multiplexW))\r\n        if overlap:\r\n            CellH = np.floor(inputH / multiplexH*2)\r\n            CellW = np.floor(inputW / multiplexW*2)\r\n            CellH=int(CellH)\r\n            CellW=int(CellW)\r\n        Wd=False\r\n        Hd=False\r\n        if CellH%2==0:\r\n            Hd=True\r\n        if CellW%2==0:\r\n            Wd=True\r\n        with tf.variable_scope(name) as vs:\r\n            Welist=[]\r\n            Bilist=[]\r\n            for i in range(multiplexH):\r\n                for j in range(multiplexW):\r\n\r\n                    We = tf.get_variable(name='weights%d-%d'%(i,j), shape=[size, size, channels, filters],\r\n                                               initializer=tf.truncated_normal_initializer(stddev=0.03),\r\n                                               dtype=tf.float32, trainable=True)\r\n                    bi = tf.get_variable(name='biases%d-%d'%(i,j), shape=[filters, ],\r\n                                              initializer=tf.constant_initializer(value=0.1),\r\n                                              dtype=tf.float32, trainable=True)\r\n                    Welist.append(We)\r\n                    Bilist.append(bi)\r\n        Convij=[]\r\n        for i in range(multiplexH):\r\n            for j in range(multiplexW):\r\n                ci=np.floor((i+0.5)*CellHm-0.01)+1\r\n                cj=np.floor((j+0.5)*CellWm-0.01)+1\r\n                if not overlap:\r\n                    if i==0:\r\n                        hcs=0\r\n                        hce=hcs+CellH+size-1\r\n                    elif i==multiplexH-1:\r\n                        hce=inputH\r\n                        hcs = hce-CellH-size+1\r\n                    elif Hd:\r\n                        hcs=ci-(CellH+size-1)/2\r\n                        hce=ci+(CellH+size-1)/2\r\n                    else:\r\n                        hcs=ci-np.floor((CellH+size-1)*0.5-0.01)-1\r\n                        hce=ci+np.floor((CellH+size-1)*0.5-0.01)\r\n                    if j==0:\r\n                        wcs=0\r\n                        wce=wcs+CellW+size-1\r\n                    elif j==multiplexW-1:\r\n                        wce=inputW\r\n                        wcs = wce-CellW-size+1\r\n                    elif Wd:\r\n                        wcs=cj-(CellW+size-1)/2\r\n                        wce=cj+(CellW+size-1)/2\r\n                    else:\r\n                        wcs=cj-np.floor((CellW+size-1)*0.5-0.01)-1\r\n                        wce=cj+np.floor((CellW+size-1)*0.5-0.01)\r\n                else:\r\n                    if i == 0:\r\n                        hcs = 0\r\n                        hce = hcs + CellH\r\n                    elif i == multiplexH - 1:\r\n                        hce = inputH\r\n                        hcs = hce - CellH\r\n                    elif Hd:\r\n                        hcs = ci - (CellH) / 2\r\n                        hce = ci + (CellH) / 2\r\n                    else:\r\n                        hcs = ci - np.floor((CellH ) * 0.5 - 0.01)-1\r\n                        hce = ci + np.floor((CellH ) * 0.5 - 0.01)\r\n                    if j == 0:\r\n                        wcs = 0\r\n                        wce = wcs + CellW\r\n                    elif j == multiplexW - 1:\r\n                        wce = inputW\r\n                        wcs = wce - CellW\r\n                    elif Wd:\r\n                        wcs = cj - (CellW ) / 2\r\n                        wce = cj + (CellW ) / 2\r\n                    else:\r\n                        wcs = cj - np.floor((CellW ) * 0.5 - 0.01)-1\r\n                        wce = cj + np.floor((CellW) * 0.5 - 0.01)\r\n                hcs=int(hcs)\r\n                wcs=int(wcs)\r\n                hce=int(hce)\r\n                wce=int(wce)\r\n                it=self.inputs[:,hcs:hce,wcs:wce,:]\r\n                convtemp=tf.nn.conv2d(it,Welist[multiplexW*i+j], strides=[1, stride, stride, 1], padding='VALID')\r\n                convtemp=tf.add(convtemp, Bilist[multiplexW*i+j])\r\n                Convij.append(convtemp)\r\n        convli=[]\r\n        for i in range(multiplexH):\r\n            convlii=[]\r\n            for j in range(multiplexW):\r\n                convlii.append(Convij[multiplexW * i + j])\r\n            convli.append(convlii)\r\n        convt=[]\r\n        for i in range(multiplexH):\r\n            convt.append(tf.concat(convli[i],axis=2))\r\n        convfin=tf.concat(convt,axis=1)\r\n        self.outputs =act(convfin)\r\n        self.all_layers = list(layer.all_layers)\r\n        self.all_params = list(layer.all_params)\r\n        self.all_drop = dict(layer.all_drop)\r\n        self.all_layers.extend([self.outputs])\r\n        self.all_params.extend(Welist)\r\n        self.all_params.extend(Bilist)\r\n```\r\n"}