{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/431258963", "html_url": "https://github.com/tensorflow/tensorflow/pull/22642#issuecomment-431258963", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22642", "id": 431258963, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTI1ODk2Mw==", "user": {"login": "wangsiyu", "id": 5387343, "node_id": "MDQ6VXNlcjUzODczNDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5387343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangsiyu", "html_url": "https://github.com/wangsiyu", "followers_url": "https://api.github.com/users/wangsiyu/followers", "following_url": "https://api.github.com/users/wangsiyu/following{/other_user}", "gists_url": "https://api.github.com/users/wangsiyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangsiyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangsiyu/subscriptions", "organizations_url": "https://api.github.com/users/wangsiyu/orgs", "repos_url": "https://api.github.com/users/wangsiyu/repos", "events_url": "https://api.github.com/users/wangsiyu/events{/privacy}", "received_events_url": "https://api.github.com/users/wangsiyu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-19T06:30:53Z", "updated_at": "2018-10-19T06:30:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1647833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuefengz\">@yuefengz</a> <code>CollectiveAllReduceStrategy </code> is in sync mode but it is not based on Parameter Server architecture. Sometimes the users pick <code>ParameterServerStrategy</code> for distributing the training variables across servers.</p>", "body_text": "@yuefengz CollectiveAllReduceStrategy  is in sync mode but it is not based on Parameter Server architecture. Sometimes the users pick ParameterServerStrategy for distributing the training variables across servers.", "body": "@yuefengz `CollectiveAllReduceStrategy ` is in sync mode but it is not based on Parameter Server architecture. Sometimes the users pick `ParameterServerStrategy` for distributing the training variables across servers.  "}