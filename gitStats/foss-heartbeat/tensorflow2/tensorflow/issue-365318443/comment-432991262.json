{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/432991262", "html_url": "https://github.com/tensorflow/tensorflow/pull/22642#issuecomment-432991262", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22642", "id": 432991262, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjk5MTI2Mg==", "user": {"login": "wangsiyu", "id": 5387343, "node_id": "MDQ6VXNlcjUzODczNDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5387343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangsiyu", "html_url": "https://github.com/wangsiyu", "followers_url": "https://api.github.com/users/wangsiyu/followers", "following_url": "https://api.github.com/users/wangsiyu/following{/other_user}", "gists_url": "https://api.github.com/users/wangsiyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangsiyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangsiyu/subscriptions", "organizations_url": "https://api.github.com/users/wangsiyu/orgs", "repos_url": "https://api.github.com/users/wangsiyu/repos", "events_url": "https://api.github.com/users/wangsiyu/events{/privacy}", "received_events_url": "https://api.github.com/users/wangsiyu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-25T10:00:05Z", "updated_at": "2018-10-25T10:00:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=32556631\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/anj-s\">@anj-s</a> This is because many businesses use sync mode based on Parameter Server architecture in Alibaba including CNN and Seq2Seq models. CNN may works well with <code>CollectiveAllReduceStrategy </code> because we usually don't need to partition variables across different servers. However, some Seq2Seq tasks may involved with large variables but update in sparse.  Although async mode is also suitable for computation in this case but sync mode may be better for convergence.</p>\n<p>You mentioned that you are not actively working on sync mode in PS strategy and will explore other solutions which is not depend on <code>SyncReplicasOptimizer</code>. Does it mean PS strategy will deprecat <code>SyncReplicasOptimizer</code> in the future? Why ? Is it because we cannot avoid modifying models code to wrap common optimizer\uff1f</p>", "body_text": "@anj-s This is because many businesses use sync mode based on Parameter Server architecture in Alibaba including CNN and Seq2Seq models. CNN may works well with CollectiveAllReduceStrategy  because we usually don't need to partition variables across different servers. However, some Seq2Seq tasks may involved with large variables but update in sparse.  Although async mode is also suitable for computation in this case but sync mode may be better for convergence.\nYou mentioned that you are not actively working on sync mode in PS strategy and will explore other solutions which is not depend on SyncReplicasOptimizer. Does it mean PS strategy will deprecat SyncReplicasOptimizer in the future? Why ? Is it because we cannot avoid modifying models code to wrap common optimizer\uff1f", "body": "@anj-s This is because many businesses use sync mode based on Parameter Server architecture in Alibaba including CNN and Seq2Seq models. CNN may works well with `CollectiveAllReduceStrategy ` because we usually don't need to partition variables across different servers. However, some Seq2Seq tasks may involved with large variables but update in sparse.  Although async mode is also suitable for computation in this case but sync mode may be better for convergence. \r\n\r\nYou mentioned that you are not actively working on sync mode in PS strategy and will explore other solutions which is not depend on `SyncReplicasOptimizer`. Does it mean PS strategy will deprecat `SyncReplicasOptimizer` in the future? Why ? Is it because we cannot avoid modifying models code to wrap common optimizer\uff1f"}