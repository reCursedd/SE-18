{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423733796", "html_url": "https://github.com/tensorflow/tensorflow/issues/16028#issuecomment-423733796", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16028", "id": 423733796, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzczMzc5Ng==", "user": {"login": "ajay-anand", "id": 4209718, "node_id": "MDQ6VXNlcjQyMDk3MTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/4209718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ajay-anand", "html_url": "https://github.com/ajay-anand", "followers_url": "https://api.github.com/users/ajay-anand/followers", "following_url": "https://api.github.com/users/ajay-anand/following{/other_user}", "gists_url": "https://api.github.com/users/ajay-anand/gists{/gist_id}", "starred_url": "https://api.github.com/users/ajay-anand/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ajay-anand/subscriptions", "organizations_url": "https://api.github.com/users/ajay-anand/orgs", "repos_url": "https://api.github.com/users/ajay-anand/repos", "events_url": "https://api.github.com/users/ajay-anand/events{/privacy}", "received_events_url": "https://api.github.com/users/ajay-anand/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-22T10:31:10Z", "updated_at": "2018-09-22T10:31:10Z", "author_association": "NONE", "body_html": "<p>Just a comment,</p>\n<p>Let y = max( a, b) such that a &gt;&gt; b,<br>\nthen we can approximate it as<br>\ny = log(e^a + e^b)<br>\nIf we compute the gradient with respect to x,<br>\n\u2202y/\u2202x = \u2202a/\u2202x e^a / (e^a+e^b) + \u2202b/\u2202x e^b / (e^a+e^b)<br>\nNotice that with the assumption a &gt;&gt; b, the first term approaches to \u2202a/\u2202x and the second term vanishes. This means if a &gt;&gt; b, it is enough to propagate gradients and update parameters associated only with <strong>a</strong> whenever expression max(a,b) appears.</p>\n<p>If the assumption that (a &gt;&gt; b) or (b &gt;&gt; a) is true, only then max(a, b) can be converted to a continuous function as done above. Otherwise we need another measure, say a distance measure to approximate non-continuous max with a  continuous function in a fuzzy sense. This 'distance' in essence is a fuzzification parameter that will help relatively define whether a and b are 'similar' or 'way too different'. But this distance measure depends on the nature and distribution of data, so it has to be either supplied by the user or discovered from the data itself (if at all a broader view of data is available).</p>", "body_text": "Just a comment,\nLet y = max( a, b) such that a >> b,\nthen we can approximate it as\ny = log(e^a + e^b)\nIf we compute the gradient with respect to x,\n\u2202y/\u2202x = \u2202a/\u2202x e^a / (e^a+e^b) + \u2202b/\u2202x e^b / (e^a+e^b)\nNotice that with the assumption a >> b, the first term approaches to \u2202a/\u2202x and the second term vanishes. This means if a >> b, it is enough to propagate gradients and update parameters associated only with a whenever expression max(a,b) appears.\nIf the assumption that (a >> b) or (b >> a) is true, only then max(a, b) can be converted to a continuous function as done above. Otherwise we need another measure, say a distance measure to approximate non-continuous max with a  continuous function in a fuzzy sense. This 'distance' in essence is a fuzzification parameter that will help relatively define whether a and b are 'similar' or 'way too different'. But this distance measure depends on the nature and distribution of data, so it has to be either supplied by the user or discovered from the data itself (if at all a broader view of data is available).", "body": "Just a comment,\r\n\r\nLet y = max( a, b) such that a >> b,\r\nthen we can approximate it as \r\ny = log(e^a + e^b)\r\nIf we compute the gradient with respect to x, \r\n\u2202y/\u2202x = \u2202a/\u2202x e^a / (e^a+e^b) + \u2202b/\u2202x e^b / (e^a+e^b)\r\nNotice that with the assumption a >> b, the first term approaches to \u2202a/\u2202x and the second term vanishes. This means if a >> b, it is enough to propagate gradients and update parameters associated only with **a** whenever expression max(a,b) appears. \r\n\r\nIf the assumption that (a >> b) or (b >> a) is true, only then max(a, b) can be converted to a continuous function as done above. Otherwise we need another measure, say a distance measure to approximate non-continuous max with a  continuous function in a fuzzy sense. This 'distance' in essence is a fuzzification parameter that will help relatively define whether a and b are 'similar' or 'way too different'. But this distance measure depends on the nature and distribution of data, so it has to be either supplied by the user or discovered from the data itself (if at all a broader view of data is available). "}