{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356972650", "html_url": "https://github.com/tensorflow/tensorflow/issues/16028#issuecomment-356972650", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16028", "id": 356972650, "node_id": "MDEyOklzc3VlQ29tbWVudDM1Njk3MjY1MA==", "user": {"login": "olegserov", "id": 129835, "node_id": "MDQ6VXNlcjEyOTgzNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/129835?v=4", "gravatar_id": "", "url": "https://api.github.com/users/olegserov", "html_url": "https://github.com/olegserov", "followers_url": "https://api.github.com/users/olegserov/followers", "following_url": "https://api.github.com/users/olegserov/following{/other_user}", "gists_url": "https://api.github.com/users/olegserov/gists{/gist_id}", "starred_url": "https://api.github.com/users/olegserov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/olegserov/subscriptions", "organizations_url": "https://api.github.com/users/olegserov/orgs", "repos_url": "https://api.github.com/users/olegserov/repos", "events_url": "https://api.github.com/users/olegserov/events{/privacy}", "received_events_url": "https://api.github.com/users/olegserov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-11T16:00:44Z", "updated_at": "2018-01-11T16:33:10Z", "author_association": "NONE", "body_html": "<p><strong>Mandatory questions:</strong><br>\nHave I written custom code: N/A? Yes? I have no idea.<br>\nOS Platform and Distribution - Any N/A<br>\nTensorFlow installed from - pypi<br>\nTensorFlow version - '1.4.1'<br>\nBazel version - N/A<br>\nCUDA/cuDNN version - N/A<br>\nGPU model and memory - N/A<br>\nExact command to reproduce.</p>\n<p><strong>Slow optimizer:</strong></p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\na = tf.Variable(10.0)\nb = tf.Variable(9.0)\nloss = tf.maximum(a, b)\n\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\nsess = tf.InteractiveSession()\n\ntf.global_variables_initializer().run()\n\n\ndef print_state(sess, step, a, b, loss):\n    ret = sess.run({\n        'a': a,\n        'b': b,\n        'loss': loss\n    }, feed_dict={})\n    \n    print (\"Step: %3s:\\ta=%04.01f\\tb=%04.01f\\tloss=%04.01f\" % (\n        step,\n        ret['a'],\n        ret['b'],\n        ret['loss']\n    ))\n\n\nprint_state(sess, -1, a, b, loss)\n\nfor step in range(10):\n    sess.run(train_step, feed_dict={})\n\n    print_state(sess, step, a, b, loss)\n\n</code></pre>\n<p>Outputs:</p>\n<pre><code>Step:  -1:\ta=10.0\tb=09.0\tloss=10.0\nStep:   0:\ta=09.5\tb=09.0\tloss=09.5\nStep:   1:\ta=09.0\tb=09.0\tloss=09.0\nStep:   2:\ta=08.5\tb=09.0\tloss=09.0\nStep:   3:\ta=08.5\tb=08.5\tloss=08.5\nStep:   4:\ta=08.0\tb=08.5\tloss=08.5\nStep:   5:\ta=08.0\tb=08.0\tloss=08.0\nStep:   6:\ta=07.5\tb=08.0\tloss=08.0\nStep:   7:\ta=07.5\tb=07.5\tloss=07.5\nStep:   8:\ta=07.0\tb=07.5\tloss=07.5\nStep:   9:\ta=07.0\tb=07.0\tloss=07.0\n</code></pre>\n<p>As you can see it is pulling <code>a</code> down, then <code>b</code>, and then again. It will require less steps if it is going to change both <code>a</code> and <code>b</code> together.</p>\n<p><strong>Optimizer playing whack-a-feature game</strong></p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\na = tf.Variable(10.0)\nb = 10.0 - a\nloss = tf.maximum(a, b)\n\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\nsess = tf.InteractiveSession()\n\ntf.global_variables_initializer().run()\n\n\ndef print_state(sess, step, a, b, loss):\n    ret = sess.run({\n        'a': a,\n        'b': b,\n        'loss': loss\n    }, feed_dict={})\n    \n    print (\"Step: %3s:\\ta=%04.01f\\tb=%04.01f\\tloss=%04.01f\" % (\n        step,\n        ret['a'],\n        ret['b'],\n        ret['loss']\n    ))\n\n\nprint_state(sess, -1, a, b, loss)\n\nfor step in range(20):\n    sess.run(train_step, feed_dict={})\n\n    print_state(sess, step, a, b, loss)\n</code></pre>\n<pre><code>Step:  -1:\ta=10.0\tb=00.0\tloss=10.0\nStep:   0:\ta=09.5\tb=00.5\tloss=09.5\nStep:   1:\ta=09.0\tb=01.0\tloss=09.0\nStep:   2:\ta=08.5\tb=01.5\tloss=08.5\nStep:   3:\ta=08.0\tb=02.0\tloss=08.0\nStep:   4:\ta=07.5\tb=02.5\tloss=07.5\nStep:   5:\ta=07.0\tb=03.0\tloss=07.0\nStep:   6:\ta=06.5\tb=03.5\tloss=06.5\nStep:   7:\ta=06.0\tb=04.0\tloss=06.0\nStep:   8:\ta=05.5\tb=04.5\tloss=05.5\nStep:   9:\ta=05.0\tb=05.0\tloss=05.0\nStep:  10:\ta=04.5\tb=05.5\tloss=05.5\nStep:  11:\ta=05.0\tb=05.0\tloss=05.0\nStep:  12:\ta=04.5\tb=05.5\tloss=05.5\nStep:  13:\ta=05.0\tb=05.0\tloss=05.0\nStep:  14:\ta=04.5\tb=05.5\tloss=05.5\nStep:  15:\ta=05.0\tb=05.0\tloss=05.0\nStep:  16:\ta=04.5\tb=05.5\tloss=05.5\nStep:  17:\ta=05.0\tb=05.0\tloss=05.0\nStep:  18:\ta=04.5\tb=05.5\tloss=05.5\nStep:  19:\ta=05.0\tb=05.0\tloss=05.0\n</code></pre>\n<p>As you can see, it is stuck in a cycle. If it considered loss for both <code>a</code> and <code>b</code> it would have quickly come to the equilibrium.</p>", "body_text": "Mandatory questions:\nHave I written custom code: N/A? Yes? I have no idea.\nOS Platform and Distribution - Any N/A\nTensorFlow installed from - pypi\nTensorFlow version - '1.4.1'\nBazel version - N/A\nCUDA/cuDNN version - N/A\nGPU model and memory - N/A\nExact command to reproduce.\nSlow optimizer:\nimport numpy as np\nimport tensorflow as tf\n\na = tf.Variable(10.0)\nb = tf.Variable(9.0)\nloss = tf.maximum(a, b)\n\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\nsess = tf.InteractiveSession()\n\ntf.global_variables_initializer().run()\n\n\ndef print_state(sess, step, a, b, loss):\n    ret = sess.run({\n        'a': a,\n        'b': b,\n        'loss': loss\n    }, feed_dict={})\n    \n    print (\"Step: %3s:\\ta=%04.01f\\tb=%04.01f\\tloss=%04.01f\" % (\n        step,\n        ret['a'],\n        ret['b'],\n        ret['loss']\n    ))\n\n\nprint_state(sess, -1, a, b, loss)\n\nfor step in range(10):\n    sess.run(train_step, feed_dict={})\n\n    print_state(sess, step, a, b, loss)\n\n\nOutputs:\nStep:  -1:\ta=10.0\tb=09.0\tloss=10.0\nStep:   0:\ta=09.5\tb=09.0\tloss=09.5\nStep:   1:\ta=09.0\tb=09.0\tloss=09.0\nStep:   2:\ta=08.5\tb=09.0\tloss=09.0\nStep:   3:\ta=08.5\tb=08.5\tloss=08.5\nStep:   4:\ta=08.0\tb=08.5\tloss=08.5\nStep:   5:\ta=08.0\tb=08.0\tloss=08.0\nStep:   6:\ta=07.5\tb=08.0\tloss=08.0\nStep:   7:\ta=07.5\tb=07.5\tloss=07.5\nStep:   8:\ta=07.0\tb=07.5\tloss=07.5\nStep:   9:\ta=07.0\tb=07.0\tloss=07.0\n\nAs you can see it is pulling a down, then b, and then again. It will require less steps if it is going to change both a and b together.\nOptimizer playing whack-a-feature game\nimport numpy as np\nimport tensorflow as tf\n\na = tf.Variable(10.0)\nb = 10.0 - a\nloss = tf.maximum(a, b)\n\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\nsess = tf.InteractiveSession()\n\ntf.global_variables_initializer().run()\n\n\ndef print_state(sess, step, a, b, loss):\n    ret = sess.run({\n        'a': a,\n        'b': b,\n        'loss': loss\n    }, feed_dict={})\n    \n    print (\"Step: %3s:\\ta=%04.01f\\tb=%04.01f\\tloss=%04.01f\" % (\n        step,\n        ret['a'],\n        ret['b'],\n        ret['loss']\n    ))\n\n\nprint_state(sess, -1, a, b, loss)\n\nfor step in range(20):\n    sess.run(train_step, feed_dict={})\n\n    print_state(sess, step, a, b, loss)\n\nStep:  -1:\ta=10.0\tb=00.0\tloss=10.0\nStep:   0:\ta=09.5\tb=00.5\tloss=09.5\nStep:   1:\ta=09.0\tb=01.0\tloss=09.0\nStep:   2:\ta=08.5\tb=01.5\tloss=08.5\nStep:   3:\ta=08.0\tb=02.0\tloss=08.0\nStep:   4:\ta=07.5\tb=02.5\tloss=07.5\nStep:   5:\ta=07.0\tb=03.0\tloss=07.0\nStep:   6:\ta=06.5\tb=03.5\tloss=06.5\nStep:   7:\ta=06.0\tb=04.0\tloss=06.0\nStep:   8:\ta=05.5\tb=04.5\tloss=05.5\nStep:   9:\ta=05.0\tb=05.0\tloss=05.0\nStep:  10:\ta=04.5\tb=05.5\tloss=05.5\nStep:  11:\ta=05.0\tb=05.0\tloss=05.0\nStep:  12:\ta=04.5\tb=05.5\tloss=05.5\nStep:  13:\ta=05.0\tb=05.0\tloss=05.0\nStep:  14:\ta=04.5\tb=05.5\tloss=05.5\nStep:  15:\ta=05.0\tb=05.0\tloss=05.0\nStep:  16:\ta=04.5\tb=05.5\tloss=05.5\nStep:  17:\ta=05.0\tb=05.0\tloss=05.0\nStep:  18:\ta=04.5\tb=05.5\tloss=05.5\nStep:  19:\ta=05.0\tb=05.0\tloss=05.0\n\nAs you can see, it is stuck in a cycle. If it considered loss for both a and b it would have quickly come to the equilibrium.", "body": "**Mandatory questions:**\r\nHave I written custom code: N/A? Yes? I have no idea.\r\nOS Platform and Distribution - Any N/A\r\nTensorFlow installed from - pypi\r\nTensorFlow version - '1.4.1'\r\nBazel version - N/A\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce.\r\n\r\n**Slow optimizer:**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(10.0)\r\nb = tf.Variable(9.0)\r\nloss = tf.maximum(a, b)\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\n\r\nsess = tf.InteractiveSession()\r\n\r\ntf.global_variables_initializer().run()\r\n\r\n\r\ndef print_state(sess, step, a, b, loss):\r\n    ret = sess.run({\r\n        'a': a,\r\n        'b': b,\r\n        'loss': loss\r\n    }, feed_dict={})\r\n    \r\n    print (\"Step: %3s:\\ta=%04.01f\\tb=%04.01f\\tloss=%04.01f\" % (\r\n        step,\r\n        ret['a'],\r\n        ret['b'],\r\n        ret['loss']\r\n    ))\r\n\r\n\r\nprint_state(sess, -1, a, b, loss)\r\n\r\nfor step in range(10):\r\n    sess.run(train_step, feed_dict={})\r\n\r\n    print_state(sess, step, a, b, loss)\r\n\r\n```\r\nOutputs:\r\n```\r\nStep:  -1:\ta=10.0\tb=09.0\tloss=10.0\r\nStep:   0:\ta=09.5\tb=09.0\tloss=09.5\r\nStep:   1:\ta=09.0\tb=09.0\tloss=09.0\r\nStep:   2:\ta=08.5\tb=09.0\tloss=09.0\r\nStep:   3:\ta=08.5\tb=08.5\tloss=08.5\r\nStep:   4:\ta=08.0\tb=08.5\tloss=08.5\r\nStep:   5:\ta=08.0\tb=08.0\tloss=08.0\r\nStep:   6:\ta=07.5\tb=08.0\tloss=08.0\r\nStep:   7:\ta=07.5\tb=07.5\tloss=07.5\r\nStep:   8:\ta=07.0\tb=07.5\tloss=07.5\r\nStep:   9:\ta=07.0\tb=07.0\tloss=07.0\r\n```\r\n\r\nAs you can see it is pulling `a` down, then `b`, and then again. It will require less steps if it is going to change both `a` and `b` together.\r\n\r\n**Optimizer playing whack-a-feature game**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(10.0)\r\nb = 10.0 - a\r\nloss = tf.maximum(a, b)\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\n\r\nsess = tf.InteractiveSession()\r\n\r\ntf.global_variables_initializer().run()\r\n\r\n\r\ndef print_state(sess, step, a, b, loss):\r\n    ret = sess.run({\r\n        'a': a,\r\n        'b': b,\r\n        'loss': loss\r\n    }, feed_dict={})\r\n    \r\n    print (\"Step: %3s:\\ta=%04.01f\\tb=%04.01f\\tloss=%04.01f\" % (\r\n        step,\r\n        ret['a'],\r\n        ret['b'],\r\n        ret['loss']\r\n    ))\r\n\r\n\r\nprint_state(sess, -1, a, b, loss)\r\n\r\nfor step in range(20):\r\n    sess.run(train_step, feed_dict={})\r\n\r\n    print_state(sess, step, a, b, loss)\r\n```\r\n\r\n```\r\nStep:  -1:\ta=10.0\tb=00.0\tloss=10.0\r\nStep:   0:\ta=09.5\tb=00.5\tloss=09.5\r\nStep:   1:\ta=09.0\tb=01.0\tloss=09.0\r\nStep:   2:\ta=08.5\tb=01.5\tloss=08.5\r\nStep:   3:\ta=08.0\tb=02.0\tloss=08.0\r\nStep:   4:\ta=07.5\tb=02.5\tloss=07.5\r\nStep:   5:\ta=07.0\tb=03.0\tloss=07.0\r\nStep:   6:\ta=06.5\tb=03.5\tloss=06.5\r\nStep:   7:\ta=06.0\tb=04.0\tloss=06.0\r\nStep:   8:\ta=05.5\tb=04.5\tloss=05.5\r\nStep:   9:\ta=05.0\tb=05.0\tloss=05.0\r\nStep:  10:\ta=04.5\tb=05.5\tloss=05.5\r\nStep:  11:\ta=05.0\tb=05.0\tloss=05.0\r\nStep:  12:\ta=04.5\tb=05.5\tloss=05.5\r\nStep:  13:\ta=05.0\tb=05.0\tloss=05.0\r\nStep:  14:\ta=04.5\tb=05.5\tloss=05.5\r\nStep:  15:\ta=05.0\tb=05.0\tloss=05.0\r\nStep:  16:\ta=04.5\tb=05.5\tloss=05.5\r\nStep:  17:\ta=05.0\tb=05.0\tloss=05.0\r\nStep:  18:\ta=04.5\tb=05.5\tloss=05.5\r\nStep:  19:\ta=05.0\tb=05.0\tloss=05.0\r\n```\r\n\r\nAs you can see, it is stuck in a cycle. If it considered loss for both `a` and `b` it would have quickly come to the equilibrium.\r\n\r\n\r\n\r\n\r\n"}