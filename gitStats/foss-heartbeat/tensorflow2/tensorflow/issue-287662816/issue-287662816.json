{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16028", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16028/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16028/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16028/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16028", "id": 287662816, "node_id": "MDU6SXNzdWUyODc2NjI4MTY=", "number": 16028, "title": "Optimzer: Better handling of gradients for min/max ops.", "user": {"login": "olegserov", "id": 129835, "node_id": "MDQ6VXNlcjEyOTgzNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/129835?v=4", "gravatar_id": "", "url": "https://api.github.com/users/olegserov", "html_url": "https://github.com/olegserov", "followers_url": "https://api.github.com/users/olegserov/followers", "following_url": "https://api.github.com/users/olegserov/following{/other_user}", "gists_url": "https://api.github.com/users/olegserov/gists{/gist_id}", "starred_url": "https://api.github.com/users/olegserov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/olegserov/subscriptions", "organizations_url": "https://api.github.com/users/olegserov/orgs", "repos_url": "https://api.github.com/users/olegserov/repos", "events_url": "https://api.github.com/users/olegserov/events{/privacy}", "received_events_url": "https://api.github.com/users/olegserov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2018-01-11T04:47:49Z", "updated_at": "2018-11-23T08:38:26Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Please don't kick me too hard for this. If this ticket should not be here, please direct me to the proper place. It's not a help or support request, just a very naive idea/request/improvement.</p>\n<p>Image a model has this op in the graph: <code>y = tf.maximum(a, b)</code>. If <code>y</code> is contributing error to the loss (higher <code>y</code> higher is loss), then you are interested in minimizing both <code>a</code> and <code>b</code>. Otherwise, your optimizer will play whack-a-mole game forever. Especially, if you have something like this in your model: <code>b = 1 - a</code> and <code>y = tf.maximum(a, b)</code></p>\n<p>I've researching how does optimizer works and looks like each op has a function that defines how to calculate its gradients.<br>\nIn the current implementation for maximum/minimum (python's version) is located here:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L901\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L901</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L883\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L883</a></p>\n<p>In the current implementation, it discards gradients for <code>a</code>, if <code>b</code> is bigger than <code>a</code> and other way around.</p>\n<p>I think, that there is some place for improvement of training speed simply by considering gradients for all variables in min/max operations (including reduce_min/reduce_max and all max/min pooling). There are a lot of models with max pooling. It makes sense that they will train faster if they stop playing whack-a-feature each time it has max pooling layer.</p>\n<p>If it make sense, I would like to do a small PoC.</p>", "body_text": "Please don't kick me too hard for this. If this ticket should not be here, please direct me to the proper place. It's not a help or support request, just a very naive idea/request/improvement.\nImage a model has this op in the graph: y = tf.maximum(a, b). If y is contributing error to the loss (higher y higher is loss), then you are interested in minimizing both a and b. Otherwise, your optimizer will play whack-a-mole game forever. Especially, if you have something like this in your model: b = 1 - a and y = tf.maximum(a, b)\nI've researching how does optimizer works and looks like each op has a function that defines how to calculate its gradients.\nIn the current implementation for maximum/minimum (python's version) is located here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L901\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L883\nIn the current implementation, it discards gradients for a, if b is bigger than a and other way around.\nI think, that there is some place for improvement of training speed simply by considering gradients for all variables in min/max operations (including reduce_min/reduce_max and all max/min pooling). There are a lot of models with max pooling. It makes sense that they will train faster if they stop playing whack-a-feature each time it has max pooling layer.\nIf it make sense, I would like to do a small PoC.", "body": "Please don't kick me too hard for this. If this ticket should not be here, please direct me to the proper place. It's not a help or support request, just a very naive idea/request/improvement.\r\n\r\nImage a model has this op in the graph: `y = tf.maximum(a, b)`. If `y` is contributing error to the loss (higher `y` higher is loss), then you are interested in minimizing both `a` and `b`. Otherwise, your optimizer will play whack-a-mole game forever. Especially, if you have something like this in your model: `b = 1 - a` and `y = tf.maximum(a, b)`\r\n\r\nI've researching how does optimizer works and looks like each op has a function that defines how to calculate its gradients. \r\nIn the current implementation for maximum/minimum (python's version) is located here: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L901\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L883\r\n\r\nIn the current implementation, it discards gradients for `a`, if `b` is bigger than `a` and other way around. \r\n\r\nI think, that there is some place for improvement of training speed simply by considering gradients for all variables in min/max operations (including reduce_min/reduce_max and all max/min pooling). There are a lot of models with max pooling. It makes sense that they will train faster if they stop playing whack-a-feature each time it has max pooling layer.\r\n\r\nIf it make sense, I would like to do a small PoC.\r\n"}