{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361797598", "html_url": "https://github.com/tensorflow/tensorflow/issues/16028#issuecomment-361797598", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16028", "id": 361797598, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTc5NzU5OA==", "user": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T01:53:32Z", "updated_at": "2018-01-31T01:53:32Z", "author_association": "MEMBER", "body_html": "<p>So the canonical way to tackle this problem is probably to \"soften\" the non-linear function, e.g use something like:</p>\n<pre><code>a_is_bigger = tf.sigmoid((a-b) / width)\ny = a_is_bigger * a + (1 - a_is_bigger) * b\n</code></pre>\n<p>and increase <code>width</code> over time.</p>\n<p>The second canonical way is to use an optimizer that implements the proximal <a href=\"http://www.stat.cmu.edu/~ryantibs/convexopt-S15/lectures/08-prox-grad.pdf\" rel=\"nofollow\">gradients</a>.</p>\n<p>If you have a practical solution, we're always open to good algorithms! In general if it's been tested on a competitive benchmark and it works, it would be great to contribute.</p>", "body_text": "So the canonical way to tackle this problem is probably to \"soften\" the non-linear function, e.g use something like:\na_is_bigger = tf.sigmoid((a-b) / width)\ny = a_is_bigger * a + (1 - a_is_bigger) * b\n\nand increase width over time.\nThe second canonical way is to use an optimizer that implements the proximal gradients.\nIf you have a practical solution, we're always open to good algorithms! In general if it's been tested on a competitive benchmark and it works, it would be great to contribute.", "body": "So the canonical way to tackle this problem is probably to \"soften\" the non-linear function, e.g use something like:\r\n```\r\na_is_bigger = tf.sigmoid((a-b) / width)\r\ny = a_is_bigger * a + (1 - a_is_bigger) * b\r\n```\r\nand increase `width` over time.\r\n\r\nThe second canonical way is to use an optimizer that implements the proximal [gradients](http://www.stat.cmu.edu/~ryantibs/convexopt-S15/lectures/08-prox-grad.pdf).\r\n\r\nIf you have a practical solution, we're always open to good algorithms! In general if it's been tested on a competitive benchmark and it works, it would be great to contribute."}