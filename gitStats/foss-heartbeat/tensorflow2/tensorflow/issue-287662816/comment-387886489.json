{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/387886489", "html_url": "https://github.com/tensorflow/tensorflow/issues/16028#issuecomment-387886489", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16028", "id": 387886489, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Nzg4NjQ4OQ==", "user": {"login": "julien2512", "id": 838473, "node_id": "MDQ6VXNlcjgzODQ3Mw==", "avatar_url": "https://avatars0.githubusercontent.com/u/838473?v=4", "gravatar_id": "", "url": "https://api.github.com/users/julien2512", "html_url": "https://github.com/julien2512", "followers_url": "https://api.github.com/users/julien2512/followers", "following_url": "https://api.github.com/users/julien2512/following{/other_user}", "gists_url": "https://api.github.com/users/julien2512/gists{/gist_id}", "starred_url": "https://api.github.com/users/julien2512/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/julien2512/subscriptions", "organizations_url": "https://api.github.com/users/julien2512/orgs", "repos_url": "https://api.github.com/users/julien2512/repos", "events_url": "https://api.github.com/users/julien2512/events{/privacy}", "received_events_url": "https://api.github.com/users/julien2512/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T21:50:38Z", "updated_at": "2018-05-09T21:50:38Z", "author_association": "NONE", "body_html": "<p>The code for current max (and min !) gradient :</p>\n<pre><code>def _MinOrMaxGrad(op, grad):\n  \"\"\"Gradient for Min or Max. Amazingly it's precisely the same code.\"\"\"\n  input_shape = array_ops.shape(op.inputs[0])\n  output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\n  y = op.outputs[0]\n  y = array_ops.reshape(y, output_shape_kept_dims)\n  grad = array_ops.reshape(grad, output_shape_kept_dims)\n\n  # Compute the number of selected (maximum or minimum) elements in each\n  # reduction dimension. If there are multiple minimum or maximum elements\n  # then the gradient will be divided between them.\n  indicators = math_ops.cast(math_ops.equal(y, op.inputs[0]), grad.dtype)\n  num_selected = array_ops.reshape(\n      math_ops.reduce_sum(indicators, op.inputs[1]), output_shape_kept_dims)\n\nreturn [math_ops.div(indicators, num_selected) * grad, None]\n</code></pre>", "body_text": "The code for current max (and min !) gradient :\ndef _MinOrMaxGrad(op, grad):\n  \"\"\"Gradient for Min or Max. Amazingly it's precisely the same code.\"\"\"\n  input_shape = array_ops.shape(op.inputs[0])\n  output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\n  y = op.outputs[0]\n  y = array_ops.reshape(y, output_shape_kept_dims)\n  grad = array_ops.reshape(grad, output_shape_kept_dims)\n\n  # Compute the number of selected (maximum or minimum) elements in each\n  # reduction dimension. If there are multiple minimum or maximum elements\n  # then the gradient will be divided between them.\n  indicators = math_ops.cast(math_ops.equal(y, op.inputs[0]), grad.dtype)\n  num_selected = array_ops.reshape(\n      math_ops.reduce_sum(indicators, op.inputs[1]), output_shape_kept_dims)\n\nreturn [math_ops.div(indicators, num_selected) * grad, None]", "body": "The code for current max (and min !) gradient : \r\n```\r\ndef _MinOrMaxGrad(op, grad):\r\n  \"\"\"Gradient for Min or Max. Amazingly it's precisely the same code.\"\"\"\r\n  input_shape = array_ops.shape(op.inputs[0])\r\n  output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\r\n  y = op.outputs[0]\r\n  y = array_ops.reshape(y, output_shape_kept_dims)\r\n  grad = array_ops.reshape(grad, output_shape_kept_dims)\r\n\r\n  # Compute the number of selected (maximum or minimum) elements in each\r\n  # reduction dimension. If there are multiple minimum or maximum elements\r\n  # then the gradient will be divided between them.\r\n  indicators = math_ops.cast(math_ops.equal(y, op.inputs[0]), grad.dtype)\r\n  num_selected = array_ops.reshape(\r\n      math_ops.reduce_sum(indicators, op.inputs[1]), output_shape_kept_dims)\r\n\r\nreturn [math_ops.div(indicators, num_selected) * grad, None]\r\n```"}