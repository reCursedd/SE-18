{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1418", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1418/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1418/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1418/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1418", "id": 138937916, "node_id": "MDU6SXNzdWUxMzg5Mzc5MTY=", "number": 1418, "title": "Using 2 computers - Distributed Tensorflow", "user": {"login": "Fhrozen", "id": 11988996, "node_id": "MDQ6VXNlcjExOTg4OTk2", "avatar_url": "https://avatars3.githubusercontent.com/u/11988996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Fhrozen", "html_url": "https://github.com/Fhrozen", "followers_url": "https://api.github.com/users/Fhrozen/followers", "following_url": "https://api.github.com/users/Fhrozen/following{/other_user}", "gists_url": "https://api.github.com/users/Fhrozen/gists{/gist_id}", "starred_url": "https://api.github.com/users/Fhrozen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Fhrozen/subscriptions", "organizations_url": "https://api.github.com/users/Fhrozen/orgs", "repos_url": "https://api.github.com/users/Fhrozen/repos", "events_url": "https://api.github.com/users/Fhrozen/events{/privacy}", "received_events_url": "https://api.github.com/users/Fhrozen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-03-07T10:03:37Z", "updated_at": "2016-08-18T20:36:07Z", "closed_at": "2016-03-08T04:35:27Z", "author_association": "NONE", "body_html": "<p>Copied from: <a href=\"http://stackoverflow.com/questions/35824742/tensorflow-distributed-assing-devices\" rel=\"nofollow\">http://stackoverflow.com/questions/35824742/tensorflow-distributed-assing-devices</a></p>\n<p>I recently installed the version of tensorflow for distributed computers. From the trend: <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime\">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime</a> , i tried to implemented multiples gpus at multiples computers, and also looking at \"<a href=\"https://stackoverflow.com/questions/34439045/tensorflow-setup-for-distributed-computing\" rel=\"nofollow\">https://stackoverflow.com/questions/34439045/tensorflow-setup-for-distributed-computing</a>\" found a white paper for some additional specifications. I can run the server and a worker on 2 different computers with 2 and 1 gpu, and using the session gprc, allocate and run the program on remote or local mode. i ran locally tensorflow in the remote computer with:</p>\n<pre><code>bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='local|localhost:2500' --job_name=local --task_id=0 &amp;\n</code></pre>\n<p>and for using on the server</p>\n<pre><code>bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='worker|192.168.170.193:2500,prs|192.168.170.226:2500' --job_name=worker --task_id=0 \\\n--job_name=prs --task_id=0 &amp;\n</code></pre>\n<p>However, when i try to specify the device for running on 2 computers at the same time the python show me the error:</p>\n<p><code>Could not satisfy explicit device specification '/job:worker/task:0'</code></p>\n<p>when i use</p>\n<pre><code>with tf.device(\"/job:prs/task:0/device:gpu:0\"):\n  x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n  W = tf.Variable(tf.zeros([784, 10]), name='weights')\nwith tf.device(\"/job:prs/task:0/device:gpu:1\"):\n  b = tf.Variable(tf.zeros([10], name='bias'))\n# Use a name scope to organize nodes in the graph visualizer\nwith tf.device(\"/job:worker/task:0/device:gpu:0\"):\n  with tf.name_scope('Wx_b'):\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\n</code></pre>\n<p>or even changing the name of job. So I am wondering if it is required to Add a New Device (<a href=\"https://stackoverflow.com/questions/35213172/add-a-new-device-in-tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/35213172/add-a-new-device-in-tensorflow</a>) or probably i am doing something wrong with the initialization of the cluster.</p>\n<p>Best Regards</p>", "body_text": "Copied from: http://stackoverflow.com/questions/35824742/tensorflow-distributed-assing-devices\nI recently installed the version of tensorflow for distributed computers. From the trend: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime , i tried to implemented multiples gpus at multiples computers, and also looking at \"https://stackoverflow.com/questions/34439045/tensorflow-setup-for-distributed-computing\" found a white paper for some additional specifications. I can run the server and a worker on 2 different computers with 2 and 1 gpu, and using the session gprc, allocate and run the program on remote or local mode. i ran locally tensorflow in the remote computer with:\nbazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='local|localhost:2500' --job_name=local --task_id=0 &\n\nand for using on the server\nbazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='worker|192.168.170.193:2500,prs|192.168.170.226:2500' --job_name=worker --task_id=0 \\\n--job_name=prs --task_id=0 &\n\nHowever, when i try to specify the device for running on 2 computers at the same time the python show me the error:\nCould not satisfy explicit device specification '/job:worker/task:0'\nwhen i use\nwith tf.device(\"/job:prs/task:0/device:gpu:0\"):\n  x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n  W = tf.Variable(tf.zeros([784, 10]), name='weights')\nwith tf.device(\"/job:prs/task:0/device:gpu:1\"):\n  b = tf.Variable(tf.zeros([10], name='bias'))\n# Use a name scope to organize nodes in the graph visualizer\nwith tf.device(\"/job:worker/task:0/device:gpu:0\"):\n  with tf.name_scope('Wx_b'):\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\n\nor even changing the name of job. So I am wondering if it is required to Add a New Device (https://stackoverflow.com/questions/35213172/add-a-new-device-in-tensorflow) or probably i am doing something wrong with the initialization of the cluster.\nBest Regards", "body": "Copied from: http://stackoverflow.com/questions/35824742/tensorflow-distributed-assing-devices\n\nI recently installed the version of tensorflow for distributed computers. From the trend: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime , i tried to implemented multiples gpus at multiples computers, and also looking at \"https://stackoverflow.com/questions/34439045/tensorflow-setup-for-distributed-computing\" found a white paper for some additional specifications. I can run the server and a worker on 2 different computers with 2 and 1 gpu, and using the session gprc, allocate and run the program on remote or local mode. i ran locally tensorflow in the remote computer with:\n\n```\nbazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='local|localhost:2500' --job_name=local --task_id=0 &\n```\n\nand for using on the server\n\n```\nbazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='worker|192.168.170.193:2500,prs|192.168.170.226:2500' --job_name=worker --task_id=0 \\\n--job_name=prs --task_id=0 &\n```\n\nHowever, when i try to specify the device for running on 2 computers at the same time the python show me the error:\n\n`Could not satisfy explicit device specification '/job:worker/task:0'`\n\nwhen i use\n\n```\nwith tf.device(\"/job:prs/task:0/device:gpu:0\"):\n  x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n  W = tf.Variable(tf.zeros([784, 10]), name='weights')\nwith tf.device(\"/job:prs/task:0/device:gpu:1\"):\n  b = tf.Variable(tf.zeros([10], name='bias'))\n# Use a name scope to organize nodes in the graph visualizer\nwith tf.device(\"/job:worker/task:0/device:gpu:0\"):\n  with tf.name_scope('Wx_b'):\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\n```\n\nor even changing the name of job. So I am wondering if it is required to Add a New Device (https://stackoverflow.com/questions/35213172/add-a-new-device-in-tensorflow) or probably i am doing something wrong with the initialization of the cluster.\n\nBest Regards\n"}