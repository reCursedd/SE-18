{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435072079", "html_url": "https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435072079", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15323", "id": 435072079, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTA3MjA3OQ==", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-01T15:11:08Z", "updated_at": "2018-11-01T15:11:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> FYI, I revisited this for my Scala API after all this time, because it felt quite awkward to have to do this and I realized that there is a very simple design change that resolves the issue and allows tiling to be handled within the beam search decoder. All that is needed is for the memory and the memory sequence lengths tensors to be part of the attention wrapper RNN cell, rather than provided at construction time. It's a simple change but has resolved the issue for me and cannot see why it would not resolve it for the Python implementation too.</p>", "body_text": "@ebrevdo FYI, I revisited this for my Scala API after all this time, because it felt quite awkward to have to do this and I realized that there is a very simple design change that resolves the issue and allows tiling to be handled within the beam search decoder. All that is needed is for the memory and the memory sequence lengths tensors to be part of the attention wrapper RNN cell, rather than provided at construction time. It's a simple change but has resolved the issue for me and cannot see why it would not resolve it for the Python implementation too.", "body": "@ebrevdo FYI, I revisited this for my Scala API after all this time, because it felt quite awkward to have to do this and I realized that there is a very simple design change that resolves the issue and allows tiling to be handled within the beam search decoder. All that is needed is for the memory and the memory sequence lengths tensors to be part of the attention wrapper RNN cell, rather than provided at construction time. It's a simple change but has resolved the issue for me and cannot see why it would not resolve it for the Python implementation too."}