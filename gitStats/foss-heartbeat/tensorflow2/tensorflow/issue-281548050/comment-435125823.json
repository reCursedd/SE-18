{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435125823", "html_url": "https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435125823", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15323", "id": 435125823, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTEyNTgyMw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-01T17:49:04Z", "updated_at": "2018-11-01T17:49:04Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Thanks for this suggestion!  I will see if we have some cycles to try this\nout.  We may try to make this change in the transition to TF 2.0; where we\ncan make backwards breaking changes in the API.\n\n+scott, james</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Thu, Nov 1, 2018 at 9:33 AM, Anthony Platanios ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; The actual commit is this\n &lt;<a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b\" class=\"commit-link\">eaplatanios/tensorflow_scala@<tt>8efc106</tt></a>&gt;,\n however, it might be hard to follow if you are not familiar with Scala. In\n summary, the changes were the following:\n\n    - When constructing attention mechanisms, no memory needs to be\n    provided, as shown in this diff\n    &lt;<a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-ef76330f5dbe0574036286de5f6430f8\" class=\"commit-link\">eaplatanios/tensorflow_scala@<tt>8efc106</tt>#diff-ef76330f5dbe0574036286de5f6430f8</a>&gt;.\n    In order to construct the initial attention state, a batch size needs to be\n    provided and in order to compute an alignment, some previous attention\n    state needs to be provided, that contains the keys, values, previous\n    attention state, and optionally sequence lengths. It is the job of the\n    attention wrapper cell to provide those, as described next.\n    - When constructed, the attention wrapper cell takes as input pairs of\n    memories, and attention mechanisms (instead of just attention mechanisms),\n    as shown in this diff\n    &lt;<a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412e\" class=\"commit-link\">eaplatanios/tensorflow_scala@<tt>8efc106</tt>#diff-e5b5a14ee310ebfe2ede73ae49f8412e</a>&gt;.\n    These memories are used to initialize the attention states, as shown\n    here\n    &lt;<a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115\" class=\"commit-link\">eaplatanios/tensorflow_scala@<tt>8efc106</tt>#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115</a>&gt;,\n    which are part of the RNN cell state and can thus be tiled automatically by\n    the beam search decoder.\n\n Note that for the tiling I do recursion on the type structures of the cell\n state, at compile-time, but something similar could be done in Python using\n the TF nest module.\n\n Does this help? Please let me know and if needed I could provide more\n detail. :)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"281548050\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/15323\" href=\"https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435095669\">#15323 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimzrJUXDEwnmvZ-TVqmCJ3hkkYif_ks5uqyJVgaJpZM4Q_reC\">https://github.com/notifications/unsubscribe-auth/ABtimzrJUXDEwnmvZ-TVqmCJ3hkkYif_ks5uqyJVgaJpZM4Q_reC</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Thanks for this suggestion!  I will see if we have some cycles to try this\nout.  We may try to make this change in the transition to TF 2.0; where we\ncan make backwards breaking changes in the API.\n\n+scott, james\n\u2026\nOn Thu, Nov 1, 2018 at 9:33 AM, Anthony Platanios ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo> The actual commit is this\n <eaplatanios/tensorflow_scala@8efc106>,\n however, it might be hard to follow if you are not familiar with Scala. In\n summary, the changes were the following:\n\n    - When constructing attention mechanisms, no memory needs to be\n    provided, as shown in this diff\n    <eaplatanios/tensorflow_scala@8efc106#diff-ef76330f5dbe0574036286de5f6430f8>.\n    In order to construct the initial attention state, a batch size needs to be\n    provided and in order to compute an alignment, some previous attention\n    state needs to be provided, that contains the keys, values, previous\n    attention state, and optionally sequence lengths. It is the job of the\n    attention wrapper cell to provide those, as described next.\n    - When constructed, the attention wrapper cell takes as input pairs of\n    memories, and attention mechanisms (instead of just attention mechanisms),\n    as shown in this diff\n    <eaplatanios/tensorflow_scala@8efc106#diff-e5b5a14ee310ebfe2ede73ae49f8412e>.\n    These memories are used to initialize the attention states, as shown\n    here\n    <eaplatanios/tensorflow_scala@8efc106#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115>,\n    which are part of the RNN cell state and can thus be tiled automatically by\n    the beam search decoder.\n\n Note that for the tiling I do recursion on the type structures of the cell\n state, at compile-time, but something similar could be done in Python using\n the TF nest module.\n\n Does this help? Please let me know and if needed I could provide more\n detail. :)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#15323 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtimzrJUXDEwnmvZ-TVqmCJ3hkkYif_ks5uqyJVgaJpZM4Q_reC>\n .", "body": "Thanks for this suggestion!  I will see if we have some cycles to try this\nout.  We may try to make this change in the transition to TF 2.0; where we\ncan make backwards breaking changes in the API.\n\n+scott, james\n\nOn Thu, Nov 1, 2018 at 9:33 AM, Anthony Platanios <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> The actual commit is this\n> <https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b>,\n> however, it might be hard to follow if you are not familiar with Scala. In\n> summary, the changes were the following:\n>\n>    - When constructing attention mechanisms, no memory needs to be\n>    provided, as shown in this diff\n>    <https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-ef76330f5dbe0574036286de5f6430f8>.\n>    In order to construct the initial attention state, a batch size needs to be\n>    provided and in order to compute an alignment, some previous attention\n>    state needs to be provided, that contains the keys, values, previous\n>    attention state, and optionally sequence lengths. It is the job of the\n>    attention wrapper cell to provide those, as described next.\n>    - When constructed, the attention wrapper cell takes as input pairs of\n>    memories, and attention mechanisms (instead of just attention mechanisms),\n>    as shown in this diff\n>    <https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412e>.\n>    These memories are used to initialize the attention states, as shown\n>    here\n>    <https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115>,\n>    which are part of the RNN cell state and can thus be tiled automatically by\n>    the beam search decoder.\n>\n> Note that for the tiling I do recursion on the type structures of the cell\n> state, at compile-time, but something similar could be done in Python using\n> the TF nest module.\n>\n> Does this help? Please let me know and if needed I could provide more\n> detail. :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435095669>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzrJUXDEwnmvZ-TVqmCJ3hkkYif_ks5uqyJVgaJpZM4Q_reC>\n> .\n>\n"}