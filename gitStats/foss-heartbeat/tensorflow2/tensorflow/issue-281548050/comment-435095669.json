{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435095669", "html_url": "https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435095669", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15323", "id": 435095669, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTA5NTY2OQ==", "user": {"login": "eaplatanios", "id": 1294940, "node_id": "MDQ6VXNlcjEyOTQ5NDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1294940?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eaplatanios", "html_url": "https://github.com/eaplatanios", "followers_url": "https://api.github.com/users/eaplatanios/followers", "following_url": "https://api.github.com/users/eaplatanios/following{/other_user}", "gists_url": "https://api.github.com/users/eaplatanios/gists{/gist_id}", "starred_url": "https://api.github.com/users/eaplatanios/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eaplatanios/subscriptions", "organizations_url": "https://api.github.com/users/eaplatanios/orgs", "repos_url": "https://api.github.com/users/eaplatanios/repos", "events_url": "https://api.github.com/users/eaplatanios/events{/privacy}", "received_events_url": "https://api.github.com/users/eaplatanios/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-01T16:18:37Z", "updated_at": "2018-11-01T16:18:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> The actual commit is <a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b\">this</a>, however, it might be hard to follow if you are not familiar with Scala. In summary, the changes were the following:</p>\n<ul>\n<li>When constructing attention mechanisms, no memory needs to be provided, as shown in <a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-ef76330f5dbe0574036286de5f6430f8\">this diff</a>. In order to construct the initial attention state, a batch size needs to be provided and in order to compute an alignment, some previous attention state needs to be provided, that contains the keys, values, previous attention state, and optionally sequence lengths. It is the job of the attention wrapper cell to provide those, as described next.</li>\n<li>When constructed, the attention wrapper cell takes as input pairs of memories, and attention mechanisms (instead of just attention mechanisms), as shown in <a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412e\">this diff</a>. These memories are used to initialize the attention states, as shown <a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115\">here</a>, which are part of the RNN cell state and can thus be tiled automatically by the beam search decoder.</li>\n</ul>\n<p>Note that for the tiling I do recursion on the type structures of the cell state, at compile-time, but something similar could be done in Python using the TF nest module.</p>\n<p>Does this help? Please let me know and if needed I could provide more detail. :)</p>", "body_text": "@ebrevdo The actual commit is this, however, it might be hard to follow if you are not familiar with Scala. In summary, the changes were the following:\n\nWhen constructing attention mechanisms, no memory needs to be provided, as shown in this diff. In order to construct the initial attention state, a batch size needs to be provided and in order to compute an alignment, some previous attention state needs to be provided, that contains the keys, values, previous attention state, and optionally sequence lengths. It is the job of the attention wrapper cell to provide those, as described next.\nWhen constructed, the attention wrapper cell takes as input pairs of memories, and attention mechanisms (instead of just attention mechanisms), as shown in this diff. These memories are used to initialize the attention states, as shown here, which are part of the RNN cell state and can thus be tiled automatically by the beam search decoder.\n\nNote that for the tiling I do recursion on the type structures of the cell state, at compile-time, but something similar could be done in Python using the TF nest module.\nDoes this help? Please let me know and if needed I could provide more detail. :)", "body": "@ebrevdo The actual commit is [this](https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b), however, it might be hard to follow if you are not familiar with Scala. In summary, the changes were the following:\r\n\r\n- When constructing attention mechanisms, no memory needs to be provided, as shown in [this diff](https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-ef76330f5dbe0574036286de5f6430f8). In order to construct the initial attention state, a batch size needs to be provided and in order to compute an alignment, some previous attention state needs to be provided, that contains the keys, values, previous attention state, and optionally sequence lengths. It is the job of the attention wrapper cell to provide those, as described next.\r\n- When constructed, the attention wrapper cell takes as input pairs of memories, and attention mechanisms (instead of just attention mechanisms), as shown in [this diff](https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412e). These memories are used to initialize the attention states, as shown [here](https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115), which are part of the RNN cell state and can thus be tiled automatically by the beam search decoder.\r\n\r\nNote that for the tiling I do recursion on the type structures of the cell state, at compile-time, but something similar could be done in Python using the TF nest module.\r\n\r\nDoes this help? Please let me know and if needed I could provide more detail. :)"}