{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435125878", "html_url": "https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435125878", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15323", "id": 435125878, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTEyNTg3OA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-01T17:49:13Z", "updated_at": "2018-11-01T17:49:13Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">+thang</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Thu, Nov 1, 2018 at 10:48 AM, Eugene Brevdo ***@***.***&gt; wrote:\n Thanks for this suggestion!  I will see if we have some cycles to try this\n out.  We may try to make this change in the transition to TF 2.0; where we\n can make backwards breaking changes in the API.\n\n +scott, james\n\n On Thu, Nov 1, 2018 at 9:33 AM, Anthony Platanios &lt;\n ***@***.***&gt; wrote:\n\n&gt; <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; The actual commit is this\n&gt; &lt;<a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b\" class=\"commit-link\">eaplatanios/tensorflow_scala@<tt>8efc106</tt></a>&gt;,\n&gt; however, it might be hard to follow if you are not familiar with Scala. In\n&gt; summary, the changes were the following:\n&gt;\n&gt;    - When constructing attention mechanisms, no memory needs to be\n&gt;    provided, as shown in this diff\n&gt;    &lt;<a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-ef76330f5dbe0574036286de5f6430f8\" class=\"commit-link\">eaplatanios/tensorflow_scala@<tt>8efc106</tt>#diff-ef76330f5dbe0574036286de5f6430f8</a>&gt;.\n&gt;    In order to construct the initial attention state, a batch size needs to be\n&gt;    provided and in order to compute an alignment, some previous attention\n&gt;    state needs to be provided, that contains the keys, values, previous\n&gt;    attention state, and optionally sequence lengths. It is the job of the\n&gt;    attention wrapper cell to provide those, as described next.\n&gt;    - When constructed, the attention wrapper cell takes as input pairs\n&gt;    of memories, and attention mechanisms (instead of just attention\n&gt;    mechanisms), as shown in this diff\n&gt;    &lt;<a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412e\" class=\"commit-link\">eaplatanios/tensorflow_scala@<tt>8efc106</tt>#diff-e5b5a14ee310ebfe2ede73ae49f8412e</a>&gt;.\n&gt;    These memories are used to initialize the attention states, as shown\n&gt;    here\n&gt;    &lt;<a href=\"https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115\" class=\"commit-link\">eaplatanios/tensorflow_scala@<tt>8efc106</tt>#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115</a>&gt;,\n&gt;    which are part of the RNN cell state and can thus be tiled automatically by\n&gt;    the beam search decoder.\n&gt;\n&gt; Note that for the tiling I do recursion on the type structures of the\n&gt; cell state, at compile-time, but something similar could be done in Python\n&gt; using the TF nest module.\n&gt;\n&gt; Does this help? Please let me know and if needed I could provide more\n&gt; detail. :)\n&gt;\n&gt; \u2014\n&gt; You are receiving this because you were mentioned.\n&gt; Reply to this email directly, view it on GitHub\n&gt; &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"281548050\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/15323\" href=\"https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435095669\">#15323 (comment)</a>&gt;,\n&gt; or mute the thread\n&gt; &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimzrJUXDEwnmvZ-TVqmCJ3hkkYif_ks5uqyJVgaJpZM4Q_reC\">https://github.com/notifications/unsubscribe-auth/ABtimzrJUXDEwnmvZ-TVqmCJ3hkkYif_ks5uqyJVgaJpZM4Q_reC</a>&gt;\n&gt; .\n&gt;\n\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "+thang\n\u2026\nOn Thu, Nov 1, 2018 at 10:48 AM, Eugene Brevdo ***@***.***> wrote:\n Thanks for this suggestion!  I will see if we have some cycles to try this\n out.  We may try to make this change in the transition to TF 2.0; where we\n can make backwards breaking changes in the API.\n\n +scott, james\n\n On Thu, Nov 1, 2018 at 9:33 AM, Anthony Platanios <\n ***@***.***> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> The actual commit is this\n> <eaplatanios/tensorflow_scala@8efc106>,\n> however, it might be hard to follow if you are not familiar with Scala. In\n> summary, the changes were the following:\n>\n>    - When constructing attention mechanisms, no memory needs to be\n>    provided, as shown in this diff\n>    <eaplatanios/tensorflow_scala@8efc106#diff-ef76330f5dbe0574036286de5f6430f8>.\n>    In order to construct the initial attention state, a batch size needs to be\n>    provided and in order to compute an alignment, some previous attention\n>    state needs to be provided, that contains the keys, values, previous\n>    attention state, and optionally sequence lengths. It is the job of the\n>    attention wrapper cell to provide those, as described next.\n>    - When constructed, the attention wrapper cell takes as input pairs\n>    of memories, and attention mechanisms (instead of just attention\n>    mechanisms), as shown in this diff\n>    <eaplatanios/tensorflow_scala@8efc106#diff-e5b5a14ee310ebfe2ede73ae49f8412e>.\n>    These memories are used to initialize the attention states, as shown\n>    here\n>    <eaplatanios/tensorflow_scala@8efc106#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115>,\n>    which are part of the RNN cell state and can thus be tiled automatically by\n>    the beam search decoder.\n>\n> Note that for the tiling I do recursion on the type structures of the\n> cell state, at compile-time, but something similar could be done in Python\n> using the TF nest module.\n>\n> Does this help? Please let me know and if needed I could provide more\n> detail. :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <#15323 (comment)>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzrJUXDEwnmvZ-TVqmCJ3hkkYif_ks5uqyJVgaJpZM4Q_reC>\n> .\n>", "body": "+thang\n\nOn Thu, Nov 1, 2018 at 10:48 AM, Eugene Brevdo <ebrevdo@google.com> wrote:\n\n> Thanks for this suggestion!  I will see if we have some cycles to try this\n> out.  We may try to make this change in the transition to TF 2.0; where we\n> can make backwards breaking changes in the API.\n>\n> +scott, james\n>\n> On Thu, Nov 1, 2018 at 9:33 AM, Anthony Platanios <\n> notifications@github.com> wrote:\n>\n>> @ebrevdo <https://github.com/ebrevdo> The actual commit is this\n>> <https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b>,\n>> however, it might be hard to follow if you are not familiar with Scala. In\n>> summary, the changes were the following:\n>>\n>>    - When constructing attention mechanisms, no memory needs to be\n>>    provided, as shown in this diff\n>>    <https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-ef76330f5dbe0574036286de5f6430f8>.\n>>    In order to construct the initial attention state, a batch size needs to be\n>>    provided and in order to compute an alignment, some previous attention\n>>    state needs to be provided, that contains the keys, values, previous\n>>    attention state, and optionally sequence lengths. It is the job of the\n>>    attention wrapper cell to provide those, as described next.\n>>    - When constructed, the attention wrapper cell takes as input pairs\n>>    of memories, and attention mechanisms (instead of just attention\n>>    mechanisms), as shown in this diff\n>>    <https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412e>.\n>>    These memories are used to initialize the attention states, as shown\n>>    here\n>>    <https://github.com/eaplatanios/tensorflow_scala/commit/8efc1069d0a29ae92faf9a5eb3864c92ffbacc3b#diff-e5b5a14ee310ebfe2ede73ae49f8412eR115>,\n>>    which are part of the RNN cell state and can thus be tiled automatically by\n>>    the beam search decoder.\n>>\n>> Note that for the tiling I do recursion on the type structures of the\n>> cell state, at compile-time, but something similar could be done in Python\n>> using the TF nest module.\n>>\n>> Does this help? Please let me know and if needed I could provide more\n>> detail. :)\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435095669>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtimzrJUXDEwnmvZ-TVqmCJ3hkkYif_ks5uqyJVgaJpZM4Q_reC>\n>> .\n>>\n>\n>\n"}