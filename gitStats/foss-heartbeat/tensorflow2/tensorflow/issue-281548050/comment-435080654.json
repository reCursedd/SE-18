{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435080654", "html_url": "https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435080654", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15323", "id": 435080654, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTA4MDY1NA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-01T15:35:30Z", "updated_at": "2018-11-01T15:35:30Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Thanks for the suggestion Anthony, do you have an example diff in your\ncodebase showing the change to the attention wrapper rnn cell?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Thu, Nov 1, 2018 at 8:28 AM, Anthony Platanios ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; FYI, I revisited this for my Scala\n API after all this time, because it felt quite awkward to have to do this\n and I realized that there is a very simple design change that resolves the\n issue and allows tiling to be handled within the beam search decoder. All\n that is needed is for the memory and the memory sequence lengths tensors to\n be part of the attention wrapper RNN cell, rather than provided at\n construction time. It's a simple change but has resolved the issue for me\n and cannot see why it would not resolve it for the Python implementation\n too.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"281548050\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/15323\" href=\"https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435072079\">#15323 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimz_yj-WLXDJzHodEVzLsUawvqgmXks5uqxMfgaJpZM4Q_reC\">https://github.com/notifications/unsubscribe-auth/ABtimz_yj-WLXDJzHodEVzLsUawvqgmXks5uqxMfgaJpZM4Q_reC</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Thanks for the suggestion Anthony, do you have an example diff in your\ncodebase showing the change to the attention wrapper rnn cell?\n\u2026\nOn Thu, Nov 1, 2018 at 8:28 AM, Anthony Platanios ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo> FYI, I revisited this for my Scala\n API after all this time, because it felt quite awkward to have to do this\n and I realized that there is a very simple design change that resolves the\n issue and allows tiling to be handled within the beam search decoder. All\n that is needed is for the memory and the memory sequence lengths tensors to\n be part of the attention wrapper RNN cell, rather than provided at\n construction time. It's a simple change but has resolved the issue for me\n and cannot see why it would not resolve it for the Python implementation\n too.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#15323 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtimz_yj-WLXDJzHodEVzLsUawvqgmXks5uqxMfgaJpZM4Q_reC>\n .", "body": "Thanks for the suggestion Anthony, do you have an example diff in your\ncodebase showing the change to the attention wrapper rnn cell?\n\nOn Thu, Nov 1, 2018 at 8:28 AM, Anthony Platanios <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> FYI, I revisited this for my Scala\n> API after all this time, because it felt quite awkward to have to do this\n> and I realized that there is a very simple design change that resolves the\n> issue and allows tiling to be handled within the beam search decoder. All\n> that is needed is for the memory and the memory sequence lengths tensors to\n> be part of the attention wrapper RNN cell, rather than provided at\n> construction time. It's a simple change but has resolved the issue for me\n> and cannot see why it would not resolve it for the Python implementation\n> too.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15323#issuecomment-435072079>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimz_yj-WLXDJzHodEVzLsUawvqgmXks5uqxMfgaJpZM4Q_reC>\n> .\n>\n"}