{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20391", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20391/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20391/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20391/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20391", "id": 336782301, "node_id": "MDU6SXNzdWUzMzY3ODIzMDE=", "number": 20391, "title": "Feature Request: tf.data.Dataset.truncated_batch()", "user": {"login": "rongou", "id": 497101, "node_id": "MDQ6VXNlcjQ5NzEwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/497101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rongou", "html_url": "https://github.com/rongou", "followers_url": "https://api.github.com/users/rongou/followers", "following_url": "https://api.github.com/users/rongou/following{/other_user}", "gists_url": "https://api.github.com/users/rongou/gists{/gist_id}", "starred_url": "https://api.github.com/users/rongou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rongou/subscriptions", "organizations_url": "https://api.github.com/users/rongou/orgs", "repos_url": "https://api.github.com/users/rongou/repos", "events_url": "https://api.github.com/users/rongou/events{/privacy}", "received_events_url": "https://api.github.com/users/rongou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-06-28T21:13:44Z", "updated_at": "2018-07-06T20:48:28Z", "closed_at": "2018-07-06T20:48:28Z", "author_association": "NONE", "body_html": "<p>The <code>Dataset</code> api currently provides a <code>padded_batch</code> transformation, but for dense sequential data (audio, video) it might be better to drop a few frames to make the batched examples even. Can we have a new function to <code>Dataset</code> like this?</p>\n<div class=\"highlight highlight-source-python\"><pre>truncated_batch(\n    batch_size,\n    truncating_shapes\n)</pre></div>\n<p>The semantic would be very similar to <code>padded_batch</code> but in reverse. The <code>truncating_shapes</code> argument determines the resulting shape for each dimension of each component in an output element:</p>\n<ul>\n<li>If the dimension is a constant (e.g. <code>tf.Dimension(37)</code>), the component will be truncated to that length in that dimension.</li>\n<li>If the dimension is unknown (e.g. <code>tf.Dimension(None)</code>), the component will be truncated to the minimum length of all elements in that dimension.</li>\n</ul>\n<p>Thoughts? <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a></p>", "body_text": "The Dataset api currently provides a padded_batch transformation, but for dense sequential data (audio, video) it might be better to drop a few frames to make the batched examples even. Can we have a new function to Dataset like this?\ntruncated_batch(\n    batch_size,\n    truncating_shapes\n)\nThe semantic would be very similar to padded_batch but in reverse. The truncating_shapes argument determines the resulting shape for each dimension of each component in an output element:\n\nIf the dimension is a constant (e.g. tf.Dimension(37)), the component will be truncated to that length in that dimension.\nIf the dimension is unknown (e.g. tf.Dimension(None)), the component will be truncated to the minimum length of all elements in that dimension.\n\nThoughts? @mrry", "body": "The `Dataset` api currently provides a `padded_batch` transformation, but for dense sequential data (audio, video) it might be better to drop a few frames to make the batched examples even. Can we have a new function to `Dataset` like this?\r\n```python\r\ntruncated_batch(\r\n    batch_size,\r\n    truncating_shapes\r\n)\r\n```\r\n\r\nThe semantic would be very similar to `padded_batch` but in reverse. The `truncating_shapes` argument determines the resulting shape for each dimension of each component in an output element:\r\n* If the dimension is a constant (e.g. `tf.Dimension(37)`), the component will be truncated to that length in that dimension.\r\n* If the dimension is unknown (e.g. `tf.Dimension(None)`), the component will be truncated to the minimum length of all elements in that dimension.\r\n\r\nThoughts? @mrry "}