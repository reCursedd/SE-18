{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242239461", "html_url": "https://github.com/tensorflow/tensorflow/issues/3992#issuecomment-242239461", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3992", "id": 242239461, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MjIzOTQ2MQ==", "user": {"login": "iportillo", "id": 10001157, "node_id": "MDQ6VXNlcjEwMDAxMTU3", "avatar_url": "https://avatars2.githubusercontent.com/u/10001157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iportillo", "html_url": "https://github.com/iportillo", "followers_url": "https://api.github.com/users/iportillo/followers", "following_url": "https://api.github.com/users/iportillo/following{/other_user}", "gists_url": "https://api.github.com/users/iportillo/gists{/gist_id}", "starred_url": "https://api.github.com/users/iportillo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iportillo/subscriptions", "organizations_url": "https://api.github.com/users/iportillo/orgs", "repos_url": "https://api.github.com/users/iportillo/repos", "events_url": "https://api.github.com/users/iportillo/events{/privacy}", "received_events_url": "https://api.github.com/users/iportillo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-24T23:28:44Z", "updated_at": "2016-08-24T23:30:02Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a> Sorry to open this issue again and thanks for the suggestion of using placeholders / computing them on runtime.</p>\n<p>However, I have a follow up question related to the implementation of the<code>SlowAppend&lt;TYPE&gt;ArrayToTensorProto</code> functions. Seems that most of these functions follow the pattern:</p>\n<pre><code>def SlowAppend&lt;TYPE&gt;ArrayToTensorProto(tensor_proto, proto_values):\n    tensor_proto.&lt;TYPE&gt;_val.extend([np.asscalar(x) for x in proto_values])\n</code></pre>\n<p>I was wondering if there is an special reason for not using the <code>tolist()</code> method from numpy, as in:</p>\n<pre><code> def SlowAppend&lt;TYPE&gt;ArrayToTensorProto(tensor_proto, proto_values):\n    tensor_proto.&lt;TYPE&gt;_val.extend(proto_values.tolist())\n</code></pre>\n<p>This is much faster for large arrays, and doing some tests seems that the values obtained using both methods are the same (value &amp; type). So I just wanted to check if I am missing something.<br>\nThe only reason for not using <code>tolist()</code> I can think of is that it does not return the same type as <code>np.asscalar()</code>. Reading the documentation for <code>tolist()</code> it says:</p>\n<blockquote>\n<p>Data items are converted to the nearest compatible Python type.</p>\n</blockquote>\n<p>whereas for <code>np.asscalar()</code> it reads:</p>\n<blockquote>\n<p>A copy of the specified element of the array as a suitable Python scalar</p>\n</blockquote>\n<p>Isn't the nearest compatible Python type the same as the suitable Python scalar? I just wanted to check because changing this in the code is super-fast and the speed-up for big constant tensors is really huge.</p>\n<p>Thanks again!</p>", "body_text": "@prb12 Sorry to open this issue again and thanks for the suggestion of using placeholders / computing them on runtime.\nHowever, I have a follow up question related to the implementation of theSlowAppend<TYPE>ArrayToTensorProto functions. Seems that most of these functions follow the pattern:\ndef SlowAppend<TYPE>ArrayToTensorProto(tensor_proto, proto_values):\n    tensor_proto.<TYPE>_val.extend([np.asscalar(x) for x in proto_values])\n\nI was wondering if there is an special reason for not using the tolist() method from numpy, as in:\n def SlowAppend<TYPE>ArrayToTensorProto(tensor_proto, proto_values):\n    tensor_proto.<TYPE>_val.extend(proto_values.tolist())\n\nThis is much faster for large arrays, and doing some tests seems that the values obtained using both methods are the same (value & type). So I just wanted to check if I am missing something.\nThe only reason for not using tolist() I can think of is that it does not return the same type as np.asscalar(). Reading the documentation for tolist() it says:\n\nData items are converted to the nearest compatible Python type.\n\nwhereas for np.asscalar() it reads:\n\nA copy of the specified element of the array as a suitable Python scalar\n\nIsn't the nearest compatible Python type the same as the suitable Python scalar? I just wanted to check because changing this in the code is super-fast and the speed-up for big constant tensors is really huge.\nThanks again!", "body": "@prb12 Sorry to open this issue again and thanks for the suggestion of using placeholders / computing them on runtime. \n\nHowever, I have a follow up question related to the implementation of the`SlowAppend<TYPE>ArrayToTensorProto` functions. Seems that most of these functions follow the pattern:\n\n```\ndef SlowAppend<TYPE>ArrayToTensorProto(tensor_proto, proto_values):\n    tensor_proto.<TYPE>_val.extend([np.asscalar(x) for x in proto_values])\n```\n\nI was wondering if there is an special reason for not using the `tolist()` method from numpy, as in:\n\n```\n def SlowAppend<TYPE>ArrayToTensorProto(tensor_proto, proto_values):\n    tensor_proto.<TYPE>_val.extend(proto_values.tolist())\n```\n\nThis is much faster for large arrays, and doing some tests seems that the values obtained using both methods are the same (value & type). So I just wanted to check if I am missing something.\nThe only reason for not using `tolist()` I can think of is that it does not return the same type as `np.asscalar()`. Reading the documentation for `tolist()` it says:\n\n> Data items are converted to the nearest compatible Python type.\n\nwhereas for `np.asscalar()` it reads:\n\n> A copy of the specified element of the array as a suitable Python scalar\n\nIsn't the nearest compatible Python type the same as the suitable Python scalar? I just wanted to check because changing this in the code is super-fast and the speed-up for big constant tensors is really huge.\n\nThanks again!\n"}