{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/304423133", "html_url": "https://github.com/tensorflow/tensorflow/issues/10091#issuecomment-304423133", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10091", "id": 304423133, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDQyMzEzMw==", "user": {"login": "Catk1ns0n", "id": 20182783, "node_id": "MDQ6VXNlcjIwMTgyNzgz", "avatar_url": "https://avatars3.githubusercontent.com/u/20182783?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Catk1ns0n", "html_url": "https://github.com/Catk1ns0n", "followers_url": "https://api.github.com/users/Catk1ns0n/followers", "following_url": "https://api.github.com/users/Catk1ns0n/following{/other_user}", "gists_url": "https://api.github.com/users/Catk1ns0n/gists{/gist_id}", "starred_url": "https://api.github.com/users/Catk1ns0n/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Catk1ns0n/subscriptions", "organizations_url": "https://api.github.com/users/Catk1ns0n/orgs", "repos_url": "https://api.github.com/users/Catk1ns0n/repos", "events_url": "https://api.github.com/users/Catk1ns0n/events{/privacy}", "received_events_url": "https://api.github.com/users/Catk1ns0n/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-27T03:09:47Z", "updated_at": "2017-05-27T03:09:47Z", "author_association": "NONE", "body_html": "<p>I'm sorry I do not fully understand you. Essentially I am trying to achieve is this:</p>\n<pre><code>\nimport tensorflow as tf\nimport numpy as np\n\nsess = tf.Session()\nx = tf.Variable(tf.truncated_normal([2, 4], stddev=0.04))\nsess.run(tf.variables_initializer([x,]))\n\n# Inefficient declaration of z because values are copied not shared\nz = tf.Variable(tf.concat((x, tf.truncated_normal([1, 4], stddev=0.04)), axis=0))\nsess.run(tf.variables_initializer([z,]))\n\nprint(tf.add(z, np.ones([3, 4])).eval(session=sess))\n</code></pre>\n<p>Except I do not want to waste space in memory because variable z contains the exact values found in x (plus a few extra) and I will never need the matrix x anymore. The reason why I want this functionality is because I want to declare a weight matrix in my neural network which will become larger each iteration but I do not want to waste space by having copies of the previous smaller matrix still in memory.</p>", "body_text": "I'm sorry I do not fully understand you. Essentially I am trying to achieve is this:\n\nimport tensorflow as tf\nimport numpy as np\n\nsess = tf.Session()\nx = tf.Variable(tf.truncated_normal([2, 4], stddev=0.04))\nsess.run(tf.variables_initializer([x,]))\n\n# Inefficient declaration of z because values are copied not shared\nz = tf.Variable(tf.concat((x, tf.truncated_normal([1, 4], stddev=0.04)), axis=0))\nsess.run(tf.variables_initializer([z,]))\n\nprint(tf.add(z, np.ones([3, 4])).eval(session=sess))\n\nExcept I do not want to waste space in memory because variable z contains the exact values found in x (plus a few extra) and I will never need the matrix x anymore. The reason why I want this functionality is because I want to declare a weight matrix in my neural network which will become larger each iteration but I do not want to waste space by having copies of the previous smaller matrix still in memory.", "body": "I'm sorry I do not fully understand you. Essentially I am trying to achieve is this:\r\n\r\n<pre><code>\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsess = tf.Session()\r\nx = tf.Variable(tf.truncated_normal([2, 4], stddev=0.04))\r\nsess.run(tf.variables_initializer([x,]))\r\n\r\n# Inefficient declaration of z because values are copied not shared\r\nz = tf.Variable(tf.concat((x, tf.truncated_normal([1, 4], stddev=0.04)), axis=0))\r\nsess.run(tf.variables_initializer([z,]))\r\n\r\nprint(tf.add(z, np.ones([3, 4])).eval(session=sess))\r\n</pre></code>\r\n\r\nExcept I do not want to waste space in memory because variable z contains the exact values found in x (plus a few extra) and I will never need the matrix x anymore. The reason why I want this functionality is because I want to declare a weight matrix in my neural network which will become larger each iteration but I do not want to waste space by having copies of the previous smaller matrix still in memory."}