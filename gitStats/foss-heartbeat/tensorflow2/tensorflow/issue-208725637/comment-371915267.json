{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/371915267", "html_url": "https://github.com/tensorflow/tensorflow/issues/7669#issuecomment-371915267", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7669", "id": 371915267, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTkxNTI2Nw==", "user": {"login": "NatLun091238", "id": 26412310, "node_id": "MDQ6VXNlcjI2NDEyMzEw", "avatar_url": "https://avatars0.githubusercontent.com/u/26412310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NatLun091238", "html_url": "https://github.com/NatLun091238", "followers_url": "https://api.github.com/users/NatLun091238/followers", "following_url": "https://api.github.com/users/NatLun091238/following{/other_user}", "gists_url": "https://api.github.com/users/NatLun091238/gists{/gist_id}", "starred_url": "https://api.github.com/users/NatLun091238/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NatLun091238/subscriptions", "organizations_url": "https://api.github.com/users/NatLun091238/orgs", "repos_url": "https://api.github.com/users/NatLun091238/repos", "events_url": "https://api.github.com/users/NatLun091238/events{/privacy}", "received_events_url": "https://api.github.com/users/NatLun091238/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-09T19:08:33Z", "updated_at": "2018-03-09T19:08:33Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20136420\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lelugom\">@lelugom</a><br>\nThank you for sharing your solution. I have implemented similar one for Dee-n-Weed model (tf.contrib.learn.DNNLinearCombinedClassifier) using tf.contrib.learn.Experiment . Model compiles well and runs on GCP Datalab instance. However, I noticed that if I run Google ML engine training on the same model, the training stalls after the first check point (does not produce any more check points while time goes). On the other hand, if I run training on GCP instance using <code>python -m trainer.task</code>  with parameters, the model converge as it should. What could be the reason for that difference between ML engine training and ordinary training??</p>", "body_text": "@lelugom\nThank you for sharing your solution. I have implemented similar one for Dee-n-Weed model (tf.contrib.learn.DNNLinearCombinedClassifier) using tf.contrib.learn.Experiment . Model compiles well and runs on GCP Datalab instance. However, I noticed that if I run Google ML engine training on the same model, the training stalls after the first check point (does not produce any more check points while time goes). On the other hand, if I run training on GCP instance using python -m trainer.task  with parameters, the model converge as it should. What could be the reason for that difference between ML engine training and ordinary training??", "body": "@lelugom \r\nThank you for sharing your solution. I have implemented similar one for Dee-n-Weed model (tf.contrib.learn.DNNLinearCombinedClassifier) using tf.contrib.learn.Experiment . Model compiles well and runs on GCP Datalab instance. However, I noticed that if I run Google ML engine training on the same model, the training stalls after the first check point (does not produce any more check points while time goes). On the other hand, if I run training on GCP instance using `python -m trainer.task`  with parameters, the model converge as it should. What could be the reason for that difference between ML engine training and ordinary training??"}