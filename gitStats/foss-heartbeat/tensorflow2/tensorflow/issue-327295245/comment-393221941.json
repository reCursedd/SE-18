{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/393221941", "html_url": "https://github.com/tensorflow/tensorflow/issues/19619#issuecomment-393221941", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19619", "id": 393221941, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzIyMTk0MQ==", "user": {"login": "yegord", "id": 1595829, "node_id": "MDQ6VXNlcjE1OTU4Mjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1595829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yegord", "html_url": "https://github.com/yegord", "followers_url": "https://api.github.com/users/yegord/followers", "following_url": "https://api.github.com/users/yegord/following{/other_user}", "gists_url": "https://api.github.com/users/yegord/gists{/gist_id}", "starred_url": "https://api.github.com/users/yegord/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yegord/subscriptions", "organizations_url": "https://api.github.com/users/yegord/orgs", "repos_url": "https://api.github.com/users/yegord/repos", "events_url": "https://api.github.com/users/yegord/events{/privacy}", "received_events_url": "https://api.github.com/users/yegord/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-30T16:13:35Z", "updated_at": "2018-05-30T16:13:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Please find a minimal example here: <a href=\"https://yadi.sk/d/797LMYGT3Wi2yy\" rel=\"nofollow\">https://yadi.sk/d/797LMYGT3Wi2yy</a><br>\n<code>./minimal_example.py --mode tf</code> gives me <code>5487MiB / 11172MiB</code> GPU memory usage on 1080 Ti, according to nvidia-smi.<br>\n<code>./minimal_example.py --mode trt</code> gives me <code>9987MiB / 11172MiB</code>, which is 80% larger than with plain TensorFlow, is over the max_workspace_size_bytes (which is around 6G, not sure if all of that is used by TensorRT, though) and is close to the physical memory limit.</p>\n<p>Specifying placements of ops into TRT segments sounds like what I proposed here.<br>\nSharing of the allocator also sounds great.<br>\nI guess, I should wait for the TensorRT 4 based release and repeat the experiment.<br>\nCurrently, almost twofold memory usage is a blocker for the use of <code>tf.contrib.tensorrt</code> for me.</p>", "body_text": "Please find a minimal example here: https://yadi.sk/d/797LMYGT3Wi2yy\n./minimal_example.py --mode tf gives me 5487MiB / 11172MiB GPU memory usage on 1080 Ti, according to nvidia-smi.\n./minimal_example.py --mode trt gives me 9987MiB / 11172MiB, which is 80% larger than with plain TensorFlow, is over the max_workspace_size_bytes (which is around 6G, not sure if all of that is used by TensorRT, though) and is close to the physical memory limit.\nSpecifying placements of ops into TRT segments sounds like what I proposed here.\nSharing of the allocator also sounds great.\nI guess, I should wait for the TensorRT 4 based release and repeat the experiment.\nCurrently, almost twofold memory usage is a blocker for the use of tf.contrib.tensorrt for me.", "body": "Please find a minimal example here: https://yadi.sk/d/797LMYGT3Wi2yy\r\n`./minimal_example.py --mode tf` gives me `5487MiB / 11172MiB` GPU memory usage on 1080 Ti, according to nvidia-smi.\r\n`./minimal_example.py --mode trt` gives me `9987MiB / 11172MiB`, which is 80% larger than with plain TensorFlow, is over the max_workspace_size_bytes (which is around 6G, not sure if all of that is used by TensorRT, though) and is close to the physical memory limit.\r\n\r\nSpecifying placements of ops into TRT segments sounds like what I proposed here.\r\nSharing of the allocator also sounds great.\r\nI guess, I should wait for the TensorRT 4 based release and repeat the experiment.\r\nCurrently, almost twofold memory usage is a blocker for the use of `tf.contrib.tensorrt` for me."}