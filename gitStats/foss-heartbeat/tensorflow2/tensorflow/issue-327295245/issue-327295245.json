{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19619", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19619/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19619/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19619/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19619", "id": 327295245, "node_id": "MDU6SXNzdWUzMjcyOTUyNDU=", "number": 19619, "title": "TensorRT: Large memory consumption on SSD-like graphs [Feature Request/Discussion]", "user": {"login": "yegord", "id": 1595829, "node_id": "MDQ6VXNlcjE1OTU4Mjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1595829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yegord", "html_url": "https://github.com/yegord", "followers_url": "https://api.github.com/users/yegord/followers", "following_url": "https://api.github.com/users/yegord/following{/other_user}", "gists_url": "https://api.github.com/users/yegord/gists{/gist_id}", "starred_url": "https://api.github.com/users/yegord/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yegord/subscriptions", "organizations_url": "https://api.github.com/users/yegord/orgs", "repos_url": "https://api.github.com/users/yegord/repos", "events_url": "https://api.github.com/users/yegord/events{/privacy}", "received_events_url": "https://api.github.com/users/yegord/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 17, "created_at": "2018-05-29T12:18:02Z", "updated_at": "2018-08-21T21:36:00Z", "closed_at": "2018-08-21T21:36:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi,</p>\n<p><code>tf.contrib.tensorrt</code> currently automatically segments a given graph into subgraphs which can be fused into TensorRT nodes. This approach works well with networks having linear topology, e.g., CNN classifiers like VGG or ResNet-N: a single node is created, taking an image batch as input and producing a single tensor with predictions.</p>\n<p>The situation with SSD-like networks is less favorable. Such a network has a topology of the form a-&gt;b-&gt;c-&gt;d-&gt;e-&gt;f, a-&gt;a_1, a-&gt;a_2, b-&gt;b_1, b-&gt;b_2, ..., f-&gt;f_1, f-&gt;f2, where a-&gt;b-&gt;c-&gt;d-&gt;e-&gt;f is a feature extractor and a_1, a_2, b_1, b_2, ..., f_1, f_2 are branches stemming from the feature extractor and predicting, e.g., classes and exact locations of the objects in the predefined anchor boxes. The <code>tf.contrib.tensorrt</code>'s segmentation algorithm on such a graph selects a subgraph consisting of all the feature extractor's nodes, plus maybe parts of the branches (e.g., convolutions computing the logits, but not the argmaxes computing the class ids; TensorRT as of version 3 does not support argmax). As a result, we get a huge operation with lots of outputs (e.g., all logits and all raw location amendments, before argmaxes, reshapes, or NCHW-&gt;NHWC transpositions), which all must simultaneously fit in GPU memory at the moment of TensorRT's op's completion.</p>\n<p>When the same graph is computed on TensorFlow, this high peak in memory usage can be (and seems to be) avoided: TensorFlow can compute the feature extractor up to a next level, compute the branches, copy the results computed in the branches into host RAM, go to the next level, and so on.</p>\n<p>This means, <code>tf.contrib.tensorrt</code>-optimized graph can use (and seems to actually use in my experiments) significantly more GPU memory than the original graph, which can lead (and seems to lead in my experiments) to out-of-memory errors.</p>\n<p>One workaround that I tried was to add to <code>tf.contrib.tensorrt.create_inference_graph</code> a parameter for specifying a list of subgraphs which should be independently segmented into subsubgraphs for fusion into TensorRT nodes. I passed individual levels of the feature extractor plus the branches at this level as such subgraphs. This reduced the memory use by TensorRT by a factor of around two and fixed the out-of-memory errors in my case.</p>\n<p>Should such a parameter then maybe added to the mainline <code>tf.contrib.tensorrt</code>?<br>\nMaybe you have a better idea of avoiding a high GPU memory consumption in SSD-like graphs?<br>\nComments, ideas are welcome.</p>\n<p>I guess, I should invite <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10539540\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/samikama\">@samikama</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3709243\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jjsjann123\">@jjsjann123</a> to the discussion.</p>\n<p>In case it matters, my experience comes from the experiments with TensorFlow 1.8, TensorRT-3.0.4 running on Ubuntu 16.04 (AMD64) with GTX 1080 Ti.</p>\n<p>Thanks!</p>", "body_text": "Hi,\ntf.contrib.tensorrt currently automatically segments a given graph into subgraphs which can be fused into TensorRT nodes. This approach works well with networks having linear topology, e.g., CNN classifiers like VGG or ResNet-N: a single node is created, taking an image batch as input and producing a single tensor with predictions.\nThe situation with SSD-like networks is less favorable. Such a network has a topology of the form a->b->c->d->e->f, a->a_1, a->a_2, b->b_1, b->b_2, ..., f->f_1, f->f2, where a->b->c->d->e->f is a feature extractor and a_1, a_2, b_1, b_2, ..., f_1, f_2 are branches stemming from the feature extractor and predicting, e.g., classes and exact locations of the objects in the predefined anchor boxes. The tf.contrib.tensorrt's segmentation algorithm on such a graph selects a subgraph consisting of all the feature extractor's nodes, plus maybe parts of the branches (e.g., convolutions computing the logits, but not the argmaxes computing the class ids; TensorRT as of version 3 does not support argmax). As a result, we get a huge operation with lots of outputs (e.g., all logits and all raw location amendments, before argmaxes, reshapes, or NCHW->NHWC transpositions), which all must simultaneously fit in GPU memory at the moment of TensorRT's op's completion.\nWhen the same graph is computed on TensorFlow, this high peak in memory usage can be (and seems to be) avoided: TensorFlow can compute the feature extractor up to a next level, compute the branches, copy the results computed in the branches into host RAM, go to the next level, and so on.\nThis means, tf.contrib.tensorrt-optimized graph can use (and seems to actually use in my experiments) significantly more GPU memory than the original graph, which can lead (and seems to lead in my experiments) to out-of-memory errors.\nOne workaround that I tried was to add to tf.contrib.tensorrt.create_inference_graph a parameter for specifying a list of subgraphs which should be independently segmented into subsubgraphs for fusion into TensorRT nodes. I passed individual levels of the feature extractor plus the branches at this level as such subgraphs. This reduced the memory use by TensorRT by a factor of around two and fixed the out-of-memory errors in my case.\nShould such a parameter then maybe added to the mainline tf.contrib.tensorrt?\nMaybe you have a better idea of avoiding a high GPU memory consumption in SSD-like graphs?\nComments, ideas are welcome.\nI guess, I should invite @drpngx, @samikama, @jjsjann123 to the discussion.\nIn case it matters, my experience comes from the experiments with TensorFlow 1.8, TensorRT-3.0.4 running on Ubuntu 16.04 (AMD64) with GTX 1080 Ti.\nThanks!", "body": "Hi,\r\n\r\n`tf.contrib.tensorrt` currently automatically segments a given graph into subgraphs which can be fused into TensorRT nodes. This approach works well with networks having linear topology, e.g., CNN classifiers like VGG or ResNet-N: a single node is created, taking an image batch as input and producing a single tensor with predictions.\r\n\r\nThe situation with SSD-like networks is less favorable. Such a network has a topology of the form a->b->c->d->e->f, a->a_1, a->a_2, b->b_1, b->b_2, ..., f->f_1, f->f2, where a->b->c->d->e->f is a feature extractor and a_1, a_2, b_1, b_2, ..., f_1, f_2 are branches stemming from the feature extractor and predicting, e.g., classes and exact locations of the objects in the predefined anchor boxes. The `tf.contrib.tensorrt`'s segmentation algorithm on such a graph selects a subgraph consisting of all the feature extractor's nodes, plus maybe parts of the branches (e.g., convolutions computing the logits, but not the argmaxes computing the class ids; TensorRT as of version 3 does not support argmax). As a result, we get a huge operation with lots of outputs (e.g., all logits and all raw location amendments, before argmaxes, reshapes, or NCHW->NHWC transpositions), which all must simultaneously fit in GPU memory at the moment of TensorRT's op's completion.\r\n\r\nWhen the same graph is computed on TensorFlow, this high peak in memory usage can be (and seems to be) avoided: TensorFlow can compute the feature extractor up to a next level, compute the branches, copy the results computed in the branches into host RAM, go to the next level, and so on.\r\n\r\nThis means, `tf.contrib.tensorrt`-optimized graph can use (and seems to actually use in my experiments) significantly more GPU memory than the original graph, which can lead (and seems to lead in my experiments) to out-of-memory errors.\r\n\r\nOne workaround that I tried was to add to `tf.contrib.tensorrt.create_inference_graph` a parameter for specifying a list of subgraphs which should be independently segmented into subsubgraphs for fusion into TensorRT nodes. I passed individual levels of the feature extractor plus the branches at this level as such subgraphs. This reduced the memory use by TensorRT by a factor of around two and fixed the out-of-memory errors in my case.\r\n\r\nShould such a parameter then maybe added to the mainline `tf.contrib.tensorrt`?\r\nMaybe you have a better idea of avoiding a high GPU memory consumption in SSD-like graphs?\r\nComments, ideas are welcome.\r\n\r\nI guess, I should invite @drpngx, @samikama, @jjsjann123 to the discussion.\r\n\r\nIn case it matters, my experience comes from the experiments with TensorFlow 1.8, TensorRT-3.0.4 running on Ubuntu 16.04 (AMD64) with GTX 1080 Ti.\r\n\r\nThanks!"}