{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8817", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8817/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8817/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8817/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8817", "id": 217987537, "node_id": "MDU6SXNzdWUyMTc5ODc1Mzc=", "number": 8817, "title": "Conditionally trainable variables and stochastic depth neural networks", "user": {"login": "awav", "id": 24483645, "node_id": "MDQ6VXNlcjI0NDgzNjQ1", "avatar_url": "https://avatars0.githubusercontent.com/u/24483645?v=4", "gravatar_id": "", "url": "https://api.github.com/users/awav", "html_url": "https://github.com/awav", "followers_url": "https://api.github.com/users/awav/followers", "following_url": "https://api.github.com/users/awav/following{/other_user}", "gists_url": "https://api.github.com/users/awav/gists{/gist_id}", "starred_url": "https://api.github.com/users/awav/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/awav/subscriptions", "organizations_url": "https://api.github.com/users/awav/orgs", "repos_url": "https://api.github.com/users/awav/repos", "events_url": "https://api.github.com/users/awav/events{/privacy}", "received_events_url": "https://api.github.com/users/awav/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-03-29T19:43:24Z", "updated_at": "2018-03-11T22:15:05Z", "closed_at": "2017-04-14T00:07:56Z", "author_association": "NONE", "body_html": "<p>I came across with a task where I would like to apply stochastic depth <em>regularization</em> technique using Tensorflow (<a href=\"https://arxiv.org/pdf/1603.09382.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1603.09382.pdf</a>). Tensorflow doesn't provide enough settings to implement this one. I found closed issue  <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"146110322\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1784\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1784/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1784\">#1784</a> which is similar to this request, where guys finished the discussion with claim that [ <code>tf.cond</code> | <code>tf.select</code> ] primitives are enough for this task. But if you carefully read the paper it says that during training the depth changes for both directions: forward and backward propagation steps. Therefore number of tranable W parameters of the network changes too. The core conception of the Tensorflow is building computation graph before session of training is run. Currently, I can not create dynamic computation graph, so that depending on a boolean value W parameters of a layer were not engaged in optimisation process.</p>\n<p>If <code>tf.Variable</code> accepted <code>trainable</code> parameter as a boolean tensor apart from built-in boolean value it would solve the problem. In this case, it would mean that Tensorflow operates natively with dynamic computational graphs, which in fact very powerful tool.</p>\n<p>I would appreciate any suggestions and ideas, so that this question was closed for good and all.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a></p>", "body_text": "I came across with a task where I would like to apply stochastic depth regularization technique using Tensorflow (https://arxiv.org/pdf/1603.09382.pdf). Tensorflow doesn't provide enough settings to implement this one. I found closed issue  #1784 which is similar to this request, where guys finished the discussion with claim that [ tf.cond | tf.select ] primitives are enough for this task. But if you carefully read the paper it says that during training the depth changes for both directions: forward and backward propagation steps. Therefore number of tranable W parameters of the network changes too. The core conception of the Tensorflow is building computation graph before session of training is run. Currently, I can not create dynamic computation graph, so that depending on a boolean value W parameters of a layer were not engaged in optimisation process.\nIf tf.Variable accepted trainable parameter as a boolean tensor apart from built-in boolean value it would solve the problem. In this case, it would mean that Tensorflow operates natively with dynamic computational graphs, which in fact very powerful tool.\nI would appreciate any suggestions and ideas, so that this question was closed for good and all.\n@vrv, @martinwicke, @aselle", "body": "I came across with a task where I would like to apply stochastic depth _regularization_ technique using Tensorflow (https://arxiv.org/pdf/1603.09382.pdf). Tensorflow doesn't provide enough settings to implement this one. I found closed issue  #1784 which is similar to this request, where guys finished the discussion with claim that [ `tf.cond` | `tf.select` ] primitives are enough for this task. But if you carefully read the paper it says that during training the depth changes for both directions: forward and backward propagation steps. Therefore number of tranable W parameters of the network changes too. The core conception of the Tensorflow is building computation graph before session of training is run. Currently, I can not create dynamic computation graph, so that depending on a boolean value W parameters of a layer were not engaged in optimisation process.\r\n\r\nIf `tf.Variable` accepted `trainable` parameter as a boolean tensor apart from built-in boolean value it would solve the problem. In this case, it would mean that Tensorflow operates natively with dynamic computational graphs, which in fact very powerful tool.\r\n\r\nI would appreciate any suggestions and ideas, so that this question was closed for good and all.\r\n\r\n@vrv, @martinwicke, @aselle"}