{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296060815", "html_url": "https://github.com/tensorflow/tensorflow/issues/8817#issuecomment-296060815", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8817", "id": 296060815, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjA2MDgxNQ==", "user": {"login": "samjabrahams", "id": 11607205, "node_id": "MDQ6VXNlcjExNjA3MjA1", "avatar_url": "https://avatars0.githubusercontent.com/u/11607205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samjabrahams", "html_url": "https://github.com/samjabrahams", "followers_url": "https://api.github.com/users/samjabrahams/followers", "following_url": "https://api.github.com/users/samjabrahams/following{/other_user}", "gists_url": "https://api.github.com/users/samjabrahams/gists{/gist_id}", "starred_url": "https://api.github.com/users/samjabrahams/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samjabrahams/subscriptions", "organizations_url": "https://api.github.com/users/samjabrahams/orgs", "repos_url": "https://api.github.com/users/samjabrahams/repos", "events_url": "https://api.github.com/users/samjabrahams/events{/privacy}", "received_events_url": "https://api.github.com/users/samjabrahams/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-21T04:41:54Z", "updated_at": "2017-04-21T04:41:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=24483645\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/awav\">@awav</a> - correct me if I'm missing something, but is the goal to simply not update Variables that aren't used due to a conditional? TensorFlow already zeros out these gradients. Here's some sample code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\ntf.reset_default_graph()\n\na <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">10.0</span>)\nb <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">10.0</span>)\nswitch <span class=\"pl-k\">=</span> tf.placeholder(tf.bool)\nres <span class=\"pl-k\">=</span> tf.cond(switch, <span class=\"pl-k\">lambda</span>: tf.mul(<span class=\"pl-c1\">2.0</span>, a), <span class=\"pl-k\">lambda</span>: tf.square(b))\nopt <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.05</span>)\ngrads <span class=\"pl-k\">=</span> opt.compute_gradients(res)\ntrain <span class=\"pl-k\">=</span> opt.apply_gradients(grads)\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(init)\n    <span class=\"pl-c1\">print</span>(sess.run(grads, {switch: <span class=\"pl-c1\">True</span>}))</pre></div>\n<p>If you adjust the <code>{switch: True}</code> dict to be set to false, you'll see that the gradient values will flip depending on which path is taken.</p>\n<p>When <code> {switch: True}</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>[(<span class=\"pl-c1\">2.0</span>, <span class=\"pl-c1\">10.0</span>), (<span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">10.0</span>)]</pre></div>\n<p>When <code> {switch: False}</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>[(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">10.0</span>), (<span class=\"pl-c1\">20.0</span>, <span class=\"pl-c1\">10.0</span>)]</pre></div>\n<p>For completeness, if you apply the gradients with different switches set, you only update one or the other:</p>\n<p><code> {switch: True}</code>:</p>\n<pre><code>with tf.Session() as sess:\n    sess.run(init)\n    sess.run(train, {switch: True})\n    print(sess.run([a, b]))\n\n&gt;&gt;&gt; [9.8999996, 10.0]\n</code></pre>\n<p><code> {switch: False}</code>:</p>\n<pre><code>with tf.Session() as sess:\n    sess.run(init)\n    sess.run(train, {switch: False})\n    print(sess.run([a, b]))\n\n&gt;&gt;&gt; [10.0, 9.0]\n</code></pre>\n<p>I think the most likely problem that might occur when trying to implement stochastic depth is that you may not see the reduced computation due to the less-lazy way <code>tf.cond</code> executes (<a href=\"https://www.tensorflow.org/api_docs/python/tf/cond\" rel=\"nofollow\">see the last paragraph of the documentation before the \"args\" section</a>).</p>", "body_text": "@awav - correct me if I'm missing something, but is the goal to simply not update Variables that aren't used due to a conditional? TensorFlow already zeros out these gradients. Here's some sample code:\nimport tensorflow as tf\n\ntf.reset_default_graph()\n\na = tf.Variable(10.0)\nb = tf.Variable(10.0)\nswitch = tf.placeholder(tf.bool)\nres = tf.cond(switch, lambda: tf.mul(2.0, a), lambda: tf.square(b))\nopt = tf.train.GradientDescentOptimizer(0.05)\ngrads = opt.compute_gradients(res)\ntrain = opt.apply_gradients(grads)\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    print(sess.run(grads, {switch: True}))\nIf you adjust the {switch: True} dict to be set to false, you'll see that the gradient values will flip depending on which path is taken.\nWhen  {switch: True}:\n[(2.0, 10.0), (0.0, 10.0)]\nWhen  {switch: False}:\n[(0.0, 10.0), (20.0, 10.0)]\nFor completeness, if you apply the gradients with different switches set, you only update one or the other:\n {switch: True}:\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(train, {switch: True})\n    print(sess.run([a, b]))\n\n>>> [9.8999996, 10.0]\n\n {switch: False}:\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(train, {switch: False})\n    print(sess.run([a, b]))\n\n>>> [10.0, 9.0]\n\nI think the most likely problem that might occur when trying to implement stochastic depth is that you may not see the reduced computation due to the less-lazy way tf.cond executes (see the last paragraph of the documentation before the \"args\" section).", "body": "@awav - correct me if I'm missing something, but is the goal to simply not update Variables that aren't used due to a conditional? TensorFlow already zeros out these gradients. Here's some sample code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.reset_default_graph()\r\n\r\na = tf.Variable(10.0)\r\nb = tf.Variable(10.0)\r\nswitch = tf.placeholder(tf.bool)\r\nres = tf.cond(switch, lambda: tf.mul(2.0, a), lambda: tf.square(b))\r\nopt = tf.train.GradientDescentOptimizer(0.05)\r\ngrads = opt.compute_gradients(res)\r\ntrain = opt.apply_gradients(grads)\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    print(sess.run(grads, {switch: True}))\r\n```\r\n\r\nIf you adjust the `{switch: True}` dict to be set to false, you'll see that the gradient values will flip depending on which path is taken.\r\n\r\nWhen ` {switch: True}`:\r\n\r\n```python\r\n[(2.0, 10.0), (0.0, 10.0)]\r\n```\r\n\r\nWhen ` {switch: False}`:\r\n\r\n```python\r\n[(0.0, 10.0), (20.0, 10.0)]\r\n```\r\n\r\nFor completeness, if you apply the gradients with different switches set, you only update one or the other:\r\n\r\n ` {switch: True}`:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(train, {switch: True})\r\n    print(sess.run([a, b]))\r\n\r\n>>> [9.8999996, 10.0]\r\n```\r\n\r\n` {switch: False}`:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(train, {switch: False})\r\n    print(sess.run([a, b]))\r\n\r\n>>> [10.0, 9.0]\r\n```\r\n\r\nI think the most likely problem that might occur when trying to implement stochastic depth is that you may not see the reduced computation due to the less-lazy way `tf.cond` executes ([see the last paragraph of the documentation before the \"args\" section](https://www.tensorflow.org/api_docs/python/tf/cond))."}