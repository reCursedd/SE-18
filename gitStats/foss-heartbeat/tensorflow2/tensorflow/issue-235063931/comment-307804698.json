{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/307804698", "html_url": "https://github.com/tensorflow/tensorflow/issues/10632#issuecomment-307804698", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10632", "id": 307804698, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzgwNDY5OA==", "user": {"login": "jiewwantan", "id": 5772491, "node_id": "MDQ6VXNlcjU3NzI0OTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/5772491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiewwantan", "html_url": "https://github.com/jiewwantan", "followers_url": "https://api.github.com/users/jiewwantan/followers", "following_url": "https://api.github.com/users/jiewwantan/following{/other_user}", "gists_url": "https://api.github.com/users/jiewwantan/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiewwantan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiewwantan/subscriptions", "organizations_url": "https://api.github.com/users/jiewwantan/orgs", "repos_url": "https://api.github.com/users/jiewwantan/repos", "events_url": "https://api.github.com/users/jiewwantan/events{/privacy}", "received_events_url": "https://api.github.com/users/jiewwantan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-12T14:23:39Z", "updated_at": "2017-06-12T14:23:39Z", "author_association": "NONE", "body_html": "<p>Thanks, lakshayg for sharing an example.  I recheck and think I do feed the relevant values into the placeholders, I'm not very sure though. Below is my graph and 2 feed_dict. For the same placeholders, <em>self.optimizer.run</em> feed_dict works well. But <em>not <em>summary_str = self.session.run(merged_summary_op</em></em></p>\n<p>Here is a snapshot of my codes:</p>\n<pre><code>def get_initial_data():\n    data_dictionary = {}\n    data_dictionary[\"input\"] = len(x_train[0][0]) + 1 \n    data_dictionary[\"action\"] = 3 \n    data_dictionary[\"hidden_layer_1_size\"] = 40\n    data_dictionary[\"hidden_layer_2_size\"] = 20 \n    data_dictionary[\"x_train\"] = x_train\n    data_dictionary[\"x_test\"] = x_test\n    data_dictionary[\"y_test\"] = y_test\n    data_dictionary[\"y_train\"] = y_train\n    return data_dictionary\n\nclass DQN():\n    # DQN Agent\n    def __init__(self, data_dictionary):\n        self.replay_buffer = deque()\n        self.time_step = 0\n        self.epsilon = INITIAL_EPSILON #1\n        self.state_dim = data_dictionary[\"input\"] #21\n        self.action_dim = data_dictionary[\"action\"] #3\n        self.create_Q_network(data_dictionary)\n        self.create_training_method()\n\n        # Init session\n        self.session = tf.InteractiveSession()\n        self.session.run(tf.initialize_all_variables())\n        global summary_writer\n        summary_writer = tf.summary.FileWriter('logs',graph=self.session.graph)\n\n  def create_Q_network(self, data_dictionary):       \n        # network weights \n        #[21,40]\n        W1 = self.weight_variable([self.state_dim,data_dictionary[\"hidden_layer_1_size\"]])\n        variable_summaries(W1, \"layer1/weights\")\n        # [40]\n        b1 = self.bias_variable([data_dictionary[\"hidden_layer_1_size\"]])\n        variable_summaries(b1, \"layer1/bias\")\n        # [21,3]\n        W2 = self.weight_variable([data_dictionary[\"hidden_layer_1_size\"],self.action_dim])\n        variable_summaries(W2, \"layer2/weights\")\n        # [3]\n        b2 = self.bias_variable([self.action_dim])\n        variable_summaries(b2, \"layer2/bias\")\n        #tf.summary.scalar(\"second_layer_bias_scaler\", b2)\n        self.b2 = b2\n        \n        # input layer\n        # [None,21]\n        self.state_input = tf.placeholder(tf.float32,[None,self.state_dim])\n        # hidden layers\n        h_layer = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\n        # Q Value layer\n        self.Q_value = tf.matmul(h_layer,W2) + b2\n\n  def create_training_method(self):\n        # [none,3]\n        self.action_input = tf.placeholder(tf.float32,[None,self.action_dim]) # one hot presentation\n        self.y_input = tf.placeholder(tf.float32,[None])\n        Q_action = tf.reduce_sum(tf.multiply(self.Q_value,self.action_input),reduction_indices = 1)\n        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n        tf.summary.scalar(\"loss\",self.cost)\n        global merged_summary_op\n        merged_summary_op = tf.summary.merge_all()\n        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n\n  def train_Q_network(self):\n        # Step 1: obtain random minibatch from replay memory\n        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n        #print(\"minibatch: \",minibatch)\n        state_batch = [data[0] for data in minibatch]\n        action_batch = [data[1] for data in minibatch]\n        reward_batch = [data[2] for data in minibatch]\n        next_state_batch = [data[3] for data in minibatch]\n\n        # Step 2: calculate y\n        y_batch = []\n        Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\n        for i in range(0,BATCH_SIZE):\n            done = minibatch[i][4]\n            if done:\n                y_batch.append(reward_batch[i])\n            else :\n                y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n\n        self.optimizer.run(feed_dict={\n            self.y_input:y_batch,\n            self.action_input:action_batch,\n            self.state_input:state_batch\n            })\n\n       summary_str = self.session.run(merged_summary_op,feed_dict={\n       self.y_input : y_batch,\n       self.action_input : action_batch,\n       self.state_input : state_batch })    \n       summary_writer.add_summary(summary_str,self.time_step)\n</code></pre>", "body_text": "Thanks, lakshayg for sharing an example.  I recheck and think I do feed the relevant values into the placeholders, I'm not very sure though. Below is my graph and 2 feed_dict. For the same placeholders, self.optimizer.run feed_dict works well. But not summary_str = self.session.run(merged_summary_op\nHere is a snapshot of my codes:\ndef get_initial_data():\n    data_dictionary = {}\n    data_dictionary[\"input\"] = len(x_train[0][0]) + 1 \n    data_dictionary[\"action\"] = 3 \n    data_dictionary[\"hidden_layer_1_size\"] = 40\n    data_dictionary[\"hidden_layer_2_size\"] = 20 \n    data_dictionary[\"x_train\"] = x_train\n    data_dictionary[\"x_test\"] = x_test\n    data_dictionary[\"y_test\"] = y_test\n    data_dictionary[\"y_train\"] = y_train\n    return data_dictionary\n\nclass DQN():\n    # DQN Agent\n    def __init__(self, data_dictionary):\n        self.replay_buffer = deque()\n        self.time_step = 0\n        self.epsilon = INITIAL_EPSILON #1\n        self.state_dim = data_dictionary[\"input\"] #21\n        self.action_dim = data_dictionary[\"action\"] #3\n        self.create_Q_network(data_dictionary)\n        self.create_training_method()\n\n        # Init session\n        self.session = tf.InteractiveSession()\n        self.session.run(tf.initialize_all_variables())\n        global summary_writer\n        summary_writer = tf.summary.FileWriter('logs',graph=self.session.graph)\n\n  def create_Q_network(self, data_dictionary):       \n        # network weights \n        #[21,40]\n        W1 = self.weight_variable([self.state_dim,data_dictionary[\"hidden_layer_1_size\"]])\n        variable_summaries(W1, \"layer1/weights\")\n        # [40]\n        b1 = self.bias_variable([data_dictionary[\"hidden_layer_1_size\"]])\n        variable_summaries(b1, \"layer1/bias\")\n        # [21,3]\n        W2 = self.weight_variable([data_dictionary[\"hidden_layer_1_size\"],self.action_dim])\n        variable_summaries(W2, \"layer2/weights\")\n        # [3]\n        b2 = self.bias_variable([self.action_dim])\n        variable_summaries(b2, \"layer2/bias\")\n        #tf.summary.scalar(\"second_layer_bias_scaler\", b2)\n        self.b2 = b2\n        \n        # input layer\n        # [None,21]\n        self.state_input = tf.placeholder(tf.float32,[None,self.state_dim])\n        # hidden layers\n        h_layer = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\n        # Q Value layer\n        self.Q_value = tf.matmul(h_layer,W2) + b2\n\n  def create_training_method(self):\n        # [none,3]\n        self.action_input = tf.placeholder(tf.float32,[None,self.action_dim]) # one hot presentation\n        self.y_input = tf.placeholder(tf.float32,[None])\n        Q_action = tf.reduce_sum(tf.multiply(self.Q_value,self.action_input),reduction_indices = 1)\n        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n        tf.summary.scalar(\"loss\",self.cost)\n        global merged_summary_op\n        merged_summary_op = tf.summary.merge_all()\n        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n\n  def train_Q_network(self):\n        # Step 1: obtain random minibatch from replay memory\n        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n        #print(\"minibatch: \",minibatch)\n        state_batch = [data[0] for data in minibatch]\n        action_batch = [data[1] for data in minibatch]\n        reward_batch = [data[2] for data in minibatch]\n        next_state_batch = [data[3] for data in minibatch]\n\n        # Step 2: calculate y\n        y_batch = []\n        Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\n        for i in range(0,BATCH_SIZE):\n            done = minibatch[i][4]\n            if done:\n                y_batch.append(reward_batch[i])\n            else :\n                y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n\n        self.optimizer.run(feed_dict={\n            self.y_input:y_batch,\n            self.action_input:action_batch,\n            self.state_input:state_batch\n            })\n\n       summary_str = self.session.run(merged_summary_op,feed_dict={\n       self.y_input : y_batch,\n       self.action_input : action_batch,\n       self.state_input : state_batch })    \n       summary_writer.add_summary(summary_str,self.time_step)", "body": "Thanks, lakshayg for sharing an example.  I recheck and think I do feed the relevant values into the placeholders, I'm not very sure though. Below is my graph and 2 feed_dict. For the same placeholders, _self.optimizer.run_ feed_dict works well. But _not _summary_str = self.session.run(merged_summary_op__\r\n\r\nHere is a snapshot of my codes: \r\n\r\n```\r\ndef get_initial_data():\r\n    data_dictionary = {}\r\n    data_dictionary[\"input\"] = len(x_train[0][0]) + 1 \r\n    data_dictionary[\"action\"] = 3 \r\n    data_dictionary[\"hidden_layer_1_size\"] = 40\r\n    data_dictionary[\"hidden_layer_2_size\"] = 20 \r\n    data_dictionary[\"x_train\"] = x_train\r\n    data_dictionary[\"x_test\"] = x_test\r\n    data_dictionary[\"y_test\"] = y_test\r\n    data_dictionary[\"y_train\"] = y_train\r\n    return data_dictionary\r\n\r\nclass DQN():\r\n    # DQN Agent\r\n    def __init__(self, data_dictionary):\r\n        self.replay_buffer = deque()\r\n        self.time_step = 0\r\n        self.epsilon = INITIAL_EPSILON #1\r\n        self.state_dim = data_dictionary[\"input\"] #21\r\n        self.action_dim = data_dictionary[\"action\"] #3\r\n        self.create_Q_network(data_dictionary)\r\n        self.create_training_method()\r\n\r\n        # Init session\r\n        self.session = tf.InteractiveSession()\r\n        self.session.run(tf.initialize_all_variables())\r\n        global summary_writer\r\n        summary_writer = tf.summary.FileWriter('logs',graph=self.session.graph)\r\n\r\n  def create_Q_network(self, data_dictionary):       \r\n        # network weights \r\n        #[21,40]\r\n        W1 = self.weight_variable([self.state_dim,data_dictionary[\"hidden_layer_1_size\"]])\r\n        variable_summaries(W1, \"layer1/weights\")\r\n        # [40]\r\n        b1 = self.bias_variable([data_dictionary[\"hidden_layer_1_size\"]])\r\n        variable_summaries(b1, \"layer1/bias\")\r\n        # [21,3]\r\n        W2 = self.weight_variable([data_dictionary[\"hidden_layer_1_size\"],self.action_dim])\r\n        variable_summaries(W2, \"layer2/weights\")\r\n        # [3]\r\n        b2 = self.bias_variable([self.action_dim])\r\n        variable_summaries(b2, \"layer2/bias\")\r\n        #tf.summary.scalar(\"second_layer_bias_scaler\", b2)\r\n        self.b2 = b2\r\n        \r\n        # input layer\r\n        # [None,21]\r\n        self.state_input = tf.placeholder(tf.float32,[None,self.state_dim])\r\n        # hidden layers\r\n        h_layer = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\r\n        # Q Value layer\r\n        self.Q_value = tf.matmul(h_layer,W2) + b2\r\n\r\n  def create_training_method(self):\r\n        # [none,3]\r\n        self.action_input = tf.placeholder(tf.float32,[None,self.action_dim]) # one hot presentation\r\n        self.y_input = tf.placeholder(tf.float32,[None])\r\n        Q_action = tf.reduce_sum(tf.multiply(self.Q_value,self.action_input),reduction_indices = 1)\r\n        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\r\n        tf.summary.scalar(\"loss\",self.cost)\r\n        global merged_summary_op\r\n        merged_summary_op = tf.summary.merge_all()\r\n        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\r\n\r\n  def train_Q_network(self):\r\n        # Step 1: obtain random minibatch from replay memory\r\n        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\r\n        #print(\"minibatch: \",minibatch)\r\n        state_batch = [data[0] for data in minibatch]\r\n        action_batch = [data[1] for data in minibatch]\r\n        reward_batch = [data[2] for data in minibatch]\r\n        next_state_batch = [data[3] for data in minibatch]\r\n\r\n        # Step 2: calculate y\r\n        y_batch = []\r\n        Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\r\n        for i in range(0,BATCH_SIZE):\r\n            done = minibatch[i][4]\r\n            if done:\r\n                y_batch.append(reward_batch[i])\r\n            else :\r\n                y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\r\n\r\n        self.optimizer.run(feed_dict={\r\n            self.y_input:y_batch,\r\n            self.action_input:action_batch,\r\n            self.state_input:state_batch\r\n            })\r\n\r\n       summary_str = self.session.run(merged_summary_op,feed_dict={\r\n       self.y_input : y_batch,\r\n       self.action_input : action_batch,\r\n       self.state_input : state_batch })    \r\n       summary_writer.add_summary(summary_str,self.time_step)\r\n```"}