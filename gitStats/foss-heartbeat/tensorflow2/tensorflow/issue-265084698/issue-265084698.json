{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13669", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13669/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13669/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13669/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13669", "id": 265084698, "node_id": "MDU6SXNzdWUyNjUwODQ2OTg=", "number": 13669, "title": "monotonic attention is buggy", "user": {"login": "georgesterpu", "id": 6018251, "node_id": "MDQ6VXNlcjYwMTgyNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6018251?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgesterpu", "html_url": "https://github.com/georgesterpu", "followers_url": "https://api.github.com/users/georgesterpu/followers", "following_url": "https://api.github.com/users/georgesterpu/following{/other_user}", "gists_url": "https://api.github.com/users/georgesterpu/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgesterpu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgesterpu/subscriptions", "organizations_url": "https://api.github.com/users/georgesterpu/orgs", "repos_url": "https://api.github.com/users/georgesterpu/repos", "events_url": "https://api.github.com/users/georgesterpu/events{/privacy}", "received_events_url": "https://api.github.com/users/georgesterpu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-10-12T21:02:34Z", "updated_at": "2017-10-30T16:42:24Z", "closed_at": "2017-10-17T21:17:54Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li>**Have I written custom code **: Yes</li>\n<li><strong>OS Platform and Distribution</strong>: Manjaro Linux, kernel 4.13.5</li>\n<li>**TensorFlow installed from **: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3, 1.4 nightly (11 oct)</li>\n<li><strong>Python version</strong>:  3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: cuda8, cudnn 7 &amp; 6</li>\n<li><strong>GPU model and memory</strong>: gtx 1080</li>\n</ul>\n<h3>the problem</h3>\n<p>The two monotonic attention mechanisms, LuongMonotonicAttention and BahdanauMonotonicAttention, do not seem to work as expected on my task.</p>\n<p>In the case of LuongMonotonicAttention, the alignment that I obtain looks like a horizontal line drawn on the first row of the image. However, the alignment has a diagonal shape using BahdanauAttention or LuongAttention, and I am instantiating these classes with the same parameters.</p>\n<p>In the case of BahdanauMonotonicAttention, I simply receive an error message:<br>\n<a href=\"https://pastebin.com/raw/mPPWnEH5\" rel=\"nofollow\">https://pastebin.com/raw/mPPWnEH5</a></p>\n<p>Is there any preprocessing I should do in addition to the non-monotonic case ?</p>", "body_text": "System information\n\n**Have I written custom code **: Yes\nOS Platform and Distribution: Manjaro Linux, kernel 4.13.5\n**TensorFlow installed from **: binary\nTensorFlow version (use command below): 1.3, 1.4 nightly (11 oct)\nPython version:  3.6\nBazel version (if compiling from source):\nCUDA/cuDNN version: cuda8, cudnn 7 & 6\nGPU model and memory: gtx 1080\n\nthe problem\nThe two monotonic attention mechanisms, LuongMonotonicAttention and BahdanauMonotonicAttention, do not seem to work as expected on my task.\nIn the case of LuongMonotonicAttention, the alignment that I obtain looks like a horizontal line drawn on the first row of the image. However, the alignment has a diagonal shape using BahdanauAttention or LuongAttention, and I am instantiating these classes with the same parameters.\nIn the case of BahdanauMonotonicAttention, I simply receive an error message:\nhttps://pastebin.com/raw/mPPWnEH5\nIs there any preprocessing I should do in addition to the non-monotonic case ?", "body": "### System information\r\n- **Have I written custom code **: Yes\r\n- **OS Platform and Distribution**: Manjaro Linux, kernel 4.13.5\r\n- **TensorFlow installed from **: binary\r\n- **TensorFlow version (use command below)**: 1.3, 1.4 nightly (11 oct)\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cuda8, cudnn 7 & 6\r\n- **GPU model and memory**: gtx 1080\r\n\r\n### the problem\r\n\r\nThe two monotonic attention mechanisms, LuongMonotonicAttention and BahdanauMonotonicAttention, do not seem to work as expected on my task.\r\n\r\nIn the case of LuongMonotonicAttention, the alignment that I obtain looks like a horizontal line drawn on the first row of the image. However, the alignment has a diagonal shape using BahdanauAttention or LuongAttention, and I am instantiating these classes with the same parameters.\r\n\r\nIn the case of BahdanauMonotonicAttention, I simply receive an error message:\r\nhttps://pastebin.com/raw/mPPWnEH5\r\n\r\nIs there any preprocessing I should do in addition to the non-monotonic case ?"}