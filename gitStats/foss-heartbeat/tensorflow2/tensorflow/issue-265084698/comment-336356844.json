{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/336356844", "html_url": "https://github.com/tensorflow/tensorflow/issues/13669#issuecomment-336356844", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13669", "id": 336356844, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjM1Njg0NA==", "user": {"login": "craffel", "id": 417568, "node_id": "MDQ6VXNlcjQxNzU2OA==", "avatar_url": "https://avatars0.githubusercontent.com/u/417568?v=4", "gravatar_id": "", "url": "https://api.github.com/users/craffel", "html_url": "https://github.com/craffel", "followers_url": "https://api.github.com/users/craffel/followers", "following_url": "https://api.github.com/users/craffel/following{/other_user}", "gists_url": "https://api.github.com/users/craffel/gists{/gist_id}", "starred_url": "https://api.github.com/users/craffel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/craffel/subscriptions", "organizations_url": "https://api.github.com/users/craffel/orgs", "repos_url": "https://api.github.com/users/craffel/repos", "events_url": "https://api.github.com/users/craffel/events{/privacy}", "received_events_url": "https://api.github.com/users/craffel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-13T05:49:47Z", "updated_at": "2017-10-13T05:49:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, monotonic attention won't always work right out of the box when you swap softmax attention out for it.  Please see the practitioner's guide at the end of <a href=\"https://arxiv.org/abs/1704.00784\" rel=\"nofollow\">the paper</a>.  If the attention is always attending to the first encoder timestep for all decoder timesteps, you probably should try setting <code>score_bias_init</code> to a negative value.  You also will almost certainly get better results if you set <code>scale=True</code>.  If you post images of your softmax attention and the produced monotonic attention alignments, I can probably help debug further.</p>\n<p>For the BahdanauMonotonicAttention shape error, that seems more like it may be a bug.  Can you post a full code snippet to reproduce this error?</p>", "body_text": "Hi, monotonic attention won't always work right out of the box when you swap softmax attention out for it.  Please see the practitioner's guide at the end of the paper.  If the attention is always attending to the first encoder timestep for all decoder timesteps, you probably should try setting score_bias_init to a negative value.  You also will almost certainly get better results if you set scale=True.  If you post images of your softmax attention and the produced monotonic attention alignments, I can probably help debug further.\nFor the BahdanauMonotonicAttention shape error, that seems more like it may be a bug.  Can you post a full code snippet to reproduce this error?", "body": "Hi, monotonic attention won't always work right out of the box when you swap softmax attention out for it.  Please see the practitioner's guide at the end of [the paper](https://arxiv.org/abs/1704.00784).  If the attention is always attending to the first encoder timestep for all decoder timesteps, you probably should try setting `score_bias_init` to a negative value.  You also will almost certainly get better results if you set `scale=True`.  If you post images of your softmax attention and the produced monotonic attention alignments, I can probably help debug further.\r\n\r\nFor the BahdanauMonotonicAttention shape error, that seems more like it may be a bug.  Can you post a full code snippet to reproduce this error?"}