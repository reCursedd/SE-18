{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/159338133", "html_url": "https://github.com/tensorflow/tensorflow/issues/323#issuecomment-159338133", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/323", "id": 159338133, "node_id": "MDEyOklzc3VlQ29tbWVudDE1OTMzODEzMw==", "user": {"login": "vincentvanhoucke", "id": 15737127, "node_id": "MDQ6VXNlcjE1NzM3MTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/15737127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vincentvanhoucke", "html_url": "https://github.com/vincentvanhoucke", "followers_url": "https://api.github.com/users/vincentvanhoucke/followers", "following_url": "https://api.github.com/users/vincentvanhoucke/following{/other_user}", "gists_url": "https://api.github.com/users/vincentvanhoucke/gists{/gist_id}", "starred_url": "https://api.github.com/users/vincentvanhoucke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vincentvanhoucke/subscriptions", "organizations_url": "https://api.github.com/users/vincentvanhoucke/orgs", "repos_url": "https://api.github.com/users/vincentvanhoucke/repos", "events_url": "https://api.github.com/users/vincentvanhoucke/events{/privacy}", "received_events_url": "https://api.github.com/users/vincentvanhoucke/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-24T16:57:18Z", "updated_at": "2015-11-24T16:57:18Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2369107\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jpiabrantes\">@jpiabrantes</a> One thing that stands out here is that you use a square root in your loss. That's known to be very unstable numerically for very small values. Can you try 1) not taking the square root or 2) adding a small constant, e.g.: tf.sqrt(1e-4 + ...).<br>\nIf that's indeed the problem, then one possible approach we can provide tooling around is to optionally cap the gradient of the sqrt function, because it's a common use case. Another possibility is to use gradient clipping in general (see: clip_by_norm(), which we should provide better examples for).<br>\nLet me know what happens.</p>\n<p>The other avenue is to tune your learning rate decay. Gradients tend to paradoxically grow in magnitude as training progresses, and sometimes that introduces numerical issues late in training. It is still possible that we have issues with numerical precision somewhere, but my hunch is that these errors are legitimate divergence issues (for some twisted definition of 'legitimate').</p>", "body_text": "@jpiabrantes One thing that stands out here is that you use a square root in your loss. That's known to be very unstable numerically for very small values. Can you try 1) not taking the square root or 2) adding a small constant, e.g.: tf.sqrt(1e-4 + ...).\nIf that's indeed the problem, then one possible approach we can provide tooling around is to optionally cap the gradient of the sqrt function, because it's a common use case. Another possibility is to use gradient clipping in general (see: clip_by_norm(), which we should provide better examples for).\nLet me know what happens.\nThe other avenue is to tune your learning rate decay. Gradients tend to paradoxically grow in magnitude as training progresses, and sometimes that introduces numerical issues late in training. It is still possible that we have issues with numerical precision somewhere, but my hunch is that these errors are legitimate divergence issues (for some twisted definition of 'legitimate').", "body": "@jpiabrantes One thing that stands out here is that you use a square root in your loss. That's known to be very unstable numerically for very small values. Can you try 1) not taking the square root or 2) adding a small constant, e.g.: tf.sqrt(1e-4 + ...).\nIf that's indeed the problem, then one possible approach we can provide tooling around is to optionally cap the gradient of the sqrt function, because it's a common use case. Another possibility is to use gradient clipping in general (see: clip_by_norm(), which we should provide better examples for).\nLet me know what happens.\n\nThe other avenue is to tune your learning rate decay. Gradients tend to paradoxically grow in magnitude as training progresses, and sometimes that introduces numerical issues late in training. It is still possible that we have issues with numerical precision somewhere, but my hunch is that these errors are legitimate divergence issues (for some twisted definition of 'legitimate').\n"}