{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/159314385", "html_url": "https://github.com/tensorflow/tensorflow/issues/323#issuecomment-159314385", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/323", "id": 159314385, "node_id": "MDEyOklzc3VlQ29tbWVudDE1OTMxNDM4NQ==", "user": {"login": "delip", "id": 347398, "node_id": "MDQ6VXNlcjM0NzM5OA==", "avatar_url": "https://avatars1.githubusercontent.com/u/347398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/delip", "html_url": "https://github.com/delip", "followers_url": "https://api.github.com/users/delip/followers", "following_url": "https://api.github.com/users/delip/following{/other_user}", "gists_url": "https://api.github.com/users/delip/gists{/gist_id}", "starred_url": "https://api.github.com/users/delip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/delip/subscriptions", "organizations_url": "https://api.github.com/users/delip/orgs", "repos_url": "https://api.github.com/users/delip/repos", "events_url": "https://api.github.com/users/delip/events{/privacy}", "received_events_url": "https://api.github.com/users/delip/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-24T16:03:23Z", "updated_at": "2015-11-24T16:03:23Z", "author_association": "NONE", "body_html": "<p>OP: it might help if you can paste your entire code in a gist and report it<br>\nhere.</p>\n<p>On Tuesday, November 24, 2015, Vincent Vanhoucke <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>One thing you can do is run with a tiny learning rate, or even zero<br>\nlearning rate. If you still have divergence then, you have a bug in your<br>\nsetup. If not, increase your rate slowly and see if there is a regime in<br>\nwhich things train without diverging. It's completely possible to have<br>\nweights that are in a good range, but activations or gradients going to<br>\ninfinity because of the shape of the loss, or too high a learning rate.<br>\nIt's obviously always a possibility that there is a bug in the optimizers,<br>\nbut in my experience, every single instance of this kind of problem could<br>\nbe traced back to a weirdly wired model, learning rate issues, bad<br>\nrandomization of the input examples, or - in the case of Adam or RMSProp -<br>\nissues with the epsilon value.</p>\n<p>\u2014<br>\nReply to this email directly or view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"118251717\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/323\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/323/hovercard?comment_id=159294978&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/323#issuecomment-159294978\">#323 (comment)</a><br>\n.</p>\n</blockquote>", "body_text": "OP: it might help if you can paste your entire code in a gist and report it\nhere.\nOn Tuesday, November 24, 2015, Vincent Vanhoucke notifications@github.com\nwrote:\n\nOne thing you can do is run with a tiny learning rate, or even zero\nlearning rate. If you still have divergence then, you have a bug in your\nsetup. If not, increase your rate slowly and see if there is a regime in\nwhich things train without diverging. It's completely possible to have\nweights that are in a good range, but activations or gradients going to\ninfinity because of the shape of the loss, or too high a learning rate.\nIt's obviously always a possibility that there is a bug in the optimizers,\nbut in my experience, every single instance of this kind of problem could\nbe traced back to a weirdly wired model, learning rate issues, bad\nrandomization of the input examples, or - in the case of Adam or RMSProp -\nissues with the epsilon value.\n\u2014\nReply to this email directly or view it on GitHub\n#323 (comment)\n.", "body": "OP: it might help if you can paste your entire code in a gist and report it\nhere.\n\nOn Tuesday, November 24, 2015, Vincent Vanhoucke notifications@github.com\nwrote:\n\n> One thing you can do is run with a tiny learning rate, or even zero\n> learning rate. If you still have divergence then, you have a bug in your\n> setup. If not, increase your rate slowly and see if there is a regime in\n> which things train without diverging. It's completely possible to have\n> weights that are in a good range, but activations or gradients going to\n> infinity because of the shape of the loss, or too high a learning rate.\n> It's obviously always a possibility that there is a bug in the optimizers,\n> but in my experience, every single instance of this kind of problem could\n> be traced back to a weirdly wired model, learning rate issues, bad\n> randomization of the input examples, or - in the case of Adam or RMSProp -\n> issues with the epsilon value.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/323#issuecomment-159294978\n> .\n"}