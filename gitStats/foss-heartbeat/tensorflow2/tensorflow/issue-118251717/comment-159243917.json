{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/159243917", "html_url": "https://github.com/tensorflow/tensorflow/issues/323#issuecomment-159243917", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/323", "id": 159243917, "node_id": "MDEyOklzc3VlQ29tbWVudDE1OTI0MzkxNw==", "user": {"login": "jpiabrantes", "id": 2369107, "node_id": "MDQ6VXNlcjIzNjkxMDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2369107?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jpiabrantes", "html_url": "https://github.com/jpiabrantes", "followers_url": "https://api.github.com/users/jpiabrantes/followers", "following_url": "https://api.github.com/users/jpiabrantes/following{/other_user}", "gists_url": "https://api.github.com/users/jpiabrantes/gists{/gist_id}", "starred_url": "https://api.github.com/users/jpiabrantes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jpiabrantes/subscriptions", "organizations_url": "https://api.github.com/users/jpiabrantes/orgs", "repos_url": "https://api.github.com/users/jpiabrantes/repos", "events_url": "https://api.github.com/users/jpiabrantes/events{/privacy}", "received_events_url": "https://api.github.com/users/jpiabrantes/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-24T11:52:41Z", "updated_at": "2015-11-24T12:02:23Z", "author_association": "NONE", "body_html": "<p>I am getting the same error on my Neural Network. I get this error when using <code>AdagradOptimizer</code>, <code>AdamOptimizer</code>, <code>MomentumOptimizer</code> and even <code>GradientDescentOptimizer</code>. The error happens after the first epoch so the algorithm was already trained with each minibatch at least once. The function I am minimizing is the Root Mean Square error of the output of the networks with the labels.</p>\n<p>I am logging the weights max and min and they are between -1 and 1 until the error is raised. If I use a sigmoid activation function the weights will be between -1 and 1 until they get nan values. I have no idea of why this is happening, how can I debug it?</p>", "body_text": "I am getting the same error on my Neural Network. I get this error when using AdagradOptimizer, AdamOptimizer, MomentumOptimizer and even GradientDescentOptimizer. The error happens after the first epoch so the algorithm was already trained with each minibatch at least once. The function I am minimizing is the Root Mean Square error of the output of the networks with the labels.\nI am logging the weights max and min and they are between -1 and 1 until the error is raised. If I use a sigmoid activation function the weights will be between -1 and 1 until they get nan values. I have no idea of why this is happening, how can I debug it?", "body": "I am getting the same error on my Neural Network. I get this error when using `AdagradOptimizer`, `AdamOptimizer`, `MomentumOptimizer` and even `GradientDescentOptimizer`. The error happens after the first epoch so the algorithm was already trained with each minibatch at least once. The function I am minimizing is the Root Mean Square error of the output of the networks with the labels. \n\nI am logging the weights max and min and they are between -1 and 1 until the error is raised. If I use a sigmoid activation function the weights will be between -1 and 1 until they get nan values. I have no idea of why this is happening, how can I debug it?\n"}