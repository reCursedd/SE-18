{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/161507230", "html_url": "https://github.com/tensorflow/tensorflow/issues/323#issuecomment-161507230", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/323", "id": 161507230, "node_id": "MDEyOklzc3VlQ29tbWVudDE2MTUwNzIzMA==", "user": {"login": "vincentvanhoucke", "id": 15737127, "node_id": "MDQ6VXNlcjE1NzM3MTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/15737127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vincentvanhoucke", "html_url": "https://github.com/vincentvanhoucke", "followers_url": "https://api.github.com/users/vincentvanhoucke/followers", "following_url": "https://api.github.com/users/vincentvanhoucke/following{/other_user}", "gists_url": "https://api.github.com/users/vincentvanhoucke/gists{/gist_id}", "starred_url": "https://api.github.com/users/vincentvanhoucke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vincentvanhoucke/subscriptions", "organizations_url": "https://api.github.com/users/vincentvanhoucke/orgs", "repos_url": "https://api.github.com/users/vincentvanhoucke/repos", "events_url": "https://api.github.com/users/vincentvanhoucke/events{/privacy}", "received_events_url": "https://api.github.com/users/vincentvanhoucke/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-03T03:47:13Z", "updated_at": "2015-12-03T03:47:13Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1769590\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/navraj28\">@navraj28</a> diverging when you use SGD can happen for many reasons:</p>\n<ul>\n<li>your learning rate is too high. Note that the right learning rate can be data dependent.</li>\n<li>your input data is not well randomized, or highly redundant, which could happen if you're just feeding the same data with few distortions.<br>\nThis bug is about the AdamOptimizer. I am going to close it. Feel free to reopen if you have new evidence of a bug.</li>\n</ul>", "body_text": "@navraj28 diverging when you use SGD can happen for many reasons:\n\nyour learning rate is too high. Note that the right learning rate can be data dependent.\nyour input data is not well randomized, or highly redundant, which could happen if you're just feeding the same data with few distortions.\nThis bug is about the AdamOptimizer. I am going to close it. Feel free to reopen if you have new evidence of a bug.", "body": "@navraj28 diverging when you use SGD can happen for many reasons:\n- your learning rate is too high. Note that the right learning rate can be data dependent.\n- your input data is not well randomized, or highly redundant, which could happen if you're just feeding the same data with few distortions.\n  This bug is about the AdamOptimizer. I am going to close it. Feel free to reopen if you have new evidence of a bug.\n"}