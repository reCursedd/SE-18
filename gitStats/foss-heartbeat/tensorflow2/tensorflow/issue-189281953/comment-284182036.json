{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284182036", "html_url": "https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-284182036", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5609", "id": 284182036, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDE4MjAzNg==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-04T21:18:12Z", "updated_at": "2017-03-04T21:18:12Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">A simpler solution (not requiring TensorArray) is probably to create a\nqueue that you feed sequences into one at a time in a separate python\nthread - the \"reader\"; and then in the main thread you read from that queue\nand pass that to bucket_by_sequence_length.  I'll try to include that in\nthe new tutorial.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sat, Mar 4, 2017 at 3:38 AM, Andreas Madsen ***@***.***&gt; wrote:\n I finally got bucket_by_sequence_length it to work, here is what I think\n was had:\n\n    - It is not clear that bucket_by_sequence_length needs input_length\n    and tensors to be elements from a queue.\n    - It is not clear that input_length just controls the bucketing, the\n    padding works separately.\n    - Creating a queue that contains tensors of different sequence lengths\n    is not trivial.\n\n Here is the example I got to work:\n\n import numpy as npimport tensorflow as tf\n\n class SequenceTable:\n     def __init__(self, data):\n         # A TensorArray is required as the sequences don't have the same\n         # length. Alternatively a FIFO query can be used.\n         # Because the data is read more than once by the queue,\n         # clear_after_read is set to False (but I can't confirm an effect).\n         # Because the items has diffrent sequence lengths the infer_shape\n         # is set to False. The shape is then restored in the .read method.\n         self.table = tf.TensorArray(size=len(data),\n                                     dtype=data[0].dtype,\n                                     dynamic_size=False,\n                                     clear_after_read=False,\n                                     infer_shape=False)\n\n         # initialize table\n         for i, datum in enumerate(data):\n             self.table = self.table.write(i, datum)\n\n         # setup infered element shape\n         self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])\n\n     def read(self, index):\n         # read index from table and set infered shape\n         read = self.table.read(index)\n         read.set_shape(self.element_shape)\n         return read\n\n def shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):\n     # bucket_by_sequence_length requires the input_length and tensors\n     # arguments to be queues. Use a range_input_producer queue to shuffle\n     # an index for sliceing the input_length and tensors laters.\n     # This strategy is idendical to the one used in slice_input_producer.\n     table_index = tf.train.range_input_producer(\n         int(input_length.get_shape()[0]), shuffle=shuffle\n     ).dequeue()\n\n     # the first argument is the sequence length specifed in the input_length\n     # I did not find a ue for it.\n     _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(\n         input_length=tf.gather(input_length, table_index),\n         tensors=[tensor.read(table_index) for tensor in tensors],\n         **kwargs\n     )\n\n     return tuple(batch_tensors)\n\n # these values specify the length of the sequence and this controls how# the data is bucketed. The value is not required to be the acutal length,# which is also problematic when using pairs of sequences that have diffrent# length. In that case just specify a value that gives the best performance,# for example \"the max length\".\n length_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)\n\n source_table = SequenceTable([\n     np.asarray([3, 4], dtype=np.int32),\n     np.asarray([2, 3, 4], dtype=np.int32),\n     np.asarray([1, 3, 4], dtype=np.int32),\n     np.asarray([5, 3, 4], dtype=np.int32),\n     np.asarray([6, 3, 4], dtype=np.int32),\n     np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)\n ])\n\n target_table = SequenceTable([\n     np.asarray([9], dtype=np.int32),\n     np.asarray([9, 3, 4, 5], dtype=np.int32),\n     np.asarray([9, 3, 4], dtype=np.int32),\n     np.asarray([9, 3, 4, 6], dtype=np.int32),\n     np.asarray([9, 3], dtype=np.int32),\n     np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)\n ])\n\n source_batch, target_batch = shuffle_bucket_batch(\n     length_table, [source_table, target_table],\n     batch_size=2,\n     # devices buckets into [len &lt; 3, 3 &lt;= len &lt; 5, 5 &lt;= len]\n     bucket_boundaries=[3, 5],\n     # this will bad the source_batch and target_batch independently\n     dynamic_pad=True,\n     capacity=2\n )\n with tf.Session() as sess:\n     coord = tf.train.Coordinator()\n     threads = tf.train.start_queue_runners(sess, coord)\n\n     for i in range(6):\n         source, target = sess.run((source_batch, target_batch))\n         print(f'source_output[{i}]')\n         print(source)\n         print(f'target_output[{i}]')\n         print(target)\n         print('')\n\n     coord.request_stop()\n     coord.join(threads)\n\n This outputs something like:\n\n source_output[0]\n [[6 3 4]\n  [5 3 4]]\n target_output[0]\n [[9 3 0 0]\n  [9 3 4 6]]\n\n source_output[1]\n [[1 3 4]\n  [2 3 4]]\n target_output[1]\n [[9 3 4 0]\n  [9 3 4 5]]\n\n source_output[2]\n [[6 3 4]\n  [2 3 4]]\n target_output[2]\n [[9 3 0 0]\n  [9 3 4 5]]\n\n source_output[3]\n [[3 3 3 3 3 3]\n  [3 3 3 3 3 3]]\n target_output[3]\n [[9 3 3 3 3 3 2]\n  [9 3 3 3 3 3 2]]\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"189281953\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5609\" href=\"https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-284146137\">#5609 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim0qiVXzQWLANQo7XSsHobK_v_6hXks5riU09gaJpZM4KyDNA\">https://github.com/notifications/unsubscribe-auth/ABtim0qiVXzQWLANQo7XSsHobK_v_6hXks5riU09gaJpZM4KyDNA</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "A simpler solution (not requiring TensorArray) is probably to create a\nqueue that you feed sequences into one at a time in a separate python\nthread - the \"reader\"; and then in the main thread you read from that queue\nand pass that to bucket_by_sequence_length.  I'll try to include that in\nthe new tutorial.\n\u2026\nOn Sat, Mar 4, 2017 at 3:38 AM, Andreas Madsen ***@***.***> wrote:\n I finally got bucket_by_sequence_length it to work, here is what I think\n was had:\n\n    - It is not clear that bucket_by_sequence_length needs input_length\n    and tensors to be elements from a queue.\n    - It is not clear that input_length just controls the bucketing, the\n    padding works separately.\n    - Creating a queue that contains tensors of different sequence lengths\n    is not trivial.\n\n Here is the example I got to work:\n\n import numpy as npimport tensorflow as tf\n\n class SequenceTable:\n     def __init__(self, data):\n         # A TensorArray is required as the sequences don't have the same\n         # length. Alternatively a FIFO query can be used.\n         # Because the data is read more than once by the queue,\n         # clear_after_read is set to False (but I can't confirm an effect).\n         # Because the items has diffrent sequence lengths the infer_shape\n         # is set to False. The shape is then restored in the .read method.\n         self.table = tf.TensorArray(size=len(data),\n                                     dtype=data[0].dtype,\n                                     dynamic_size=False,\n                                     clear_after_read=False,\n                                     infer_shape=False)\n\n         # initialize table\n         for i, datum in enumerate(data):\n             self.table = self.table.write(i, datum)\n\n         # setup infered element shape\n         self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])\n\n     def read(self, index):\n         # read index from table and set infered shape\n         read = self.table.read(index)\n         read.set_shape(self.element_shape)\n         return read\n\n def shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):\n     # bucket_by_sequence_length requires the input_length and tensors\n     # arguments to be queues. Use a range_input_producer queue to shuffle\n     # an index for sliceing the input_length and tensors laters.\n     # This strategy is idendical to the one used in slice_input_producer.\n     table_index = tf.train.range_input_producer(\n         int(input_length.get_shape()[0]), shuffle=shuffle\n     ).dequeue()\n\n     # the first argument is the sequence length specifed in the input_length\n     # I did not find a ue for it.\n     _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(\n         input_length=tf.gather(input_length, table_index),\n         tensors=[tensor.read(table_index) for tensor in tensors],\n         **kwargs\n     )\n\n     return tuple(batch_tensors)\n\n # these values specify the length of the sequence and this controls how# the data is bucketed. The value is not required to be the acutal length,# which is also problematic when using pairs of sequences that have diffrent# length. In that case just specify a value that gives the best performance,# for example \"the max length\".\n length_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)\n\n source_table = SequenceTable([\n     np.asarray([3, 4], dtype=np.int32),\n     np.asarray([2, 3, 4], dtype=np.int32),\n     np.asarray([1, 3, 4], dtype=np.int32),\n     np.asarray([5, 3, 4], dtype=np.int32),\n     np.asarray([6, 3, 4], dtype=np.int32),\n     np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)\n ])\n\n target_table = SequenceTable([\n     np.asarray([9], dtype=np.int32),\n     np.asarray([9, 3, 4, 5], dtype=np.int32),\n     np.asarray([9, 3, 4], dtype=np.int32),\n     np.asarray([9, 3, 4, 6], dtype=np.int32),\n     np.asarray([9, 3], dtype=np.int32),\n     np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)\n ])\n\n source_batch, target_batch = shuffle_bucket_batch(\n     length_table, [source_table, target_table],\n     batch_size=2,\n     # devices buckets into [len < 3, 3 <= len < 5, 5 <= len]\n     bucket_boundaries=[3, 5],\n     # this will bad the source_batch and target_batch independently\n     dynamic_pad=True,\n     capacity=2\n )\n with tf.Session() as sess:\n     coord = tf.train.Coordinator()\n     threads = tf.train.start_queue_runners(sess, coord)\n\n     for i in range(6):\n         source, target = sess.run((source_batch, target_batch))\n         print(f'source_output[{i}]')\n         print(source)\n         print(f'target_output[{i}]')\n         print(target)\n         print('')\n\n     coord.request_stop()\n     coord.join(threads)\n\n This outputs something like:\n\n source_output[0]\n [[6 3 4]\n  [5 3 4]]\n target_output[0]\n [[9 3 0 0]\n  [9 3 4 6]]\n\n source_output[1]\n [[1 3 4]\n  [2 3 4]]\n target_output[1]\n [[9 3 4 0]\n  [9 3 4 5]]\n\n source_output[2]\n [[6 3 4]\n  [2 3 4]]\n target_output[2]\n [[9 3 0 0]\n  [9 3 4 5]]\n\n source_output[3]\n [[3 3 3 3 3 3]\n  [3 3 3 3 3 3]]\n target_output[3]\n [[9 3 3 3 3 3 2]\n  [9 3 3 3 3 3 2]]\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#5609 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim0qiVXzQWLANQo7XSsHobK_v_6hXks5riU09gaJpZM4KyDNA>\n .", "body": "A simpler solution (not requiring TensorArray) is probably to create a\nqueue that you feed sequences into one at a time in a separate python\nthread - the \"reader\"; and then in the main thread you read from that queue\nand pass that to bucket_by_sequence_length.  I'll try to include that in\nthe new tutorial.\n\nOn Sat, Mar 4, 2017 at 3:38 AM, Andreas Madsen <notifications@github.com>\nwrote:\n\n> I finally got bucket_by_sequence_length it to work, here is what I think\n> was had:\n>\n>    - It is not clear that bucket_by_sequence_length needs input_length\n>    and tensors to be elements from a queue.\n>    - It is not clear that input_length just controls the bucketing, the\n>    padding works separately.\n>    - Creating a queue that contains tensors of different sequence lengths\n>    is not trivial.\n>\n> Here is the example I got to work:\n>\n> import numpy as npimport tensorflow as tf\n>\n> class SequenceTable:\n>     def __init__(self, data):\n>         # A TensorArray is required as the sequences don't have the same\n>         # length. Alternatively a FIFO query can be used.\n>         # Because the data is read more than once by the queue,\n>         # clear_after_read is set to False (but I can't confirm an effect).\n>         # Because the items has diffrent sequence lengths the infer_shape\n>         # is set to False. The shape is then restored in the .read method.\n>         self.table = tf.TensorArray(size=len(data),\n>                                     dtype=data[0].dtype,\n>                                     dynamic_size=False,\n>                                     clear_after_read=False,\n>                                     infer_shape=False)\n>\n>         # initialize table\n>         for i, datum in enumerate(data):\n>             self.table = self.table.write(i, datum)\n>\n>         # setup infered element shape\n>         self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])\n>\n>     def read(self, index):\n>         # read index from table and set infered shape\n>         read = self.table.read(index)\n>         read.set_shape(self.element_shape)\n>         return read\n>\n> def shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):\n>     # bucket_by_sequence_length requires the input_length and tensors\n>     # arguments to be queues. Use a range_input_producer queue to shuffle\n>     # an index for sliceing the input_length and tensors laters.\n>     # This strategy is idendical to the one used in slice_input_producer.\n>     table_index = tf.train.range_input_producer(\n>         int(input_length.get_shape()[0]), shuffle=shuffle\n>     ).dequeue()\n>\n>     # the first argument is the sequence length specifed in the input_length\n>     # I did not find a ue for it.\n>     _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(\n>         input_length=tf.gather(input_length, table_index),\n>         tensors=[tensor.read(table_index) for tensor in tensors],\n>         **kwargs\n>     )\n>\n>     return tuple(batch_tensors)\n>\n> # these values specify the length of the sequence and this controls how# the data is bucketed. The value is not required to be the acutal length,# which is also problematic when using pairs of sequences that have diffrent# length. In that case just specify a value that gives the best performance,# for example \"the max length\".\n> length_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)\n>\n> source_table = SequenceTable([\n>     np.asarray([3, 4], dtype=np.int32),\n>     np.asarray([2, 3, 4], dtype=np.int32),\n>     np.asarray([1, 3, 4], dtype=np.int32),\n>     np.asarray([5, 3, 4], dtype=np.int32),\n>     np.asarray([6, 3, 4], dtype=np.int32),\n>     np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)\n> ])\n>\n> target_table = SequenceTable([\n>     np.asarray([9], dtype=np.int32),\n>     np.asarray([9, 3, 4, 5], dtype=np.int32),\n>     np.asarray([9, 3, 4], dtype=np.int32),\n>     np.asarray([9, 3, 4, 6], dtype=np.int32),\n>     np.asarray([9, 3], dtype=np.int32),\n>     np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)\n> ])\n>\n> source_batch, target_batch = shuffle_bucket_batch(\n>     length_table, [source_table, target_table],\n>     batch_size=2,\n>     # devices buckets into [len < 3, 3 <= len < 5, 5 <= len]\n>     bucket_boundaries=[3, 5],\n>     # this will bad the source_batch and target_batch independently\n>     dynamic_pad=True,\n>     capacity=2\n> )\n> with tf.Session() as sess:\n>     coord = tf.train.Coordinator()\n>     threads = tf.train.start_queue_runners(sess, coord)\n>\n>     for i in range(6):\n>         source, target = sess.run((source_batch, target_batch))\n>         print(f'source_output[{i}]')\n>         print(source)\n>         print(f'target_output[{i}]')\n>         print(target)\n>         print('')\n>\n>     coord.request_stop()\n>     coord.join(threads)\n>\n> This outputs something like:\n>\n> source_output[0]\n> [[6 3 4]\n>  [5 3 4]]\n> target_output[0]\n> [[9 3 0 0]\n>  [9 3 4 6]]\n>\n> source_output[1]\n> [[1 3 4]\n>  [2 3 4]]\n> target_output[1]\n> [[9 3 4 0]\n>  [9 3 4 5]]\n>\n> source_output[2]\n> [[6 3 4]\n>  [2 3 4]]\n> target_output[2]\n> [[9 3 0 0]\n>  [9 3 4 5]]\n>\n> source_output[3]\n> [[3 3 3 3 3 3]\n>  [3 3 3 3 3 3]]\n> target_output[3]\n> [[9 3 3 3 3 3 2]\n>  [9 3 3 3 3 3 2]]\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-284146137>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0qiVXzQWLANQo7XSsHobK_v_6hXks5riU09gaJpZM4KyDNA>\n> .\n>\n"}