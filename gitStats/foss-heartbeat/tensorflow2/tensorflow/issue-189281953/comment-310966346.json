{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/310966346", "html_url": "https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-310966346", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5609", "id": 310966346, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDk2NjM0Ng==", "user": {"login": "randomrandom", "id": 1579822, "node_id": "MDQ6VXNlcjE1Nzk4MjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1579822?v=4", "gravatar_id": "", "url": "https://api.github.com/users/randomrandom", "html_url": "https://github.com/randomrandom", "followers_url": "https://api.github.com/users/randomrandom/followers", "following_url": "https://api.github.com/users/randomrandom/following{/other_user}", "gists_url": "https://api.github.com/users/randomrandom/gists{/gist_id}", "starred_url": "https://api.github.com/users/randomrandom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/randomrandom/subscriptions", "organizations_url": "https://api.github.com/users/randomrandom/orgs", "repos_url": "https://api.github.com/users/randomrandom/repos", "events_url": "https://api.github.com/users/randomrandom/events{/privacy}", "received_events_url": "https://api.github.com/users/randomrandom/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-26T05:23:05Z", "updated_at": "2017-06-26T05:23:05Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4138767\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sonalgupta\">@sonalgupta</a> I've decided to open source an Atrous CNN architecture for text classification, there I have implemented the sequence with variable length reading from a file, you can see the project <a href=\"https://github.com/randomrandom/deep-atrous-cnn-sentiment\">here</a>, the code that you need is located in the <a href=\"https://github.com/randomrandom/deep-atrous-cnn-sentiment/blob/master/data/base_data_loader.py\">BaseDataLoader class</a>. This is the actual snippet that you need:</p>\n<pre><code>def __load_batch(self, file_names, record_defaults, data_column, bucket_boundaries, field_delim=_CSV_DELIM,\n                     skip_header_lines=0,\n                     num_epochs=None, shuffle=True):\n\n        original_file_names = file_names[:]\n        file_names = self.__generate_preprocessed_files(file_names, data_column, bucket_boundaries,\n                                                        field_delim=field_delim)\n\n        filename_queue = tf.train.string_input_producer(\n            file_names, num_epochs=num_epochs, shuffle=shuffle\n        )\n\n        self.shuffle_queue = tf.RandomShuffleQueue(capacity=self._capacity, min_after_dequeue=self._min_after_dequeue,\n                                                   dtypes=[tf.int64, tf.int32], shapes=None)\n\n\n        example, label = self._read_file(filename_queue, record_defaults, field_delim, skip_header_lines)\n\n        voca_path, voca_name = BaseDataLoader._split_file_to_path_and_name(\n            original_file_names[0])  # TODO: will be break with multiple filenames\n        voca_name = KagglePreprocessor.VOCABULARY_PREFIX + voca_name\n        self.__vocabulary_file = voca_path + voca_name\n\n        # load look up table that maps words to ids\n        self.table = tf.contrib.lookup.index_table_from_file(vocabulary_file=voca_path + voca_name,\n                                                             default_value=KagglePreprocessor.UNK_TOKEN_ID,\n                                                             num_oov_buckets=0)\n\n        # convert to tensor of strings\n        split_example = tf.string_split([example], \" \")\n\n        # determine lengths of sequences\n        line_number = split_example.indices[:, 0]\n        line_position = split_example.indices[:, 1]\n        lengths = (tf.segment_max(data=line_position,\n                                  segment_ids=line_number) + 1).sg_cast(dtype=tf.int32)\n\n        # convert sparse to dense\n        dense_example = tf.sparse_tensor_to_dense(split_example, default_value=\"\")\n        dense_example = self.table.lookup(dense_example)\n\n        # get the enqueue op to pass to a coordintor to be run\n        self.enqueue_op = self.shuffle_queue.enqueue([dense_example, label])\n        dense_example, label = self.shuffle_queue.dequeue()\n\n        # add queue to queue runner\n        self.qr = tf.train.QueueRunner(self.shuffle_queue, [self.enqueue_op] * self.num_threads)\n        tf.train.queue_runner.add_queue_runner(self.qr)\n\n        # reshape from &lt;unknown&gt; shape into proper form after dequeue from random shuffle queue\n        # this is needed so next queue can automatically infer the shape properly\n        dense_example = dense_example.sg_reshape(shape=[1, -1])\n        label = label.sg_reshape(shape=[1])\n\n        _, (padded_examples, label_examples) = tf.contrib.training.bucket_by_sequence_length(lengths,\n                                                                                             [dense_example, label],\n                                                                                             batch_size=self._batch_size,\n                                                                                             bucket_boundaries=bucket_boundaries,\n                                                                                             dynamic_pad=True,\n                                                                                             capacity=self._capacity,\n                                                                                             num_threads=self._num_threads)\n\n        # reshape shape into proper form after dequeue from bucket queue\n        padded_examples = padded_examples.sg_reshape(shape=[self._batch_size, -1])\n        label_examples = label_examples.sg_reshape(shape=[self._batch_size])\n\n        return padded_examples, label_examples\n</code></pre>\n<p>The above piece of code:</p>\n<ol>\n<li>picks a file name from a queue with file name</li>\n<li>reads single examples from the file via file reader</li>\n<li>loads vocabulary of words from a preprocessed file</li>\n<li>uses the vocabulary of words to turn the string tensor into tensor of ids</li>\n<li>puts the single examples into a RandomShuffleQueue which allows all the examples to be shuffled</li>\n<li>reads a single example from the RandomShuffleQueue and puts it to a bucket_by_sequence queue with dynamic padding</li>\n<li>and finally reads batches of examples from the bucket_by_sequence queue</li>\n</ol>\n<p>Hope this is helpful to you</p>", "body_text": "@sonalgupta I've decided to open source an Atrous CNN architecture for text classification, there I have implemented the sequence with variable length reading from a file, you can see the project here, the code that you need is located in the BaseDataLoader class. This is the actual snippet that you need:\ndef __load_batch(self, file_names, record_defaults, data_column, bucket_boundaries, field_delim=_CSV_DELIM,\n                     skip_header_lines=0,\n                     num_epochs=None, shuffle=True):\n\n        original_file_names = file_names[:]\n        file_names = self.__generate_preprocessed_files(file_names, data_column, bucket_boundaries,\n                                                        field_delim=field_delim)\n\n        filename_queue = tf.train.string_input_producer(\n            file_names, num_epochs=num_epochs, shuffle=shuffle\n        )\n\n        self.shuffle_queue = tf.RandomShuffleQueue(capacity=self._capacity, min_after_dequeue=self._min_after_dequeue,\n                                                   dtypes=[tf.int64, tf.int32], shapes=None)\n\n\n        example, label = self._read_file(filename_queue, record_defaults, field_delim, skip_header_lines)\n\n        voca_path, voca_name = BaseDataLoader._split_file_to_path_and_name(\n            original_file_names[0])  # TODO: will be break with multiple filenames\n        voca_name = KagglePreprocessor.VOCABULARY_PREFIX + voca_name\n        self.__vocabulary_file = voca_path + voca_name\n\n        # load look up table that maps words to ids\n        self.table = tf.contrib.lookup.index_table_from_file(vocabulary_file=voca_path + voca_name,\n                                                             default_value=KagglePreprocessor.UNK_TOKEN_ID,\n                                                             num_oov_buckets=0)\n\n        # convert to tensor of strings\n        split_example = tf.string_split([example], \" \")\n\n        # determine lengths of sequences\n        line_number = split_example.indices[:, 0]\n        line_position = split_example.indices[:, 1]\n        lengths = (tf.segment_max(data=line_position,\n                                  segment_ids=line_number) + 1).sg_cast(dtype=tf.int32)\n\n        # convert sparse to dense\n        dense_example = tf.sparse_tensor_to_dense(split_example, default_value=\"\")\n        dense_example = self.table.lookup(dense_example)\n\n        # get the enqueue op to pass to a coordintor to be run\n        self.enqueue_op = self.shuffle_queue.enqueue([dense_example, label])\n        dense_example, label = self.shuffle_queue.dequeue()\n\n        # add queue to queue runner\n        self.qr = tf.train.QueueRunner(self.shuffle_queue, [self.enqueue_op] * self.num_threads)\n        tf.train.queue_runner.add_queue_runner(self.qr)\n\n        # reshape from <unknown> shape into proper form after dequeue from random shuffle queue\n        # this is needed so next queue can automatically infer the shape properly\n        dense_example = dense_example.sg_reshape(shape=[1, -1])\n        label = label.sg_reshape(shape=[1])\n\n        _, (padded_examples, label_examples) = tf.contrib.training.bucket_by_sequence_length(lengths,\n                                                                                             [dense_example, label],\n                                                                                             batch_size=self._batch_size,\n                                                                                             bucket_boundaries=bucket_boundaries,\n                                                                                             dynamic_pad=True,\n                                                                                             capacity=self._capacity,\n                                                                                             num_threads=self._num_threads)\n\n        # reshape shape into proper form after dequeue from bucket queue\n        padded_examples = padded_examples.sg_reshape(shape=[self._batch_size, -1])\n        label_examples = label_examples.sg_reshape(shape=[self._batch_size])\n\n        return padded_examples, label_examples\n\nThe above piece of code:\n\npicks a file name from a queue with file name\nreads single examples from the file via file reader\nloads vocabulary of words from a preprocessed file\nuses the vocabulary of words to turn the string tensor into tensor of ids\nputs the single examples into a RandomShuffleQueue which allows all the examples to be shuffled\nreads a single example from the RandomShuffleQueue and puts it to a bucket_by_sequence queue with dynamic padding\nand finally reads batches of examples from the bucket_by_sequence queue\n\nHope this is helpful to you", "body": "@sonalgupta I've decided to open source an Atrous CNN architecture for text classification, there I have implemented the sequence with variable length reading from a file, you can see the project [here](https://github.com/randomrandom/deep-atrous-cnn-sentiment), the code that you need is located in the [BaseDataLoader class](https://github.com/randomrandom/deep-atrous-cnn-sentiment/blob/master/data/base_data_loader.py). This is the actual snippet that you need:\r\n\r\n```\r\ndef __load_batch(self, file_names, record_defaults, data_column, bucket_boundaries, field_delim=_CSV_DELIM,\r\n                     skip_header_lines=0,\r\n                     num_epochs=None, shuffle=True):\r\n\r\n        original_file_names = file_names[:]\r\n        file_names = self.__generate_preprocessed_files(file_names, data_column, bucket_boundaries,\r\n                                                        field_delim=field_delim)\r\n\r\n        filename_queue = tf.train.string_input_producer(\r\n            file_names, num_epochs=num_epochs, shuffle=shuffle\r\n        )\r\n\r\n        self.shuffle_queue = tf.RandomShuffleQueue(capacity=self._capacity, min_after_dequeue=self._min_after_dequeue,\r\n                                                   dtypes=[tf.int64, tf.int32], shapes=None)\r\n\r\n\r\n        example, label = self._read_file(filename_queue, record_defaults, field_delim, skip_header_lines)\r\n\r\n        voca_path, voca_name = BaseDataLoader._split_file_to_path_and_name(\r\n            original_file_names[0])  # TODO: will be break with multiple filenames\r\n        voca_name = KagglePreprocessor.VOCABULARY_PREFIX + voca_name\r\n        self.__vocabulary_file = voca_path + voca_name\r\n\r\n        # load look up table that maps words to ids\r\n        self.table = tf.contrib.lookup.index_table_from_file(vocabulary_file=voca_path + voca_name,\r\n                                                             default_value=KagglePreprocessor.UNK_TOKEN_ID,\r\n                                                             num_oov_buckets=0)\r\n\r\n        # convert to tensor of strings\r\n        split_example = tf.string_split([example], \" \")\r\n\r\n        # determine lengths of sequences\r\n        line_number = split_example.indices[:, 0]\r\n        line_position = split_example.indices[:, 1]\r\n        lengths = (tf.segment_max(data=line_position,\r\n                                  segment_ids=line_number) + 1).sg_cast(dtype=tf.int32)\r\n\r\n        # convert sparse to dense\r\n        dense_example = tf.sparse_tensor_to_dense(split_example, default_value=\"\")\r\n        dense_example = self.table.lookup(dense_example)\r\n\r\n        # get the enqueue op to pass to a coordintor to be run\r\n        self.enqueue_op = self.shuffle_queue.enqueue([dense_example, label])\r\n        dense_example, label = self.shuffle_queue.dequeue()\r\n\r\n        # add queue to queue runner\r\n        self.qr = tf.train.QueueRunner(self.shuffle_queue, [self.enqueue_op] * self.num_threads)\r\n        tf.train.queue_runner.add_queue_runner(self.qr)\r\n\r\n        # reshape from <unknown> shape into proper form after dequeue from random shuffle queue\r\n        # this is needed so next queue can automatically infer the shape properly\r\n        dense_example = dense_example.sg_reshape(shape=[1, -1])\r\n        label = label.sg_reshape(shape=[1])\r\n\r\n        _, (padded_examples, label_examples) = tf.contrib.training.bucket_by_sequence_length(lengths,\r\n                                                                                             [dense_example, label],\r\n                                                                                             batch_size=self._batch_size,\r\n                                                                                             bucket_boundaries=bucket_boundaries,\r\n                                                                                             dynamic_pad=True,\r\n                                                                                             capacity=self._capacity,\r\n                                                                                             num_threads=self._num_threads)\r\n\r\n        # reshape shape into proper form after dequeue from bucket queue\r\n        padded_examples = padded_examples.sg_reshape(shape=[self._batch_size, -1])\r\n        label_examples = label_examples.sg_reshape(shape=[self._batch_size])\r\n\r\n        return padded_examples, label_examples\r\n```\r\nThe above piece of code:\r\n1. picks a file name from a queue with file name\r\n1. reads single examples from the file via file reader\r\n1. loads vocabulary of words from a preprocessed file\r\n1. uses the vocabulary of words to turn the string tensor into tensor of ids\r\n1. puts the single examples into a RandomShuffleQueue which allows all the examples to be shuffled\r\n1. reads a single example from the RandomShuffleQueue and puts it to a bucket_by_sequence queue with dynamic padding\r\n1. and finally reads batches of examples from the bucket_by_sequence queue\r\n\r\nHope this is helpful to you"}