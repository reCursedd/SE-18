{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284146137", "html_url": "https://github.com/tensorflow/tensorflow/issues/5609#issuecomment-284146137", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5609", "id": 284146137, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDE0NjEzNw==", "user": {"login": "AndreasMadsen", "id": 505333, "node_id": "MDQ6VXNlcjUwNTMzMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/505333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreasMadsen", "html_url": "https://github.com/AndreasMadsen", "followers_url": "https://api.github.com/users/AndreasMadsen/followers", "following_url": "https://api.github.com/users/AndreasMadsen/following{/other_user}", "gists_url": "https://api.github.com/users/AndreasMadsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreasMadsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreasMadsen/subscriptions", "organizations_url": "https://api.github.com/users/AndreasMadsen/orgs", "repos_url": "https://api.github.com/users/AndreasMadsen/repos", "events_url": "https://api.github.com/users/AndreasMadsen/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreasMadsen/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-04T11:37:55Z", "updated_at": "2017-03-04T21:53:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I finally got <code>bucket_by_sequence_length</code> to work, here is what I think was hard:</p>\n<ul>\n<li>It is not clear that <code>bucket_by_sequence_length</code> needs <code>input_length</code> and <code>tensors</code> to be elements from a queue.</li>\n<li>It is not clear that <code>input_length</code> just controls the bucketing, the padding works separately.</li>\n<li>Creating a queue that contains tensors of different sequence lengths is not trivial.</li>\n</ul>\n<p>Here is the example I got to work:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">SequenceTable</span>:\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">data</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> A TensorArray is required as the sequences don't have the same</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> length. Alternatively a FIFOQueue can be used.</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Because the data is read more than once by the queue,</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> clear_after_read is set to False (but I can't confirm an effect).</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Because the items has diffrent sequence lengths the infer_shape</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> is set to False. The shape is then restored in the .read method.</span>\n        <span class=\"pl-c1\">self</span>.table <span class=\"pl-k\">=</span> tf.TensorArray(<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">len</span>(data),\n                                    <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>data[<span class=\"pl-c1\">0</span>].dtype,\n                                    <span class=\"pl-v\">dynamic_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                                    <span class=\"pl-v\">clear_after_read</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                                    <span class=\"pl-v\">infer_shape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> initialize table</span>\n        <span class=\"pl-k\">for</span> i, datum <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(data):\n            <span class=\"pl-c1\">self</span>.table <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.table.write(i, datum)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> setup infered element shape</span>\n        <span class=\"pl-c1\">self</span>.element_shape <span class=\"pl-k\">=</span> tf.TensorShape((<span class=\"pl-c1\">None</span>, ) <span class=\"pl-k\">+</span> data[<span class=\"pl-c1\">0</span>].shape[<span class=\"pl-c1\">1</span>:])\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">read</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">index</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> read index from table and set infered shape</span>\n        read <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.table.read(index)\n        read.set_shape(<span class=\"pl-c1\">self</span>.element_shape)\n        <span class=\"pl-k\">return</span> read\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">shuffle_bucket_batch</span>(<span class=\"pl-smi\">input_length</span>, <span class=\"pl-smi\">tensors</span>, <span class=\"pl-smi\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> bucket_by_sequence_length requires the input_length and tensors</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> arguments to be queues. Use a range_input_producer queue to shuffle</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> an index for sliceing the input_length and tensors laters.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> This strategy is idendical to the one used in slice_input_producer.</span>\n    table_index <span class=\"pl-k\">=</span> tf.train.range_input_producer(\n        <span class=\"pl-c1\">int</span>(input_length.get_shape()[<span class=\"pl-c1\">0</span>]), <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span>shuffle\n    ).dequeue()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> the first argument is the sequence length specifed in the input_length</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> I did not find a ue for it.</span>\n    _, batch_tensors <span class=\"pl-k\">=</span> tf.contrib.training.bucket_by_sequence_length(\n        <span class=\"pl-v\">input_length</span><span class=\"pl-k\">=</span>tf.gather(input_length, table_index),\n        <span class=\"pl-v\">tensors</span><span class=\"pl-k\">=</span>[tensor.read(table_index) <span class=\"pl-k\">for</span> tensor <span class=\"pl-k\">in</span> tensors],\n        <span class=\"pl-k\">**</span>kwargs\n    )\n\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">tuple</span>(batch_tensors)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> these values specify the length of the sequence and this controls how</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the data is bucketed. The value is not required to be the acutal length,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> which is also problematic when using pairs of sequences that have diffrent</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> length. In that case just specify a value that gives the best performance,</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> for example \"the max length\".</span>\nlength_table <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">7</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n\nsource_table <span class=\"pl-k\">=</span> SequenceTable([\n    np.asarray([<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32)\n])\n\ntarget_table <span class=\"pl-k\">=</span> SequenceTable([\n    np.asarray([<span class=\"pl-c1\">9</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">6</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32),\n    np.asarray([<span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.int32)\n])\n\nsource_batch, target_batch <span class=\"pl-k\">=</span> shuffle_bucket_batch(\n    length_table, [source_table, target_table],\n    <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>,\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> devices buckets into [len &lt; 3, 3 &lt;= len &lt; 5, 5 &lt;= len]</span>\n    <span class=\"pl-v\">bucket_boundaries</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>],\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> this will bad the source_batch and target_batch independently</span>\n    <span class=\"pl-v\">dynamic_pad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">capacity</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>\n)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    coord <span class=\"pl-k\">=</span> tf.train.Coordinator()\n    threads <span class=\"pl-k\">=</span> tf.train.start_queue_runners(sess, coord)\n\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">6</span>):\n        source, target <span class=\"pl-k\">=</span> sess.run((source_batch, target_batch))\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\">f</span><span class=\"pl-pds\">'</span><span class=\"pl-s\">source_output[</span><span class=\"pl-c1\">{</span>i<span class=\"pl-c1\">}</span><span class=\"pl-s\">]</span><span class=\"pl-pds\">'</span>)\n        <span class=\"pl-c1\">print</span>(source)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\">f</span><span class=\"pl-pds\">'</span><span class=\"pl-s\">target_output[</span><span class=\"pl-c1\">{</span>i<span class=\"pl-c1\">}</span><span class=\"pl-s\">]</span><span class=\"pl-pds\">'</span>)\n        <span class=\"pl-c1\">print</span>(target)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>)\n\n    coord.request_stop()\n    coord.join(threads)</pre></div>\n<p>This outputs something like:</p>\n<pre><code>source_output[0]\n[[6 3 4]\n [5 3 4]]\ntarget_output[0]\n[[9 3 0 0]\n [9 3 4 6]]\n\nsource_output[1]\n[[1 3 4]\n [2 3 4]]\ntarget_output[1]\n[[9 3 4 0]\n [9 3 4 5]]\n\nsource_output[2]\n[[6 3 4]\n [2 3 4]]\ntarget_output[2]\n[[9 3 0 0]\n [9 3 4 5]]\n\nsource_output[3]\n[[3 3 3 3 3 3]\n [3 3 3 3 3 3]]\ntarget_output[3]\n[[9 3 3 3 3 3 2]\n [9 3 3 3 3 3 2]]\n</code></pre>", "body_text": "I finally got bucket_by_sequence_length to work, here is what I think was hard:\n\nIt is not clear that bucket_by_sequence_length needs input_length and tensors to be elements from a queue.\nIt is not clear that input_length just controls the bucketing, the padding works separately.\nCreating a queue that contains tensors of different sequence lengths is not trivial.\n\nHere is the example I got to work:\nimport numpy as np\nimport tensorflow as tf\n\n\nclass SequenceTable:\n    def __init__(self, data):\n        # A TensorArray is required as the sequences don't have the same\n        # length. Alternatively a FIFOQueue can be used.\n        # Because the data is read more than once by the queue,\n        # clear_after_read is set to False (but I can't confirm an effect).\n        # Because the items has diffrent sequence lengths the infer_shape\n        # is set to False. The shape is then restored in the .read method.\n        self.table = tf.TensorArray(size=len(data),\n                                    dtype=data[0].dtype,\n                                    dynamic_size=False,\n                                    clear_after_read=False,\n                                    infer_shape=False)\n\n        # initialize table\n        for i, datum in enumerate(data):\n            self.table = self.table.write(i, datum)\n\n        # setup infered element shape\n        self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])\n\n    def read(self, index):\n        # read index from table and set infered shape\n        read = self.table.read(index)\n        read.set_shape(self.element_shape)\n        return read\n\n\ndef shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):\n    # bucket_by_sequence_length requires the input_length and tensors\n    # arguments to be queues. Use a range_input_producer queue to shuffle\n    # an index for sliceing the input_length and tensors laters.\n    # This strategy is idendical to the one used in slice_input_producer.\n    table_index = tf.train.range_input_producer(\n        int(input_length.get_shape()[0]), shuffle=shuffle\n    ).dequeue()\n\n    # the first argument is the sequence length specifed in the input_length\n    # I did not find a ue for it.\n    _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(\n        input_length=tf.gather(input_length, table_index),\n        tensors=[tensor.read(table_index) for tensor in tensors],\n        **kwargs\n    )\n\n    return tuple(batch_tensors)\n\n\n# these values specify the length of the sequence and this controls how\n# the data is bucketed. The value is not required to be the acutal length,\n# which is also problematic when using pairs of sequences that have diffrent\n# length. In that case just specify a value that gives the best performance,\n# for example \"the max length\".\nlength_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)\n\nsource_table = SequenceTable([\n    np.asarray([3, 4], dtype=np.int32),\n    np.asarray([2, 3, 4], dtype=np.int32),\n    np.asarray([1, 3, 4], dtype=np.int32),\n    np.asarray([5, 3, 4], dtype=np.int32),\n    np.asarray([6, 3, 4], dtype=np.int32),\n    np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)\n])\n\ntarget_table = SequenceTable([\n    np.asarray([9], dtype=np.int32),\n    np.asarray([9, 3, 4, 5], dtype=np.int32),\n    np.asarray([9, 3, 4], dtype=np.int32),\n    np.asarray([9, 3, 4, 6], dtype=np.int32),\n    np.asarray([9, 3], dtype=np.int32),\n    np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)\n])\n\nsource_batch, target_batch = shuffle_bucket_batch(\n    length_table, [source_table, target_table],\n    batch_size=2,\n    # devices buckets into [len < 3, 3 <= len < 5, 5 <= len]\n    bucket_boundaries=[3, 5],\n    # this will bad the source_batch and target_batch independently\n    dynamic_pad=True,\n    capacity=2\n)\n\nwith tf.Session() as sess:\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess, coord)\n\n    for i in range(6):\n        source, target = sess.run((source_batch, target_batch))\n        print(f'source_output[{i}]')\n        print(source)\n        print(f'target_output[{i}]')\n        print(target)\n        print('')\n\n    coord.request_stop()\n    coord.join(threads)\nThis outputs something like:\nsource_output[0]\n[[6 3 4]\n [5 3 4]]\ntarget_output[0]\n[[9 3 0 0]\n [9 3 4 6]]\n\nsource_output[1]\n[[1 3 4]\n [2 3 4]]\ntarget_output[1]\n[[9 3 4 0]\n [9 3 4 5]]\n\nsource_output[2]\n[[6 3 4]\n [2 3 4]]\ntarget_output[2]\n[[9 3 0 0]\n [9 3 4 5]]\n\nsource_output[3]\n[[3 3 3 3 3 3]\n [3 3 3 3 3 3]]\ntarget_output[3]\n[[9 3 3 3 3 3 2]\n [9 3 3 3 3 3 2]]", "body": "I finally got `bucket_by_sequence_length` to work, here is what I think was hard:\r\n\r\n* It is not clear that `bucket_by_sequence_length` needs `input_length` and `tensors` to be elements from a queue.\r\n* It is not clear that `input_length` just controls the bucketing, the padding works separately.\r\n* Creating a queue that contains tensors of different sequence lengths is not trivial.\r\n\r\nHere is the example I got to work:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass SequenceTable:\r\n    def __init__(self, data):\r\n        # A TensorArray is required as the sequences don't have the same\r\n        # length. Alternatively a FIFOQueue can be used.\r\n        # Because the data is read more than once by the queue,\r\n        # clear_after_read is set to False (but I can't confirm an effect).\r\n        # Because the items has diffrent sequence lengths the infer_shape\r\n        # is set to False. The shape is then restored in the .read method.\r\n        self.table = tf.TensorArray(size=len(data),\r\n                                    dtype=data[0].dtype,\r\n                                    dynamic_size=False,\r\n                                    clear_after_read=False,\r\n                                    infer_shape=False)\r\n\r\n        # initialize table\r\n        for i, datum in enumerate(data):\r\n            self.table = self.table.write(i, datum)\r\n\r\n        # setup infered element shape\r\n        self.element_shape = tf.TensorShape((None, ) + data[0].shape[1:])\r\n\r\n    def read(self, index):\r\n        # read index from table and set infered shape\r\n        read = self.table.read(index)\r\n        read.set_shape(self.element_shape)\r\n        return read\r\n\r\n\r\ndef shuffle_bucket_batch(input_length, tensors, shuffle=True, **kwargs):\r\n    # bucket_by_sequence_length requires the input_length and tensors\r\n    # arguments to be queues. Use a range_input_producer queue to shuffle\r\n    # an index for sliceing the input_length and tensors laters.\r\n    # This strategy is idendical to the one used in slice_input_producer.\r\n    table_index = tf.train.range_input_producer(\r\n        int(input_length.get_shape()[0]), shuffle=shuffle\r\n    ).dequeue()\r\n\r\n    # the first argument is the sequence length specifed in the input_length\r\n    # I did not find a ue for it.\r\n    _, batch_tensors = tf.contrib.training.bucket_by_sequence_length(\r\n        input_length=tf.gather(input_length, table_index),\r\n        tensors=[tensor.read(table_index) for tensor in tensors],\r\n        **kwargs\r\n    )\r\n\r\n    return tuple(batch_tensors)\r\n\r\n\r\n# these values specify the length of the sequence and this controls how\r\n# the data is bucketed. The value is not required to be the acutal length,\r\n# which is also problematic when using pairs of sequences that have diffrent\r\n# length. In that case just specify a value that gives the best performance,\r\n# for example \"the max length\".\r\nlength_table = tf.constant([2, 4, 3, 4, 3, 7], dtype=tf.int32)\r\n\r\nsource_table = SequenceTable([\r\n    np.asarray([3, 4], dtype=np.int32),\r\n    np.asarray([2, 3, 4], dtype=np.int32),\r\n    np.asarray([1, 3, 4], dtype=np.int32),\r\n    np.asarray([5, 3, 4], dtype=np.int32),\r\n    np.asarray([6, 3, 4], dtype=np.int32),\r\n    np.asarray([3, 3, 3, 3, 3, 3], dtype=np.int32)\r\n])\r\n\r\ntarget_table = SequenceTable([\r\n    np.asarray([9], dtype=np.int32),\r\n    np.asarray([9, 3, 4, 5], dtype=np.int32),\r\n    np.asarray([9, 3, 4], dtype=np.int32),\r\n    np.asarray([9, 3, 4, 6], dtype=np.int32),\r\n    np.asarray([9, 3], dtype=np.int32),\r\n    np.asarray([9, 3, 3, 3, 3, 3, 2], dtype=np.int32)\r\n])\r\n\r\nsource_batch, target_batch = shuffle_bucket_batch(\r\n    length_table, [source_table, target_table],\r\n    batch_size=2,\r\n    # devices buckets into [len < 3, 3 <= len < 5, 5 <= len]\r\n    bucket_boundaries=[3, 5],\r\n    # this will bad the source_batch and target_batch independently\r\n    dynamic_pad=True,\r\n    capacity=2\r\n)\r\n\r\nwith tf.Session() as sess:\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess, coord)\r\n\r\n    for i in range(6):\r\n        source, target = sess.run((source_batch, target_batch))\r\n        print(f'source_output[{i}]')\r\n        print(source)\r\n        print(f'target_output[{i}]')\r\n        print(target)\r\n        print('')\r\n\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n```\r\n\r\nThis outputs something like:\r\n\r\n```\r\nsource_output[0]\r\n[[6 3 4]\r\n [5 3 4]]\r\ntarget_output[0]\r\n[[9 3 0 0]\r\n [9 3 4 6]]\r\n\r\nsource_output[1]\r\n[[1 3 4]\r\n [2 3 4]]\r\ntarget_output[1]\r\n[[9 3 4 0]\r\n [9 3 4 5]]\r\n\r\nsource_output[2]\r\n[[6 3 4]\r\n [2 3 4]]\r\ntarget_output[2]\r\n[[9 3 0 0]\r\n [9 3 4 5]]\r\n\r\nsource_output[3]\r\n[[3 3 3 3 3 3]\r\n [3 3 3 3 3 3]]\r\ntarget_output[3]\r\n[[9 3 3 3 3 3 2]\r\n [9 3 3 3 3 3 2]]\r\n```"}