{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/384349990", "html_url": "https://github.com/tensorflow/tensorflow/issues/16781#issuecomment-384349990", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16781", "id": 384349990, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDM0OTk5MA==", "user": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-25T16:30:36Z", "updated_at": "2018-04-25T16:30:36Z", "author_association": "MEMBER", "body_html": "<p>As of TensorFlow 1.5 you can use <code>optimizer.variables()</code> to get a list of variables associated with the optimizer. For Adam this is its <code>m</code> and <code>v</code> slot variables along with <code>beta1_power</code> and <code>beta2_power</code>. Note that this only works after a training step has been built, since optimizers create these variables lazily.</p>\n<p>I think this works for the use-case you've described? You can use <code>.variables()</code> to filter the <code>var_list</code> passed to a <code>Saver</code> for restore, and you can use it to initialize the variables.</p>\n<pre><code>import tensorflow as tf\n\nv = tf.get_variable(name=\"v\", shape=[], use_resource=True)\noptimizer = tf.train.AdamOptimizer(0.001)\ntrain_op = optimizer.minimize(lambda: v.read_value())\nprint(\"All variables\", tf.global_variables())\nprint(\"Non-optimizer variables\",\n      set(tf.global_variables()) - set(optimizer.variables()))\n</code></pre>\n<p>Prints:</p>\n<pre><code>All variables [&lt;tf.Variable 'v:0' shape=() dtype=float32&gt;, &lt;tf.Variable 'beta1_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'beta2_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'v/Adam:0' shape=() dtype=float32&gt;, &lt;tf.Variable 'v/Adam_1:0' shape=() dtype=float32&gt;]\nNon-optimizer variables {&lt;tf.Variable 'v:0' shape=() dtype=float32&gt;}\n</code></pre>\n<p>This should work for any optimizer.</p>", "body_text": "As of TensorFlow 1.5 you can use optimizer.variables() to get a list of variables associated with the optimizer. For Adam this is its m and v slot variables along with beta1_power and beta2_power. Note that this only works after a training step has been built, since optimizers create these variables lazily.\nI think this works for the use-case you've described? You can use .variables() to filter the var_list passed to a Saver for restore, and you can use it to initialize the variables.\nimport tensorflow as tf\n\nv = tf.get_variable(name=\"v\", shape=[], use_resource=True)\noptimizer = tf.train.AdamOptimizer(0.001)\ntrain_op = optimizer.minimize(lambda: v.read_value())\nprint(\"All variables\", tf.global_variables())\nprint(\"Non-optimizer variables\",\n      set(tf.global_variables()) - set(optimizer.variables()))\n\nPrints:\nAll variables [<tf.Variable 'v:0' shape=() dtype=float32>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'v/Adam:0' shape=() dtype=float32>, <tf.Variable 'v/Adam_1:0' shape=() dtype=float32>]\nNon-optimizer variables {<tf.Variable 'v:0' shape=() dtype=float32>}\n\nThis should work for any optimizer.", "body": "As of TensorFlow 1.5 you can use `optimizer.variables()` to get a list of variables associated with the optimizer. For Adam this is its `m` and `v` slot variables along with `beta1_power` and `beta2_power`. Note that this only works after a training step has been built, since optimizers create these variables lazily.\r\n\r\nI think this works for the use-case you've described? You can use `.variables()` to filter the `var_list` passed to a `Saver` for restore, and you can use it to initialize the variables.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nv = tf.get_variable(name=\"v\", shape=[], use_resource=True)\r\noptimizer = tf.train.AdamOptimizer(0.001)\r\ntrain_op = optimizer.minimize(lambda: v.read_value())\r\nprint(\"All variables\", tf.global_variables())\r\nprint(\"Non-optimizer variables\",\r\n      set(tf.global_variables()) - set(optimizer.variables()))\r\n```\r\nPrints:\r\n```\r\nAll variables [<tf.Variable 'v:0' shape=() dtype=float32>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'v/Adam:0' shape=() dtype=float32>, <tf.Variable 'v/Adam_1:0' shape=() dtype=float32>]\r\nNon-optimizer variables {<tf.Variable 'v:0' shape=() dtype=float32>}\r\n```\r\nThis should work for any optimizer."}