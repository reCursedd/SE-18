{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16781", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16781/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16781/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16781/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16781", "id": 294537159, "node_id": "MDU6SXNzdWUyOTQ1MzcxNTk=", "number": 16781, "title": "Feature request: Save optimizer variables under separate name scope", "user": {"login": "tsoernes", "id": 6782404, "node_id": "MDQ6VXNlcjY3ODI0MDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/6782404?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tsoernes", "html_url": "https://github.com/tsoernes", "followers_url": "https://api.github.com/users/tsoernes/followers", "following_url": "https://api.github.com/users/tsoernes/following{/other_user}", "gists_url": "https://api.github.com/users/tsoernes/gists{/gist_id}", "starred_url": "https://api.github.com/users/tsoernes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tsoernes/subscriptions", "organizations_url": "https://api.github.com/users/tsoernes/orgs", "repos_url": "https://api.github.com/users/tsoernes/repos", "events_url": "https://api.github.com/users/tsoernes/events{/privacy}", "received_events_url": "https://api.github.com/users/tsoernes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2018-02-05T20:48:28Z", "updated_at": "2018-04-25T16:59:47Z", "closed_at": "2018-04-25T16:59:47Z", "author_association": "NONE", "body_html": "<p>Tensorflow version b'v1.3.0-rc1-3011-gd86448938' 1.3.0<br>\nHave I written custom code N/A<br>\nOS Platform and Distribution N/A<br>\nTensorFlow installed from N/A<br>\nBazel version N/A<br>\nCUDA/cuDNN version N/A<br>\nGPU model and memory N/A<br>\nExact command to reproduce N/A</p>\n<p>The variables initialized by the Adam optimizer is not created under its own name scope, but inherits the name of trainable variables, e.g. <code>model/q_networks/online/conv2d/bias/Adam_1</code> (the Adam optimizer was not creater under the name scopes <code>model</code>, <code>q_networks</code> or <code>online</code>).</p>\n<p>This creates an issue if you want to use another optimizer for a restored model, because you can't use the name scope to avoid restoring the variables related to the optimizer.</p>\n<p>Here's a workaround:</p>\n<pre><code>        build_model()  # Build model under namescope 'model', without optimizer\n        temp = set(tf.global_variables())\n        if save or restore:\n            saver = tf.train.Saver(\n                var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model'))\n        sess.run(tf.global_variables_initializer())\n        if restore:\n            saver.restore(sess, model_path)\n        trainer = tf.train.AdamOptimizer(1e-4)\n        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))  # Initialize Adam variables\n</code></pre>\n<p>It would be simpler (to figure out) if the optimizer variables had their own scope.</p>", "body_text": "Tensorflow version b'v1.3.0-rc1-3011-gd86448938' 1.3.0\nHave I written custom code N/A\nOS Platform and Distribution N/A\nTensorFlow installed from N/A\nBazel version N/A\nCUDA/cuDNN version N/A\nGPU model and memory N/A\nExact command to reproduce N/A\nThe variables initialized by the Adam optimizer is not created under its own name scope, but inherits the name of trainable variables, e.g. model/q_networks/online/conv2d/bias/Adam_1 (the Adam optimizer was not creater under the name scopes model, q_networks or online).\nThis creates an issue if you want to use another optimizer for a restored model, because you can't use the name scope to avoid restoring the variables related to the optimizer.\nHere's a workaround:\n        build_model()  # Build model under namescope 'model', without optimizer\n        temp = set(tf.global_variables())\n        if save or restore:\n            saver = tf.train.Saver(\n                var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model'))\n        sess.run(tf.global_variables_initializer())\n        if restore:\n            saver.restore(sess, model_path)\n        trainer = tf.train.AdamOptimizer(1e-4)\n        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))  # Initialize Adam variables\n\nIt would be simpler (to figure out) if the optimizer variables had their own scope.", "body": "Tensorflow version b'v1.3.0-rc1-3011-gd86448938' 1.3.0\r\nHave I written custom code N/A\r\nOS Platform and Distribution N/A \r\nTensorFlow installed from N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A\r\n\r\nThe variables initialized by the Adam optimizer is not created under its own name scope, but inherits the name of trainable variables, e.g. `model/q_networks/online/conv2d/bias/Adam_1` (the Adam optimizer was not creater under the name scopes `model`, `q_networks` or `online`).\r\n\r\nThis creates an issue if you want to use another optimizer for a restored model, because you can't use the name scope to avoid restoring the variables related to the optimizer.\r\n\r\nHere's a workaround:\r\n```\r\n        build_model()  # Build model under namescope 'model', without optimizer\r\n        temp = set(tf.global_variables())\r\n        if save or restore:\r\n            saver = tf.train.Saver(\r\n                var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model'))\r\n        sess.run(tf.global_variables_initializer())\r\n        if restore:\r\n            saver.restore(sess, model_path)\r\n        trainer = tf.train.AdamOptimizer(1e-4)\r\n        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))  # Initialize Adam variables\r\n```\r\nIt would be simpler (to figure out) if the optimizer variables had their own scope.\r\n\r\n"}