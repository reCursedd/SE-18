{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/384353595", "html_url": "https://github.com/tensorflow/tensorflow/issues/16781#issuecomment-384353595", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16781", "id": 384353595, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDM1MzU5NQ==", "user": {"login": "tsoernes", "id": 6782404, "node_id": "MDQ6VXNlcjY3ODI0MDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/6782404?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tsoernes", "html_url": "https://github.com/tsoernes", "followers_url": "https://api.github.com/users/tsoernes/followers", "following_url": "https://api.github.com/users/tsoernes/following{/other_user}", "gists_url": "https://api.github.com/users/tsoernes/gists{/gist_id}", "starred_url": "https://api.github.com/users/tsoernes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tsoernes/subscriptions", "organizations_url": "https://api.github.com/users/tsoernes/orgs", "repos_url": "https://api.github.com/users/tsoernes/repos", "events_url": "https://api.github.com/users/tsoernes/events{/privacy}", "received_events_url": "https://api.github.com/users/tsoernes/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-25T16:42:02Z", "updated_at": "2018-04-25T16:42:02Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Good enough for me. Thanks!\n\n2018-04-25 18:33 GMT+02:00 Allen Lavoie &lt;notifications@github.com&gt;:</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\"> As of TensorFlow 1.5 you can use optimizer.variables() to get a list of\n variables associated with the optimizer. For Adam this is its m and v\n slot variables along with beta1_power and beta2_power. Note that this\n only works after a training step has been built, since optimizers create\n these variables lazily.\n\n I think this works for the use-case you've described? You can use\n .variables() to filter the var_list passed to a Saver for restore, and\n you can use it to initialize the variables.\n\n import tensorflow as tf\n\n v = tf.get_variable(name=\"v\", shape=[], use_resource=True)\n optimizer = tf.train.AdamOptimizer(0.001)\n train_op = optimizer.minimize(lambda: v.read_value())\n print(\"All variables\", tf.global_variables())\n print(\"Non-optimizer variables\",\n       set(tf.global_variables()) - set(optimizer.variables()))\n\n Prints:\n\n All variables [&lt;tf.Variable 'v:0' shape=() dtype=float32&gt;, &lt;tf.Variable 'beta1_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'beta2_power:0' shape=() dtype=float32_ref&gt;, &lt;tf.Variable 'v/Adam:0' shape=() dtype=float32&gt;, &lt;tf.Variable 'v/Adam_1:0' shape=() dtype=float32&gt;]\n Non-optimizer variables {&lt;tf.Variable 'v:0' shape=() dtype=float32&gt;}\n\n This should work for any optimizer.\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"294537159\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/16781\" href=\"https://github.com/tensorflow/tensorflow/issues/16781#issuecomment-384349990\">#16781 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AGd9xEOq--B56Lkq3HNf5-wZk2snYBnNks5tsKVugaJpZM4R6E3o\">https://github.com/notifications/unsubscribe-auth/AGd9xEOq--B56Lkq3HNf5-wZk2snYBnNks5tsKVugaJpZM4R6E3o</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Good enough for me. Thanks!\n\n2018-04-25 18:33 GMT+02:00 Allen Lavoie <notifications@github.com>:\n\u2026\n As of TensorFlow 1.5 you can use optimizer.variables() to get a list of\n variables associated with the optimizer. For Adam this is its m and v\n slot variables along with beta1_power and beta2_power. Note that this\n only works after a training step has been built, since optimizers create\n these variables lazily.\n\n I think this works for the use-case you've described? You can use\n .variables() to filter the var_list passed to a Saver for restore, and\n you can use it to initialize the variables.\n\n import tensorflow as tf\n\n v = tf.get_variable(name=\"v\", shape=[], use_resource=True)\n optimizer = tf.train.AdamOptimizer(0.001)\n train_op = optimizer.minimize(lambda: v.read_value())\n print(\"All variables\", tf.global_variables())\n print(\"Non-optimizer variables\",\n       set(tf.global_variables()) - set(optimizer.variables()))\n\n Prints:\n\n All variables [<tf.Variable 'v:0' shape=() dtype=float32>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'v/Adam:0' shape=() dtype=float32>, <tf.Variable 'v/Adam_1:0' shape=() dtype=float32>]\n Non-optimizer variables {<tf.Variable 'v:0' shape=() dtype=float32>}\n\n This should work for any optimizer.\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#16781 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AGd9xEOq--B56Lkq3HNf5-wZk2snYBnNks5tsKVugaJpZM4R6E3o>\n .", "body": "Good enough for me. Thanks!\n\n2018-04-25 18:33 GMT+02:00 Allen Lavoie <notifications@github.com>:\n\n> As of TensorFlow 1.5 you can use optimizer.variables() to get a list of\n> variables associated with the optimizer. For Adam this is its m and v\n> slot variables along with beta1_power and beta2_power. Note that this\n> only works after a training step has been built, since optimizers create\n> these variables lazily.\n>\n> I think this works for the use-case you've described? You can use\n> .variables() to filter the var_list passed to a Saver for restore, and\n> you can use it to initialize the variables.\n>\n> import tensorflow as tf\n>\n> v = tf.get_variable(name=\"v\", shape=[], use_resource=True)\n> optimizer = tf.train.AdamOptimizer(0.001)\n> train_op = optimizer.minimize(lambda: v.read_value())\n> print(\"All variables\", tf.global_variables())\n> print(\"Non-optimizer variables\",\n>       set(tf.global_variables()) - set(optimizer.variables()))\n>\n> Prints:\n>\n> All variables [<tf.Variable 'v:0' shape=() dtype=float32>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'v/Adam:0' shape=() dtype=float32>, <tf.Variable 'v/Adam_1:0' shape=() dtype=float32>]\n> Non-optimizer variables {<tf.Variable 'v:0' shape=() dtype=float32>}\n>\n> This should work for any optimizer.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16781#issuecomment-384349990>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGd9xEOq--B56Lkq3HNf5-wZk2snYBnNks5tsKVugaJpZM4R6E3o>\n> .\n>\n"}