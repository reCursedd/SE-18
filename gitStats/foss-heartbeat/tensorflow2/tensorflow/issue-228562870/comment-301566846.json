{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/301566846", "html_url": "https://github.com/tensorflow/tensorflow/issues/9895#issuecomment-301566846", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9895", "id": 301566846, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTU2Njg0Ng==", "user": {"login": "thomasquintana", "id": 1891840, "node_id": "MDQ6VXNlcjE4OTE4NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1891840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomasquintana", "html_url": "https://github.com/thomasquintana", "followers_url": "https://api.github.com/users/thomasquintana/followers", "following_url": "https://api.github.com/users/thomasquintana/following{/other_user}", "gists_url": "https://api.github.com/users/thomasquintana/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomasquintana/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomasquintana/subscriptions", "organizations_url": "https://api.github.com/users/thomasquintana/orgs", "repos_url": "https://api.github.com/users/thomasquintana/repos", "events_url": "https://api.github.com/users/thomasquintana/events{/privacy}", "received_events_url": "https://api.github.com/users/thomasquintana/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-15T18:43:24Z", "updated_at": "2017-05-22T22:31:36Z", "author_association": "NONE", "body_html": "<p>I am implementing the <a href=\"https://arxiv.org/pdf/1504.00941v2.pdf\" rel=\"nofollow\">IRNN</a> paper but the RNN layer runs really slow which prompted me to give XLA a try. I have done my best to simplify the code as much as possible. I hope this helps.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> urllib.request\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">fetch</span>(<span class=\"pl-smi\">destination</span>, <span class=\"pl-smi\">source</span>):\n  <span class=\"pl-k\">if</span> os.path.exists(destination):\n    <span class=\"pl-k\">return</span> destination\n  target_dir <span class=\"pl-k\">=</span> os.path.dirname(destination)\n  <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> os.path.exists(target_dir):\n    os.makedirs(target_dir)\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">progress</span>(<span class=\"pl-smi\">count</span>, <span class=\"pl-smi\">block_size</span>, <span class=\"pl-smi\">total_size</span>):\n    bytes_downloaded <span class=\"pl-k\">=</span> <span class=\"pl-c1\">float</span>(count <span class=\"pl-k\">*</span> block_size)\n    percent_downloaded <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(bytes_downloaded <span class=\"pl-k\">/</span> <span class=\"pl-c1\">float</span>(total_size) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">100.0</span>)\n    sys.stdout.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\r</span>Downloading from <span class=\"pl-c1\">{}</span> <span class=\"pl-c1\">{}</span>%<span class=\"pl-pds\">'</span></span>.format(\n      source, percent_downloaded\n    ))\n    sys.stdout.flush()\n  target_path, _ <span class=\"pl-k\">=</span> urllib.urlretrieve(source, destination, progress)\n  target_info <span class=\"pl-k\">=</span> os.stat(target_path)\n  sys.stdout.write(\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span>Successfully downloaded <span class=\"pl-c1\">{}</span> <span class=\"pl-c1\">{}</span> bytes.<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>.format(\n      target_path, target_info.st_size\n    )\n  )\n  sys.stdout.flush()\n  <span class=\"pl-k\">return</span> target_path\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">load_data</span>():\n  path <span class=\"pl-k\">=</span> fetch(\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tmp/mnist.npz<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>https://s3.amazonaws.com/img-datasets/mnist.npz<span class=\"pl-pds\">'</span></span>\n  )\n  inputs <span class=\"pl-k\">=</span> np.load(path)\n  <span class=\"pl-k\">try</span>:\n    <span class=\"pl-k\">return</span> (inputs[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x_train<span class=\"pl-pds\">'</span></span>], inputs[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y_train<span class=\"pl-pds\">'</span></span>]), \\\n           (inputs[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x_test<span class=\"pl-pds\">'</span></span>], inputs[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y_test<span class=\"pl-pds\">'</span></span>])\n  <span class=\"pl-k\">finally</span>:\n    inputs.close()\n\n\nepochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">900</span>\nbatch_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">600</span>\nbatch_sz <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Load the training data.</span>\n(x_train, y_train), (x_test, y_test) <span class=\"pl-k\">=</span> load_data()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Cast the examples.</span>\nx_train <span class=\"pl-k\">=</span> x_train.astype(np.float32)\nx_test <span class=\"pl-k\">=</span> x_test.astype(np.float32)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Subtract the mean and scale the examples.</span>\nx_train <span class=\"pl-k\">=</span> (x_train <span class=\"pl-k\">-</span> np.mean(x_train)) <span class=\"pl-k\">/</span> \\\n          (np.max(x_train) <span class=\"pl-k\">-</span> np.min(x_train))\nx_test <span class=\"pl-k\">=</span> (x_test <span class=\"pl-k\">-</span> np.mean(x_test)) <span class=\"pl-k\">/</span> \\\n         (np.max(x_test) <span class=\"pl-k\">-</span> np.min(x_test))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Reshape the inputs for the rnn layer.</span>\nx_train <span class=\"pl-k\">=</span> np.reshape(x_train, [<span class=\"pl-c1\">60000</span>, <span class=\"pl-c1\">784</span>, <span class=\"pl-c1\">1</span>])\nx_train <span class=\"pl-k\">=</span> np.transpose(x_train, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])\nx_test <span class=\"pl-k\">=</span> np.reshape(x_test, [<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">784</span>, <span class=\"pl-c1\">1</span>])\nx_test <span class=\"pl-k\">=</span> np.transpose(x_test, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Build the tensorflow graph.</span>\ngraph <span class=\"pl-k\">=</span> tf.Graph()\n<span class=\"pl-k\">with</span> graph.as_default():\n  x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">1</span>])\n  y <span class=\"pl-k\">=</span> tf.placeholder(tf.int32)\n  <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn_1<span class=\"pl-pds\">'</span></span>):\n    weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weights<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.truncated_normal(\n      [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">100</span>], <span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32\n    ))\n    state <span class=\"pl-k\">=</span> tf.get_variable(\n      <span class=\"pl-s\"><span class=\"pl-pds\">'</span>state<span class=\"pl-pds\">'</span></span>, \n      <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>],\n      <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(np.identity(<span class=\"pl-c1\">100</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32))\n    bias <span class=\"pl-k\">=</span> tf.get_variable(\n      <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">100</span>], \n      <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    )\n    x_shape <span class=\"pl-k\">=</span> tf.shape(x)\n    timesteps <span class=\"pl-k\">=</span> x_shape[<span class=\"pl-c1\">0</span>]\n    batch_size <span class=\"pl-k\">=</span> x_shape[<span class=\"pl-c1\">1</span>]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Compute the linear transformation for every time step.</span>\n    output <span class=\"pl-k\">=</span> tf.reshape(x, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>])\n    output <span class=\"pl-k\">=</span> tf.add(tf.matmul(output, weights), bias)\n    output <span class=\"pl-k\">=</span> tf.reshape(output, [timesteps, batch_size, <span class=\"pl-c1\">100</span>])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a tensor array to hold the output of our rnn layer.</span>\n    temp <span class=\"pl-k\">=</span> tf.TensorArray(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>timesteps)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">step</span>(<span class=\"pl-smi\">c</span>, <span class=\"pl-smi\">t</span>, <span class=\"pl-smi\">h</span>, <span class=\"pl-smi\">s</span>, <span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">o</span>):\n      h <span class=\"pl-k\">=</span> tf.nn.relu(tf.add(tf.matmul(h, s), i[c]))\n      o <span class=\"pl-k\">=</span> o.write(c, h)\n      <span class=\"pl-k\">return</span> [c <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, t, h, s, i, o]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">step_condition</span>(<span class=\"pl-smi\">c</span>, <span class=\"pl-smi\">t</span>, <span class=\"pl-smi\">h</span>, <span class=\"pl-smi\">s</span>, <span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">o</span>):\n      <span class=\"pl-k\">return</span> tf.less(c, t)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a counter to track the number of timesteps.</span>\n    count <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the initial hidden state.</span>\n    hidden <span class=\"pl-k\">=</span> tf.zeros([batch_size, <span class=\"pl-c1\">100</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n    _, _, _, _, _, output <span class=\"pl-k\">=</span> tf.while_loop(\n      step_condition,\n      step,\n      [count, timesteps, hidden, state, output, temp]\n    )\n\n    output <span class=\"pl-k\">=</span> output.stack()\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> We're only interested in the last output from the RNN layer.</span>\n  output <span class=\"pl-k\">=</span> output[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, :, :]\n  <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>ff_1<span class=\"pl-pds\">'</span></span>):\n    weights <span class=\"pl-k\">=</span> tf.get_variable(\n      <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weights<span class=\"pl-pds\">'</span></span>,\n      <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10</span>],\n      <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.contrib.layers.xavier_initializer(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    )\n    bias <span class=\"pl-k\">=</span> tf.get_variable(\n      <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>,\n      <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">10</span>], \n      <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    )\n    output <span class=\"pl-k\">=</span> tf.add(tf.matmul(output, weights), bias)\n  predictions <span class=\"pl-k\">=</span> tf.nn.softmax(output)\n  error <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(\n    <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>output, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>y\n  )\n  error <span class=\"pl-k\">=</span> tf.reduce_mean(error)\n  optimizer <span class=\"pl-k\">=</span> tf.train.RMSPropOptimizer(<span class=\"pl-c1\">1e-8</span>)\n  gradients <span class=\"pl-k\">=</span> optimizer.compute_gradients(error)\n  train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(\n    [(tf.clip_by_value(grad, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>.), var) <span class=\"pl-k\">for</span> grad, var <span class=\"pl-k\">in</span> gradients]\n  )\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Train the model.</span>\nconfig <span class=\"pl-k\">=</span> tf.ConfigProto()\nconfig.graph_options.optimizer_options.global_jit_level <span class=\"pl-k\">=</span> tf.OptimizerOptions.<span class=\"pl-c1\">ON_1</span>\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config, <span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>graph) <span class=\"pl-k\">as</span> session:\n  session.run(tf.global_variables_initializer())\n  <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(epochs):\n    epoch_duration <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n    epoch_error <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n    start_time <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(batch_count):\n      batch_error, _ <span class=\"pl-k\">=</span> session.run([error, train_op], {\n        x: x_train[:, batch <span class=\"pl-k\">*</span> batch_sz:batch <span class=\"pl-k\">*</span> batch_sz <span class=\"pl-k\">+</span> batch_sz, :],\n        y: y_train[batch <span class=\"pl-k\">*</span> batch_sz:batch <span class=\"pl-k\">*</span> batch_sz <span class=\"pl-k\">+</span> batch_sz]\n      })\n      epoch_error <span class=\"pl-k\">+=</span> batch_error\n    end_time <span class=\"pl-k\">=</span> time.time()\n    epoch_duration <span class=\"pl-k\">=</span> end_time <span class=\"pl-k\">-</span> start_time\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Epoch <span class=\"pl-c1\">%d</span>: loss = <span class=\"pl-c1\">%.2f</span> (<span class=\"pl-c1\">%.3f</span> sec)<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (\n      epoch <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, epoch_error, epoch_duration\n    ))</pre></div>", "body_text": "I am implementing the IRNN paper but the RNN layer runs really slow which prompted me to give XLA a try. I have done my best to simplify the code as much as possible. I hope this helps.\nimport os\nimport sys\nimport time\nimport urllib.request\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef fetch(destination, source):\n  if os.path.exists(destination):\n    return destination\n  target_dir = os.path.dirname(destination)\n  if not os.path.exists(target_dir):\n    os.makedirs(target_dir)\n  def progress(count, block_size, total_size):\n    bytes_downloaded = float(count * block_size)\n    percent_downloaded = int(bytes_downloaded / float(total_size) * 100.0)\n    sys.stdout.write('\\rDownloading from {} {}%'.format(\n      source, percent_downloaded\n    ))\n    sys.stdout.flush()\n  target_path, _ = urllib.urlretrieve(source, destination, progress)\n  target_info = os.stat(target_path)\n  sys.stdout.write(\n    '\\nSuccessfully downloaded {} {} bytes.\\n'.format(\n      target_path, target_info.st_size\n    )\n  )\n  sys.stdout.flush()\n  return target_path\n\ndef load_data():\n  path = fetch(\n    '/tmp/mnist.npz', 'https://s3.amazonaws.com/img-datasets/mnist.npz'\n  )\n  inputs = np.load(path)\n  try:\n    return (inputs['x_train'], inputs['y_train']), \\\n           (inputs['x_test'], inputs['y_test'])\n  finally:\n    inputs.close()\n\n\nepochs = 900\nbatch_count = 600\nbatch_sz = 100\n# Load the training data.\n(x_train, y_train), (x_test, y_test) = load_data()\n# Cast the examples.\nx_train = x_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\n# Subtract the mean and scale the examples.\nx_train = (x_train - np.mean(x_train)) / \\\n          (np.max(x_train) - np.min(x_train))\nx_test = (x_test - np.mean(x_test)) / \\\n         (np.max(x_test) - np.min(x_test))\n# Reshape the inputs for the rnn layer.\nx_train = np.reshape(x_train, [60000, 784, 1])\nx_train = np.transpose(x_train, [1, 0, 2])\nx_test = np.reshape(x_test, [10000, 784, 1])\nx_test = np.transpose(x_test, [1, 0, 2])\n# Build the tensorflow graph.\ngraph = tf.Graph()\nwith graph.as_default():\n  x = tf.placeholder(tf.float32, [None, None, 1])\n  y = tf.placeholder(tf.int32)\n  with tf.variable_scope('rnn_1'):\n    weights = tf.get_variable('weights', initializer=tf.truncated_normal(\n      [1, 100], 0.0, 0.01, dtype=tf.float32\n    ))\n    state = tf.get_variable(\n      'state', \n      shape=[100, 100],\n      initializer=tf.constant_initializer(np.identity(100), dtype=tf.float32))\n    bias = tf.get_variable(\n      'bias', shape=[100], \n      initializer=tf.constant_initializer(0.0, dtype=tf.float32)\n    )\n    x_shape = tf.shape(x)\n    timesteps = x_shape[0]\n    batch_size = x_shape[1]\n    # Compute the linear transformation for every time step.\n    output = tf.reshape(x, [-1, 1])\n    output = tf.add(tf.matmul(output, weights), bias)\n    output = tf.reshape(output, [timesteps, batch_size, 100])\n    # Create a tensor array to hold the output of our rnn layer.\n    temp = tf.TensorArray(dtype=tf.float32, size=timesteps)\n\n    def step(c, t, h, s, i, o):\n      h = tf.nn.relu(tf.add(tf.matmul(h, s), i[c]))\n      o = o.write(c, h)\n      return [c + 1, t, h, s, i, o]\n\n    def step_condition(c, t, h, s, i, o):\n      return tf.less(c, t)\n\n    # Create a counter to track the number of timesteps.\n    count = tf.constant(0)\n    # Define the initial hidden state.\n    hidden = tf.zeros([batch_size, 100], dtype=tf.float32)\n\n    _, _, _, _, _, output = tf.while_loop(\n      step_condition,\n      step,\n      [count, timesteps, hidden, state, output, temp]\n    )\n\n    output = output.stack()\n  # We're only interested in the last output from the RNN layer.\n  output = output[-1, :, :]\n  with tf.variable_scope('ff_1'):\n    weights = tf.get_variable(\n      'weights',\n      shape=[100, 10],\n      initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float32)\n    )\n    bias = tf.get_variable(\n      'bias',\n      shape=[10], \n      initializer=tf.constant_initializer(0.0, dtype=tf.float32)\n    )\n    output = tf.add(tf.matmul(output, weights), bias)\n  predictions = tf.nn.softmax(output)\n  error = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits=output, labels=y\n  )\n  error = tf.reduce_mean(error)\n  optimizer = tf.train.RMSPropOptimizer(1e-8)\n  gradients = optimizer.compute_gradients(error)\n  train_op = optimizer.apply_gradients(\n    [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n  )\n# Train the model.\nconfig = tf.ConfigProto()\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\nwith tf.Session(config=config, graph=graph) as session:\n  session.run(tf.global_variables_initializer())\n  for epoch in range(epochs):\n    epoch_duration = 0.0\n    epoch_error = 0.0\n    start_time = time.time()\n    for batch in range(batch_count):\n      batch_error, _ = session.run([error, train_op], {\n        x: x_train[:, batch * batch_sz:batch * batch_sz + batch_sz, :],\n        y: y_train[batch * batch_sz:batch * batch_sz + batch_sz]\n      })\n      epoch_error += batch_error\n    end_time = time.time()\n    epoch_duration = end_time - start_time\n    print('Epoch %d: loss = %.2f (%.3f sec)' % (\n      epoch + 1, epoch_error, epoch_duration\n    ))", "body": "I am implementing the [IRNN](https://arxiv.org/pdf/1504.00941v2.pdf) paper but the RNN layer runs really slow which prompted me to give XLA a try. I have done my best to simplify the code as much as possible. I hope this helps.\r\n\r\n```python\r\nimport os\r\nimport sys\r\nimport time\r\nimport urllib.request\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef fetch(destination, source):\r\n  if os.path.exists(destination):\r\n    return destination\r\n  target_dir = os.path.dirname(destination)\r\n  if not os.path.exists(target_dir):\r\n    os.makedirs(target_dir)\r\n  def progress(count, block_size, total_size):\r\n    bytes_downloaded = float(count * block_size)\r\n    percent_downloaded = int(bytes_downloaded / float(total_size) * 100.0)\r\n    sys.stdout.write('\\rDownloading from {} {}%'.format(\r\n      source, percent_downloaded\r\n    ))\r\n    sys.stdout.flush()\r\n  target_path, _ = urllib.urlretrieve(source, destination, progress)\r\n  target_info = os.stat(target_path)\r\n  sys.stdout.write(\r\n    '\\nSuccessfully downloaded {} {} bytes.\\n'.format(\r\n      target_path, target_info.st_size\r\n    )\r\n  )\r\n  sys.stdout.flush()\r\n  return target_path\r\n\r\ndef load_data():\r\n  path = fetch(\r\n    '/tmp/mnist.npz', 'https://s3.amazonaws.com/img-datasets/mnist.npz'\r\n  )\r\n  inputs = np.load(path)\r\n  try:\r\n    return (inputs['x_train'], inputs['y_train']), \\\r\n           (inputs['x_test'], inputs['y_test'])\r\n  finally:\r\n    inputs.close()\r\n\r\n\r\nepochs = 900\r\nbatch_count = 600\r\nbatch_sz = 100\r\n# Load the training data.\r\n(x_train, y_train), (x_test, y_test) = load_data()\r\n# Cast the examples.\r\nx_train = x_train.astype(np.float32)\r\nx_test = x_test.astype(np.float32)\r\n# Subtract the mean and scale the examples.\r\nx_train = (x_train - np.mean(x_train)) / \\\r\n          (np.max(x_train) - np.min(x_train))\r\nx_test = (x_test - np.mean(x_test)) / \\\r\n         (np.max(x_test) - np.min(x_test))\r\n# Reshape the inputs for the rnn layer.\r\nx_train = np.reshape(x_train, [60000, 784, 1])\r\nx_train = np.transpose(x_train, [1, 0, 2])\r\nx_test = np.reshape(x_test, [10000, 784, 1])\r\nx_test = np.transpose(x_test, [1, 0, 2])\r\n# Build the tensorflow graph.\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  x = tf.placeholder(tf.float32, [None, None, 1])\r\n  y = tf.placeholder(tf.int32)\r\n  with tf.variable_scope('rnn_1'):\r\n    weights = tf.get_variable('weights', initializer=tf.truncated_normal(\r\n      [1, 100], 0.0, 0.01, dtype=tf.float32\r\n    ))\r\n    state = tf.get_variable(\r\n      'state', \r\n      shape=[100, 100],\r\n      initializer=tf.constant_initializer(np.identity(100), dtype=tf.float32))\r\n    bias = tf.get_variable(\r\n      'bias', shape=[100], \r\n      initializer=tf.constant_initializer(0.0, dtype=tf.float32)\r\n    )\r\n    x_shape = tf.shape(x)\r\n    timesteps = x_shape[0]\r\n    batch_size = x_shape[1]\r\n    # Compute the linear transformation for every time step.\r\n    output = tf.reshape(x, [-1, 1])\r\n    output = tf.add(tf.matmul(output, weights), bias)\r\n    output = tf.reshape(output, [timesteps, batch_size, 100])\r\n    # Create a tensor array to hold the output of our rnn layer.\r\n    temp = tf.TensorArray(dtype=tf.float32, size=timesteps)\r\n\r\n    def step(c, t, h, s, i, o):\r\n      h = tf.nn.relu(tf.add(tf.matmul(h, s), i[c]))\r\n      o = o.write(c, h)\r\n      return [c + 1, t, h, s, i, o]\r\n\r\n    def step_condition(c, t, h, s, i, o):\r\n      return tf.less(c, t)\r\n\r\n    # Create a counter to track the number of timesteps.\r\n    count = tf.constant(0)\r\n    # Define the initial hidden state.\r\n    hidden = tf.zeros([batch_size, 100], dtype=tf.float32)\r\n\r\n    _, _, _, _, _, output = tf.while_loop(\r\n      step_condition,\r\n      step,\r\n      [count, timesteps, hidden, state, output, temp]\r\n    )\r\n\r\n    output = output.stack()\r\n  # We're only interested in the last output from the RNN layer.\r\n  output = output[-1, :, :]\r\n  with tf.variable_scope('ff_1'):\r\n    weights = tf.get_variable(\r\n      'weights',\r\n      shape=[100, 10],\r\n      initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float32)\r\n    )\r\n    bias = tf.get_variable(\r\n      'bias',\r\n      shape=[10], \r\n      initializer=tf.constant_initializer(0.0, dtype=tf.float32)\r\n    )\r\n    output = tf.add(tf.matmul(output, weights), bias)\r\n  predictions = tf.nn.softmax(output)\r\n  error = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n    logits=output, labels=y\r\n  )\r\n  error = tf.reduce_mean(error)\r\n  optimizer = tf.train.RMSPropOptimizer(1e-8)\r\n  gradients = optimizer.compute_gradients(error)\r\n  train_op = optimizer.apply_gradients(\r\n    [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\r\n  )\r\n# Train the model.\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nwith tf.Session(config=config, graph=graph) as session:\r\n  session.run(tf.global_variables_initializer())\r\n  for epoch in range(epochs):\r\n    epoch_duration = 0.0\r\n    epoch_error = 0.0\r\n    start_time = time.time()\r\n    for batch in range(batch_count):\r\n      batch_error, _ = session.run([error, train_op], {\r\n        x: x_train[:, batch * batch_sz:batch * batch_sz + batch_sz, :],\r\n        y: y_train[batch * batch_sz:batch * batch_sz + batch_sz]\r\n      })\r\n      epoch_error += batch_error\r\n    end_time = time.time()\r\n    epoch_duration = end_time - start_time\r\n    print('Epoch %d: loss = %.2f (%.3f sec)' % (\r\n      epoch + 1, epoch_error, epoch_duration\r\n    ))\r\n```"}