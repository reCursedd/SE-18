{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16793", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16793/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16793/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16793/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16793", "id": 294661049, "node_id": "MDU6SXNzdWUyOTQ2NjEwNDk=", "number": 16793, "title": "tf.gradients(colocate_gradients_with_ops=True) set wrong device when using CPU param weight decay", "user": {"login": "liuhu-bigeye", "id": 13741214, "node_id": "MDQ6VXNlcjEzNzQxMjE0", "avatar_url": "https://avatars1.githubusercontent.com/u/13741214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liuhu-bigeye", "html_url": "https://github.com/liuhu-bigeye", "followers_url": "https://api.github.com/users/liuhu-bigeye/followers", "following_url": "https://api.github.com/users/liuhu-bigeye/following{/other_user}", "gists_url": "https://api.github.com/users/liuhu-bigeye/gists{/gist_id}", "starred_url": "https://api.github.com/users/liuhu-bigeye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liuhu-bigeye/subscriptions", "organizations_url": "https://api.github.com/users/liuhu-bigeye/orgs", "repos_url": "https://api.github.com/users/liuhu-bigeye/repos", "events_url": "https://api.github.com/users/liuhu-bigeye/events{/privacy}", "received_events_url": "https://api.github.com/users/liuhu-bigeye/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-02-06T07:45:38Z", "updated_at": "2018-09-13T22:57:49Z", "closed_at": "2018-05-04T01:09:35Z", "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>OS Platform and Distribution: CentOS Linux 7(x86-64)</li>\n<li>TensorFlow installed from (source or binary): binary</li>\n<li>TensorFlow version: 1.4.1</li>\n<li>Python version: 2.7.6</li>\n<li>CUDA/cuDNN version: 8.0/6.0</li>\n<li>Have I written custom code: YES</li>\n<li>Bazel version: N/A</li>\n<li>GPU model and memory: PS Toy model</li>\n<li>Exact command to reproduce: List at the end</li>\n</ul>\n<p>I am trying to define a two-layered dnn for mnist classification, first fc on cpu second on gpu. The devices are set with replica_device_setter. I computed gradients with <strong>colocate_gradients_with_ops=True</strong>. If as expected, the gradient of first layer should be on <strong>/job:worker/replica:0/task:0/device:CPU:0</strong> and gradient of the second layer on <strong>/job:worker/task:0/device:GPU:0</strong>.</p>\n<p>However, i was confused that gradient of the first layer is on device <strong>/job:ps/replica:0/task:0/device:CPU:0</strong>!</p>\n<p>I noticed that this error can be avoided by omitting <code>loss += l2_loss * weight_decay</code>, BUT WHY? It's there some conflict between CPU param weight_decay and gradients with colocate_gradients_with_ops=True?</p>\n<p><strong>Source Code</strong></p>\n<pre><code>import tensorflow as tf\n\n# cluster specification\nparameter_servers = [\"10.194.43.100:2222\"]\nworkers = [\"10.194.43.100:%d\"%(2230+i) for i in range(2)]\n\ncluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\nworker_prefix = \"/job:worker/task:%d\"%0\ncpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/cpu:0', cluster=cluster)\ngpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/gpu:0', cluster=cluster, ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(1, tf.contrib.training.byte_size_load_fn))\n\nnormal_initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=\"float32\")\nweight_decay = 1e-2\n\nwith tf.device(cpu_device):\n    W0 = tf.get_variable('W0', shape=[784, 100], initializer=normal_initializer, dtype=\"float32\")\n    b0 = tf.get_variable('b0', shape=[100], initializer=tf.constant_initializer(0), dtype=\"float32\")\n    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input_%d\"%i)\n    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input_%d\"%i)\n\n    cpu_output = tf.add(tf.matmul(x, W0), b0)\n\nwith tf.variable_scope('v', reuse=False), tf.name_scope('tower_0') as name_scope:\n    with tf.device(gpu_device):\n        W1 = tf.get_variable('W1', shape=[100, 10], initializer=normal_initializer, dtype=\"float32\")\n        b1 = tf.get_variable('b1', shape=[10], initializer=tf.constant_initializer(0), dtype=\"float32\")\n\n        gpu_output = tf.add(tf.matmul(cpu_output, W1), b1)\n        loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=gpu_output)\n        loss = tf.reduce_mean(loss)\n\n        params = tf.trainable_variables()\n        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in params])\n        loss += l2_loss * weight_decay\n        grads = tf.gradients(loss, params, colocate_gradients_with_ops=True, aggregation_method=tf.AggregationMethod.DEFAULT)\n\nwith tf.device(cpu_device):\n    for grad in grads:\n        print grad, grad.device\nprint 'done'\n</code></pre>\n<p><strong>Logs</strong></p>\n<pre><code>Tensor(\"v/tower_0/gradients/AddN_3:0\", shape=(784, 100), dtype=float32, device=/job:ps/task:0) /job:ps/task:0\nTensor(\"v/tower_0/gradients/AddN_2:0\", shape=(100,), dtype=float32, device=/job:ps/task:0) /job:ps/task:0\nTensor(\"v/tower_0/gradients/AddN_1:0\", shape=(100, 10), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0\nTensor(\"v/tower_0/gradients/AddN:0\", shape=(10,), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0\ndone\n</code></pre>", "body_text": "System information\n\nOS Platform and Distribution: CentOS Linux 7(x86-64)\nTensorFlow installed from (source or binary): binary\nTensorFlow version: 1.4.1\nPython version: 2.7.6\nCUDA/cuDNN version: 8.0/6.0\nHave I written custom code: YES\nBazel version: N/A\nGPU model and memory: PS Toy model\nExact command to reproduce: List at the end\n\nI am trying to define a two-layered dnn for mnist classification, first fc on cpu second on gpu. The devices are set with replica_device_setter. I computed gradients with colocate_gradients_with_ops=True. If as expected, the gradient of first layer should be on /job:worker/replica:0/task:0/device:CPU:0 and gradient of the second layer on /job:worker/task:0/device:GPU:0.\nHowever, i was confused that gradient of the first layer is on device /job:ps/replica:0/task:0/device:CPU:0!\nI noticed that this error can be avoided by omitting loss += l2_loss * weight_decay, BUT WHY? It's there some conflict between CPU param weight_decay and gradients with colocate_gradients_with_ops=True?\nSource Code\nimport tensorflow as tf\n\n# cluster specification\nparameter_servers = [\"10.194.43.100:2222\"]\nworkers = [\"10.194.43.100:%d\"%(2230+i) for i in range(2)]\n\ncluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\nworker_prefix = \"/job:worker/task:%d\"%0\ncpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/cpu:0', cluster=cluster)\ngpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/gpu:0', cluster=cluster, ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(1, tf.contrib.training.byte_size_load_fn))\n\nnormal_initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=\"float32\")\nweight_decay = 1e-2\n\nwith tf.device(cpu_device):\n    W0 = tf.get_variable('W0', shape=[784, 100], initializer=normal_initializer, dtype=\"float32\")\n    b0 = tf.get_variable('b0', shape=[100], initializer=tf.constant_initializer(0), dtype=\"float32\")\n    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input_%d\"%i)\n    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input_%d\"%i)\n\n    cpu_output = tf.add(tf.matmul(x, W0), b0)\n\nwith tf.variable_scope('v', reuse=False), tf.name_scope('tower_0') as name_scope:\n    with tf.device(gpu_device):\n        W1 = tf.get_variable('W1', shape=[100, 10], initializer=normal_initializer, dtype=\"float32\")\n        b1 = tf.get_variable('b1', shape=[10], initializer=tf.constant_initializer(0), dtype=\"float32\")\n\n        gpu_output = tf.add(tf.matmul(cpu_output, W1), b1)\n        loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=gpu_output)\n        loss = tf.reduce_mean(loss)\n\n        params = tf.trainable_variables()\n        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in params])\n        loss += l2_loss * weight_decay\n        grads = tf.gradients(loss, params, colocate_gradients_with_ops=True, aggregation_method=tf.AggregationMethod.DEFAULT)\n\nwith tf.device(cpu_device):\n    for grad in grads:\n        print grad, grad.device\nprint 'done'\n\nLogs\nTensor(\"v/tower_0/gradients/AddN_3:0\", shape=(784, 100), dtype=float32, device=/job:ps/task:0) /job:ps/task:0\nTensor(\"v/tower_0/gradients/AddN_2:0\", shape=(100,), dtype=float32, device=/job:ps/task:0) /job:ps/task:0\nTensor(\"v/tower_0/gradients/AddN_1:0\", shape=(100, 10), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0\nTensor(\"v/tower_0/gradients/AddN:0\", shape=(10,), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0\ndone", "body": "**System information**\r\n\r\n- OS Platform and Distribution: CentOS Linux 7(x86-64)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.4.1\r\n- Python version: 2.7.6\r\n- CUDA/cuDNN version: 8.0/6.0\r\n- Have I written custom code: YES\r\n- Bazel version: N/A\r\n- GPU model and memory: PS Toy model\r\n- Exact command to reproduce: List at the end\r\n\r\nI am trying to define a two-layered dnn for mnist classification, first fc on cpu second on gpu. The devices are set with replica_device_setter. I computed gradients with **colocate_gradients_with_ops=True**. If as expected, the gradient of first layer should be on **/job:worker/replica:0/task:0/device:CPU:0** and gradient of the second layer on **/job:worker/task:0/device:GPU:0**.\r\n\r\nHowever, i was confused that gradient of the first layer is on device **/job:ps/replica:0/task:0/device:CPU:0**! \r\n\r\nI noticed that this error can be avoided by omitting `loss += l2_loss * weight_decay`, BUT WHY? It's there some conflict between CPU param weight_decay and gradients with colocate_gradients_with_ops=True?\r\n\r\n**Source Code**\r\n\r\n    import tensorflow as tf\r\n\r\n    # cluster specification\r\n    parameter_servers = [\"10.194.43.100:2222\"]\r\n    workers = [\"10.194.43.100:%d\"%(2230+i) for i in range(2)]\r\n\r\n    cluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\r\n    worker_prefix = \"/job:worker/task:%d\"%0\r\n    cpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/cpu:0', cluster=cluster)\r\n    gpu_device = tf.train.replica_device_setter(worker_device=worker_prefix+'/gpu:0', cluster=cluster, ps_strategy=tf.contrib.training.GreedyLoadBalancingStrategy(1, tf.contrib.training.byte_size_load_fn))\r\n\r\n    normal_initializer = tf.truncated_normal_initializer(stddev=0.1, dtype=\"float32\")\r\n    weight_decay = 1e-2\r\n\r\n    with tf.device(cpu_device):\r\n        W0 = tf.get_variable('W0', shape=[784, 100], initializer=normal_initializer, dtype=\"float32\")\r\n        b0 = tf.get_variable('b0', shape=[100], initializer=tf.constant_initializer(0), dtype=\"float32\")\r\n        x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input_%d\"%i)\r\n        y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input_%d\"%i)\r\n\r\n        cpu_output = tf.add(tf.matmul(x, W0), b0)\r\n\r\n    with tf.variable_scope('v', reuse=False), tf.name_scope('tower_0') as name_scope:\r\n        with tf.device(gpu_device):\r\n            W1 = tf.get_variable('W1', shape=[100, 10], initializer=normal_initializer, dtype=\"float32\")\r\n            b1 = tf.get_variable('b1', shape=[10], initializer=tf.constant_initializer(0), dtype=\"float32\")\r\n\r\n            gpu_output = tf.add(tf.matmul(cpu_output, W1), b1)\r\n            loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=gpu_output)\r\n            loss = tf.reduce_mean(loss)\r\n\r\n            params = tf.trainable_variables()\r\n            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in params])\r\n            loss += l2_loss * weight_decay\r\n            grads = tf.gradients(loss, params, colocate_gradients_with_ops=True, aggregation_method=tf.AggregationMethod.DEFAULT)\r\n\r\n    with tf.device(cpu_device):\r\n        for grad in grads:\r\n            print grad, grad.device\r\n    print 'done'\r\n\r\n**Logs**\r\n\r\n    Tensor(\"v/tower_0/gradients/AddN_3:0\", shape=(784, 100), dtype=float32, device=/job:ps/task:0) /job:ps/task:0\r\n    Tensor(\"v/tower_0/gradients/AddN_2:0\", shape=(100,), dtype=float32, device=/job:ps/task:0) /job:ps/task:0\r\n    Tensor(\"v/tower_0/gradients/AddN_1:0\", shape=(100, 10), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0\r\n    Tensor(\"v/tower_0/gradients/AddN:0\", shape=(10,), dtype=float32, device=/job:worker/task:0/device:GPU:0) /job:worker/task:0/device:GPU:0\r\n    done"}