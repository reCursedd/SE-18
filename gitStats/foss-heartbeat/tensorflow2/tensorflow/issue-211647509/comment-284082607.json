{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284082607", "html_url": "https://github.com/tensorflow/tensorflow/issues/8051#issuecomment-284082607", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8051", "id": 284082607, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDA4MjYwNw==", "user": {"login": "ageron", "id": 76661, "node_id": "MDQ6VXNlcjc2NjYx", "avatar_url": "https://avatars3.githubusercontent.com/u/76661?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ageron", "html_url": "https://github.com/ageron", "followers_url": "https://api.github.com/users/ageron/followers", "following_url": "https://api.github.com/users/ageron/following{/other_user}", "gists_url": "https://api.github.com/users/ageron/gists{/gist_id}", "starred_url": "https://api.github.com/users/ageron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ageron/subscriptions", "organizations_url": "https://api.github.com/users/ageron/orgs", "repos_url": "https://api.github.com/users/ageron/repos", "events_url": "https://api.github.com/users/ageron/events{/privacy}", "received_events_url": "https://api.github.com/users/ageron/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-03T22:03:07Z", "updated_at": "2017-03-03T22:03:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> That's a great question. I propose the following alternative solution, to avoid any risk.</p>\n<p>When a tensor's LHS magic function is called, then before doing anything we check the following conditions (for example for <code>Tensor.__add__()</code>):</p>\n<ul>\n<li><code>type(RHS)</code> has <code>__radd__</code></li>\n<li><code>RHS.__tensor_priority__</code> is defined and greater than <code>LHS.__tensor_priority__</code> (which defaults to 0).</li>\n</ul>\n<p>If both conditions apply, then delegate to the RHS (by returning <code>NotImplemented</code>). Or else, proceed as usual, and if an error occurs when calling <code>convert_to_tensor()</code> and if condition 1 applies, then display a friendly error message explaining the <code>__tensor_priority__</code> logic.</p>\n<p>The advantages are:</p>\n<ul>\n<li>This gives full control to developers who want to build libraries on top of TensorFlow. If they set <code>__tensor_priority__</code>, they can have full control even when tensors are on the LHS.</li>\n<li>This won't break any existing code since nothing changes unless <code>__tensor_priority__</code> is defined by the RHS.</li>\n<li>It's consistent with NumPy's approach.</li>\n<li>We don't use <code>__array_priority__</code> which may lead to conflicts.</li>\n<li>Compared to the first approach I proposed, this only requires the developer to define <code>__tensor_priority__</code> and the error message will explain what to do.</li>\n</ul>\n<p>Wdyt?</p>", "body_text": "@yaroslavvb That's a great question. I propose the following alternative solution, to avoid any risk.\nWhen a tensor's LHS magic function is called, then before doing anything we check the following conditions (for example for Tensor.__add__()):\n\ntype(RHS) has __radd__\nRHS.__tensor_priority__ is defined and greater than LHS.__tensor_priority__ (which defaults to 0).\n\nIf both conditions apply, then delegate to the RHS (by returning NotImplemented). Or else, proceed as usual, and if an error occurs when calling convert_to_tensor() and if condition 1 applies, then display a friendly error message explaining the __tensor_priority__ logic.\nThe advantages are:\n\nThis gives full control to developers who want to build libraries on top of TensorFlow. If they set __tensor_priority__, they can have full control even when tensors are on the LHS.\nThis won't break any existing code since nothing changes unless __tensor_priority__ is defined by the RHS.\nIt's consistent with NumPy's approach.\nWe don't use __array_priority__ which may lead to conflicts.\nCompared to the first approach I proposed, this only requires the developer to define __tensor_priority__ and the error message will explain what to do.\n\nWdyt?", "body": "@yaroslavvb That's a great question. I propose the following alternative solution, to avoid any risk.\r\n\r\nWhen a tensor's LHS magic function is called, then before doing anything we check the following conditions (for example for `Tensor.__add__()`):\r\n* `type(RHS)` has `__radd__`\r\n* `RHS.__tensor_priority__` is defined and greater than `LHS.__tensor_priority__` (which defaults to 0).\r\n\r\nIf both conditions apply, then delegate to the RHS (by returning `NotImplemented`). Or else, proceed as usual, and if an error occurs when calling `convert_to_tensor()` and if condition 1 applies, then display a friendly error message explaining the `__tensor_priority__` logic.\r\n\r\nThe advantages are:\r\n* This gives full control to developers who want to build libraries on top of TensorFlow. If they set `__tensor_priority__`, they can have full control even when tensors are on the LHS.\r\n* This won't break any existing code since nothing changes unless `__tensor_priority__` is defined by the RHS.\r\n* It's consistent with NumPy's approach.\r\n* We don't use `__array_priority__` which may lead to conflicts.\r\n* Compared to the first approach I proposed, this only requires the developer to define `__tensor_priority__` and the error message will explain what to do.\r\n\r\nWdyt?"}