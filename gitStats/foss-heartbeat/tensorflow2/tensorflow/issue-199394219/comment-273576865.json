{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273576865", "html_url": "https://github.com/tensorflow/tensorflow/issues/6717#issuecomment-273576865", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6717", "id": 273576865, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzU3Njg2NQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-18T19:33:38Z", "updated_at": "2017-01-18T19:33:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For numerical stability, we use tf.nn.softmax_cross_entropy_with_logits to calculate the entropy.  I just found out that this operation does not perform backprop with respect to the \"labels\" argument (the exponentiated probabilities term of the entropy).  Thus the zeros.</p>\n<p>Probably the best way to fix this is for us to implement the numerically stable equivalent of your reduce_sum.</p>", "body_text": "For numerical stability, we use tf.nn.softmax_cross_entropy_with_logits to calculate the entropy.  I just found out that this operation does not perform backprop with respect to the \"labels\" argument (the exponentiated probabilities term of the entropy).  Thus the zeros.\nProbably the best way to fix this is for us to implement the numerically stable equivalent of your reduce_sum.", "body": "For numerical stability, we use tf.nn.softmax_cross_entropy_with_logits to calculate the entropy.  I just found out that this operation does not perform backprop with respect to the \"labels\" argument (the exponentiated probabilities term of the entropy).  Thus the zeros.\r\n\r\nProbably the best way to fix this is for us to implement the numerically stable equivalent of your reduce_sum."}