{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6717", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6717/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6717/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6717/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6717", "id": 199394219, "node_id": "MDU6SXNzdWUxOTkzOTQyMTk=", "number": 6717, "title": "Incorrect gradient for categorical distribution entropy", "user": {"login": "lwohlhart", "id": 5650500, "node_id": "MDQ6VXNlcjU2NTA1MDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/5650500?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lwohlhart", "html_url": "https://github.com/lwohlhart", "followers_url": "https://api.github.com/users/lwohlhart/followers", "following_url": "https://api.github.com/users/lwohlhart/following{/other_user}", "gists_url": "https://api.github.com/users/lwohlhart/gists{/gist_id}", "starred_url": "https://api.github.com/users/lwohlhart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lwohlhart/subscriptions", "organizations_url": "https://api.github.com/users/lwohlhart/orgs", "repos_url": "https://api.github.com/users/lwohlhart/repos", "events_url": "https://api.github.com/users/lwohlhart/events{/privacy}", "received_events_url": "https://api.github.com/users/lwohlhart/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-01-08T00:29:44Z", "updated_at": "2017-01-24T01:57:33Z", "closed_at": "2017-01-24T01:57:33Z", "author_association": "NONE", "body_html": "<p>The <strong>Categorical</strong> distribution class provides an awesome <strong>entropy</strong> operator but apparently the <strong>gradient</strong> calculation w.r.t. the input operators <strong>doesn't work</strong>.</p>\n<div class=\"highlight highlight-source-python\"><pre>logits <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-v\">initial_value</span><span class=\"pl-k\">=</span>[[<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">2</span>., <span class=\"pl-c1\">3</span>.], [<span class=\"pl-c1\">2</span>., <span class=\"pl-c1\">5</span>., <span class=\"pl-c1\">1</span>.]])\n\nprobabilities <span class=\"pl-k\">=</span> tf.nn.softmax(logits)\nlog_probabilities <span class=\"pl-k\">=</span> tf.nn.log_softmax(logits)\nentropy <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span> tf.reduce_sum(probabilities <span class=\"pl-k\">*</span> log_probabilities, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> using the actual distribution would be nicer but gradients seem buggy</span>\ncategorical_distribution <span class=\"pl-k\">=</span> tf.contrib.distributions.Categorical(<span class=\"pl-v\">p</span><span class=\"pl-k\">=</span>probabilities)\ncategorical_distribution_entropy <span class=\"pl-k\">=</span> categorical_distribution.entropy()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> initialize</span>\ninit <span class=\"pl-k\">=</span> tf.global_variables_initializer()\nsess <span class=\"pl-k\">=</span> tf.Session()\nsess.run(init)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> works</span>\n<span class=\"pl-c1\">print</span>(sess.run(entropy))\n<span class=\"pl-c1\">print</span>(sess.run(tf.gradients(entropy, [logits])))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> apparently loses gradient information</span>\n<span class=\"pl-c1\">print</span>(sess.run(categorical_distribution_entropy))\n<span class=\"pl-c1\">print</span>(sess.run(tf.gradients(categorical_distribution_entropy, [logits])))</pre></div>\n<p>In the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn't work when I try to maximize this entropy using any optimizer.</p>\n<pre><code>[ 0.83239555  0.27431309]\n[array([[ 0.14181709,  0.14077036, -0.28258738],\n       [ 0.13012242, -0.19513965,  0.06501719]], dtype=float32)]\n[ 0.83239555  0.27431309]\n[array([[ 0.,  0.,  0.],\n       [ 0.,  0.,  0.]], dtype=float32)]\n</code></pre>", "body_text": "The Categorical distribution class provides an awesome entropy operator but apparently the gradient calculation w.r.t. the input operators doesn't work.\nlogits = tf.Variable(initial_value=[[1., 2., 3.], [2., 5., 1.]])\n\nprobabilities = tf.nn.softmax(logits)\nlog_probabilities = tf.nn.log_softmax(logits)\nentropy = - tf.reduce_sum(probabilities * log_probabilities, axis=-1)\n\n# using the actual distribution would be nicer but gradients seem buggy\ncategorical_distribution = tf.contrib.distributions.Categorical(p=probabilities)\ncategorical_distribution_entropy = categorical_distribution.entropy()\n\n# initialize\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\n# works\nprint(sess.run(entropy))\nprint(sess.run(tf.gradients(entropy, [logits])))\n\n# apparently loses gradient information\nprint(sess.run(categorical_distribution_entropy))\nprint(sess.run(tf.gradients(categorical_distribution_entropy, [logits])))\nIn the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn't work when I try to maximize this entropy using any optimizer.\n[ 0.83239555  0.27431309]\n[array([[ 0.14181709,  0.14077036, -0.28258738],\n       [ 0.13012242, -0.19513965,  0.06501719]], dtype=float32)]\n[ 0.83239555  0.27431309]\n[array([[ 0.,  0.,  0.],\n       [ 0.,  0.,  0.]], dtype=float32)]", "body": "The **Categorical** distribution class provides an awesome **entropy** operator but apparently the **gradient** calculation w.r.t. the input operators **doesn't work**.\r\n\r\n```python\r\nlogits = tf.Variable(initial_value=[[1., 2., 3.], [2., 5., 1.]])\r\n\r\nprobabilities = tf.nn.softmax(logits)\r\nlog_probabilities = tf.nn.log_softmax(logits)\r\nentropy = - tf.reduce_sum(probabilities * log_probabilities, axis=-1)\r\n\r\n# using the actual distribution would be nicer but gradients seem buggy\r\ncategorical_distribution = tf.contrib.distributions.Categorical(p=probabilities)\r\ncategorical_distribution_entropy = categorical_distribution.entropy()\r\n\r\n# initialize\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\n# works\r\nprint(sess.run(entropy))\r\nprint(sess.run(tf.gradients(entropy, [logits])))\r\n\r\n# apparently loses gradient information\r\nprint(sess.run(categorical_distribution_entropy))\r\nprint(sess.run(tf.gradients(categorical_distribution_entropy, [logits])))\r\n```\r\n\r\nIn the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn't work when I try to maximize this entropy using any optimizer.\r\n```\r\n[ 0.83239555  0.27431309]\r\n[array([[ 0.14181709,  0.14077036, -0.28258738],\r\n       [ 0.13012242, -0.19513965,  0.06501719]], dtype=float32)]\r\n[ 0.83239555  0.27431309]\r\n[array([[ 0.,  0.,  0.],\r\n       [ 0.,  0.,  0.]], dtype=float32)]\r\n```"}