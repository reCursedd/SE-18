{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/434807008", "html_url": "https://github.com/tensorflow/tensorflow/issues/23272#issuecomment-434807008", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23272", "id": 434807008, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDgwNzAwOA==", "user": {"login": "sseveran", "id": 449906, "node_id": "MDQ6VXNlcjQ0OTkwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/449906?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sseveran", "html_url": "https://github.com/sseveran", "followers_url": "https://api.github.com/users/sseveran/followers", "following_url": "https://api.github.com/users/sseveran/following{/other_user}", "gists_url": "https://api.github.com/users/sseveran/gists{/gist_id}", "starred_url": "https://api.github.com/users/sseveran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sseveran/subscriptions", "organizations_url": "https://api.github.com/users/sseveran/orgs", "repos_url": "https://api.github.com/users/sseveran/repos", "events_url": "https://api.github.com/users/sseveran/events{/privacy}", "received_events_url": "https://api.github.com/users/sseveran/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-31T18:56:42Z", "updated_at": "2018-10-31T19:00:13Z", "author_association": "NONE", "body_html": "<p>I have run this down I believe. The issue is somewhere in TensorFlow.contrib.data.map_and_batch. I have run a variety of variations of my training input function to my custom estimator.<br>\nThis version crashes:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">training_data_input_fn_foo</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        files <span class=\"pl-k\">=</span> tf.data.Dataset.list_files(os.path.join(<span class=\"pl-c1\">self</span>.parameters.get(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>examples_path<span class=\"pl-pds\">'</span></span>), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.tfrecord.gz<span class=\"pl-pds\">\"</span></span>))\n        dataset <span class=\"pl-k\">=</span> files.apply(tf.contrib.data.parallel_interleave(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>:\n                    tf.data.TFRecordDataset(x, <span class=\"pl-v\">compression_type</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>GZIP<span class=\"pl-pds\">'</span></span>), <span class=\"pl-v\">cycle_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>))\n        dataset <span class=\"pl-k\">=</span> dataset.apply(tf.contrib.data.shuffle_and_repeat(<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">None</span>))\n        dataset <span class=\"pl-k\">=</span> dataset.apply(tf.contrib.data.map_and_batch(<span class=\"pl-c1\">self</span>.parse_example, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.parameters.batch_size,\n                                                              <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.parameters.num_gpus <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span>))\n        dataset <span class=\"pl-k\">=</span> dataset.prefetch(<span class=\"pl-c1\">self</span>.parameters.batch_size <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">100</span>)\n        <span class=\"pl-k\">return</span> dataset</pre></div>\n<p>This version does not crash:</p>\n<div class=\"highlight highlight-source-python\"><pre> <span class=\"pl-k\">def</span> <span class=\"pl-en\">training_data_input_fn_safe</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        files <span class=\"pl-k\">=</span> tf.data.Dataset.list_files(os.path.join(<span class=\"pl-c1\">self</span>.parameters.get(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>examples_path<span class=\"pl-pds\">'</span></span>), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.tfrecord.gz<span class=\"pl-pds\">\"</span></span>))\n        dataset <span class=\"pl-k\">=</span> files.apply(tf.contrib.data.parallel_interleave(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>:\n                    tf.data.TFRecordDataset(x, <span class=\"pl-v\">compression_type</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>GZIP<span class=\"pl-pds\">'</span></span>), <span class=\"pl-v\">cycle_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>))\n        dataset <span class=\"pl-k\">=</span> dataset.apply(tf.contrib.data.shuffle_and_repeat(<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">None</span>))\n        dataset <span class=\"pl-k\">=</span> dataset.map(<span class=\"pl-c1\">self</span>.parse_example, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.parameters.num_gpus <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span>)\n        dataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">self</span>.parameters.batch_size)\n        dataset <span class=\"pl-k\">=</span> dataset.prefetch(<span class=\"pl-c1\">self</span>.parameters.batch_size <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">100</span>)\n        <span class=\"pl-k\">return</span> dataset</pre></div>\n<p>My parsing function I call in map is quite simple:</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">features</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> {\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>data<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([<span class=\"pl-c1\">300</span>, <span class=\"pl-c1\">15</span>, <span class=\"pl-c1\">64</span>], tf.float32),\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>label<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([<span class=\"pl-c1\">1</span>], tf.string)\n        }\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">parse_example</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        parsed <span class=\"pl-k\">=</span> tf.parse_single_example(x, <span class=\"pl-c1\">self</span>.features)\n        <span class=\"pl-k\">return</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>data<span class=\"pl-pds\">'</span></span>: parsed[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>data<span class=\"pl-pds\">'</span></span>]}, parsed[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>label<span class=\"pl-pds\">'</span></span>]</pre></div>\n<p>I am happy to provide more specific information to whoever needs it if I can. I could now create an encapsulated reproduction.</p>\n<p>Steve</p>", "body_text": "I have run this down I believe. The issue is somewhere in TensorFlow.contrib.data.map_and_batch. I have run a variety of variations of my training input function to my custom estimator.\nThis version crashes:\ndef training_data_input_fn_foo(self):\n        files = tf.data.Dataset.list_files(os.path.join(self.parameters.get('examples_path'), \"*.tfrecord.gz\"))\n        dataset = files.apply(tf.contrib.data.parallel_interleave(lambda x:\n                    tf.data.TFRecordDataset(x, compression_type='GZIP'), cycle_length=4))\n        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(10000, None))\n        dataset = dataset.apply(tf.contrib.data.map_and_batch(self.parse_example, batch_size=self.parameters.batch_size,\n                                                              num_parallel_calls=self.parameters.num_gpus * 4))\n        dataset = dataset.prefetch(self.parameters.batch_size * 4 * 100)\n        return dataset\nThis version does not crash:\n def training_data_input_fn_safe(self):\n        files = tf.data.Dataset.list_files(os.path.join(self.parameters.get('examples_path'), \"*.tfrecord.gz\"))\n        dataset = files.apply(tf.contrib.data.parallel_interleave(lambda x:\n                    tf.data.TFRecordDataset(x, compression_type='GZIP'), cycle_length=4))\n        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(10000, None))\n        dataset = dataset.map(self.parse_example, num_parallel_calls=self.parameters.num_gpus * 4)\n        dataset = dataset.batch(self.parameters.batch_size)\n        dataset = dataset.prefetch(self.parameters.batch_size * 4 * 100)\n        return dataset\nMy parsing function I call in map is quite simple:\n    @property\n    def features(self):\n        return {\n            'data': tf.FixedLenFeature([300, 15, 64], tf.float32),\n            'label': tf.FixedLenFeature([1], tf.string)\n        }\n\n    def parse_example(self, x):\n        parsed = tf.parse_single_example(x, self.features)\n        return {'data': parsed['data']}, parsed['label']\nI am happy to provide more specific information to whoever needs it if I can. I could now create an encapsulated reproduction.\nSteve", "body": "I have run this down I believe. The issue is somewhere in TensorFlow.contrib.data.map_and_batch. I have run a variety of variations of my training input function to my custom estimator.\r\nThis version crashes:\r\n```python   \r\ndef training_data_input_fn_foo(self):\r\n        files = tf.data.Dataset.list_files(os.path.join(self.parameters.get('examples_path'), \"*.tfrecord.gz\"))\r\n        dataset = files.apply(tf.contrib.data.parallel_interleave(lambda x:\r\n                    tf.data.TFRecordDataset(x, compression_type='GZIP'), cycle_length=4))\r\n        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(10000, None))\r\n        dataset = dataset.apply(tf.contrib.data.map_and_batch(self.parse_example, batch_size=self.parameters.batch_size,\r\n                                                              num_parallel_calls=self.parameters.num_gpus * 4))\r\n        dataset = dataset.prefetch(self.parameters.batch_size * 4 * 100)\r\n        return dataset\r\n```\r\nThis version does not crash:\r\n\r\n```python \r\n\r\n def training_data_input_fn_safe(self):\r\n        files = tf.data.Dataset.list_files(os.path.join(self.parameters.get('examples_path'), \"*.tfrecord.gz\"))\r\n        dataset = files.apply(tf.contrib.data.parallel_interleave(lambda x:\r\n                    tf.data.TFRecordDataset(x, compression_type='GZIP'), cycle_length=4))\r\n        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(10000, None))\r\n        dataset = dataset.map(self.parse_example, num_parallel_calls=self.parameters.num_gpus * 4)\r\n        dataset = dataset.batch(self.parameters.batch_size)\r\n        dataset = dataset.prefetch(self.parameters.batch_size * 4 * 100)\r\n        return dataset\r\n```\r\n\r\nMy parsing function I call in map is quite simple:\r\n```python\r\n    @property\r\n    def features(self):\r\n        return {\r\n            'data': tf.FixedLenFeature([300, 15, 64], tf.float32),\r\n            'label': tf.FixedLenFeature([1], tf.string)\r\n        }\r\n\r\n    def parse_example(self, x):\r\n        parsed = tf.parse_single_example(x, self.features)\r\n        return {'data': parsed['data']}, parsed['label']\r\n```\r\n\r\nI am happy to provide more specific information to whoever needs it if I can. I could now create an encapsulated reproduction.\r\n\r\nSteve"}