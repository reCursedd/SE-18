{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23272", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23272/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23272/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23272/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23272", "id": 374175034, "node_id": "MDU6SXNzdWUzNzQxNzUwMzQ=", "number": 23272, "title": "Seg Fault when using tf.data.contrib.map_and_batch.", "user": {"login": "sseveran", "id": 449906, "node_id": "MDQ6VXNlcjQ0OTkwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/449906?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sseveran", "html_url": "https://github.com/sseveran", "followers_url": "https://api.github.com/users/sseveran/followers", "following_url": "https://api.github.com/users/sseveran/following{/other_user}", "gists_url": "https://api.github.com/users/sseveran/gists{/gist_id}", "starred_url": "https://api.github.com/users/sseveran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sseveran/subscriptions", "organizations_url": "https://api.github.com/users/sseveran/orgs", "repos_url": "https://api.github.com/users/sseveran/repos", "events_url": "https://api.github.com/users/sseveran/events{/privacy}", "received_events_url": "https://api.github.com/users/sseveran/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1114343535, "node_id": "MDU6TGFiZWwxMTE0MzQzNTM1", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:data", "name": "comp:data", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2018-10-25T23:46:37Z", "updated_at": "2018-11-13T00:53:03Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a bug. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):<br>\nYes. I am using a modified ResNet50 model architecture.</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):<br>\nUbuntu 16.04. Specifically I am using the AWS DL AMI 16.0 Ubunutu</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:</li>\n<li>TensorFlow installed from (source or binary): binary</li>\n<li>TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0</li>\n<li>Python version: 3.6</li>\n<li>Bazel version (if compiling from source): N/A</li>\n<li>GCC/Compiler version (if compiling from source): N/A</li>\n<li>CUDA/cuDNN version: 9/7 I believe</li>\n<li>GPU model and memory: Nvidia Tesla K80 (AWS p2.8xlarge)</li>\n</ul>\n<p>You can collect some of this information using our environment capture <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">script</a><br>\nYou can also obtain the TensorFlow version with<br>\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<p><strong>Describe the current behavior</strong><br>\nSegmentation Fault</p>\n<p><strong>Describe the expected behavior</strong><br>\nNot a segmentation fault.</p>\n<p><strong>Code to reproduce the issue</strong><br>\nI do not have a minimal repro case which is unfortunate. My model is built using the TF Estimator API and I use the MirroredStrategy for utilizing multiple GPUs. I don't have any custom lower level code (C++).</p>\n<p>I have been able to trigger this behavior in 2 ways.</p>\n<ol>\n<li>Using all the GPUs on the system. It will always occur if I use all 8 GPUs.</li>\n<li>Using train_and_evaluate as opposed to train. This seems to work if I use N-2 GPUs.</li>\n</ol>\n<p>I am happy to run with any additional configurations or rebuild TF to provide more specific information to help debug this problem.</p>\n<p><strong>Other info / logs</strong></p>\n<pre><code>INFO:tensorflow:loss = 1147.4103, step = 0\n\nThread 73 \"python3\" received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7ffeebfff700 (LWP 68942)]\n0x00007fff7f601bf3 in std::_Function_handler&lt;void (long, long), Eigen::internal::TensorExecutor&lt;Eigen::TensorAssignOp&lt;Eigen::TensorChippingOp&lt;0l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 2, 1, long&gt;, 16, Eigen::MakePointer&gt; &gt;, Eigen::TensorChippingOp&lt;0l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const, Eigen::ThreadPoolDevice, true&gt;::run(Eigen::TensorAssignOp&lt;Eigen::TensorChippingOp&lt;0l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 2, 1, long&gt;, 16, Eigen::MakePointer&gt; &gt;, Eigen::TensorChippingOp&lt;0l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const&amp;, Eigen::ThreadPoolDevice const&amp;)::{lambda(long, long)#1}&gt;::_M_invoke(std::_Any_data const&amp;, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n(gdb) bt\n#0  0x00007fff7f601bf3 in std::_Function_handler&lt;void (long, long), Eigen::internal::TensorExecutor&lt;Eigen::TensorAssignOp&lt;Eigen::TensorChippingOp&lt;0l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 2, 1, long&gt;, 16, Eigen::MakePointer&gt; &gt;, Eigen::TensorChippingOp&lt;0l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const, Eigen::ThreadPoolDevice, true&gt;::run(Eigen::TensorAssignOp&lt;Eigen::TensorChippingOp&lt;0l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 2, 1, long&gt;, 16, Eigen::MakePointer&gt; &gt;, Eigen::TensorChippingOp&lt;0l, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 2, 1, long&gt;, 16, Eigen::MakePointer&gt; const&gt; const&gt; const&amp;, Eigen::ThreadPoolDevice const&amp;)::{lambda(long, long)#1}&gt;::_M_invoke(std::_Any_data const&amp;, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#1  0x00007fff7e87f019 in std::_Function_handler&lt;void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}&gt;::_M_invoke(std::_Any_data const&amp;, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#2  0x00007fff7e87efef in std::_Function_handler&lt;void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}&gt;::_M_invoke(std::_Any_data const&amp;, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#3  0x00007fff7e87efef in std::_Function_handler&lt;void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&amp;, std::function&lt;long (long)&gt;, std::function&lt;void (long, long)&gt;) const::{lambda(long, long)#1}&gt;::_M_invoke(std::_Any_data const&amp;, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#4  0x00007fff7c32693a in Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(int) ()\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\n#5  0x00007fff7c325a02 in std::_Function_handler&lt;void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function&lt;void ()&gt;)::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;) ()\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\n#6  0x00007fff9c62ec5c in std::execute_native_thread_routine_compat (__p=&lt;optimized out&gt;) at /opt/conda/conda-bld/compilers_linux-64_1520532893746/work/.build/src/gcc-7.2.0/libstdc++-v3/src/c++11/thread.cc:110\n#7  0x00007ffff7bc16ba in start_thread (arg=0x7ffeebfff700) at pthread_create.c:333\n#8  0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n</code></pre>", "body_text": "Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes. I am using a modified ResNet50 model architecture.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04. Specifically I am using the AWS DL AMI 16.0 Ubunutu\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0\nPython version: 3.6\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: 9/7 I believe\nGPU model and memory: Nvidia Tesla K80 (AWS p2.8xlarge)\n\nYou can collect some of this information using our environment capture script\nYou can also obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the current behavior\nSegmentation Fault\nDescribe the expected behavior\nNot a segmentation fault.\nCode to reproduce the issue\nI do not have a minimal repro case which is unfortunate. My model is built using the TF Estimator API and I use the MirroredStrategy for utilizing multiple GPUs. I don't have any custom lower level code (C++).\nI have been able to trigger this behavior in 2 ways.\n\nUsing all the GPUs on the system. It will always occur if I use all 8 GPUs.\nUsing train_and_evaluate as opposed to train. This seems to work if I use N-2 GPUs.\n\nI am happy to run with any additional configurations or rebuild TF to provide more specific information to help debug this problem.\nOther info / logs\nINFO:tensorflow:loss = 1147.4103, step = 0\n\nThread 73 \"python3\" received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7ffeebfff700 (LWP 68942)]\n0x00007fff7f601bf3 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n(gdb) bt\n#0  0x00007fff7f601bf3 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#1  0x00007fff7e87f019 in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#2  0x00007fff7e87efef in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#3  0x00007fff7e87efef in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\n#4  0x00007fff7c32693a in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\n#5  0x00007fff7c325a02 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\n#6  0x00007fff9c62ec5c in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1520532893746/work/.build/src/gcc-7.2.0/libstdc++-v3/src/c++11/thread.cc:110\n#7  0x00007ffff7bc16ba in start_thread (arg=0x7ffeebfff700) at pthread_create.c:333\n#8  0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes. I am using a modified ResNet50 model architecture.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04. Specifically I am using the AWS DL AMI 16.0 Ubunutu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9/7 I believe\r\n- GPU model and memory: Nvidia Tesla K80 (AWS p2.8xlarge)\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nSegmentation Fault\r\n\r\n**Describe the expected behavior**\r\nNot a segmentation fault.\r\n\r\n**Code to reproduce the issue**\r\nI do not have a minimal repro case which is unfortunate. My model is built using the TF Estimator API and I use the MirroredStrategy for utilizing multiple GPUs. I don't have any custom lower level code (C++).\r\n\r\nI have been able to trigger this behavior in 2 ways.\r\n\r\n1. Using all the GPUs on the system. It will always occur if I use all 8 GPUs.\r\n2. Using train_and_evaluate as opposed to train. This seems to work if I use N-2 GPUs.\r\n\r\nI am happy to run with any additional configurations or rebuild TF to provide more specific information to help debug this problem.\r\n\r\n**Other info / logs**\r\n```\r\nINFO:tensorflow:loss = 1147.4103, step = 0\r\n\r\nThread 73 \"python3\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7ffeebfff700 (LWP 68942)]\r\n0x00007fff7f601bf3 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n(gdb) bt\r\n#0  0x00007fff7f601bf3 in std::_Function_handler<void (long, long), Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer> >, Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007fff7e87f019 in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff7e87efef in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fff7e87efef in std::_Function_handler<void (long, long), Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long, long) () from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff7c32693a in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fff7c325a02 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#6  0x00007fff9c62ec5c in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1520532893746/work/.build/src/gcc-7.2.0/libstdc++-v3/src/c++11/thread.cc:110\r\n#7  0x00007ffff7bc16ba in start_thread (arg=0x7ffeebfff700) at pthread_create.c:333\r\n#8  0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n```"}