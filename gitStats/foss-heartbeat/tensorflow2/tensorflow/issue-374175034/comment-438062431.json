{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/438062431", "html_url": "https://github.com/tensorflow/tensorflow/issues/23272#issuecomment-438062431", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23272", "id": 438062431, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODA2MjQzMQ==", "user": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-12T23:09:01Z", "updated_at": "2018-11-12T23:35:23Z", "author_association": "MEMBER", "body_html": "<p>One possibility you are seeing the segfault is because your host machine is running out of memory (and TensorFlow code base fails to check whether a dynamic allocation succeeded). This is somewhat more likely to happen with <code>map_and_batch()</code> because it, unlike <code>map().batch()</code>, uses internal buffering.</p>\n<p>Can you report what the host memory usage is? You almost certainly do not want to prefetch <code>self.parameters.batch_size * 4 * 100</code> elements since elements at that point are batched. You should be prefetching <code>self.parameters.num_gpus</code> elements. I would try that and see whether the problem goes away.</p>", "body_text": "One possibility you are seeing the segfault is because your host machine is running out of memory (and TensorFlow code base fails to check whether a dynamic allocation succeeded). This is somewhat more likely to happen with map_and_batch() because it, unlike map().batch(), uses internal buffering.\nCan you report what the host memory usage is? You almost certainly do not want to prefetch self.parameters.batch_size * 4 * 100 elements since elements at that point are batched. You should be prefetching self.parameters.num_gpus elements. I would try that and see whether the problem goes away.", "body": "One possibility you are seeing the segfault is because your host machine is running out of memory (and TensorFlow code base fails to check whether a dynamic allocation succeeded). This is somewhat more likely to happen with `map_and_batch()` because it, unlike `map().batch()`, uses internal buffering.\r\n\r\nCan you report what the host memory usage is? You almost certainly do not want to prefetch `self.parameters.batch_size * 4 * 100` elements since elements at that point are batched. You should be prefetching `self.parameters.num_gpus` elements. I would try that and see whether the problem goes away."}