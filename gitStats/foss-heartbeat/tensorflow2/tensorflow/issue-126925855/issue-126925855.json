{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/785", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/785/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/785/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/785/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/785", "id": 126925855, "node_id": "MDU6SXNzdWUxMjY5MjU4NTU=", "number": 785, "title": "OOM error when using dropout", "user": {"login": "alexatknit", "id": 15474222, "node_id": "MDQ6VXNlcjE1NDc0MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/15474222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexatknit", "html_url": "https://github.com/alexatknit", "followers_url": "https://api.github.com/users/alexatknit/followers", "following_url": "https://api.github.com/users/alexatknit/following{/other_user}", "gists_url": "https://api.github.com/users/alexatknit/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexatknit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexatknit/subscriptions", "organizations_url": "https://api.github.com/users/alexatknit/orgs", "repos_url": "https://api.github.com/users/alexatknit/repos", "events_url": "https://api.github.com/users/alexatknit/events{/privacy}", "received_events_url": "https://api.github.com/users/alexatknit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-01-15T18:06:11Z", "updated_at": "2016-03-08T19:24:37Z", "closed_at": "2016-03-08T19:24:37Z", "author_association": "NONE", "body_html": "<p>I thought I might have been going crazy so I only got around to posting this just now. I've got a GPU that can handle a batch size of 160 on my network, but I run it at 128. Thats just to show that I've got about 20% space left on my GPU ram when the network starts training. If I am using dropout, after about 100,000 iterations i get an OOM error.</p>\n<pre><code>tensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 128 } dim { size: 40 } dim { size: 40 } dim { size: 64 }\n         [[Node: range_1/_1993 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_25333_range_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n</code></pre>\n<p>Is it possible that there could be a memory leak somewhere?</p>", "body_text": "I thought I might have been going crazy so I only got around to posting this just now. I've got a GPU that can handle a batch size of 160 on my network, but I run it at 128. Thats just to show that I've got about 20% space left on my GPU ram when the network starts training. If I am using dropout, after about 100,000 iterations i get an OOM error.\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 128 } dim { size: 40 } dim { size: 40 } dim { size: 64 }\n         [[Node: range_1/_1993 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_25333_range_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nIs it possible that there could be a memory leak somewhere?", "body": "I thought I might have been going crazy so I only got around to posting this just now. I've got a GPU that can handle a batch size of 160 on my network, but I run it at 128. Thats just to show that I've got about 20% space left on my GPU ram when the network starts training. If I am using dropout, after about 100,000 iterations i get an OOM error.\n\n```\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 128 } dim { size: 40 } dim { size: 40 } dim { size: 64 }\n         [[Node: range_1/_1993 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_25333_range_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n```\n\nIs it possible that there could be a memory leak somewhere?\n"}