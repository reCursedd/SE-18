{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13281", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13281/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13281/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13281/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13281", "id": 260119081, "node_id": "MDU6SXNzdWUyNjAxMTkwODE=", "number": 13281, "title": "Non-working example in documentation, with recommendations for how to fix", "user": {"login": "ecpoppenheimer", "id": 31865490, "node_id": "MDQ6VXNlcjMxODY1NDkw", "avatar_url": "https://avatars1.githubusercontent.com/u/31865490?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ecpoppenheimer", "html_url": "https://github.com/ecpoppenheimer", "followers_url": "https://api.github.com/users/ecpoppenheimer/followers", "following_url": "https://api.github.com/users/ecpoppenheimer/following{/other_user}", "gists_url": "https://api.github.com/users/ecpoppenheimer/gists{/gist_id}", "starred_url": "https://api.github.com/users/ecpoppenheimer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ecpoppenheimer/subscriptions", "organizations_url": "https://api.github.com/users/ecpoppenheimer/orgs", "repos_url": "https://api.github.com/users/ecpoppenheimer/repos", "events_url": "https://api.github.com/users/ecpoppenheimer/events{/privacy}", "received_events_url": "https://api.github.com/users/ecpoppenheimer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 390482148, "node_id": "MDU6TGFiZWwzOTA0ODIxNDg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20review", "name": "awaiting review", "color": "fef2c0", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "MarkDaoust", "id": 1414837, "node_id": "MDQ6VXNlcjE0MTQ4Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1414837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarkDaoust", "html_url": "https://github.com/MarkDaoust", "followers_url": "https://api.github.com/users/MarkDaoust/followers", "following_url": "https://api.github.com/users/MarkDaoust/following{/other_user}", "gists_url": "https://api.github.com/users/MarkDaoust/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarkDaoust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarkDaoust/subscriptions", "organizations_url": "https://api.github.com/users/MarkDaoust/orgs", "repos_url": "https://api.github.com/users/MarkDaoust/repos", "events_url": "https://api.github.com/users/MarkDaoust/events{/privacy}", "received_events_url": "https://api.github.com/users/MarkDaoust/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "MarkDaoust", "id": 1414837, "node_id": "MDQ6VXNlcjE0MTQ4Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1414837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarkDaoust", "html_url": "https://github.com/MarkDaoust", "followers_url": "https://api.github.com/users/MarkDaoust/followers", "following_url": "https://api.github.com/users/MarkDaoust/following{/other_user}", "gists_url": "https://api.github.com/users/MarkDaoust/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarkDaoust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarkDaoust/subscriptions", "organizations_url": "https://api.github.com/users/MarkDaoust/orgs", "repos_url": "https://api.github.com/users/MarkDaoust/repos", "events_url": "https://api.github.com/users/MarkDaoust/events{/privacy}", "received_events_url": "https://api.github.com/users/MarkDaoust/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 21, "created_at": "2017-09-24T21:58:17Z", "updated_at": "2018-07-18T06:50:44Z", "closed_at": "2018-07-18T06:50:43Z", "author_association": "NONE", "body_html": "<p>== cat /etc/issue ===============================================<br>\nLinux EricDesktop 4.4.0-96-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116265486\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/119\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/119/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/119\">#119</a>-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux<br>\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"<br>\nVERSION_ID=\"16.04\"<br>\nVERSION_CODENAME=xenial</p>\n<p>== are we in docker =============================================<br>\nNo</p>\n<p>== compiler =====================================================<br>\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609<br>\nCopyright (C) 2015 Free Software Foundation, Inc.<br>\nThis is free software; see the source for copying conditions.  There is NO<br>\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</p>\n<p>== uname -a =====================================================<br>\nLinux EricDesktop 4.4.0-96-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116265486\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/119\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/119/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/119\">#119</a>-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</p>\n<p>== check pips ===================================================<br>\nnumpy (1.13.1)<br>\nprotobuf (3.4.0)<br>\ntensorflow (1.3.0)<br>\ntensorflow-tensorboard (0.1.6)</p>\n<p>== check for virtualenv =========================================<br>\nFalse</p>\n<p>== tensorflow import ============================================<br>\ntf.VERSION = 1.3.0<br>\ntf.GIT_VERSION = v1.3.0-rc1-2409-g5ee3804<br>\ntf.COMPILER_VERSION = v1.3.0-rc1-2409-g5ee3804<br>\nSanity check: array([1], dtype=int32)</p>\n<p>== env ==========================================================<br>\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64<br>\nDYLD_LIBRARY_PATH is unset</p>\n<p>== nvidia-smi ===================================================<br>\nSun Sep 24 14:08:03 2017<br>\n+-----------------------------------------------------------------------------+<br>\n| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================+======================|<br>\n|   0  GeForce GTX 670     Off  | 0000:03:00.0     N/A |                  N/A |<br>\n| 33%   50C    P0    N/A /  N/A |    254MiB /  4031MiB |     N/A      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   1  GeForce GTX 670     Off  | 0000:04:00.0     N/A |                  N/A |<br>\n| 32%   48C    P0    N/A /  N/A |    253MiB /  4036MiB |     N/A      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory |<br>\n|  GPU       PID  Type  Process name                               Usage      |<br>\n|=============================================================================|<br>\n|    0                  Not Supported                                         |<br>\n|    1                  Not Supported                                         |<br>\n+-----------------------------------------------------------------------------+</p>\n<p>== cuda libs  ===================================================<br>\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7<br>\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61</p>\n<h3>Describe the problem</h3>\n<pre><code>I have been trying to get a custom op working for many hours, and have finally collected enough information to know that it isn't entirely my fault, and I can help you improve the custom op documentation to save others such effort.  \n</code></pre>\n<p>Walkthrough to see what the problem is:</p>\n<ol>\n<li>Following the instructions on the documentation page (<a href=\"https://www.tensorflow.org/extend/adding_an_op\" rel=\"nofollow\">https://www.tensorflow.org/extend/adding_an_op</a>) I copy the zero_out.cc code</li>\n<li>Compile with Bazel</li>\n<li>Test with python.  Everything works, this example behaves exactly as it should.</li>\n<li>But I want a GPU kernel, so now I try the \"example\" example.<br>\nCopy the example.h, example.cc and example.cu.cc files on the documentation page.  Didn't make any changes.</li>\n<li>Now I hit some problems.  The documentation isn't very clear on how to compile this more complicated op with bazel.  I managed to figure it out, but I would strongly recommend notifying users about the \"gpu_srcs\" argument that can be used inside the BUILD file, since this took me quite a while to discover.</li>\n</ol>\n<p>Working BUILD file:<br>\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")</p>\n<p>tf_custom_op_library(<br>\nname = \"example.so\",<br>\nsrcs = [\"example.cc\", \"example.h\"],<br>\ngpu_srcs = [\"example.cu.cc\", \"example.h\"],<br>\n)<br>\nWorking Bazel command to invoke the BUILD file:<br>\nbazel build --config opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/core/user_ops:example.so</p>\n<ol start=\"6\">\n<li>\n<p>Now it compiles.  But the op does not load properly in python; the module created by tf.load_op_library does not contain the actual op, as you can see by my tests under the source code section.</p>\n</li>\n<li>\n<p>After a long time, I discover the extremely obvious problem: the example code in the documentation does not have any REGISTER_OP macro call.  This is clearly an omission in the documentation.</p>\n</li>\n<li>\n<p>I try copying the REGISTER_OP macro call from the zero_out example, and it doesn't really work since the \"example\" example is more complicated, but python now at least recognizes the op.  I am sure if I were to spend the time to figure out how to correctly call the REGISTER_OP macro in example.cc, the example would work correctly for me.</p>\n<p>I could make some other recommendations about changing the \"Adding a New Op\" documentation page, though I will refrain for now since I am not sure this is the appropriate place to do so.  I would be happy to take a stab at editing the doc page myself, though since I am very new to github I am concerned that I would end up creating more problems than I would fix, so it might be better to have someone with more experience do it.  I would be happy to help though.</p>\n<p>One thing I will say though: I would really love it if some TF expert would add a fully fleshed out template op to the user op documentation page with all the bells and whistles, and with clear instructions on exactly how to compile and run it.  It should have both GPU support and a gradient implementation.  And ideally it would be multi-threaded (if the \"example\" example isn't already - its not obvious to me either way).  The template would be provided with everything you need, and would have a clearly labeled code block where the user can add their own code:  \"here is a for loop that iterates over every element in the tensor.  write whatever you want here.\"  If you can make it really easy for users to add their own, high-quality operators, you might be able to cut down on your workload responding to problems with custom ops and with users requesting new ops.  And I think that a template that we can fill in would be enough to do that for many people.</p>\n</li>\n</ol>\n<h3>Source code / logs</h3>\n<p>=========================================== start test.py:<br>\nimport tensorflow as tf<br>\nzero_out_module = tf.load_op_library('./zero_out.so')<br>\nwith tf.Session(''):<br>\nprint \"zero_out:\"<br>\nprint(zero_out_module.zero_out([[1, 2], [3, 4]]).eval())</p>\n<p>example_module = tf.load_op_library('./example.so')<br>\nwith tf.Session(''):<br>\nprint \"example:\"<br>\ntry:<br>\nprint(example_module.example([[1, 2], [3, 4]]).eval())<br>\nexcept AttributeError as err:<br>\nprint \"Error: \", err</p>\n<p>print \"Analysis of zero_out_module:\"<br>\nprint zero_out_module.<strong>dict</strong>.keys()<br>\nprint zero_out_module.OP_LIST</p>\n<p>print \"Analysis of example_module:\"<br>\nprint example_module.<strong>dict</strong>.keys()<br>\nprint example_module.OP_LIST</p>\n<p>====================================== end test.py</p>\n<p>Output of running test.py<br>\neric@EricDesktop:~/tensorflow/tensorflow/core/user_ops$ python test.py2017-09-24 14:14:08.183509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero<br>\n2017-09-24 14:14:08.183794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:<br>\nname: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455<br>\npciBusID: 0000:04:00.0<br>\ntotalMemory: 3.94GiB freeMemory: 3.66GiB<br>\n2017-09-24 14:14:08.208238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero<br>\n2017-09-24 14:14:08.208514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 1 with properties:<br>\nname: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455<br>\npciBusID: 0000:03:00.0<br>\ntotalMemory: 3.94GiB freeMemory: 3.66GiB<br>\n2017-09-24 14:14:08.208964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:980] Device peer to peer matrix<br>\n2017-09-24 14:14:08.208988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] DMA: 0 1<br>\n2017-09-24 14:14:08.208994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 0:   Y Y<br>\n2017-09-24 14:14:08.209000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 1:   Y Y<br>\n2017-09-24 14:14:08.209012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)<br>\n2017-09-24 14:14:08.209020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)<br>\nzero_out:<br>\n[[1 0]<br>\n[0 0]]<br>\n2017-09-24 14:14:08.239952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)<br>\n2017-09-24 14:14:08.239970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)<br>\nexample:<br>\nError:  'module' object has no attribute 'example'<br>\nAnalysis of zero_out_module:<br>\n['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', '<strong>builtins</strong>', 'zero_out', '<strong>package</strong>', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', '<strong>name</strong>', '_InitOpDefLibrary', '<strong>doc</strong>']<br>\nop {<br>\nname: \"ZeroOut\"<br>\ninput_arg {<br>\nname: \"to_zero\"<br>\ntype: DT_INT32<br>\n}<br>\noutput_arg {<br>\nname: \"zeroed\"<br>\ntype: DT_INT32<br>\n}<br>\n}</p>\n<p>Analysis of example_module:<br>\n['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', '<strong>builtins</strong>', '<strong>package</strong>', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', '<strong>name</strong>', '_InitOpDefLibrary', '<strong>doc</strong>']</p>", "body_text": "== cat /etc/issue ===============================================\nLinux EricDesktop 4.4.0-96-generic #119-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n== are we in docker =============================================\nNo\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n== uname -a =====================================================\nLinux EricDesktop 4.4.0-96-generic #119-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n== check pips ===================================================\nnumpy (1.13.1)\nprotobuf (3.4.0)\ntensorflow (1.3.0)\ntensorflow-tensorboard (0.1.6)\n== check for virtualenv =========================================\nFalse\n== tensorflow import ============================================\ntf.VERSION = 1.3.0\ntf.GIT_VERSION = v1.3.0-rc1-2409-g5ee3804\ntf.COMPILER_VERSION = v1.3.0-rc1-2409-g5ee3804\nSanity check: array([1], dtype=int32)\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64\nDYLD_LIBRARY_PATH is unset\n== nvidia-smi ===================================================\nSun Sep 24 14:08:03 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 670     Off  | 0000:03:00.0     N/A |                  N/A |\n| 33%   50C    P0    N/A /  N/A |    254MiB /  4031MiB |     N/A      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX 670     Off  | 0000:04:00.0     N/A |                  N/A |\n| 32%   48C    P0    N/A /  N/A |    253MiB /  4036MiB |     N/A      Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0                  Not Supported                                         |\n|    1                  Not Supported                                         |\n+-----------------------------------------------------------------------------+\n== cuda libs  ===================================================\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\nDescribe the problem\nI have been trying to get a custom op working for many hours, and have finally collected enough information to know that it isn't entirely my fault, and I can help you improve the custom op documentation to save others such effort.  \n\nWalkthrough to see what the problem is:\n\nFollowing the instructions on the documentation page (https://www.tensorflow.org/extend/adding_an_op) I copy the zero_out.cc code\nCompile with Bazel\nTest with python.  Everything works, this example behaves exactly as it should.\nBut I want a GPU kernel, so now I try the \"example\" example.\nCopy the example.h, example.cc and example.cu.cc files on the documentation page.  Didn't make any changes.\nNow I hit some problems.  The documentation isn't very clear on how to compile this more complicated op with bazel.  I managed to figure it out, but I would strongly recommend notifying users about the \"gpu_srcs\" argument that can be used inside the BUILD file, since this took me quite a while to discover.\n\nWorking BUILD file:\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\ntf_custom_op_library(\nname = \"example.so\",\nsrcs = [\"example.cc\", \"example.h\"],\ngpu_srcs = [\"example.cu.cc\", \"example.h\"],\n)\nWorking Bazel command to invoke the BUILD file:\nbazel build --config opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/core/user_ops:example.so\n\n\nNow it compiles.  But the op does not load properly in python; the module created by tf.load_op_library does not contain the actual op, as you can see by my tests under the source code section.\n\n\nAfter a long time, I discover the extremely obvious problem: the example code in the documentation does not have any REGISTER_OP macro call.  This is clearly an omission in the documentation.\n\n\nI try copying the REGISTER_OP macro call from the zero_out example, and it doesn't really work since the \"example\" example is more complicated, but python now at least recognizes the op.  I am sure if I were to spend the time to figure out how to correctly call the REGISTER_OP macro in example.cc, the example would work correctly for me.\nI could make some other recommendations about changing the \"Adding a New Op\" documentation page, though I will refrain for now since I am not sure this is the appropriate place to do so.  I would be happy to take a stab at editing the doc page myself, though since I am very new to github I am concerned that I would end up creating more problems than I would fix, so it might be better to have someone with more experience do it.  I would be happy to help though.\nOne thing I will say though: I would really love it if some TF expert would add a fully fleshed out template op to the user op documentation page with all the bells and whistles, and with clear instructions on exactly how to compile and run it.  It should have both GPU support and a gradient implementation.  And ideally it would be multi-threaded (if the \"example\" example isn't already - its not obvious to me either way).  The template would be provided with everything you need, and would have a clearly labeled code block where the user can add their own code:  \"here is a for loop that iterates over every element in the tensor.  write whatever you want here.\"  If you can make it really easy for users to add their own, high-quality operators, you might be able to cut down on your workload responding to problems with custom ops and with users requesting new ops.  And I think that a template that we can fill in would be enough to do that for many people.\n\n\nSource code / logs\n=========================================== start test.py:\nimport tensorflow as tf\nzero_out_module = tf.load_op_library('./zero_out.so')\nwith tf.Session(''):\nprint \"zero_out:\"\nprint(zero_out_module.zero_out([[1, 2], [3, 4]]).eval())\nexample_module = tf.load_op_library('./example.so')\nwith tf.Session(''):\nprint \"example:\"\ntry:\nprint(example_module.example([[1, 2], [3, 4]]).eval())\nexcept AttributeError as err:\nprint \"Error: \", err\nprint \"Analysis of zero_out_module:\"\nprint zero_out_module.dict.keys()\nprint zero_out_module.OP_LIST\nprint \"Analysis of example_module:\"\nprint example_module.dict.keys()\nprint example_module.OP_LIST\n====================================== end test.py\nOutput of running test.py\neric@EricDesktop:~/tensorflow/tensorflow/core/user_ops$ python test.py2017-09-24 14:14:08.183509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-09-24 14:14:08.183794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:\nname: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455\npciBusID: 0000:04:00.0\ntotalMemory: 3.94GiB freeMemory: 3.66GiB\n2017-09-24 14:14:08.208238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-09-24 14:14:08.208514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 1 with properties:\nname: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455\npciBusID: 0000:03:00.0\ntotalMemory: 3.94GiB freeMemory: 3.66GiB\n2017-09-24 14:14:08.208964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:980] Device peer to peer matrix\n2017-09-24 14:14:08.208988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] DMA: 0 1\n2017-09-24 14:14:08.208994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 0:   Y Y\n2017-09-24 14:14:08.209000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 1:   Y Y\n2017-09-24 14:14:08.209012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)\n2017-09-24 14:14:08.209020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)\nzero_out:\n[[1 0]\n[0 0]]\n2017-09-24 14:14:08.239952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)\n2017-09-24 14:14:08.239970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)\nexample:\nError:  'module' object has no attribute 'example'\nAnalysis of zero_out_module:\n['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', 'builtins', 'zero_out', 'package', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', 'name', '_InitOpDefLibrary', 'doc']\nop {\nname: \"ZeroOut\"\ninput_arg {\nname: \"to_zero\"\ntype: DT_INT32\n}\noutput_arg {\nname: \"zeroed\"\ntype: DT_INT32\n}\n}\nAnalysis of example_module:\n['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', 'builtins', 'package', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', 'name', '_InitOpDefLibrary', 'doc']", "body": "== cat /etc/issue ===============================================\r\nLinux EricDesktop 4.4.0-96-generic #119-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux EricDesktop 4.4.0-96-generic #119-Ubuntu SMP Tue Sep 12 14:59:54 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.6)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc1-2409-g5ee3804\r\ntf.COMPILER_VERSION = v1.3.0-rc1-2409-g5ee3804\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSun Sep 24 14:08:03 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 670     Off  | 0000:03:00.0     N/A |                  N/A |\r\n| 33%   50C    P0    N/A /  N/A |    254MiB /  4031MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 670     Off  | 0000:04:00.0     N/A |                  N/A |\r\n| 32%   48C    P0    N/A /  N/A |    253MiB /  4036MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0                  Not Supported                                         |\r\n|    1                  Not Supported                                         |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n\r\n### Describe the problem\r\n\r\n    I have been trying to get a custom op working for many hours, and have finally collected enough information to know that it isn't entirely my fault, and I can help you improve the custom op documentation to save others such effort.  \r\nWalkthrough to see what the problem is:\r\n1) Following the instructions on the documentation page (https://www.tensorflow.org/extend/adding_an_op) I copy the zero_out.cc code\r\n2) Compile with Bazel\r\n3) Test with python.  Everything works, this example behaves exactly as it should.\r\n4) But I want a GPU kernel, so now I try the \"example\" example.\r\nCopy the example.h, example.cc and example.cu.cc files on the documentation page.  Didn't make any changes.\r\n5) Now I hit some problems.  The documentation isn't very clear on how to compile this more complicated op with bazel.  I managed to figure it out, but I would strongly recommend notifying users about the \"gpu_srcs\" argument that can be used inside the BUILD file, since this took me quite a while to discover.\r\n\r\nWorking BUILD file:\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\r\n\r\ntf_custom_op_library(\r\n    name = \"example.so\",\r\n    srcs = [\"example.cc\", \"example.h\"],\r\n    gpu_srcs = [\"example.cu.cc\", \"example.h\"],\r\n)\r\nWorking Bazel command to invoke the BUILD file:\r\nbazel build --config opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/core/user_ops:example.so\r\n\r\n6) Now it compiles.  But the op does not load properly in python; the module created by tf.load_op_library does not contain the actual op, as you can see by my tests under the source code section.\r\n7) After a long time, I discover the extremely obvious problem: the example code in the documentation does not have any REGISTER_OP macro call.  This is clearly an omission in the documentation.\r\n8) I try copying the REGISTER_OP macro call from the zero_out example, and it doesn't really work since the \"example\" example is more complicated, but python now at least recognizes the op.  I am sure if I were to spend the time to figure out how to correctly call the REGISTER_OP macro in example.cc, the example would work correctly for me.\r\n\r\n    I could make some other recommendations about changing the \"Adding a New Op\" documentation page, though I will refrain for now since I am not sure this is the appropriate place to do so.  I would be happy to take a stab at editing the doc page myself, though since I am very new to github I am concerned that I would end up creating more problems than I would fix, so it might be better to have someone with more experience do it.  I would be happy to help though.\r\n\r\n    One thing I will say though: I would really love it if some TF expert would add a fully fleshed out template op to the user op documentation page with all the bells and whistles, and with clear instructions on exactly how to compile and run it.  It should have both GPU support and a gradient implementation.  And ideally it would be multi-threaded (if the \"example\" example isn't already - its not obvious to me either way).  The template would be provided with everything you need, and would have a clearly labeled code block where the user can add their own code:  \"here is a for loop that iterates over every element in the tensor.  write whatever you want here.\"  If you can make it really easy for users to add their own, high-quality operators, you might be able to cut down on your workload responding to problems with custom ops and with users requesting new ops.  And I think that a template that we can fill in would be enough to do that for many people.\r\n\r\n### Source code / logs\r\n\r\n=========================================== start test.py:\r\nimport tensorflow as tf\r\nzero_out_module = tf.load_op_library('./zero_out.so')\r\nwith tf.Session(''):\r\n  print \"zero_out:\"\r\n  print(zero_out_module.zero_out([[1, 2], [3, 4]]).eval())\r\n\r\nexample_module = tf.load_op_library('./example.so')\r\nwith tf.Session(''):\r\n  print \"example:\"\r\n  try:\r\n    print(example_module.example([[1, 2], [3, 4]]).eval())\r\n  except AttributeError as err:\r\n    print \"Error: \", err\r\n    \r\n  \r\nprint \"Analysis of zero_out_module:\"  \r\nprint zero_out_module.__dict__.keys()\r\nprint zero_out_module.OP_LIST\r\n\r\nprint \"Analysis of example_module:\"\r\nprint example_module.__dict__.keys()\r\nprint example_module.OP_LIST\r\n\r\n====================================== end test.py\r\n\r\nOutput of running test.py\r\neric@EricDesktop:~/tensorflow/tensorflow/core/user_ops$ python test.py2017-09-24 14:14:08.183509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-24 14:14:08.183794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: \r\nname: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 3.94GiB freeMemory: 3.66GiB\r\n2017-09-24 14:14:08.208238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-24 14:14:08.208514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 1 with properties: \r\nname: GeForce GTX 670 major: 3 minor: 0 memoryClockRate(GHz): 1.0455\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 3.94GiB freeMemory: 3.66GiB\r\n2017-09-24 14:14:08.208964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:980] Device peer to peer matrix\r\n2017-09-24 14:14:08.208988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] DMA: 0 1 \r\n2017-09-24 14:14:08.208994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 0:   Y Y \r\n2017-09-24 14:14:08.209000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:996] 1:   Y Y \r\n2017-09-24 14:14:08.209012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)\r\n2017-09-24 14:14:08.209020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)\r\nzero_out:\r\n[[1 0]\r\n [0 0]]\r\n2017-09-24 14:14:08.239952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 670, pci bus id: 0000:04:00.0, compute capability: 3.0)\r\n2017-09-24 14:14:08.239970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 670, pci bus id: 0000:03:00.0, compute capability: 3.0)\r\nexample:\r\nError:  'module' object has no attribute 'example'\r\nAnalysis of zero_out_module:\r\n['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', '__builtins__', 'zero_out', '__package__', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', '__name__', '_InitOpDefLibrary', '__doc__']\r\nop {\r\n  name: \"ZeroOut\"\r\n  input_arg {\r\n    name: \"to_zero\"\r\n    type: DT_INT32\r\n  }\r\n  output_arg {\r\n    name: \"zeroed\"\r\n    type: DT_INT32\r\n  }\r\n}\r\n\r\nAnalysis of example_module:\r\n['_op_def_pb2', '_op_def_lib', '_op_def_registry', '_ops', '_collections', '_common_shapes', '__builtins__', '__package__', '_op_def_library', 'OP_LIST', 'LIB_HANDLE', '__name__', '_InitOpDefLibrary', '__doc__']\r\n\r\n\r\n"}