{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15269", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15269/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15269/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15269/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15269", "id": 280976450, "node_id": "MDU6SXNzdWUyODA5NzY0NTA=", "number": 15269, "title": "tf.layers.Layer.set_scope() problem might cause unexpected duplicate name ValueError", "user": {"login": "adamcavendish", "id": 1346161, "node_id": "MDQ6VXNlcjEzNDYxNjE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1346161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamcavendish", "html_url": "https://github.com/adamcavendish", "followers_url": "https://api.github.com/users/adamcavendish/followers", "following_url": "https://api.github.com/users/adamcavendish/following{/other_user}", "gists_url": "https://api.github.com/users/adamcavendish/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamcavendish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamcavendish/subscriptions", "organizations_url": "https://api.github.com/users/adamcavendish/orgs", "repos_url": "https://api.github.com/users/adamcavendish/repos", "events_url": "https://api.github.com/users/adamcavendish/events{/privacy}", "received_events_url": "https://api.github.com/users/adamcavendish/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2017-12-11T11:05:17Z", "updated_at": "2018-03-09T03:07:46Z", "closed_at": "2018-03-09T03:07:45Z", "author_association": "NONE", "body_html": "<ul>\n<li><strong>OS Platform and Distribution</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version</strong>: 1.4.1</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 8.0, CUDNN 7.0.5</li>\n<li><strong>GPU model and memory</strong>: GTX1080 (8G)</li>\n<li><strong>Exact command to reproduce</strong>: python3 test.py</li>\n<li><strong>Have I written custom code</strong>: True</li>\n<li><strong>Bazel version</strong>: N/A</li>\n<li><strong>GCC version</strong>: N/A</li>\n</ul>\n<p>Repoduce code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c1\">_BATCH_NORM_DECAY</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.997</span>\n<span class=\"pl-c1\">_BATCH_NORM_EPSILON</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1e-5</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">two_batchnorm</span>(<span class=\"pl-smi\">inputs</span>):\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>two_batchnorm<span class=\"pl-pds\">'</span></span>):\n        inputs <span class=\"pl-k\">=</span> tf.layers.batch_normalization(\n            <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>inputs,\n            <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>,\n            <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">_BATCH_NORM_DECAY</span>,\n            <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">_BATCH_NORM_EPSILON</span>,\n            <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        inputs <span class=\"pl-k\">=</span> tf.layers.batch_normalization(\n            <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>inputs,\n            <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>,\n            <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">_BATCH_NORM_DECAY</span>,\n            <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">_BATCH_NORM_EPSILON</span>,\n            <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">return</span> inputs\n\ninputs <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">3</span>])\nx <span class=\"pl-k\">=</span> inputs\nx <span class=\"pl-k\">=</span> two_batchnorm(x)\nx <span class=\"pl-k\">=</span> two_batchnorm(x)</pre></div>\n<p>It'll trigger an unexpected ValueError as following:</p>\n<pre><code>ValueError: Variable two_batchnorm/batch_normalization/gamma already exists, disallowed.\n</code></pre>\n<p>Removing the variable scope <code>with tf.variable_scope('two_batchnorm')</code> in <code>two_batchnorm</code> will work as expected.</p>\n<p>All variables defined in the graph should be (in creation sequense):</p>\n<pre><code>two_batchnorm/batch_normalization/gamma:0\ntwo_batchnorm/batch_normalization/beta:0\ntwo_batchnorm/batch_normalization/moving_mean:0\ntwo_batchnorm/batch_normalization/moving_variance:0\ntwo_batchnorm/batch_normalization_1/gamma:0\ntwo_batchnorm/batch_normalization_1/beta:0\ntwo_batchnorm/batch_normalization_1/moving_mean:0\ntwo_batchnorm/batch_normalization_1/moving_variance:0\ntwo_batchnorm/batch_normalization_2/gamma:0\ntwo_batchnorm/batch_normalization_2/beta:0\ntwo_batchnorm/batch_normalization_2/moving_mean:0\ntwo_batchnorm/batch_normalization_2/moving_variance:0\ntwo_batchnorm/batch_normalization_3/gamma:0\ntwo_batchnorm/batch_normalization_3/beta:0\ntwo_batchnorm/batch_normalization_3/moving_mean:0\ntwo_batchnorm/batch_normalization_3/moving_variance:0\n</code></pre>\n<p>However, with <code>tf.layers.Layer</code>'s <code>add_variable</code> logics, it'll result in an unexpected value name as following (in creation sequence):</p>\n<pre><code>two_batchnorm/batch_normalization/gamma:0\ntwo_batchnorm/batch_normalization/beta:0\ntwo_batchnorm/batch_normalization/moving_mean:0\ntwo_batchnorm/batch_normalization/moving_variance:0\ntwo_batchnorm/batch_normalization_1/gamma:0\ntwo_batchnorm/batch_normalization_1/beta:0\ntwo_batchnorm/batch_normalization_1/moving_mean:0\ntwo_batchnorm/batch_normalization_1/moving_variance:0\ntwo_batchnorm/batch_normalization/gamma:0                        &lt;-- ValueError raised here.\ntwo_batchnorm/batch_normalization/beta:0\ntwo_batchnorm/batch_normalization/moving_mean:0\ntwo_batchnorm/batch_normalization/moving_variance:0\ntwo_batchnorm/batch_normalization_1/gamma:0\ntwo_batchnorm/batch_normalization_1/beta:0\ntwo_batchnorm/batch_normalization_1/moving_mean:0\ntwo_batchnorm/batch_normalization_1/moving_variance:0\n</code></pre>\n<p>One solution might be using <code>self._name</code> to setup Layer's scope, not <code>self._base_name</code>.</p>", "body_text": "OS Platform and Distribution: Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version: 1.4.1\nPython version: 3.5.2\nCUDA/cuDNN version: CUDA 8.0, CUDNN 7.0.5\nGPU model and memory: GTX1080 (8G)\nExact command to reproduce: python3 test.py\nHave I written custom code: True\nBazel version: N/A\nGCC version: N/A\n\nRepoduce code:\nimport tensorflow as tf\n\n_BATCH_NORM_DECAY = 0.997\n_BATCH_NORM_EPSILON = 1e-5\n\ndef two_batchnorm(inputs):\n    with tf.variable_scope('two_batchnorm'):\n        inputs = tf.layers.batch_normalization(\n            inputs=inputs,\n            axis=3,\n            momentum=_BATCH_NORM_DECAY,\n            epsilon=_BATCH_NORM_EPSILON,\n            center=True,\n            scale=True,\n            training=True,\n            fused=True)\n        inputs = tf.layers.batch_normalization(\n            inputs=inputs,\n            axis=3,\n            momentum=_BATCH_NORM_DECAY,\n            epsilon=_BATCH_NORM_EPSILON,\n            center=True,\n            scale=True,\n            training=True,\n            fused=True)\n    return inputs\n\ninputs = tf.placeholder(tf.float32, [1, 5, 5, 3])\nx = inputs\nx = two_batchnorm(x)\nx = two_batchnorm(x)\nIt'll trigger an unexpected ValueError as following:\nValueError: Variable two_batchnorm/batch_normalization/gamma already exists, disallowed.\n\nRemoving the variable scope with tf.variable_scope('two_batchnorm') in two_batchnorm will work as expected.\nAll variables defined in the graph should be (in creation sequense):\ntwo_batchnorm/batch_normalization/gamma:0\ntwo_batchnorm/batch_normalization/beta:0\ntwo_batchnorm/batch_normalization/moving_mean:0\ntwo_batchnorm/batch_normalization/moving_variance:0\ntwo_batchnorm/batch_normalization_1/gamma:0\ntwo_batchnorm/batch_normalization_1/beta:0\ntwo_batchnorm/batch_normalization_1/moving_mean:0\ntwo_batchnorm/batch_normalization_1/moving_variance:0\ntwo_batchnorm/batch_normalization_2/gamma:0\ntwo_batchnorm/batch_normalization_2/beta:0\ntwo_batchnorm/batch_normalization_2/moving_mean:0\ntwo_batchnorm/batch_normalization_2/moving_variance:0\ntwo_batchnorm/batch_normalization_3/gamma:0\ntwo_batchnorm/batch_normalization_3/beta:0\ntwo_batchnorm/batch_normalization_3/moving_mean:0\ntwo_batchnorm/batch_normalization_3/moving_variance:0\n\nHowever, with tf.layers.Layer's add_variable logics, it'll result in an unexpected value name as following (in creation sequence):\ntwo_batchnorm/batch_normalization/gamma:0\ntwo_batchnorm/batch_normalization/beta:0\ntwo_batchnorm/batch_normalization/moving_mean:0\ntwo_batchnorm/batch_normalization/moving_variance:0\ntwo_batchnorm/batch_normalization_1/gamma:0\ntwo_batchnorm/batch_normalization_1/beta:0\ntwo_batchnorm/batch_normalization_1/moving_mean:0\ntwo_batchnorm/batch_normalization_1/moving_variance:0\ntwo_batchnorm/batch_normalization/gamma:0                        <-- ValueError raised here.\ntwo_batchnorm/batch_normalization/beta:0\ntwo_batchnorm/batch_normalization/moving_mean:0\ntwo_batchnorm/batch_normalization/moving_variance:0\ntwo_batchnorm/batch_normalization_1/gamma:0\ntwo_batchnorm/batch_normalization_1/beta:0\ntwo_batchnorm/batch_normalization_1/moving_mean:0\ntwo_batchnorm/batch_normalization_1/moving_variance:0\n\nOne solution might be using self._name to setup Layer's scope, not self._base_name.", "body": "- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version**: 1.4.1\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: CUDA 8.0, CUDNN 7.0.5\r\n- **GPU model and memory**: GTX1080 (8G)\r\n- **Exact command to reproduce**: python3 test.py\r\n- **Have I written custom code**: True\r\n- **Bazel version**: N/A\r\n- **GCC version**: N/A\r\n\r\n\r\nRepoduce code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n_BATCH_NORM_DECAY = 0.997\r\n_BATCH_NORM_EPSILON = 1e-5\r\n\r\ndef two_batchnorm(inputs):\r\n    with tf.variable_scope('two_batchnorm'):\r\n        inputs = tf.layers.batch_normalization(\r\n            inputs=inputs,\r\n            axis=3,\r\n            momentum=_BATCH_NORM_DECAY,\r\n            epsilon=_BATCH_NORM_EPSILON,\r\n            center=True,\r\n            scale=True,\r\n            training=True,\r\n            fused=True)\r\n        inputs = tf.layers.batch_normalization(\r\n            inputs=inputs,\r\n            axis=3,\r\n            momentum=_BATCH_NORM_DECAY,\r\n            epsilon=_BATCH_NORM_EPSILON,\r\n            center=True,\r\n            scale=True,\r\n            training=True,\r\n            fused=True)\r\n    return inputs\r\n\r\ninputs = tf.placeholder(tf.float32, [1, 5, 5, 3])\r\nx = inputs\r\nx = two_batchnorm(x)\r\nx = two_batchnorm(x)\r\n```\r\n\r\nIt'll trigger an unexpected ValueError as following:\r\n\r\n```\r\nValueError: Variable two_batchnorm/batch_normalization/gamma already exists, disallowed.\r\n```\r\n\r\nRemoving the variable scope `with tf.variable_scope('two_batchnorm')` in `two_batchnorm` will work as expected.\r\n\r\nAll variables defined in the graph should be (in creation sequense):\r\n\r\n```\r\ntwo_batchnorm/batch_normalization/gamma:0\r\ntwo_batchnorm/batch_normalization/beta:0\r\ntwo_batchnorm/batch_normalization/moving_mean:0\r\ntwo_batchnorm/batch_normalization/moving_variance:0\r\ntwo_batchnorm/batch_normalization_1/gamma:0\r\ntwo_batchnorm/batch_normalization_1/beta:0\r\ntwo_batchnorm/batch_normalization_1/moving_mean:0\r\ntwo_batchnorm/batch_normalization_1/moving_variance:0\r\ntwo_batchnorm/batch_normalization_2/gamma:0\r\ntwo_batchnorm/batch_normalization_2/beta:0\r\ntwo_batchnorm/batch_normalization_2/moving_mean:0\r\ntwo_batchnorm/batch_normalization_2/moving_variance:0\r\ntwo_batchnorm/batch_normalization_3/gamma:0\r\ntwo_batchnorm/batch_normalization_3/beta:0\r\ntwo_batchnorm/batch_normalization_3/moving_mean:0\r\ntwo_batchnorm/batch_normalization_3/moving_variance:0\r\n```\r\n\r\nHowever, with `tf.layers.Layer`'s `add_variable` logics, it'll result in an unexpected value name as following (in creation sequence):\r\n\r\n```\r\ntwo_batchnorm/batch_normalization/gamma:0\r\ntwo_batchnorm/batch_normalization/beta:0\r\ntwo_batchnorm/batch_normalization/moving_mean:0\r\ntwo_batchnorm/batch_normalization/moving_variance:0\r\ntwo_batchnorm/batch_normalization_1/gamma:0\r\ntwo_batchnorm/batch_normalization_1/beta:0\r\ntwo_batchnorm/batch_normalization_1/moving_mean:0\r\ntwo_batchnorm/batch_normalization_1/moving_variance:0\r\ntwo_batchnorm/batch_normalization/gamma:0                        <-- ValueError raised here.\r\ntwo_batchnorm/batch_normalization/beta:0\r\ntwo_batchnorm/batch_normalization/moving_mean:0\r\ntwo_batchnorm/batch_normalization/moving_variance:0\r\ntwo_batchnorm/batch_normalization_1/gamma:0\r\ntwo_batchnorm/batch_normalization_1/beta:0\r\ntwo_batchnorm/batch_normalization_1/moving_mean:0\r\ntwo_batchnorm/batch_normalization_1/moving_variance:0\r\n```\r\n\r\nOne solution might be using `self._name` to setup Layer's scope, not `self._base_name`."}