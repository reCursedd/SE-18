{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/292062723", "html_url": "https://github.com/tensorflow/tensorflow/issues/8911#issuecomment-292062723", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8911", "id": 292062723, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MjA2MjcyMw==", "user": {"login": "AshishBora", "id": 4586769, "node_id": "MDQ6VXNlcjQ1ODY3Njk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4586769?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AshishBora", "html_url": "https://github.com/AshishBora", "followers_url": "https://api.github.com/users/AshishBora/followers", "following_url": "https://api.github.com/users/AshishBora/following{/other_user}", "gists_url": "https://api.github.com/users/AshishBora/gists{/gist_id}", "starred_url": "https://api.github.com/users/AshishBora/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AshishBora/subscriptions", "organizations_url": "https://api.github.com/users/AshishBora/orgs", "repos_url": "https://api.github.com/users/AshishBora/repos", "events_url": "https://api.github.com/users/AshishBora/events{/privacy}", "received_events_url": "https://api.github.com/users/AshishBora/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-06T03:59:36Z", "updated_at": "2017-04-06T03:59:54Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8366121\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/siryog90\">@siryog90</a></p>\n<p>Yes, I think we should be using the original network by replacing weights with ones raised to power <code>p</code>.</p>\n<p>It does not matter whether you use the original network or create a new one with identities. This is because the input (all ones tensor) is positive, and absolute value of (weights)^p is positive, each neuron will have positive activation and the ReLU wouldn't change anything.</p>\n<p>More generally the PathNorm makes sense for any positive homogeneous non-linearity i.e. if</p>\n<p><code>g(ax) = ag(x) for all x, and all a &gt;= 0</code>.</p>\n<p>This technique of passing all ones tensor will work with a minor modification accounting for the slope on the positive part for any such non-linearity. But for a first pass, we can focus on ReLUs and worry about this later.</p>", "body_text": "@siryog90\nYes, I think we should be using the original network by replacing weights with ones raised to power p.\nIt does not matter whether you use the original network or create a new one with identities. This is because the input (all ones tensor) is positive, and absolute value of (weights)^p is positive, each neuron will have positive activation and the ReLU wouldn't change anything.\nMore generally the PathNorm makes sense for any positive homogeneous non-linearity i.e. if\ng(ax) = ag(x) for all x, and all a >= 0.\nThis technique of passing all ones tensor will work with a minor modification accounting for the slope on the positive part for any such non-linearity. But for a first pass, we can focus on ReLUs and worry about this later.", "body": "@siryog90 \r\n\r\nYes, I think we should be using the original network by replacing weights with ones raised to power `p`.\r\n\r\nIt does not matter whether you use the original network or create a new one with identities. This is because the input (all ones tensor) is positive, and absolute value of (weights)^p is positive, each neuron will have positive activation and the ReLU wouldn't change anything.\r\n\r\nMore generally the PathNorm makes sense for any positive homogeneous non-linearity i.e. if \r\n\r\n`g(ax) = ag(x) for all x, and all a >= 0`.\r\n\r\nThis technique of passing all ones tensor will work with a minor modification accounting for the slope on the positive part for any such non-linearity. But for a first pass, we can focus on ReLUs and worry about this later.\r\n"}