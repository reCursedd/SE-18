{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/292081572", "html_url": "https://github.com/tensorflow/tensorflow/issues/8911#issuecomment-292081572", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8911", "id": 292081572, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MjA4MTU3Mg==", "user": {"login": "AshishBora", "id": 4586769, "node_id": "MDQ6VXNlcjQ1ODY3Njk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4586769?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AshishBora", "html_url": "https://github.com/AshishBora", "followers_url": "https://api.github.com/users/AshishBora/followers", "following_url": "https://api.github.com/users/AshishBora/following{/other_user}", "gists_url": "https://api.github.com/users/AshishBora/gists{/gist_id}", "starred_url": "https://api.github.com/users/AshishBora/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AshishBora/subscriptions", "organizations_url": "https://api.github.com/users/AshishBora/orgs", "repos_url": "https://api.github.com/users/AshishBora/repos", "events_url": "https://api.github.com/users/AshishBora/events{/privacy}", "received_events_url": "https://api.github.com/users/AshishBora/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-06T06:31:38Z", "updated_at": "2017-04-06T06:37:17Z", "author_association": "NONE", "body_html": "<p>Also, I think a more extensible approach can be to use a context manager.</p>\n<p>All weight variables defined within that context automatically have pseudo weight tensors created for them that are <code>pow(abs(weight), p)</code>. All new bias variables will create pseudo bias of the same size but all zeros. Then every time a new tensor is created within that context, a pseudo tensor is created in parallel to mimic the operation but with pseudo weights, biases, and previously computed pseudotensors. This essentially creates the parallel copy of the tensorflow graph created within that context, but with pseudo things.</p>\n<p>Now when you ask for a <code>Lp-PathNorm</code> between a and b: First check if <code>a</code> and <code>b</code> have pseudo counterparts. (maybe maintain a dictionary mapping from real to pseudo). If not, then raise exception. If yes, then return<br>\n<code>sess.run(pow(reduce_sum(b_pseudo), 1.0/p), feed_dict={a_pseudo: ones_like(a)})</code></p>\n<p>This should return the <code>Lp-PathNorm</code>.</p>\n<p>Going further, we would like this to be differentiable, i.e. not just returning a value but a tensor. So I guess there can be another function <code>make_parallel_graph(a, model_fn)</code>, where model_fn is a function that creates the model from, i.e. <code>b = model_fn(a)</code>.</p>\n<p>This <code>make_parallel_graph</code> function should first create <code>a_pseudo = ones_like(a)</code>. Then call <code>model_fn</code> in the context defined above making parallel graph all the way. Finally it should return <code>(b, pow(reduce_sum(b_pseudo), 1.0/p))</code>,<br>\nwhere  the second term in the tuple is the basically the <code>Lp-PathNorm</code> we wanted. This is then a differentiable function of the weights and can added to the total loss or whatever.</p>\n<p>I like this context manager based approach because it can work with convolutional networks or any other complex topology as well.</p>", "body_text": "Also, I think a more extensible approach can be to use a context manager.\nAll weight variables defined within that context automatically have pseudo weight tensors created for them that are pow(abs(weight), p). All new bias variables will create pseudo bias of the same size but all zeros. Then every time a new tensor is created within that context, a pseudo tensor is created in parallel to mimic the operation but with pseudo weights, biases, and previously computed pseudotensors. This essentially creates the parallel copy of the tensorflow graph created within that context, but with pseudo things.\nNow when you ask for a Lp-PathNorm between a and b: First check if a and b have pseudo counterparts. (maybe maintain a dictionary mapping from real to pseudo). If not, then raise exception. If yes, then return\nsess.run(pow(reduce_sum(b_pseudo), 1.0/p), feed_dict={a_pseudo: ones_like(a)})\nThis should return the Lp-PathNorm.\nGoing further, we would like this to be differentiable, i.e. not just returning a value but a tensor. So I guess there can be another function make_parallel_graph(a, model_fn), where model_fn is a function that creates the model from, i.e. b = model_fn(a).\nThis make_parallel_graph function should first create a_pseudo = ones_like(a). Then call model_fn in the context defined above making parallel graph all the way. Finally it should return (b, pow(reduce_sum(b_pseudo), 1.0/p)),\nwhere  the second term in the tuple is the basically the Lp-PathNorm we wanted. This is then a differentiable function of the weights and can added to the total loss or whatever.\nI like this context manager based approach because it can work with convolutional networks or any other complex topology as well.", "body": "Also, I think a more extensible approach can be to use a context manager.\r\n\r\nAll weight variables defined within that context automatically have pseudo weight tensors created for them that are `pow(abs(weight), p)`. All new bias variables will create pseudo bias of the same size but all zeros. Then every time a new tensor is created within that context, a pseudo tensor is created in parallel to mimic the operation but with pseudo weights, biases, and previously computed pseudotensors. This essentially creates the parallel copy of the tensorflow graph created within that context, but with pseudo things.\r\n\r\nNow when you ask for a `Lp-PathNorm` between a and b: First check if `a` and `b` have pseudo counterparts. (maybe maintain a dictionary mapping from real to pseudo). If not, then raise exception. If yes, then return\r\n`sess.run(pow(reduce_sum(b_pseudo), 1.0/p), feed_dict={a_pseudo: ones_like(a)})`\r\n\r\nThis should return the `Lp-PathNorm`.\r\n\r\nGoing further, we would like this to be differentiable, i.e. not just returning a value but a tensor. So I guess there can be another function `make_parallel_graph(a, model_fn)`, where model_fn is a function that creates the model from, i.e. `b = model_fn(a)`.\r\n\r\nThis `make_parallel_graph` function should first create `a_pseudo = ones_like(a)`. Then call `model_fn` in the context defined above making parallel graph all the way. Finally it should return `(b, pow(reduce_sum(b_pseudo), 1.0/p))`,\r\nwhere  the second term in the tuple is the basically the `Lp-PathNorm` we wanted. This is then a differentiable function of the weights and can added to the total loss or whatever.\r\n\r\nI like this context manager based approach because it can work with convolutional networks or any other complex topology as well."}