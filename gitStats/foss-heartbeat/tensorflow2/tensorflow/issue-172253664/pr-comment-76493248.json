{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/76493248", "pull_request_review_id": null, "id": 76493248, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc2NDkzMjQ4", "diff_hunk": "@@ -137,128 +137,133 @@ def main(unused_argv):\n     raise ValueError(\"Invalid num_parameter_servers value: %d\" %\n                      FLAGS.num_parameter_servers)\n \n-  is_chief = (FLAGS.worker_index == 0)\n-\n-  if FLAGS.sync_replicas:\n-    if FLAGS.replicas_to_aggregate is None:\n-      replicas_to_aggregate = FLAGS.num_workers\n-    else:\n-      replicas_to_aggregate = FLAGS.replicas_to_aggregate\n-\n-  # Construct device setter object\n-  device_setter = get_device_setter(FLAGS.num_parameter_servers,\n-                                    FLAGS.num_workers)\n-\n-  # The device setter will automatically place Variables ops on separate\n-  # parameter servers (ps). The non-Variable ops will be placed on the workers.\n-  with tf.device(device_setter):\n-    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n-\n-    # Variables of the hidden layer\n-    hid_w = tf.Variable(\n-        tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\n-                            stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\n-    hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name=\"hid_b\")\n-\n-    # Variables of the softmax layer\n-    sm_w = tf.Variable(\n-        tf.truncated_normal([FLAGS.hidden_units, 10],\n-                            stddev=1.0 / math.sqrt(FLAGS.hidden_units)),\n-        name=\"sm_w\")\n-    sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n-\n-    # Ops: located on the worker specified with FLAGS.worker_index\n-    x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n-    y_ = tf.placeholder(tf.float32, [None, 10])\n-\n-    hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n-    hid = tf.nn.relu(hid_lin)\n+  # Construct cluster setter object\n+  cluster, server = get_cluster_setter()\n+  \n+  if FLAGS.job_name == \"ps\":\n+    server.join()\n+  elif FLAGS.job_name == \"worker\":\n \n-    y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n-    cross_entropy = -tf.reduce_sum(y_ *\n-                                   tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n+    is_chief = (FLAGS.worker_index == 0)\n \n-    opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n     if FLAGS.sync_replicas:\n-      opt = tf.train.SyncReplicasOptimizer(\n-          opt,\n-          replicas_to_aggregate=replicas_to_aggregate,\n-          total_num_replicas=FLAGS.num_workers,\n-          replica_id=FLAGS.worker_index,\n-          name=\"mnist_sync_replicas\")\n-\n-    train_step = opt.minimize(cross_entropy,\n-                              global_step=global_step)\n-\n-    if FLAGS.sync_replicas and is_chief:\n-      # Initial token and chief queue runners required by the sync_replicas mode\n-      chief_queue_runner = opt.get_chief_queue_runner()\n-      init_tokens_op = opt.get_init_tokens_op()\n-\n-    init_op = tf.initialize_all_variables()\n-    train_dir = tempfile.mkdtemp()\n-    sv = tf.train.Supervisor(is_chief=is_chief,\n-                             logdir=train_dir,\n-                             init_op=init_op,\n-                             recovery_wait_secs=1,\n-                             global_step=global_step)\n-\n-    sess_config = tf.ConfigProto(\n-        allow_soft_placement=True,\n-        log_device_placement=True,\n-        device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.worker_index])\n-\n-    # The chief worker (worker_index==0) session will prepare the session,\n-    # while the remaining workers will wait for the preparation to complete.\n-    if is_chief:\n-      print(\"Worker %d: Initializing session...\" % FLAGS.worker_index)\n-    else:\n-      print(\"Worker %d: Waiting for session to be initialized...\" %\n-            FLAGS.worker_index)\n-\n-    sess = sv.prepare_or_wait_for_session(FLAGS.worker_grpc_url,\n-                                          config=sess_config)\n-\n-    print(\"Worker %d: Session initialization complete.\" % FLAGS.worker_index)\n-\n-    if FLAGS.sync_replicas and is_chief:\n-      # Chief worker will start the chief queue runner and call the init op\n-      print(\"Starting chief queue runner and running init_tokens_op\")\n-      sv.start_queue_runners(sess, [chief_queue_runner])\n-      sess.run(init_tokens_op)\n-\n-    # Perform training\n-    time_begin = time.time()\n-    print(\"Training begins @ %f\" % time_begin)\n-\n-    local_step = 0\n-    while True:\n-      # Training feed\n-      batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n-      train_feed = {x: batch_xs,\n-                    y_: batch_ys}\n-\n-      _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n-      local_step += 1\n-\n-      now = time.time()\n-      print(\"%f: Worker %d: training step %d done (global step: %d)\" %\n-            (now, FLAGS.worker_index, local_step, step))\n-\n-      if step >= FLAGS.train_steps:\n-        break\n-\n-    time_end = time.time()\n-    print(\"Training ends @ %f\" % time_end)\n-    training_time = time_end - time_begin\n-    print(\"Training elapsed time: %f s\" % training_time)\n-\n-    # Validation feed\n-    val_feed = {x: mnist.validation.images,\n-                y_: mnist.validation.labels}\n-    val_xent = sess.run(cross_entropy, feed_dict=val_feed)\n-    print(\"After %d training step(s), validation cross entropy = %g\" %\n-          (FLAGS.train_steps, val_xent))\n+      if FLAGS.replicas_to_aggregate is None:\n+        replicas_to_aggregate = FLAGS.num_workers\n+      else:\n+        replicas_to_aggregate = FLAGS.replicas_to_aggregate", "path": "tensorflow/tools/dist_test/python/mnist_replica.py", "position": null, "original_position": 217, "commit_id": "aa18b64322020939d36b30114e74622754f99f88", "original_commit_id": "6035c7a6902acfdb22e50927fa84fbfc9eca7b2e", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "body": "Move this `if` statement to the same block where the `tf.train.SyncReplicasOptimizer` is constructed.\n", "created_at": "2016-08-26T21:47:20Z", "updated_at": "2016-09-08T09:52:28Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/3935#discussion_r76493248", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3935", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/76493248"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/3935#discussion_r76493248"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3935"}}, "body_html": "<p>Move this <code>if</code> statement to the same block where the <code>tf.train.SyncReplicasOptimizer</code> is constructed.</p>", "body_text": "Move this if statement to the same block where the tf.train.SyncReplicasOptimizer is constructed."}