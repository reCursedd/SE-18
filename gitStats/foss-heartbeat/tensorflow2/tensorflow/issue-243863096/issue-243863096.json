{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11591", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11591/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11591/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11591/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11591", "id": 243863096, "node_id": "MDU6SXNzdWUyNDM4NjMwOTY=", "number": 11591, "title": "slice_input_producer bug (slows down exponentially for larger datasets)", "user": {"login": "jrbtaylor", "id": 15191858, "node_id": "MDQ6VXNlcjE1MTkxODU4", "avatar_url": "https://avatars3.githubusercontent.com/u/15191858?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrbtaylor", "html_url": "https://github.com/jrbtaylor", "followers_url": "https://api.github.com/users/jrbtaylor/followers", "following_url": "https://api.github.com/users/jrbtaylor/following{/other_user}", "gists_url": "https://api.github.com/users/jrbtaylor/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrbtaylor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrbtaylor/subscriptions", "organizations_url": "https://api.github.com/users/jrbtaylor/orgs", "repos_url": "https://api.github.com/users/jrbtaylor/repos", "events_url": "https://api.github.com/users/jrbtaylor/events{/privacy}", "received_events_url": "https://api.github.com/users/jrbtaylor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-07-18T21:54:02Z", "updated_at": "2018-03-23T17:34:57Z", "closed_at": "2017-07-25T00:43:09Z", "author_association": "NONE", "body_html": "<p>My input pipeline involves sampling a filename and label with slice_input_producer, then data augmentation, then batching with tf.train.shuffle_batch. For smaller datasets (&lt;1M), this works fine. For larger datasets, slice_input_producer slows down, even though all it's doing is sampling from two lists. It's bad enough that it's several orders of magnitude slower than all the rest of my input pipeline and training combined. I wrote a quick test to measure the time per call to slice_input_producer for different lengths of input lists:</p>\n<pre><code>import time\n\nimport tensorflow as tf\n\nrept = 1000\nfor datasize in [1e3,1e6,1e7,1e8]:\n    sym_x = tf.train.slice_input_producer(\n        [list(range(int(datasize)))], shuffle=True,\n        capacity=10)\n    with tf.Session() as sess:\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.global_variables_initializer())\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n        start_time = time.time()\n        for r in range(rept):\n            x = sess.run(sym_x)\n        print('Datalength: %i  ---  time = %.3f ms/sample'\n              % (datasize,(time.time()-start_time)/rept*1000))\n        coord.request_stop()\n        coord.join(threads)\n        sess.close()\n</code></pre>\n<p>Which gives:</p>\n<blockquote>\n<p>Datalength: 1000  ---  time = 0.172 ms/sample<br>\nDatalength: 1000000  ---  time = 0.207 ms/sample<br>\nDatalength: 10000000  ---  time = 0.537 ms/sample<br>\nDatalength: 100000000  ---  time = 13.991 ms/sample</p>\n</blockquote>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"<br>\n('v1.2.0-0-g12f033d', '1.2.0')</p>", "body_text": "My input pipeline involves sampling a filename and label with slice_input_producer, then data augmentation, then batching with tf.train.shuffle_batch. For smaller datasets (<1M), this works fine. For larger datasets, slice_input_producer slows down, even though all it's doing is sampling from two lists. It's bad enough that it's several orders of magnitude slower than all the rest of my input pipeline and training combined. I wrote a quick test to measure the time per call to slice_input_producer for different lengths of input lists:\nimport time\n\nimport tensorflow as tf\n\nrept = 1000\nfor datasize in [1e3,1e6,1e7,1e8]:\n    sym_x = tf.train.slice_input_producer(\n        [list(range(int(datasize)))], shuffle=True,\n        capacity=10)\n    with tf.Session() as sess:\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.global_variables_initializer())\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n        start_time = time.time()\n        for r in range(rept):\n            x = sess.run(sym_x)\n        print('Datalength: %i  ---  time = %.3f ms/sample'\n              % (datasize,(time.time()-start_time)/rept*1000))\n        coord.request_stop()\n        coord.join(threads)\n        sess.close()\n\nWhich gives:\n\nDatalength: 1000  ---  time = 0.172 ms/sample\nDatalength: 1000000  ---  time = 0.207 ms/sample\nDatalength: 10000000  ---  time = 0.537 ms/sample\nDatalength: 100000000  ---  time = 13.991 ms/sample\n\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\n('v1.2.0-0-g12f033d', '1.2.0')", "body": "My input pipeline involves sampling a filename and label with slice_input_producer, then data augmentation, then batching with tf.train.shuffle_batch. For smaller datasets (<1M), this works fine. For larger datasets, slice_input_producer slows down, even though all it's doing is sampling from two lists. It's bad enough that it's several orders of magnitude slower than all the rest of my input pipeline and training combined. I wrote a quick test to measure the time per call to slice_input_producer for different lengths of input lists:\r\n```\r\nimport time\r\n\r\nimport tensorflow as tf\r\n\r\nrept = 1000\r\nfor datasize in [1e3,1e6,1e7,1e8]:\r\n    sym_x = tf.train.slice_input_producer(\r\n        [list(range(int(datasize)))], shuffle=True,\r\n        capacity=10)\r\n    with tf.Session() as sess:\r\n        sess.run(tf.local_variables_initializer())\r\n        sess.run(tf.global_variables_initializer())\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n        start_time = time.time()\r\n        for r in range(rept):\r\n            x = sess.run(sym_x)\r\n        print('Datalength: %i  ---  time = %.3f ms/sample'\r\n              % (datasize,(time.time()-start_time)/rept*1000))\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n        sess.close()\r\n```\r\n\r\nWhich gives:\r\n\r\n> Datalength: 1000  ---  time = 0.172 ms/sample\r\n> Datalength: 1000000  ---  time = 0.207 ms/sample\r\n> Datalength: 10000000  ---  time = 0.537 ms/sample\r\n> Datalength: 100000000  ---  time = 13.991 ms/sample\r\n\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.2.0-0-g12f033d', '1.2.0')"}