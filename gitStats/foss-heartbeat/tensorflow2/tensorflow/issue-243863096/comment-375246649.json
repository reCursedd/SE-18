{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/375246649", "html_url": "https://github.com/tensorflow/tensorflow/issues/11591#issuecomment-375246649", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11591", "id": 375246649, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTI0NjY0OQ==", "user": {"login": "kvanhoey", "id": 22071874, "node_id": "MDQ6VXNlcjIyMDcxODc0", "avatar_url": "https://avatars2.githubusercontent.com/u/22071874?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kvanhoey", "html_url": "https://github.com/kvanhoey", "followers_url": "https://api.github.com/users/kvanhoey/followers", "following_url": "https://api.github.com/users/kvanhoey/following{/other_user}", "gists_url": "https://api.github.com/users/kvanhoey/gists{/gist_id}", "starred_url": "https://api.github.com/users/kvanhoey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kvanhoey/subscriptions", "organizations_url": "https://api.github.com/users/kvanhoey/orgs", "repos_url": "https://api.github.com/users/kvanhoey/repos", "events_url": "https://api.github.com/users/kvanhoey/events{/privacy}", "received_events_url": "https://api.github.com/users/kvanhoey/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-22T10:18:28Z", "updated_at": "2018-03-22T10:18:28Z", "author_association": "NONE", "body_html": "<p>Hello.</p>\n<p>I experience the same behavior as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15191858\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jrbtaylor\">@jrbtaylor</a> on a much smaller dataset already (the MNIST data, 60K training examples).<br>\nShuffling progressively slows down the learning iterations, up until the end of an epoch, then it continues at the initial speed again:</p>\n<ul>\n<li>No shuffling:</li>\n</ul>\n<blockquote>\n<p>Elapsed time for 100 iterations: 0.7071466445922852<br>\nElapsed time for 100 iterations: 0.928595781326294<br>\nElapsed time for 100 iterations: 0.9202249050140381<br>\nElapsed time for 100 iterations: 0.9153792858123779<br>\nElapsed time for 100 iterations: 0.9065940380096436<br>\nElapsed time for 100 iterations: 0.9027049541473389<br>\nEPOCH++<br>\nElapsed time for 100 iterations: 0.903191328048706<br>\nElapsed time for 100 iterations: 0.9209485054016113<br>\nElapsed time for 100 iterations: 0.9005069732666016<br>\nElapsed time for 100 iterations: 0.9149377346038818<br>\nElapsed time for 100 iterations: 0.9105048179626465<br>\nElapsed time for 100 iterations: 0.8968446254730225<br>\nEPOCH++<br>\nElapsed time for 100 iterations: 0.899205207824707<br>\nElapsed time for 100 iterations: 0.9078717231750488<br>\nElapsed time for 100 iterations: 0.9038975238800049<br>\nElapsed time for 100 iterations: 0.9130148887634277<br>\nElapsed time for 100 iterations: 0.9119799137115479<br>\nElapsed time for 100 iterations: 0.9025707244873047</p>\n</blockquote>\n<ul>\n<li>With shuffling</li>\n</ul>\n<blockquote>\n<p>Elapsed time for 100 iterations: 0.756305456161499<br>\nElapsed time for 100 iterations: 0.9620733261108398<br>\nElapsed time for 100 iterations: 1.1773226261138916<br>\nElapsed time for 100 iterations: 2.098752498626709<br>\nElapsed time for 100 iterations: 3.113154411315918<br>\nElapsed time for 100 iterations: 3.66085147857666<br>\nEPOCH++<br>\nElapsed time for 100 iterations: 1.9388504028320312<br>\nElapsed time for 100 iterations: 0.9970040321350098<br>\nElapsed time for 100 iterations: 1.2153730392456055<br>\nElapsed time for 100 iterations: 2.1299166679382324<br>\nElapsed time for 100 iterations: 3.369262933731079<br>\nElapsed time for 100 iterations: 3.6159825325012207<br>\nEPOCH++<br>\nElapsed time for 100 iterations: 1.98651123046875<br>\nElapsed time for 100 iterations: 1.0247056484222412<br>\nElapsed time for 100 iterations: 1.1780986785888672<br>\nElapsed time for 100 iterations: 1.7233843803405762<br>\nElapsed time for 100 iterations: 3.0474486351013184<br>\nElapsed time for 100 iterations: 3.7090988159179688</p>\n</blockquote>\n<p>I tested this both with a single-epoch dataset of which I capture the tf.errors.OutOfRangeError  followed by re-initilaizing the dataset, and with a dataset.repeat().</p>\n<p>So my question now essentially is: <strong>what is happening inside a dataset when shuffling that makes it so much slower?</strong> (apologies if that's in the documentation and I missed it).<br>\nOr <strong>what am I doing wrong in its usage?</strong></p>\n<p>My dataset pipeline is similar to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15191858\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jrbtaylor\">@jrbtaylor</a>'s, I tested with the MNIST data with a batch size of 100.</p>\n<blockquote>\n<p>mnist_imgs = tf.constant(img_filenames) # img_filenames is a python list of strings<br>\nmnist_labels = tf.constant(labels)<br>\ndataset = tf.data.Dataset.from_tensor_slices((mnist_imgs, mnist_labels))<br>\ndataset = dataset.shuffle(epoch_size) # OPTIONAL<br>\ndataset = dataset.repeat(nb_epochs_to_do)<br>\ndataset = dataset.map(_parse_function,num_parallel_calls=FLAGS.num_parallel_cores) # where _parse_function reads and decodes the image files<br>\ndataset = dataset.batch(FLAGS.batch_size)<br>\ndataset = dataset.prefetch(1)<br>\niterator = dataset.make_initializable_iterator()</p>\n</blockquote>\n<p>Thank you.</p>\n<p>Kenneth</p>", "body_text": "Hello.\nI experience the same behavior as @jrbtaylor on a much smaller dataset already (the MNIST data, 60K training examples).\nShuffling progressively slows down the learning iterations, up until the end of an epoch, then it continues at the initial speed again:\n\nNo shuffling:\n\n\nElapsed time for 100 iterations: 0.7071466445922852\nElapsed time for 100 iterations: 0.928595781326294\nElapsed time for 100 iterations: 0.9202249050140381\nElapsed time for 100 iterations: 0.9153792858123779\nElapsed time for 100 iterations: 0.9065940380096436\nElapsed time for 100 iterations: 0.9027049541473389\nEPOCH++\nElapsed time for 100 iterations: 0.903191328048706\nElapsed time for 100 iterations: 0.9209485054016113\nElapsed time for 100 iterations: 0.9005069732666016\nElapsed time for 100 iterations: 0.9149377346038818\nElapsed time for 100 iterations: 0.9105048179626465\nElapsed time for 100 iterations: 0.8968446254730225\nEPOCH++\nElapsed time for 100 iterations: 0.899205207824707\nElapsed time for 100 iterations: 0.9078717231750488\nElapsed time for 100 iterations: 0.9038975238800049\nElapsed time for 100 iterations: 0.9130148887634277\nElapsed time for 100 iterations: 0.9119799137115479\nElapsed time for 100 iterations: 0.9025707244873047\n\n\nWith shuffling\n\n\nElapsed time for 100 iterations: 0.756305456161499\nElapsed time for 100 iterations: 0.9620733261108398\nElapsed time for 100 iterations: 1.1773226261138916\nElapsed time for 100 iterations: 2.098752498626709\nElapsed time for 100 iterations: 3.113154411315918\nElapsed time for 100 iterations: 3.66085147857666\nEPOCH++\nElapsed time for 100 iterations: 1.9388504028320312\nElapsed time for 100 iterations: 0.9970040321350098\nElapsed time for 100 iterations: 1.2153730392456055\nElapsed time for 100 iterations: 2.1299166679382324\nElapsed time for 100 iterations: 3.369262933731079\nElapsed time for 100 iterations: 3.6159825325012207\nEPOCH++\nElapsed time for 100 iterations: 1.98651123046875\nElapsed time for 100 iterations: 1.0247056484222412\nElapsed time for 100 iterations: 1.1780986785888672\nElapsed time for 100 iterations: 1.7233843803405762\nElapsed time for 100 iterations: 3.0474486351013184\nElapsed time for 100 iterations: 3.7090988159179688\n\nI tested this both with a single-epoch dataset of which I capture the tf.errors.OutOfRangeError  followed by re-initilaizing the dataset, and with a dataset.repeat().\nSo my question now essentially is: what is happening inside a dataset when shuffling that makes it so much slower? (apologies if that's in the documentation and I missed it).\nOr what am I doing wrong in its usage?\nMy dataset pipeline is similar to @jrbtaylor's, I tested with the MNIST data with a batch size of 100.\n\nmnist_imgs = tf.constant(img_filenames) # img_filenames is a python list of strings\nmnist_labels = tf.constant(labels)\ndataset = tf.data.Dataset.from_tensor_slices((mnist_imgs, mnist_labels))\ndataset = dataset.shuffle(epoch_size) # OPTIONAL\ndataset = dataset.repeat(nb_epochs_to_do)\ndataset = dataset.map(_parse_function,num_parallel_calls=FLAGS.num_parallel_cores) # where _parse_function reads and decodes the image files\ndataset = dataset.batch(FLAGS.batch_size)\ndataset = dataset.prefetch(1)\niterator = dataset.make_initializable_iterator()\n\nThank you.\nKenneth", "body": "Hello.\r\n\r\nI experience the same behavior as @jrbtaylor on a much smaller dataset already (the MNIST data, 60K training examples).\r\nShuffling progressively slows down the learning iterations, up until the end of an epoch, then it continues at the initial speed again:\r\n\r\n- No shuffling:\r\n\r\n> Elapsed time for 100 iterations: 0.7071466445922852\r\nElapsed time for 100 iterations: 0.928595781326294\r\nElapsed time for 100 iterations: 0.9202249050140381\r\nElapsed time for 100 iterations: 0.9153792858123779\r\nElapsed time for 100 iterations: 0.9065940380096436\r\nElapsed time for 100 iterations: 0.9027049541473389\r\nEPOCH++\r\nElapsed time for 100 iterations: 0.903191328048706\r\nElapsed time for 100 iterations: 0.9209485054016113\r\nElapsed time for 100 iterations: 0.9005069732666016\r\nElapsed time for 100 iterations: 0.9149377346038818\r\nElapsed time for 100 iterations: 0.9105048179626465\r\nElapsed time for 100 iterations: 0.8968446254730225\r\nEPOCH++\r\nElapsed time for 100 iterations: 0.899205207824707\r\nElapsed time for 100 iterations: 0.9078717231750488\r\nElapsed time for 100 iterations: 0.9038975238800049\r\nElapsed time for 100 iterations: 0.9130148887634277\r\nElapsed time for 100 iterations: 0.9119799137115479\r\nElapsed time for 100 iterations: 0.9025707244873047\r\n\r\n- With shuffling\r\n> Elapsed time for 100 iterations: 0.756305456161499\r\nElapsed time for 100 iterations: 0.9620733261108398\r\nElapsed time for 100 iterations: 1.1773226261138916\r\nElapsed time for 100 iterations: 2.098752498626709\r\nElapsed time for 100 iterations: 3.113154411315918\r\nElapsed time for 100 iterations: 3.66085147857666\r\nEPOCH++\r\nElapsed time for 100 iterations: 1.9388504028320312\r\nElapsed time for 100 iterations: 0.9970040321350098\r\nElapsed time for 100 iterations: 1.2153730392456055\r\nElapsed time for 100 iterations: 2.1299166679382324\r\nElapsed time for 100 iterations: 3.369262933731079\r\nElapsed time for 100 iterations: 3.6159825325012207\r\nEPOCH++\r\nElapsed time for 100 iterations: 1.98651123046875\r\nElapsed time for 100 iterations: 1.0247056484222412\r\nElapsed time for 100 iterations: 1.1780986785888672\r\nElapsed time for 100 iterations: 1.7233843803405762\r\nElapsed time for 100 iterations: 3.0474486351013184\r\nElapsed time for 100 iterations: 3.7090988159179688\r\n\r\nI tested this both with a single-epoch dataset of which I capture the tf.errors.OutOfRangeError  followed by re-initilaizing the dataset, and with a dataset.repeat().\r\n\r\nSo my question now essentially is: **what is happening inside a dataset when shuffling that makes it so much slower?** (apologies if that's in the documentation and I missed it).\r\nOr **what am I doing wrong in its usage?**\r\n\r\nMy dataset pipeline is similar to @jrbtaylor's, I tested with the MNIST data with a batch size of 100.\r\n\r\n> mnist_imgs = tf.constant(img_filenames) # img_filenames is a python list of strings\r\n   mnist_labels = tf.constant(labels)\r\n   dataset = tf.data.Dataset.from_tensor_slices((mnist_imgs, mnist_labels))\r\n   dataset = dataset.shuffle(epoch_size) # OPTIONAL\r\n   dataset = dataset.repeat(nb_epochs_to_do)\r\n   dataset = dataset.map(_parse_function,num_parallel_calls=FLAGS.num_parallel_cores) # where _parse_function reads and decodes the image files\r\n   dataset = dataset.batch(FLAGS.batch_size)\r\n   dataset = dataset.prefetch(1)\r\n   iterator = dataset.make_initializable_iterator()\r\n\r\n\r\nThank you.\r\n\r\nKenneth"}