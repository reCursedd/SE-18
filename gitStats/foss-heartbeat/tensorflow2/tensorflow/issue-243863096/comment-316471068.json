{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316471068", "html_url": "https://github.com/tensorflow/tensorflow/issues/11591#issuecomment-316471068", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11591", "id": 316471068, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjQ3MTA2OA==", "user": {"login": "jrbtaylor", "id": 15191858, "node_id": "MDQ6VXNlcjE1MTkxODU4", "avatar_url": "https://avatars3.githubusercontent.com/u/15191858?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrbtaylor", "html_url": "https://github.com/jrbtaylor", "followers_url": "https://api.github.com/users/jrbtaylor/followers", "following_url": "https://api.github.com/users/jrbtaylor/following{/other_user}", "gists_url": "https://api.github.com/users/jrbtaylor/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrbtaylor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrbtaylor/subscriptions", "organizations_url": "https://api.github.com/users/jrbtaylor/orgs", "repos_url": "https://api.github.com/users/jrbtaylor/repos", "events_url": "https://api.github.com/users/jrbtaylor/events{/privacy}", "received_events_url": "https://api.github.com/users/jrbtaylor/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-19T18:11:16Z", "updated_at": "2017-07-19T18:11:16Z", "author_association": "NONE", "body_html": "<p>I wrote a similar test for the tf.contrib.data functions I would be using for the same task above:</p>\n<pre><code>import time\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops,dtypes\nfrom tensorflow.contrib.data import Dataset\nfrom tensorflow.contrib.data import Iterator\n\nrept = 1000\nfor datasize in [1e3, 1e6, 1e7, 1e8]:\n    data = list(range(int(datasize)))\n    data_tensor = ops.convert_to_tensor(data)\n    dataset = Dataset.from_tensor_slices(data_tensor)\n    dataset = dataset.shuffle(int(datasize))\n    iterator = Iterator.from_structure(dataset.output_types,\n                                       dataset.output_shapes)\n    init_op = iterator.make_initializer(dataset)\n    next_element = iterator.get_next()\n\n    with tf.Session() as sess:\n        sess.run(init_op)\n        start_time = time.time()\n        for r in range(rept):\n            x = sess.run(next_element)\n        print('Datalength: %i  ---  time = %.3f ms/sample'%(\n        datasize, (time.time()-start_time)/rept*1000))\n        sess.close()\n</code></pre>\n<p>It actually does worse:</p>\n<blockquote>\n<p>Datalength: 1000  ---  time = 0.113 ms/sample<br>\nDatalength: 1000000  ---  time = 0.374 ms/sample<br>\nDatalength: 10000000  ---  time = 2.647 ms/sample<br>\nDatalength: 100000000  ---  time = 27.247 ms/sample</p>\n</blockquote>\n<p>In both cases (tf.contrib.data and the old functions above), the overhead appears to be in shuffling larger datasets. The tf.contrib.data functions at least allow access to the buffer size for shuffling, so I can do shuffling on subsets of the data (not true random, but better than nothing). This solves the problem in practice for large datasets. Why not offer an option to do shuffling by random sampling with replacement? This would scale better - there's no need to actually shuffle a huge list, just sample randomly.</p>\n<p>In practice, tf.contrib.data is about 3x slower than my old input pipeline on real disk reads, so <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a> maybe you could expand on why you recommend tf.contrib.data. Is there a reason beyond being able to set the buffer size for the shuffle operation?</p>", "body_text": "I wrote a similar test for the tf.contrib.data functions I would be using for the same task above:\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops,dtypes\nfrom tensorflow.contrib.data import Dataset\nfrom tensorflow.contrib.data import Iterator\n\nrept = 1000\nfor datasize in [1e3, 1e6, 1e7, 1e8]:\n    data = list(range(int(datasize)))\n    data_tensor = ops.convert_to_tensor(data)\n    dataset = Dataset.from_tensor_slices(data_tensor)\n    dataset = dataset.shuffle(int(datasize))\n    iterator = Iterator.from_structure(dataset.output_types,\n                                       dataset.output_shapes)\n    init_op = iterator.make_initializer(dataset)\n    next_element = iterator.get_next()\n\n    with tf.Session() as sess:\n        sess.run(init_op)\n        start_time = time.time()\n        for r in range(rept):\n            x = sess.run(next_element)\n        print('Datalength: %i  ---  time = %.3f ms/sample'%(\n        datasize, (time.time()-start_time)/rept*1000))\n        sess.close()\n\nIt actually does worse:\n\nDatalength: 1000  ---  time = 0.113 ms/sample\nDatalength: 1000000  ---  time = 0.374 ms/sample\nDatalength: 10000000  ---  time = 2.647 ms/sample\nDatalength: 100000000  ---  time = 27.247 ms/sample\n\nIn both cases (tf.contrib.data and the old functions above), the overhead appears to be in shuffling larger datasets. The tf.contrib.data functions at least allow access to the buffer size for shuffling, so I can do shuffling on subsets of the data (not true random, but better than nothing). This solves the problem in practice for large datasets. Why not offer an option to do shuffling by random sampling with replacement? This would scale better - there's no need to actually shuffle a huge list, just sample randomly.\nIn practice, tf.contrib.data is about 3x slower than my old input pipeline on real disk reads, so @drpngx maybe you could expand on why you recommend tf.contrib.data. Is there a reason beyond being able to set the buffer size for the shuffle operation?", "body": "I wrote a similar test for the tf.contrib.data functions I would be using for the same task above:\r\n\r\n```\r\nimport time\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops,dtypes\r\nfrom tensorflow.contrib.data import Dataset\r\nfrom tensorflow.contrib.data import Iterator\r\n\r\nrept = 1000\r\nfor datasize in [1e3, 1e6, 1e7, 1e8]:\r\n    data = list(range(int(datasize)))\r\n    data_tensor = ops.convert_to_tensor(data)\r\n    dataset = Dataset.from_tensor_slices(data_tensor)\r\n    dataset = dataset.shuffle(int(datasize))\r\n    iterator = Iterator.from_structure(dataset.output_types,\r\n                                       dataset.output_shapes)\r\n    init_op = iterator.make_initializer(dataset)\r\n    next_element = iterator.get_next()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(init_op)\r\n        start_time = time.time()\r\n        for r in range(rept):\r\n            x = sess.run(next_element)\r\n        print('Datalength: %i  ---  time = %.3f ms/sample'%(\r\n        datasize, (time.time()-start_time)/rept*1000))\r\n        sess.close()\r\n```\r\n\r\nIt actually does worse:\r\n\r\n> Datalength: 1000  ---  time = 0.113 ms/sample\r\n> Datalength: 1000000  ---  time = 0.374 ms/sample\r\n> Datalength: 10000000  ---  time = 2.647 ms/sample\r\n> Datalength: 100000000  ---  time = 27.247 ms/sample\r\n\r\nIn both cases (tf.contrib.data and the old functions above), the overhead appears to be in shuffling larger datasets. The tf.contrib.data functions at least allow access to the buffer size for shuffling, so I can do shuffling on subsets of the data (not true random, but better than nothing). This solves the problem in practice for large datasets. Why not offer an option to do shuffling by random sampling with replacement? This would scale better - there's no need to actually shuffle a huge list, just sample randomly.\r\n\r\nIn practice, tf.contrib.data is about 3x slower than my old input pipeline on real disk reads, so @drpngx maybe you could expand on why you recommend tf.contrib.data. Is there a reason beyond being able to set the buffer size for the shuffle operation?"}