{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316856098", "html_url": "https://github.com/tensorflow/tensorflow/issues/11591#issuecomment-316856098", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11591", "id": 316856098, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjg1NjA5OA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-20T23:11:52Z", "updated_at": "2017-07-20T23:11:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Just to clarify, those two programs aren't equivalent. In the <code>tf.contrib.data</code> version, the equivalent to <code>capacity</code> is set to <code>int(datasize)</code>, whereas the <code>slice_input_producer</code> version uses <code>capacity=10</code>. This means that your <code>tf.contrib.data</code> version has to accumulate the entire input before producing the first record, whereas the <code>slice_input_producer</code> version accumulates only 10 records. The following program has more encouraging behavior:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.framework <span class=\"pl-k\">import</span> ops,dtypes\n<span class=\"pl-k\">from</span> tensorflow.contrib.data <span class=\"pl-k\">import</span> Dataset\n<span class=\"pl-k\">from</span> tensorflow.contrib.data <span class=\"pl-k\">import</span> Iterator\n\nrept <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n<span class=\"pl-k\">for</span> datasize <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">1e3</span>, <span class=\"pl-c1\">1e6</span>, <span class=\"pl-c1\">1e7</span>, <span class=\"pl-c1\">1e8</span>]:\n  <span class=\"pl-k\">with</span> tf.Graph().as_default():\n    data <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">int</span>(datasize)))\n    data_tensor <span class=\"pl-k\">=</span> ops.convert_to_tensor(data)\n    dataset <span class=\"pl-k\">=</span> Dataset.from_tensor_slices(data_tensor)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>dataset = Dataset.range(int(datasize))</span>\n    dataset <span class=\"pl-k\">=</span> dataset.shuffle(<span class=\"pl-c1\">10</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span>int(datasize))</span>\n    iterator <span class=\"pl-k\">=</span> Iterator.from_structure(dataset.output_types,\n                                       dataset.output_shapes)\n    init_op <span class=\"pl-k\">=</span> iterator.make_initializer(dataset)\n    next_element <span class=\"pl-k\">=</span> iterator.get_next()\n\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        sess.run(init_op)\n        start_time <span class=\"pl-k\">=</span> time.time()\n        <span class=\"pl-k\">for</span> r <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(rept):\n            x <span class=\"pl-k\">=</span> sess.run(next_element)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Datalength: <span class=\"pl-c1\">%i</span>  ---  time = <span class=\"pl-c1\">%.3f</span> ms/sample<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">%</span>(\n        datasize, (time.time()<span class=\"pl-k\">-</span>start_time)<span class=\"pl-k\">/</span>rept<span class=\"pl-k\">*</span><span class=\"pl-c1\">1000</span>))</pre></div>\n<blockquote>\n<p>Datalength: 1000  ---  time = 0.123 ms/sample<br>\nDatalength: 1000000  ---  time = 0.109 ms/sample<br>\nDatalength: 10000000  ---  time = 0.102 ms/sample<br>\nDatalength: 100000000  ---  time = 0.714 ms/sample</p>\n</blockquote>\n<p>I suspect the problem actually stems from this line:</p>\n<div class=\"highlight highlight-source-python\"><pre>    data_tensor <span class=\"pl-k\">=</span> ops.convert_to_tensor(data)</pre></div>\n<p>Embedding large tensors in the graph is slow, and you pay a noticeable and increasing penalty for both the <code>ops.convert_to_tensor()</code> call and the first <code>sess.run()</code> call after converting the tensor. Essentially, the tensor is copied several times, serialized and deserialized. I think the <code>1e8</code> case is slower because my local machine was running out of memory due to all of the copies!</p>\n<p>We haven't historically used random sampling with replacement in TensorFlow because it's tricky to scale it to datasets that are larger than memory. However, it would certainly be possible to add a <code>Dataset</code> that implemented it.</p>", "body_text": "Just to clarify, those two programs aren't equivalent. In the tf.contrib.data version, the equivalent to capacity is set to int(datasize), whereas the slice_input_producer version uses capacity=10. This means that your tf.contrib.data version has to accumulate the entire input before producing the first record, whereas the slice_input_producer version accumulates only 10 records. The following program has more encouraging behavior:\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops,dtypes\nfrom tensorflow.contrib.data import Dataset\nfrom tensorflow.contrib.data import Iterator\n\nrept = 1000\nfor datasize in [1e3, 1e6, 1e7, 1e8]:\n  with tf.Graph().as_default():\n    data = list(range(int(datasize)))\n    data_tensor = ops.convert_to_tensor(data)\n    dataset = Dataset.from_tensor_slices(data_tensor)\n    #dataset = Dataset.range(int(datasize))\n    dataset = dataset.shuffle(10)  #int(datasize))\n    iterator = Iterator.from_structure(dataset.output_types,\n                                       dataset.output_shapes)\n    init_op = iterator.make_initializer(dataset)\n    next_element = iterator.get_next()\n\n    with tf.Session() as sess:\n        sess.run(init_op)\n        start_time = time.time()\n        for r in range(rept):\n            x = sess.run(next_element)\n        print('Datalength: %i  ---  time = %.3f ms/sample'%(\n        datasize, (time.time()-start_time)/rept*1000))\n\nDatalength: 1000  ---  time = 0.123 ms/sample\nDatalength: 1000000  ---  time = 0.109 ms/sample\nDatalength: 10000000  ---  time = 0.102 ms/sample\nDatalength: 100000000  ---  time = 0.714 ms/sample\n\nI suspect the problem actually stems from this line:\n    data_tensor = ops.convert_to_tensor(data)\nEmbedding large tensors in the graph is slow, and you pay a noticeable and increasing penalty for both the ops.convert_to_tensor() call and the first sess.run() call after converting the tensor. Essentially, the tensor is copied several times, serialized and deserialized. I think the 1e8 case is slower because my local machine was running out of memory due to all of the copies!\nWe haven't historically used random sampling with replacement in TensorFlow because it's tricky to scale it to datasets that are larger than memory. However, it would certainly be possible to add a Dataset that implemented it.", "body": "Just to clarify, those two programs aren't equivalent. In the `tf.contrib.data` version, the equivalent to `capacity` is set to `int(datasize)`, whereas the `slice_input_producer` version uses `capacity=10`. This means that your `tf.contrib.data` version has to accumulate the entire input before producing the first record, whereas the `slice_input_producer` version accumulates only 10 records. The following program has more encouraging behavior:\r\n\r\n```python\r\nimport time\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops,dtypes\r\nfrom tensorflow.contrib.data import Dataset\r\nfrom tensorflow.contrib.data import Iterator\r\n\r\nrept = 1000\r\nfor datasize in [1e3, 1e6, 1e7, 1e8]:\r\n  with tf.Graph().as_default():\r\n    data = list(range(int(datasize)))\r\n    data_tensor = ops.convert_to_tensor(data)\r\n    dataset = Dataset.from_tensor_slices(data_tensor)\r\n    #dataset = Dataset.range(int(datasize))\r\n    dataset = dataset.shuffle(10)  #int(datasize))\r\n    iterator = Iterator.from_structure(dataset.output_types,\r\n                                       dataset.output_shapes)\r\n    init_op = iterator.make_initializer(dataset)\r\n    next_element = iterator.get_next()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(init_op)\r\n        start_time = time.time()\r\n        for r in range(rept):\r\n            x = sess.run(next_element)\r\n        print('Datalength: %i  ---  time = %.3f ms/sample'%(\r\n        datasize, (time.time()-start_time)/rept*1000))\r\n```\r\n\r\n> Datalength: 1000  ---  time = 0.123 ms/sample\r\n> Datalength: 1000000  ---  time = 0.109 ms/sample\r\n> Datalength: 10000000  ---  time = 0.102 ms/sample\r\n> Datalength: 100000000  ---  time = 0.714 ms/sample\r\n\r\nI suspect the problem actually stems from this line: \r\n\r\n```python\r\n    data_tensor = ops.convert_to_tensor(data)\r\n```\r\n\r\nEmbedding large tensors in the graph is slow, and you pay a noticeable and increasing penalty for both the `ops.convert_to_tensor()` call and the first `sess.run()` call after converting the tensor. Essentially, the tensor is copied several times, serialized and deserialized. I think the `1e8` case is slower because my local machine was running out of memory due to all of the copies!\r\n\r\nWe haven't historically used random sampling with replacement in TensorFlow because it's tricky to scale it to datasets that are larger than memory. However, it would certainly be possible to add a `Dataset` that implemented it."}