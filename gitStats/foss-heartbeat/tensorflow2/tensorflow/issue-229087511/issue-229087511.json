{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9937", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9937/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9937/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9937/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9937", "id": 229087511, "node_id": "MDU6SXNzdWUyMjkwODc1MTE=", "number": 9937, "title": "reimplement core/util/cuda_kernel_helper.h?", "user": {"login": "zasdfgbnm", "id": 1032377, "node_id": "MDQ6VXNlcjEwMzIzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1032377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zasdfgbnm", "html_url": "https://github.com/zasdfgbnm", "followers_url": "https://api.github.com/users/zasdfgbnm/followers", "following_url": "https://api.github.com/users/zasdfgbnm/following{/other_user}", "gists_url": "https://api.github.com/users/zasdfgbnm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zasdfgbnm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zasdfgbnm/subscriptions", "organizations_url": "https://api.github.com/users/zasdfgbnm/orgs", "repos_url": "https://api.github.com/users/zasdfgbnm/repos", "events_url": "https://api.github.com/users/zasdfgbnm/events{/privacy}", "received_events_url": "https://api.github.com/users/zasdfgbnm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-05-16T16:07:31Z", "updated_at": "2017-05-17T14:17:59Z", "closed_at": "2017-05-17T14:17:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi,</p>\n<p>I'm trying to implement a <code>GetCuda3DLaunchConfig</code> into <code>cuda_kernel_helper.h</code>, but while reading the code, I feel it's a bit confusing and there might be a better way to implement it.</p>\n<p>Here is the pointer:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/cuda_kernel_helper.h\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/cuda_kernel_helper.h</a></p>\n<p>In line 55-57, there is something like <code>block_count = physical_thread_count / thread_per_block</code>. Why use <code>thread_per_block</code> instead of <code>virtual_thread_count</code>?  The number of blocks can be higher than physical maximum and cuda will automatically put these blocks into queue. On the other hand, limiting the number of blocks to physical limit would make the computation incomplete when the number of threads is large.<br>\nSee: <a href=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png\" rel=\"nofollow\">http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png</a></p>\n<p>The the kernel launch config computed is not optimal. I would suggest using the API provided by cuda &gt;=6.5<br>\nSee: <a href=\"https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/\" rel=\"nofollow\">https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/</a></p>\n<p>Can anyone confirm what I said? If my suggestion make sense, I would like to reimplement these functions using cuda's occupancy api while writing my <code>GetCuda3DLaunchConfig</code>.</p>", "body_text": "Hi,\nI'm trying to implement a GetCuda3DLaunchConfig into cuda_kernel_helper.h, but while reading the code, I feel it's a bit confusing and there might be a better way to implement it.\nHere is the pointer:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/cuda_kernel_helper.h\nIn line 55-57, there is something like block_count = physical_thread_count / thread_per_block. Why use thread_per_block instead of virtual_thread_count?  The number of blocks can be higher than physical maximum and cuda will automatically put these blocks into queue. On the other hand, limiting the number of blocks to physical limit would make the computation incomplete when the number of threads is large.\nSee: http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png\nThe the kernel launch config computed is not optimal. I would suggest using the API provided by cuda >=6.5\nSee: https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/\nCan anyone confirm what I said? If my suggestion make sense, I would like to reimplement these functions using cuda's occupancy api while writing my GetCuda3DLaunchConfig.", "body": "Hi,\r\n\r\nI'm trying to implement a `GetCuda3DLaunchConfig` into `cuda_kernel_helper.h`, but while reading the code, I feel it's a bit confusing and there might be a better way to implement it.\r\n\r\nHere is the pointer:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/cuda_kernel_helper.h\r\n\r\nIn line 55-57, there is something like `block_count = physical_thread_count / thread_per_block`. Why use `thread_per_block` instead of `virtual_thread_count`?  The number of blocks can be higher than physical maximum and cuda will automatically put these blocks into queue. On the other hand, limiting the number of blocks to physical limit would make the computation incomplete when the number of threads is large.\r\nSee: http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/automatic-scalability.png\r\n\r\nThe the kernel launch config computed is not optimal. I would suggest using the API provided by cuda >=6.5\r\nSee: https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/\r\n\r\nCan anyone confirm what I said? If my suggestion make sense, I would like to reimplement these functions using cuda's occupancy api while writing my `GetCuda3DLaunchConfig`."}