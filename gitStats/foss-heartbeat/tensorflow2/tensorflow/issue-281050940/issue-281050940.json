{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15274", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15274/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15274/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15274/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15274", "id": 281050940, "node_id": "MDU6SXNzdWUyODEwNTA5NDA=", "number": 15274, "title": "breaking change to distributed training moving from TF v1.3 to v1.4: \u201cUnavailableError: Trying to connect an http1.x server\u201d", "user": {"login": "AngusSinclair", "id": 24782937, "node_id": "MDQ6VXNlcjI0NzgyOTM3", "avatar_url": "https://avatars0.githubusercontent.com/u/24782937?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AngusSinclair", "html_url": "https://github.com/AngusSinclair", "followers_url": "https://api.github.com/users/AngusSinclair/followers", "following_url": "https://api.github.com/users/AngusSinclair/following{/other_user}", "gists_url": "https://api.github.com/users/AngusSinclair/gists{/gist_id}", "starred_url": "https://api.github.com/users/AngusSinclair/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AngusSinclair/subscriptions", "organizations_url": "https://api.github.com/users/AngusSinclair/orgs", "repos_url": "https://api.github.com/users/AngusSinclair/repos", "events_url": "https://api.github.com/users/AngusSinclair/events{/privacy}", "received_events_url": "https://api.github.com/users/AngusSinclair/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2017-12-11T15:11:23Z", "updated_at": "2018-06-17T18:33:00Z", "closed_at": "2018-06-17T18:29:45Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nn/a</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nRed Hat Enterprise Linux Server release 7.3 (Maipo)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.4 (moving from 1,3)<br>\nb'v1.4.0-4-g9283868' 1.4.0 in specific</li>\n<li><strong>Python version</strong>:<br>\nPython 3.6.2 :: Anaconda custom (64-bit)</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.5.4</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nN/a</li>\n<li><strong>GPU model and memory</strong>:<br>\nN/a</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nsee below. Appears to be any distributed training use case with the cluster spec defined as below.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I believe this could be fairly characterized as a BUG.</p>\n<p>I raised (and solved) this issue on stackoverflow <a href=\"https://stackoverflow.com/questions/47143043/breaking-change-to-distributed-training-moving-from-tf-v1-3-to-v1-4-unavailabl/47191081#47191081\" rel=\"nofollow\">here</a></p>\n<p>In short, upgrading from tf v1.3 to 1.4 distributed training broke with the error<br>\n\"tensorflow.python.framework.errors_impl.UnavailableError: Trying to connect an http1.x server\".<br>\nI was training across multiple CPU cores all on the same localhost.</p>\n<p>The fix was to change the ps to the  string \"localhost\" explicitly in the cluster spec instead of the IP \"127.0.0.1\". It stops trying to connect to the localhost via my proxy server (which indeed, was HTTP1.x only i think). I would call this a bug since that IP should be the equivalent, and I'm not sure what other kind of behavior might have inadvertently changed between versions (since this always worked in TF &lt;= 1.3).</p>\n<p>To be fair, I'm also not sure if this is a tensorflow or more gRPC specific issue. In either case, the above is a work around for the problem.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nn/a\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nRed Hat Enterprise Linux Server release 7.3 (Maipo)\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\n1.4 (moving from 1,3)\nb'v1.4.0-4-g9283868' 1.4.0 in specific\nPython version:\nPython 3.6.2 :: Anaconda custom (64-bit)\nBazel version (if compiling from source):\n0.5.4\nGCC/Compiler version (if compiling from source):\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\nCUDA/cuDNN version:\nN/a\nGPU model and memory:\nN/a\nExact command to reproduce:\nsee below. Appears to be any distributed training use case with the cluster spec defined as below.\n\nDescribe the problem\nI believe this could be fairly characterized as a BUG.\nI raised (and solved) this issue on stackoverflow here\nIn short, upgrading from tf v1.3 to 1.4 distributed training broke with the error\n\"tensorflow.python.framework.errors_impl.UnavailableError: Trying to connect an http1.x server\".\nI was training across multiple CPU cores all on the same localhost.\nThe fix was to change the ps to the  string \"localhost\" explicitly in the cluster spec instead of the IP \"127.0.0.1\". It stops trying to connect to the localhost via my proxy server (which indeed, was HTTP1.x only i think). I would call this a bug since that IP should be the equivalent, and I'm not sure what other kind of behavior might have inadvertently changed between versions (since this always worked in TF <= 1.3).\nTo be fair, I'm also not sure if this is a tensorflow or more gRPC specific issue. In either case, the above is a work around for the problem.", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nn/a\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRed Hat Enterprise Linux Server release 7.3 (Maipo)\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.4 (moving from 1,3)\r\nb'v1.4.0-4-g9283868' 1.4.0 in specific\r\n- **Python version**: \r\nPython 3.6.2 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\n0.5.4\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\n- **CUDA/cuDNN version**:\r\nN/a\r\n- **GPU model and memory**:\r\nN/a\r\n- **Exact command to reproduce**:\r\nsee below. Appears to be any distributed training use case with the cluster spec defined as below.\r\n\r\n### Describe the problem\r\n\r\nI believe this could be fairly characterized as a BUG.\r\n\r\nI raised (and solved) this issue on stackoverflow [here](https://stackoverflow.com/questions/47143043/breaking-change-to-distributed-training-moving-from-tf-v1-3-to-v1-4-unavailabl/47191081#47191081)\r\n\r\nIn short, upgrading from tf v1.3 to 1.4 distributed training broke with the error\r\n\"tensorflow.python.framework.errors_impl.UnavailableError: Trying to connect an http1.x server\".\r\nI was training across multiple CPU cores all on the same localhost.\r\n\r\nThe fix was to change the ps to the  string \"localhost\" explicitly in the cluster spec instead of the IP \"127.0.0.1\". It stops trying to connect to the localhost via my proxy server (which indeed, was HTTP1.x only i think). I would call this a bug since that IP should be the equivalent, and I'm not sure what other kind of behavior might have inadvertently changed between versions (since this always worked in TF <= 1.3). \r\n\r\nTo be fair, I'm also not sure if this is a tensorflow or more gRPC specific issue. In either case, the above is a work around for the problem.\r\n"}