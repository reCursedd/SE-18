{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296912954", "html_url": "https://github.com/tensorflow/tensorflow/issues/9423#issuecomment-296912954", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9423", "id": 296912954, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjkxMjk1NA==", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-25T05:22:14Z", "updated_at": "2017-04-25T05:22:14Z", "author_association": "MEMBER", "body_html": "<p>If you are not up to making a full patch (But please do if you can).. you can just define a gradient for the op ad-hoc in your code using the gradient_override_map... i.e.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@tf.RegisterGradient</span>(\u2018CustomSquare\u2019)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">custom_square_grad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> \u2026</span>\n\nc <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">5.0</span>)\ns1 <span class=\"pl-k\">=</span> tf.square(c)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Uses default gradient for square</span>\n<span class=\"pl-k\">with</span> graph.gradient_override_map({\u2018Square\u2019: \u2018CustomSquare\u2019}):\n  s2 <span class=\"pl-k\">=</span> tf.square(c)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Uses _compute_square_grad</span></pre></div>", "body_text": "If you are not up to making a full patch (But please do if you can).. you can just define a gradient for the op ad-hoc in your code using the gradient_override_map... i.e.\n@tf.RegisterGradient(\u2018CustomSquare\u2019)\ndef custom_square_grad(op, grad):\n  # \u2026\n\nc = tf.constant(5.0)\ns1 = tf.square(c)  # Uses default gradient for square\nwith graph.gradient_override_map({\u2018Square\u2019: \u2018CustomSquare\u2019}):\n  s2 = tf.square(c)  # Uses _compute_square_grad", "body": "If you are not up to making a full patch (But please do if you can).. you can just define a gradient for the op ad-hoc in your code using the gradient_override_map... i.e.\r\n\r\n```python\r\n@tf.RegisterGradient(\u2018CustomSquare\u2019)\r\ndef custom_square_grad(op, grad):\r\n  # \u2026\r\n\r\nc = tf.constant(5.0)\r\ns1 = tf.square(c)  # Uses default gradient for square\r\nwith graph.gradient_override_map({\u2018Square\u2019: \u2018CustomSquare\u2019}):\r\n  s2 = tf.square(c)  # Uses _compute_square_grad\r\n```"}