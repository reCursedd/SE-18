{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422506263", "html_url": "https://github.com/tensorflow/tensorflow/issues/21470#issuecomment-422506263", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21470", "id": 422506263, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjUwNjI2Mw==", "user": {"login": "guptapriya", "id": 14104855, "node_id": "MDQ6VXNlcjE0MTA0ODU1", "avatar_url": "https://avatars1.githubusercontent.com/u/14104855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guptapriya", "html_url": "https://github.com/guptapriya", "followers_url": "https://api.github.com/users/guptapriya/followers", "following_url": "https://api.github.com/users/guptapriya/following{/other_user}", "gists_url": "https://api.github.com/users/guptapriya/gists{/gist_id}", "starred_url": "https://api.github.com/users/guptapriya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guptapriya/subscriptions", "organizations_url": "https://api.github.com/users/guptapriya/orgs", "repos_url": "https://api.github.com/users/guptapriya/repos", "events_url": "https://api.github.com/users/guptapriya/events{/privacy}", "received_events_url": "https://api.github.com/users/guptapriya/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-18T18:51:14Z", "updated_at": "2018-09-18T18:51:14Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4530735\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Windaway\">@Windaway</a> nccl is only useful if there are GPU to GPU connections available in your setup. Is that the case? If yes, you could try finding a nccl binary for Windows.</p>\n<p>If not, then it would be better to try some of the non nccl options. To get those, try the following:</p>\n<p>Option 1:<br>\nTry using hierarchical copy.</p>\n<pre><code>cross_tower_ops = tf.contrib.distribute.AllReduceCrossTowerOps(\n    'hierarchical_copy', num_packs=num_gpus))\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\n</code></pre>\n<p>Option 2:<br>\nReduce to first GPU:</p>\n<pre><code>cross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps()\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\n</code></pre>\n<p>Option 3:<br>\nReduce to CPU:</p>\n<pre><code>cross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps(\n    reduce_to_device=\"/device:CPU:0\")\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\n</code></pre>\n<p>You will have to try out the 2 approaches and see which one works and gives the best performance for your use case.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1647833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yuefengz\">@yuefengz</a> - for use cases like this, perhaps we should detect if nccl is not available, give a warning, and default to something else that will work for sure?</p>", "body_text": "@Windaway nccl is only useful if there are GPU to GPU connections available in your setup. Is that the case? If yes, you could try finding a nccl binary for Windows.\nIf not, then it would be better to try some of the non nccl options. To get those, try the following:\nOption 1:\nTry using hierarchical copy.\ncross_tower_ops = tf.contrib.distribute.AllReduceCrossTowerOps(\n    'hierarchical_copy', num_packs=num_gpus))\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\n\nOption 2:\nReduce to first GPU:\ncross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps()\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\n\nOption 3:\nReduce to CPU:\ncross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps(\n    reduce_to_device=\"/device:CPU:0\")\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\n\nYou will have to try out the 2 approaches and see which one works and gives the best performance for your use case.\n@yuefengz - for use cases like this, perhaps we should detect if nccl is not available, give a warning, and default to something else that will work for sure?", "body": "@Windaway nccl is only useful if there are GPU to GPU connections available in your setup. Is that the case? If yes, you could try finding a nccl binary for Windows.\r\n\r\nIf not, then it would be better to try some of the non nccl options. To get those, try the following:\r\n\r\nOption 1:\r\nTry using hierarchical copy. \r\n```\r\ncross_tower_ops = tf.contrib.distribute.AllReduceCrossTowerOps(\r\n    'hierarchical_copy', num_packs=num_gpus))\r\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n```\r\n\r\nOption 2: \r\nReduce to first GPU:\r\n```\r\ncross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps()\r\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n```\r\n\r\nOption 3: \r\nReduce to CPU:\r\n```\r\ncross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps(\r\n    reduce_to_device=\"/device:CPU:0\")\r\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n```\r\n\r\nYou will have to try out the 2 approaches and see which one works and gives the best performance for your use case. \r\n\r\n@yuefengz - for use cases like this, perhaps we should detect if nccl is not available, give a warning, and default to something else that will work for sure? \r\n"}