{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/334004386", "html_url": "https://github.com/tensorflow/tensorflow/issues/2386#issuecomment-334004386", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2386", "id": 334004386, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDAwNDM4Ng==", "user": {"login": "jiayiliu", "id": 1511514, "node_id": "MDQ6VXNlcjE1MTE1MTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1511514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiayiliu", "html_url": "https://github.com/jiayiliu", "followers_url": "https://api.github.com/users/jiayiliu/followers", "following_url": "https://api.github.com/users/jiayiliu/following{/other_user}", "gists_url": "https://api.github.com/users/jiayiliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiayiliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiayiliu/subscriptions", "organizations_url": "https://api.github.com/users/jiayiliu/orgs", "repos_url": "https://api.github.com/users/jiayiliu/repos", "events_url": "https://api.github.com/users/jiayiliu/events{/privacy}", "received_events_url": "https://api.github.com/users/jiayiliu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-03T23:06:16Z", "updated_at": "2017-10-03T23:06:16Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15792374\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jmchen-g\">@jmchen-g</a> Is there any changes with v1.3?<br>\nI tried to run with the MonitoredTrainingSession(), but the process seemed to hang in the middle.<br>\nBasically I followed the asynchronized tutorial (works fine) and add the SyncReplicasOptimizer.<br>\nI tried to play with the <code>get_init_tokens_op()</code> but no luck..<br>\nAny hints are welcome.  Thanks!</p>\n<p>Here is my sample code and outputs:</p>\n<p>`import tensorflow as tf<br>\nimport sys</p>\n<p>cluster_spec = {\"ps\":[\"localhost:8864\"],<br>\n\"worker\":[\"localhost:8865\",\"localhost:8866\"]}</p>\n<p>if <strong>name</strong> == \"<strong>main</strong>\":<br>\nif len(sys.argv) != 3:<br>\nprint(\"test.py ps|worker 0|1\")<br>\nexit(1)</p>\n<pre><code>role = sys.argv[1]\ntask_id = int(sys.argv[2])\n\ncluster = tf.train.ClusterSpec(cluster_spec)\nserver = tf.train.Server(cluster, job_name=role, task_index=task_id)\n\nif role == \"ps\":\n    server.join()\nelse:\n    with tf.device(\n            tf.train.replica_device_setter(cluster=cluster_spec,\n                worker_device=\"/job:worker/task:%d\"%task_id)):\n        global_steps = tf.contrib.framework.get_or_create_global_step()\n        # a very simple model with pseudo data\n        inputs = tf.placeholder(tf.float32, shape=[None, 3])\n        labels = tf.placeholder(tf.float32, shape=[None])\n        w = tf.Variable([0.11,10,0.3])\n        y = tf.reduce_sum(tf.multiply(inputs, w), axis=1)\n        loss = tf.losses.mean_squared_error(labels, y)\n        opt = tf.train.GradientDescentOptimizer(0.00001)\n        opt_sync = tf.train.SyncReplicasOptimizer(opt,\n                                                  replicas_to_aggregate=2,\n                                                  # total_num_replicas=2,\n                                                  use_locking=True)\n        hooks = [opt_sync.make_session_run_hook((task_id==0))]\n        op = opt_sync.minimize(loss, global_step=global_steps)\n\n    with tf.train.MonitoredTrainingSession(master=server.target,\n                                           is_chief=(task_id==0),\n                                           hooks=hooks) as sess:\n        for i in range(10):\n            j = sess.run(global_steps)\n            t = sess.run(loss, feed_dict={inputs:[[1,2,3]],labels:[1]})\n            sess.run(op, feed_dict={inputs:[[1,2,3],[2,3,1]],labels:[2,1]})\n            print(\"%d %d %f\"%(i,j,t))\n\n    print(\"finished\")`\n</code></pre>\n<p>from worker 1:<br>\n<code>2017-10-03 15:48:37.079376: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session eb05b8a25a96918e with config:  0 0 400.400116 1 1 400.16366</code></p>\n<p>from worker 2:<br>\n<code>2017-10-03 15:48:3 6.882892: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session e54098121f5f8494 with config:  0 0 400.400116 1 1 400.400116</code></p>", "body_text": "@jmchen-g Is there any changes with v1.3?\nI tried to run with the MonitoredTrainingSession(), but the process seemed to hang in the middle.\nBasically I followed the asynchronized tutorial (works fine) and add the SyncReplicasOptimizer.\nI tried to play with the get_init_tokens_op() but no luck..\nAny hints are welcome.  Thanks!\nHere is my sample code and outputs:\n`import tensorflow as tf\nimport sys\ncluster_spec = {\"ps\":[\"localhost:8864\"],\n\"worker\":[\"localhost:8865\",\"localhost:8866\"]}\nif name == \"main\":\nif len(sys.argv) != 3:\nprint(\"test.py ps|worker 0|1\")\nexit(1)\nrole = sys.argv[1]\ntask_id = int(sys.argv[2])\n\ncluster = tf.train.ClusterSpec(cluster_spec)\nserver = tf.train.Server(cluster, job_name=role, task_index=task_id)\n\nif role == \"ps\":\n    server.join()\nelse:\n    with tf.device(\n            tf.train.replica_device_setter(cluster=cluster_spec,\n                worker_device=\"/job:worker/task:%d\"%task_id)):\n        global_steps = tf.contrib.framework.get_or_create_global_step()\n        # a very simple model with pseudo data\n        inputs = tf.placeholder(tf.float32, shape=[None, 3])\n        labels = tf.placeholder(tf.float32, shape=[None])\n        w = tf.Variable([0.11,10,0.3])\n        y = tf.reduce_sum(tf.multiply(inputs, w), axis=1)\n        loss = tf.losses.mean_squared_error(labels, y)\n        opt = tf.train.GradientDescentOptimizer(0.00001)\n        opt_sync = tf.train.SyncReplicasOptimizer(opt,\n                                                  replicas_to_aggregate=2,\n                                                  # total_num_replicas=2,\n                                                  use_locking=True)\n        hooks = [opt_sync.make_session_run_hook((task_id==0))]\n        op = opt_sync.minimize(loss, global_step=global_steps)\n\n    with tf.train.MonitoredTrainingSession(master=server.target,\n                                           is_chief=(task_id==0),\n                                           hooks=hooks) as sess:\n        for i in range(10):\n            j = sess.run(global_steps)\n            t = sess.run(loss, feed_dict={inputs:[[1,2,3]],labels:[1]})\n            sess.run(op, feed_dict={inputs:[[1,2,3],[2,3,1]],labels:[2,1]})\n            print(\"%d %d %f\"%(i,j,t))\n\n    print(\"finished\")`\n\nfrom worker 1:\n2017-10-03 15:48:37.079376: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session eb05b8a25a96918e with config:  0 0 400.400116 1 1 400.16366\nfrom worker 2:\n2017-10-03 15:48:3 6.882892: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session e54098121f5f8494 with config:  0 0 400.400116 1 1 400.400116", "body": "@jmchen-g Is there any changes with v1.3?\r\nI tried to run with the MonitoredTrainingSession(), but the process seemed to hang in the middle.\r\nBasically I followed the asynchronized tutorial (works fine) and add the SyncReplicasOptimizer.\r\nI tried to play with the `get_init_tokens_op()` but no luck..\r\nAny hints are welcome.  Thanks!\r\n\r\nHere is my sample code and outputs:\r\n\r\n`import tensorflow as tf\r\nimport sys\r\n\r\ncluster_spec = {\"ps\":[\"localhost:8864\"],\r\n                 \"worker\":[\"localhost:8865\",\"localhost:8866\"]}\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    if len(sys.argv) != 3:\r\n        print(\"test.py ps|worker 0|1\")\r\n        exit(1)\r\n\r\n    role = sys.argv[1]\r\n    task_id = int(sys.argv[2])\r\n\r\n    cluster = tf.train.ClusterSpec(cluster_spec)\r\n    server = tf.train.Server(cluster, job_name=role, task_index=task_id)\r\n\r\n    if role == \"ps\":\r\n        server.join()\r\n    else:\r\n        with tf.device(\r\n                tf.train.replica_device_setter(cluster=cluster_spec,\r\n                    worker_device=\"/job:worker/task:%d\"%task_id)):\r\n            global_steps = tf.contrib.framework.get_or_create_global_step()\r\n            # a very simple model with pseudo data\r\n            inputs = tf.placeholder(tf.float32, shape=[None, 3])\r\n            labels = tf.placeholder(tf.float32, shape=[None])\r\n            w = tf.Variable([0.11,10,0.3])\r\n            y = tf.reduce_sum(tf.multiply(inputs, w), axis=1)\r\n            loss = tf.losses.mean_squared_error(labels, y)\r\n            opt = tf.train.GradientDescentOptimizer(0.00001)\r\n            opt_sync = tf.train.SyncReplicasOptimizer(opt,\r\n                                                      replicas_to_aggregate=2,\r\n                                                      # total_num_replicas=2,\r\n                                                      use_locking=True)\r\n            hooks = [opt_sync.make_session_run_hook((task_id==0))]\r\n            op = opt_sync.minimize(loss, global_step=global_steps)\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                               is_chief=(task_id==0),\r\n                                               hooks=hooks) as sess:\r\n            for i in range(10):\r\n                j = sess.run(global_steps)\r\n                t = sess.run(loss, feed_dict={inputs:[[1,2,3]],labels:[1]})\r\n                sess.run(op, feed_dict={inputs:[[1,2,3],[2,3,1]],labels:[2,1]})\r\n                print(\"%d %d %f\"%(i,j,t))\r\n\r\n        print(\"finished\")`\r\n\r\nfrom worker 1:\r\n`2017-10-03 15:48:37.079376: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session eb05b8a25a96918e with config: \r\n0 0 400.400116\r\n1 1 400.16366`\r\n\r\nfrom worker 2:\r\n`2017-10-03 15:48:3 6.882892: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session e54098121f5f8494 with config: \r\n0 0 400.400116\r\n1 1 400.400116`\r\n"}