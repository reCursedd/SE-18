{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/219939220", "html_url": "https://github.com/tensorflow/tensorflow/issues/2386#issuecomment-219939220", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2386", "id": 219939220, "node_id": "MDEyOklzc3VlQ29tbWVudDIxOTkzOTIyMA==", "user": {"login": "jmchen-g", "id": 15792374, "node_id": "MDQ6VXNlcjE1NzkyMzc0", "avatar_url": "https://avatars3.githubusercontent.com/u/15792374?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmchen-g", "html_url": "https://github.com/jmchen-g", "followers_url": "https://api.github.com/users/jmchen-g/followers", "following_url": "https://api.github.com/users/jmchen-g/following{/other_user}", "gists_url": "https://api.github.com/users/jmchen-g/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmchen-g/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmchen-g/subscriptions", "organizations_url": "https://api.github.com/users/jmchen-g/orgs", "repos_url": "https://api.github.com/users/jmchen-g/repos", "events_url": "https://api.github.com/users/jmchen-g/events{/privacy}", "received_events_url": "https://api.github.com/users/jmchen-g/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-18T06:43:40Z", "updated_at": "2016-05-18T06:43:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Actually the inception distributed train is using slim which has its own trick related to device setter. For you case, can you try follow the instructions for the replica_device_setter:</p>\n<h1>To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker</h1>\n<h1>jobs on hosts worker0, worker1 and worker2.</h1>\n<p>cluster_spec = {<br>\n\"ps\": [\"ps0:2222\", \"ps1:2222\"],<br>\n\"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}<br>\nwith tf.device(tf.replica_device_setter(cluster=cluster_spec)):<br>\n# Build your graph<br>\nv1 = tf.Variable(...)  # assigned to /job:ps/task:0<br>\nv2 = tf.Variable(...)  # assigned to /job:ps/task:1<br>\nv3 = tf.Variable(...)  # assigned to /job:ps/task:0</p>", "body_text": "Actually the inception distributed train is using slim which has its own trick related to device setter. For you case, can you try follow the instructions for the replica_device_setter:\nTo build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker\njobs on hosts worker0, worker1 and worker2.\ncluster_spec = {\n\"ps\": [\"ps0:2222\", \"ps1:2222\"],\n\"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}\nwith tf.device(tf.replica_device_setter(cluster=cluster_spec)):\n# Build your graph\nv1 = tf.Variable(...)  # assigned to /job:ps/task:0\nv2 = tf.Variable(...)  # assigned to /job:ps/task:1\nv3 = tf.Variable(...)  # assigned to /job:ps/task:0", "body": "Actually the inception distributed train is using slim which has its own trick related to device setter. For you case, can you try follow the instructions for the replica_device_setter:\n  # To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker\n  # jobs on hosts worker0, worker1 and worker2.\n  cluster_spec = {\n      \"ps\": [\"ps0:2222\", \"ps1:2222\"],\n      \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]}\n  with tf.device(tf.replica_device_setter(cluster=cluster_spec)):\n    # Build your graph\n    v1 = tf.Variable(...)  # assigned to /job:ps/task:0\n    v2 = tf.Variable(...)  # assigned to /job:ps/task:1\n    v3 = tf.Variable(...)  # assigned to /job:ps/task:0\n"}