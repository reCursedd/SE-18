{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/219953503", "html_url": "https://github.com/tensorflow/tensorflow/issues/2386#issuecomment-219953503", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2386", "id": 219953503, "node_id": "MDEyOklzc3VlQ29tbWVudDIxOTk1MzUwMw==", "user": {"login": "jmchen-g", "id": 15792374, "node_id": "MDQ6VXNlcjE1NzkyMzc0", "avatar_url": "https://avatars3.githubusercontent.com/u/15792374?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmchen-g", "html_url": "https://github.com/jmchen-g", "followers_url": "https://api.github.com/users/jmchen-g/followers", "following_url": "https://api.github.com/users/jmchen-g/following{/other_user}", "gists_url": "https://api.github.com/users/jmchen-g/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmchen-g/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmchen-g/subscriptions", "organizations_url": "https://api.github.com/users/jmchen-g/orgs", "repos_url": "https://api.github.com/users/jmchen-g/repos", "events_url": "https://api.github.com/users/jmchen-g/events{/privacy}", "received_events_url": "https://api.github.com/users/jmchen-g/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-18T07:58:12Z", "updated_at": "2016-05-18T07:58:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Can you remove all the checkpoint before starting the run?</p>\n<p>If you want to have checkpoint, you need to make sure the path is<br>\naccessible from all PS and chief worker.</p>\n<p>On Wed, May 18, 2016 at 12:52 AM, smartcat2010 <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15792374\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jmchen-g\">@jmchen-g</a> <a href=\"https://github.com/jmchen-g\">https://github.com/jmchen-g</a><br>\nThank you very much for your quick help!<br>\nI updated my code following your guide. I run 1 ps and 2 workers like this:</p>\n<p>cluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"], \"worker\" : [\"10.141.33.61:2223\", \"10.141.33.65:2223\"] })<br>\nwith tf.device(tf.train.replica_device_setter(cluster = cluster_spec)) :</p>\n<p>My code file:<br>\nmnist_softmax.py.txt<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/269861/mnist_softmax.py.txt\">https://github.com/tensorflow/tensorflow/files/269861/mnist_softmax.py.txt</a></p>\n<p>Then worker0 runs smoothly, worker1 shows the following error:</p>\n<p>I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -&gt; {10.141.33.61:2222}<br>\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -&gt; {10.141.33.61:2223, localhost:2223}<br>\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223<br>\nTraceback (most recent call last):<br>\nFile \"./mnist_softmax.py\", line 83, in <br>\ntf.app.run()<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run<br>\nsys.exit(main(sys.argv))<br>\nFile \"./mnist_softmax.py\", line 80, in main<br>\nrun_training(server, cluster_spec)<br>\nFile \"./mnist_softmax.py\", line 70, in run_training<br>\n<em>, cost, acc, step = sess.run([train_step, cross_entropy, accuracy, global_step], feed_dict = { x: source_data, y</em> : labels_one_hot })<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run<br>\nrun_metadata_ptr)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run<br>\nfeed_dict_string, options, run_metadata)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run<br>\ntarget_list, options, run_metadata)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call<br>\ne.code)<br>\ntensorflow.python.framework.errors.InvalidArgumentError: Expected size[0] in [0, 0], but got 1<br>\n[[Node: get_local_step = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/gpu:0\"](local_steps_S11, Reshape, get_local_step/size)]]<br>\nCaused by op u'get_local_step', defined at:<br>\nFile \"./mnist_softmax.py\", line 83, in <br>\ntf.app.run()<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run<br>\nsys.exit(main(sys.argv))<br>\nFile \"./mnist_softmax.py\", line 80, in main<br>\nrun_training(server, cluster_spec)<br>\nFile \"./mnist_softmax.py\", line 45, in run_training<br>\ntrain_step = opt.minimize(cross_entropy, global_step = global_step)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 192, in minimize<br>\nname=name)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 334, in apply_gradients<br>\nname=\"get_local_step\")<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 217, in slice<br>\nreturn gen_array_ops.<em>slice(input</em>, begin, size, name=name)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1318, in _slice<br>\nname=name)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op<br>\nop_def=op_def)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in <strong>init</strong><br>\nself._traceback = _extract_stack()</p>\n<p>When I change it to run the 2 workers in the same machine as follows, then<br>\nboth of the workers runs smoothly.<br>\ncluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"],<br>\n\"worker\" : [\"10.141.33.61:2223\", \"10.141.33.61:2224\"] })</p>\n<p>So is there something wrong in my multi-machine code?</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly or view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"155022605\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2386\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2386/hovercard?comment_id=219951942&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/2386#issuecomment-219951942\">#2386 (comment)</a></p>\n</blockquote>", "body_text": "Can you remove all the checkpoint before starting the run?\nIf you want to have checkpoint, you need to make sure the path is\naccessible from all PS and chief worker.\nOn Wed, May 18, 2016 at 12:52 AM, smartcat2010 notifications@github.com\nwrote:\n\n@jmchen-g https://github.com/jmchen-g\nThank you very much for your quick help!\nI updated my code following your guide. I run 1 ps and 2 workers like this:\ncluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"], \"worker\" : [\"10.141.33.61:2223\", \"10.141.33.65:2223\"] })\nwith tf.device(tf.train.replica_device_setter(cluster = cluster_spec)) :\nMy code file:\nmnist_softmax.py.txt\nhttps://github.com/tensorflow/tensorflow/files/269861/mnist_softmax.py.txt\nThen worker0 runs smoothly, worker1 shows the following error:\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {10.141.33.61:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {10.141.33.61:2223, localhost:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nTraceback (most recent call last):\nFile \"./mnist_softmax.py\", line 83, in \ntf.app.run()\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\nsys.exit(main(sys.argv))\nFile \"./mnist_softmax.py\", line 80, in main\nrun_training(server, cluster_spec)\nFile \"./mnist_softmax.py\", line 70, in run_training\n, cost, acc, step = sess.run([train_step, cross_entropy, accuracy, global_step], feed_dict = { x: source_data, y : labels_one_hot })\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\nrun_metadata_ptr)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\nfeed_dict_string, options, run_metadata)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\ntarget_list, options, run_metadata)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\ne.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Expected size[0] in [0, 0], but got 1\n[[Node: get_local_step = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/gpu:0\"](local_steps_S11, Reshape, get_local_step/size)]]\nCaused by op u'get_local_step', defined at:\nFile \"./mnist_softmax.py\", line 83, in \ntf.app.run()\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\nsys.exit(main(sys.argv))\nFile \"./mnist_softmax.py\", line 80, in main\nrun_training(server, cluster_spec)\nFile \"./mnist_softmax.py\", line 45, in run_training\ntrain_step = opt.minimize(cross_entropy, global_step = global_step)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 192, in minimize\nname=name)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 334, in apply_gradients\nname=\"get_local_step\")\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 217, in slice\nreturn gen_array_ops.slice(input, begin, size, name=name)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1318, in _slice\nname=name)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\nop_def=op_def)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in init\nself._traceback = _extract_stack()\nWhen I change it to run the 2 workers in the same machine as follows, then\nboth of the workers runs smoothly.\ncluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"],\n\"worker\" : [\"10.141.33.61:2223\", \"10.141.33.61:2224\"] })\nSo is there something wrong in my multi-machine code?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n#2386 (comment)", "body": "Can you remove all the checkpoint before starting the run?\n\nIf you want to have checkpoint, you need to make sure the path is\naccessible from all PS and chief worker.\n\nOn Wed, May 18, 2016 at 12:52 AM, smartcat2010 notifications@github.com\nwrote:\n\n> @jmchen-g https://github.com/jmchen-g\n> Thank you very much for your quick help!\n> I updated my code following your guide. I run 1 ps and 2 workers like this:\n> \n> cluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"], \"worker\" : [\"10.141.33.61:2223\", \"10.141.33.65:2223\"] })\n> with tf.device(tf.train.replica_device_setter(cluster = cluster_spec)) :\n> \n> My code file:\n> mnist_softmax.py.txt\n> https://github.com/tensorflow/tensorflow/files/269861/mnist_softmax.py.txt\n> \n> Then worker0 runs smoothly, worker1 shows the following error:\n> \n> I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {10.141.33.61:2222}\n> I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {10.141.33.61:2223, localhost:2223}\n> I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\n> Traceback (most recent call last):\n>   File \"./mnist_softmax.py\", line 83, in <module>\n>     tf.app.run()\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n>     sys.exit(main(sys.argv))\n>   File \"./mnist_softmax.py\", line 80, in main\n>     run_training(server, cluster_spec)\n>   File \"./mnist_softmax.py\", line 70, in run_training\n>     _, cost, acc, step = sess.run([train_step, cross_entropy, accuracy, global_step], feed_dict = { x: source_data, y_ : labels_one_hot })\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n>     run_metadata_ptr)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n>     feed_dict_string, options, run_metadata)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n>     target_list, options, run_metadata)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n>     e.code)\n> tensorflow.python.framework.errors.InvalidArgumentError: Expected size[0] in [0, 0], but got 1\n>      [[Node: get_local_step = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/gpu:0\"](local_steps_S11, Reshape, get_local_step/size)]]\n> Caused by op u'get_local_step', defined at:\n>   File \"./mnist_softmax.py\", line 83, in <module>\n>     tf.app.run()\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n>     sys.exit(main(sys.argv))\n>   File \"./mnist_softmax.py\", line 80, in main\n>     run_training(server, cluster_spec)\n>   File \"./mnist_softmax.py\", line 45, in run_training\n>     train_step = opt.minimize(cross_entropy, global_step = global_step)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 192, in minimize\n>     name=name)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 334, in apply_gradients\n>     name=\"get_local_step\")\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 217, in slice\n>     return gen_array_ops._slice(input_, begin, size, name=name)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1318, in _slice\n>     name=name)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n>     op_def=op_def)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n>     self._traceback = _extract_stack()\n> \n> When I change it to run the 2 workers in the same machine as follows, then\n> both of the workers runs smoothly.\n> cluster_spec = tf.train.ClusterSpec({ \"ps\":[\"10.141.33.61:2222\"],\n> \"worker\" : [\"10.141.33.61:2223\", \"10.141.33.61:2224\"] })\n> \n> So is there something wrong in my multi-machine code?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2386#issuecomment-219951942\n"}