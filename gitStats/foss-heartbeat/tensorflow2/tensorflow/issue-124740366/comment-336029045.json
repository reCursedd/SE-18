{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/336029045", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-336029045", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 336029045, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjAyOTA0NQ==", "user": {"login": "ModarTensai", "id": 7101743, "node_id": "MDQ6VXNlcjcxMDE3NDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/7101743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ModarTensai", "html_url": "https://github.com/ModarTensai", "followers_url": "https://api.github.com/users/ModarTensai/followers", "following_url": "https://api.github.com/users/ModarTensai/following{/other_user}", "gists_url": "https://api.github.com/users/ModarTensai/gists{/gist_id}", "starred_url": "https://api.github.com/users/ModarTensai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ModarTensai/subscriptions", "organizations_url": "https://api.github.com/users/ModarTensai/orgs", "repos_url": "https://api.github.com/users/ModarTensai/repos", "events_url": "https://api.github.com/users/ModarTensai/events{/privacy}", "received_events_url": "https://api.github.com/users/ModarTensai/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-12T06:00:09Z", "updated_at": "2017-10-22T00:26:36Z", "author_association": "NONE", "body_html": "<p>For me, I prefer computing the Jacobian matrix using very lightweight operations in the graph.<br>\nSince tf.gradients returns the sum, I mask the layer at a single index and then compute the gradient.<br>\nI compute the Jacobian for each point in batches then I stack them at the end outside the graph.<br>\nHere is a running example based on <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26734737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/oborchers\">@oborchers</a> example that produces (s x m x n) array:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">jacobian</span>(<span class=\"pl-smi\">session</span>, <span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">points</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">as_matrix</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>The Jacobian matrix of `y` w.r.t. `x` at `points`</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Let f(x) be some function that has a Jacobian A at point p</span>\n<span class=\"pl-s\">    then, f(p) = y = Ap+b</span>\n<span class=\"pl-s\">    where A of shape mxn, p of shape nx1 and b of shape mx1</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        y: The output tensor</span>\n<span class=\"pl-s\">        x: The input tensor</span>\n<span class=\"pl-s\">        points: The points of linearization where it can be many points</span>\n<span class=\"pl-s\">            of shape [num_points, *self.features_shape]</span>\n<span class=\"pl-s\">        batch_size: How many rows of the Jacobian to compute at once</span>\n<span class=\"pl-s\">        as_matrix: Whether to return the Jacobian as a matrix or retain</span>\n<span class=\"pl-s\">            the shape of the input</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        The Jacobian matrices for the given points</span>\n<span class=\"pl-s\">        of shape [num_points, *jacobian_shape]</span>\n<span class=\"pl-s\">        If `as_matrix`, jacobian_shape is [y.size, *x.shape]</span>\n<span class=\"pl-s\">        else, jacobian_shape is [y.size, x.size]</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> add and/or get cached ops to the graph</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">hasattr</span>(session.graph, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_placeholder<span class=\"pl-pds\">\"</span></span>):\n        session.graph._placeholder <span class=\"pl-k\">=</span> {}\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">hasattr</span>(session.graph, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_gradient<span class=\"pl-pds\">\"</span></span>):\n        session.graph._gradient <span class=\"pl-k\">=</span> {}\n    <span class=\"pl-k\">with</span> session.graph.as_default():\n        <span class=\"pl-k\">if</span> y.dtype <span class=\"pl-k\">in</span> session.graph._placeholder:\n            placeholder <span class=\"pl-k\">=</span> session.graph._placeholder[y.dtype]\n        <span class=\"pl-k\">else</span>:\n            placeholder <span class=\"pl-k\">=</span> tf.placeholder(y.dtype)\n            session.graph._placeholder[y.dtype] <span class=\"pl-k\">=</span> placeholder\n\n        <span class=\"pl-k\">if</span> (y, x) <span class=\"pl-k\">in</span> session.graph._gradient:\n            gradient <span class=\"pl-k\">=</span> session.graph._gradient[(y, x)]\n        <span class=\"pl-k\">else</span>:\n            gradient <span class=\"pl-k\">=</span> tf.gradients(placeholder <span class=\"pl-k\">*</span> y, x)[<span class=\"pl-c1\">0</span>]\n            session.graph._gradient[(y, x)] <span class=\"pl-k\">=</span> gradient\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> extract the Jacobians for all points</span>\n    jacobians_list <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(points.shape[<span class=\"pl-c1\">0</span>]):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> extract the Jacobian matrix for a single point</span>\n        partials_list <span class=\"pl-k\">=</span> []\n        point <span class=\"pl-k\">=</span> points[i:i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, :]\n        shape <span class=\"pl-k\">=</span> y.shape.as_list()[<span class=\"pl-c1\">1</span>:]\n        repeated_point <span class=\"pl-k\">=</span> point\n        <span class=\"pl-k\">for</span> mask <span class=\"pl-k\">in</span> masks_batches(shape, batch_size):\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> repeat the point according to the mask's batch_size</span>\n            batch_size <span class=\"pl-k\">=</span> mask.shape[<span class=\"pl-c1\">0</span>]\n            <span class=\"pl-k\">if</span> repeated_point.shape[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">&lt;</span> batch_size:\n                repeated_point <span class=\"pl-k\">=</span> np.vstack([point] <span class=\"pl-k\">*</span> batch_size)\n            <span class=\"pl-k\">if</span> repeated_point.shape[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">&gt;</span> batch_size:\n                repeated_point <span class=\"pl-k\">=</span> repeated_point[:batch_size, :]\n            feed <span class=\"pl-k\">=</span> {placeholder: mask, x: repeated_point}\n            partial <span class=\"pl-k\">=</span> session.run(gradient, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed)\n            partials_list.append(partial)\n        jacobian <span class=\"pl-k\">=</span> np.vstack(partials_list)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> reshape it as a matrix</span>\n        <span class=\"pl-k\">if</span> as_matrix:\n            jacobian <span class=\"pl-k\">=</span> jacobian.reshape(jacobian.shape[<span class=\"pl-c1\">0</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n        jacobians_list.append(jacobian)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> stack Jacobians</span>\n    jacobians <span class=\"pl-k\">=</span> np.stack(jacobians_list)\n\n    <span class=\"pl-k\">return</span> jacobians\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">masks_batches</span>(<span class=\"pl-smi\">shape</span>, <span class=\"pl-smi\">batch_size</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Batches iterator over all possible masks of the given shape</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    A mask is a numpy.ndarray of shape `shape` of all zeros except</span>\n<span class=\"pl-s\">    for a single position it is one. It is useful to get those masks</span>\n<span class=\"pl-s\">    in batches instead of getting them one by one.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        shape: The shape of each mask</span>\n<span class=\"pl-s\">        batch_size: How many masks to return in each iteration</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        A batch of masks of shape [batch_size, *shape]</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    num_rows <span class=\"pl-k\">=</span> np.prod(shape)\n    <span class=\"pl-k\">if</span> num_rows <span class=\"pl-k\">&lt;</span> batch_size:\n        batch_size <span class=\"pl-k\">=</span> num_rows\n\n    eye <span class=\"pl-k\">=</span> np.eye(batch_size)\n    _mask <span class=\"pl-k\">=</span> np.zeros((batch_size, <span class=\"pl-k\">*</span>shape))\n    mask <span class=\"pl-k\">=</span> _mask.reshape(batch_size, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n    num_batches <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>(<span class=\"pl-k\">-</span>num_rows <span class=\"pl-k\">//</span> batch_size)\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_batches):\n        start <span class=\"pl-k\">=</span> i <span class=\"pl-k\">*</span> batch_size\n        end <span class=\"pl-k\">=</span> <span class=\"pl-c1\">min</span>(start <span class=\"pl-k\">+</span> batch_size, num_rows)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> check if last batch is smaller than batch size</span>\n        <span class=\"pl-k\">if</span> end <span class=\"pl-k\">-</span> start <span class=\"pl-k\">&lt;</span> batch_size:\n            batch_size <span class=\"pl-k\">=</span> end <span class=\"pl-k\">-</span> start\n            eye <span class=\"pl-k\">=</span> np.eye(batch_size)\n            _mask <span class=\"pl-k\">=</span> np.zeros((batch_size, <span class=\"pl-k\">*</span>shape))\n            mask <span class=\"pl-k\">=</span> _mask.reshape(batch_size, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n        mask[:, start:end] <span class=\"pl-k\">=</span> eye\n        <span class=\"pl-k\">yield</span> _mask\n        mask[:, start:end] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    m <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">500</span>\n    s <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\n\n    x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\n    w <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal([n, m], <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>))\n    b <span class=\"pl-k\">=</span> tf.Variable(tf.constant(<span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[m]))\n    y <span class=\"pl-k\">=</span> tf.matmul(x, w) <span class=\"pl-k\">+</span> b\n\n    init <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n    sess <span class=\"pl-k\">=</span> tf.Session()\n    sess.run(init)\n\n    start <span class=\"pl-k\">=</span> time.time()\n    j_out <span class=\"pl-k\">=</span> jacobian(sess, y, x, np.random.rand(s, n), m)\n    w_out <span class=\"pl-k\">=</span> sess.run(w)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> they should be equal and error ~ &lt; 1e-6 (single precision)</span>\n    error <span class=\"pl-k\">=</span> np.linalg.norm(w_out.T <span class=\"pl-k\">-</span> np.mean(j_out, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>))\n    <span class=\"pl-k\">if</span> error <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1e-6</span>:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Correct Jacobian!<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Error was <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(error))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">str</span>(<span class=\"pl-c1\">int</span>(time.time() <span class=\"pl-k\">-</span> start)) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> Seconds: <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(j_out.shape))\n</pre></div>", "body_text": "For me, I prefer computing the Jacobian matrix using very lightweight operations in the graph.\nSince tf.gradients returns the sum, I mask the layer at a single index and then compute the gradient.\nI compute the Jacobian for each point in batches then I stack them at the end outside the graph.\nHere is a running example based on @oborchers example that produces (s x m x n) array:\nimport time\nimport numpy as np\nimport tensorflow as tf\n\n\ndef jacobian(session, y, x, points, batch_size, as_matrix=True):\n    \"\"\"The Jacobian matrix of `y` w.r.t. `x` at `points`\n\n    Let f(x) be some function that has a Jacobian A at point p\n    then, f(p) = y = Ap+b\n    where A of shape mxn, p of shape nx1 and b of shape mx1\n\n    Args:\n        y: The output tensor\n        x: The input tensor\n        points: The points of linearization where it can be many points\n            of shape [num_points, *self.features_shape]\n        batch_size: How many rows of the Jacobian to compute at once\n        as_matrix: Whether to return the Jacobian as a matrix or retain\n            the shape of the input\n\n    Returns:\n        The Jacobian matrices for the given points\n        of shape [num_points, *jacobian_shape]\n        If `as_matrix`, jacobian_shape is [y.size, *x.shape]\n        else, jacobian_shape is [y.size, x.size]\n    \"\"\"\n    # add and/or get cached ops to the graph\n    if not hasattr(session.graph, \"_placeholder\"):\n        session.graph._placeholder = {}\n    if not hasattr(session.graph, \"_gradient\"):\n        session.graph._gradient = {}\n    with session.graph.as_default():\n        if y.dtype in session.graph._placeholder:\n            placeholder = session.graph._placeholder[y.dtype]\n        else:\n            placeholder = tf.placeholder(y.dtype)\n            session.graph._placeholder[y.dtype] = placeholder\n\n        if (y, x) in session.graph._gradient:\n            gradient = session.graph._gradient[(y, x)]\n        else:\n            gradient = tf.gradients(placeholder * y, x)[0]\n            session.graph._gradient[(y, x)] = gradient\n\n    # extract the Jacobians for all points\n    jacobians_list = []\n    for i in range(points.shape[0]):\n        # extract the Jacobian matrix for a single point\n        partials_list = []\n        point = points[i:i + 1, :]\n        shape = y.shape.as_list()[1:]\n        repeated_point = point\n        for mask in masks_batches(shape, batch_size):\n            # repeat the point according to the mask's batch_size\n            batch_size = mask.shape[0]\n            if repeated_point.shape[0] < batch_size:\n                repeated_point = np.vstack([point] * batch_size)\n            if repeated_point.shape[0] > batch_size:\n                repeated_point = repeated_point[:batch_size, :]\n            feed = {placeholder: mask, x: repeated_point}\n            partial = session.run(gradient, feed_dict=feed)\n            partials_list.append(partial)\n        jacobian = np.vstack(partials_list)\n\n        # reshape it as a matrix\n        if as_matrix:\n            jacobian = jacobian.reshape(jacobian.shape[0], -1)\n\n        jacobians_list.append(jacobian)\n\n    # stack Jacobians\n    jacobians = np.stack(jacobians_list)\n\n    return jacobians\n\n\ndef masks_batches(shape, batch_size):\n    \"\"\"Batches iterator over all possible masks of the given shape\n\n    A mask is a numpy.ndarray of shape `shape` of all zeros except\n    for a single position it is one. It is useful to get those masks\n    in batches instead of getting them one by one.\n\n    Args:\n        shape: The shape of each mask\n        batch_size: How many masks to return in each iteration\n\n    Returns:\n        A batch of masks of shape [batch_size, *shape]\n    \"\"\"\n    num_rows = np.prod(shape)\n    if num_rows < batch_size:\n        batch_size = num_rows\n\n    eye = np.eye(batch_size)\n    _mask = np.zeros((batch_size, *shape))\n    mask = _mask.reshape(batch_size, -1)\n\n    num_batches = -(-num_rows // batch_size)\n    for i in range(num_batches):\n        start = i * batch_size\n        end = min(start + batch_size, num_rows)\n\n        # check if last batch is smaller than batch size\n        if end - start < batch_size:\n            batch_size = end - start\n            eye = np.eye(batch_size)\n            _mask = np.zeros((batch_size, *shape))\n            mask = _mask.reshape(batch_size, -1)\n\n        mask[:, start:end] = eye\n        yield _mask\n        mask[:, start:end] = 0\n\n\nif __name__ == '__main__':\n    m = 10\n    n = 500\n    s = 20\n\n    x = tf.placeholder(tf.float32)\n    w = tf.Variable(tf.truncated_normal([n, m], stddev=0.1))\n    b = tf.Variable(tf.constant(0.1, shape=[m]))\n    y = tf.matmul(x, w) + b\n\n    init = tf.global_variables_initializer()\n    sess = tf.Session()\n    sess.run(init)\n\n    start = time.time()\n    j_out = jacobian(sess, y, x, np.random.rand(s, n), m)\n    w_out = sess.run(w)\n    # they should be equal and error ~ < 1e-6 (single precision)\n    error = np.linalg.norm(w_out.T - np.mean(j_out, axis=0))\n    if error < 1e-6:\n        print(\"Correct Jacobian!\")\n    else:\n        print(\"Error was {}\".format(error))\n    print(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))", "body": "For me, I prefer computing the Jacobian matrix using very lightweight operations in the graph.\r\nSince tf.gradients returns the sum, I mask the layer at a single index and then compute the gradient.\r\nI compute the Jacobian for each point in batches then I stack them at the end outside the graph. \r\nHere is a running example based on @oborchers example that produces (s x m x n) array:\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef jacobian(session, y, x, points, batch_size, as_matrix=True):\r\n    \"\"\"The Jacobian matrix of `y` w.r.t. `x` at `points`\r\n\r\n    Let f(x) be some function that has a Jacobian A at point p\r\n    then, f(p) = y = Ap+b\r\n    where A of shape mxn, p of shape nx1 and b of shape mx1\r\n\r\n    Args:\r\n        y: The output tensor\r\n        x: The input tensor\r\n        points: The points of linearization where it can be many points\r\n            of shape [num_points, *self.features_shape]\r\n        batch_size: How many rows of the Jacobian to compute at once\r\n        as_matrix: Whether to return the Jacobian as a matrix or retain\r\n            the shape of the input\r\n\r\n    Returns:\r\n        The Jacobian matrices for the given points\r\n        of shape [num_points, *jacobian_shape]\r\n        If `as_matrix`, jacobian_shape is [y.size, *x.shape]\r\n        else, jacobian_shape is [y.size, x.size]\r\n    \"\"\"\r\n    # add and/or get cached ops to the graph\r\n    if not hasattr(session.graph, \"_placeholder\"):\r\n        session.graph._placeholder = {}\r\n    if not hasattr(session.graph, \"_gradient\"):\r\n        session.graph._gradient = {}\r\n    with session.graph.as_default():\r\n        if y.dtype in session.graph._placeholder:\r\n            placeholder = session.graph._placeholder[y.dtype]\r\n        else:\r\n            placeholder = tf.placeholder(y.dtype)\r\n            session.graph._placeholder[y.dtype] = placeholder\r\n\r\n        if (y, x) in session.graph._gradient:\r\n            gradient = session.graph._gradient[(y, x)]\r\n        else:\r\n            gradient = tf.gradients(placeholder * y, x)[0]\r\n            session.graph._gradient[(y, x)] = gradient\r\n\r\n    # extract the Jacobians for all points\r\n    jacobians_list = []\r\n    for i in range(points.shape[0]):\r\n        # extract the Jacobian matrix for a single point\r\n        partials_list = []\r\n        point = points[i:i + 1, :]\r\n        shape = y.shape.as_list()[1:]\r\n        repeated_point = point\r\n        for mask in masks_batches(shape, batch_size):\r\n            # repeat the point according to the mask's batch_size\r\n            batch_size = mask.shape[0]\r\n            if repeated_point.shape[0] < batch_size:\r\n                repeated_point = np.vstack([point] * batch_size)\r\n            if repeated_point.shape[0] > batch_size:\r\n                repeated_point = repeated_point[:batch_size, :]\r\n            feed = {placeholder: mask, x: repeated_point}\r\n            partial = session.run(gradient, feed_dict=feed)\r\n            partials_list.append(partial)\r\n        jacobian = np.vstack(partials_list)\r\n\r\n        # reshape it as a matrix\r\n        if as_matrix:\r\n            jacobian = jacobian.reshape(jacobian.shape[0], -1)\r\n\r\n        jacobians_list.append(jacobian)\r\n\r\n    # stack Jacobians\r\n    jacobians = np.stack(jacobians_list)\r\n\r\n    return jacobians\r\n\r\n\r\ndef masks_batches(shape, batch_size):\r\n    \"\"\"Batches iterator over all possible masks of the given shape\r\n\r\n    A mask is a numpy.ndarray of shape `shape` of all zeros except\r\n    for a single position it is one. It is useful to get those masks\r\n    in batches instead of getting them one by one.\r\n\r\n    Args:\r\n        shape: The shape of each mask\r\n        batch_size: How many masks to return in each iteration\r\n\r\n    Returns:\r\n        A batch of masks of shape [batch_size, *shape]\r\n    \"\"\"\r\n    num_rows = np.prod(shape)\r\n    if num_rows < batch_size:\r\n        batch_size = num_rows\r\n\r\n    eye = np.eye(batch_size)\r\n    _mask = np.zeros((batch_size, *shape))\r\n    mask = _mask.reshape(batch_size, -1)\r\n\r\n    num_batches = -(-num_rows // batch_size)\r\n    for i in range(num_batches):\r\n        start = i * batch_size\r\n        end = min(start + batch_size, num_rows)\r\n\r\n        # check if last batch is smaller than batch size\r\n        if end - start < batch_size:\r\n            batch_size = end - start\r\n            eye = np.eye(batch_size)\r\n            _mask = np.zeros((batch_size, *shape))\r\n            mask = _mask.reshape(batch_size, -1)\r\n\r\n        mask[:, start:end] = eye\r\n        yield _mask\r\n        mask[:, start:end] = 0\r\n\r\n\r\nif __name__ == '__main__':\r\n    m = 10\r\n    n = 500\r\n    s = 20\r\n\r\n    x = tf.placeholder(tf.float32)\r\n    w = tf.Variable(tf.truncated_normal([n, m], stddev=0.1))\r\n    b = tf.Variable(tf.constant(0.1, shape=[m]))\r\n    y = tf.matmul(x, w) + b\r\n\r\n    init = tf.global_variables_initializer()\r\n    sess = tf.Session()\r\n    sess.run(init)\r\n\r\n    start = time.time()\r\n    j_out = jacobian(sess, y, x, np.random.rand(s, n), m)\r\n    w_out = sess.run(w)\r\n    # they should be equal and error ~ < 1e-6 (single precision)\r\n    error = np.linalg.norm(w_out.T - np.mean(j_out, axis=0))\r\n    if error < 1e-6:\r\n        print(\"Correct Jacobian!\")\r\n    else:\r\n        print(\"Error was {}\".format(error))\r\n    print(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))\r\n\r\n```"}