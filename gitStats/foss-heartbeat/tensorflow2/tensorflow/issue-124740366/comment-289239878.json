{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289239878", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-289239878", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 289239878, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTIzOTg3OA==", "user": {"login": "isabeaups", "id": 12017880, "node_id": "MDQ6VXNlcjEyMDE3ODgw", "avatar_url": "https://avatars2.githubusercontent.com/u/12017880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isabeaups", "html_url": "https://github.com/isabeaups", "followers_url": "https://api.github.com/users/isabeaups/followers", "following_url": "https://api.github.com/users/isabeaups/following{/other_user}", "gists_url": "https://api.github.com/users/isabeaups/gists{/gist_id}", "starred_url": "https://api.github.com/users/isabeaups/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isabeaups/subscriptions", "organizations_url": "https://api.github.com/users/isabeaups/orgs", "repos_url": "https://api.github.com/users/isabeaups/repos", "events_url": "https://api.github.com/users/isabeaups/events{/privacy}", "received_events_url": "https://api.github.com/users/isabeaups/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-25T21:12:44Z", "updated_at": "2017-03-25T21:12:44Z", "author_association": "NONE", "body_html": "<p>Just trying to understand something. I was trying to make a hack of <code>tf.gradient</code> that would give, for a <code>y</code> of rank (M,N) and an <code>x</code> of rank (Q,P) a gradient tensor of rank (M,N,Q,P) as one would naturally expect. However, as mentioned already here, what one gets is a rank (Q,P) which is the grad of the sum of the elements of <code>y</code>.  Now what I can't figure out, looking into the tensorflow code is where is that sum over elements of <code>y</code>  made? Is it as the beginning or at the end? Could someone help me pinpoint the lines of code where that is done?</p>", "body_text": "Just trying to understand something. I was trying to make a hack of tf.gradient that would give, for a y of rank (M,N) and an x of rank (Q,P) a gradient tensor of rank (M,N,Q,P) as one would naturally expect. However, as mentioned already here, what one gets is a rank (Q,P) which is the grad of the sum of the elements of y.  Now what I can't figure out, looking into the tensorflow code is where is that sum over elements of y  made? Is it as the beginning or at the end? Could someone help me pinpoint the lines of code where that is done?", "body": "Just trying to understand something. I was trying to make a hack of `tf.gradient` that would give, for a `y` of rank (M,N) and an `x` of rank (Q,P) a gradient tensor of rank (M,N,Q,P) as one would naturally expect. However, as mentioned already here, what one gets is a rank (Q,P) which is the grad of the sum of the elements of `y`.  Now what I can't figure out, looking into the tensorflow code is where is that sum over elements of `y`  made? Is it as the beginning or at the end? Could someone help me pinpoint the lines of code where that is done? "}