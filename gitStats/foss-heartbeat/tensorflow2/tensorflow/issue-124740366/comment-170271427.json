{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/170271427", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-170271427", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 170271427, "node_id": "MDEyOklzc3VlQ29tbWVudDE3MDI3MTQyNw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-09T19:16:17Z", "updated_at": "2016-01-09T19:29:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Differentiating with respect to one variable is similar to how it works in Theano. I agree it may be confusing when TensorFlow automatically turns many variables into one by taking the sum. An alternative would be to fail if there's more than 1 output variable specified, or have a wrapper that automatically calls existing gradient function on each output variable</p>\n<p>The reason for \"one output variable at a time\" in TensorFlow (and Theano) is because we do reverse mode AD by default. In reverse AD you have a single target scalar quantity and you propagate sensitivities with respect to that quantity. In contrast, if you we did forward AD instead, we would naturally support multiple output variables, but only compute derivative with respect to one scalar variable at a time. Supporting mixed mode propagation to cover \"multiple inputs/multiple outputs\" case in the most efficient way could be a lot of extra plumbing.</p>\n<p>If you have a small number of output variables but large number of input variables, standard thing to do is to apply reverse AD with respect to each variable in a loop. This is what Theano recommends to do for compute Hessian for instance: <a href=\"http://deeplearning.net/software/theano/tutorial/gradients.html#computing-the-hessian\" rel=\"nofollow\">http://deeplearning.net/software/theano/tutorial/gradients.html#computing-the-hessian</a>. If you have a small number of input variables but large number of output variables, then the most efficient thing to do would be to run forward-mode AD for all the input variables in a loop. Forward mode AD is not implemented and would require adding an equivalent of Theano's \"Rop\" operator to differentiable ops and some plumbing to call them instead of existing op \"gradient\" function (existing gradient function is an equivalent of Lop operation, or \"left multiply sensitivity vector by op's jacobian\" operation)</p>", "body_text": "Differentiating with respect to one variable is similar to how it works in Theano. I agree it may be confusing when TensorFlow automatically turns many variables into one by taking the sum. An alternative would be to fail if there's more than 1 output variable specified, or have a wrapper that automatically calls existing gradient function on each output variable\nThe reason for \"one output variable at a time\" in TensorFlow (and Theano) is because we do reverse mode AD by default. In reverse AD you have a single target scalar quantity and you propagate sensitivities with respect to that quantity. In contrast, if you we did forward AD instead, we would naturally support multiple output variables, but only compute derivative with respect to one scalar variable at a time. Supporting mixed mode propagation to cover \"multiple inputs/multiple outputs\" case in the most efficient way could be a lot of extra plumbing.\nIf you have a small number of output variables but large number of input variables, standard thing to do is to apply reverse AD with respect to each variable in a loop. This is what Theano recommends to do for compute Hessian for instance: http://deeplearning.net/software/theano/tutorial/gradients.html#computing-the-hessian. If you have a small number of input variables but large number of output variables, then the most efficient thing to do would be to run forward-mode AD for all the input variables in a loop. Forward mode AD is not implemented and would require adding an equivalent of Theano's \"Rop\" operator to differentiable ops and some plumbing to call them instead of existing op \"gradient\" function (existing gradient function is an equivalent of Lop operation, or \"left multiply sensitivity vector by op's jacobian\" operation)", "body": "Differentiating with respect to one variable is similar to how it works in Theano. I agree it may be confusing when TensorFlow automatically turns many variables into one by taking the sum. An alternative would be to fail if there's more than 1 output variable specified, or have a wrapper that automatically calls existing gradient function on each output variable\n\nThe reason for \"one output variable at a time\" in TensorFlow (and Theano) is because we do reverse mode AD by default. In reverse AD you have a single target scalar quantity and you propagate sensitivities with respect to that quantity. In contrast, if you we did forward AD instead, we would naturally support multiple output variables, but only compute derivative with respect to one scalar variable at a time. Supporting mixed mode propagation to cover \"multiple inputs/multiple outputs\" case in the most efficient way could be a lot of extra plumbing.\n\nIf you have a small number of output variables but large number of input variables, standard thing to do is to apply reverse AD with respect to each variable in a loop. This is what Theano recommends to do for compute Hessian for instance: http://deeplearning.net/software/theano/tutorial/gradients.html#computing-the-hessian. If you have a small number of input variables but large number of output variables, then the most efficient thing to do would be to run forward-mode AD for all the input variables in a loop. Forward mode AD is not implemented and would require adding an equivalent of Theano's \"Rop\" operator to differentiable ops and some plumbing to call them instead of existing op \"gradient\" function (existing gradient function is an equivalent of Lop operation, or \"left multiply sensitivity vector by op's jacobian\" operation) \n"}