{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/168833656", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-168833656", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 168833656, "node_id": "MDEyOklzc3VlQ29tbWVudDE2ODgzMzY1Ng==", "user": {"login": "zackchase", "id": 2390222, "node_id": "MDQ6VXNlcjIzOTAyMjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2390222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zackchase", "html_url": "https://github.com/zackchase", "followers_url": "https://api.github.com/users/zackchase/followers", "following_url": "https://api.github.com/users/zackchase/following{/other_user}", "gists_url": "https://api.github.com/users/zackchase/gists{/gist_id}", "starred_url": "https://api.github.com/users/zackchase/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zackchase/subscriptions", "organizations_url": "https://api.github.com/users/zackchase/orgs", "repos_url": "https://api.github.com/users/zackchase/repos", "events_url": "https://api.github.com/users/zackchase/events{/privacy}", "received_events_url": "https://api.github.com/users/zackchase/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-04T22:37:11Z", "updated_at": "2016-01-04T22:44:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Cool. The terminology gets funny when we talk about rank-R decompositions of tensors, meaning the tensor can be represented as a sum of R outer products of rank-1 tensors, but probably not a problem for us to solve here.</p>\n<p>One thing I thought of is that I would like to compute the frobenius norm of the Jacobian of the log probabilities for use as a smoothness penalty much like the smoothness penalty used in a contractive autoencoder. In this case, as we only seek a scalar at the end, is there a more efficient method than separately calculating the derivative of each output with respect to the inputs?</p>", "body_text": "Cool. The terminology gets funny when we talk about rank-R decompositions of tensors, meaning the tensor can be represented as a sum of R outer products of rank-1 tensors, but probably not a problem for us to solve here.\nOne thing I thought of is that I would like to compute the frobenius norm of the Jacobian of the log probabilities for use as a smoothness penalty much like the smoothness penalty used in a contractive autoencoder. In this case, as we only seek a scalar at the end, is there a more efficient method than separately calculating the derivative of each output with respect to the inputs?", "body": "Cool. The terminology gets funny when we talk about rank-R decompositions of tensors, meaning the tensor can be represented as a sum of R outer products of rank-1 tensors, but probably not a problem for us to solve here.\n\nOne thing I thought of is that I would like to compute the frobenius norm of the Jacobian of the log probabilities for use as a smoothness penalty much like the smoothness penalty used in a contractive autoencoder. In this case, as we only seek a scalar at the end, is there a more efficient method than separately calculating the derivative of each output with respect to the inputs?\n"}