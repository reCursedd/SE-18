{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289242922", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-289242922", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 289242922, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTI0MjkyMg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-25T22:09:32Z", "updated_at": "2017-03-25T22:09:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Note that \"summing ys together\" is sort of buried deep in backprop. Note that TensorFlow uses Reverse Mode AD in order to compute gradients, and reverse mode AD only supports a scalar output function. If you have non-scalar output, we don't have any algorithm much better than calling reverse mode AD on each of the components of the output separate.</p>\n<p>(there's a trick mentioned by ian <a href=\"https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-253056458\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4897/hovercard\">here</a> which is a bit better on Python overhead compared to calling gradients many times)</p>\n<p>That said, the place where <code>summation of ys</code> happens is in line 479-481 in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py#L479\">gradients_impl.py</a></p>\n<p>First note that <code>grad_ys</code> are set to the same value in this line<br>\n<code>grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops) </code><br>\nLater there's this block</p>\n<pre><code>\n # Add the initial gradients for the ys.\n    for y, grad_y in zip(ys, grad_ys):\n      _SetGrad(grads, y, grad_y)\n</code></pre>\n<p>Note that if you had a single y*=y1+y2+y3+..., then the gradients would propagate in the same way -- the same backprop value from y would be copied over to nodes y1,y2,y3, so this block implicitly treats <code>ys</code> as being added up into the final sum</p>", "body_text": "Note that \"summing ys together\" is sort of buried deep in backprop. Note that TensorFlow uses Reverse Mode AD in order to compute gradients, and reverse mode AD only supports a scalar output function. If you have non-scalar output, we don't have any algorithm much better than calling reverse mode AD on each of the components of the output separate.\n(there's a trick mentioned by ian here which is a bit better on Python overhead compared to calling gradients many times)\nThat said, the place where summation of ys happens is in line 479-481 in gradients_impl.py\nFirst note that grad_ys are set to the same value in this line\ngrad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops) \nLater there's this block\n\n # Add the initial gradients for the ys.\n    for y, grad_y in zip(ys, grad_ys):\n      _SetGrad(grads, y, grad_y)\n\nNote that if you had a single y*=y1+y2+y3+..., then the gradients would propagate in the same way -- the same backprop value from y would be copied over to nodes y1,y2,y3, so this block implicitly treats ys as being added up into the final sum", "body": "Note that \"summing ys together\" is sort of buried deep in backprop. Note that TensorFlow uses Reverse Mode AD in order to compute gradients, and reverse mode AD only supports a scalar output function. If you have non-scalar output, we don't have any algorithm much better than calling reverse mode AD on each of the components of the output separate.\r\n\r\n(there's a trick mentioned by ian [here](https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-253056458) which is a bit better on Python overhead compared to calling gradients many times)\r\n\r\nThat said, the place where `summation of ys` happens is in line 479-481 in [gradients_impl.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py#L479)\r\n\r\nFirst note that `grad_ys` are set to the same value in this line\r\n`grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)\r\n`\r\nLater there's this block\r\n```\r\n\r\n # Add the initial gradients for the ys.\r\n    for y, grad_y in zip(ys, grad_ys):\r\n      _SetGrad(grads, y, grad_y)\r\n```\r\n\r\nNote that if you had a single y*=y1+y2+y3+..., then the gradients would propagate in the same way -- the same backprop value from y would be copied over to nodes y1,y2,y3, so this block implicitly treats `ys` as being added up into the final sum"}