{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/168711518", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-168711518", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 168711518, "node_id": "MDEyOklzc3VlQ29tbWVudDE2ODcxMTUxOA==", "user": {"login": "keveman", "id": 229914, "node_id": "MDQ6VXNlcjIyOTkxNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/229914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keveman", "html_url": "https://github.com/keveman", "followers_url": "https://api.github.com/users/keveman/followers", "following_url": "https://api.github.com/users/keveman/following{/other_user}", "gists_url": "https://api.github.com/users/keveman/gists{/gist_id}", "starred_url": "https://api.github.com/users/keveman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keveman/subscriptions", "organizations_url": "https://api.github.com/users/keveman/orgs", "repos_url": "https://api.github.com/users/keveman/repos", "events_url": "https://api.github.com/users/keveman/events{/privacy}", "received_events_url": "https://api.github.com/users/keveman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-01-04T15:45:05Z", "updated_at": "2016-01-04T15:45:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p>zackchase@, you are right about the current <code>gradients</code> function. Currently, you can compute the Jacobian of, say, a vector, by calling <code>gradients</code> multiple times, one for every scalar component (obtained by slicing) of the original vector, and reassembling the results. Contributions are welcome to make this nicer and efficient.</p>", "body_text": "zackchase@, you are right about the current gradients function. Currently, you can compute the Jacobian of, say, a vector, by calling gradients multiple times, one for every scalar component (obtained by slicing) of the original vector, and reassembling the results. Contributions are welcome to make this nicer and efficient.", "body": "zackchase@, you are right about the current `gradients` function. Currently, you can compute the Jacobian of, say, a vector, by calling `gradients` multiple times, one for every scalar component (obtained by slicing) of the original vector, and reassembling the results. Contributions are welcome to make this nicer and efficient.\n"}