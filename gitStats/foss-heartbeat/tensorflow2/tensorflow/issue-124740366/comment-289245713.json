{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289245713", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-289245713", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 289245713, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTI0NTcxMw==", "user": {"login": "isabeaups", "id": 12017880, "node_id": "MDQ6VXNlcjEyMDE3ODgw", "avatar_url": "https://avatars2.githubusercontent.com/u/12017880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isabeaups", "html_url": "https://github.com/isabeaups", "followers_url": "https://api.github.com/users/isabeaups/followers", "following_url": "https://api.github.com/users/isabeaups/following{/other_user}", "gists_url": "https://api.github.com/users/isabeaups/gists{/gist_id}", "starred_url": "https://api.github.com/users/isabeaups/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isabeaups/subscriptions", "organizations_url": "https://api.github.com/users/isabeaups/orgs", "repos_url": "https://api.github.com/users/isabeaups/repos", "events_url": "https://api.github.com/users/isabeaups/events{/privacy}", "received_events_url": "https://api.github.com/users/isabeaups/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-25T23:05:51Z", "updated_at": "2017-03-25T23:09:09Z", "author_association": "NONE", "body_html": "<p>Thanks for your response <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> !</p>\n<p>But there are two things which confuse me from your answer:</p>\n<p><strong>(1)</strong></p>\n<blockquote>\n<p>we don't have any algorithm much better than calling reverse mode AD on each of the components of the output separate.</p>\n</blockquote>\n<p>I'm pretty sure that that cannot be the case. If you think of a deep network outputting a vector <code>y</code>, then the gradient of <code>y[0]</code> is basically the same as that from <code>y[1]</code>, they only defer by the last weight matrix elements. i.e. <code>y[0] = W(0,j) V(j, ...) </code> while <code>y[1] = W(1,j) V(j,...)</code>.</p>\n<p><strong>(2)</strong>     You say the summation happens at line 479-481 of gradients_impl.py<br>\nBut just above those lines, there is the following comment in the code:<br>\n# grads: op =&gt; list of gradients received on each output endpoint of the<br>\n# op.  The gradients for each endpoint are initially collected as a list.<br>\n# When it is time to call the op's gradient function, for each endpoint we<br>\n# aggregate the list of received gradients into a Add() Operation if there<br>\n# is more than one.</p>\n<p>And indeed when one looks into the function <code>_SetGrad</code> there is no addition, only appending to a list, and all the <code>y</code> 's, if I understand correctly, are still kept separate by being different keys to the dictionary <code>grads</code>.</p>\n<p>So I am utterly confused by this.</p>\n<p>Also thank you very much for <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=387866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/goodfeli\">@goodfeli</a> 's trick, but I don't really understand what he means or how to implement it in practice.</p>", "body_text": "Thanks for your response @yaroslavvb !\nBut there are two things which confuse me from your answer:\n(1)\n\nwe don't have any algorithm much better than calling reverse mode AD on each of the components of the output separate.\n\nI'm pretty sure that that cannot be the case. If you think of a deep network outputting a vector y, then the gradient of y[0] is basically the same as that from y[1], they only defer by the last weight matrix elements. i.e. y[0] = W(0,j) V(j, ...)  while y[1] = W(1,j) V(j,...).\n(2)     You say the summation happens at line 479-481 of gradients_impl.py\nBut just above those lines, there is the following comment in the code:\n# grads: op => list of gradients received on each output endpoint of the\n# op.  The gradients for each endpoint are initially collected as a list.\n# When it is time to call the op's gradient function, for each endpoint we\n# aggregate the list of received gradients into a Add() Operation if there\n# is more than one.\nAnd indeed when one looks into the function _SetGrad there is no addition, only appending to a list, and all the y 's, if I understand correctly, are still kept separate by being different keys to the dictionary grads.\nSo I am utterly confused by this.\nAlso thank you very much for @goodfeli 's trick, but I don't really understand what he means or how to implement it in practice.", "body": "Thanks for your response @yaroslavvb !\r\n\r\nBut there are two things which confuse me from your answer:\r\n\r\n**(1)** \r\n\r\n> we don't have any algorithm much better than calling reverse mode AD on each of the components of the output separate.\r\n\r\nI'm pretty sure that that cannot be the case. If you think of a deep network outputting a vector `y`, then the gradient of `y[0]` is basically the same as that from `y[1]`, they only defer by the last weight matrix elements. i.e. `y[0] = W(0,j) V(j, ...) ` while `y[1] = W(1,j) V(j,...)`.\r\n\r\n**(2)**     You say the summation happens at line 479-481 of gradients_impl.py\r\nBut just above those lines, there is the following comment in the code: \r\n    # grads: op => list of gradients received on each output endpoint of the\r\n    # op.  The gradients for each endpoint are initially collected as a list.\r\n    # When it is time to call the op's gradient function, for each endpoint we\r\n    # aggregate the list of received gradients into a Add() Operation if there\r\n    # is more than one.\r\n\r\nAnd indeed when one looks into the function `_SetGrad` there is no addition, only appending to a list, and all the `y` 's, if I understand correctly, are still kept separate by being different keys to the dictionary `grads`.  \r\n\r\nSo I am utterly confused by this.\r\n\r\nAlso thank you very much for @goodfeli 's trick, but I don't really understand what he means or how to implement it in practice. \r\n"}