{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/215130212", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-215130212", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 215130212, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNTEzMDIxMg==", "user": {"login": "tillahoffmann", "id": 966348, "node_id": "MDQ6VXNlcjk2NjM0OA==", "avatar_url": "https://avatars2.githubusercontent.com/u/966348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tillahoffmann", "html_url": "https://github.com/tillahoffmann", "followers_url": "https://api.github.com/users/tillahoffmann/followers", "following_url": "https://api.github.com/users/tillahoffmann/following{/other_user}", "gists_url": "https://api.github.com/users/tillahoffmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/tillahoffmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tillahoffmann/subscriptions", "organizations_url": "https://api.github.com/users/tillahoffmann/orgs", "repos_url": "https://api.github.com/users/tillahoffmann/repos", "events_url": "https://api.github.com/users/tillahoffmann/events{/privacy}", "received_events_url": "https://api.github.com/users/tillahoffmann/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-27T15:58:36Z", "updated_at": "2016-04-27T15:58:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I was hoping to implement higher order derivatives using the map function but am getting an error message I can't quite get my head around. My implementation is (in pseudo code)</p>\n<pre><code>params = tf.Variable(\"some initial value\")\nloss = some_function(params)\ngrads = tf.gradients(loss, params)[0]\nhess = tf.map_fn(lambda grad: tf.gradients(grad, X)[0], grads)\n</code></pre>\n<p>When I fetch the hessian, I get the error message</p>\n<pre><code>InvalidArgumentError: All inputs to node map/while/gradients/map/TensorArrayUnpack_grad/TensorArrayGrad/TensorArrayGrad must be from the same frame.\n</code></pre>\n<p>I assumed that tensorflow has an issue because it doesn't know about <code>params</code> in the loop (cf. <a href=\"http://deeplearning.net/software/theano/library/scan.html\" rel=\"nofollow\"><code>non_sequences</code> in theano <code>scan</code></a>), and extended <code>map_fn</code> to pass extra arguments to the loop. Unfortunately, the extra arguments get wrapped in an identity transformation and <code>tf.gradients(params, tf.identity(params))</code> gives <code>[None]</code>, which seems a bit unintuitive.</p>\n<p>Looping in python is of course fine but I'd like to avoid introducing an extra node to the graph for every parameter. Any suggestions?</p>", "body_text": "I was hoping to implement higher order derivatives using the map function but am getting an error message I can't quite get my head around. My implementation is (in pseudo code)\nparams = tf.Variable(\"some initial value\")\nloss = some_function(params)\ngrads = tf.gradients(loss, params)[0]\nhess = tf.map_fn(lambda grad: tf.gradients(grad, X)[0], grads)\n\nWhen I fetch the hessian, I get the error message\nInvalidArgumentError: All inputs to node map/while/gradients/map/TensorArrayUnpack_grad/TensorArrayGrad/TensorArrayGrad must be from the same frame.\n\nI assumed that tensorflow has an issue because it doesn't know about params in the loop (cf. non_sequences in theano scan), and extended map_fn to pass extra arguments to the loop. Unfortunately, the extra arguments get wrapped in an identity transformation and tf.gradients(params, tf.identity(params)) gives [None], which seems a bit unintuitive.\nLooping in python is of course fine but I'd like to avoid introducing an extra node to the graph for every parameter. Any suggestions?", "body": "I was hoping to implement higher order derivatives using the map function but am getting an error message I can't quite get my head around. My implementation is (in pseudo code)\n\n```\nparams = tf.Variable(\"some initial value\")\nloss = some_function(params)\ngrads = tf.gradients(loss, params)[0]\nhess = tf.map_fn(lambda grad: tf.gradients(grad, X)[0], grads)\n```\n\nWhen I fetch the hessian, I get the error message\n\n```\nInvalidArgumentError: All inputs to node map/while/gradients/map/TensorArrayUnpack_grad/TensorArrayGrad/TensorArrayGrad must be from the same frame.\n```\n\nI assumed that tensorflow has an issue because it doesn't know about `params` in the loop (cf. [`non_sequences` in theano `scan`](http://deeplearning.net/software/theano/library/scan.html)), and extended `map_fn` to pass extra arguments to the loop. Unfortunately, the extra arguments get wrapped in an identity transformation and `tf.gradients(params, tf.identity(params))` gives `[None]`, which seems a bit unintuitive.\n\nLooping in python is of course fine but I'd like to avoid introducing an extra node to the graph for every parameter. Any suggestions?\n"}