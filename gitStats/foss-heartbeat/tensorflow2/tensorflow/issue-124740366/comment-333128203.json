{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/333128203", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-333128203", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 333128203, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzEyODIwMw==", "user": {"login": "oborchers", "id": 26734737, "node_id": "MDQ6VXNlcjI2NzM0NzM3", "avatar_url": "https://avatars2.githubusercontent.com/u/26734737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oborchers", "html_url": "https://github.com/oborchers", "followers_url": "https://api.github.com/users/oborchers/followers", "following_url": "https://api.github.com/users/oborchers/following{/other_user}", "gists_url": "https://api.github.com/users/oborchers/gists{/gist_id}", "starred_url": "https://api.github.com/users/oborchers/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oborchers/subscriptions", "organizations_url": "https://api.github.com/users/oborchers/orgs", "repos_url": "https://api.github.com/users/oborchers/repos", "events_url": "https://api.github.com/users/oborchers/events{/privacy}", "received_events_url": "https://api.github.com/users/oborchers/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-29T13:36:39Z", "updated_at": "2017-09-29T13:53:23Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1128863\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jeisses\">@jeisses</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1217238\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shoyer\">@shoyer</a> I am somewhat confused by the implementation because of the resulting shape of J.</p>\n<p>According to <a href=\"url\">https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant</a><br>\nLet f: R^n -&gt; R^m , then the J(f) is a m * n matrix. Given, that the input has a size s, J should be a s * m * n matrix, which the implementation of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26007201\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/candidj0\">@candidj0</a> gives us. (However, it is slow due to the for loop.)</p>\n<p>[None, n] doesn't work</p>\n<p>Setting s = 1,<br>\nJacobian_1 gives (1, 1, 1, 500) in 3s<br>\nJacobian_2 gives (1, 10, 500) in 3s,<br>\nJacobian_3 gives (1, 10, 1, 500) in 3s,</p>\n<p>whereas an s=20 is outputting<br>\nJacobian_1 gives (20, 1, 20, 500) in 3s,<br>\nJacobian_2 gives (20, 10, 500) in 19s,<br>\nJacobian_3 gives (20, 10, 20, 500) in 61,</p>\n<p>Have I missed a point somewhere in the implementation (because I really like to get that speedup)?</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport time\n\ndef jacobian_1(y_flat, x):\n    n = y_flat.shape[0]\n\n    loop_vars = [\n        tf.constant(0, tf.int32),\n        tf.TensorArray(tf.float32, size=n),\n    ]\n    _, jacobian = tf.while_loop(\n        lambda j, _: j &lt; n,\n        lambda j, result: (j+1, result.write(j, tf.gradients(y_flat[j], x))),\n        loop_vars)\n    return jacobian.stack()\n\ndef jacobian_2(y, x, n):\n    y_list = tf.unstack(y, num = n)\n    jacobian_list = [[tf.gradients(y_, x)[0][i] for y_ in tf.unstack(y_list[i])] for i in range(n)] # list [grad(y0, x), grad(y1, x), ...]\n    return tf.stack(jacobian_list)\n\ndef jacobian_3(y, x):\n  y_flat = tf.reshape(y, (-1,))\n  jacobian_flat = tf.stack(\n      [tf.gradients(y_i, x)[0] for y_i in tf.unstack(y_flat)])\n  return tf.reshape(jacobian_flat, y.shape.concatenate(x.shape))\n\ns = 20\nn = 500\nm = 10\n\nx = tf.placeholder(tf.float32, [s, n])\nw = tf.Variable(tf.truncated_normal([n, m], stddev=0.1))\nb = tf.Variable(tf.constant(0.1, shape=[m]))\ny = tf.matmul(x, w) + b\n\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\nstart = time.time()\nj = jacobian_1(y, x)\nj_out = sess.run(j, feed_dict={x:np.random.rand(s,n)})\nprint(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))\n\nstart = time.time()\nj_2 = jacobian_2(y,x, s)\nj_out = sess.run(j_2, feed_dict={x:np.random.rand(s,n)})\nprint(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))\n\nstart = time.time()\nj_3 = jacobian_3(y,x)\nj_out = sess.run(j_3, feed_dict={x:np.random.rand(s,n)})\nprint(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))\n</code></pre>", "body_text": "@jeisses and @shoyer I am somewhat confused by the implementation because of the resulting shape of J.\nAccording to https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant\nLet f: R^n -> R^m , then the J(f) is a m * n matrix. Given, that the input has a size s, J should be a s * m * n matrix, which the implementation of @candidj0 gives us. (However, it is slow due to the for loop.)\n[None, n] doesn't work\nSetting s = 1,\nJacobian_1 gives (1, 1, 1, 500) in 3s\nJacobian_2 gives (1, 10, 500) in 3s,\nJacobian_3 gives (1, 10, 1, 500) in 3s,\nwhereas an s=20 is outputting\nJacobian_1 gives (20, 1, 20, 500) in 3s,\nJacobian_2 gives (20, 10, 500) in 19s,\nJacobian_3 gives (20, 10, 20, 500) in 61,\nHave I missed a point somewhere in the implementation (because I really like to get that speedup)?\nimport tensorflow as tf\nimport numpy as np\nimport time\n\ndef jacobian_1(y_flat, x):\n    n = y_flat.shape[0]\n\n    loop_vars = [\n        tf.constant(0, tf.int32),\n        tf.TensorArray(tf.float32, size=n),\n    ]\n    _, jacobian = tf.while_loop(\n        lambda j, _: j < n,\n        lambda j, result: (j+1, result.write(j, tf.gradients(y_flat[j], x))),\n        loop_vars)\n    return jacobian.stack()\n\ndef jacobian_2(y, x, n):\n    y_list = tf.unstack(y, num = n)\n    jacobian_list = [[tf.gradients(y_, x)[0][i] for y_ in tf.unstack(y_list[i])] for i in range(n)] # list [grad(y0, x), grad(y1, x), ...]\n    return tf.stack(jacobian_list)\n\ndef jacobian_3(y, x):\n  y_flat = tf.reshape(y, (-1,))\n  jacobian_flat = tf.stack(\n      [tf.gradients(y_i, x)[0] for y_i in tf.unstack(y_flat)])\n  return tf.reshape(jacobian_flat, y.shape.concatenate(x.shape))\n\ns = 20\nn = 500\nm = 10\n\nx = tf.placeholder(tf.float32, [s, n])\nw = tf.Variable(tf.truncated_normal([n, m], stddev=0.1))\nb = tf.Variable(tf.constant(0.1, shape=[m]))\ny = tf.matmul(x, w) + b\n\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\nstart = time.time()\nj = jacobian_1(y, x)\nj_out = sess.run(j, feed_dict={x:np.random.rand(s,n)})\nprint(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))\n\nstart = time.time()\nj_2 = jacobian_2(y,x, s)\nj_out = sess.run(j_2, feed_dict={x:np.random.rand(s,n)})\nprint(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))\n\nstart = time.time()\nj_3 = jacobian_3(y,x)\nj_out = sess.run(j_3, feed_dict={x:np.random.rand(s,n)})\nprint(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))", "body": "@jeisses and @shoyer I am somewhat confused by the implementation because of the resulting shape of J.\r\n\r\nAccording to [https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant](url)\r\nLet f: R^n -> R^m , then the J(f) is a m * n matrix. Given, that the input has a size s, J should be a s * m * n matrix, which the implementation of @candidj0 gives us. (However, it is slow due to the for loop.)\r\n\r\n[None, n] doesn't work\r\n\r\nSetting s = 1, \r\nJacobian_1 gives (1, 1, 1, 500) in 3s \r\nJacobian_2 gives (1, 10, 500) in 3s, \r\nJacobian_3 gives (1, 10, 1, 500) in 3s,\r\n\r\nwhereas an s=20 is outputting\r\nJacobian_1 gives (20, 1, 20, 500) in 3s,\r\nJacobian_2 gives (20, 10, 500) in 19s, \r\nJacobian_3 gives (20, 10, 20, 500) in 61,\r\n\r\nHave I missed a point somewhere in the implementation (because I really like to get that speedup)?\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\ndef jacobian_1(y_flat, x):\r\n    n = y_flat.shape[0]\r\n\r\n    loop_vars = [\r\n        tf.constant(0, tf.int32),\r\n        tf.TensorArray(tf.float32, size=n),\r\n    ]\r\n    _, jacobian = tf.while_loop(\r\n        lambda j, _: j < n,\r\n        lambda j, result: (j+1, result.write(j, tf.gradients(y_flat[j], x))),\r\n        loop_vars)\r\n    return jacobian.stack()\r\n\r\ndef jacobian_2(y, x, n):\r\n    y_list = tf.unstack(y, num = n)\r\n    jacobian_list = [[tf.gradients(y_, x)[0][i] for y_ in tf.unstack(y_list[i])] for i in range(n)] # list [grad(y0, x), grad(y1, x), ...]\r\n    return tf.stack(jacobian_list)\r\n\r\ndef jacobian_3(y, x):\r\n  y_flat = tf.reshape(y, (-1,))\r\n  jacobian_flat = tf.stack(\r\n      [tf.gradients(y_i, x)[0] for y_i in tf.unstack(y_flat)])\r\n  return tf.reshape(jacobian_flat, y.shape.concatenate(x.shape))\r\n\r\ns = 20\r\nn = 500\r\nm = 10\r\n\r\nx = tf.placeholder(tf.float32, [s, n])\r\nw = tf.Variable(tf.truncated_normal([n, m], stddev=0.1))\r\nb = tf.Variable(tf.constant(0.1, shape=[m]))\r\ny = tf.matmul(x, w) + b\r\n\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\nstart = time.time()\r\nj = jacobian_1(y, x)\r\nj_out = sess.run(j, feed_dict={x:np.random.rand(s,n)})\r\nprint(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))\r\n\r\nstart = time.time()\r\nj_2 = jacobian_2(y,x, s)\r\nj_out = sess.run(j_2, feed_dict={x:np.random.rand(s,n)})\r\nprint(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))\r\n\r\nstart = time.time()\r\nj_3 = jacobian_3(y,x)\r\nj_out = sess.run(j_3, feed_dict={x:np.random.rand(s,n)})\r\nprint(str(int(time.time() - start)) + \" Seconds: \" + str(j_out.shape))\r\n```"}