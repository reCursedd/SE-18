{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362853672", "html_url": "https://github.com/tensorflow/tensorflow/issues/675#issuecomment-362853672", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/675", "id": 362853672, "node_id": "MDEyOklzc3VlQ29tbWVudDM2Mjg1MzY3Mg==", "user": {"login": "mholzel", "id": 7227349, "node_id": "MDQ6VXNlcjcyMjczNDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/7227349?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mholzel", "html_url": "https://github.com/mholzel", "followers_url": "https://api.github.com/users/mholzel/followers", "following_url": "https://api.github.com/users/mholzel/following{/other_user}", "gists_url": "https://api.github.com/users/mholzel/gists{/gist_id}", "starred_url": "https://api.github.com/users/mholzel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mholzel/subscriptions", "organizations_url": "https://api.github.com/users/mholzel/orgs", "repos_url": "https://api.github.com/users/mholzel/repos", "events_url": "https://api.github.com/users/mholzel/events{/privacy}", "received_events_url": "https://api.github.com/users/mholzel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-03T20:56:16Z", "updated_at": "2018-02-04T08:23:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Nobody seems to have posted any followup, but the codes proposed by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26007201\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/candidj0\">@candidj0</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1128863\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jeisses\">@jeisses</a> do not work when nested (testing in tensorflow 1.5.0). So computing the hessian by nesting will not work (<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=966348\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tillahoffmann\">@tillahoffmann</a>). Let me make this a bit more concrete. I am using the jacobian</p>\n<pre><code>def map(f, x, dtype=None, parallel_iterations=10):\n    '''\n    Apply f to each of the elements in x using the specified number of parallel iterations.\n\n    Important points:\n    1. By \"elements in x\", we mean that we will be applying f to x[0],...x[tf.shape(x)[0]-1].\n    2. The output size of f(x[i]) can be arbitrary. However, if the dtype of that output\n       is different than the dtype of x, then you need to specify that as an additional argument.\n    '''\n    if dtype is None:\n        dtype = x.dtype\n\n    n = tf.shape(x)[0]\n    loop_vars = [\n        tf.constant(0, n.dtype),\n        tf.TensorArray(dtype, size=n),\n    ]\n    _, fx = tf.while_loop(\n        lambda j, _: j &lt; n,\n        lambda j, result: (j + 1, result.write(j, f(x[j]))),\n        loop_vars,\n        parallel_iterations=parallel_iterations\n    )\n    return fx.stack()\n\ndef jacobian(fx, x, parallel_iterations=10):\n    '''\n    Given a tensor fx, which is a function of x, vectorize fx (via tf.reshape(fx, [-1])),\n    and then compute the jacobian of each entry of fx with respect to x.\n    Specifically, if x has shape (m,n,...,p), and fx has L entries (tf.size(fx)=L), then\n    the output will be (L,m,n,...,p), where output[i] will be (m,n,...,p), with each entry denoting the\n    gradient of output[i] wrt the corresponding element of x.\n    '''\n    return map(lambda fxi: tf.gradients(fxi, x)[0],\n               tf.reshape(fx, [-1]),\n               dtype=x.dtype,\n               parallel_iterations=parallel_iterations)\n</code></pre>\n<p>I am using this because it supports dynamic sizes, that is, one of the dimensions can be <code>None</code>.</p>\n<p>However, this simple test</p>\n<pre><code>    import numpy\n    from numpy.random import randn\n\n    numpy.random.seed(0)\n\n    # Here is how everything would look in numpy\n    x = randn(3, 3)\n    A = randn(2, 3)\n    y = numpy.dot(A,numpy.dot(x,x))\n\n    # and in tensorflow... \n    xtf = tf.constant(x, tf.float64)\n    Atf = tf.constant(A, tf.float64)\n    ytf = tf.matmul(tf.matmul(Atf, xtf), xtf)\n\n    with tf.Session() as sess:\n\n        # Now let's try to compute the jacobian \n        dydx = jacobian(ytf, xtf)\n        print(sess.run(dydx))\n        \n        # and the hessian... \n        d2ydx2 = tf.squeeze(jacobian(dydx, xtf))\n        print(sess.run(d2ydx2))\n</code></pre>\n<p>throws the error</p>\n<p><code>ValueError: Cannot use 'while_1/while/gradients/f_count_1' as input to 'while_1/while/gradients/f_count' because they are in different while loops. See info log for more details. </code></p>\n<p>Does anybody know the issue here?<br>\nThe first jacobian is correct. The second one (essentially the hessian) throws an error.</p>", "body_text": "Nobody seems to have posted any followup, but the codes proposed by @candidj0 and @jeisses do not work when nested (testing in tensorflow 1.5.0). So computing the hessian by nesting will not work (@tillahoffmann). Let me make this a bit more concrete. I am using the jacobian\ndef map(f, x, dtype=None, parallel_iterations=10):\n    '''\n    Apply f to each of the elements in x using the specified number of parallel iterations.\n\n    Important points:\n    1. By \"elements in x\", we mean that we will be applying f to x[0],...x[tf.shape(x)[0]-1].\n    2. The output size of f(x[i]) can be arbitrary. However, if the dtype of that output\n       is different than the dtype of x, then you need to specify that as an additional argument.\n    '''\n    if dtype is None:\n        dtype = x.dtype\n\n    n = tf.shape(x)[0]\n    loop_vars = [\n        tf.constant(0, n.dtype),\n        tf.TensorArray(dtype, size=n),\n    ]\n    _, fx = tf.while_loop(\n        lambda j, _: j < n,\n        lambda j, result: (j + 1, result.write(j, f(x[j]))),\n        loop_vars,\n        parallel_iterations=parallel_iterations\n    )\n    return fx.stack()\n\ndef jacobian(fx, x, parallel_iterations=10):\n    '''\n    Given a tensor fx, which is a function of x, vectorize fx (via tf.reshape(fx, [-1])),\n    and then compute the jacobian of each entry of fx with respect to x.\n    Specifically, if x has shape (m,n,...,p), and fx has L entries (tf.size(fx)=L), then\n    the output will be (L,m,n,...,p), where output[i] will be (m,n,...,p), with each entry denoting the\n    gradient of output[i] wrt the corresponding element of x.\n    '''\n    return map(lambda fxi: tf.gradients(fxi, x)[0],\n               tf.reshape(fx, [-1]),\n               dtype=x.dtype,\n               parallel_iterations=parallel_iterations)\n\nI am using this because it supports dynamic sizes, that is, one of the dimensions can be None.\nHowever, this simple test\n    import numpy\n    from numpy.random import randn\n\n    numpy.random.seed(0)\n\n    # Here is how everything would look in numpy\n    x = randn(3, 3)\n    A = randn(2, 3)\n    y = numpy.dot(A,numpy.dot(x,x))\n\n    # and in tensorflow... \n    xtf = tf.constant(x, tf.float64)\n    Atf = tf.constant(A, tf.float64)\n    ytf = tf.matmul(tf.matmul(Atf, xtf), xtf)\n\n    with tf.Session() as sess:\n\n        # Now let's try to compute the jacobian \n        dydx = jacobian(ytf, xtf)\n        print(sess.run(dydx))\n        \n        # and the hessian... \n        d2ydx2 = tf.squeeze(jacobian(dydx, xtf))\n        print(sess.run(d2ydx2))\n\nthrows the error\nValueError: Cannot use 'while_1/while/gradients/f_count_1' as input to 'while_1/while/gradients/f_count' because they are in different while loops. See info log for more details. \nDoes anybody know the issue here?\nThe first jacobian is correct. The second one (essentially the hessian) throws an error.", "body": "Nobody seems to have posted any followup, but the codes proposed by @candidj0 and @jeisses do not work when nested (testing in tensorflow 1.5.0). So computing the hessian by nesting will not work (@tillahoffmann). Let me make this a bit more concrete. I am using the jacobian\r\n\r\n```\r\ndef map(f, x, dtype=None, parallel_iterations=10):\r\n    '''\r\n    Apply f to each of the elements in x using the specified number of parallel iterations.\r\n\r\n    Important points:\r\n    1. By \"elements in x\", we mean that we will be applying f to x[0],...x[tf.shape(x)[0]-1].\r\n    2. The output size of f(x[i]) can be arbitrary. However, if the dtype of that output\r\n       is different than the dtype of x, then you need to specify that as an additional argument.\r\n    '''\r\n    if dtype is None:\r\n        dtype = x.dtype\r\n\r\n    n = tf.shape(x)[0]\r\n    loop_vars = [\r\n        tf.constant(0, n.dtype),\r\n        tf.TensorArray(dtype, size=n),\r\n    ]\r\n    _, fx = tf.while_loop(\r\n        lambda j, _: j < n,\r\n        lambda j, result: (j + 1, result.write(j, f(x[j]))),\r\n        loop_vars,\r\n        parallel_iterations=parallel_iterations\r\n    )\r\n    return fx.stack()\r\n\r\ndef jacobian(fx, x, parallel_iterations=10):\r\n    '''\r\n    Given a tensor fx, which is a function of x, vectorize fx (via tf.reshape(fx, [-1])),\r\n    and then compute the jacobian of each entry of fx with respect to x.\r\n    Specifically, if x has shape (m,n,...,p), and fx has L entries (tf.size(fx)=L), then\r\n    the output will be (L,m,n,...,p), where output[i] will be (m,n,...,p), with each entry denoting the\r\n    gradient of output[i] wrt the corresponding element of x.\r\n    '''\r\n    return map(lambda fxi: tf.gradients(fxi, x)[0],\r\n               tf.reshape(fx, [-1]),\r\n               dtype=x.dtype,\r\n               parallel_iterations=parallel_iterations)\r\n```\r\n\r\nI am using this because it supports dynamic sizes, that is, one of the dimensions can be `None`. \r\n\r\nHowever, this simple test \r\n\r\n```\r\n    import numpy\r\n    from numpy.random import randn\r\n\r\n    numpy.random.seed(0)\r\n\r\n    # Here is how everything would look in numpy\r\n    x = randn(3, 3)\r\n    A = randn(2, 3)\r\n    y = numpy.dot(A,numpy.dot(x,x))\r\n\r\n    # and in tensorflow... \r\n    xtf = tf.constant(x, tf.float64)\r\n    Atf = tf.constant(A, tf.float64)\r\n    ytf = tf.matmul(tf.matmul(Atf, xtf), xtf)\r\n\r\n    with tf.Session() as sess:\r\n\r\n        # Now let's try to compute the jacobian \r\n        dydx = jacobian(ytf, xtf)\r\n        print(sess.run(dydx))\r\n        \r\n        # and the hessian... \r\n        d2ydx2 = tf.squeeze(jacobian(dydx, xtf))\r\n        print(sess.run(d2ydx2))\r\n```\r\nthrows the error \r\n\r\n`ValueError: Cannot use 'while_1/while/gradients/f_count_1' as input to 'while_1/while/gradients/f_count' because they are in different while loops. See info log for more details.\r\n`\r\n\r\nDoes anybody know the issue here? \r\nThe first jacobian is correct. The second one (essentially the hessian) throws an error.\r\n\r\n\r\n"}