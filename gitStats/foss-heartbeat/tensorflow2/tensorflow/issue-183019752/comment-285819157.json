{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285819157", "html_url": "https://github.com/tensorflow/tensorflow/issues/4965#issuecomment-285819157", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4965", "id": 285819157, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTgxOTE1Nw==", "user": {"login": "dustinvtran", "id": 2569867, "node_id": "MDQ6VXNlcjI1Njk4Njc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2569867?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dustinvtran", "html_url": "https://github.com/dustinvtran", "followers_url": "https://api.github.com/users/dustinvtran/followers", "following_url": "https://api.github.com/users/dustinvtran/following{/other_user}", "gists_url": "https://api.github.com/users/dustinvtran/gists{/gist_id}", "starred_url": "https://api.github.com/users/dustinvtran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dustinvtran/subscriptions", "organizations_url": "https://api.github.com/users/dustinvtran/orgs", "repos_url": "https://api.github.com/users/dustinvtran/repos", "events_url": "https://api.github.com/users/dustinvtran/events{/privacy}", "received_events_url": "https://api.github.com/users/dustinvtran/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-11T00:08:00Z", "updated_at": "2017-03-11T00:08:55Z", "author_association": "MEMBER", "body_html": "<p>A more condensed version (i.e., without all the subclassing) of Edward's <a href=\"https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py\"><code>hmc.py</code></a> would be valuable and likely the easiest.</p>\n<p>An open question is 1. how to represent the model; and 2. how to manage the output of the inferred posterior.</p>\n<ol>\n<li>Following <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/bayesflow/python/ops/variational_inference_impl.py#L173\">Bayesflow's implementation of variational inference</a>, a log joint representation makes sense. However, I'm not sure how to define what variables to infer without resorting to <a href=\"http://edwardlib.org/api/model\" rel=\"nofollow\">Edward random variables</a> or <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/contrib.bayesflow.stochastic_tensor/\" rel=\"nofollow\">Bayesflow stochastic tensors</a>.</li>\n<li>Following <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/train/\" rel=\"nofollow\">TensorFlow optimizers</a>, Edward, and Bayesflow, I think inference should modify tf.Variables in place, perhaps returning a train op that the user runs in a loop.</li>\n</ol>", "body_text": "A more condensed version (i.e., without all the subclassing) of Edward's hmc.py would be valuable and likely the easiest.\nAn open question is 1. how to represent the model; and 2. how to manage the output of the inferred posterior.\n\nFollowing Bayesflow's implementation of variational inference, a log joint representation makes sense. However, I'm not sure how to define what variables to infer without resorting to Edward random variables or Bayesflow stochastic tensors.\nFollowing TensorFlow optimizers, Edward, and Bayesflow, I think inference should modify tf.Variables in place, perhaps returning a train op that the user runs in a loop.", "body": "A more condensed version (i.e., without all the subclassing) of Edward's [`hmc.py`](https://github.com/blei-lab/edward/blob/master/edward/inferences/hmc.py) would be valuable and likely the easiest.\r\n\r\nAn open question is 1. how to represent the model; and 2. how to manage the output of the inferred posterior. \r\n\r\n1. Following [Bayesflow's implementation of variational inference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/bayesflow/python/ops/variational_inference_impl.py#L173), a log joint representation makes sense. However, I'm not sure how to define what variables to infer without resorting to [Edward random variables](http://edwardlib.org/api/model) or [Bayesflow stochastic tensors](https://www.tensorflow.org/versions/master/api_docs/python/contrib.bayesflow.stochastic_tensor/).\r\n2. Following [TensorFlow optimizers](https://www.tensorflow.org/versions/master/api_docs/python/train/), Edward, and Bayesflow, I think inference should modify tf.Variables in place, perhaps returning a train op that the user runs in a loop. "}