{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253920153", "html_url": "https://github.com/tensorflow/tensorflow/issues/4965#issuecomment-253920153", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4965", "id": 253920153, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzkyMDE1Mw==", "user": {"login": "betanalpha", "id": 1527190, "node_id": "MDQ6VXNlcjE1MjcxOTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1527190?v=4", "gravatar_id": "", "url": "https://api.github.com/users/betanalpha", "html_url": "https://github.com/betanalpha", "followers_url": "https://api.github.com/users/betanalpha/followers", "following_url": "https://api.github.com/users/betanalpha/following{/other_user}", "gists_url": "https://api.github.com/users/betanalpha/gists{/gist_id}", "starred_url": "https://api.github.com/users/betanalpha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/betanalpha/subscriptions", "organizations_url": "https://api.github.com/users/betanalpha/orgs", "repos_url": "https://api.github.com/users/betanalpha/repos", "events_url": "https://api.github.com/users/betanalpha/events{/privacy}", "received_events_url": "https://api.github.com/users/betanalpha/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-14T21:10:03Z", "updated_at": "2016-10-14T21:10:03Z", "author_association": "NONE", "body_html": "<p>First a quick correction: Metropolis-Hastings is a general procedure for constructing Markov chains that target a specified distribution using a proposal distribution.  <em>Random Walk Metropolis</em> is the implementation of Metropolis-Hastings using a Gaussian proposal distribution, and it's Random Walk Metropolis that scales so poorly.  The difference is important because even algorithms like Hamiltonian Monte Carlo can be cast as certain Metropolis-Hastings implementations.</p>\n<p>Hamiltonian Monte Carlo is absolutely critical to scaling Markov chain Monte Carlo up to anything more than O(10) dimensions, and while we could have a long theoretical argument as to why it's probably easier to offer empirical data.  Just search Google scholar for \"mc-stan.org\" or take a look at the papers at <a href=\"http://mc-stan.org/citations/\" rel=\"nofollow\">http://mc-stan.org/citations/</a>.  Unfortunately there is significant misinformation in many fields, including astronomy, that confounds discussions and comparisons of methods and slows the adoption of techniques like Hamiltonian Monte Carlo.  That said, here are three examples where the analyses would not have worked with any other Markov chain Monte Carlo algorithm: <a href=\"https://arxiv.org/abs/1507.01602\" rel=\"nofollow\">https://arxiv.org/abs/1507.01602</a>, <a href=\"https://arxiv.org/abs/1404.2004\" rel=\"nofollow\">https://arxiv.org/abs/1404.2004</a>, <a href=\"https://arxiv.org/abs/1606.05770\" rel=\"nofollow\">https://arxiv.org/abs/1606.05770</a>.</p>\n<p>All of that said, what is the relevance for Hamiltonian Monte Carlo (or its efficient implementations like the No-U-Turn sampler) in languages like TensorFlow?  As an algorithm, Hamiltonian Monte Carlo takes the target log probability density and its gradient and then generates an extremely efficient Markov transition.  Importantly, there is essentially no room for parallelization in the algorithm itself, and hence no opportunity to exploit the main features of TensorFlow.  Instead, any exploitation would have to come in the calculation of the log probability density and its gradient.  Even then, any potential benefit, as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2569867\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dustinvtran\">@dustinvtran</a> suggested, depends strongly on the exact structure of the log probability density function.  If TensorFlow is ignorant to this structure then it cannot exploit it, and even it it can identify this structure then it can exploit it only if the structure is appropriate (the more complex the model, the less opportunities for parallelization and other speed ups).</p>\n<p>As a modeling language Stan focuses on specifying the log posterior density as a continuous function amenable to automatic differentiation and does not try to identify any particular structure in that function that could be exploited.  I disagree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2569867\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dustinvtran\">@dustinvtran</a> that this makes the language \"improper\", but that's an ill-defined term anyways.  More generally, I am very strongly of the opinion that exploiting bespoke structure of a particular model is not significantly useful for scaling statistical computation.  The kinds of structures required for significant speedups without drastically compromising accuracy are limited to very simple models, essentially models that only weakly deviate from independent Gaussians, which are never really capture the structure in realistic data sets.  I should note that this opinion is backed up by many, many experiments and plenty of theory!</p>\n<p>To summarize: given its automatic differentiation engine, there is no reason why the No-U-Turn sampler cannot be implemented in TensorFlow.  In fact, it would be a straightforward port of the Stan implementation, <a href=\"https://github.com/stan-dev/stan/blob/develop/src/stan/mcmc/hmc/nuts/base_nuts.hpp\">https://github.com/stan-dev/stan/blob/develop/src/stan/mcmc/hmc/nuts/base_nuts.hpp</a>, as demonstrated by the PyMC developers.  One should not expect a significant performance gain over existing implementations, but that's no reason to provide an implementation to existing TensorFlow users.  If anyone wants to work on an implementation and has any questions, I'm happy to answer.</p>\n<p>As a postscript, I should note that I've always been bitter about the naming of TensorFlow as Hamiltonian Monte Carlo was pushing tensor fields along flows for decades before TensorFlow was released.  <g-emoji class=\"g-emoji\" alias=\"wink\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f609.png\">\ud83d\ude09</g-emoji></p>", "body_text": "First a quick correction: Metropolis-Hastings is a general procedure for constructing Markov chains that target a specified distribution using a proposal distribution.  Random Walk Metropolis is the implementation of Metropolis-Hastings using a Gaussian proposal distribution, and it's Random Walk Metropolis that scales so poorly.  The difference is important because even algorithms like Hamiltonian Monte Carlo can be cast as certain Metropolis-Hastings implementations.\nHamiltonian Monte Carlo is absolutely critical to scaling Markov chain Monte Carlo up to anything more than O(10) dimensions, and while we could have a long theoretical argument as to why it's probably easier to offer empirical data.  Just search Google scholar for \"mc-stan.org\" or take a look at the papers at http://mc-stan.org/citations/.  Unfortunately there is significant misinformation in many fields, including astronomy, that confounds discussions and comparisons of methods and slows the adoption of techniques like Hamiltonian Monte Carlo.  That said, here are three examples where the analyses would not have worked with any other Markov chain Monte Carlo algorithm: https://arxiv.org/abs/1507.01602, https://arxiv.org/abs/1404.2004, https://arxiv.org/abs/1606.05770.\nAll of that said, what is the relevance for Hamiltonian Monte Carlo (or its efficient implementations like the No-U-Turn sampler) in languages like TensorFlow?  As an algorithm, Hamiltonian Monte Carlo takes the target log probability density and its gradient and then generates an extremely efficient Markov transition.  Importantly, there is essentially no room for parallelization in the algorithm itself, and hence no opportunity to exploit the main features of TensorFlow.  Instead, any exploitation would have to come in the calculation of the log probability density and its gradient.  Even then, any potential benefit, as @dustinvtran suggested, depends strongly on the exact structure of the log probability density function.  If TensorFlow is ignorant to this structure then it cannot exploit it, and even it it can identify this structure then it can exploit it only if the structure is appropriate (the more complex the model, the less opportunities for parallelization and other speed ups).\nAs a modeling language Stan focuses on specifying the log posterior density as a continuous function amenable to automatic differentiation and does not try to identify any particular structure in that function that could be exploited.  I disagree with @dustinvtran that this makes the language \"improper\", but that's an ill-defined term anyways.  More generally, I am very strongly of the opinion that exploiting bespoke structure of a particular model is not significantly useful for scaling statistical computation.  The kinds of structures required for significant speedups without drastically compromising accuracy are limited to very simple models, essentially models that only weakly deviate from independent Gaussians, which are never really capture the structure in realistic data sets.  I should note that this opinion is backed up by many, many experiments and plenty of theory!\nTo summarize: given its automatic differentiation engine, there is no reason why the No-U-Turn sampler cannot be implemented in TensorFlow.  In fact, it would be a straightforward port of the Stan implementation, https://github.com/stan-dev/stan/blob/develop/src/stan/mcmc/hmc/nuts/base_nuts.hpp, as demonstrated by the PyMC developers.  One should not expect a significant performance gain over existing implementations, but that's no reason to provide an implementation to existing TensorFlow users.  If anyone wants to work on an implementation and has any questions, I'm happy to answer.\nAs a postscript, I should note that I've always been bitter about the naming of TensorFlow as Hamiltonian Monte Carlo was pushing tensor fields along flows for decades before TensorFlow was released.  \ud83d\ude09", "body": "First a quick correction: Metropolis-Hastings is a general procedure for constructing Markov chains that target a specified distribution using a proposal distribution.  _Random Walk Metropolis_ is the implementation of Metropolis-Hastings using a Gaussian proposal distribution, and it's Random Walk Metropolis that scales so poorly.  The difference is important because even algorithms like Hamiltonian Monte Carlo can be cast as certain Metropolis-Hastings implementations.\n\nHamiltonian Monte Carlo is absolutely critical to scaling Markov chain Monte Carlo up to anything more than O(10) dimensions, and while we could have a long theoretical argument as to why it's probably easier to offer empirical data.  Just search Google scholar for \"mc-stan.org\" or take a look at the papers at http://mc-stan.org/citations/.  Unfortunately there is significant misinformation in many fields, including astronomy, that confounds discussions and comparisons of methods and slows the adoption of techniques like Hamiltonian Monte Carlo.  That said, here are three examples where the analyses would not have worked with any other Markov chain Monte Carlo algorithm: https://arxiv.org/abs/1507.01602, https://arxiv.org/abs/1404.2004, https://arxiv.org/abs/1606.05770.\n\nAll of that said, what is the relevance for Hamiltonian Monte Carlo (or its efficient implementations like the No-U-Turn sampler) in languages like TensorFlow?  As an algorithm, Hamiltonian Monte Carlo takes the target log probability density and its gradient and then generates an extremely efficient Markov transition.  Importantly, there is essentially no room for parallelization in the algorithm itself, and hence no opportunity to exploit the main features of TensorFlow.  Instead, any exploitation would have to come in the calculation of the log probability density and its gradient.  Even then, any potential benefit, as @dustinvtran suggested, depends strongly on the exact structure of the log probability density function.  If TensorFlow is ignorant to this structure then it cannot exploit it, and even it it can identify this structure then it can exploit it only if the structure is appropriate (the more complex the model, the less opportunities for parallelization and other speed ups).\n\nAs a modeling language Stan focuses on specifying the log posterior density as a continuous function amenable to automatic differentiation and does not try to identify any particular structure in that function that could be exploited.  I disagree with @dustinvtran that this makes the language \"improper\", but that's an ill-defined term anyways.  More generally, I am very strongly of the opinion that exploiting bespoke structure of a particular model is not significantly useful for scaling statistical computation.  The kinds of structures required for significant speedups without drastically compromising accuracy are limited to very simple models, essentially models that only weakly deviate from independent Gaussians, which are never really capture the structure in realistic data sets.  I should note that this opinion is backed up by many, many experiments and plenty of theory!\n\nTo summarize: given its automatic differentiation engine, there is no reason why the No-U-Turn sampler cannot be implemented in TensorFlow.  In fact, it would be a straightforward port of the Stan implementation, https://github.com/stan-dev/stan/blob/develop/src/stan/mcmc/hmc/nuts/base_nuts.hpp, as demonstrated by the PyMC developers.  One should not expect a significant performance gain over existing implementations, but that's no reason to provide an implementation to existing TensorFlow users.  If anyone wants to work on an implementation and has any questions, I'm happy to answer.\n\nAs a postscript, I should note that I've always been bitter about the naming of TensorFlow as Hamiltonian Monte Carlo was pushing tensor fields along flows for decades before TensorFlow was released.  \ud83d\ude09\n"}