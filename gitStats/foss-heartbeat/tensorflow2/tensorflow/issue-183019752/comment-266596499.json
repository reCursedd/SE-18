{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266596499", "html_url": "https://github.com/tensorflow/tensorflow/issues/4965#issuecomment-266596499", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4965", "id": 266596499, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjU5NjQ5OQ==", "user": {"login": "jvdillon", "id": 1137078, "node_id": "MDQ6VXNlcjExMzcwNzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1137078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jvdillon", "html_url": "https://github.com/jvdillon", "followers_url": "https://api.github.com/users/jvdillon/followers", "following_url": "https://api.github.com/users/jvdillon/following{/other_user}", "gists_url": "https://api.github.com/users/jvdillon/gists{/gist_id}", "starred_url": "https://api.github.com/users/jvdillon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jvdillon/subscriptions", "organizations_url": "https://api.github.com/users/jvdillon/orgs", "repos_url": "https://api.github.com/users/jvdillon/repos", "events_url": "https://api.github.com/users/jvdillon/events{/privacy}", "received_events_url": "https://api.github.com/users/jvdillon/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-13T00:20:22Z", "updated_at": "2016-12-13T00:20:22Z", "author_association": "MEMBER", "body_html": "<div class=\"email-quoted-reply\">On Mon, Oct 17, 2016 at 8:18 PM, Daniel Lee ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/jvdillon\">@jvdillon</a> &lt;<a href=\"https://github.com/jvdillon\">https://github.com/jvdillon</a>&gt;, although I'm really glad all\n this discussion happened, I don't think it illuminates what you need to get\n an implementation going. I'll try to describe what's necessary for you to\n implement HMC as an algorithm.\n</div>\n<div class=\"email-fragment\">Hi Daniel--thanks for your sharing thoughts. Responses inline.</div>\n<div class=\"email-quoted-reply\"> HMC requires:\n\n    - the evaluation of the log of the joint probability distribution\n    function with respect to data and parameters up to an additive constant\n    (constant with respect to parameters). I usually write this out as: log\n    p(X, theta) where X is data, theta are the parameters, and p(X, theta) is\n    the joint distribution function.\n    - the evaluation of the gradients of the log joint distribution\n    function above with respect to the parameters. That is: d log p(X, theta) /\n    d theta usually you have multiple thetas (say 1000), so it's: d log p(X,\n    theta) / d theta_1, d log p(X, theta) / d theta_2, ..., d log p(X, theta) /\n    d theta_1000 (You can see why we, at Stan, went with reverse-mode autodiff.\n    This needs to be evaluated for every leapfrog step.)\n\n This is currently the most painful piece in TF. I'm currently looking into</div>\n<div class=\"email-fragment\">the complexity of supporting a Jacobian op in TF.  Currently tf.gradients\nactually returns the tf.add_n of what would otherwise be the Jacobian.</div>\n<div class=\"email-quoted-reply\">\n    - math dictates that the log joint distribution function be continuous\n    and differentiable for HMC to work; ignore this condition at your own risk.\n    You can implement the algorithm and it'll do something, but it won't be\n    guaranteed to do anything correctly.\n\n Things that are good for HMC:\n\n    - transforming parameters using a change of variables so that the\n    parameters have range -infinity to + infinity. If not, you have to\n    implement bouncing, which is slow and hard generally.\n\n Part of the delay in supporting HMC is that we wanted to get our</div>\n<div class=\"email-fragment\">TransformedDistribution / Bijector API's dialed in.  I believe we're very\nclose to having something which has minimal feature readiness.</div>\n<div class=\"email-quoted-reply\">\n    - evaluating the joint distribution function on the log scale due to\n    floating point\n    - implementing NUTS\n\n I don't know tensorflow well enough to write up the actual computation\n necessary, but if you wanted to put up a simple example, I can walk you\n through exactly what's required in order to implement HMC. Hopefully that\n clarifies what you need to be able to compute in order to use HMC.\n</div>\n<div class=\"email-fragment\">Hmm I don't have a simple example in mind.  Perhaps you can think of a good\nexample?  If not that's ok too; I was planning on doing a light lit survey\nbefore delving in deeper.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\"> \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"183019752\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4965\" href=\"https://github.com/tensorflow/tensorflow/issues/4965#issuecomment-254395765\">#4965 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABFZttHabSZQMDpGRkvColm6WplGMbUxks5q1DprgaJpZM4KW4SN\">https://github.com/notifications/unsubscribe-auth/ABFZttHabSZQMDpGRkvColm6WplGMbUxks5q1DprgaJpZM4KW4SN</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "On Mon, Oct 17, 2016 at 8:18 PM, Daniel Lee ***@***.***> wrote:\n @jvdillon <https://github.com/jvdillon>, although I'm really glad all\n this discussion happened, I don't think it illuminates what you need to get\n an implementation going. I'll try to describe what's necessary for you to\n implement HMC as an algorithm.\n\nHi Daniel--thanks for your sharing thoughts. Responses inline.\n HMC requires:\n\n    - the evaluation of the log of the joint probability distribution\n    function with respect to data and parameters up to an additive constant\n    (constant with respect to parameters). I usually write this out as: log\n    p(X, theta) where X is data, theta are the parameters, and p(X, theta) is\n    the joint distribution function.\n    - the evaluation of the gradients of the log joint distribution\n    function above with respect to the parameters. That is: d log p(X, theta) /\n    d theta usually you have multiple thetas (say 1000), so it's: d log p(X,\n    theta) / d theta_1, d log p(X, theta) / d theta_2, ..., d log p(X, theta) /\n    d theta_1000 (You can see why we, at Stan, went with reverse-mode autodiff.\n    This needs to be evaluated for every leapfrog step.)\n\n This is currently the most painful piece in TF. I'm currently looking into\nthe complexity of supporting a Jacobian op in TF.  Currently tf.gradients\nactually returns the tf.add_n of what would otherwise be the Jacobian.\n\n    - math dictates that the log joint distribution function be continuous\n    and differentiable for HMC to work; ignore this condition at your own risk.\n    You can implement the algorithm and it'll do something, but it won't be\n    guaranteed to do anything correctly.\n\n Things that are good for HMC:\n\n    - transforming parameters using a change of variables so that the\n    parameters have range -infinity to + infinity. If not, you have to\n    implement bouncing, which is slow and hard generally.\n\n Part of the delay in supporting HMC is that we wanted to get our\nTransformedDistribution / Bijector API's dialed in.  I believe we're very\nclose to having something which has minimal feature readiness.\n\n    - evaluating the joint distribution function on the log scale due to\n    floating point\n    - implementing NUTS\n\n I don't know tensorflow well enough to write up the actual computation\n necessary, but if you wanted to put up a simple example, I can walk you\n through exactly what's required in order to implement HMC. Hopefully that\n clarifies what you need to be able to compute in order to use HMC.\n\nHmm I don't have a simple example in mind.  Perhaps you can think of a good\nexample?  If not that's ok too; I was planning on doing a light lit survey\nbefore delving in deeper.\n\u2026\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#4965 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABFZttHabSZQMDpGRkvColm6WplGMbUxks5q1DprgaJpZM4KW4SN>\n .", "body": "On Mon, Oct 17, 2016 at 8:18 PM, Daniel Lee <notifications@github.com>\nwrote:\n\n> @jvdillon <https://github.com/jvdillon>, although I'm really glad all\n> this discussion happened, I don't think it illuminates what you need to get\n> an implementation going. I'll try to describe what's necessary for you to\n> implement HMC as an algorithm.\n>\nHi Daniel--thanks for your sharing thoughts. Responses inline.\n\n\n> HMC requires:\n>\n>    - the evaluation of the log of the joint probability distribution\n>    function with respect to data and parameters up to an additive constant\n>    (constant with respect to parameters). I usually write this out as: log\n>    p(X, theta) where X is data, theta are the parameters, and p(X, theta) is\n>    the joint distribution function.\n>    - the evaluation of the gradients of the log joint distribution\n>    function above with respect to the parameters. That is: d log p(X, theta) /\n>    d theta usually you have multiple thetas (say 1000), so it's: d log p(X,\n>    theta) / d theta_1, d log p(X, theta) / d theta_2, ..., d log p(X, theta) /\n>    d theta_1000 (You can see why we, at Stan, went with reverse-mode autodiff.\n>    This needs to be evaluated for every leapfrog step.)\n>\n> This is currently the most painful piece in TF. I'm currently looking into\nthe complexity of supporting a Jacobian op in TF.  Currently tf.gradients\nactually returns the tf.add_n of what would otherwise be the Jacobian.\n\n\n>\n>    - math dictates that the log joint distribution function be continuous\n>    and differentiable for HMC to work; ignore this condition at your own risk.\n>    You can implement the algorithm and it'll do something, but it won't be\n>    guaranteed to do anything correctly.\n>\n> Things that are good for HMC:\n>\n>    - transforming parameters using a change of variables so that the\n>    parameters have range -infinity to + infinity. If not, you have to\n>    implement bouncing, which is slow and hard generally.\n>\n> Part of the delay in supporting HMC is that we wanted to get our\nTransformedDistribution / Bijector API's dialed in.  I believe we're very\nclose to having something which has minimal feature readiness.\n\n>\n>    - evaluating the joint distribution function on the log scale due to\n>    floating point\n>    - implementing NUTS\n>\n> I don't know tensorflow well enough to write up the actual computation\n> necessary, but if you wanted to put up a simple example, I can walk you\n> through exactly what's required in order to implement HMC. Hopefully that\n> clarifies what you need to be able to compute in order to use HMC.\n>\nHmm I don't have a simple example in mind.  Perhaps you can think of a good\nexample?  If not that's ok too; I was planning on doing a light lit survey\nbefore delving in deeper.\n\n\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4965#issuecomment-254395765>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABFZttHabSZQMDpGRkvColm6WplGMbUxks5q1DprgaJpZM4KW4SN>\n> .\n>\n"}