{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/348672170", "html_url": "https://github.com/tensorflow/tensorflow/issues/4965#issuecomment-348672170", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4965", "id": 348672170, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODY3MjE3MA==", "user": {"login": "johnpjust", "id": 31108737, "node_id": "MDQ6VXNlcjMxMTA4NzM3", "avatar_url": "https://avatars3.githubusercontent.com/u/31108737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johnpjust", "html_url": "https://github.com/johnpjust", "followers_url": "https://api.github.com/users/johnpjust/followers", "following_url": "https://api.github.com/users/johnpjust/following{/other_user}", "gists_url": "https://api.github.com/users/johnpjust/gists{/gist_id}", "starred_url": "https://api.github.com/users/johnpjust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johnpjust/subscriptions", "organizations_url": "https://api.github.com/users/johnpjust/orgs", "repos_url": "https://api.github.com/users/johnpjust/repos", "events_url": "https://api.github.com/users/johnpjust/events{/privacy}", "received_events_url": "https://api.github.com/users/johnpjust/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-02T06:24:48Z", "updated_at": "2017-12-02T22:05:40Z", "author_association": "NONE", "body_html": "<p>This is a great feature.  Thanks for working on it!</p>\n<p>I tried this out and compared to Matlab's HMC version.  The Matlab one is at the very least an order of magnitude faster, all else the same (see below for code).  I am using the base TF build that doesn't support the additional AVX instructions available for my CPU, but still....that seems like a huge diff.  Actually i run one more loop on the Matlab code even.  Curious if this is expected?</p>\n<p>''' Python Version'''<br>\ndef log_joint(x):<br>\nreturn tf.log(tf.exp(-0.5<em>tf.square((x + 16)/1)) + tf.exp(-0.5</em>tf.square((x - 16)/1)))<br>\nres = []<br>\nfor n in range(1,10):<br>\nchain, acceptance_probs = tf.contrib.bayesflow.hmc.chain(1000, .5, 2, tf.random_uniform([1], -10, 10), log_joint, event_dims=[0])<br>\n# Discard first half of chain as warmup/burn-in<br>\nwarmed_up = chain[500:]<br>\nres.extend(sess.run(warmed_up))</p>\n<p>df = pd.DataFrame(np.concatenate(res))<br>\ndf.plot.density()</p>\n<p>%%%%% Matlab<br>\nmeans = [16,-16];<br>\nstandevs = [1;1];</p>\n<p>samples = zeros(500,10);</p>\n<p>logpdf = @(theta)normalDistGrad(theta,means,standevs);<br>\nfor n=1:10<br>\nstartpoint = 20*rand-10;<br>\nsmp = hmcSampler(logpdf,startpoint);<br>\nsamples(:,n) = drawSamples(smp, 'burnin', 500, 'NumSamples', 500);<br>\nend</p>\n<p>histogram(samples, 50)</p>\n<p>function [lpdf,glpdf] = normalDistGrad(X,Mu,Sigma)<br>\nlpdf = log(exp(-0.5*((X + Mu(1))/Sigma(1)).^2) + exp(-0.5*((X + Mu(2))/Sigma(2)).^2));<br>\nglpdf = 1./(exp(-0.5*((X + Mu(1))/Sigma(1)).^2) + exp(-0.5*((X + Mu(2))/Sigma(2)).^2)) * ...<br>\n(-((X + Mu(1))/Sigma(1))<em>exp(-0.5</em>((X + Mu(1))/Sigma(1)).^2)/Sigma(1) - ...<br>\n((X + Mu(2))/Sigma(2))<em>exp(-0.5</em>((X + Mu(2))/Sigma(2)).^2)/Sigma(2));<br>\nend</p>", "body_text": "This is a great feature.  Thanks for working on it!\nI tried this out and compared to Matlab's HMC version.  The Matlab one is at the very least an order of magnitude faster, all else the same (see below for code).  I am using the base TF build that doesn't support the additional AVX instructions available for my CPU, but still....that seems like a huge diff.  Actually i run one more loop on the Matlab code even.  Curious if this is expected?\n''' Python Version'''\ndef log_joint(x):\nreturn tf.log(tf.exp(-0.5tf.square((x + 16)/1)) + tf.exp(-0.5tf.square((x - 16)/1)))\nres = []\nfor n in range(1,10):\nchain, acceptance_probs = tf.contrib.bayesflow.hmc.chain(1000, .5, 2, tf.random_uniform([1], -10, 10), log_joint, event_dims=[0])\n# Discard first half of chain as warmup/burn-in\nwarmed_up = chain[500:]\nres.extend(sess.run(warmed_up))\ndf = pd.DataFrame(np.concatenate(res))\ndf.plot.density()\n%%%%% Matlab\nmeans = [16,-16];\nstandevs = [1;1];\nsamples = zeros(500,10);\nlogpdf = @(theta)normalDistGrad(theta,means,standevs);\nfor n=1:10\nstartpoint = 20*rand-10;\nsmp = hmcSampler(logpdf,startpoint);\nsamples(:,n) = drawSamples(smp, 'burnin', 500, 'NumSamples', 500);\nend\nhistogram(samples, 50)\nfunction [lpdf,glpdf] = normalDistGrad(X,Mu,Sigma)\nlpdf = log(exp(-0.5*((X + Mu(1))/Sigma(1)).^2) + exp(-0.5*((X + Mu(2))/Sigma(2)).^2));\nglpdf = 1./(exp(-0.5*((X + Mu(1))/Sigma(1)).^2) + exp(-0.5*((X + Mu(2))/Sigma(2)).^2)) * ...\n(-((X + Mu(1))/Sigma(1))exp(-0.5((X + Mu(1))/Sigma(1)).^2)/Sigma(1) - ...\n((X + Mu(2))/Sigma(2))exp(-0.5((X + Mu(2))/Sigma(2)).^2)/Sigma(2));\nend", "body": "This is a great feature.  Thanks for working on it!\r\n\r\nI tried this out and compared to Matlab's HMC version.  The Matlab one is at the very least an order of magnitude faster, all else the same (see below for code).  I am using the base TF build that doesn't support the additional AVX instructions available for my CPU, but still....that seems like a huge diff.  Actually i run one more loop on the Matlab code even.  Curious if this is expected?\r\n\r\n''' Python Version'''\r\ndef log_joint(x):\r\n    return tf.log(tf.exp(-0.5*tf.square((x + 16)/1)) + tf.exp(-0.5*tf.square((x - 16)/1)))\r\nres = []\r\nfor n in range(1,10):\r\n    chain, acceptance_probs = tf.contrib.bayesflow.hmc.chain(1000, .5, 2, tf.random_uniform([1], -10, 10), log_joint, event_dims=[0])\r\n    # Discard first half of chain as warmup/burn-in\r\n    warmed_up = chain[500:]\r\n    res.extend(sess.run(warmed_up))\r\n\r\ndf = pd.DataFrame(np.concatenate(res))\r\ndf.plot.density()\r\n\r\n%%%%% Matlab\r\nmeans = [16,-16];\r\nstandevs = [1;1];\r\n\r\nsamples = zeros(500,10);\r\n\r\nlogpdf = @(theta)normalDistGrad(theta,means,standevs);\r\nfor n=1:10\r\n    startpoint = 20*rand-10;\r\n    smp = hmcSampler(logpdf,startpoint);\r\n    samples(:,n) = drawSamples(smp, 'burnin', 500, 'NumSamples', 500);\r\nend\r\n\r\nhistogram(samples, 50)\r\n\r\nfunction [lpdf,glpdf] = normalDistGrad(X,Mu,Sigma)\r\nlpdf = log(exp(-0.5*((X + Mu(1))/Sigma(1)).^2) + exp(-0.5*((X + Mu(2))/Sigma(2)).^2));\r\nglpdf = 1./(exp(-0.5*((X + Mu(1))/Sigma(1)).^2) + exp(-0.5*((X + Mu(2))/Sigma(2)).^2)) * ...\r\n(-((X + Mu(1))/Sigma(1))*exp(-0.5*((X + Mu(1))/Sigma(1)).^2)/Sigma(1) - ...\r\n((X + Mu(2))/Sigma(2))*exp(-0.5*((X + Mu(2))/Sigma(2)).^2)/Sigma(2));\r\nend\r\n"}