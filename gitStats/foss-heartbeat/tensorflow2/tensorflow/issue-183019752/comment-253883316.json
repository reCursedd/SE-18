{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253883316", "html_url": "https://github.com/tensorflow/tensorflow/issues/4965#issuecomment-253883316", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4965", "id": 253883316, "node_id": "MDEyOklzc3VlQ29tbWVudDI1Mzg4MzMxNg==", "user": {"login": "dustinvtran", "id": 2569867, "node_id": "MDQ6VXNlcjI1Njk4Njc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2569867?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dustinvtran", "html_url": "https://github.com/dustinvtran", "followers_url": "https://api.github.com/users/dustinvtran/followers", "following_url": "https://api.github.com/users/dustinvtran/following{/other_user}", "gists_url": "https://api.github.com/users/dustinvtran/gists{/gist_id}", "starred_url": "https://api.github.com/users/dustinvtran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dustinvtran/subscriptions", "organizations_url": "https://api.github.com/users/dustinvtran/orgs", "repos_url": "https://api.github.com/users/dustinvtran/repos", "events_url": "https://api.github.com/users/dustinvtran/events{/privacy}", "received_events_url": "https://api.github.com/users/dustinvtran/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-14T18:26:57Z", "updated_at": "2016-10-14T18:45:43Z", "author_association": "MEMBER", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6596998\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alexggmatthews\">@alexggmatthews</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=154383\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jameshensman\">@jameshensman</a>! This discussion is sure to be useful. I'm also looping in a few others who I've chatted about this stuff in detail: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=509707\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/matthewdhoffman\">@matthewdhoffman</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4632336\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/murphyk\">@murphyk</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1137078\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jvdillon\">@jvdillon</a> (among those who I can find their GitHub username).</p>\n<p>First, we might want to consider the general problem of how to implement posterior inference algorithms. That is, if we want HMC inside TensorFlow, we'd like an implementation that naturally extends to other algorithms\u2014whether it be other MCMC, variational inference such as BBVI and SVI, or even exact inference such as conjugacy. After all, it can be difficult to predict what algorithms are most applicable in the future. And certainly for inference developers like us, being able to experiment with many algorithms is handy.</p>\n<p>Very quickly this runs into open research challenges, in the general scope of probabilistic programming.</p>\n<p>For example, how do we design the interface to inference algorithms? For point estimation, it's clear using TensorFlow optimizers. They simply need a <code>loss</code> tensor, and they modify the state of variables in the graph, which serve as parameters. And maybe in the case of HMC and BBVI, it's easy to assume the user passes in a function or class that implements a model's <code>log_joint(x, z)</code> density. But this doesn't extend to algorithms which take advantage of structure: for example, Gibbs, SVI, and expectation propagation.</p>\n<p>Ultimately, my perspective is that a proper implementation of inference first requires a proper modeling language, with structure exposed to the user. And we can't necessarily work with classes and manual methods such as <code>model.log_joint</code>, <code>model.log_lik</code>, <code>model.log_prior</code>, etc.</p>\n<p>Design problems like these are very important. Without solving them, we quickly constrain ourselves on either the class of models or inference algorithms available in the framework. For example, as far as I'm aware, there is no probabilistic programming language that can properly handle data subsampling on global and local latent variables\u2014a crucial property to scale to large datasets.</p>\n<p><a href=\"http://edwardlib.org\" rel=\"nofollow\">Edward</a> is one proposed solution for issues like these. (With the caveat of self-advertising, I recommend a relevant <a href=\"https://www.periscope.tv/w/1yNGanvpOPjJj\" rel=\"nofollow\">talk I gave at Twitter</a> a month ago.) Another proposed solution is by <a href=\"https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.bayesflow.variational_inference.html\" rel=\"nofollow\"><code>BayesFlow</code></a>. Both proposals require additional abstractions, and perhaps this necessitates a layer of higher level API that's separate from the lower-level TensorFlow functions.</p>", "body_text": "Thanks @alexggmatthews, @jameshensman! This discussion is sure to be useful. I'm also looping in a few others who I've chatted about this stuff in detail: @matthewdhoffman, @ebrevdo, @murphyk, @jvdillon (among those who I can find their GitHub username).\nFirst, we might want to consider the general problem of how to implement posterior inference algorithms. That is, if we want HMC inside TensorFlow, we'd like an implementation that naturally extends to other algorithms\u2014whether it be other MCMC, variational inference such as BBVI and SVI, or even exact inference such as conjugacy. After all, it can be difficult to predict what algorithms are most applicable in the future. And certainly for inference developers like us, being able to experiment with many algorithms is handy.\nVery quickly this runs into open research challenges, in the general scope of probabilistic programming.\nFor example, how do we design the interface to inference algorithms? For point estimation, it's clear using TensorFlow optimizers. They simply need a loss tensor, and they modify the state of variables in the graph, which serve as parameters. And maybe in the case of HMC and BBVI, it's easy to assume the user passes in a function or class that implements a model's log_joint(x, z) density. But this doesn't extend to algorithms which take advantage of structure: for example, Gibbs, SVI, and expectation propagation.\nUltimately, my perspective is that a proper implementation of inference first requires a proper modeling language, with structure exposed to the user. And we can't necessarily work with classes and manual methods such as model.log_joint, model.log_lik, model.log_prior, etc.\nDesign problems like these are very important. Without solving them, we quickly constrain ourselves on either the class of models or inference algorithms available in the framework. For example, as far as I'm aware, there is no probabilistic programming language that can properly handle data subsampling on global and local latent variables\u2014a crucial property to scale to large datasets.\nEdward is one proposed solution for issues like these. (With the caveat of self-advertising, I recommend a relevant talk I gave at Twitter a month ago.) Another proposed solution is by BayesFlow. Both proposals require additional abstractions, and perhaps this necessitates a layer of higher level API that's separate from the lower-level TensorFlow functions.", "body": "Thanks @alexggmatthews, @jameshensman! This discussion is sure to be useful. I'm also looping in a few others who I've chatted about this stuff in detail: @matthewdhoffman, @ebrevdo, @murphyk, @jvdillon (among those who I can find their GitHub username).\n\nFirst, we might want to consider the general problem of how to implement posterior inference algorithms. That is, if we want HMC inside TensorFlow, we'd like an implementation that naturally extends to other algorithms\u2014whether it be other MCMC, variational inference such as BBVI and SVI, or even exact inference such as conjugacy. After all, it can be difficult to predict what algorithms are most applicable in the future. And certainly for inference developers like us, being able to experiment with many algorithms is handy.\n\nVery quickly this runs into open research challenges, in the general scope of probabilistic programming.\n\nFor example, how do we design the interface to inference algorithms? For point estimation, it's clear using TensorFlow optimizers. They simply need a `loss` tensor, and they modify the state of variables in the graph, which serve as parameters. And maybe in the case of HMC and BBVI, it's easy to assume the user passes in a function or class that implements a model's `log_joint(x, z)` density. But this doesn't extend to algorithms which take advantage of structure: for example, Gibbs, SVI, and expectation propagation. \n\nUltimately, my perspective is that a proper implementation of inference first requires a proper modeling language, with structure exposed to the user. And we can't necessarily work with classes and manual methods such as `model.log_joint`, `model.log_lik`, `model.log_prior`, etc.\n\nDesign problems like these are very important. Without solving them, we quickly constrain ourselves on either the class of models or inference algorithms available in the framework. For example, as far as I'm aware, there is no probabilistic programming language that can properly handle data subsampling on global and local latent variables\u2014a crucial property to scale to large datasets.\n\n[Edward](http://edwardlib.org) is one proposed solution for issues like these. (With the caveat of self-advertising, I recommend a relevant [talk I gave at Twitter](https://www.periscope.tv/w/1yNGanvpOPjJj) a month ago.) Another proposed solution is by [`BayesFlow`](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.bayesflow.variational_inference.html). Both proposals require additional abstractions, and perhaps this necessitates a layer of higher level API that's separate from the lower-level TensorFlow functions.\n"}