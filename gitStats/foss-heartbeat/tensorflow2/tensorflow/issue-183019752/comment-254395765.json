{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/254395765", "html_url": "https://github.com/tensorflow/tensorflow/issues/4965#issuecomment-254395765", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4965", "id": 254395765, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NDM5NTc2NQ==", "user": {"login": "syclik", "id": 425751, "node_id": "MDQ6VXNlcjQyNTc1MQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/425751?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syclik", "html_url": "https://github.com/syclik", "followers_url": "https://api.github.com/users/syclik/followers", "following_url": "https://api.github.com/users/syclik/following{/other_user}", "gists_url": "https://api.github.com/users/syclik/gists{/gist_id}", "starred_url": "https://api.github.com/users/syclik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syclik/subscriptions", "organizations_url": "https://api.github.com/users/syclik/orgs", "repos_url": "https://api.github.com/users/syclik/repos", "events_url": "https://api.github.com/users/syclik/events{/privacy}", "received_events_url": "https://api.github.com/users/syclik/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-18T03:16:09Z", "updated_at": "2016-10-18T03:16:09Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1137078\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jvdillon\">@jvdillon</a>, although I'm really glad all this discussion happened, I don't think it illuminates what you need to get an implementation going. I'll try to describe what's necessary for you to implement HMC as an algorithm.</p>\n<p>HMC requires:</p>\n<ul>\n<li>the evaluation of the log of the joint probability distribution function with respect to data and parameters up to an additive constant (constant with respect to parameters). I usually write this out as:<br>\nlog p(X, theta)<br>\nwhere X is data, theta are the parameters, and p(X, theta) is the joint distribution function.</li>\n<li>the evaluation of the gradients of the log joint distribution function above with respect to the parameters. That is:<br>\nd log p(X, theta) / d theta<br>\nusually you have multiple thetas (say 1000), so it's:<br>\nd log p(X, theta) / d theta_1, d log p(X, theta) / d theta_2, ..., d log p(X, theta) / d theta_1000<br>\n(You can see why we, at Stan, went with reverse-mode autodiff. This needs to be evaluated for every leapfrog step.)</li>\n<li>math dictates that the log joint distribution function be continuous and differentiable for HMC to work; ignore this condition at your own risk. You can implement the algorithm and it'll do something, but it won't be guaranteed to do anything correctly.</li>\n</ul>\n<p>Things that are good for HMC:</p>\n<ul>\n<li>transforming parameters using a change of variables so that the parameters have range -infinity to + infinity. If not, you have to implement bouncing, which is slow and hard generally.</li>\n<li>evaluating the joint distribution function on the log scale due to floating point</li>\n<li>implementing NUTS</li>\n</ul>\n<p>I don't know tensorflow well enough to write up the actual computation necessary, but if you wanted to put up a simple example, I can walk you through exactly what's required in order to implement HMC. Hopefully that clarifies what you need to be able to compute in order to use HMC.</p>", "body_text": "@jvdillon, although I'm really glad all this discussion happened, I don't think it illuminates what you need to get an implementation going. I'll try to describe what's necessary for you to implement HMC as an algorithm.\nHMC requires:\n\nthe evaluation of the log of the joint probability distribution function with respect to data and parameters up to an additive constant (constant with respect to parameters). I usually write this out as:\nlog p(X, theta)\nwhere X is data, theta are the parameters, and p(X, theta) is the joint distribution function.\nthe evaluation of the gradients of the log joint distribution function above with respect to the parameters. That is:\nd log p(X, theta) / d theta\nusually you have multiple thetas (say 1000), so it's:\nd log p(X, theta) / d theta_1, d log p(X, theta) / d theta_2, ..., d log p(X, theta) / d theta_1000\n(You can see why we, at Stan, went with reverse-mode autodiff. This needs to be evaluated for every leapfrog step.)\nmath dictates that the log joint distribution function be continuous and differentiable for HMC to work; ignore this condition at your own risk. You can implement the algorithm and it'll do something, but it won't be guaranteed to do anything correctly.\n\nThings that are good for HMC:\n\ntransforming parameters using a change of variables so that the parameters have range -infinity to + infinity. If not, you have to implement bouncing, which is slow and hard generally.\nevaluating the joint distribution function on the log scale due to floating point\nimplementing NUTS\n\nI don't know tensorflow well enough to write up the actual computation necessary, but if you wanted to put up a simple example, I can walk you through exactly what's required in order to implement HMC. Hopefully that clarifies what you need to be able to compute in order to use HMC.", "body": "@jvdillon, although I'm really glad all this discussion happened, I don't think it illuminates what you need to get an implementation going. I'll try to describe what's necessary for you to implement HMC as an algorithm.\n\nHMC requires:\n- the evaluation of the log of the joint probability distribution function with respect to data and parameters up to an additive constant (constant with respect to parameters). I usually write this out as:\n  log p(X, theta)\n  where X is data, theta are the parameters, and p(X, theta) is the joint distribution function.\n- the evaluation of the gradients of the log joint distribution function above with respect to the parameters. That is:\n  d log p(X, theta) / d theta\n  usually you have multiple thetas (say 1000), so it's:\n  d log p(X, theta) / d theta_1, d log p(X, theta) / d theta_2, ..., d log p(X, theta) / d theta_1000\n  (You can see why we, at Stan, went with reverse-mode autodiff. This needs to be evaluated for every leapfrog step.)\n- math dictates that the log joint distribution function be continuous and differentiable for HMC to work; ignore this condition at your own risk. You can implement the algorithm and it'll do something, but it won't be guaranteed to do anything correctly.\n\nThings that are good for HMC:\n- transforming parameters using a change of variables so that the parameters have range -infinity to + infinity. If not, you have to implement bouncing, which is slow and hard generally.\n- evaluating the joint distribution function on the log scale due to floating point\n- implementing NUTS\n\nI don't know tensorflow well enough to write up the actual computation necessary, but if you wanted to put up a simple example, I can walk you through exactly what's required in order to implement HMC. Hopefully that clarifies what you need to be able to compute in order to use HMC.\n"}