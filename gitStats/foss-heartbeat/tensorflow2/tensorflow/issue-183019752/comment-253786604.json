{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253786604", "html_url": "https://github.com/tensorflow/tensorflow/issues/4965#issuecomment-253786604", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4965", "id": 253786604, "node_id": "MDEyOklzc3VlQ29tbWVudDI1Mzc4NjYwNA==", "user": {"login": "alexggmatthews", "id": 6596998, "node_id": "MDQ6VXNlcjY1OTY5OTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/6596998?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexggmatthews", "html_url": "https://github.com/alexggmatthews", "followers_url": "https://api.github.com/users/alexggmatthews/followers", "following_url": "https://api.github.com/users/alexggmatthews/following{/other_user}", "gists_url": "https://api.github.com/users/alexggmatthews/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexggmatthews/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexggmatthews/subscriptions", "organizations_url": "https://api.github.com/users/alexggmatthews/orgs", "repos_url": "https://api.github.com/users/alexggmatthews/repos", "events_url": "https://api.github.com/users/alexggmatthews/events{/privacy}", "received_events_url": "https://api.github.com/users/alexggmatthews/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-14T12:30:27Z", "updated_at": "2016-10-14T12:30:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3842318\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jtlz2\">@jtlz2</a> Thanks for your interest.</p>\n<p>I guess by \"plan old MCMC\" you mean the Metropolis-Hastings (MH) algorithm. The MH algorithm uses a random walk and often mixes slower than HMC because it doesn't use gradients. The autodiff in TensorFlow means that gradients would be readily available and it would be a shame to ignore them.</p>\n<p>Nested sampling and parallel tempering are interesting methods but they are significantly more complex.</p>\n<p>I do believe that HMC has been found to be useful in a broad variety of probabilistic modelling tasks. As already stated the popular Stan library uses HMC and its variants. Perhaps have a look at the extensive list of case studies on that web page:</p>\n<p><a href=\"http://mc-stan.org/documentation/case-studies.html\" rel=\"nofollow\">http://mc-stan.org/documentation/case-studies.html</a></p>", "body_text": "@jtlz2 Thanks for your interest.\nI guess by \"plan old MCMC\" you mean the Metropolis-Hastings (MH) algorithm. The MH algorithm uses a random walk and often mixes slower than HMC because it doesn't use gradients. The autodiff in TensorFlow means that gradients would be readily available and it would be a shame to ignore them.\nNested sampling and parallel tempering are interesting methods but they are significantly more complex.\nI do believe that HMC has been found to be useful in a broad variety of probabilistic modelling tasks. As already stated the popular Stan library uses HMC and its variants. Perhaps have a look at the extensive list of case studies on that web page:\nhttp://mc-stan.org/documentation/case-studies.html", "body": "@jtlz2 Thanks for your interest.\n\nI guess by \"plan old MCMC\" you mean the Metropolis-Hastings (MH) algorithm. The MH algorithm uses a random walk and often mixes slower than HMC because it doesn't use gradients. The autodiff in TensorFlow means that gradients would be readily available and it would be a shame to ignore them. \n\nNested sampling and parallel tempering are interesting methods but they are significantly more complex. \n\nI do believe that HMC has been found to be useful in a broad variety of probabilistic modelling tasks. As already stated the popular Stan library uses HMC and its variants. Perhaps have a look at the extensive list of case studies on that web page:\n\nhttp://mc-stan.org/documentation/case-studies.html\n"}