{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19644", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19644/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19644/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19644/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19644", "id": 327674572, "node_id": "MDU6SXNzdWUzMjc2NzQ1NzI=", "number": 19644, "title": "Feature Request: 1 bit quantization in FakeQuantWithMinMaxVars ", "user": {"login": "fgr1986", "id": 10627849, "node_id": "MDQ6VXNlcjEwNjI3ODQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/10627849?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fgr1986", "html_url": "https://github.com/fgr1986", "followers_url": "https://api.github.com/users/fgr1986/followers", "following_url": "https://api.github.com/users/fgr1986/following{/other_user}", "gists_url": "https://api.github.com/users/fgr1986/gists{/gist_id}", "starred_url": "https://api.github.com/users/fgr1986/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fgr1986/subscriptions", "organizations_url": "https://api.github.com/users/fgr1986/orgs", "repos_url": "https://api.github.com/users/fgr1986/repos", "events_url": "https://api.github.com/users/fgr1986/events{/privacy}", "received_events_url": "https://api.github.com/users/fgr1986/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-05-30T10:43:41Z", "updated_at": "2018-10-24T14:52:30Z", "closed_at": "2018-10-24T14:51:54Z", "author_association": "NONE", "body_html": "<p>Feature Request: 1 bit quantization in FakeQuantWithMinMaxVars</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8</li>\n<li><strong>Python version</strong>:  3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: Does not apply</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: Does not apply</li>\n<li><strong>CUDA/cuDNN version</strong>: Does not apply</li>\n<li><strong>GPU model and memory</strong>: Does not apply</li>\n<li><strong>Exact command to reproduce</strong>: Does not apply</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<p>Binary NN are raising the attention of researchers/developers.<br>\nRight now <code>fake_quant </code>operations do support quantization in the range 2-16 bits. It would be really usefull extending this (relying on FakeQuantWithMinMaxVars) to 1b quantization.<br>\nIf so, <code>Quantize</code> methods (such as  <code>tf.contrib.quantize.create_eval_graph</code>) should be updated to include this new quantization limit (now  <code>tf.contrib.quantize.experimental_create_eval_graph</code> supports bit selection for quantization).</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>", "body_text": "Feature Request: 1 bit quantization in FakeQuantWithMinMaxVars\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.8\nPython version:  3.6\nBazel version (if compiling from source): Does not apply\nGCC/Compiler version (if compiling from source): Does not apply\nCUDA/cuDNN version: Does not apply\nGPU model and memory: Does not apply\nExact command to reproduce: Does not apply\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nBinary NN are raising the attention of researchers/developers.\nRight now fake_quant operations do support quantization in the range 2-16 bits. It would be really usefull extending this (relying on FakeQuantWithMinMaxVars) to 1b quantization.\nIf so, Quantize methods (such as  tf.contrib.quantize.create_eval_graph) should be updated to include this new quantization limit (now  tf.contrib.quantize.experimental_create_eval_graph supports bit selection for quantization).\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "body": "Feature Request: 1 bit quantization in FakeQuantWithMinMaxVars \r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: Does not apply\r\n- **GCC/Compiler version (if compiling from source)**: Does not apply\r\n- **CUDA/cuDNN version**: Does not apply\r\n- **GPU model and memory**: Does not apply\r\n- **Exact command to reproduce**: Does not apply\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nBinary NN are raising the attention of researchers/developers.\r\nRight now `fake_quant `operations do support quantization in the range 2-16 bits. It would be really usefull extending this (relying on FakeQuantWithMinMaxVars) to 1b quantization.\r\nIf so, `Quantize` methods (such as  `tf.contrib.quantize.create_eval_graph`) should be updated to include this new quantization limit (now  `tf.contrib.quantize.experimental_create_eval_graph` supports bit selection for quantization).\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n"}