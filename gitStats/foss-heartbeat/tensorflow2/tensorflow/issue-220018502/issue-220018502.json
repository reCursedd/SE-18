{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9028", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9028/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9028/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9028/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9028", "id": 220018502, "node_id": "MDU6SXNzdWUyMjAwMTg1MDI=", "number": 9028, "title": "XLA with tower parallelization", "user": {"login": "AndreasMadsen", "id": 505333, "node_id": "MDQ6VXNlcjUwNTMzMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/505333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreasMadsen", "html_url": "https://github.com/AndreasMadsen", "followers_url": "https://api.github.com/users/AndreasMadsen/followers", "following_url": "https://api.github.com/users/AndreasMadsen/following{/other_user}", "gists_url": "https://api.github.com/users/AndreasMadsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreasMadsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreasMadsen/subscriptions", "organizations_url": "https://api.github.com/users/AndreasMadsen/orgs", "repos_url": "https://api.github.com/users/AndreasMadsen/repos", "events_url": "https://api.github.com/users/AndreasMadsen/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreasMadsen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-06T20:13:01Z", "updated_at": "2017-06-16T19:10:05Z", "closed_at": "2017-06-16T19:10:05Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>You must complete this information or else your issue will be closed</h3>\n<ul>\n<li><em>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?</em>: yes</li>\n<li><em>TensorFlow installed from (source or binary)?</em>: source</li>\n<li><em>TensorFlow version</em>: 1.1rc1</li>\n<li><em>Bazel version (if compiling from source)</em>: 0.4.5</li>\n<li><em>CUDA/cuDNN version</em>: 8.0/5.1</li>\n<li><em>GPU Model and Memory</em>: Titan X (12 GB)</li>\n<li><em>Exact command to reproduce</em>: NA</li>\n</ul>\n<h3>Describe the problem clearly</h3>\n<p>Using the automatic XLA option<code>sess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1</code> causes an 8x performance improvement for my model (while training) when running with 1 GPU, amazing! I would like to parallelize this with a standard tower setup, without XLA this scales linearly (using 4 GPUs) but with XLA I see no difference. The timeline profiler (<code>chrome://tracing</code>) indicates that it just executes each tower one at the time on each GPU.</p>\n<p>The example in <a href=\"https://github.com/tensorflow/models/blob/master/neural_gpu/neural_gpu.py\">tensorflow/models/neural_gpu/neural_gpu.py</a> indicates that one can use the manual <code>tf.contrib.compiler.jit.experimental_jit_scope</code> context creator, using this causes a x6 times performance peanality compared to not using XLA, but does scale with multiple GPUs.</p>\n<p>I would like a feature where <code>experimental_jit_scope</code> works similarly to the automatic <code>sess_config</code> XLA option or the <code>sess_config</code> works with multiple GPUs.</p>\n<h3>Source Code / Logs</h3>\n<p>My model is almost identical to the one in <a href=\"https://github.com/buriburisuri/ByteNet/blob/master/train.py\">https://github.com/buriburisuri/ByteNet</a>.</p>", "body_text": "You must complete this information or else your issue will be closed\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow)?: yes\nTensorFlow installed from (source or binary)?: source\nTensorFlow version: 1.1rc1\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: 8.0/5.1\nGPU Model and Memory: Titan X (12 GB)\nExact command to reproduce: NA\n\nDescribe the problem clearly\nUsing the automatic XLA optionsess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1 causes an 8x performance improvement for my model (while training) when running with 1 GPU, amazing! I would like to parallelize this with a standard tower setup, without XLA this scales linearly (using 4 GPUs) but with XLA I see no difference. The timeline profiler (chrome://tracing) indicates that it just executes each tower one at the time on each GPU.\nThe example in tensorflow/models/neural_gpu/neural_gpu.py indicates that one can use the manual tf.contrib.compiler.jit.experimental_jit_scope context creator, using this causes a x6 times performance peanality compared to not using XLA, but does scale with multiple GPUs.\nI would like a feature where experimental_jit_scope works similarly to the automatic sess_config XLA option or the sess_config works with multiple GPUs.\nSource Code / Logs\nMy model is almost identical to the one in https://github.com/buriburisuri/ByteNet.", "body": "### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: yes\r\n- *TensorFlow installed from (source or binary)?*: source\r\n- *TensorFlow version*: 1.1rc1\r\n- *Bazel version (if compiling from source)*: 0.4.5\r\n- *CUDA/cuDNN version*: 8.0/5.1\r\n- *GPU Model and Memory*: Titan X (12 GB)\r\n- *Exact command to reproduce*: NA\r\n\r\n### Describe the problem clearly\r\n\r\nUsing the automatic XLA option`sess_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1` causes an 8x performance improvement for my model (while training) when running with 1 GPU, amazing! I would like to parallelize this with a standard tower setup, without XLA this scales linearly (using 4 GPUs) but with XLA I see no difference. The timeline profiler (`chrome://tracing`) indicates that it just executes each tower one at the time on each GPU.\r\n\r\nThe example in [tensorflow/models/neural_gpu/neural_gpu.py](https://github.com/tensorflow/models/blob/master/neural_gpu/neural_gpu.py) indicates that one can use the manual `tf.contrib.compiler.jit.experimental_jit_scope` context creator, using this causes a x6 times performance peanality compared to not using XLA, but does scale with multiple GPUs.\r\n\r\nI would like a feature where `experimental_jit_scope` works similarly to the automatic `sess_config` XLA option or the `sess_config` works with multiple GPUs.\r\n\r\n### Source Code / Logs\r\n\r\nMy model is almost identical to the one in [https://github.com/buriburisuri/ByteNet](https://github.com/buriburisuri/ByteNet/blob/master/train.py).\r\n"}