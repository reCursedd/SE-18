{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266297116", "html_url": "https://github.com/tensorflow/tensorflow/issues/5722#issuecomment-266297116", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5722", "id": 266297116, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjI5NzExNg==", "user": {"login": "TimZaman", "id": 7721540, "node_id": "MDQ6VXNlcjc3MjE1NDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7721540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TimZaman", "html_url": "https://github.com/TimZaman", "followers_url": "https://api.github.com/users/TimZaman/followers", "following_url": "https://api.github.com/users/TimZaman/following{/other_user}", "gists_url": "https://api.github.com/users/TimZaman/gists{/gist_id}", "starred_url": "https://api.github.com/users/TimZaman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TimZaman/subscriptions", "organizations_url": "https://api.github.com/users/TimZaman/orgs", "repos_url": "https://api.github.com/users/TimZaman/repos", "events_url": "https://api.github.com/users/TimZaman/events{/privacy}", "received_events_url": "https://api.github.com/users/TimZaman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-11T18:07:31Z", "updated_at": "2016-12-11T18:07:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>OK <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3183610\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nmiculinic\">@nmiculinic</a>, I implemented the rotating buffer with a size of 2. One is where the GPUs pull from, the other is where the feeder pushes to.<br>\nI took me a while to get the most optimal implementation while avoiding R/W race conditions.<br>\nI made a Gist for this: <a href=\"https://gist.github.com/TimZaman/639576c25693ca4890457efa5f67d103\">https://gist.github.com/TimZaman/639576c25693ca4890457efa5f67d103</a></p>\n<ol>\n<li>init stage: <code>sess.run([put_in_buffer])</code></li>\n<li>init stage: <code>sess.run([move_buffer])</code></li>\n<li>loop: <code>sess.run([pull])</code></li>\n</ol>\n<p>Defitinion of <code>pull</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.control_dependencies([move_buffer]):\n        pull <span class=\"pl-k\">=</span> tf.group(put_in_buffer, test_out)</pre></div>\n<p>So <code>pull</code> first refreshes the buffer, and then simultaneously puts new data in the buffer, and reads for GPU computation from the other buffer element.</p>\n<p>I have a lot of trouble with these <code>control_dependencies</code>: the above yields accurate results, but if you read carefully i (1) push (2) move (3) loop(move, put_in_buffer, test_output). As you can see, i move 2x on the first iteration, but the value stays the same.<br>\nThe Gist is setup such that i can monitor the values of the both buffers and the output. Here is an output where the first 20 buffer values are 0:1:20:</p>\n<pre><code>[BUF_IN][BUF_OUT][RESULT]:[[[1 1 1]]...][[[0 0 0]]...][0]\n[BUF_IN][BUF_OUT][RESULT]:[[[2 2 2]]...][[[1 1 1]]...][0.99999082]\n[BUF_IN][BUF_OUT][RESULT]:[[[3 3 3]]...][[[2 2 2]]...][1.9999816]\n[BUF_IN][BUF_OUT][RESULT]:[[[4 4 4]]...][[[3 3 3]]...][3.0000162]\n[BUF_IN][BUF_OUT][RESULT]:[[[5 5 5]]...][[[4 4 4]]...][3.9999633]\n[BUF_IN][BUF_OUT][RESULT]:[[[6 6 6]]...][[[5 5 5]]...][5.0000668]\n[BUF_IN][BUF_OUT][RESULT]:[[[7 7 7]]...][[[6 6 6]]...][6.0000324]\n[BUF_IN][BUF_OUT][RESULT]:[[[8 8 8]]...][[[7 7 7]]...][6.9999766]\n[BUF_IN][BUF_OUT][RESULT]:[[[9 9 9]]...][[[8 8 8]]...][7.9999266]\n[BUF_IN][BUF_OUT][RESULT]:[[[10 10 10]]...][[[9 9 9]]...][8.99991]\n[BUF_IN][BUF_OUT][RESULT]:[[[11 11 11]]...][[[10 10 10]]...][10.000134]\n</code></pre>\n<p>The above reveals that the input buffer is always +1 with respect to the output buffer, and the output buffers value is the same as the result: so it's working.<br>\nFor this particular example, you can see the amazing benefit of a GPU resident queue:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/7721540/21082114/1cfdbe10-bfdd-11e6-8cb0-69bc352f6963.png\"><img src=\"https://cloud.githubusercontent.com/assets/7721540/21082114/1cfdbe10-bfdd-11e6-8cb0-69bc352f6963.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>Now, I would like to reiterate this comment <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"178499435\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4526\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4526/hovercard?comment_id=249026120&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/4526#issuecomment-249026120\">#4526 (comment)</a> from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=476135\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mkolod\">@mkolod</a>:</p>\n<blockquote>\n<p>I think though that it would be good for this to be baked into the framework rather than shifting flow control workarounds to the user. Data prefetch is pretty much a universal need, since the input has to come from somewhere, hopefully in an efficient manner to leverage available compute cycles.</p>\n</blockquote>\n<p>I would like to suggest opening up this issue: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"178499435\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4526\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4526/hovercard?comment_id=249026120&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/4526#issuecomment-249026120\">#4526 (comment)</a><br>\nIt would bring amazing benefit from operations that are heavy on data, and light on compute.</p>", "body_text": "OK @prb12 @yaroslavvb @nmiculinic, I implemented the rotating buffer with a size of 2. One is where the GPUs pull from, the other is where the feeder pushes to.\nI took me a while to get the most optimal implementation while avoiding R/W race conditions.\nI made a Gist for this: https://gist.github.com/TimZaman/639576c25693ca4890457efa5f67d103\n\ninit stage: sess.run([put_in_buffer])\ninit stage: sess.run([move_buffer])\nloop: sess.run([pull])\n\nDefitinion of pull:\nwith tf.control_dependencies([move_buffer]):\n        pull = tf.group(put_in_buffer, test_out)\nSo pull first refreshes the buffer, and then simultaneously puts new data in the buffer, and reads for GPU computation from the other buffer element.\nI have a lot of trouble with these control_dependencies: the above yields accurate results, but if you read carefully i (1) push (2) move (3) loop(move, put_in_buffer, test_output). As you can see, i move 2x on the first iteration, but the value stays the same.\nThe Gist is setup such that i can monitor the values of the both buffers and the output. Here is an output where the first 20 buffer values are 0:1:20:\n[BUF_IN][BUF_OUT][RESULT]:[[[1 1 1]]...][[[0 0 0]]...][0]\n[BUF_IN][BUF_OUT][RESULT]:[[[2 2 2]]...][[[1 1 1]]...][0.99999082]\n[BUF_IN][BUF_OUT][RESULT]:[[[3 3 3]]...][[[2 2 2]]...][1.9999816]\n[BUF_IN][BUF_OUT][RESULT]:[[[4 4 4]]...][[[3 3 3]]...][3.0000162]\n[BUF_IN][BUF_OUT][RESULT]:[[[5 5 5]]...][[[4 4 4]]...][3.9999633]\n[BUF_IN][BUF_OUT][RESULT]:[[[6 6 6]]...][[[5 5 5]]...][5.0000668]\n[BUF_IN][BUF_OUT][RESULT]:[[[7 7 7]]...][[[6 6 6]]...][6.0000324]\n[BUF_IN][BUF_OUT][RESULT]:[[[8 8 8]]...][[[7 7 7]]...][6.9999766]\n[BUF_IN][BUF_OUT][RESULT]:[[[9 9 9]]...][[[8 8 8]]...][7.9999266]\n[BUF_IN][BUF_OUT][RESULT]:[[[10 10 10]]...][[[9 9 9]]...][8.99991]\n[BUF_IN][BUF_OUT][RESULT]:[[[11 11 11]]...][[[10 10 10]]...][10.000134]\n\nThe above reveals that the input buffer is always +1 with respect to the output buffer, and the output buffers value is the same as the result: so it's working.\nFor this particular example, you can see the amazing benefit of a GPU resident queue:\n\nNow, I would like to reiterate this comment #4526 (comment) from @mkolod:\n\nI think though that it would be good for this to be baked into the framework rather than shifting flow control workarounds to the user. Data prefetch is pretty much a universal need, since the input has to come from somewhere, hopefully in an efficient manner to leverage available compute cycles.\n\nI would like to suggest opening up this issue: #4526 (comment)\nIt would bring amazing benefit from operations that are heavy on data, and light on compute.", "body": "OK @prb12 @yaroslavvb @nmiculinic, I implemented the rotating buffer with a size of 2. One is where the GPUs pull from, the other is where the feeder pushes to.\r\nI took me a while to get the most optimal implementation while avoiding R/W race conditions.\r\nI made a Gist for this: https://gist.github.com/TimZaman/639576c25693ca4890457efa5f67d103\r\n\r\n1. init stage: `sess.run([put_in_buffer])`\r\n2. init stage: `sess.run([move_buffer])`\r\n3. loop: `sess.run([pull])`\r\n\r\nDefitinion of `pull`:\r\n```py\r\nwith tf.control_dependencies([move_buffer]):\r\n        pull = tf.group(put_in_buffer, test_out)\r\n```\r\nSo `pull` first refreshes the buffer, and then simultaneously puts new data in the buffer, and reads for GPU computation from the other buffer element.\r\n\r\nI have a lot of trouble with these `control_dependencies`: the above yields accurate results, but if you read carefully i (1) push (2) move (3) loop(move, put_in_buffer, test_output). As you can see, i move 2x on the first iteration, but the value stays the same.\r\nThe Gist is setup such that i can monitor the values of the both buffers and the output. Here is an output where the first 20 buffer values are 0:1:20:\r\n```\r\n[BUF_IN][BUF_OUT][RESULT]:[[[1 1 1]]...][[[0 0 0]]...][0]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[2 2 2]]...][[[1 1 1]]...][0.99999082]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[3 3 3]]...][[[2 2 2]]...][1.9999816]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[4 4 4]]...][[[3 3 3]]...][3.0000162]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[5 5 5]]...][[[4 4 4]]...][3.9999633]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[6 6 6]]...][[[5 5 5]]...][5.0000668]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[7 7 7]]...][[[6 6 6]]...][6.0000324]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[8 8 8]]...][[[7 7 7]]...][6.9999766]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[9 9 9]]...][[[8 8 8]]...][7.9999266]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[10 10 10]]...][[[9 9 9]]...][8.99991]\r\n[BUF_IN][BUF_OUT][RESULT]:[[[11 11 11]]...][[[10 10 10]]...][10.000134]\r\n```\r\nThe above reveals that the input buffer is always +1 with respect to the output buffer, and the output buffers value is the same as the result: so it's working.\r\nFor this particular example, you can see the amazing benefit of a GPU resident queue:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/7721540/21082114/1cfdbe10-bfdd-11e6-8cb0-69bc352f6963.png)\r\n\r\nNow, I would like to reiterate this comment https://github.com/tensorflow/tensorflow/issues/4526#issuecomment-249026120 from @mkolod:\r\n\r\n> I think though that it would be good for this to be baked into the framework rather than shifting flow control workarounds to the user. Data prefetch is pretty much a universal need, since the input has to come from somewhere, hopefully in an efficient manner to leverage available compute cycles.\r\n\r\nI would like to suggest opening up this issue: https://github.com/tensorflow/tensorflow/issues/4526#issuecomment-249026120\r\nIt would bring amazing benefit from operations that are heavy on data, and light on compute.\r\n\r\n"}