{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271437117", "html_url": "https://github.com/tensorflow/tensorflow/issues/5722#issuecomment-271437117", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5722", "id": 271437117, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTQzNzExNw==", "user": {"login": "eraoul", "id": 1067070, "node_id": "MDQ6VXNlcjEwNjcwNzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1067070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eraoul", "html_url": "https://github.com/eraoul", "followers_url": "https://api.github.com/users/eraoul/followers", "following_url": "https://api.github.com/users/eraoul/following{/other_user}", "gists_url": "https://api.github.com/users/eraoul/gists{/gist_id}", "starred_url": "https://api.github.com/users/eraoul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eraoul/subscriptions", "organizations_url": "https://api.github.com/users/eraoul/orgs", "repos_url": "https://api.github.com/users/eraoul/repos", "events_url": "https://api.github.com/users/eraoul/events{/privacy}", "received_events_url": "https://api.github.com/users/eraoul/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-09T23:08:31Z", "updated_at": "2017-01-09T23:08:31Z", "author_association": "NONE", "body_html": "<p>+1 to built-in support for GPU-based data queues. Not all of us are running huge inception-style models with images. I definitely have a application with a smaller network but huge data sets, and got a graphics card with a large amount of RAM in order to queue up all the data in memory. I was really surprised that some people don't consider this a standard use case.</p>", "body_text": "+1 to built-in support for GPU-based data queues. Not all of us are running huge inception-style models with images. I definitely have a application with a smaller network but huge data sets, and got a graphics card with a large amount of RAM in order to queue up all the data in memory. I was really surprised that some people don't consider this a standard use case.", "body": "+1 to built-in support for GPU-based data queues. Not all of us are running huge inception-style models with images. I definitely have a application with a smaller network but huge data sets, and got a graphics card with a large amount of RAM in order to queue up all the data in memory. I was really surprised that some people don't consider this a standard use case."}