{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263938159", "html_url": "https://github.com/tensorflow/tensorflow/issues/5722#issuecomment-263938159", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5722", "id": 263938159, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzkzODE1OQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-30T17:29:17Z", "updated_at": "2016-11-30T17:29:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In principle one could write a version of the <code>FIFOQueue</code> that managed GPU buffers rather than CPU buffers, and then use the existing mechanisms (like queue runners) to drive a subgraph that copies the data from CPU to GPU and enqueues it in this queue. I'm not sure what the relative latency is between these copies and the framework overhead to run a step that performs the enqueuing, but I suspect it could be profitable in some cases.</p>", "body_text": "In principle one could write a version of the FIFOQueue that managed GPU buffers rather than CPU buffers, and then use the existing mechanisms (like queue runners) to drive a subgraph that copies the data from CPU to GPU and enqueues it in this queue. I'm not sure what the relative latency is between these copies and the framework overhead to run a step that performs the enqueuing, but I suspect it could be profitable in some cases.", "body": "In principle one could write a version of the `FIFOQueue` that managed GPU buffers rather than CPU buffers, and then use the existing mechanisms (like queue runners) to drive a subgraph that copies the data from CPU to GPU and enqueues it in this queue. I'm not sure what the relative latency is between these copies and the framework overhead to run a step that performs the enqueuing, but I suspect it could be profitable in some cases."}