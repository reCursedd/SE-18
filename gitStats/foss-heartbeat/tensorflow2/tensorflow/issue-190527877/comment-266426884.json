{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266426884", "html_url": "https://github.com/tensorflow/tensorflow/issues/5722#issuecomment-266426884", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5722", "id": 266426884, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjQyNjg4NA==", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-12T13:08:31Z", "updated_at": "2016-12-12T13:08:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I wonder if it's possible to build the prefetch into the scheduler? Consider the following diagram:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/3530212/21098743/4774b4dc-c073-11e6-9fa1-f90ecb4a213b.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/3530212/21098743/4774b4dc-c073-11e6-9fa1-f90ecb4a213b.jpg\" alt=\"img_20161212_134953255\" style=\"max-width:100%;\"></a></p>\n<p>We have three separate sub-graphs [(foo-&gt;bar, qux-&gt;baz, plug-&gt;thud)] that require execution on a (GPU?) device, depending on inputs [(a,b,c),(f,g,h,i,k), (l,m)] located across a device boundary and whose outputs [(d,e),(k),(n)] must be transmitted across the boundary once they are complete.</p>\n<p>As it currently stands, tensorflow appears (to me) <em>to treat transfers and compute operations as the same \"type\" of operation, rather than as operations that can be concurrently scheduled</em>.  As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a>  notes  in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"160137156\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2848\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2848/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2848\">#2848</a>, Recvs are scheduled immediately and Sends are scheduled once the input data is available.  So on a rough timeline the above sub-graphs seem to currently be scheduled as (see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"160137156\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2848\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2848/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2848\">#2848</a> for e.g. where sends are scheduled immediately)</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/3530212/21099282/05f56d00-c076-11e6-9983-4919a9cce0da.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/3530212/21099282/05f56d00-c076-11e6-9983-4919a9cce0da.jpg\" alt=\"img_20161212_141704007\" style=\"max-width:100%;\"></a></p>\n<p>The problem with this scheduling is that it wastes both I/O bandwidth and device compute. Given that these operations <em>can</em> be concurrently scheduled an optimal order of transfer and execution might be:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/3530212/21098800/945d2130-c073-11e6-8eef-b3d28ebeb4f9.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/3530212/21098800/945d2130-c073-11e6-8eef-b3d28ebeb4f9.jpg\" alt=\"img_20161212_135248066\" style=\"max-width:100%;\"></a></p>\n<p>(a,b) must be transferred before foo can execute, (c) must be transferred before bar can and so on. The output transfers for [(d,e), (k), (n)] can be scheduled last.</p>\n<p>The point is that scheduling the ops on the (GPU?) device necessarily imposes an ordering on how the inputs should be transferred. While my understanding of how ops are currently scheduled is limited  I would ideally expect it to be something like [a, b, foo, c, bar, f, g, h, qux, i, j, baz, l, plug, m, thud, d, e, k, n].</p>\n<p>Running the ops <em>as soon</em> as their inputs are available should use the device optimally. If your compute beats your I/O then the device is used optimally. If not, then you've done the best you can given the algorithms in the ops.</p>\n<p>All this information is available in the graph. I think the tricky part is estimating how many sub-graphs can be scheduled in one go -- this would involve balancing device memory vs the size of the input , temporary result and output tensors. But looking at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"160137156\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2848\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2848/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2848\">#2848</a> and other issues suggests that tensorflow does this already. It just doesn't currently schedule transfers and executions optimally.</p>\n<p>Further optimisations based on the number of copy engines on the GPU are possible.</p>", "body_text": "I wonder if it's possible to build the prefetch into the scheduler? Consider the following diagram:\n\nWe have three separate sub-graphs [(foo->bar, qux->baz, plug->thud)] that require execution on a (GPU?) device, depending on inputs [(a,b,c),(f,g,h,i,k), (l,m)] located across a device boundary and whose outputs [(d,e),(k),(n)] must be transmitted across the boundary once they are complete.\nAs it currently stands, tensorflow appears (to me) to treat transfers and compute operations as the same \"type\" of operation, rather than as operations that can be concurrently scheduled.  As @prb12  notes  in #2848, Recvs are scheduled immediately and Sends are scheduled once the input data is available.  So on a rough timeline the above sub-graphs seem to currently be scheduled as (see #2848 for e.g. where sends are scheduled immediately)\n\nThe problem with this scheduling is that it wastes both I/O bandwidth and device compute. Given that these operations can be concurrently scheduled an optimal order of transfer and execution might be:\n\n(a,b) must be transferred before foo can execute, (c) must be transferred before bar can and so on. The output transfers for [(d,e), (k), (n)] can be scheduled last.\nThe point is that scheduling the ops on the (GPU?) device necessarily imposes an ordering on how the inputs should be transferred. While my understanding of how ops are currently scheduled is limited  I would ideally expect it to be something like [a, b, foo, c, bar, f, g, h, qux, i, j, baz, l, plug, m, thud, d, e, k, n].\nRunning the ops as soon as their inputs are available should use the device optimally. If your compute beats your I/O then the device is used optimally. If not, then you've done the best you can given the algorithms in the ops.\nAll this information is available in the graph. I think the tricky part is estimating how many sub-graphs can be scheduled in one go -- this would involve balancing device memory vs the size of the input , temporary result and output tensors. But looking at #2848 and other issues suggests that tensorflow does this already. It just doesn't currently schedule transfers and executions optimally.\nFurther optimisations based on the number of copy engines on the GPU are possible.", "body": "I wonder if it's possible to build the prefetch into the scheduler? Consider the following diagram:\r\n\r\n![img_20161212_134953255](https://cloud.githubusercontent.com/assets/3530212/21098743/4774b4dc-c073-11e6-9fa1-f90ecb4a213b.jpg)\r\n\r\nWe have three separate sub-graphs [(foo->bar, qux->baz, plug->thud)] that require execution on a (GPU?) device, depending on inputs [(a,b,c),(f,g,h,i,k), (l,m)] located across a device boundary and whose outputs [(d,e),(k),(n)] must be transmitted across the boundary once they are complete.\r\n\r\nAs it currently stands, tensorflow appears (to me) *to treat transfers and compute operations as the same \"type\" of operation, rather than as operations that can be concurrently scheduled*.  As @prb12  notes  in https://github.com/tensorflow/tensorflow/issues/2848, Recvs are scheduled immediately and Sends are scheduled once the input data is available.  So on a rough timeline the above sub-graphs seem to currently be scheduled as (see https://github.com/tensorflow/tensorflow/issues/2848 for e.g. where sends are scheduled immediately)\r\n\r\n![img_20161212_141704007](https://cloud.githubusercontent.com/assets/3530212/21099282/05f56d00-c076-11e6-9983-4919a9cce0da.jpg)\r\n\r\nThe problem with this scheduling is that it wastes both I/O bandwidth and device compute. Given that these operations *can* be concurrently scheduled an optimal order of transfer and execution might be:\r\n\r\n![img_20161212_135248066](https://cloud.githubusercontent.com/assets/3530212/21098800/945d2130-c073-11e6-8eef-b3d28ebeb4f9.jpg)\r\n\r\n(a,b) must be transferred before foo can execute, (c) must be transferred before bar can and so on. The output transfers for [(d,e), (k), (n)] can be scheduled last.\r\n\r\nThe point is that scheduling the ops on the (GPU?) device necessarily imposes an ordering on how the inputs should be transferred. While my understanding of how ops are currently scheduled is limited  I would ideally expect it to be something like [a, b, foo, c, bar, f, g, h, qux, i, j, baz, l, plug, m, thud, d, e, k, n].\r\n\r\nRunning the ops *as soon* as their inputs are available should use the device optimally. If your compute beats your I/O then the device is used optimally. If not, then you've done the best you can given the algorithms in the ops.\r\n\r\nAll this information is available in the graph. I think the tricky part is estimating how many sub-graphs can be scheduled in one go -- this would involve balancing device memory vs the size of the input , temporary result and output tensors. But looking at https://github.com/tensorflow/tensorflow/issues/2848 and other issues suggests that tensorflow does this already. It just doesn't currently schedule transfers and executions optimally.\r\n\r\nFurther optimisations based on the number of copy engines on the GPU are possible.\r\n\r\n\r\n\r\n\r\n"}