{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/267598486", "html_url": "https://github.com/tensorflow/tensorflow/issues/5722#issuecomment-267598486", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5722", "id": 267598486, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NzU5ODQ4Ng==", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-16T13:53:44Z", "updated_at": "2016-12-20T14:04:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a> Thanks for taking the time to write your response, the information is very useful information for those coming from a CUDA background.</p>\n<blockquote>\n<p>Each transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\". (subtlety - on GPU this means when the op which computes the Tensor has been added to the GPU compute stream.) The 'Recv' op has no inputs, an hence is ready to be dispatched immediately, but the Send and Recv ops actually execute a rendezvous and the actual copy/DMA between devices is scheduled once the rendezvous is complete.</p>\n</blockquote>\n<p>I browsed through the code, but was unable to follow through all the Send/Recv/Rendezvouz/CUDA abstractions. Briefly, is this similar to the following pattern for a GPU to CPU transfer? In one thread, asynchronously issue:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> for some kernel computing a result placed in d_result</span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> kernel&lt;..., stream&gt;(arg0, arg1, d_result);</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> \"Send op\" starts here</span>\n<span class=\"pl-en\">cudaMemcpyAsync</span>(s_result, d_result, stream);\ncudaEvent_t done;\n<span class=\"pl-en\">cudaEventRecord</span>(done, stream);</pre></div>\n<p>and in some synchronisation thread:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> \"Recv op\" starts here</span>\n<span class=\"pl-en\">cudaEventSynchronise</span>(done);\n<span class=\"pl-en\">release_to_pool</span>(d_result);\n<span class=\"pl-en\">probably_in_another_thread_use</span>(s_result);</pre></div>\n<blockquote>\n<p>This isn't really true. Transfers to and from GPU are executed on their own CUDA streams - and hence can happen in parallel with compute. The ops dependent on Tensors which are being transferred to GPU cause the compute strream to take a dependency on the memcpy_host_to_device stream using the CUDA RecordEvent and StreamWaitEvent primitives. There are some awkward case to deal with here and the current implementation is suboptimal in a couple of places (mainly due to there being a single compute stream), but it's a lot better than what you describe.</p>\n</blockquote>\n<p>OK I'm familiar with this pattern. Thanks for clarifying this, but I have an example below where I'm struggling to achieve this overlap through submitting separate compute to <code>tf.Session.run</code> in multiple threads.</p>\n<blockquote>\n<p>In fact, what currently happens is that a, c, c, f, g, h will all get scheduled on some <strong>non-deterministic order</strong> on the device_to_host stream shortly after their inputs are available on the CPU device.</p>\n</blockquote>\n<p>The non-determinism is what I want to niggle about. More below:</p>\n<blockquote>\n<p>As soon as the async MemcpyH2D has been enqueued for all of the inputs for foo, the compute stream will be serialized on the copy stream (using an event) and the CUDA kernel for foo will be enqueued on the compute stream. Once foo and bar have been enqueued, the device_to_host stream is made to depend on the compute stream using another event, and an async MemCopyD2H is ennqueued on the device_to_host stream. This can execute concurrently wrt quz, baz, plug, thd et al.</p>\n</blockquote>\n<blockquote>\n<p>Each transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\"</p>\n</blockquote>\n<p>OK tensorflow is scheduling ops non-deterministically because its not assuming knowledge of when inputs will become available (network transfers, multiple ops running on multiple CPU cores). Yeah I can see why this is a difficult problem. I think this is perhaps where CUDA programmers struggle. e.g. thinks like \"I know all my inputs are on the CPU and can be scheduled in a specific order, why isn't tensorflow scheduling it like I know I could do?\"</p>\n<p><em>BUT</em> this non-determinism does bite as it can result in input transfers for multiple sub-graphs being unnecessarily scheduled upfront before compute occurs (example below).  Perhaps a possible solution is to logically group inputs required by a GPU op for transfer? I don't know enough of the scheduling internals to ascertain whether this is a reasonable suggestion. A GPU queue would achieve the same effect</p>\n<p>I've another concrete example to demonstrate. We're developing radio telescope simulator and the challenge (in terms of data transfer) is that 550MB (!) voxel cubes are used to model how the beam of an antenna affects the signal from a radio source. I've sharded feeding (3 threads), compute (3 threads) and output dequeues (1 thread). I have 6 units of compute that I want to perform. The algorithm is compute bound.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/3530212/21262989/ef9e90b6-c39c-11e6-98fd-c8e1630dd628.png\"><img src=\"https://cloud.githubusercontent.com/assets/3530212/21262989/ef9e90b6-c39c-11e6-98fd-c8e1630dd628.png\" alt=\"titan_x_3_shards\" style=\"max-width:100%;\"></a></p>\n<p>I've attached:</p>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/files/657167/nvvp_prof.zip\">CUDA profile</a> on a Titan X. Relevant run from 20.65s to 21.99s. (merging RunMetadata objects for multiple threads produces a trace file that is too large for chrome).</li>\n<li>Chrome <a href=\"https://github.com/tensorflow/tensorflow/files/657206/timeline.json.zip\">trace</a> a on laptop GTX960. Single thread feeding, single thread computing.</li>\n</ul>\n<p>The above profile shows the 6 units of compute sheduled into two sections of 3 units each (presumably because the 3 compute threads submit work to <code>Session.run</code> at a similar time). As the above profile and image shows, the 3 voxel cubes (1.5GB!) associated with 3 work units (or, using the terminology in my previous post, sub-graphs) scheduled for transfer upfront. Additionally the 3 transfers for the last 3 work units/sub-graphs are not overlapped with the compute of the first 3.</p>\n<p>This is what I was describing using the toy example in the previous post:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/3530212/21099282/05f56d00-c076-11e6-9983-4919a9cce0da.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/3530212/21099282/05f56d00-c076-11e6-9983-4919a9cce0da.jpg\" alt=\"prev\" style=\"max-width:100%;\"></a></p>\n<p>I think the associated code is too large to serve as a test case. Would a minimal example be useful here?</p>", "body_text": "@prb12 Thanks for taking the time to write your response, the information is very useful information for those coming from a CUDA background.\n\nEach transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\". (subtlety - on GPU this means when the op which computes the Tensor has been added to the GPU compute stream.) The 'Recv' op has no inputs, an hence is ready to be dispatched immediately, but the Send and Recv ops actually execute a rendezvous and the actual copy/DMA between devices is scheduled once the rendezvous is complete.\n\nI browsed through the code, but was unable to follow through all the Send/Recv/Rendezvouz/CUDA abstractions. Briefly, is this similar to the following pattern for a GPU to CPU transfer? In one thread, asynchronously issue:\n// for some kernel computing a result placed in d_result\n// kernel<..., stream>(arg0, arg1, d_result);\n\n// \"Send op\" starts here\ncudaMemcpyAsync(s_result, d_result, stream);\ncudaEvent_t done;\ncudaEventRecord(done, stream);\nand in some synchronisation thread:\n// \"Recv op\" starts here\ncudaEventSynchronise(done);\nrelease_to_pool(d_result);\nprobably_in_another_thread_use(s_result);\n\nThis isn't really true. Transfers to and from GPU are executed on their own CUDA streams - and hence can happen in parallel with compute. The ops dependent on Tensors which are being transferred to GPU cause the compute strream to take a dependency on the memcpy_host_to_device stream using the CUDA RecordEvent and StreamWaitEvent primitives. There are some awkward case to deal with here and the current implementation is suboptimal in a couple of places (mainly due to there being a single compute stream), but it's a lot better than what you describe.\n\nOK I'm familiar with this pattern. Thanks for clarifying this, but I have an example below where I'm struggling to achieve this overlap through submitting separate compute to tf.Session.run in multiple threads.\n\nIn fact, what currently happens is that a, c, c, f, g, h will all get scheduled on some non-deterministic order on the device_to_host stream shortly after their inputs are available on the CPU device.\n\nThe non-determinism is what I want to niggle about. More below:\n\nAs soon as the async MemcpyH2D has been enqueued for all of the inputs for foo, the compute stream will be serialized on the copy stream (using an event) and the CUDA kernel for foo will be enqueued on the compute stream. Once foo and bar have been enqueued, the device_to_host stream is made to depend on the compute stream using another event, and an async MemCopyD2H is ennqueued on the device_to_host stream. This can execute concurrently wrt quz, baz, plug, thd et al.\n\n\nEach transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\"\n\nOK tensorflow is scheduling ops non-deterministically because its not assuming knowledge of when inputs will become available (network transfers, multiple ops running on multiple CPU cores). Yeah I can see why this is a difficult problem. I think this is perhaps where CUDA programmers struggle. e.g. thinks like \"I know all my inputs are on the CPU and can be scheduled in a specific order, why isn't tensorflow scheduling it like I know I could do?\"\nBUT this non-determinism does bite as it can result in input transfers for multiple sub-graphs being unnecessarily scheduled upfront before compute occurs (example below).  Perhaps a possible solution is to logically group inputs required by a GPU op for transfer? I don't know enough of the scheduling internals to ascertain whether this is a reasonable suggestion. A GPU queue would achieve the same effect\nI've another concrete example to demonstrate. We're developing radio telescope simulator and the challenge (in terms of data transfer) is that 550MB (!) voxel cubes are used to model how the beam of an antenna affects the signal from a radio source. I've sharded feeding (3 threads), compute (3 threads) and output dequeues (1 thread). I have 6 units of compute that I want to perform. The algorithm is compute bound.\n\nI've attached:\n\nCUDA profile on a Titan X. Relevant run from 20.65s to 21.99s. (merging RunMetadata objects for multiple threads produces a trace file that is too large for chrome).\nChrome trace a on laptop GTX960. Single thread feeding, single thread computing.\n\nThe above profile shows the 6 units of compute sheduled into two sections of 3 units each (presumably because the 3 compute threads submit work to Session.run at a similar time). As the above profile and image shows, the 3 voxel cubes (1.5GB!) associated with 3 work units (or, using the terminology in my previous post, sub-graphs) scheduled for transfer upfront. Additionally the 3 transfers for the last 3 work units/sub-graphs are not overlapped with the compute of the first 3.\nThis is what I was describing using the toy example in the previous post:\n\nI think the associated code is too large to serve as a test case. Would a minimal example be useful here?", "body": "@prb12 Thanks for taking the time to write your response, the information is very useful information for those coming from a CUDA background.\r\n\r\n> Each transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\". (subtlety - on GPU this means when the op which computes the Tensor has been added to the GPU compute stream.) The 'Recv' op has no inputs, an hence is ready to be dispatched immediately, but the Send and Recv ops actually execute a rendezvous and the actual copy/DMA between devices is scheduled once the rendezvous is complete.\r\n\r\nI browsed through the code, but was unable to follow through all the Send/Recv/Rendezvouz/CUDA abstractions. Briefly, is this similar to the following pattern for a GPU to CPU transfer? In one thread, asynchronously issue:\r\n\r\n```cpp\r\n// for some kernel computing a result placed in d_result\r\n// kernel<..., stream>(arg0, arg1, d_result);\r\n\r\n// \"Send op\" starts here\r\ncudaMemcpyAsync(s_result, d_result, stream);\r\ncudaEvent_t done;\r\ncudaEventRecord(done, stream);\r\n```\r\n\r\nand in some synchronisation thread:\r\n\r\n```cpp\r\n// \"Recv op\" starts here\r\ncudaEventSynchronise(done);\r\nrelease_to_pool(d_result);\r\nprobably_in_another_thread_use(s_result);\r\n```\r\n\r\n> This isn't really true. Transfers to and from GPU are executed on their own CUDA streams - and hence can happen in parallel with compute. The ops dependent on Tensors which are being transferred to GPU cause the compute strream to take a dependency on the memcpy_host_to_device stream using the CUDA RecordEvent and StreamWaitEvent primitives. There are some awkward case to deal with here and the current implementation is suboptimal in a couple of places (mainly due to there being a single compute stream), but it's a lot better than what you describe.\r\n\r\nOK I'm familiar with this pattern. Thanks for clarifying this, but I have an example below where I'm struggling to achieve this overlap through submitting separate compute to `tf.Session.run` in multiple threads.\r\n\r\n> In fact, what currently happens is that a, c, c, f, g, h will all get scheduled on some **non-deterministic order** on the device_to_host stream shortly after their inputs are available on the CPU device.\r\n\r\nThe non-determinism is what I want to niggle about. More below:\r\n\r\n> As soon as the async MemcpyH2D has been enqueued for all of the inputs for foo, the compute stream will be serialized on the copy stream (using an event) and the CUDA kernel for foo will be enqueued on the compute stream. Once foo and bar have been enqueued, the device_to_host stream is made to depend on the compute stream using another event, and an async MemCopyD2H is ennqueued on the device_to_host stream. This can execute concurrently wrt quz, baz, plug, thd et al.\r\n\r\n> Each transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\"\r\n\r\nOK tensorflow is scheduling ops non-deterministically because its not assuming knowledge of when inputs will become available (network transfers, multiple ops running on multiple CPU cores). Yeah I can see why this is a difficult problem. I think this is perhaps where CUDA programmers struggle. e.g. thinks like \"I know all my inputs are on the CPU and can be scheduled in a specific order, why isn't tensorflow scheduling it like I know I could do?\"\r\n\r\n*BUT* this non-determinism does bite as it can result in input transfers for multiple sub-graphs being unnecessarily scheduled upfront before compute occurs (example below).  Perhaps a possible solution is to logically group inputs required by a GPU op for transfer? I don't know enough of the scheduling internals to ascertain whether this is a reasonable suggestion. A GPU queue would achieve the same effect\r\n\r\nI've another concrete example to demonstrate. We're developing radio telescope simulator and the challenge (in terms of data transfer) is that 550MB (!) voxel cubes are used to model how the beam of an antenna affects the signal from a radio source. I've sharded feeding (3 threads), compute (3 threads) and output dequeues (1 thread). I have 6 units of compute that I want to perform. The algorithm is compute bound.\r\n\r\n![titan_x_3_shards](https://cloud.githubusercontent.com/assets/3530212/21262989/ef9e90b6-c39c-11e6-98fd-c8e1630dd628.png)\r\n\r\nI've attached:\r\n-  [CUDA profile](https://github.com/tensorflow/tensorflow/files/657167/nvvp_prof.zip) on a Titan X. Relevant run from 20.65s to 21.99s. (merging RunMetadata objects for multiple threads produces a trace file that is too large for chrome).\r\n- Chrome [trace](https://github.com/tensorflow/tensorflow/files/657206/timeline.json.zip) a on laptop GTX960. Single thread feeding, single thread computing.\r\n\r\nThe above profile shows the 6 units of compute sheduled into two sections of 3 units each (presumably because the 3 compute threads submit work to `Session.run` at a similar time). As the above profile and image shows, the 3 voxel cubes (1.5GB!) associated with 3 work units (or, using the terminology in my previous post, sub-graphs) scheduled for transfer upfront. Additionally the 3 transfers for the last 3 work units/sub-graphs are not overlapped with the compute of the first 3.\r\n\r\nThis is what I was describing using the toy example in the previous post:\r\n\r\n![prev](https://cloud.githubusercontent.com/assets/3530212/21099282/05f56d00-c076-11e6-9983-4919a9cce0da.jpg)\r\n\r\nI think the associated code is too large to serve as a test case. Would a minimal example be useful here?\r\n\r\n\r\n\r\n"}