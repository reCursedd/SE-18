{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/262039609", "html_url": "https://github.com/tensorflow/tensorflow/issues/5722#issuecomment-262039609", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5722", "id": 262039609, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MjAzOTYwOQ==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-21T19:23:09Z", "updated_at": "2016-11-21T19:23:09Z", "author_association": "MEMBER", "body_html": "<p>As you say, queues are currently only implemented on CPU and cannot cross devices.</p>\n<p>If you don't have any useful computation which can happen while the <em>async</em> transfer D2H is in progress then the only way to overlap this transfer would be to do it in a different call to session.run and store the result in some sort of variable.  However, this would rely on coordination between steps</p>\n<p>The only stateful ops available on GPU are Variables and TensorArrays.</p>\n<p>You <em>could</em> imagine manually building some sort of pipeline or double-buffering where the session.run call computed on one Variable and simultaneously transferred the input for the next step into a second Variable.</p>\n<p>Alternatively, you could look at how the dynamic while loop speculatively executes multiple loop iterations in parallel, overlapping transfers for subsequent steps.</p>", "body_text": "As you say, queues are currently only implemented on CPU and cannot cross devices.\nIf you don't have any useful computation which can happen while the async transfer D2H is in progress then the only way to overlap this transfer would be to do it in a different call to session.run and store the result in some sort of variable.  However, this would rely on coordination between steps\nThe only stateful ops available on GPU are Variables and TensorArrays.\nYou could imagine manually building some sort of pipeline or double-buffering where the session.run call computed on one Variable and simultaneously transferred the input for the next step into a second Variable.\nAlternatively, you could look at how the dynamic while loop speculatively executes multiple loop iterations in parallel, overlapping transfers for subsequent steps.", "body": "As you say, queues are currently only implemented on CPU and cannot cross devices.\r\n\r\nIf you don't have any useful computation which can happen while the *async* transfer D2H is in progress then the only way to overlap this transfer would be to do it in a different call to session.run and store the result in some sort of variable.  However, this would rely on coordination between steps \r\n\r\nThe only stateful ops available on GPU are Variables and TensorArrays.  \r\n\r\nYou *could* imagine manually building some sort of pipeline or double-buffering where the session.run call computed on one Variable and simultaneously transferred the input for the next step into a second Variable.\r\n\r\nAlternatively, you could look at how the dynamic while loop speculatively executes multiple loop iterations in parallel, overlapping transfers for subsequent steps.\r\n\r\n"}