{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266498577", "html_url": "https://github.com/tensorflow/tensorflow/issues/5722#issuecomment-266498577", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5722", "id": 266498577, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjQ5ODU3Nw==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-12T17:42:39Z", "updated_at": "2016-12-12T18:04:57Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3530212\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sjperkins\">@sjperkins</a> Thanks for your suggestions.  Here are some brief comments: (I don't really have time to go into this at length)</p>\n<blockquote>\n<p>Recvs are scheduled immediately and Sends are scheduled once the input data is available. So on a rough timeline the above sub-graphs seem to currently be scheduled as (see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"160137156\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2848\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2848/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2848\">#2848</a> for e.g. where sends are scheduled immediately)</p>\n</blockquote>\n<p>I think you are confused about what <code>Send</code> and <code>Recv</code> <em>ops</em> actually do.</p>\n<p>Each transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\". (subtlety - on GPU this means when the op which computes the Tensor has been added to the GPU compute stream.)  The 'Recv' op has no inputs, an hence is ready to be dispatched immediately, but the Send and Recv ops actually execute a <em>rendezvous</em> and the  copy/DMA between devices is only scheduled once the rendezvous is complete.   Many Send and Recv ops can be dispatched simultaneously (<code>inter_op_parallelism_threads</code>), and they do not (at this point) have any ordering constraints with respect to other computation ops which have no dataflow dependency.</p>\n<blockquote>\n<p>The point is that scheduling the ops on the (GPU?) device necessarily imposes an ordering on how the inputs should be transferred.</p>\n</blockquote>\n<p>I strongly encourage you to read <code>executor.cc</code>.  The above described mechanism is there purely to capture these ordering dependencies in  a way which is safe for async memory allocation and kernel execution.</p>\n<blockquote>\n<p>appears (to me) to treat transfers and compute operations as the same \"type\" of operation, rather than as operations that can be concurrently scheduled.</p>\n</blockquote>\n<p>This isn't really true. Transfers to and from GPU are executed on their own CUDA streams - and hence can happen in parallel with compute.  The ops dependent on Tensors which are being transferred to GPU cause the compute strream to take a dependency on the memcpy_host_to_device stream using the CUDA <code>RecordEvent</code> and <code>StreamWaitEvent</code> primitives.   There are some awkward case to deal with here and the current implementation is suboptimal in a couple of places (mainly due to there being a single compute stream), but it's a lot better than what you describe.</p>\n<blockquote>\n<p>While my understanding of how ops are currently scheduled is limited I would ideally expect it to be something like [a, b, foo, c, bar, f, g, h, qux, i, j, baz, l, plug, m, thud, d, e, k, n].</p>\n</blockquote>\n<p>In fact, what currently happens is that a, c, c, f, g, h will all get scheduled on some non-deterministic order on the device_to_host stream shortly after their inputs are available on the CPU device.  As soon as the <strong>async</strong>  MemcpyH2D has been enqueued for all of the inputs for foo, the compute stream will be serialized on the copy stream (using an event) and the CUDA kernel for <code>foo</code> will be enqueued on the compute stream.  Once <code>foo</code> and <code>bar</code> have been enqueued, the device_to_host stream is made to depend on the compute stream using another event, and an async MemCopyD2H is ennqueued on the device_to_host stream.  This can execute concurrently wrt quz, baz, plug, thd et al.</p>\n<p>The sub-optimal behavior, which <em>sometimes</em> matters, especially with device-to-device transfers is that unrelated compute ops can get added to the compute stream <em>before</em> the event which we use to serialize the copy back to the host (or other GPU).  The fix for this is not <em>too</em> bad, and involves adding an event to the compute stream eagerly as part of enqueueing and compute op which feeds into a <code>Send</code>.  However, this comes at the cost of needing quite a large number of CUDA Events, and we didn't deem this performance tradeoff worthwhile.  I suspect this decision will be revisited soon since the impact on multi-GPU configurations can be significant.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7721540\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/TimZaman\">@TimZaman</a></p>\n<blockquote>\n<p>It would bring amazing benefit from operations that are heavy on data, and light on compute.</p>\n</blockquote>\n<p>This is really not a very common case in my experience... e.g. when training an inception model on a single GPU the step time is around 2 seconds and DMA transfers are largely in the noise.  Latency sensitive inference steps may be more of an issue.  I would be open to seeing more evidence of where this matters for non-trivial workloads.</p>", "body_text": "@sjperkins Thanks for your suggestions.  Here are some brief comments: (I don't really have time to go into this at length)\n\nRecvs are scheduled immediately and Sends are scheduled once the input data is available. So on a rough timeline the above sub-graphs seem to currently be scheduled as (see #2848 for e.g. where sends are scheduled immediately)\n\nI think you are confused about what Send and Recv ops actually do.\nEach transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\". (subtlety - on GPU this means when the op which computes the Tensor has been added to the GPU compute stream.)  The 'Recv' op has no inputs, an hence is ready to be dispatched immediately, but the Send and Recv ops actually execute a rendezvous and the  copy/DMA between devices is only scheduled once the rendezvous is complete.   Many Send and Recv ops can be dispatched simultaneously (inter_op_parallelism_threads), and they do not (at this point) have any ordering constraints with respect to other computation ops which have no dataflow dependency.\n\nThe point is that scheduling the ops on the (GPU?) device necessarily imposes an ordering on how the inputs should be transferred.\n\nI strongly encourage you to read executor.cc.  The above described mechanism is there purely to capture these ordering dependencies in  a way which is safe for async memory allocation and kernel execution.\n\nappears (to me) to treat transfers and compute operations as the same \"type\" of operation, rather than as operations that can be concurrently scheduled.\n\nThis isn't really true. Transfers to and from GPU are executed on their own CUDA streams - and hence can happen in parallel with compute.  The ops dependent on Tensors which are being transferred to GPU cause the compute strream to take a dependency on the memcpy_host_to_device stream using the CUDA RecordEvent and StreamWaitEvent primitives.   There are some awkward case to deal with here and the current implementation is suboptimal in a couple of places (mainly due to there being a single compute stream), but it's a lot better than what you describe.\n\nWhile my understanding of how ops are currently scheduled is limited I would ideally expect it to be something like [a, b, foo, c, bar, f, g, h, qux, i, j, baz, l, plug, m, thud, d, e, k, n].\n\nIn fact, what currently happens is that a, c, c, f, g, h will all get scheduled on some non-deterministic order on the device_to_host stream shortly after their inputs are available on the CPU device.  As soon as the async  MemcpyH2D has been enqueued for all of the inputs for foo, the compute stream will be serialized on the copy stream (using an event) and the CUDA kernel for foo will be enqueued on the compute stream.  Once foo and bar have been enqueued, the device_to_host stream is made to depend on the compute stream using another event, and an async MemCopyD2H is ennqueued on the device_to_host stream.  This can execute concurrently wrt quz, baz, plug, thd et al.\nThe sub-optimal behavior, which sometimes matters, especially with device-to-device transfers is that unrelated compute ops can get added to the compute stream before the event which we use to serialize the copy back to the host (or other GPU).  The fix for this is not too bad, and involves adding an event to the compute stream eagerly as part of enqueueing and compute op which feeds into a Send.  However, this comes at the cost of needing quite a large number of CUDA Events, and we didn't deem this performance tradeoff worthwhile.  I suspect this decision will be revisited soon since the impact on multi-GPU configurations can be significant.\n@TimZaman\n\nIt would bring amazing benefit from operations that are heavy on data, and light on compute.\n\nThis is really not a very common case in my experience... e.g. when training an inception model on a single GPU the step time is around 2 seconds and DMA transfers are largely in the noise.  Latency sensitive inference steps may be more of an issue.  I would be open to seeing more evidence of where this matters for non-trivial workloads.", "body": "@sjperkins Thanks for your suggestions.  Here are some brief comments: (I don't really have time to go into this at length)\r\n\r\n>  Recvs are scheduled immediately and Sends are scheduled once the input data is available. So on a rough timeline the above sub-graphs seem to currently be scheduled as (see #2848 for e.g. where sends are scheduled immediately)\r\n\r\nI think you are confused about what `Send` and `Recv` *ops* actually do.   \r\n\r\nEach transfer between devices involves two ops - the Send op on the source device is executed as soon as its input tensor is \"ready\". (subtlety - on GPU this means when the op which computes the Tensor has been added to the GPU compute stream.)  The 'Recv' op has no inputs, an hence is ready to be dispatched immediately, but the Send and Recv ops actually execute a _rendezvous_ and the  copy/DMA between devices is only scheduled once the rendezvous is complete.   Many Send and Recv ops can be dispatched simultaneously (`inter_op_parallelism_threads`), and they do not (at this point) have any ordering constraints with respect to other computation ops which have no dataflow dependency.\r\n\r\n> The point is that scheduling the ops on the (GPU?) device necessarily imposes an ordering on how the inputs should be transferred.\r\n\r\nI strongly encourage you to read `executor.cc`.  The above described mechanism is there purely to capture these ordering dependencies in  a way which is safe for async memory allocation and kernel execution.\r\n\r\n> appears (to me) to treat transfers and compute operations as the same \"type\" of operation, rather than as operations that can be concurrently scheduled. \r\n\r\nThis isn't really true. Transfers to and from GPU are executed on their own CUDA streams - and hence can happen in parallel with compute.  The ops dependent on Tensors which are being transferred to GPU cause the compute strream to take a dependency on the memcpy_host_to_device stream using the CUDA `RecordEvent` and `StreamWaitEvent` primitives.   There are some awkward case to deal with here and the current implementation is suboptimal in a couple of places (mainly due to there being a single compute stream), but it's a lot better than what you describe.\r\n\r\n> While my understanding of how ops are currently scheduled is limited I would ideally expect it to be something like [a, b, foo, c, bar, f, g, h, qux, i, j, baz, l, plug, m, thud, d, e, k, n].\r\n\r\nIn fact, what currently happens is that a, c, c, f, g, h will all get scheduled on some non-deterministic order on the device_to_host stream shortly after their inputs are available on the CPU device.  As soon as the **async**  MemcpyH2D has been enqueued for all of the inputs for foo, the compute stream will be serialized on the copy stream (using an event) and the CUDA kernel for `foo` will be enqueued on the compute stream.  Once `foo` and `bar` have been enqueued, the device_to_host stream is made to depend on the compute stream using another event, and an async MemCopyD2H is ennqueued on the device_to_host stream.  This can execute concurrently wrt quz, baz, plug, thd et al.\r\n\r\nThe sub-optimal behavior, which *sometimes* matters, especially with device-to-device transfers is that unrelated compute ops can get added to the compute stream *before* the event which we use to serialize the copy back to the host (or other GPU).  The fix for this is not *too* bad, and involves adding an event to the compute stream eagerly as part of enqueueing and compute op which feeds into a `Send`.  However, this comes at the cost of needing quite a large number of CUDA Events, and we didn't deem this performance tradeoff worthwhile.  I suspect this decision will be revisited soon since the impact on multi-GPU configurations can be significant.\r\n\r\n@TimZaman \r\n> It would bring amazing benefit from operations that are heavy on data, and light on compute.\r\n\r\nThis is really not a very common case in my experience... e.g. when training an inception model on a single GPU the step time is around 2 seconds and DMA transfers are largely in the noise.  Latency sensitive inference steps may be more of an issue.  I would be open to seeing more evidence of where this matters for non-trivial workloads.\r\n\r\n"}