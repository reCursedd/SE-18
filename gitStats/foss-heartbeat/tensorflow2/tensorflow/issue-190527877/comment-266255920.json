{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266255920", "html_url": "https://github.com/tensorflow/tensorflow/issues/5722#issuecomment-266255920", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5722", "id": 266255920, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjI1NTkyMA==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-11T02:13:40Z", "updated_at": "2016-12-11T02:36:31Z", "author_association": "MEMBER", "body_html": "<p>Two comments on the code:</p>\n<ol>\n<li>Measuring the execution time of a step when you are tracing and retrieving a large RunMetadata proto back to Python is never a good idea - especially if you haven't installed the C++ protobuf library.</li>\n<li>I <em>think</em> there's probably a simpler way to do this:\n<ul>\n<li>Place a (non-trainable) Variable on the GPU to hold a single cached value.</li>\n<li>Each step, in parallel:\n<ul>\n<li>Take a snapshot of the current value of the GPU variable using an Identity op</li>\n<li>Dequeue from the FIFO (i.e. kick off the copy from the CPU)</li>\n</ul>\n</li>\n<li>Serialized after the read, Assign the value from the FIFO into the variable for use in the next step.</li>\n<li>Compute with the snapshot value.</li>\n</ul>\n</li>\n</ol>\n<p>This may be equivalent to your code... but I admit I got a little lost in all the X's ;-)<br>\nI'm also not 100% sure whether this would play well with auto-gradient code though...</p>", "body_text": "Two comments on the code:\n\nMeasuring the execution time of a step when you are tracing and retrieving a large RunMetadata proto back to Python is never a good idea - especially if you haven't installed the C++ protobuf library.\nI think there's probably a simpler way to do this:\n\nPlace a (non-trainable) Variable on the GPU to hold a single cached value.\nEach step, in parallel:\n\nTake a snapshot of the current value of the GPU variable using an Identity op\nDequeue from the FIFO (i.e. kick off the copy from the CPU)\n\n\nSerialized after the read, Assign the value from the FIFO into the variable for use in the next step.\nCompute with the snapshot value.\n\n\n\nThis may be equivalent to your code... but I admit I got a little lost in all the X's ;-)\nI'm also not 100% sure whether this would play well with auto-gradient code though...", "body": "Two comments on the code:\r\n1) Measuring the execution time of a step when you are tracing and retrieving a large RunMetadata proto back to Python is never a good idea - especially if you haven't installed the C++ protobuf library.  \r\n2) I *think* there's probably a simpler way to do this:\r\n   * Place a (non-trainable) Variable on the GPU to hold a single cached value.  \r\n   * Each step, in parallel: \r\n       * Take a snapshot of the current value of the GPU variable using an Identity op\r\n       * Dequeue from the FIFO (i.e. kick off the copy from the CPU)\r\n   * Serialized after the read, Assign the value from the FIFO into the variable for use in the next step.\r\n   * Compute with the snapshot value.\r\n\r\nThis may be equivalent to your code... but I admit I got a little lost in all the X's ;-)\r\nI'm also not 100% sure whether this would play well with auto-gradient code though...\r\n"}