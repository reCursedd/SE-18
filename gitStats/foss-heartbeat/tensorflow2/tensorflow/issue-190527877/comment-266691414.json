{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266691414", "html_url": "https://github.com/tensorflow/tensorflow/issues/5722#issuecomment-266691414", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5722", "id": 266691414, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjY5MTQxNA==", "user": {"login": "llhe", "id": 192829, "node_id": "MDQ6VXNlcjE5MjgyOQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/192829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/llhe", "html_url": "https://github.com/llhe", "followers_url": "https://api.github.com/users/llhe/followers", "following_url": "https://api.github.com/users/llhe/following{/other_user}", "gists_url": "https://api.github.com/users/llhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/llhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/llhe/subscriptions", "organizations_url": "https://api.github.com/users/llhe/orgs", "repos_url": "https://api.github.com/users/llhe/repos", "events_url": "https://api.github.com/users/llhe/events{/privacy}", "received_events_url": "https://api.github.com/users/llhe/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-13T09:38:56Z", "updated_at": "2016-12-13T09:38:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a></p>\n<blockquote>\n<p>This is really not a very common case in my experience... e.g. when training an inception model on a single GPU the step time is around 2 seconds and DMA transfers are largely in the noise. Latency sensitive inference steps may be more of an issue. I would be open to seeing more evidence of where this matters for non-trivial workloads.</p>\n</blockquote>\n<p>What about the shallow model (such as LR, deep and wide) workloads? I think this should be one of the heavy on data case. The H2D transfer may not be the major bottleneck compared to IO, but indeed it can improve if async transfer is supported.</p>", "body_text": "@prb12\n\nThis is really not a very common case in my experience... e.g. when training an inception model on a single GPU the step time is around 2 seconds and DMA transfers are largely in the noise. Latency sensitive inference steps may be more of an issue. I would be open to seeing more evidence of where this matters for non-trivial workloads.\n\nWhat about the shallow model (such as LR, deep and wide) workloads? I think this should be one of the heavy on data case. The H2D transfer may not be the major bottleneck compared to IO, but indeed it can improve if async transfer is supported.", "body": "@prb12 \r\n> This is really not a very common case in my experience... e.g. when training an inception model on a single GPU the step time is around 2 seconds and DMA transfers are largely in the noise. Latency sensitive inference steps may be more of an issue. I would be open to seeing more evidence of where this matters for non-trivial workloads.\r\n\r\nWhat about the shallow model (such as LR, deep and wide) workloads? I think this should be one of the heavy on data case. The H2D transfer may not be the major bottleneck compared to IO, but indeed it can improve if async transfer is supported."}