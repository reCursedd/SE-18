{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4465", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4465/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4465/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4465/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4465", "id": 177808857, "node_id": "MDU6SXNzdWUxNzc4MDg4NTc=", "number": 4465, "title": "Distributed training optimization", "user": {"login": "JianbangZ", "id": 15835199, "node_id": "MDQ6VXNlcjE1ODM1MTk5", "avatar_url": "https://avatars1.githubusercontent.com/u/15835199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JianbangZ", "html_url": "https://github.com/JianbangZ", "followers_url": "https://api.github.com/users/JianbangZ/followers", "following_url": "https://api.github.com/users/JianbangZ/following{/other_user}", "gists_url": "https://api.github.com/users/JianbangZ/gists{/gist_id}", "starred_url": "https://api.github.com/users/JianbangZ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JianbangZ/subscriptions", "organizations_url": "https://api.github.com/users/JianbangZ/orgs", "repos_url": "https://api.github.com/users/JianbangZ/repos", "events_url": "https://api.github.com/users/JianbangZ/events{/privacy}", "received_events_url": "https://api.github.com/users/JianbangZ/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-09-19T14:59:24Z", "updated_at": "2016-09-19T21:59:10Z", "closed_at": "2016-09-19T21:59:10Z", "author_association": "NONE", "body_html": "<p>After playing with the current distributed training implementation for a while, I think it views each GPU as a separate worker.However, It is common now to have 2~4 GPUs in one box. Isn't it better to adopt the single box multi-GPU methodology to compute average gradients in single box first and then sync up across multiple nodes? This way it ease the I/O traffic a lot, which is always the bottleneck in data parallelism.</p>", "body_text": "After playing with the current distributed training implementation for a while, I think it views each GPU as a separate worker.However, It is common now to have 2~4 GPUs in one box. Isn't it better to adopt the single box multi-GPU methodology to compute average gradients in single box first and then sync up across multiple nodes? This way it ease the I/O traffic a lot, which is always the bottleneck in data parallelism.", "body": "After playing with the current distributed training implementation for a while, I think it views each GPU as a separate worker.However, It is common now to have 2~4 GPUs in one box. Isn't it better to adopt the single box multi-GPU methodology to compute average gradients in single box first and then sync up across multiple nodes? This way it ease the I/O traffic a lot, which is always the bottleneck in data parallelism.  \n"}