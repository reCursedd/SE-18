{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1727", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1727/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1727/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1727/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1727", "id": 144958009, "node_id": "MDU6SXNzdWUxNDQ5NTgwMDk=", "number": 1727, "title": "GPU resources not released when session is closed", "user": {"login": "gustavla", "id": 902935, "node_id": "MDQ6VXNlcjkwMjkzNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/902935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gustavla", "html_url": "https://github.com/gustavla", "followers_url": "https://api.github.com/users/gustavla/followers", "following_url": "https://api.github.com/users/gustavla/following{/other_user}", "gists_url": "https://api.github.com/users/gustavla/gists{/gist_id}", "starred_url": "https://api.github.com/users/gustavla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gustavla/subscriptions", "organizations_url": "https://api.github.com/users/gustavla/orgs", "repos_url": "https://api.github.com/users/gustavla/repos", "events_url": "https://api.github.com/users/gustavla/events{/privacy}", "received_events_url": "https://api.github.com/users/gustavla/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2016-03-31T17:01:32Z", "updated_at": "2017-05-03T00:40:17Z", "closed_at": "2017-05-03T00:40:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As I understand from the documentation, running <code>sess.close()</code> is supposed to release the resources, but it doesn't. I have been running the following test:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n        matrix1 <span class=\"pl-k\">=</span> tf.constant([[<span class=\"pl-c1\">3</span>., <span class=\"pl-c1\">3</span>]])\n        matrix2 <span class=\"pl-k\">=</span> tf.constant([[<span class=\"pl-c1\">2</span>.], [<span class=\"pl-c1\">2</span>.]])\n        product <span class=\"pl-k\">=</span> tf.matmul(matrix1, matrix2)\n        result <span class=\"pl-k\">=</span> sess.run(product)\n        <span class=\"pl-c1\">print</span>(result)</pre></div>\n<p>This allocates all the free memory of gpu0, but it is not released when <code>sess</code> is closed (both using a context manager as in the code above, but also when calling <code>sess.close()</code> manually). The memory usage persists until that Python process is terminated. The way I have been checking memory usage is through <code>nvidia-smi</code>, but I have also confirmed that other processes can't allocate that GPU memory until the process terminates (not the session closes). I would like to be able to free the resources and still keep the Python process running.</p>\n<h3>Environment info</h3>\n<p>I am running a 64-bit Linux (CentOS) with a computer that has two Tesla K40c (driver 346.46, CUDA 7.0). I installed the 0.7.1 tensorflow for Linux and Python 3.4 through pip. The output of <code>tf.__version__</code> is 0.7.1.</p>\n<h3>Steps to reproduce</h3>\n<p>Simply running the code above should according to the document allocate and then release the memory. However, the GPU memory is still allocated and thus unusable by other processes. However, it can be re-used by the same Python process, meaning that I can re-run the snippet over and over as long as I do it from the same Python process.</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>Here is a log of the session. At the end, the memory is still allocated. Note that another user is connected to both GPUs through Torch7, and is actively using gpu0.</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nI tensorflow<span class=\"pl-k\">/</span>stream_executor<span class=\"pl-k\">/</span>dso_loader.cc:<span class=\"pl-c1\">105</span>] successfully opened <span class=\"pl-c1\">CUDA</span> library libcublas.so locally\nI tensorflow<span class=\"pl-k\">/</span>stream_executor<span class=\"pl-k\">/</span>dso_loader.cc:<span class=\"pl-c1\">105</span>] successfully opened <span class=\"pl-c1\">CUDA</span> library libcudnn.so locally\nI tensorflow<span class=\"pl-k\">/</span>stream_executor<span class=\"pl-k\">/</span>dso_loader.cc:<span class=\"pl-c1\">105</span>] successfully opened <span class=\"pl-c1\">CUDA</span> library libcufft.so locally\nI tensorflow<span class=\"pl-k\">/</span>stream_executor<span class=\"pl-k\">/</span>dso_loader.cc:<span class=\"pl-c1\">105</span>] successfully opened <span class=\"pl-c1\">CUDA</span> library libcuda.so.1 locally\nI tensorflow<span class=\"pl-k\">/</span>stream_executor<span class=\"pl-k\">/</span>dso_loader.cc:<span class=\"pl-c1\">105</span>] successfully opened <span class=\"pl-c1\">CUDA</span> library libcurand.so locally\n\nIn [<span class=\"pl-c1\">2</span>]: <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:1<span class=\"pl-pds\">'</span></span>):\n        matrix1 <span class=\"pl-k\">=</span> tf.constant([[<span class=\"pl-c1\">3</span>., <span class=\"pl-c1\">3</span>]])\n        matrix2 <span class=\"pl-k\">=</span> tf.constant([[<span class=\"pl-c1\">2</span>.], [<span class=\"pl-c1\">2</span>.]])\n        product <span class=\"pl-k\">=</span> tf.matmul(matrix1, matrix2)\n        result <span class=\"pl-k\">=</span> sess.run(product)\n        <span class=\"pl-c1\">print</span>(result)\n   <span class=\"pl-c1\">...</span>:\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_init.cc:<span class=\"pl-c1\">102</span>] Found device <span class=\"pl-c1\">0</span> <span class=\"pl-k\">with</span> properties:\nname: Tesla K40c\nmajor: <span class=\"pl-c1\">3</span> minor: <span class=\"pl-c1\">5</span> memoryClockRate (GHz) <span class=\"pl-c1\">0.745</span>\npciBusID <span class=\"pl-c1\">0000</span>:<span class=\"pl-c1\">0<span class=\"pl-ii\">3</span></span>:<span class=\"pl-c1\">00.0</span>\nTotal memory: <span class=\"pl-c1\">11.</span><span class=\"pl-ii\">25GiB</span>\nFree memory: <span class=\"pl-c1\">3.</span><span class=\"pl-ii\">27GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_init.cc:<span class=\"pl-c1\">102</span>] Found device <span class=\"pl-c1\">1</span> <span class=\"pl-k\">with</span> properties:\nname: Tesla K40c\nmajor: <span class=\"pl-c1\">3</span> minor: <span class=\"pl-c1\">5</span> memoryClockRate (GHz) <span class=\"pl-c1\">0.745</span>\npciBusID <span class=\"pl-c1\">0000</span>:<span class=\"pl-c1\">82</span>:<span class=\"pl-c1\">00.0</span>\nTotal memory: <span class=\"pl-c1\">11.</span><span class=\"pl-ii\">25GiB</span>\nFree memory: <span class=\"pl-c1\">11.</span><span class=\"pl-ii\">05GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_init.cc:<span class=\"pl-c1\">59</span>] cannot enable peer access <span class=\"pl-k\">from</span> device ordinal <span class=\"pl-c1\">0</span> to device ordinal <span class=\"pl-c1\">1</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_init.cc:<span class=\"pl-c1\">59</span>] cannot enable peer access <span class=\"pl-k\">from</span> device ordinal <span class=\"pl-c1\">1</span> to device ordinal <span class=\"pl-c1\">0</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_init.cc:<span class=\"pl-c1\">126</span>] <span class=\"pl-c1\">DMA</span>: <span class=\"pl-c1\">0</span> <span class=\"pl-c1\">1</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_init.cc:<span class=\"pl-c1\">136</span>] <span class=\"pl-c1\">0</span>:   Y N\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_init.cc:<span class=\"pl-c1\">136</span>] <span class=\"pl-c1\">1</span>:   N Y\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_device.cc:<span class=\"pl-c1\">717</span>] Creating TensorFlow device (<span class=\"pl-k\">/</span>gpu:<span class=\"pl-c1\">0</span>) <span class=\"pl-ii\">-&gt;</span> (device: <span class=\"pl-c1\">0</span>, name: Tesla K40c, pci bus <span class=\"pl-c1\">id</span>: <span class=\"pl-c1\">0000</span>:<span class=\"pl-c1\">0<span class=\"pl-ii\">3</span></span>:<span class=\"pl-c1\">00.0</span>)\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_device.cc:<span class=\"pl-c1\">717</span>] Creating TensorFlow device (<span class=\"pl-k\">/</span>gpu:<span class=\"pl-c1\">1</span>) <span class=\"pl-ii\">-&gt;</span> (device: <span class=\"pl-c1\">1</span>, name: Tesla K40c, pci bus <span class=\"pl-c1\">id</span>: <span class=\"pl-c1\">0000</span>:<span class=\"pl-c1\">82</span>:<span class=\"pl-c1\">00.0</span>)\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">1.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">2.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">4.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">8.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">16.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">32.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">64.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">128.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">256.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">512.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">1.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">2.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">4.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">8.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">16.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">32.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">64.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">128.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">256.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">512.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">1.</span><span class=\"pl-ii\">00GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">2.</span><span class=\"pl-ii\">00GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">4.</span><span class=\"pl-ii\">00GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">1.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">2.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">4.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">8.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">16.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">32.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">64.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">128.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">256.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">512.</span><span class=\"pl-ii\">0KiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">1.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">2.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">4.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">8.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">16.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">32.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">64.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">128.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">256.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">512.</span><span class=\"pl-ii\">00MiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">1.</span><span class=\"pl-ii\">00GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">2.</span><span class=\"pl-ii\">00GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">4.</span><span class=\"pl-ii\">00GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">8.</span><span class=\"pl-ii\">00GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">51</span>] Creating <span class=\"pl-c1\">bin</span> of <span class=\"pl-c1\">max</span> chunk size <span class=\"pl-c1\">16.</span><span class=\"pl-ii\">00GiB</span>\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">73</span>] Allocating <span class=\"pl-c1\">10.</span><span class=\"pl-ii\">50GiB</span> <span class=\"pl-c1\">bytes</span>.\nI tensorflow<span class=\"pl-k\">/</span>core<span class=\"pl-k\">/</span>common_runtime<span class=\"pl-k\">/</span>gpu<span class=\"pl-k\">/</span>gpu_bfc_allocator.cc:<span class=\"pl-c1\">83</span>] <span class=\"pl-c1\">GPU</span> <span class=\"pl-c1\">1</span> memory begins at <span class=\"pl-c1\"><span class=\"pl-k\">0x</span>4208f40000</span> extends to <span class=\"pl-c1\"><span class=\"pl-k\">0x</span>44a8b030cd</span>\n[[ <span class=\"pl-c1\">12</span>.]]\n\nIn [<span class=\"pl-c1\">3</span>]: </pre></div>", "body_text": "As I understand from the documentation, running sess.close() is supposed to release the resources, but it doesn't. I have been running the following test:\nwith tf.Session() as sess:\n    with tf.device('/gpu:0'):\n        matrix1 = tf.constant([[3., 3]])\n        matrix2 = tf.constant([[2.], [2.]])\n        product = tf.matmul(matrix1, matrix2)\n        result = sess.run(product)\n        print(result)\nThis allocates all the free memory of gpu0, but it is not released when sess is closed (both using a context manager as in the code above, but also when calling sess.close() manually). The memory usage persists until that Python process is terminated. The way I have been checking memory usage is through nvidia-smi, but I have also confirmed that other processes can't allocate that GPU memory until the process terminates (not the session closes). I would like to be able to free the resources and still keep the Python process running.\nEnvironment info\nI am running a 64-bit Linux (CentOS) with a computer that has two Tesla K40c (driver 346.46, CUDA 7.0). I installed the 0.7.1 tensorflow for Linux and Python 3.4 through pip. The output of tf.__version__ is 0.7.1.\nSteps to reproduce\nSimply running the code above should according to the document allocate and then release the memory. However, the GPU memory is still allocated and thus unusable by other processes. However, it can be re-used by the same Python process, meaning that I can re-run the snippet over and over as long as I do it from the same Python process.\nLogs or other output that would be helpful\nHere is a log of the session. At the end, the memory is still allocated. Note that another user is connected to both GPUs through Torch7, and is actively using gpu0.\nIn [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n\nIn [2]: with tf.Session() as sess:\n    with tf.device('/gpu:1'):\n        matrix1 = tf.constant([[3., 3]])\n        matrix2 = tf.constant([[2.], [2.]])\n        product = tf.matmul(matrix1, matrix2)\n        result = sess.run(product)\n        print(result)\n   ...:\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:03:00.0\nTotal memory: 11.25GiB\nFree memory: 3.27GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:82:00.0\nTotal memory: 11.25GiB\nFree memory: 11.05GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:82:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.50GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 1 memory begins at 0x4208f40000 extends to 0x44a8b030cd\n[[ 12.]]\n\nIn [3]:", "body": "As I understand from the documentation, running `sess.close()` is supposed to release the resources, but it doesn't. I have been running the following test:\n\n``` python\nwith tf.Session() as sess:\n    with tf.device('/gpu:0'):\n        matrix1 = tf.constant([[3., 3]])\n        matrix2 = tf.constant([[2.], [2.]])\n        product = tf.matmul(matrix1, matrix2)\n        result = sess.run(product)\n        print(result)\n```\n\nThis allocates all the free memory of gpu0, but it is not released when `sess` is closed (both using a context manager as in the code above, but also when calling `sess.close()` manually). The memory usage persists until that Python process is terminated. The way I have been checking memory usage is through `nvidia-smi`, but I have also confirmed that other processes can't allocate that GPU memory until the process terminates (not the session closes). I would like to be able to free the resources and still keep the Python process running.\n### Environment info\n\nI am running a 64-bit Linux (CentOS) with a computer that has two Tesla K40c (driver 346.46, CUDA 7.0). I installed the 0.7.1 tensorflow for Linux and Python 3.4 through pip. The output of `tf.__version__` is 0.7.1.\n### Steps to reproduce\n\nSimply running the code above should according to the document allocate and then release the memory. However, the GPU memory is still allocated and thus unusable by other processes. However, it can be re-used by the same Python process, meaning that I can re-run the snippet over and over as long as I do it from the same Python process.\n### Logs or other output that would be helpful\n\nHere is a log of the session. At the end, the memory is still allocated. Note that another user is connected to both GPUs through Torch7, and is actively using gpu0.\n\n``` python\nIn [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n\nIn [2]: with tf.Session() as sess:\n    with tf.device('/gpu:1'):\n        matrix1 = tf.constant([[3., 3]])\n        matrix2 = tf.constant([[2.], [2.]])\n        product = tf.matmul(matrix1, matrix2)\n        result = sess.run(product)\n        print(result)\n   ...:\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:03:00.0\nTotal memory: 11.25GiB\nFree memory: 3.27GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:82:00.0\nTotal memory: 11.25GiB\nFree memory: 11.05GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:82:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.50GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 1 memory begins at 0x4208f40000 extends to 0x44a8b030cd\n[[ 12.]]\n\nIn [3]: \n```\n"}