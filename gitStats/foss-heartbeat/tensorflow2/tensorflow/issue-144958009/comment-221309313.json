{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/221309313", "html_url": "https://github.com/tensorflow/tensorflow/issues/1727#issuecomment-221309313", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1727", "id": 221309313, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMTMwOTMxMw==", "user": {"login": "sytham", "id": 5081109, "node_id": "MDQ6VXNlcjUwODExMDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/5081109?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sytham", "html_url": "https://github.com/sytham", "followers_url": "https://api.github.com/users/sytham/followers", "following_url": "https://api.github.com/users/sytham/following{/other_user}", "gists_url": "https://api.github.com/users/sytham/gists{/gist_id}", "starred_url": "https://api.github.com/users/sytham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sytham/subscriptions", "organizations_url": "https://api.github.com/users/sytham/orgs", "repos_url": "https://api.github.com/users/sytham/repos", "events_url": "https://api.github.com/users/sytham/events{/privacy}", "received_events_url": "https://api.github.com/users/sytham/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-24T15:31:08Z", "updated_at": "2016-05-24T17:42:49Z", "author_association": "NONE", "body_html": "<p>Same issue here with version 0.8.0rc0. In fact, the GPU memory isn't even released after shutting down the Python kernel. Running 64-bit Linux (CentOS) with 4 nVidia GRID K520 GPUs, Python 2.7.</p>\n<p>In lieue of fixing the issue, a quick workaround could be to allow the user to free up the memory explicitly (of course fixing the issue would be preferable)</p>\n<p>EDIT: this seems to only happen on GPU with ID 0. If I mask available GPUs through the env var CUDA_VISIBLE_DEVICES to not include 0, then all appears to go fine</p>", "body_text": "Same issue here with version 0.8.0rc0. In fact, the GPU memory isn't even released after shutting down the Python kernel. Running 64-bit Linux (CentOS) with 4 nVidia GRID K520 GPUs, Python 2.7.\nIn lieue of fixing the issue, a quick workaround could be to allow the user to free up the memory explicitly (of course fixing the issue would be preferable)\nEDIT: this seems to only happen on GPU with ID 0. If I mask available GPUs through the env var CUDA_VISIBLE_DEVICES to not include 0, then all appears to go fine", "body": "Same issue here with version 0.8.0rc0. In fact, the GPU memory isn't even released after shutting down the Python kernel. Running 64-bit Linux (CentOS) with 4 nVidia GRID K520 GPUs, Python 2.7.\n\nIn lieue of fixing the issue, a quick workaround could be to allow the user to free up the memory explicitly (of course fixing the issue would be preferable)\n\nEDIT: this seems to only happen on GPU with ID 0. If I mask available GPUs through the env var CUDA_VISIBLE_DEVICES to not include 0, then all appears to go fine\n"}