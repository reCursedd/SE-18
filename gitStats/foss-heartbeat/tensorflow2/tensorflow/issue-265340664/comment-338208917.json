{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/338208917", "html_url": "https://github.com/tensorflow/tensorflow/issues/13692#issuecomment-338208917", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13692", "id": 338208917, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODIwODkxNw==", "user": {"login": "bshao001", "id": 26190682, "node_id": "MDQ6VXNlcjI2MTkwNjgy", "avatar_url": "https://avatars2.githubusercontent.com/u/26190682?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bshao001", "html_url": "https://github.com/bshao001", "followers_url": "https://api.github.com/users/bshao001/followers", "following_url": "https://api.github.com/users/bshao001/following{/other_user}", "gists_url": "https://api.github.com/users/bshao001/gists{/gist_id}", "starred_url": "https://api.github.com/users/bshao001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bshao001/subscriptions", "organizations_url": "https://api.github.com/users/bshao001/orgs", "repos_url": "https://api.github.com/users/bshao001/repos", "events_url": "https://api.github.com/users/bshao001/events{/privacy}", "received_events_url": "https://api.github.com/users/bshao001/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-20T13:37:26Z", "updated_at": "2017-10-20T13:37:26Z", "author_association": "NONE", "body_html": "<p>If you want to use model parallelism, refer to the TensorFlow NMT model tutorial. However, based on my limited test, separating a seq2seq model (NMT model) in two layers to 2 GPUs does not speed up the training at all. It actually makes the training slightly slower. Running a 4-layer model might help using that. However, I could not see any capacity increase by increasing the number of layers (you can clearly see the capacity increase when you increase the number of units in each layer). Therefore, data parallelism is still requested by many users like me. I have one issue open here: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"264232603\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/nmt/issues/141\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/nmt/issues/141/hovercard\" href=\"https://github.com/tensorflow/nmt/issues/141\">tensorflow/nmt#141</a>, but was never replied there.</p>\n<p>Another interesting fact I would like to mention here is: If you only have 2 GPUs in one machine, you do not need to try any so-called parallelism, because setting colocate_gradients_with_ops in function tf.gradients to false cuts down the training time to half, while this requires to be true for any parallelism using TensorFlow. I am not sure why that makes such a big difference.</p>", "body_text": "If you want to use model parallelism, refer to the TensorFlow NMT model tutorial. However, based on my limited test, separating a seq2seq model (NMT model) in two layers to 2 GPUs does not speed up the training at all. It actually makes the training slightly slower. Running a 4-layer model might help using that. However, I could not see any capacity increase by increasing the number of layers (you can clearly see the capacity increase when you increase the number of units in each layer). Therefore, data parallelism is still requested by many users like me. I have one issue open here: tensorflow/nmt#141, but was never replied there.\nAnother interesting fact I would like to mention here is: If you only have 2 GPUs in one machine, you do not need to try any so-called parallelism, because setting colocate_gradients_with_ops in function tf.gradients to false cuts down the training time to half, while this requires to be true for any parallelism using TensorFlow. I am not sure why that makes such a big difference.", "body": "If you want to use model parallelism, refer to the TensorFlow NMT model tutorial. However, based on my limited test, separating a seq2seq model (NMT model) in two layers to 2 GPUs does not speed up the training at all. It actually makes the training slightly slower. Running a 4-layer model might help using that. However, I could not see any capacity increase by increasing the number of layers (you can clearly see the capacity increase when you increase the number of units in each layer). Therefore, data parallelism is still requested by many users like me. I have one issue open here: https://github.com/tensorflow/nmt/issues/141, but was never replied there.\r\n\r\nAnother interesting fact I would like to mention here is: If you only have 2 GPUs in one machine, you do not need to try any so-called parallelism, because setting colocate_gradients_with_ops in function tf.gradients to false cuts down the training time to half, while this requires to be true for any parallelism using TensorFlow. I am not sure why that makes such a big difference. "}