{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/337291624", "html_url": "https://github.com/tensorflow/tensorflow/issues/13740#issuecomment-337291624", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13740", "id": 337291624, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzI5MTYyNA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-17T16:40:01Z", "updated_at": "2017-10-17T16:40:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This part of the stack doesn't make sense:</p>\n<pre><code>07 000000c2`28bedf50 00007fff`acec6617 nvcuda!cuTexRefSetAddress+0x13d439\n08 000000c2`28bedf80 00007fff`8d0e1b50 nvcuda!cuTexRefSetAddress+0x1a8fd9\n09 000000c2`28bee040 00007fff`8d0e185e _pywrap_tensorflow_internal!perftools::gputools::port::InternalError+0xe0\n0a 000000c2`28bee250 00007fff`8d0eebd4 _pywrap_tensorflow_internal!perftools::gputools::cuda::CUDADriver::Init+0x10e\n0b 000000c2`28bee2a0 00007fff`8b6225d0 _pywrap_tensorflow_internal!perftools::gputools::cuda::CudaPlatform::VisibleDeviceCount+0x14\n0c 000000c2`28bee2d0 00007fff`8b62043b _pywrap_tensorflow_internal!tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds+0x120\n</code></pre>\n<p>...because the <a href=\"https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/stream_executor/lib/status.h#L40\"><code>port::InternalError()</code></a> function doesn't call any CUDA APIs.</p>\n<p>However, the presence of <code>InternalError()</code> on the stack makes it look like <a href=\"https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/stream_executor/cuda/cuda_driver.cc#L414\"><code>CUDADriver::Init()</code></a> is failing for some reason. Strangely, from reading the code, it seems like that particular call never fails with <code>port::InternalError()</code>, so I'm doubly surprised.</p>\n<p>Since it looks like you're comfortable using Windbg, would you be able to set a breakpoint on <code>CUDADriver::Init()</code> and step through it to see if there's any more information about the failure?</p>", "body_text": "This part of the stack doesn't make sense:\n07 000000c2`28bedf50 00007fff`acec6617 nvcuda!cuTexRefSetAddress+0x13d439\n08 000000c2`28bedf80 00007fff`8d0e1b50 nvcuda!cuTexRefSetAddress+0x1a8fd9\n09 000000c2`28bee040 00007fff`8d0e185e _pywrap_tensorflow_internal!perftools::gputools::port::InternalError+0xe0\n0a 000000c2`28bee250 00007fff`8d0eebd4 _pywrap_tensorflow_internal!perftools::gputools::cuda::CUDADriver::Init+0x10e\n0b 000000c2`28bee2a0 00007fff`8b6225d0 _pywrap_tensorflow_internal!perftools::gputools::cuda::CudaPlatform::VisibleDeviceCount+0x14\n0c 000000c2`28bee2d0 00007fff`8b62043b _pywrap_tensorflow_internal!tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds+0x120\n\n...because the port::InternalError() function doesn't call any CUDA APIs.\nHowever, the presence of InternalError() on the stack makes it look like CUDADriver::Init() is failing for some reason. Strangely, from reading the code, it seems like that particular call never fails with port::InternalError(), so I'm doubly surprised.\nSince it looks like you're comfortable using Windbg, would you be able to set a breakpoint on CUDADriver::Init() and step through it to see if there's any more information about the failure?", "body": "This part of the stack doesn't make sense:\r\n\r\n```\r\n07 000000c2`28bedf50 00007fff`acec6617 nvcuda!cuTexRefSetAddress+0x13d439\r\n08 000000c2`28bedf80 00007fff`8d0e1b50 nvcuda!cuTexRefSetAddress+0x1a8fd9\r\n09 000000c2`28bee040 00007fff`8d0e185e _pywrap_tensorflow_internal!perftools::gputools::port::InternalError+0xe0\r\n0a 000000c2`28bee250 00007fff`8d0eebd4 _pywrap_tensorflow_internal!perftools::gputools::cuda::CUDADriver::Init+0x10e\r\n0b 000000c2`28bee2a0 00007fff`8b6225d0 _pywrap_tensorflow_internal!perftools::gputools::cuda::CudaPlatform::VisibleDeviceCount+0x14\r\n0c 000000c2`28bee2d0 00007fff`8b62043b _pywrap_tensorflow_internal!tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds+0x120\r\n```\r\n\r\n...because the [`port::InternalError()`](https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/stream_executor/lib/status.h#L40) function doesn't call any CUDA APIs. \r\n\r\nHowever, the presence of `InternalError()` on the stack makes it look like [`CUDADriver::Init()`](https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/stream_executor/cuda/cuda_driver.cc#L414) is failing for some reason. Strangely, from reading the code, it seems like that particular call never fails with `port::InternalError()`, so I'm doubly surprised. \r\n\r\nSince it looks like you're comfortable using Windbg, would you be able to set a breakpoint on `CUDADriver::Init()` and step through it to see if there's any more information about the failure?"}