{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341618820", "html_url": "https://github.com/tensorflow/tensorflow/issues/6620#issuecomment-341618820", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6620", "id": 341618820, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTYxODgyMA==", "user": {"login": "protoget", "id": 5117188, "node_id": "MDQ6VXNlcjUxMTcxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5117188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/protoget", "html_url": "https://github.com/protoget", "followers_url": "https://api.github.com/users/protoget/followers", "following_url": "https://api.github.com/users/protoget/following{/other_user}", "gists_url": "https://api.github.com/users/protoget/gists{/gist_id}", "starred_url": "https://api.github.com/users/protoget/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/protoget/subscriptions", "organizations_url": "https://api.github.com/users/protoget/orgs", "repos_url": "https://api.github.com/users/protoget/repos", "events_url": "https://api.github.com/users/protoget/events{/privacy}", "received_events_url": "https://api.github.com/users/protoget/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-03T04:06:40Z", "updated_at": "2017-11-03T04:07:06Z", "author_association": "MEMBER", "body_html": "<p>AdamOptimizer should work with the new Cudnn <strong>layer API</strong> in cudnn_rnn.py</p>\n<p>We have a CL to switch tf.contrib.cudnn_rnn.CudnnLSTM to point to classes in cudnn_rnn.py instead of cudnn_rnn_ops.py, it's running a series of tests, should be submitted soon.</p>\n<p>The following should work with the new layer API.</p>\n<div class=\"highlight highlight-source-python\"><pre>  num_layers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n  num_units <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n  batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\n  dir_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n\n  inputs <span class=\"pl-k\">=</span> tf.random_uniform([\n      num_layers <span class=\"pl-k\">*</span> dir_count, batch_size, num_units], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n  lstm <span class=\"pl-k\">=</span> cudnn_rnn.CudnnLSTM(num_layers, num_units,\n                             <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>awesome_lstm<span class=\"pl-pds\">\"</span></span>)\n\n  outputs, _ <span class=\"pl-k\">=</span> lstm(inputs)\n  loss <span class=\"pl-k\">=</span> tf.reduce_sum(outputs)\n  var <span class=\"pl-k\">=</span> lstm.trainable_variables[<span class=\"pl-c1\">0</span>]\n\n  grad <span class=\"pl-k\">=</span> tf.gradients(loss, var)[<span class=\"pl-c1\">0</span>]\n  <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>grad.shape: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> grad.shape)\n  <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>var.shape: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> var.shape)\n\n  opt <span class=\"pl-k\">=</span> tf.train.AdamOptimizer()\n  train_op <span class=\"pl-k\">=</span> opt.apply_gradients([(grad, lstm.trainable_variables[<span class=\"pl-c1\">0</span>])])\n\n  <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n    <span class=\"pl-c1\">print</span>(sess.run(outputs))\n    sess.run(train_op)\n    <span class=\"pl-c1\">print</span>(sess.run(outputs))</pre></div>", "body_text": "AdamOptimizer should work with the new Cudnn layer API in cudnn_rnn.py\nWe have a CL to switch tf.contrib.cudnn_rnn.CudnnLSTM to point to classes in cudnn_rnn.py instead of cudnn_rnn_ops.py, it's running a series of tests, should be submitted soon.\nThe following should work with the new layer API.\n  num_layers = 4\n  num_units = 2\n  batch_size = 8\n  dir_count = 1\n\n  inputs = tf.random_uniform([\n      num_layers * dir_count, batch_size, num_units], dtype=tf.float32)\n\n  lstm = cudnn_rnn.CudnnLSTM(num_layers, num_units,\n                             name=\"awesome_lstm\")\n\n  outputs, _ = lstm(inputs)\n  loss = tf.reduce_sum(outputs)\n  var = lstm.trainable_variables[0]\n\n  grad = tf.gradients(loss, var)[0]\n  print('grad.shape: %s' % grad.shape)\n  print('var.shape: %s' % var.shape)\n\n  opt = tf.train.AdamOptimizer()\n  train_op = opt.apply_gradients([(grad, lstm.trainable_variables[0])])\n\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(outputs))\n    sess.run(train_op)\n    print(sess.run(outputs))", "body": "AdamOptimizer should work with the new Cudnn **layer API** in cudnn_rnn.py\r\n\r\nWe have a CL to switch tf.contrib.cudnn_rnn.CudnnLSTM to point to classes in cudnn_rnn.py instead of cudnn_rnn_ops.py, it's running a series of tests, should be submitted soon.\r\n\r\nThe following should work with the new layer API.\r\n```py\r\n  num_layers = 4\r\n  num_units = 2\r\n  batch_size = 8\r\n  dir_count = 1\r\n\r\n  inputs = tf.random_uniform([\r\n      num_layers * dir_count, batch_size, num_units], dtype=tf.float32)\r\n\r\n  lstm = cudnn_rnn.CudnnLSTM(num_layers, num_units,\r\n                             name=\"awesome_lstm\")\r\n\r\n  outputs, _ = lstm(inputs)\r\n  loss = tf.reduce_sum(outputs)\r\n  var = lstm.trainable_variables[0]\r\n\r\n  grad = tf.gradients(loss, var)[0]\r\n  print('grad.shape: %s' % grad.shape)\r\n  print('var.shape: %s' % var.shape)\r\n\r\n  opt = tf.train.AdamOptimizer()\r\n  train_op = opt.apply_gradients([(grad, lstm.trainable_variables[0])])\r\n\r\n  with tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(outputs))\r\n    sess.run(train_op)\r\n    print(sess.run(outputs))\r\n```"}