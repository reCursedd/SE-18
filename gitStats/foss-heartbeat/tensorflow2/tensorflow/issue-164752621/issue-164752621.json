{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3265", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3265/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3265/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3265/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3265", "id": 164752621, "node_id": "MDU6SXNzdWUxNjQ3NTI2MjE=", "number": 3265, "title": "Change is_training of tf.contrib.layers.batch_norm to conditional function", "user": {"login": "carpedm20", "id": 3346407, "node_id": "MDQ6VXNlcjMzNDY0MDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3346407?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carpedm20", "html_url": "https://github.com/carpedm20", "followers_url": "https://api.github.com/users/carpedm20/followers", "following_url": "https://api.github.com/users/carpedm20/following{/other_user}", "gists_url": "https://api.github.com/users/carpedm20/gists{/gist_id}", "starred_url": "https://api.github.com/users/carpedm20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carpedm20/subscriptions", "organizations_url": "https://api.github.com/users/carpedm20/orgs", "repos_url": "https://api.github.com/users/carpedm20/repos", "events_url": "https://api.github.com/users/carpedm20/events{/privacy}", "received_events_url": "https://api.github.com/users/carpedm20/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sguada", "id": 1766524, "node_id": "MDQ6VXNlcjE3NjY1MjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1766524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sguada", "html_url": "https://github.com/sguada", "followers_url": "https://api.github.com/users/sguada/followers", "following_url": "https://api.github.com/users/sguada/following{/other_user}", "gists_url": "https://api.github.com/users/sguada/gists{/gist_id}", "starred_url": "https://api.github.com/users/sguada/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sguada/subscriptions", "organizations_url": "https://api.github.com/users/sguada/orgs", "repos_url": "https://api.github.com/users/sguada/repos", "events_url": "https://api.github.com/users/sguada/events{/privacy}", "received_events_url": "https://api.github.com/users/sguada/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-07-11T03:12:31Z", "updated_at": "2016-07-29T01:46:41Z", "closed_at": "2016-07-27T20:14:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p><code>tf.contrib.layers.batch_norm</code> is using <code>is_training</code> as a python boolean variable so I may have to define two different ops which share its variables by using <code>reuse=True</code> to the second op.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L209\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L209</a></p>\n<p>However, I think it is better to use <code>tf.placeholder</code> or other tensor variables as <code>is_training</code> and use <code>tf.cond</code> to dynamically change training and testing phase without defining two different ops. Or is there any better usage of using <code>tf.contrib.layers.batch_norm</code> that I couldn't think of? Also, is <code>tf.contrib.layers.fully_connected</code> ready to use <code>batch_norm</code> as a <code>normalizer_fn</code> (because I guess this is still in contrib for several months)?</p>", "body_text": "tf.contrib.layers.batch_norm is using is_training as a python boolean variable so I may have to define two different ops which share its variables by using reuse=True to the second op.\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L209\nHowever, I think it is better to use tf.placeholder or other tensor variables as is_training and use tf.cond to dynamically change training and testing phase without defining two different ops. Or is there any better usage of using tf.contrib.layers.batch_norm that I couldn't think of? Also, is tf.contrib.layers.fully_connected ready to use batch_norm as a normalizer_fn (because I guess this is still in contrib for several months)?", "body": "`tf.contrib.layers.batch_norm` is using `is_training` as a python boolean variable so I may have to define two different ops which share its variables by using `reuse=True` to the second op.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L209\n\nHowever, I think it is better to use `tf.placeholder` or other tensor variables as `is_training` and use `tf.cond` to dynamically change training and testing phase without defining two different ops. Or is there any better usage of using `tf.contrib.layers.batch_norm` that I couldn't think of? Also, is `tf.contrib.layers.fully_connected` ready to use `batch_norm` as a `normalizer_fn` (because I guess this is still in contrib for several months)?\n"}