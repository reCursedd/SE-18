{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/209749613", "html_url": "https://github.com/tensorflow/tensorflow/issues/1879#issuecomment-209749613", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1879", "id": 209749613, "node_id": "MDEyOklzc3VlQ29tbWVudDIwOTc0OTYxMw==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-14T03:56:26Z", "updated_at": "2016-04-14T03:56:26Z", "author_association": "MEMBER", "body_html": "<p>It's exactly as you say! I think it's a bit too strong to call it \"not real\" -- the reported perplexities come from teacher-forced examples, and so are indeed not fully representative for what you'll get during decoding. But some people report this kind of perplexities in their papers and they are often a good measure to judge how the training process is progressing. In particular, I think they are enough to determine if the model is overfitting or not and whether it is converging or not. You could create a second model with the same parameters and <code>forward_only=True</code> and use that for decoding, but it'd use a lot of memory, that's why it's not done by default.</p>\n<p>To get a final measure of the model quality, it's best to take a separate test set, decode it using the <code>decode</code> function [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py#L208](which uses <code>forward_only=True</code> of course) and then use any measure on the result, e.g. BLEU. I'd also recommend to use a test set that's disjoint from the dev set for that, as you might be tuning hyper-parameters on the dev set.</p>", "body_text": "It's exactly as you say! I think it's a bit too strong to call it \"not real\" -- the reported perplexities come from teacher-forced examples, and so are indeed not fully representative for what you'll get during decoding. But some people report this kind of perplexities in their papers and they are often a good measure to judge how the training process is progressing. In particular, I think they are enough to determine if the model is overfitting or not and whether it is converging or not. You could create a second model with the same parameters and forward_only=True and use that for decoding, but it'd use a lot of memory, that's why it's not done by default.\nTo get a final measure of the model quality, it's best to take a separate test set, decode it using the decode function [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py#L208](which uses forward_only=True of course) and then use any measure on the result, e.g. BLEU. I'd also recommend to use a test set that's disjoint from the dev set for that, as you might be tuning hyper-parameters on the dev set.", "body": "It's exactly as you say! I think it's a bit too strong to call it \"not real\" -- the reported perplexities come from teacher-forced examples, and so are indeed not fully representative for what you'll get during decoding. But some people report this kind of perplexities in their papers and they are often a good measure to judge how the training process is progressing. In particular, I think they are enough to determine if the model is overfitting or not and whether it is converging or not. You could create a second model with the same parameters and `forward_only=True` and use that for decoding, but it'd use a lot of memory, that's why it's not done by default.\n\nTo get a final measure of the model quality, it's best to take a separate test set, decode it using the `decode` function [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py#L208](which uses `forward_only=True` of course) and then use any measure on the result, e.g. BLEU. I'd also recommend to use a test set that's disjoint from the dev set for that, as you might be tuning hyper-parameters on the dev set.\n"}