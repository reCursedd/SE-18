{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4176", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4176/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4176/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4176/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4176", "id": 174798817, "node_id": "MDU6SXNzdWUxNzQ3OTg4MTc=", "number": 4176, "title": "Why reuse default to True for loop_function() in seq2seq.rnn_decoder ?", "user": {"login": "RobRomijnders", "id": 16174021, "node_id": "MDQ6VXNlcjE2MTc0MDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/16174021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RobRomijnders", "html_url": "https://github.com/RobRomijnders", "followers_url": "https://api.github.com/users/RobRomijnders/followers", "following_url": "https://api.github.com/users/RobRomijnders/following{/other_user}", "gists_url": "https://api.github.com/users/RobRomijnders/gists{/gist_id}", "starred_url": "https://api.github.com/users/RobRomijnders/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RobRomijnders/subscriptions", "organizations_url": "https://api.github.com/users/RobRomijnders/orgs", "repos_url": "https://api.github.com/users/RobRomijnders/repos", "events_url": "https://api.github.com/users/RobRomijnders/events{/privacy}", "received_events_url": "https://api.github.com/users/RobRomijnders/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-09-02T16:27:49Z", "updated_at": "2017-01-27T18:13:32Z", "closed_at": "2017-01-27T18:13:32Z", "author_association": "NONE", "body_html": "<p>My question concerns the rnn_decoder() in seq2seq.</p>\n<p>I am batchnormalizing activation maps in the loop_function(). To this end, I call the tf.contrib.layers.batch_norm() function. Like many other optionalities, it calls variables using tf.get_variable().</p>\n<p>Ideally, you'd set reuse=None during the first call of loop_function(). That way, tf.get_variable() functions can construct variables. During second and subsequent calls of loop_function() you set reuse=True.</p>\n<p>However, in rnn_decoder() reuse is set to True also during the first call. This doesn't allow the tf.get_variable() functions to construct variables.</p>\n<p>My question is: why is this so? Am I doing something wrong? Or what is the logic here?</p>\n<p>More specifically, my loop_function is:</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">loop_function</span>(<span class=\"pl-smi\">output</span>, <span class=\"pl-smi\">i</span>):\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span>#some code##</span>\n      some_variable <span class=\"pl-k\">=</span> tf.contrib.layers.batch_norm(some_variable,<span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.is_training)\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span>#some code##</span>\n      <span class=\"pl-k\">return</span> some_variable</pre></div>\n<p>I made a work-around, by defining my own rnn_decoder. I copy-paste the Tensorflow implementation and set reuse=None during the first call</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">rnn_decoder_rob</span>(<span class=\"pl-smi\">decoder_inputs</span>, <span class=\"pl-smi\">initial_state</span>, <span class=\"pl-smi\">cell</span>, <span class=\"pl-smi\">loop_function</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-c1\">REUSE</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>\n  <span class=\"pl-k\">with</span> variable_scope.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rnn_decoder<span class=\"pl-pds\">\"</span></span>):\n    state <span class=\"pl-k\">=</span> initial_state\n    outputs <span class=\"pl-k\">=</span> []\n    prev <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    <span class=\"pl-k\">for</span> i, inp <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(decoder_inputs):\n      <span class=\"pl-k\">if</span> loop_function <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> prev <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-k\">with</span> variable_scope.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loop_function<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">REUSE</span>):\n          inp <span class=\"pl-k\">=</span> loop_function(prev, i)\n      <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n        variable_scope.get_variable_scope().reuse_variables()\n        <span class=\"pl-c1\">REUSE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n      output, state <span class=\"pl-k\">=</span> cell(inp, state)\n      outputs.append(output)\n      <span class=\"pl-k\">if</span> loop_function <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        prev <span class=\"pl-k\">=</span> output\n  <span class=\"pl-k\">return</span> outputs, state</pre></div>", "body_text": "My question concerns the rnn_decoder() in seq2seq.\nI am batchnormalizing activation maps in the loop_function(). To this end, I call the tf.contrib.layers.batch_norm() function. Like many other optionalities, it calls variables using tf.get_variable().\nIdeally, you'd set reuse=None during the first call of loop_function(). That way, tf.get_variable() functions can construct variables. During second and subsequent calls of loop_function() you set reuse=True.\nHowever, in rnn_decoder() reuse is set to True also during the first call. This doesn't allow the tf.get_variable() functions to construct variables.\nMy question is: why is this so? Am I doing something wrong? Or what is the logic here?\nMore specifically, my loop_function is:\n    def loop_function(output, i):\n      ##some code##\n      some_variable = tf.contrib.layers.batch_norm(some_variable,is_training= self.is_training)\n      ##some code##\n      return some_variable\nI made a work-around, by defining my own rnn_decoder. I copy-paste the Tensorflow implementation and set reuse=None during the first call\ndef rnn_decoder_rob(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  REUSE=None\n  with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n    state = initial_state\n    outputs = []\n    prev = None\n    for i, inp in enumerate(decoder_inputs):\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(\"loop_function\", reuse=REUSE):\n          inp = loop_function(prev, i)\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n        REUSE = True\n      output, state = cell(inp, state)\n      outputs.append(output)\n      if loop_function is not None:\n        prev = output\n  return outputs, state", "body": "My question concerns the rnn_decoder() in seq2seq.\n\nI am batchnormalizing activation maps in the loop_function(). To this end, I call the tf.contrib.layers.batch_norm() function. Like many other optionalities, it calls variables using tf.get_variable().\n\nIdeally, you'd set reuse=None during the first call of loop_function(). That way, tf.get_variable() functions can construct variables. During second and subsequent calls of loop_function() you set reuse=True.\n\nHowever, in rnn_decoder() reuse is set to True also during the first call. This doesn't allow the tf.get_variable() functions to construct variables. \n\nMy question is: why is this so? Am I doing something wrong? Or what is the logic here?\n\nMore specifically, my loop_function is:\n\n``` python\n    def loop_function(output, i):\n      ##some code##\n      some_variable = tf.contrib.layers.batch_norm(some_variable,is_training= self.is_training)\n      ##some code##\n      return some_variable\n```\n\nI made a work-around, by defining my own rnn_decoder. I copy-paste the Tensorflow implementation and set reuse=None during the first call\n\n``` python\ndef rnn_decoder_rob(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  REUSE=None\n  with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n    state = initial_state\n    outputs = []\n    prev = None\n    for i, inp in enumerate(decoder_inputs):\n      if loop_function is not None and prev is not None:\n        with variable_scope.variable_scope(\"loop_function\", reuse=REUSE):\n          inp = loop_function(prev, i)\n      if i > 0:\n        variable_scope.get_variable_scope().reuse_variables()\n        REUSE = True\n      output, state = cell(inp, state)\n      outputs.append(output)\n      if loop_function is not None:\n        prev = output\n  return outputs, state\n```\n"}