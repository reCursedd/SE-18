{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/380132521", "html_url": "https://github.com/tensorflow/tensorflow/issues/16004#issuecomment-380132521", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16004", "id": 380132521, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDEzMjUyMQ==", "user": {"login": "fatmausta", "id": 22526216, "node_id": "MDQ6VXNlcjIyNTI2MjE2", "avatar_url": "https://avatars3.githubusercontent.com/u/22526216?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fatmausta", "html_url": "https://github.com/fatmausta", "followers_url": "https://api.github.com/users/fatmausta/followers", "following_url": "https://api.github.com/users/fatmausta/following{/other_user}", "gists_url": "https://api.github.com/users/fatmausta/gists{/gist_id}", "starred_url": "https://api.github.com/users/fatmausta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fatmausta/subscriptions", "organizations_url": "https://api.github.com/users/fatmausta/orgs", "repos_url": "https://api.github.com/users/fatmausta/repos", "events_url": "https://api.github.com/users/fatmausta/events{/privacy}", "received_events_url": "https://api.github.com/users/fatmausta/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-10T14:59:03Z", "updated_at": "2018-04-10T15:25:07Z", "author_association": "NONE", "body_html": "<p>Hello,<br>\nI have the same error:<br>\nMy Python crushes when it gets to the first convolutional layer, I tried making the model simpler, still crushes.</p>\n<p>This is the output and error:</p>\n<p>I have the latest cuDNN version (7.1.2) On  tensorflow website, it seems cudnn 7 works with tensorflow-gpu 1.7..</p>\n<p>TensorFlow 1.7 may be the last time we support cuDNN versions below 6.0.<br>\nStarting with TensorFlow 1.8 release, 6.0 will be the minimum supported<br>\nversion. <a href=\"url\">here</a>: <a href=\"https://github.com/tensorflow/tensorflow/releases\">https://github.com/tensorflow/tensorflow/releases</a></p>\n<p>My purpose is to use cudnnLSTM instead of regular  LSTM (layer 2), because regular LSTM works so slow (it uses 10% of GPU or less)</p>\n<p>The code below works fine if I use tensorflow-gpu verison 1.1 and LSTM layer instead of  using cuDNNLSTM layer. cuDNNLSTM needs tensorflow-gpu not less than 1.2, so I updated my tensorflow-gpu to higher version (1.7.0) , but now, I have the following error. Any idea what is the issue?</p>\n<hr>\n<h1>Layer (type)                 [Output] Shape              Param #</h1>\n<p>input_1 (InputLayer)         (None, 100, 20, 20, 1)    0</p>\n<hr>\n<p>time_distributed_1 (TimeDist (None, 100, 16, 16, 20)   520</p>\n<hr>\n<p>time_distributed_2 (TimeDist (None, 100, 5120)         0</p>\n<hr>\n<h1>time_distributed_3 (TimeDist (None, 100, 1)            5121</h1>\n<p>Total params: 5,641<br>\nTrainable params: 5,641<br>\nNon-trainable params: 0</p>\n<hr>\n<p>Train on 30960 samples, validate on 7740 samples<br>\nEpoch 1/50<br>\n2018-04-10 10:40:15.085958: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2<br>\n2018-04-10 10:40:15.616355: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties:<br>\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335<br>\npciBusID: 0000:02:00.0<br>\ntotalMemory: 8.00GiB freeMemory: 6.64GiB<br>\n2018-04-10 10:40:15.616355: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0<br>\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:<br>\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0<br>\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N<br>\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6411 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)<br>\n2018-04-10 10:40:19.469530: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:396] Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7003 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.<br>\n2018-04-10 10:40:19.469530: F T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.cc:712] Check failed: stream-&gt;parent()-&gt;GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(), &amp;algorithms)</p>\n<hr>\n<p>I have the following settings<br>\nkeras                     2.1.5                    py36_0<br>\ntensorflow                1.7.0                    py36_0    aaronzs<br>\ntensorflow-gpu            1.7.0                    py36_0    aaronzs<br>\nWindows 7<br>\nCuda version 9.0<br>\ncudnn version 7 (cudnn64_7)<br>\nGPU NVIDIA GeForce GTX 1080 8 GB</p>\n<p>My code:<br>\n# Functional model definition<br>\ninputs = Input(shape=(seqLength, 20, 20, 1))  # layer[0]<br>\noutput_of_layer1 = TimeDistributed(Conv2D(20, (5, 5), activation=\"relu\"))(inputs)  # layer[1]<br>\noutput_of_layer2 = TimeDistributed(Bidirectional(CuDNNLSTM(100, activation=\"sigmoid\", return_sequences='true', unit_forget_bias = True)))(output_of_layer1 )# layer[2]<br>\noutput_of_layer3 = TimeDistributed(Flatten())(output_of_layer2)  # layer[3]<br>\noutput_of_layer4 = TimeDistributed(Dense(1, activation=\"sigmoid\"))(output_of_layer3)  # layer[4]<br>\nmodel = Model(inputs=inputs, outputs=output_of_layer8)</p>\n<pre><code>model.summary()\nnepochs = 50\n\nsgd = optimizers.SGD(lr=0.01, decay=0, momentum=0.95, nesterov=False, clipvalue=20)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\ncheckpoint = ModelCheckpoint('trained_plaqueDetection_model_bidirectional.h5', monitor='val_acc', verbose=1, save_best_only=False, mode='max')\ncallbacks_list = [checkpoint]\nhistory = model.fit(training_dataset[0], training_dataset[1], epochs = nepochs, batch_size=10, shuffle=True, verbose = 1, validation_split = 0.2, callbacks = callbacks_list)\n</code></pre>\n<p>plot_losses(history);</p>\n<p>Thanks!</p>", "body_text": "Hello,\nI have the same error:\nMy Python crushes when it gets to the first convolutional layer, I tried making the model simpler, still crushes.\nThis is the output and error:\nI have the latest cuDNN version (7.1.2) On  tensorflow website, it seems cudnn 7 works with tensorflow-gpu 1.7..\nTensorFlow 1.7 may be the last time we support cuDNN versions below 6.0.\nStarting with TensorFlow 1.8 release, 6.0 will be the minimum supported\nversion. here: https://github.com/tensorflow/tensorflow/releases\nMy purpose is to use cudnnLSTM instead of regular  LSTM (layer 2), because regular LSTM works so slow (it uses 10% of GPU or less)\nThe code below works fine if I use tensorflow-gpu verison 1.1 and LSTM layer instead of  using cuDNNLSTM layer. cuDNNLSTM needs tensorflow-gpu not less than 1.2, so I updated my tensorflow-gpu to higher version (1.7.0) , but now, I have the following error. Any idea what is the issue?\n\nLayer (type)                 [Output] Shape              Param #\ninput_1 (InputLayer)         (None, 100, 20, 20, 1)    0\n\ntime_distributed_1 (TimeDist (None, 100, 16, 16, 20)   520\n\ntime_distributed_2 (TimeDist (None, 100, 5120)         0\n\ntime_distributed_3 (TimeDist (None, 100, 1)            5121\nTotal params: 5,641\nTrainable params: 5,641\nNon-trainable params: 0\n\nTrain on 30960 samples, validate on 7740 samples\nEpoch 1/50\n2018-04-10 10:40:15.085958: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n2018-04-10 10:40:15.616355: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties:\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:02:00.0\ntotalMemory: 8.00GiB freeMemory: 6.64GiB\n2018-04-10 10:40:15.616355: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6411 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\n2018-04-10 10:40:19.469530: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:396] Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7003 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\n2018-04-10 10:40:19.469530: F T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo(), &algorithms)\n\nI have the following settings\nkeras                     2.1.5                    py36_0\ntensorflow                1.7.0                    py36_0    aaronzs\ntensorflow-gpu            1.7.0                    py36_0    aaronzs\nWindows 7\nCuda version 9.0\ncudnn version 7 (cudnn64_7)\nGPU NVIDIA GeForce GTX 1080 8 GB\nMy code:\n# Functional model definition\ninputs = Input(shape=(seqLength, 20, 20, 1))  # layer[0]\noutput_of_layer1 = TimeDistributed(Conv2D(20, (5, 5), activation=\"relu\"))(inputs)  # layer[1]\noutput_of_layer2 = TimeDistributed(Bidirectional(CuDNNLSTM(100, activation=\"sigmoid\", return_sequences='true', unit_forget_bias = True)))(output_of_layer1 )# layer[2]\noutput_of_layer3 = TimeDistributed(Flatten())(output_of_layer2)  # layer[3]\noutput_of_layer4 = TimeDistributed(Dense(1, activation=\"sigmoid\"))(output_of_layer3)  # layer[4]\nmodel = Model(inputs=inputs, outputs=output_of_layer8)\nmodel.summary()\nnepochs = 50\n\nsgd = optimizers.SGD(lr=0.01, decay=0, momentum=0.95, nesterov=False, clipvalue=20)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\ncheckpoint = ModelCheckpoint('trained_plaqueDetection_model_bidirectional.h5', monitor='val_acc', verbose=1, save_best_only=False, mode='max')\ncallbacks_list = [checkpoint]\nhistory = model.fit(training_dataset[0], training_dataset[1], epochs = nepochs, batch_size=10, shuffle=True, verbose = 1, validation_split = 0.2, callbacks = callbacks_list)\n\nplot_losses(history);\nThanks!", "body": "Hello,\r\nI have the same error:\r\nMy Python crushes when it gets to the first convolutional layer, I tried making the model simpler, still crushes. \r\n\r\nThis is the output and error: \r\n\r\nI have the latest cuDNN version (7.1.2) On  tensorflow website, it seems cudnn 7 works with tensorflow-gpu 1.7..\r\n\r\nTensorFlow 1.7 may be the last time we support cuDNN versions below 6.0.\r\nStarting with TensorFlow 1.8 release, 6.0 will be the minimum supported\r\nversion. [here](url): https://github.com/tensorflow/tensorflow/releases\r\n\r\nMy purpose is to use cudnnLSTM instead of regular  LSTM (layer 2), because regular LSTM works so slow (it uses 10% of GPU or less)\r\n\r\nThe code below works fine if I use tensorflow-gpu verison 1.1 and LSTM layer instead of  using cuDNNLSTM layer. cuDNNLSTM needs tensorflow-gpu not less than 1.2, so I updated my tensorflow-gpu to higher version (1.7.0) , but now, I have the following error. Any idea what is the issue?\r\n\r\n\r\n_________________________________________________________________\r\nLayer (type)                 [Output] Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 100, 20, 20, 1)    0         \r\n_________________________________________________________________\r\ntime_distributed_1 (TimeDist (None, 100, 16, 16, 20)   520       \r\n_________________________________________________________________\r\ntime_distributed_2 (TimeDist (None, 100, 5120)         0         \r\n_________________________________________________________________\r\ntime_distributed_3 (TimeDist (None, 100, 1)            5121      \r\n=================================================================\r\nTotal params: 5,641\r\nTrainable params: 5,641\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTrain on 30960 samples, validate on 7740 samples\r\nEpoch 1/50\r\n2018-04-10 10:40:15.085958: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-04-10 10:40:15.616355: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.64GiB\r\n2018-04-10 10:40:15.616355: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0 \r\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N \r\n2018-04-10 10:40:16.911147: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6411 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-04-10 10:40:19.469530: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:396] Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7003 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2018-04-10 10:40:19.469530: F T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\n\r\n----------------------------------------------------------\r\nI have the following settings\r\nkeras                     2.1.5                    py36_0\r\ntensorflow                1.7.0                    py36_0    aaronzs\r\ntensorflow-gpu            1.7.0                    py36_0    aaronzs\r\nWindows 7\r\nCuda version 9.0\r\ncudnn version 7 (cudnn64_7)\r\nGPU NVIDIA GeForce GTX 1080 8 GB\r\n\r\nMy code:\r\n    # Functional model definition\r\n    inputs = Input(shape=(seqLength, 20, 20, 1))  # layer[0]\r\n    output_of_layer1 = TimeDistributed(Conv2D(20, (5, 5), activation=\"relu\"))(inputs)  # layer[1]\r\n    output_of_layer2 = TimeDistributed(Bidirectional(CuDNNLSTM(100, activation=\"sigmoid\", return_sequences='true', unit_forget_bias = True)))(output_of_layer1 )# layer[2]\r\n    output_of_layer3 = TimeDistributed(Flatten())(output_of_layer2)  # layer[3]\r\n    output_of_layer4 = TimeDistributed(Dense(1, activation=\"sigmoid\"))(output_of_layer3)  # layer[4]\r\n    model = Model(inputs=inputs, outputs=output_of_layer8)\r\n\r\n    model.summary()\r\n    nepochs = 50\r\n\r\n    sgd = optimizers.SGD(lr=0.01, decay=0, momentum=0.95, nesterov=False, clipvalue=20)\r\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n    checkpoint = ModelCheckpoint('trained_plaqueDetection_model_bidirectional.h5', monitor='val_acc', verbose=1, save_best_only=False, mode='max')\r\n    callbacks_list = [checkpoint]\r\n    history = model.fit(training_dataset[0], training_dataset[1], epochs = nepochs, batch_size=10, shuffle=True, verbose = 1, validation_split = 0.2, callbacks = callbacks_list)\r\nplot_losses(history);\r\n\r\nThanks!"}