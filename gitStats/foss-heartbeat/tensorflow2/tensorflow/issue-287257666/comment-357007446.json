{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357007446", "html_url": "https://github.com/tensorflow/tensorflow/issues/15983#issuecomment-357007446", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15983", "id": 357007446, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzAwNzQ0Ng==", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-11T17:51:34Z", "updated_at": "2018-01-11T17:51:34Z", "author_association": "MEMBER", "body_html": "<p>The answer depends on how you are using TensorFlow.</p>\n<p>If you are writing plain low-level TensorFlow code and your own training loop (e.g., using MonitoredTrainingSessions interleaved with validation, this is very straightforward. The learning rate is a parameter to the optimizers, which is a Tensor. The Tensor could be computed any way you like to, including any sort of plateau detection (presubmably, by comparing two rolling averages of the loss), and if it triggers, reducing the learning rate. We may not have a function that does exactly what you want, but it is easy to write.</p>\n<p>If you are using an Estimator, you do not have easy access to the validation loss inside the training loop. This is because in distributed training, evaluation is performed on a separate worker, and in an isolated environment, while training continues on the other workers. If you want to implement learning rate decay based on evaluation loss there, you would write a hook which reads the loss from the evaluation event file, does whatever logic it needs to, and assigns a new learning rate to a training variable, which in turn is used as the learning rate in the optimizer. This is not hard to do, but it is not done.</p>\n<p>Of course, you can also use tf.keras if that works for you, that's why it's there.</p>", "body_text": "The answer depends on how you are using TensorFlow.\nIf you are writing plain low-level TensorFlow code and your own training loop (e.g., using MonitoredTrainingSessions interleaved with validation, this is very straightforward. The learning rate is a parameter to the optimizers, which is a Tensor. The Tensor could be computed any way you like to, including any sort of plateau detection (presubmably, by comparing two rolling averages of the loss), and if it triggers, reducing the learning rate. We may not have a function that does exactly what you want, but it is easy to write.\nIf you are using an Estimator, you do not have easy access to the validation loss inside the training loop. This is because in distributed training, evaluation is performed on a separate worker, and in an isolated environment, while training continues on the other workers. If you want to implement learning rate decay based on evaluation loss there, you would write a hook which reads the loss from the evaluation event file, does whatever logic it needs to, and assigns a new learning rate to a training variable, which in turn is used as the learning rate in the optimizer. This is not hard to do, but it is not done.\nOf course, you can also use tf.keras if that works for you, that's why it's there.", "body": "The answer depends on how you are using TensorFlow. \r\n\r\nIf you are writing plain low-level TensorFlow code and your own training loop (e.g., using MonitoredTrainingSessions interleaved with validation, this is very straightforward. The learning rate is a parameter to the optimizers, which is a Tensor. The Tensor could be computed any way you like to, including any sort of plateau detection (presubmably, by comparing two rolling averages of the loss), and if it triggers, reducing the learning rate. We may not have a function that does exactly what you want, but it is easy to write. \r\n\r\nIf you are using an Estimator, you do not have easy access to the validation loss inside the training loop. This is because in distributed training, evaluation is performed on a separate worker, and in an isolated environment, while training continues on the other workers. If you want to implement learning rate decay based on evaluation loss there, you would write a hook which reads the loss from the evaluation event file, does whatever logic it needs to, and assigns a new learning rate to a training variable, which in turn is used as the learning rate in the optimizer. This is not hard to do, but it is not done.\r\n\r\nOf course, you can also use tf.keras if that works for you, that's why it's there."}