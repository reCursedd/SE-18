{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361163606", "html_url": "https://github.com/tensorflow/tensorflow/issues/15983#issuecomment-361163606", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15983", "id": 361163606, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTE2MzYwNg==", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-29T07:44:20Z", "updated_at": "2018-01-29T07:44:44Z", "author_association": "MEMBER", "body_html": "<p>TFRecords and Keras is possible, but not necessarily simple.</p>\n<p>The pseudocode you describe is possible. You can add learning rate to the params arg (which is passed to the model_fn).</p>\n<p>Then, make a new estimator in each iteration of the loop. The Estimator will read the last state from disk anyway every time you call one of its methods.</p>\n<p>It would look like this:</p>\n<pre><code>def model_fn(..., params):\n    # Do something with params['learning_rate']\n    ...\n\nlr = INITIAL_LEARNING_RATE\n\n# Cycle over epochs\nfor cycle in num_cycles:\n    # Define model\n    model = tf.estimator.Estimator(model_fn=model_fn, ..., params={'learning_rate': lr})\n\n    model.train(...)\n\n    # Grab loss\n    loss = model.evaluate(...)\n\n    # Determine if hit plateau:\n    if hit_plateau:\n        lr = reduce_learning_rate(lr)\n</code></pre>\n<p>Note that constructing the Estimator is effectively free. Calling train is a loop is not something I recommend, but for local training it's ok, especially if you only call it once per epoch.</p>", "body_text": "TFRecords and Keras is possible, but not necessarily simple.\nThe pseudocode you describe is possible. You can add learning rate to the params arg (which is passed to the model_fn).\nThen, make a new estimator in each iteration of the loop. The Estimator will read the last state from disk anyway every time you call one of its methods.\nIt would look like this:\ndef model_fn(..., params):\n    # Do something with params['learning_rate']\n    ...\n\nlr = INITIAL_LEARNING_RATE\n\n# Cycle over epochs\nfor cycle in num_cycles:\n    # Define model\n    model = tf.estimator.Estimator(model_fn=model_fn, ..., params={'learning_rate': lr})\n\n    model.train(...)\n\n    # Grab loss\n    loss = model.evaluate(...)\n\n    # Determine if hit plateau:\n    if hit_plateau:\n        lr = reduce_learning_rate(lr)\n\nNote that constructing the Estimator is effectively free. Calling train is a loop is not something I recommend, but for local training it's ok, especially if you only call it once per epoch.", "body": "TFRecords and Keras is possible, but not necessarily simple.\r\n\r\nThe pseudocode you describe is possible. You can add learning rate to the params arg (which is passed to the model_fn). \r\n\r\nThen, make a new estimator in each iteration of the loop. The Estimator will read the last state from disk anyway every time you call one of its methods. \r\n\r\nIt would look like this:\r\n\r\n```\r\ndef model_fn(..., params):\r\n    # Do something with params['learning_rate']\r\n    ...\r\n\r\nlr = INITIAL_LEARNING_RATE\r\n\r\n# Cycle over epochs\r\nfor cycle in num_cycles:\r\n    # Define model\r\n    model = tf.estimator.Estimator(model_fn=model_fn, ..., params={'learning_rate': lr})\r\n\r\n    model.train(...)\r\n\r\n    # Grab loss\r\n    loss = model.evaluate(...)\r\n\r\n    # Determine if hit plateau:\r\n    if hit_plateau:\r\n        lr = reduce_learning_rate(lr)\r\n```\r\n\r\nNote that constructing the Estimator is effectively free. Calling train is a loop is not something I recommend, but for local training it's ok, especially if you only call it once per epoch."}