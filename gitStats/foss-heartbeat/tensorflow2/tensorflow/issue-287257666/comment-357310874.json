{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357310874", "html_url": "https://github.com/tensorflow/tensorflow/issues/15983#issuecomment-357310874", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15983", "id": 357310874, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzMxMDg3NA==", "user": {"login": "aneergaard", "id": 10213572, "node_id": "MDQ6VXNlcjEwMjEzNTcy", "avatar_url": "https://avatars1.githubusercontent.com/u/10213572?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aneergaard", "html_url": "https://github.com/aneergaard", "followers_url": "https://api.github.com/users/aneergaard/followers", "following_url": "https://api.github.com/users/aneergaard/following{/other_user}", "gists_url": "https://api.github.com/users/aneergaard/gists{/gist_id}", "starred_url": "https://api.github.com/users/aneergaard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aneergaard/subscriptions", "organizations_url": "https://api.github.com/users/aneergaard/orgs", "repos_url": "https://api.github.com/users/aneergaard/repos", "events_url": "https://api.github.com/users/aneergaard/events{/privacy}", "received_events_url": "https://api.github.com/users/aneergaard/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-12T18:02:11Z", "updated_at": "2018-01-12T18:05:30Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a> Thanks for the nice response. I am using the Experiment/Estimator combo, though I am not planning to do distributed training.</p>\n<p>However, wouldn't it be possible to create a loop over a training and evaluation phase using Estimator alone, grab the loss from the evaluate() call, determine learning rate decay and then reset the learning rate for the subsequent training/evaluation phases?</p>\n<p>Something like this pseudo:</p>\n<pre><code># Define model\nmodel = tf.estimator.Estimator(...)\n\n# Cycle over epochs\nfor cycle in num_cycles:\n    model.train(...)\n\n    # Grab loss\n    loss = model.evaluate(...)\n\n    # Determine if hit plateau:\n    if hit_plateau:\n        reduce_learning_rate()\n</code></pre>\n<p>Where of course the <code>reduce_learning_rate()</code> function would impute the new learning rate value to the learning rate tensor. Would that be a possible workaround?</p>\n<p>EDIT: by the way, I am not finding a lot of documentation on how to use the tf.keras module. Is it possible to use the TFRecords API with tf.keras yet, or is there no significant performance gain to this application?</p>", "body_text": "@martinwicke Thanks for the nice response. I am using the Experiment/Estimator combo, though I am not planning to do distributed training.\nHowever, wouldn't it be possible to create a loop over a training and evaluation phase using Estimator alone, grab the loss from the evaluate() call, determine learning rate decay and then reset the learning rate for the subsequent training/evaluation phases?\nSomething like this pseudo:\n# Define model\nmodel = tf.estimator.Estimator(...)\n\n# Cycle over epochs\nfor cycle in num_cycles:\n    model.train(...)\n\n    # Grab loss\n    loss = model.evaluate(...)\n\n    # Determine if hit plateau:\n    if hit_plateau:\n        reduce_learning_rate()\n\nWhere of course the reduce_learning_rate() function would impute the new learning rate value to the learning rate tensor. Would that be a possible workaround?\nEDIT: by the way, I am not finding a lot of documentation on how to use the tf.keras module. Is it possible to use the TFRecords API with tf.keras yet, or is there no significant performance gain to this application?", "body": "@martinwicke Thanks for the nice response. I am using the Experiment/Estimator combo, though I am not planning to do distributed training. \r\n\r\nHowever, wouldn't it be possible to create a loop over a training and evaluation phase using Estimator alone, grab the loss from the evaluate() call, determine learning rate decay and then reset the learning rate for the subsequent training/evaluation phases?\r\n\r\nSomething like this pseudo:\r\n```\r\n# Define model\r\nmodel = tf.estimator.Estimator(...)\r\n\r\n# Cycle over epochs\r\nfor cycle in num_cycles:\r\n    model.train(...)\r\n\r\n    # Grab loss\r\n    loss = model.evaluate(...)\r\n\r\n    # Determine if hit plateau:\r\n    if hit_plateau:\r\n        reduce_learning_rate()\r\n```\r\nWhere of course the `reduce_learning_rate()` function would impute the new learning rate value to the learning rate tensor. Would that be a possible workaround?\r\n\r\nEDIT: by the way, I am not finding a lot of documentation on how to use the tf.keras module. Is it possible to use the TFRecords API with tf.keras yet, or is there no significant performance gain to this application?"}