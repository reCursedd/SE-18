{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6333", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6333/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6333/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6333/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6333", "id": 195796123, "node_id": "MDU6SXNzdWUxOTU3OTYxMjM=", "number": 6333, "title": "Very low GPU usage", "user": {"login": "333caowei", "id": 4569055, "node_id": "MDQ6VXNlcjQ1NjkwNTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/4569055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/333caowei", "html_url": "https://github.com/333caowei", "followers_url": "https://api.github.com/users/333caowei/followers", "following_url": "https://api.github.com/users/333caowei/following{/other_user}", "gists_url": "https://api.github.com/users/333caowei/gists{/gist_id}", "starred_url": "https://api.github.com/users/333caowei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/333caowei/subscriptions", "organizations_url": "https://api.github.com/users/333caowei/orgs", "repos_url": "https://api.github.com/users/333caowei/repos", "events_url": "https://api.github.com/users/333caowei/events{/privacy}", "received_events_url": "https://api.github.com/users/333caowei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-12-15T12:42:08Z", "updated_at": "2016-12-15T17:00:23Z", "closed_at": "2016-12-15T17:00:23Z", "author_association": "NONE", "body_html": "<p>I try to use single dynamic_rnn to process very long sequence for classification task.<br>\nHere are some parameters:<br>\nrnn_size=500, seq_max_length=2500, batch_size=50, embedding_size=64, softmax_size=1600.</p>\n<p>the code is as below:</p>\n<pre><code>x_vec = tf.nn.embedding_lookup(embedding_matrix_variable, self.x)\nlstm_fw_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = word_dim)\nlstm_fw_cell = rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob, input_keep_prob=self.dropout_keep_prob)\noutputs, _ = rnn.dynamic_rnn(lstm_fw_cell, x, dtype=tf.float32, sequence_length=real_length, swap_memory=False)\n\noutputs = tf.transpose(outputs, [1, 0, 2])\noutputs = tf.unpack(outputs)\n\noutput = outputs[0]\none = tf.ones([1, hidden_unit], tf.float32)\nwith tf.variable_scope(\"output\"):\n    tf.get_variable_scope().reuse_variables()\n        for i in range(1, len(outputs_6)):\n            ind = self.real_length &lt; (i+1)\n            ind = tf.to_float(ind)\n             ind = tf.expand_dims(ind, -1)\n             mat = tf.matmul(ind, one)\n             output=tf.add(tf.mul(output, mat), tf.mul(outputs[i], 1.0-mat))\n\n\ny_prediction = tf.matmul(output, w_h2y) + b_h2y\ny_prediction = tf.nn.softmax(y_prediction)\t\n\t\t\nweight_decay = L2  * ( tf.nn.l2_loss(w_h2y) + tf.nn.l2_loss(b_h2y) )\nself.cost = tf.reduce_mean( -tf.reduce_sum(self.y*tf.log(y_prediction + 1e-10)) ) + weight_decay\nself.optimizer = tf.train.AdamOptimizer(alpha).minimize(self.cost)\n</code></pre>\n<p>The usage of GPU on TITAN is only 5%.<br>\nThe usage of CPU  is about 150%.<br>\nI am not sure what's the problem.</p>", "body_text": "I try to use single dynamic_rnn to process very long sequence for classification task.\nHere are some parameters:\nrnn_size=500, seq_max_length=2500, batch_size=50, embedding_size=64, softmax_size=1600.\nthe code is as below:\nx_vec = tf.nn.embedding_lookup(embedding_matrix_variable, self.x)\nlstm_fw_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = word_dim)\nlstm_fw_cell = rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob, input_keep_prob=self.dropout_keep_prob)\noutputs, _ = rnn.dynamic_rnn(lstm_fw_cell, x, dtype=tf.float32, sequence_length=real_length, swap_memory=False)\n\noutputs = tf.transpose(outputs, [1, 0, 2])\noutputs = tf.unpack(outputs)\n\noutput = outputs[0]\none = tf.ones([1, hidden_unit], tf.float32)\nwith tf.variable_scope(\"output\"):\n    tf.get_variable_scope().reuse_variables()\n        for i in range(1, len(outputs_6)):\n            ind = self.real_length < (i+1)\n            ind = tf.to_float(ind)\n             ind = tf.expand_dims(ind, -1)\n             mat = tf.matmul(ind, one)\n             output=tf.add(tf.mul(output, mat), tf.mul(outputs[i], 1.0-mat))\n\n\ny_prediction = tf.matmul(output, w_h2y) + b_h2y\ny_prediction = tf.nn.softmax(y_prediction)\t\n\t\t\nweight_decay = L2  * ( tf.nn.l2_loss(w_h2y) + tf.nn.l2_loss(b_h2y) )\nself.cost = tf.reduce_mean( -tf.reduce_sum(self.y*tf.log(y_prediction + 1e-10)) ) + weight_decay\nself.optimizer = tf.train.AdamOptimizer(alpha).minimize(self.cost)\n\nThe usage of GPU on TITAN is only 5%.\nThe usage of CPU  is about 150%.\nI am not sure what's the problem.", "body": "I try to use single dynamic_rnn to process very long sequence for classification task.\r\nHere are some parameters:\r\nrnn_size=500, seq_max_length=2500, batch_size=50, embedding_size=64, softmax_size=1600.\r\n\r\nthe code is as below:\r\n```\r\nx_vec = tf.nn.embedding_lookup(embedding_matrix_variable, self.x)\r\nlstm_fw_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = word_dim)\r\nlstm_fw_cell = rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob, input_keep_prob=self.dropout_keep_prob)\r\noutputs, _ = rnn.dynamic_rnn(lstm_fw_cell, x, dtype=tf.float32, sequence_length=real_length, swap_memory=False)\r\n\r\noutputs = tf.transpose(outputs, [1, 0, 2])\r\noutputs = tf.unpack(outputs)\r\n\r\noutput = outputs[0]\r\none = tf.ones([1, hidden_unit], tf.float32)\r\nwith tf.variable_scope(\"output\"):\r\n    tf.get_variable_scope().reuse_variables()\r\n        for i in range(1, len(outputs_6)):\r\n            ind = self.real_length < (i+1)\r\n            ind = tf.to_float(ind)\r\n             ind = tf.expand_dims(ind, -1)\r\n             mat = tf.matmul(ind, one)\r\n             output=tf.add(tf.mul(output, mat), tf.mul(outputs[i], 1.0-mat))\r\n\r\n\r\ny_prediction = tf.matmul(output, w_h2y) + b_h2y\r\ny_prediction = tf.nn.softmax(y_prediction)\t\r\n\t\t\r\nweight_decay = L2  * ( tf.nn.l2_loss(w_h2y) + tf.nn.l2_loss(b_h2y) )\r\nself.cost = tf.reduce_mean( -tf.reduce_sum(self.y*tf.log(y_prediction + 1e-10)) ) + weight_decay\r\nself.optimizer = tf.train.AdamOptimizer(alpha).minimize(self.cost)\r\n```\r\n\r\n\r\nThe usage of GPU on TITAN is only 5%.\r\nThe usage of CPU  is about 150%.\r\nI am not sure what's the problem."}