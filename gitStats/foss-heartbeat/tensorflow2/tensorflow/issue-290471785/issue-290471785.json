{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16287", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16287/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16287/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16287/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16287", "id": 290471785, "node_id": "MDU6SXNzdWUyOTA0NzE3ODU=", "number": 16287, "title": "[Bug] LuongMonotonicAttention in contrib/seq2seq/python/ops/attention_wrapper.py", "user": {"login": "KrnTneja", "id": 15049793, "node_id": "MDQ6VXNlcjE1MDQ5Nzkz", "avatar_url": "https://avatars0.githubusercontent.com/u/15049793?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KrnTneja", "html_url": "https://github.com/KrnTneja", "followers_url": "https://api.github.com/users/KrnTneja/followers", "following_url": "https://api.github.com/users/KrnTneja/following{/other_user}", "gists_url": "https://api.github.com/users/KrnTneja/gists{/gist_id}", "starred_url": "https://api.github.com/users/KrnTneja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KrnTneja/subscriptions", "organizations_url": "https://api.github.com/users/KrnTneja/orgs", "repos_url": "https://api.github.com/users/KrnTneja/repos", "events_url": "https://api.github.com/users/KrnTneja/events{/privacy}", "received_events_url": "https://api.github.com/users/KrnTneja/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-01-22T13:50:55Z", "updated_at": "2018-02-01T03:00:34Z", "closed_at": "2018-02-01T03:00:34Z", "author_association": "NONE", "body_html": "<p><code>LuongMonotonicAttention.__init__(...)</code> calls its parent <code>_BaseAttentionMechanism</code> with <code>query_layer</code> as follows:</p>\n<pre><code>        query_layer=layers_core.Dense(\n            num_units, name=\"query_layer\", use_bias=False),\n</code></pre>\n<p>But, it doesn't apply it on query in <code>LuongMonotonicAttention.__call__(...)</code>.</p>\n<pre><code>  def __call__(self, query, previous_alignments):\n    \"\"\"...\n    \"\"\"\n    with variable_scope.variable_scope(None, \"luong_monotonic_attention\",\n                                       [query]):\n      score = _luong_score(query, self._keys, self._scale)\n      score_bias = variable_scope.get_variable(\n          \"attention_score_bias\", dtype=query.dtype,\n          initializer=self._score_bias_init)\n      score += score_bias\n    alignments = self._probability_fn(score, previous_alignments)\n    return alignments\n</code></pre>\n<p>Guessing from the way <code>LuongAttention</code> works, there should be <code>query_layer=None</code> in <code>LuongMonotonicAttention.__init__(...)</code>.</p>", "body_text": "LuongMonotonicAttention.__init__(...) calls its parent _BaseAttentionMechanism with query_layer as follows:\n        query_layer=layers_core.Dense(\n            num_units, name=\"query_layer\", use_bias=False),\n\nBut, it doesn't apply it on query in LuongMonotonicAttention.__call__(...).\n  def __call__(self, query, previous_alignments):\n    \"\"\"...\n    \"\"\"\n    with variable_scope.variable_scope(None, \"luong_monotonic_attention\",\n                                       [query]):\n      score = _luong_score(query, self._keys, self._scale)\n      score_bias = variable_scope.get_variable(\n          \"attention_score_bias\", dtype=query.dtype,\n          initializer=self._score_bias_init)\n      score += score_bias\n    alignments = self._probability_fn(score, previous_alignments)\n    return alignments\n\nGuessing from the way LuongAttention works, there should be query_layer=None in LuongMonotonicAttention.__init__(...).", "body": "`LuongMonotonicAttention.__init__(...)` calls its parent `_BaseAttentionMechanism` with `query_layer` as follows:\r\n```\r\n        query_layer=layers_core.Dense(\r\n            num_units, name=\"query_layer\", use_bias=False),\r\n```\r\nBut, it doesn't apply it on query in `LuongMonotonicAttention.__call__(...)`.\r\n```\r\n  def __call__(self, query, previous_alignments):\r\n    \"\"\"...\r\n    \"\"\"\r\n    with variable_scope.variable_scope(None, \"luong_monotonic_attention\",\r\n                                       [query]):\r\n      score = _luong_score(query, self._keys, self._scale)\r\n      score_bias = variable_scope.get_variable(\r\n          \"attention_score_bias\", dtype=query.dtype,\r\n          initializer=self._score_bias_init)\r\n      score += score_bias\r\n    alignments = self._probability_fn(score, previous_alignments)\r\n    return alignments\r\n```\r\nGuessing from the way `LuongAttention` works, there should be `query_layer=None` in `LuongMonotonicAttention.__init__(...)`."}