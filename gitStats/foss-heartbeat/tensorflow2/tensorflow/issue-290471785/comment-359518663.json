{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/359518663", "html_url": "https://github.com/tensorflow/tensorflow/issues/16287#issuecomment-359518663", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16287", "id": 359518663, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTUxODY2Mw==", "user": {"login": "KrnTneja", "id": 15049793, "node_id": "MDQ6VXNlcjE1MDQ5Nzkz", "avatar_url": "https://avatars0.githubusercontent.com/u/15049793?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KrnTneja", "html_url": "https://github.com/KrnTneja", "followers_url": "https://api.github.com/users/KrnTneja/followers", "following_url": "https://api.github.com/users/KrnTneja/following{/other_user}", "gists_url": "https://api.github.com/users/KrnTneja/gists{/gist_id}", "starred_url": "https://api.github.com/users/KrnTneja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KrnTneja/subscriptions", "organizations_url": "https://api.github.com/users/KrnTneja/orgs", "repos_url": "https://api.github.com/users/KrnTneja/repos", "events_url": "https://api.github.com/users/KrnTneja/events{/privacy}", "received_events_url": "https://api.github.com/users/KrnTneja/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-22T18:27:46Z", "updated_at": "2018-01-22T18:27:55Z", "author_association": "NONE", "body_html": "<p>Apart from this, I noticed that <code>AttentionWrapper.call(...)</code> has the following lines:</p>\n<pre><code>...\n    cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\n...\n      attention, alignments = _compute_attention(\n          attention_mechanism, cell_output, previous_alignments[i],\n          self._attention_layers[i] if self._attention_layers else None)\n...\n</code></pre>\n<p>But as far as I know, decoders' state (along with memory) is used to compute alignments and attention. Is the above code buggy because it's passing cell output rather than the hidden state to <code>_compute_attention(...)</code>? Am I making a mistake in understanding these variables?</p>", "body_text": "Apart from this, I noticed that AttentionWrapper.call(...) has the following lines:\n...\n    cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\n...\n      attention, alignments = _compute_attention(\n          attention_mechanism, cell_output, previous_alignments[i],\n          self._attention_layers[i] if self._attention_layers else None)\n...\n\nBut as far as I know, decoders' state (along with memory) is used to compute alignments and attention. Is the above code buggy because it's passing cell output rather than the hidden state to _compute_attention(...)? Am I making a mistake in understanding these variables?", "body": "Apart from this, I noticed that `AttentionWrapper.call(...)` has the following lines:\r\n```\r\n...\r\n    cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\r\n...\r\n      attention, alignments = _compute_attention(\r\n          attention_mechanism, cell_output, previous_alignments[i],\r\n          self._attention_layers[i] if self._attention_layers else None)\r\n...\r\n```\r\nBut as far as I know, decoders' state (along with memory) is used to compute alignments and attention. Is the above code buggy because it's passing cell output rather than the hidden state to `_compute_attention(...)`? Am I making a mistake in understanding these variables?"}