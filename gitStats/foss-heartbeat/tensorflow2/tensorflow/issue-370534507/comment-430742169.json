{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/430742169", "html_url": "https://github.com/tensorflow/tensorflow/issues/23019#issuecomment-430742169", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23019", "id": 430742169, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDc0MjE2OQ==", "user": {"login": "fabio12345", "id": 33119878, "node_id": "MDQ6VXNlcjMzMTE5ODc4", "avatar_url": "https://avatars1.githubusercontent.com/u/33119878?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fabio12345", "html_url": "https://github.com/fabio12345", "followers_url": "https://api.github.com/users/fabio12345/followers", "following_url": "https://api.github.com/users/fabio12345/following{/other_user}", "gists_url": "https://api.github.com/users/fabio12345/gists{/gist_id}", "starred_url": "https://api.github.com/users/fabio12345/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fabio12345/subscriptions", "organizations_url": "https://api.github.com/users/fabio12345/orgs", "repos_url": "https://api.github.com/users/fabio12345/repos", "events_url": "https://api.github.com/users/fabio12345/events{/privacy}", "received_events_url": "https://api.github.com/users/fabio12345/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T18:40:24Z", "updated_at": "2018-10-17T18:40:24Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=29100818\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/omalleyt12\">@omalleyt12</a>, thanks for your comment.<br>\nI understand that the function is expecting a size divisible by 4 at some point, but that\u2019s not at the input of <strong>call</strong>. It is already doing some padding internally, which is dependent on the size of input in the first invocation of <strong>call</strong> on the layer object.<br>\nAs shown above, the layer works on the 41x41 input if the invocation of call on that input comes first, and doesn\u2019t work on the subsequent invocation of call on a 32x32 input in that case.<br>\nThis means that Conv layers don\u2019t support the use case where you want to <strong>call</strong> the layer on another tensor that has got a different size - I would expect the layer to do the padding internally on each invocation of call. Is this a bug, or is there any explanation of why this would be a feature?</p>", "body_text": "Hi @omalleyt12, thanks for your comment.\nI understand that the function is expecting a size divisible by 4 at some point, but that\u2019s not at the input of call. It is already doing some padding internally, which is dependent on the size of input in the first invocation of call on the layer object.\nAs shown above, the layer works on the 41x41 input if the invocation of call on that input comes first, and doesn\u2019t work on the subsequent invocation of call on a 32x32 input in that case.\nThis means that Conv layers don\u2019t support the use case where you want to call the layer on another tensor that has got a different size - I would expect the layer to do the padding internally on each invocation of call. Is this a bug, or is there any explanation of why this would be a feature?", "body": "Hi @omalleyt12, thanks for your comment. \r\nI understand that the function is expecting a size divisible by 4 at some point, but that\u2019s not at the input of __call__. It is already doing some padding internally, which is dependent on the size of input in the first invocation of __call__ on the layer object. \r\nAs shown above, the layer works on the 41x41 input if the invocation of call on that input comes first, and doesn\u2019t work on the subsequent invocation of call on a 32x32 input in that case. \r\nThis means that Conv layers don\u2019t support the use case where you want to __call__ the layer on another tensor that has got a different size - I would expect the layer to do the padding internally on each invocation of call. Is this a bug, or is there any explanation of why this would be a feature?\r\n"}