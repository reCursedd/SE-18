{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361033788", "html_url": "https://github.com/tensorflow/tensorflow/issues/12686#issuecomment-361033788", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12686", "id": 361033788, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTAzMzc4OA==", "user": {"login": "pbanavara", "id": 600054, "node_id": "MDQ6VXNlcjYwMDA1NA==", "avatar_url": "https://avatars3.githubusercontent.com/u/600054?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pbanavara", "html_url": "https://github.com/pbanavara", "followers_url": "https://api.github.com/users/pbanavara/followers", "following_url": "https://api.github.com/users/pbanavara/following{/other_user}", "gists_url": "https://api.github.com/users/pbanavara/gists{/gist_id}", "starred_url": "https://api.github.com/users/pbanavara/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pbanavara/subscriptions", "organizations_url": "https://api.github.com/users/pbanavara/orgs", "repos_url": "https://api.github.com/users/pbanavara/repos", "events_url": "https://api.github.com/users/pbanavara/events{/privacy}", "received_events_url": "https://api.github.com/users/pbanavara/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-28T02:46:07Z", "updated_at": "2018-01-28T02:46:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=88808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/skye\">@skye</a> - Revisiting this after a long gap. Sorry but I am unable to process the matrix shape mismatch error. Here is my test case</p>\n<pre><code>\nTEST_F(NNGradTest, SoftmaxCrossEntropyWithLogitsGrad) {\n  TensorShape shape({5,3});\n  TensorShape lossShape({5});\n  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\n  auto l = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\n  OutputList inputs = OutputList();\n  OutputList outputs = OutputList();\n  std::vector&lt;TensorShape&gt; inputShapes = std::vector&lt;TensorShape&gt;();\n  std::vector&lt;TensorShape&gt; outputShapes = std::vector&lt;TensorShape&gt;();\n  inputShapes.push_back(shape);\n  outputShapes.push_back(shape);\n  outputShapes.push_back(lossShape);\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, x, l);\n  inputs.push_back(x);\n  outputs.push_back(y.backprop);\n  outputs.push_back(y.loss);\n  RunTest(inputs, inputShapes, outputs, outputShapes);\n}\n</code></pre>\n<p>The shapes of backprop and loss seem to be appropriate - {5,3} and {5}. There is no way for me to print the shapes of the loss and backprop as the Output data structure doesn't have those APIs. Can you please advise what's wrong ? Here's the error.</p>\n<pre><code>To be equal to: ((ComputeGradientError&lt;float, float, float&gt;( scope_, xs, x_shapes, ys, y_shapes, &amp;max_error)))\n      Which is: Invalid argument: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [5], [5,3].\n</code></pre>\n<p>Here's my code:</p>\n<pre><code>Status SoftmaxCrossEntropyWithLogitsGrad(const Scope&amp; scope,\n                                          const Operation&amp; op,\n                                          const std::vector&lt;Output&gt;&amp;\n                                          grad_inputs,\n                                          std::vector&lt;Output&gt;* grad_outputs) {\n  // Softmax gradient with cross entropy logits function\n  // We multiply the backprop for cost with the gradients - op.output[1]\n  // There is no gradient for labels\n\n  std::cout &lt;&lt; \"Step 1\" &lt;&lt; \"\\n\";\n  auto softmaxGrad = op.output(1);\n  auto gradLoss = grad_inputs[0];\n  auto gradGrad = grad_inputs[1];\n  auto tempGrad = Mul(scope, gradLoss, softmaxGrad);\n\n  // TODO: Check if this sufficient, need a check for null ?\n  if (gradGrad.op().output_type(0) != 0) {\n          auto logits = op.input(0);\n          auto softmax = ops::Softmax(scope, logits);\n          auto prod = ops::MatMul(scope, gradGrad, softmax);\n          auto squeezeProd = ops::Squeeze(scope, prod);\n          auto fProd = Sub(scope, gradGrad, squeezeProd);\n          auto grad = Add(scope, tempGrad, fProd);\n          grad_outputs-&gt;push_back(grad);\n          grad_outputs-&gt;push_back(ops::MatMul(scope, gradLoss, ops::LogSoftmax(scope, logits)));\n          grad_outputs-&gt;push_back(NoGradient());\n  }\n  return scope.status();\n}\n</code></pre>", "body_text": "@skye - Revisiting this after a long gap. Sorry but I am unable to process the matrix shape mismatch error. Here is my test case\n\nTEST_F(NNGradTest, SoftmaxCrossEntropyWithLogitsGrad) {\n  TensorShape shape({5,3});\n  TensorShape lossShape({5});\n  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\n  auto l = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\n  OutputList inputs = OutputList();\n  OutputList outputs = OutputList();\n  std::vector<TensorShape> inputShapes = std::vector<TensorShape>();\n  std::vector<TensorShape> outputShapes = std::vector<TensorShape>();\n  inputShapes.push_back(shape);\n  outputShapes.push_back(shape);\n  outputShapes.push_back(lossShape);\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, x, l);\n  inputs.push_back(x);\n  outputs.push_back(y.backprop);\n  outputs.push_back(y.loss);\n  RunTest(inputs, inputShapes, outputs, outputShapes);\n}\n\nThe shapes of backprop and loss seem to be appropriate - {5,3} and {5}. There is no way for me to print the shapes of the loss and backprop as the Output data structure doesn't have those APIs. Can you please advise what's wrong ? Here's the error.\nTo be equal to: ((ComputeGradientError<float, float, float>( scope_, xs, x_shapes, ys, y_shapes, &max_error)))\n      Which is: Invalid argument: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [5], [5,3].\n\nHere's my code:\nStatus SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope,\n                                          const Operation& op,\n                                          const std::vector<Output>&\n                                          grad_inputs,\n                                          std::vector<Output>* grad_outputs) {\n  // Softmax gradient with cross entropy logits function\n  // We multiply the backprop for cost with the gradients - op.output[1]\n  // There is no gradient for labels\n\n  std::cout << \"Step 1\" << \"\\n\";\n  auto softmaxGrad = op.output(1);\n  auto gradLoss = grad_inputs[0];\n  auto gradGrad = grad_inputs[1];\n  auto tempGrad = Mul(scope, gradLoss, softmaxGrad);\n\n  // TODO: Check if this sufficient, need a check for null ?\n  if (gradGrad.op().output_type(0) != 0) {\n          auto logits = op.input(0);\n          auto softmax = ops::Softmax(scope, logits);\n          auto prod = ops::MatMul(scope, gradGrad, softmax);\n          auto squeezeProd = ops::Squeeze(scope, prod);\n          auto fProd = Sub(scope, gradGrad, squeezeProd);\n          auto grad = Add(scope, tempGrad, fProd);\n          grad_outputs->push_back(grad);\n          grad_outputs->push_back(ops::MatMul(scope, gradLoss, ops::LogSoftmax(scope, logits)));\n          grad_outputs->push_back(NoGradient());\n  }\n  return scope.status();\n}", "body": "@skye - Revisiting this after a long gap. Sorry but I am unable to process the matrix shape mismatch error. Here is my test case\r\n```\r\n\r\nTEST_F(NNGradTest, SoftmaxCrossEntropyWithLogitsGrad) {\r\n  TensorShape shape({5,3});\r\n  TensorShape lossShape({5});\r\n  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n  auto l = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n  OutputList inputs = OutputList();\r\n  OutputList outputs = OutputList();\r\n  std::vector<TensorShape> inputShapes = std::vector<TensorShape>();\r\n  std::vector<TensorShape> outputShapes = std::vector<TensorShape>();\r\n  inputShapes.push_back(shape);\r\n  outputShapes.push_back(shape);\r\n  outputShapes.push_back(lossShape);\r\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, x, l);\r\n  inputs.push_back(x);\r\n  outputs.push_back(y.backprop);\r\n  outputs.push_back(y.loss);\r\n  RunTest(inputs, inputShapes, outputs, outputShapes);\r\n}\r\n```\r\nThe shapes of backprop and loss seem to be appropriate - {5,3} and {5}. There is no way for me to print the shapes of the loss and backprop as the Output data structure doesn't have those APIs. Can you please advise what's wrong ? Here's the error. \r\n\r\n```\r\nTo be equal to: ((ComputeGradientError<float, float, float>( scope_, xs, x_shapes, ys, y_shapes, &max_error)))\r\n      Which is: Invalid argument: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [5], [5,3].\r\n```\r\nHere's my code:\r\n\r\n```\r\nStatus SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope,\r\n                                          const Operation& op,\r\n                                          const std::vector<Output>&\r\n                                          grad_inputs,\r\n                                          std::vector<Output>* grad_outputs) {\r\n  // Softmax gradient with cross entropy logits function\r\n  // We multiply the backprop for cost with the gradients - op.output[1]\r\n  // There is no gradient for labels\r\n\r\n  std::cout << \"Step 1\" << \"\\n\";\r\n  auto softmaxGrad = op.output(1);\r\n  auto gradLoss = grad_inputs[0];\r\n  auto gradGrad = grad_inputs[1];\r\n  auto tempGrad = Mul(scope, gradLoss, softmaxGrad);\r\n\r\n  // TODO: Check if this sufficient, need a check for null ?\r\n  if (gradGrad.op().output_type(0) != 0) {\r\n          auto logits = op.input(0);\r\n          auto softmax = ops::Softmax(scope, logits);\r\n          auto prod = ops::MatMul(scope, gradGrad, softmax);\r\n          auto squeezeProd = ops::Squeeze(scope, prod);\r\n          auto fProd = Sub(scope, gradGrad, squeezeProd);\r\n          auto grad = Add(scope, tempGrad, fProd);\r\n          grad_outputs->push_back(grad);\r\n          grad_outputs->push_back(ops::MatMul(scope, gradLoss, ops::LogSoftmax(scope, logits)));\r\n          grad_outputs->push_back(NoGradient());\r\n  }\r\n  return scope.status();\r\n}\r\n```"}