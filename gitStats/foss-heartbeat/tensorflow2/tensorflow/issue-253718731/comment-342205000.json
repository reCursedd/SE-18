{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/342205000", "html_url": "https://github.com/tensorflow/tensorflow/issues/12686#issuecomment-342205000", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12686", "id": 342205000, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjIwNTAwMA==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-06T16:32:36Z", "updated_at": "2017-11-06T16:32:36Z", "author_association": "MEMBER", "body_html": "<p>Apologies for the delayed response, you don't need to use the <code>.backprop</code> in your test. You just write the usage of the op as normal. The gradient checker will add gradients and verify that they match the numerical gradient.</p>\n<p>Here is an example of another test in that same file that does this. The only thing you need to make sure of is that the x shape and y shape match what SoftMaxWithCrossEntropyLogits does, which i believe you have correct.</p>\n<pre><code>TEST_F(NNGradTest, Conv2DGrad) {\n  TensorShape shape({1, 2, 2, 1});\n  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\n  Tensor filter = test::AsTensor&lt;float&gt;({0.5f}, {1, 1, 1, 1});\n  const std::vector&lt;int&gt; strides{1, 1, 1, 1};\n  auto y = Conv2D(scope_, x, filter, strides, \"SAME\");\n  RunTest(x, shape, y, shape);\n}\n</code></pre>", "body_text": "Apologies for the delayed response, you don't need to use the .backprop in your test. You just write the usage of the op as normal. The gradient checker will add gradients and verify that they match the numerical gradient.\nHere is an example of another test in that same file that does this. The only thing you need to make sure of is that the x shape and y shape match what SoftMaxWithCrossEntropyLogits does, which i believe you have correct.\nTEST_F(NNGradTest, Conv2DGrad) {\n  TensorShape shape({1, 2, 2, 1});\n  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\n  Tensor filter = test::AsTensor<float>({0.5f}, {1, 1, 1, 1});\n  const std::vector<int> strides{1, 1, 1, 1};\n  auto y = Conv2D(scope_, x, filter, strides, \"SAME\");\n  RunTest(x, shape, y, shape);\n}", "body": "Apologies for the delayed response, you don't need to use the `.backprop` in your test. You just write the usage of the op as normal. The gradient checker will add gradients and verify that they match the numerical gradient. \r\n\r\nHere is an example of another test in that same file that does this. The only thing you need to make sure of is that the x shape and y shape match what SoftMaxWithCrossEntropyLogits does, which i believe you have correct.\r\n\r\n```\r\nTEST_F(NNGradTest, Conv2DGrad) {\r\n  TensorShape shape({1, 2, 2, 1});\r\n  auto x = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(shape));\r\n  Tensor filter = test::AsTensor<float>({0.5f}, {1, 1, 1, 1});\r\n  const std::vector<int> strides{1, 1, 1, 1};\r\n  auto y = Conv2D(scope_, x, filter, strides, \"SAME\");\r\n  RunTest(x, shape, y, shape);\r\n}\r\n```"}