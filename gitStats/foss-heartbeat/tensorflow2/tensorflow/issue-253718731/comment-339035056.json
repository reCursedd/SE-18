{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/339035056", "html_url": "https://github.com/tensorflow/tensorflow/issues/12686#issuecomment-339035056", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12686", "id": 339035056, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTAzNTA1Ng==", "user": {"login": "pbanavara", "id": 600054, "node_id": "MDQ6VXNlcjYwMDA1NA==", "avatar_url": "https://avatars3.githubusercontent.com/u/600054?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pbanavara", "html_url": "https://github.com/pbanavara", "followers_url": "https://api.github.com/users/pbanavara/followers", "following_url": "https://api.github.com/users/pbanavara/following{/other_user}", "gists_url": "https://api.github.com/users/pbanavara/gists{/gist_id}", "starred_url": "https://api.github.com/users/pbanavara/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pbanavara/subscriptions", "organizations_url": "https://api.github.com/users/pbanavara/orgs", "repos_url": "https://api.github.com/users/pbanavara/repos", "events_url": "https://api.github.com/users/pbanavara/events{/privacy}", "received_events_url": "https://api.github.com/users/pbanavara/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-24T15:42:34Z", "updated_at": "2017-10-24T15:42:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hey guys - I got around to making some progress. Need some help and apologies for super noob questions. This is what I have done to port the Python code</p>\n<pre><code>Status SoftmaxCrossEntropyWithLogitsGrad(const Scope&amp; scope,\n                                          const Operation&amp; op,\n                                          const std::vector&lt;Output&gt;&amp;\n                                          grad_inputs,\n                                          std::vector&lt;Output&gt;* grad_outputs) {\n  // Softmax gradient with cross entropy logits function\n  // We multiply the backprop for cost with the gradients - op.output[1]\n  // There is no gradient for labels\n\n  auto softmaxGrad = op.output(1);\n  auto gradLoss = grad_inputs[0];\n  auto gradGrad = grad_inputs[1];\n\n  auto grad = Mul(scope, gradLoss, softmaxGrad);\n\n  // TODO Check if the grad is not zero\n\n  auto logits = op.input(0);\n  auto softmax = ops::Softmax(scope, logits);\n\n  auto prod = ops::MatMul(scope, gradGrad, softmax);\n  auto squeezeProd = ops::Squeeze(scope, prod);\n  auto fProd = Sub(scope, gradGrad, squeezeProd);\n  grad = Add(scope, grad, fProd);\n  grad_outputs-&gt;push_back(grad);\n  return scope.status();\n}\n</code></pre>\n<p>I get an error at the line</p>\n<p><code>grad = Add(scope, grad, fProd);</code></p>\n<p>tensorflow/cc/gradients/nn_grad.cc:74:8: error: no viable overloaded '='</p>\n<p>I have totally forgotten about operator overloading. Have I done some fundamental mistake in the code above or do I have to write a operator overloader for =.</p>\n<p>I don't think I can do something like<br>\n<code>grad = grad + fProd;</code></p>", "body_text": "Hey guys - I got around to making some progress. Need some help and apologies for super noob questions. This is what I have done to port the Python code\nStatus SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope,\n                                          const Operation& op,\n                                          const std::vector<Output>&\n                                          grad_inputs,\n                                          std::vector<Output>* grad_outputs) {\n  // Softmax gradient with cross entropy logits function\n  // We multiply the backprop for cost with the gradients - op.output[1]\n  // There is no gradient for labels\n\n  auto softmaxGrad = op.output(1);\n  auto gradLoss = grad_inputs[0];\n  auto gradGrad = grad_inputs[1];\n\n  auto grad = Mul(scope, gradLoss, softmaxGrad);\n\n  // TODO Check if the grad is not zero\n\n  auto logits = op.input(0);\n  auto softmax = ops::Softmax(scope, logits);\n\n  auto prod = ops::MatMul(scope, gradGrad, softmax);\n  auto squeezeProd = ops::Squeeze(scope, prod);\n  auto fProd = Sub(scope, gradGrad, squeezeProd);\n  grad = Add(scope, grad, fProd);\n  grad_outputs->push_back(grad);\n  return scope.status();\n}\n\nI get an error at the line\ngrad = Add(scope, grad, fProd);\ntensorflow/cc/gradients/nn_grad.cc:74:8: error: no viable overloaded '='\nI have totally forgotten about operator overloading. Have I done some fundamental mistake in the code above or do I have to write a operator overloader for =.\nI don't think I can do something like\ngrad = grad + fProd;", "body": "Hey guys - I got around to making some progress. Need some help and apologies for super noob questions. This is what I have done to port the Python code\r\n\r\n```\r\nStatus SoftmaxCrossEntropyWithLogitsGrad(const Scope& scope,\r\n                                          const Operation& op,\r\n                                          const std::vector<Output>&\r\n                                          grad_inputs,\r\n                                          std::vector<Output>* grad_outputs) {\r\n  // Softmax gradient with cross entropy logits function\r\n  // We multiply the backprop for cost with the gradients - op.output[1]\r\n  // There is no gradient for labels\r\n\r\n  auto softmaxGrad = op.output(1);\r\n  auto gradLoss = grad_inputs[0];\r\n  auto gradGrad = grad_inputs[1];\r\n\r\n  auto grad = Mul(scope, gradLoss, softmaxGrad);\r\n\r\n  // TODO Check if the grad is not zero\r\n\r\n  auto logits = op.input(0);\r\n  auto softmax = ops::Softmax(scope, logits);\r\n\r\n  auto prod = ops::MatMul(scope, gradGrad, softmax);\r\n  auto squeezeProd = ops::Squeeze(scope, prod);\r\n  auto fProd = Sub(scope, gradGrad, squeezeProd);\r\n  grad = Add(scope, grad, fProd);\r\n  grad_outputs->push_back(grad);\r\n  return scope.status();\r\n}\r\n```\r\nI get an error at the line\r\n\r\n`grad = Add(scope, grad, fProd);`\r\n\r\ntensorflow/cc/gradients/nn_grad.cc:74:8: error: no viable overloaded '='\r\n\r\nI have totally forgotten about operator overloading. Have I done some fundamental mistake in the code above or do I have to write a operator overloader for =. \r\n\r\nI don't think I can do something like \r\n`grad = grad + fProd;`"}