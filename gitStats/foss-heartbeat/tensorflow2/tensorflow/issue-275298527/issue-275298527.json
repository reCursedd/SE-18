{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14713", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14713/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14713/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14713/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14713", "id": 275298527, "node_id": "MDU6SXNzdWUyNzUyOTg1Mjc=", "number": 14713, "title": "Estimator API and transfer learning/fine-tuning", "user": {"login": "jmaye", "id": 2109115, "node_id": "MDQ6VXNlcjIxMDkxMTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/2109115?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmaye", "html_url": "https://github.com/jmaye", "followers_url": "https://api.github.com/users/jmaye/followers", "following_url": "https://api.github.com/users/jmaye/following{/other_user}", "gists_url": "https://api.github.com/users/jmaye/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmaye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmaye/subscriptions", "organizations_url": "https://api.github.com/users/jmaye/orgs", "repos_url": "https://api.github.com/users/jmaye/repos", "events_url": "https://api.github.com/users/jmaye/events{/privacy}", "received_events_url": "https://api.github.com/users/jmaye/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 39, "created_at": "2017-11-20T10:06:12Z", "updated_at": "2018-07-16T09:59:10Z", "closed_at": "2018-02-07T09:24:55Z", "author_association": "NONE", "body_html": "<p>I've been using the Estimator API with the model_fn and input_fn as shown in the official examples (<a href=\"https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py\">https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py</a> for instance).</p>\n<p>This all looks great and wonderful. However, I'm now facing an issue for going further with it. I'd like to use a model trained on a dataset and transfer it to another dataset. In practice, I would like to take the weights from the trained model up to the softmax layer and only initialize randomly this final layer. Then, I can do fine-tuning on the new dataset, which has different labels for instance.</p>\n<p>I haven't found a way to do what I want. Is it something missing in the interface? Can we have something  like a variable list to restore from a checkpoint and some other not? Ideally, it would be also good to specify variables to be frozen. Does that all make sense?</p>", "body_text": "I've been using the Estimator API with the model_fn and input_fn as shown in the official examples (https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py for instance).\nThis all looks great and wonderful. However, I'm now facing an issue for going further with it. I'd like to use a model trained on a dataset and transfer it to another dataset. In practice, I would like to take the weights from the trained model up to the softmax layer and only initialize randomly this final layer. Then, I can do fine-tuning on the new dataset, which has different labels for instance.\nI haven't found a way to do what I want. Is it something missing in the interface? Can we have something  like a variable list to restore from a checkpoint and some other not? Ideally, it would be also good to specify variables to be frozen. Does that all make sense?", "body": "I've been using the Estimator API with the model_fn and input_fn as shown in the official examples (https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py for instance).\r\n\r\nThis all looks great and wonderful. However, I'm now facing an issue for going further with it. I'd like to use a model trained on a dataset and transfer it to another dataset. In practice, I would like to take the weights from the trained model up to the softmax layer and only initialize randomly this final layer. Then, I can do fine-tuning on the new dataset, which has different labels for instance.\r\n\r\nI haven't found a way to do what I want. Is it something missing in the interface? Can we have something  like a variable list to restore from a checkpoint and some other not? Ideally, it would be also good to specify variables to be frozen. Does that all make sense?"}