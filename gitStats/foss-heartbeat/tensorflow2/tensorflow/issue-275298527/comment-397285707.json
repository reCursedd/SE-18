{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/397285707", "html_url": "https://github.com/tensorflow/tensorflow/issues/14713#issuecomment-397285707", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14713", "id": 397285707, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzI4NTcwNw==", "user": {"login": "rharish101", "id": 25344287, "node_id": "MDQ6VXNlcjI1MzQ0Mjg3", "avatar_url": "https://avatars0.githubusercontent.com/u/25344287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rharish101", "html_url": "https://github.com/rharish101", "followers_url": "https://api.github.com/users/rharish101/followers", "following_url": "https://api.github.com/users/rharish101/following{/other_user}", "gists_url": "https://api.github.com/users/rharish101/gists{/gist_id}", "starred_url": "https://api.github.com/users/rharish101/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rharish101/subscriptions", "organizations_url": "https://api.github.com/users/rharish101/orgs", "repos_url": "https://api.github.com/users/rharish101/repos", "events_url": "https://api.github.com/users/rharish101/events{/privacy}", "received_events_url": "https://api.github.com/users/rharish101/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-14T12:57:09Z", "updated_at": "2018-06-14T13:23:29Z", "author_association": "NONE", "body_html": "<p>Hello!</p>\n<p>I've been trying to use warm start to load my model and fine-tune it on some other dataset. It works fine, except when I want to \"freeze\" layers in the model by making the relevant variables non-trainable, and train only the other unfrozen layers. In this case, the non-trainable variables do not get warm-started, even when the variable names match. It also gives an error when I specify those variables manually in the dictionary for <code>var_name_to_prev_var_name</code> argument.</p>\n<p>Is there any way to warm-start the non-trainable variables?</p>", "body_text": "Hello!\nI've been trying to use warm start to load my model and fine-tune it on some other dataset. It works fine, except when I want to \"freeze\" layers in the model by making the relevant variables non-trainable, and train only the other unfrozen layers. In this case, the non-trainable variables do not get warm-started, even when the variable names match. It also gives an error when I specify those variables manually in the dictionary for var_name_to_prev_var_name argument.\nIs there any way to warm-start the non-trainable variables?", "body": "Hello!\r\n\r\nI've been trying to use warm start to load my model and fine-tune it on some other dataset. It works fine, except when I want to \"freeze\" layers in the model by making the relevant variables non-trainable, and train only the other unfrozen layers. In this case, the non-trainable variables do not get warm-started, even when the variable names match. It also gives an error when I specify those variables manually in the dictionary for `var_name_to_prev_var_name` argument.\r\n\r\nIs there any way to warm-start the non-trainable variables?"}