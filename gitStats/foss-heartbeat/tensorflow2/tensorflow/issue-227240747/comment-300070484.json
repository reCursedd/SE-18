{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300070484", "html_url": "https://github.com/tensorflow/tensorflow/issues/9777#issuecomment-300070484", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9777", "id": 300070484, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDA3MDQ4NA==", "user": {"login": "chengdianxuezi", "id": 10277403, "node_id": "MDQ6VXNlcjEwMjc3NDAz", "avatar_url": "https://avatars1.githubusercontent.com/u/10277403?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chengdianxuezi", "html_url": "https://github.com/chengdianxuezi", "followers_url": "https://api.github.com/users/chengdianxuezi/followers", "following_url": "https://api.github.com/users/chengdianxuezi/following{/other_user}", "gists_url": "https://api.github.com/users/chengdianxuezi/gists{/gist_id}", "starred_url": "https://api.github.com/users/chengdianxuezi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chengdianxuezi/subscriptions", "organizations_url": "https://api.github.com/users/chengdianxuezi/orgs", "repos_url": "https://api.github.com/users/chengdianxuezi/repos", "events_url": "https://api.github.com/users/chengdianxuezi/events{/privacy}", "received_events_url": "https://api.github.com/users/chengdianxuezi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-09T06:09:59Z", "updated_at": "2017-05-09T06:09:59Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5105569\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/suiyuan2009\">@suiyuan2009</a>  I guess the combination of embedding_lookup_sparse is done in the ps, so the result of embedding_lookup_sparse  is very small ,the shape about [1000,128], but the result of embedding_lookup is a dense tensor,and the shape is [1000,20,128].  the bottleneck is ps network</p>", "body_text": "@suiyuan2009  I guess the combination of embedding_lookup_sparse is done in the ps, so the result of embedding_lookup_sparse  is very small ,the shape about [1000,128], but the result of embedding_lookup is a dense tensor,and the shape is [1000,20,128].  the bottleneck is ps network", "body": "@suiyuan2009  I guess the combination of embedding_lookup_sparse is done in the ps, so the result of embedding_lookup_sparse  is very small ,the shape about [1000,128], but the result of embedding_lookup is a dense tensor,and the shape is [1000,20,128].  the bottleneck is ps network"}