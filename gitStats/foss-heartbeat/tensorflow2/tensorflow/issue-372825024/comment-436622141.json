{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436622141", "html_url": "https://github.com/tensorflow/tensorflow/issues/23182#issuecomment-436622141", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23182", "id": 436622141, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjYyMjE0MQ==", "user": {"login": "Efaq", "id": 42271354, "node_id": "MDQ6VXNlcjQyMjcxMzU0", "avatar_url": "https://avatars2.githubusercontent.com/u/42271354?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Efaq", "html_url": "https://github.com/Efaq", "followers_url": "https://api.github.com/users/Efaq/followers", "following_url": "https://api.github.com/users/Efaq/following{/other_user}", "gists_url": "https://api.github.com/users/Efaq/gists{/gist_id}", "starred_url": "https://api.github.com/users/Efaq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Efaq/subscriptions", "organizations_url": "https://api.github.com/users/Efaq/orgs", "repos_url": "https://api.github.com/users/Efaq/repos", "events_url": "https://api.github.com/users/Efaq/events{/privacy}", "received_events_url": "https://api.github.com/users/Efaq/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-07T13:28:00Z", "updated_at": "2018-11-07T13:30:01Z", "author_association": "NONE", "body_html": "<p>I have also come across inconsistencies when batch-evaluating a model against a dataset. I am using the tf.Estimator framework with tf.data. The loss obtained from the <code>evaluate</code> method can get very different depending solely on the batch size. The reason why I need to batch is because some bigger datasets don't fit in my GPU. BUT I also tested the same evaluation routine disabling GPU, and the problem persists, see below.<br>\nSome info:<br>\nDataset size: 18254<br>\nHow I call <code>evaluate</code>:</p>\n<pre><code>data = classifier.evaluate(\n    lambda: input_eval_function(\n        csv_used=\"my.csv\"\n    ),\n    name='eval_test',\n    steps=None\n)\nprint(data)\n</code></pre>\n<p>How my input function looks like:</p>\n<pre><code>def input_eval_function(csv_used):\n    dataset = tf.data.TextLineDataset(filenames=csv_used).skip(1)\n    dataset = dataset.map(_parse_lines_from_csv).batch(**BATCH_SIZE**, drop_remainder=False)\n    return dataset\n</code></pre>\n<p>Some pairs of (BATCH_SIZE, loss) when running with GPU:<br>\nBATCH_SIZE = 1, loss = 135.25769<br>\nBATCH_SIZE = 100, loss = 135.41405<br>\nBATCH_SIZE = 1000, loss = 148.7542<br>\nBATCH_SIZE = 5000, loss = 143.71754<br>\nBATCH_SIZE = 9127 (half dataset), loss = 135.29721<br>\nBATCH_SIZE = 11000, loss = 142.64745<br>\nBATCH_SIZE = 15000, loss = 173.06819<br>\nBATCH_SIZE = 18000, loss = 303.47687<br>\nBATCH_SIZE = 18100, loss = 128.06966<br>\nBATCH_SIZE = 18254 (whole dataset), loss = 135.29533<br>\nBATCH_SIZE = 20000 (buffer bigger than dataset), loss = 135.29533</p>\n<p>OBS1: The results are consistent per batch size, that is, running the code multiple times using the same batch size always yields the same results.</p>\n<p>The results using GPU are generally inconsistent with the results without GPU. This happens when I turn off GPU usage by adding the line <code>os.environ['CUDA_VISIBLE_DEVICES'] = '-1'</code>:<br>\nBATCH_SIZE = 1, loss = 135.25769 -&gt;consistent or no notable difference when compared to GPU<br>\nBATCH_SIZE = 100, loss = 135.41402 -&gt;+- consistent with difference in 0.00001<br>\nBATCH_SIZE = 1000, loss = 148.75395 -&gt;+- consistent with difference in 0.001<br>\nBATCH_SIZE = 5000, loss = 143.70166 -&gt;+- (in)consistent with difference in 0.01<br>\nBATCH_SIZE = 9127 (half dataset), loss = 135.20308 -&gt;inconsistent with difference in 0.1<br>\nBATCH_SIZE = 11000, loss = 142.51755 -&gt;inconsistent with difference in 0.1<br>\nBATCH_SIZE = 15000, loss = 172.92397 -&gt;inconsistent with difference in 1.0<br>\nBATCH_SIZE = 18000, loss = 302.61243 -&gt;inconsistent with difference in 1.0<br>\nBATCH_SIZE = 18100, loss = 127.067825 -&gt;inconsistent with difference in 1.0<br>\nBATCH_SIZE = 18254 (whole dataset), loss = 133.20801 -&gt;inconsistent with difference in 1.0<br>\nBATCH_SIZE = 20000 (buffer bigger than dataset), loss = 133.20801 -&gt;inconsistent with difference in 1.0</p>\n<p>That is:</p>\n<p>1)I confirm that I am also experiencing different evaluation results when using different sizes of batch, even though I am using steps=None, which should imply using the whole dataset.<br>\n2)The results with enabled GPU are slightly different from the ones without using GPU.</p>\n<p>Number 2 may be linked to how computation is performed in GPU in contrast with CPU, so I am not really concerned, nor it is the aim of this github issue. BUT number 1 is concerning: changing the batch size is having a huge impact on the value of the calculated loss.</p>\n<p>obs: I went further and compared these results with the ones obtained using the package tensorflow, that is , not tensorflow-gpu. These seem to be equal to the ones obtained using tensorflow-gpu with GPU disabled (expected).</p>", "body_text": "I have also come across inconsistencies when batch-evaluating a model against a dataset. I am using the tf.Estimator framework with tf.data. The loss obtained from the evaluate method can get very different depending solely on the batch size. The reason why I need to batch is because some bigger datasets don't fit in my GPU. BUT I also tested the same evaluation routine disabling GPU, and the problem persists, see below.\nSome info:\nDataset size: 18254\nHow I call evaluate:\ndata = classifier.evaluate(\n    lambda: input_eval_function(\n        csv_used=\"my.csv\"\n    ),\n    name='eval_test',\n    steps=None\n)\nprint(data)\n\nHow my input function looks like:\ndef input_eval_function(csv_used):\n    dataset = tf.data.TextLineDataset(filenames=csv_used).skip(1)\n    dataset = dataset.map(_parse_lines_from_csv).batch(**BATCH_SIZE**, drop_remainder=False)\n    return dataset\n\nSome pairs of (BATCH_SIZE, loss) when running with GPU:\nBATCH_SIZE = 1, loss = 135.25769\nBATCH_SIZE = 100, loss = 135.41405\nBATCH_SIZE = 1000, loss = 148.7542\nBATCH_SIZE = 5000, loss = 143.71754\nBATCH_SIZE = 9127 (half dataset), loss = 135.29721\nBATCH_SIZE = 11000, loss = 142.64745\nBATCH_SIZE = 15000, loss = 173.06819\nBATCH_SIZE = 18000, loss = 303.47687\nBATCH_SIZE = 18100, loss = 128.06966\nBATCH_SIZE = 18254 (whole dataset), loss = 135.29533\nBATCH_SIZE = 20000 (buffer bigger than dataset), loss = 135.29533\nOBS1: The results are consistent per batch size, that is, running the code multiple times using the same batch size always yields the same results.\nThe results using GPU are generally inconsistent with the results without GPU. This happens when I turn off GPU usage by adding the line os.environ['CUDA_VISIBLE_DEVICES'] = '-1':\nBATCH_SIZE = 1, loss = 135.25769 ->consistent or no notable difference when compared to GPU\nBATCH_SIZE = 100, loss = 135.41402 ->+- consistent with difference in 0.00001\nBATCH_SIZE = 1000, loss = 148.75395 ->+- consistent with difference in 0.001\nBATCH_SIZE = 5000, loss = 143.70166 ->+- (in)consistent with difference in 0.01\nBATCH_SIZE = 9127 (half dataset), loss = 135.20308 ->inconsistent with difference in 0.1\nBATCH_SIZE = 11000, loss = 142.51755 ->inconsistent with difference in 0.1\nBATCH_SIZE = 15000, loss = 172.92397 ->inconsistent with difference in 1.0\nBATCH_SIZE = 18000, loss = 302.61243 ->inconsistent with difference in 1.0\nBATCH_SIZE = 18100, loss = 127.067825 ->inconsistent with difference in 1.0\nBATCH_SIZE = 18254 (whole dataset), loss = 133.20801 ->inconsistent with difference in 1.0\nBATCH_SIZE = 20000 (buffer bigger than dataset), loss = 133.20801 ->inconsistent with difference in 1.0\nThat is:\n1)I confirm that I am also experiencing different evaluation results when using different sizes of batch, even though I am using steps=None, which should imply using the whole dataset.\n2)The results with enabled GPU are slightly different from the ones without using GPU.\nNumber 2 may be linked to how computation is performed in GPU in contrast with CPU, so I am not really concerned, nor it is the aim of this github issue. BUT number 1 is concerning: changing the batch size is having a huge impact on the value of the calculated loss.\nobs: I went further and compared these results with the ones obtained using the package tensorflow, that is , not tensorflow-gpu. These seem to be equal to the ones obtained using tensorflow-gpu with GPU disabled (expected).", "body": "I have also come across inconsistencies when batch-evaluating a model against a dataset. I am using the tf.Estimator framework with tf.data. The loss obtained from the `evaluate` method can get very different depending solely on the batch size. The reason why I need to batch is because some bigger datasets don't fit in my GPU. BUT I also tested the same evaluation routine disabling GPU, and the problem persists, see below.\r\nSome info:\r\nDataset size: 18254\r\nHow I call `evaluate`:\r\n```\r\ndata = classifier.evaluate(\r\n    lambda: input_eval_function(\r\n        csv_used=\"my.csv\"\r\n    ),\r\n    name='eval_test',\r\n    steps=None\r\n)\r\nprint(data)\r\n```\r\nHow my input function looks like:\r\n```\r\ndef input_eval_function(csv_used):\r\n    dataset = tf.data.TextLineDataset(filenames=csv_used).skip(1)\r\n    dataset = dataset.map(_parse_lines_from_csv).batch(**BATCH_SIZE**, drop_remainder=False)\r\n    return dataset\r\n```\r\nSome pairs of (BATCH_SIZE, loss) when running with GPU:\r\nBATCH_SIZE = 1, loss = 135.25769\r\nBATCH_SIZE = 100, loss = 135.41405\r\nBATCH_SIZE = 1000, loss = 148.7542\r\nBATCH_SIZE = 5000, loss = 143.71754\r\nBATCH_SIZE = 9127 (half dataset), loss = 135.29721\r\nBATCH_SIZE = 11000, loss = 142.64745\r\nBATCH_SIZE = 15000, loss = 173.06819\r\nBATCH_SIZE = 18000, loss = 303.47687\r\nBATCH_SIZE = 18100, loss = 128.06966\r\nBATCH_SIZE = 18254 (whole dataset), loss = 135.29533\r\nBATCH_SIZE = 20000 (buffer bigger than dataset), loss = 135.29533\r\n\r\nOBS1: The results are consistent per batch size, that is, running the code multiple times using the same batch size always yields the same results.\r\n\r\nThe results using GPU are generally inconsistent with the results without GPU. This happens when I turn off GPU usage by adding the line `os.environ['CUDA_VISIBLE_DEVICES'] = '-1'`:\r\nBATCH_SIZE = 1, loss = 135.25769 ->consistent or no notable difference when compared to GPU\r\nBATCH_SIZE = 100, loss = 135.41402 ->+- consistent with difference in 0.00001\r\nBATCH_SIZE = 1000, loss = 148.75395 ->+- consistent with difference in 0.001\r\nBATCH_SIZE = 5000, loss = 143.70166 ->+- (in)consistent with difference in 0.01\r\nBATCH_SIZE = 9127 (half dataset), loss = 135.20308 ->inconsistent with difference in 0.1\r\nBATCH_SIZE = 11000, loss = 142.51755 ->inconsistent with difference in 0.1\r\nBATCH_SIZE = 15000, loss = 172.92397 ->inconsistent with difference in 1.0\r\nBATCH_SIZE = 18000, loss = 302.61243 ->inconsistent with difference in 1.0\r\nBATCH_SIZE = 18100, loss = 127.067825 ->inconsistent with difference in 1.0\r\nBATCH_SIZE = 18254 (whole dataset), loss = 133.20801 ->inconsistent with difference in 1.0\r\nBATCH_SIZE = 20000 (buffer bigger than dataset), loss = 133.20801 ->inconsistent with difference in 1.0\r\n\r\nThat is:\r\n\r\n1)I confirm that I am also experiencing different evaluation results when using different sizes of batch, even though I am using steps=None, which should imply using the whole dataset.\r\n2)The results with enabled GPU are slightly different from the ones without using GPU.\r\n\r\nNumber 2 may be linked to how computation is performed in GPU in contrast with CPU, so I am not really concerned, nor it is the aim of this github issue. BUT number 1 is concerning: changing the batch size is having a huge impact on the value of the calculated loss.\r\n\r\nobs: I went further and compared these results with the ones obtained using the package tensorflow, that is , not tensorflow-gpu. These seem to be equal to the ones obtained using tensorflow-gpu with GPU disabled (expected)."}