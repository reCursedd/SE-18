{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/254087067", "html_url": "https://github.com/tensorflow/tensorflow/issues/4713#issuecomment-254087067", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4713", "id": 254087067, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NDA4NzA2Nw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-17T00:10:19Z", "updated_at": "2016-10-17T00:10:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There's no built-in API for shutting down a TensorFlow server remotely; the assumption is that whatever mechanism (e.g. a cluster manager) you used to start the processes would also kill them after the job completes.</p>\n<p>If you're feeling adventurous, you could try using TensorFlow coordination mechanisms, like a <code>tf.FIFOQueue</code> to signal when training is complete. The basic idea would be to (1) create a <em>shared</em> queue on one of the PS tasks; (2) on each of the PS tasks, run a dequeue op on that queue instead of calling `server.join(); (3) at the end of training enqueue_many N elements into the queue (where N is the number of parameter servers). If you did that, the PS tasks would all unblock when training was done, and exit.</p>", "body_text": "There's no built-in API for shutting down a TensorFlow server remotely; the assumption is that whatever mechanism (e.g. a cluster manager) you used to start the processes would also kill them after the job completes.\nIf you're feeling adventurous, you could try using TensorFlow coordination mechanisms, like a tf.FIFOQueue to signal when training is complete. The basic idea would be to (1) create a shared queue on one of the PS tasks; (2) on each of the PS tasks, run a dequeue op on that queue instead of calling `server.join(); (3) at the end of training enqueue_many N elements into the queue (where N is the number of parameter servers). If you did that, the PS tasks would all unblock when training was done, and exit.", "body": "There's no built-in API for shutting down a TensorFlow server remotely; the assumption is that whatever mechanism (e.g. a cluster manager) you used to start the processes would also kill them after the job completes.\n\nIf you're feeling adventurous, you could try using TensorFlow coordination mechanisms, like a `tf.FIFOQueue` to signal when training is complete. The basic idea would be to (1) create a _shared_ queue on one of the PS tasks; (2) on each of the PS tasks, run a dequeue op on that queue instead of calling `server.join(); (3) at the end of training enqueue_many N elements into the queue (where N is the number of parameter servers). If you did that, the PS tasks would all unblock when training was done, and exit.\n"}