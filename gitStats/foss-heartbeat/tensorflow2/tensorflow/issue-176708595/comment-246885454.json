{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/246885454", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-246885454", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 246885454, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Njg4NTQ1NA==", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-14T02:15:03Z", "updated_at": "2016-09-14T02:15:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Yes, I do mean self-contained code changes: a user should be able to use an existing binary install of TensorFlow that doesn't contain the custom device code, but then call a function to \"load a device\" and then another function to \"load the kernels\" for that device.</p>\n<p>The latter is already possible for existing devices via the tf.load_op_library() mechanism, so theoretically something similar could be done for a new tf.load_device().</p>\n<p>I'll answer your other question on SO, but I don't think the answer there will be instructive for new devices.  Every device has its own execution model, and so the way the device code is written needs to take into account the execution model.  On CPU, the ThreadPoolDevice is just a device that uses Eigen's multi-threaded CPU support to implement operations.  On GPU, we have to manage streams and execution order carefully because GPU devices are asynchronous, not synchronous.</p>\n<p>If you told us more about the execution model of your hardware, we might be able to suggest something more relevant -- it's likely that copying the CPU or the GPU way of doing things is not the right solution.</p>", "body_text": "Yes, I do mean self-contained code changes: a user should be able to use an existing binary install of TensorFlow that doesn't contain the custom device code, but then call a function to \"load a device\" and then another function to \"load the kernels\" for that device.\nThe latter is already possible for existing devices via the tf.load_op_library() mechanism, so theoretically something similar could be done for a new tf.load_device().\nI'll answer your other question on SO, but I don't think the answer there will be instructive for new devices.  Every device has its own execution model, and so the way the device code is written needs to take into account the execution model.  On CPU, the ThreadPoolDevice is just a device that uses Eigen's multi-threaded CPU support to implement operations.  On GPU, we have to manage streams and execution order carefully because GPU devices are asynchronous, not synchronous.\nIf you told us more about the execution model of your hardware, we might be able to suggest something more relevant -- it's likely that copying the CPU or the GPU way of doing things is not the right solution.", "body": "Yes, I do mean self-contained code changes: a user should be able to use an existing binary install of TensorFlow that doesn't contain the custom device code, but then call a function to \"load a device\" and then another function to \"load the kernels\" for that device.\n\nThe latter is already possible for existing devices via the tf.load_op_library() mechanism, so theoretically something similar could be done for a new tf.load_device().\n\nI'll answer your other question on SO, but I don't think the answer there will be instructive for new devices.  Every device has its own execution model, and so the way the device code is written needs to take into account the execution model.  On CPU, the ThreadPoolDevice is just a device that uses Eigen's multi-threaded CPU support to implement operations.  On GPU, we have to manage streams and execution order carefully because GPU devices are asynchronous, not synchronous.\n\nIf you told us more about the execution model of your hardware, we might be able to suggest something more relevant -- it's likely that copying the CPU or the GPU way of doing things is not the right solution.\n"}