{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281544095", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-281544095", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 281544095, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTU0NDA5NQ==", "user": {"login": "RichDubielzig", "id": 24554323, "node_id": "MDQ6VXNlcjI0NTU0MzIz", "avatar_url": "https://avatars2.githubusercontent.com/u/24554323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RichDubielzig", "html_url": "https://github.com/RichDubielzig", "followers_url": "https://api.github.com/users/RichDubielzig/followers", "following_url": "https://api.github.com/users/RichDubielzig/following{/other_user}", "gists_url": "https://api.github.com/users/RichDubielzig/gists{/gist_id}", "starred_url": "https://api.github.com/users/RichDubielzig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RichDubielzig/subscriptions", "organizations_url": "https://api.github.com/users/RichDubielzig/orgs", "repos_url": "https://api.github.com/users/RichDubielzig/repos", "events_url": "https://api.github.com/users/RichDubielzig/events{/privacy}", "received_events_url": "https://api.github.com/users/RichDubielzig/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-22T01:58:31Z", "updated_at": "2017-02-22T01:58:31Z", "author_association": "NONE", "body_html": "<p>I'm afraid I'm having trouble understanding how to handle synchronization and asynchronous kernel resources when developing in TensorFlow using TensorFlow objects.  Here is my problem:</p>\n<p>The knureon system can be thought of as a large pool of completely independent cores which can be assigned operations at any time.  So, for example, given the data flow below:</p>\n<p>A: m = a * b<br>\nB: n = c *d<br>\nC: o = e * f</p>\n<p>If I have 128 available cores, then I might request A to run on 64 cores, B to run on another 64 cores, and then I would need to wait for one of the first two operations to complete before I can C on 64 cores.  To this end, I have created a little launch queue object that can hold computation requests until compute resources are available, then run.  In pseudocode, my naive OpKernel Compute() implementation is below.  Note that this employes a Knureon-specific command queue object which can be used both to launch operations and to pend on running ones.  Multiple queues are allowed to exist and run in parallel on a single Knureon system.</p>\n<pre><code>Compute( context  input):\n  Get input tensors A &amp; B from context\n  Also get a Knureon command queue handle Q from the DeviceContext object.\n  Figure out the dimensions of the tensors\n  Assemble a call f(A,B,dims) to my Knureon matrix multipllier algorithm\n  Queue the tuple (f(),Q) in the launch queue.\n  We can now block on Q until f() completes.\n\n(in launch queue, in a separate thread:)\n  Wait for resources to run f()\n  dispatch f() using the queue Q on the Knureon complex.\n</code></pre>\n<p>The problem should be apparent:  I don't want TensorFlow to sit around waiting on Q if there are other operations that can be deployed right away on available resources.  The alternative <em>appears</em> to be to use an asynchronous opkernel:</p>\n<pre><code>ComputeAsync( context  input, DoneCallback done):\n  Get input tensors A &amp; B from context\n  Also get a Knureon command queue handle Q from the DeviceContext object.\n  Figure out the dimensions of the tensors\n  Assemble a call f(A,B,dims) to my Knureon matrix multipllier algorithm\n  Queue the tuple (f(), Q, done) in the launch queue.\n  Return immediately\n\n(in launch queue, in a separate thread:)\n  Wait for resources to run f()\n  run f() using Q on the Knureon complex and spawn a task that will wait on Q.  This task will call the ComputeAsync done() call back when f() is complete.\n</code></pre>\n<p>But I am not sure if this is the right approach, as it seems to be reserved for high-latency operations such as receiving over a network.  I've been over and through the code, and unfortunately this has only increased my confusion.  I see that the GPU uses a StreamExecutor, but the presence of all the ThenXxxx() functions in the Stream prototype makes me suspect that it is not what I want.</p>\n<p>I have also noticed that OpKernel Compute() methods can be called in parallel from concurrent Executor threads.  So do I even need to sweat about parallel execution at all? When an OpKernel's Compute() method is invoked, am I guaranteed that there will be no benefit to running asynchronously because all the other OpKernels managed by my threads' Executor have data dependencies on my operation?</p>\n<p>Thank you in advance and my apologies for rambling.  I've had to spend a few days figuring out how to phrase this question in a coherent manner, and I'm not sure I've met my goal, so if you need anything clarified please let me know.</p>", "body_text": "I'm afraid I'm having trouble understanding how to handle synchronization and asynchronous kernel resources when developing in TensorFlow using TensorFlow objects.  Here is my problem:\nThe knureon system can be thought of as a large pool of completely independent cores which can be assigned operations at any time.  So, for example, given the data flow below:\nA: m = a * b\nB: n = c *d\nC: o = e * f\nIf I have 128 available cores, then I might request A to run on 64 cores, B to run on another 64 cores, and then I would need to wait for one of the first two operations to complete before I can C on 64 cores.  To this end, I have created a little launch queue object that can hold computation requests until compute resources are available, then run.  In pseudocode, my naive OpKernel Compute() implementation is below.  Note that this employes a Knureon-specific command queue object which can be used both to launch operations and to pend on running ones.  Multiple queues are allowed to exist and run in parallel on a single Knureon system.\nCompute( context  input):\n  Get input tensors A & B from context\n  Also get a Knureon command queue handle Q from the DeviceContext object.\n  Figure out the dimensions of the tensors\n  Assemble a call f(A,B,dims) to my Knureon matrix multipllier algorithm\n  Queue the tuple (f(),Q) in the launch queue.\n  We can now block on Q until f() completes.\n\n(in launch queue, in a separate thread:)\n  Wait for resources to run f()\n  dispatch f() using the queue Q on the Knureon complex.\n\nThe problem should be apparent:  I don't want TensorFlow to sit around waiting on Q if there are other operations that can be deployed right away on available resources.  The alternative appears to be to use an asynchronous opkernel:\nComputeAsync( context  input, DoneCallback done):\n  Get input tensors A & B from context\n  Also get a Knureon command queue handle Q from the DeviceContext object.\n  Figure out the dimensions of the tensors\n  Assemble a call f(A,B,dims) to my Knureon matrix multipllier algorithm\n  Queue the tuple (f(), Q, done) in the launch queue.\n  Return immediately\n\n(in launch queue, in a separate thread:)\n  Wait for resources to run f()\n  run f() using Q on the Knureon complex and spawn a task that will wait on Q.  This task will call the ComputeAsync done() call back when f() is complete.\n\nBut I am not sure if this is the right approach, as it seems to be reserved for high-latency operations such as receiving over a network.  I've been over and through the code, and unfortunately this has only increased my confusion.  I see that the GPU uses a StreamExecutor, but the presence of all the ThenXxxx() functions in the Stream prototype makes me suspect that it is not what I want.\nI have also noticed that OpKernel Compute() methods can be called in parallel from concurrent Executor threads.  So do I even need to sweat about parallel execution at all? When an OpKernel's Compute() method is invoked, am I guaranteed that there will be no benefit to running asynchronously because all the other OpKernels managed by my threads' Executor have data dependencies on my operation?\nThank you in advance and my apologies for rambling.  I've had to spend a few days figuring out how to phrase this question in a coherent manner, and I'm not sure I've met my goal, so if you need anything clarified please let me know.", "body": "I'm afraid I'm having trouble understanding how to handle synchronization and asynchronous kernel resources when developing in TensorFlow using TensorFlow objects.  Here is my problem:\r\n\r\nThe knureon system can be thought of as a large pool of completely independent cores which can be assigned operations at any time.  So, for example, given the data flow below:\r\n\r\nA: m = a * b\r\nB: n = c *d\r\nC: o = e * f\r\n\r\nIf I have 128 available cores, then I might request A to run on 64 cores, B to run on another 64 cores, and then I would need to wait for one of the first two operations to complete before I can C on 64 cores.  To this end, I have created a little launch queue object that can hold computation requests until compute resources are available, then run.  In pseudocode, my naive OpKernel Compute() implementation is below.  Note that this employes a Knureon-specific command queue object which can be used both to launch operations and to pend on running ones.  Multiple queues are allowed to exist and run in parallel on a single Knureon system.\r\n\r\n```\r\nCompute( context  input):\r\n  Get input tensors A & B from context\r\n  Also get a Knureon command queue handle Q from the DeviceContext object.\r\n  Figure out the dimensions of the tensors\r\n  Assemble a call f(A,B,dims) to my Knureon matrix multipllier algorithm\r\n  Queue the tuple (f(),Q) in the launch queue.\r\n  We can now block on Q until f() completes.\r\n\r\n(in launch queue, in a separate thread:)\r\n  Wait for resources to run f()\r\n  dispatch f() using the queue Q on the Knureon complex.\r\n```\r\n\r\nThe problem should be apparent:  I don't want TensorFlow to sit around waiting on Q if there are other operations that can be deployed right away on available resources.  The alternative *appears* to be to use an asynchronous opkernel:\r\n\r\n```\r\nComputeAsync( context  input, DoneCallback done):\r\n  Get input tensors A & B from context\r\n  Also get a Knureon command queue handle Q from the DeviceContext object.\r\n  Figure out the dimensions of the tensors\r\n  Assemble a call f(A,B,dims) to my Knureon matrix multipllier algorithm\r\n  Queue the tuple (f(), Q, done) in the launch queue.\r\n  Return immediately\r\n\r\n(in launch queue, in a separate thread:)\r\n  Wait for resources to run f()\r\n  run f() using Q on the Knureon complex and spawn a task that will wait on Q.  This task will call the ComputeAsync done() call back when f() is complete.\r\n```\r\nBut I am not sure if this is the right approach, as it seems to be reserved for high-latency operations such as receiving over a network.  I've been over and through the code, and unfortunately this has only increased my confusion.  I see that the GPU uses a StreamExecutor, but the presence of all the ThenXxxx() functions in the Stream prototype makes me suspect that it is not what I want.\r\n\r\nI have also noticed that OpKernel Compute() methods can be called in parallel from concurrent Executor threads.  So do I even need to sweat about parallel execution at all? When an OpKernel's Compute() method is invoked, am I guaranteed that there will be no benefit to running asynchronously because all the other OpKernels managed by my threads' Executor have data dependencies on my operation?\r\n\r\nThank you in advance and my apologies for rambling.  I've had to spend a few days figuring out how to phrase this question in a coherent manner, and I'm not sure I've met my goal, so if you need anything clarified please let me know.\r\n\r\n"}