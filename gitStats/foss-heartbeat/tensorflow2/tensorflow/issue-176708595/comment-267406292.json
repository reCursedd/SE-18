{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/267406292", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-267406292", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 267406292, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NzQwNjI5Mg==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-15T18:40:10Z", "updated_at": "2016-12-15T18:49:18Z", "author_association": "MEMBER", "body_html": "<p>I think this depends a lot on the capabilities of the hardware device you are planning to support.<br>\nThere are several scenarios which I can imagine:</p>\n<ul>\n<li>\n<p>If the hardware is a 'one-trick-pony' accelerator (e.g. it provides a small number of fixed functions) then it may be sensible to expose those from a traditional TF device.  e.g. register new device-specific kernels for existing ops such as MatMul, or if the hardware does some very strange things then add custom ops and ideally some sort of GraphOptimizationPass which uses device-specific knowledge to rewrite existing TF programs to use your ops where it makes sense.</p>\n</li>\n<li>\n<p>The above approach probably works quite well if you're wanting to add some sort of black-box library implementing eg. something like MKL.  You don't want to break portability of people's TF programs, but you want to rewrite them to use custom low-level functionality at runtime.</p>\n</li>\n<li>\n<p>For an accelerator which provides a powerful set of generic computation support it is much easier to implement the small number of primitives required by XLA than the many 100s of ops registered in TensorFlow.  The TF2XLA bridge automatically identifies parts of graphs to hand to XLA and converts the TF ops into XLA primitives.  The set of ops which are currently translatable is enough to handle complex models like inception.</p>\n</li>\n<li>\n<p>Implementing XLA support for a processor supported by LLVM is not very difficult.  For something stranger you will need to write your own XLA backend.  This is strictly easier than implementing the equivalent TF ops, but you will need some compiler expertise.</p>\n</li>\n</ul>\n<p>One caveat with XLA as it currently stands, is that we don't have a way for devices to implement a consistent subset of XLA primitives.  At the moment it's basically 'all or nothing'.  However, I think this will evolve over time.</p>", "body_text": "I think this depends a lot on the capabilities of the hardware device you are planning to support.\nThere are several scenarios which I can imagine:\n\n\nIf the hardware is a 'one-trick-pony' accelerator (e.g. it provides a small number of fixed functions) then it may be sensible to expose those from a traditional TF device.  e.g. register new device-specific kernels for existing ops such as MatMul, or if the hardware does some very strange things then add custom ops and ideally some sort of GraphOptimizationPass which uses device-specific knowledge to rewrite existing TF programs to use your ops where it makes sense.\n\n\nThe above approach probably works quite well if you're wanting to add some sort of black-box library implementing eg. something like MKL.  You don't want to break portability of people's TF programs, but you want to rewrite them to use custom low-level functionality at runtime.\n\n\nFor an accelerator which provides a powerful set of generic computation support it is much easier to implement the small number of primitives required by XLA than the many 100s of ops registered in TensorFlow.  The TF2XLA bridge automatically identifies parts of graphs to hand to XLA and converts the TF ops into XLA primitives.  The set of ops which are currently translatable is enough to handle complex models like inception.\n\n\nImplementing XLA support for a processor supported by LLVM is not very difficult.  For something stranger you will need to write your own XLA backend.  This is strictly easier than implementing the equivalent TF ops, but you will need some compiler expertise.\n\n\nOne caveat with XLA as it currently stands, is that we don't have a way for devices to implement a consistent subset of XLA primitives.  At the moment it's basically 'all or nothing'.  However, I think this will evolve over time.", "body": "I think this depends a lot on the capabilities of the hardware device you are planning to support.\r\nThere are several scenarios which I can imagine:\r\n\r\n- If the hardware is a 'one-trick-pony' accelerator (e.g. it provides a small number of fixed functions) then it may be sensible to expose those from a traditional TF device.  e.g. register new device-specific kernels for existing ops such as MatMul, or if the hardware does some very strange things then add custom ops and ideally some sort of GraphOptimizationPass which uses device-specific knowledge to rewrite existing TF programs to use your ops where it makes sense.\r\n\r\n- The above approach probably works quite well if you're wanting to add some sort of black-box library implementing eg. something like MKL.  You don't want to break portability of people's TF programs, but you want to rewrite them to use custom low-level functionality at runtime.\r\n\r\n- For an accelerator which provides a powerful set of generic computation support it is much easier to implement the small number of primitives required by XLA than the many 100s of ops registered in TensorFlow.  The TF2XLA bridge automatically identifies parts of graphs to hand to XLA and converts the TF ops into XLA primitives.  The set of ops which are currently translatable is enough to handle complex models like inception.    \r\n\r\n- Implementing XLA support for a processor supported by LLVM is not very difficult.  For something stranger you will need to write your own XLA backend.  This is strictly easier than implementing the equivalent TF ops, but you will need some compiler expertise.\r\n\r\nOne caveat with XLA as it currently stands, is that we don't have a way for devices to implement a consistent subset of XLA primitives.  At the moment it's basically 'all or nothing'.  However, I think this will evolve over time. "}