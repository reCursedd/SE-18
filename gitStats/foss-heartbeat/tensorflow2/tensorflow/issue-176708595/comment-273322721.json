{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273322721", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-273322721", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 273322721, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzMyMjcyMQ==", "user": {"login": "RichDubielzig", "id": 24554323, "node_id": "MDQ6VXNlcjI0NTU0MzIz", "avatar_url": "https://avatars2.githubusercontent.com/u/24554323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RichDubielzig", "html_url": "https://github.com/RichDubielzig", "followers_url": "https://api.github.com/users/RichDubielzig/followers", "following_url": "https://api.github.com/users/RichDubielzig/following{/other_user}", "gists_url": "https://api.github.com/users/RichDubielzig/gists{/gist_id}", "starred_url": "https://api.github.com/users/RichDubielzig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RichDubielzig/subscriptions", "organizations_url": "https://api.github.com/users/RichDubielzig/orgs", "repos_url": "https://api.github.com/users/RichDubielzig/repos", "events_url": "https://api.github.com/users/RichDubielzig/events{/privacy}", "received_events_url": "https://api.github.com/users/RichDubielzig/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-17T22:35:29Z", "updated_at": "2017-01-17T22:35:29Z", "author_association": "NONE", "body_html": "<p>I <em>think</em> I've answered my second question.  Looking at compiler/xla/service/gpu/llvm_gpu_backend, I see the function LinkLibdeviceIfNecessary, which takes a directory path as a direct argument and the bitcode filename to link from a configuration argument.  It then does a bitcode-level linkage (which is our preference) against the runtime library and passes the result back to CompileModuleToPtx.</p>\n<p>So it looks like our simplest approach is to create a .bc runtime library and reuse the GPU compilation code to compile to our own ISA.</p>\n<p>A follow-up question: I don't see anything in the GPU linker or compiler that checks for maximum code size.  Is there any mechanism to guarantee or check that a requested compilation block will return a kernel under a given number of instructions?</p>", "body_text": "I think I've answered my second question.  Looking at compiler/xla/service/gpu/llvm_gpu_backend, I see the function LinkLibdeviceIfNecessary, which takes a directory path as a direct argument and the bitcode filename to link from a configuration argument.  It then does a bitcode-level linkage (which is our preference) against the runtime library and passes the result back to CompileModuleToPtx.\nSo it looks like our simplest approach is to create a .bc runtime library and reuse the GPU compilation code to compile to our own ISA.\nA follow-up question: I don't see anything in the GPU linker or compiler that checks for maximum code size.  Is there any mechanism to guarantee or check that a requested compilation block will return a kernel under a given number of instructions?", "body": "I *think* I've answered my second question.  Looking at compiler/xla/service/gpu/llvm_gpu_backend, I see the function LinkLibdeviceIfNecessary, which takes a directory path as a direct argument and the bitcode filename to link from a configuration argument.  It then does a bitcode-level linkage (which is our preference) against the runtime library and passes the result back to CompileModuleToPtx.\r\n\r\nSo it looks like our simplest approach is to create a .bc runtime library and reuse the GPU compilation code to compile to our own ISA.\r\n\r\nA follow-up question: I don't see anything in the GPU linker or compiler that checks for maximum code size.  Is there any mechanism to guarantee or check that a requested compilation block will return a kernel under a given number of instructions?"}