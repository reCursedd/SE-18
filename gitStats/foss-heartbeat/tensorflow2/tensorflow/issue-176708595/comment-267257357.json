{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/267257357", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-267257357", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 267257357, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NzI1NzM1Nw==", "user": {"login": "llhe", "id": 192829, "node_id": "MDQ6VXNlcjE5MjgyOQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/192829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/llhe", "html_url": "https://github.com/llhe", "followers_url": "https://api.github.com/users/llhe/followers", "following_url": "https://api.github.com/users/llhe/following{/other_user}", "gists_url": "https://api.github.com/users/llhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/llhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/llhe/subscriptions", "organizations_url": "https://api.github.com/users/llhe/orgs", "repos_url": "https://api.github.com/users/llhe/repos", "events_url": "https://api.github.com/users/llhe/events{/privacy}", "received_events_url": "https://api.github.com/users/llhe/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-15T07:24:37Z", "updated_at": "2016-12-15T09:21:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>And more specifically, will XLA also target for mobile devices? E.g. with customized DNN acceleration chips.</p>\n<p>Quote from XLA doc:</p>\n<blockquote>\n<p><strong>Improved portability</strong>. The compiler-based framework is designed to target different back-end hardware, including a variety of CPUs, GPUs, and custom accelerator hardware such as TPUs. The CPU and GPU back-ends currently use LLVM, while the internal Google TPU back-end (which will not be open-sourced at this time) uses custom code generation. The goal for this and other accelerators is that it should be relatively easy to write a new back-end for novel hardware, at which point a large fraction of TensorFlow programs will run unmodified on that hardware. This is in contrast with the approach of specializing individual monolithic Ops for new hardware, which requires TensorFlow programs to be rewritten to make use of those Ops.</p>\n</blockquote>\n<p>For a non CPU/GPU device, should the device provider need to write a custom code generator like TPU? I'm wondering whether writing this generator is as simple as adding a Device now mentioned above? Just by adding these <a href=\"https://www.tensorflow.org/versions/master/resources/xla_prerelease#operation_semantics\" rel=\"nofollow\">atomic ops</a> defined in XLA document, or more work involving LLVM backend stuff?</p>", "body_text": "And more specifically, will XLA also target for mobile devices? E.g. with customized DNN acceleration chips.\nQuote from XLA doc:\n\nImproved portability. The compiler-based framework is designed to target different back-end hardware, including a variety of CPUs, GPUs, and custom accelerator hardware such as TPUs. The CPU and GPU back-ends currently use LLVM, while the internal Google TPU back-end (which will not be open-sourced at this time) uses custom code generation. The goal for this and other accelerators is that it should be relatively easy to write a new back-end for novel hardware, at which point a large fraction of TensorFlow programs will run unmodified on that hardware. This is in contrast with the approach of specializing individual monolithic Ops for new hardware, which requires TensorFlow programs to be rewritten to make use of those Ops.\n\nFor a non CPU/GPU device, should the device provider need to write a custom code generator like TPU? I'm wondering whether writing this generator is as simple as adding a Device now mentioned above? Just by adding these atomic ops defined in XLA document, or more work involving LLVM backend stuff?", "body": "And more specifically, will XLA also target for mobile devices? E.g. with customized DNN acceleration chips.\r\n\r\nQuote from XLA doc:\r\n> **Improved portability**. The compiler-based framework is designed to target different back-end hardware, including a variety of CPUs, GPUs, and custom accelerator hardware such as TPUs. The CPU and GPU back-ends currently use LLVM, while the internal Google TPU back-end (which will not be open-sourced at this time) uses custom code generation. The goal for this and other accelerators is that it should be relatively easy to write a new back-end for novel hardware, at which point a large fraction of TensorFlow programs will run unmodified on that hardware. This is in contrast with the approach of specializing individual monolithic Ops for new hardware, which requires TensorFlow programs to be rewritten to make use of those Ops.\r\n\r\nFor a non CPU/GPU device, should the device provider need to write a custom code generator like TPU? I'm wondering whether writing this generator is as simple as adding a Device now mentioned above? Just by adding these [atomic ops](https://www.tensorflow.org/versions/master/resources/xla_prerelease#operation_semantics) defined in XLA document, or more work involving LLVM backend stuff?"}