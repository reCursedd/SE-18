{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/269241038", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-269241038", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 269241038, "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTI0MTAzOA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-26T20:51:11Z", "updated_at": "2016-12-26T22:40:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=24554323\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/RichDubielzig\">@RichDubielzig</a>  It helps to think of different layers of TensorFlow -- there's the client side which creates computational graph, and TensorFlow runtime side which executes it. If your hardware provides implementations for all ops required by TensorFlow runtime, you wouldn't need to know about the client-side. However I can see how you would want to know about typical things done by the client to make sure the hardware performs well for them.</p>\n<p>A typical implementation of training step constructs gradient computation using <code>tf.gradients</code> or related, and updates weight variable <code>var</code> using <code>var.assign_add(learning_rate*gradient)</code> or related.</p>\n<p><code>tf.gradients([a],[b])</code> is a \"client-side\" operation that adds nodes to the graph that give gradient of a with respect to b using reverse mode AD algorithm. In a sense it's not part of core TensorFlow since it's not accessible to people using TensorFlow from other languages. Also, we've been finding that graph created by <code>tf.gradients</code> is a memory hog, and a modified version is sometimes needed to make models fit.</p>\n<p>Here's what a typical graph you get from <code>tf.gradients</code> looks like<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/23068/21486407/f172bc40-cb68-11e6-9b47-f87e85a99293.png\"><img src=\"https://cloud.githubusercontent.com/assets/23068/21486407/f172bc40-cb68-11e6-9b47-f87e85a99293.png\" alt=\"ad-graphs1\" style=\"max-width:100%;\"></a></p>\n<p>Nodes <code>b_i</code> compute left product of a backprop vector with jacobian evaluated at <code>a_i</code>, so you can see that each node has two inputs. This is the core step of \"reverse-mode automatic differentiation\" referred to as <code>l_op</code> in Theano or <code>grad</code> or <code>gradient</code> in TensorFlow. It can correspond to a single op implementing computation in C++, or it can be implemented as a several TensorFlow ops wired together. For instance, <code>grad(a,b)</code> of <code>tf.square</code> is implemented as <code>mul(a, mul(2, b))</code> rather than a fused <code>SquareGrad</code> op.</p>\n<p>From graph you can see that to compute it you need enough memory to store <code>a1/a2/a3</code>. If that's too much, one could use a modified implementation below which saves memory at the cost of recomputing <code>a2</code></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/23068/21486412/0c68954c-cb69-11e6-8467-22c402c529c2.png\"><img src=\"https://cloud.githubusercontent.com/assets/23068/21486412/0c68954c-cb69-11e6-8467-22c402c529c2.png\" alt=\"ad-graphs2\" style=\"max-width:100%;\"></a></p>\n<p>Regarding questions</p>\n<ol>\n<li>You can see the graph you get as follows</li>\n</ol>\n<pre><code>a = tf.placeholder(tf.float32)\nb = tf.atan(a)\ntf.gradients([b],[a])\nprint(tf.get_default_graph().as_graph_def())\n\n</code></pre>\n<p>It seems to be implementing <code>atanh(z)</code> grad using <code>1/(1-z^2)</code> formula.</p>\n<ol start=\"2\">\n<li>\n<p><code>ApplyGradientDescent(var, alpha, delta)</code> will overwrite the value of var<br>\nwith new weight values.</p>\n</li>\n<li>\n<p>GradientDescentOptimizer use translates to <code>ApplyGradientDescent(var, 0.5, gradient)</code> , so <code>0.5</code> is only introduced at the last step</p>\n</li>\n</ol>", "body_text": "@RichDubielzig  It helps to think of different layers of TensorFlow -- there's the client side which creates computational graph, and TensorFlow runtime side which executes it. If your hardware provides implementations for all ops required by TensorFlow runtime, you wouldn't need to know about the client-side. However I can see how you would want to know about typical things done by the client to make sure the hardware performs well for them.\nA typical implementation of training step constructs gradient computation using tf.gradients or related, and updates weight variable var using var.assign_add(learning_rate*gradient) or related.\ntf.gradients([a],[b]) is a \"client-side\" operation that adds nodes to the graph that give gradient of a with respect to b using reverse mode AD algorithm. In a sense it's not part of core TensorFlow since it's not accessible to people using TensorFlow from other languages. Also, we've been finding that graph created by tf.gradients is a memory hog, and a modified version is sometimes needed to make models fit.\nHere's what a typical graph you get from tf.gradients looks like\n\nNodes b_i compute left product of a backprop vector with jacobian evaluated at a_i, so you can see that each node has two inputs. This is the core step of \"reverse-mode automatic differentiation\" referred to as l_op in Theano or grad or gradient in TensorFlow. It can correspond to a single op implementing computation in C++, or it can be implemented as a several TensorFlow ops wired together. For instance, grad(a,b) of tf.square is implemented as mul(a, mul(2, b)) rather than a fused SquareGrad op.\nFrom graph you can see that to compute it you need enough memory to store a1/a2/a3. If that's too much, one could use a modified implementation below which saves memory at the cost of recomputing a2\n\nRegarding questions\n\nYou can see the graph you get as follows\n\na = tf.placeholder(tf.float32)\nb = tf.atan(a)\ntf.gradients([b],[a])\nprint(tf.get_default_graph().as_graph_def())\n\n\nIt seems to be implementing atanh(z) grad using 1/(1-z^2) formula.\n\n\nApplyGradientDescent(var, alpha, delta) will overwrite the value of var\nwith new weight values.\n\n\nGradientDescentOptimizer use translates to ApplyGradientDescent(var, 0.5, gradient) , so 0.5 is only introduced at the last step", "body": "@RichDubielzig  It helps to think of different layers of TensorFlow -- there's the client side which creates computational graph, and TensorFlow runtime side which executes it. If your hardware provides implementations for all ops required by TensorFlow runtime, you wouldn't need to know about the client-side. However I can see how you would want to know about typical things done by the client to make sure the hardware performs well for them.\r\n\r\nA typical implementation of training step constructs gradient computation using `tf.gradients` or related, and updates weight variable `var` using `var.assign_add(learning_rate*gradient)` or related.\r\n\r\n`tf.gradients([a],[b])` is a \"client-side\" operation that adds nodes to the graph that give gradient of a with respect to b using reverse mode AD algorithm. In a sense it's not part of core TensorFlow since it's not accessible to people using TensorFlow from other languages. Also, we've been finding that graph created by `tf.gradients` is a memory hog, and a modified version is sometimes needed to make models fit.\r\n\r\nHere's what a typical graph you get from `tf.gradients` looks like\r\n![ad-graphs1](https://cloud.githubusercontent.com/assets/23068/21486407/f172bc40-cb68-11e6-9b47-f87e85a99293.png)\r\n\r\nNodes `b_i` compute left product of a backprop vector with jacobian evaluated at `a_i`, so you can see that each node has two inputs. This is the core step of \"reverse-mode automatic differentiation\" referred to as `l_op` in Theano or `grad` or `gradient` in TensorFlow. It can correspond to a single op implementing computation in C++, or it can be implemented as a several TensorFlow ops wired together. For instance, `grad(a,b)` of `tf.square` is implemented as `mul(a, mul(2, b))` rather than a fused `SquareGrad` op.\r\n\r\nFrom graph you can see that to compute it you need enough memory to store `a1/a2/a3`. If that's too much, one could use a modified implementation below which saves memory at the cost of recomputing `a2`\r\n\r\n![ad-graphs2](https://cloud.githubusercontent.com/assets/23068/21486412/0c68954c-cb69-11e6-8467-22c402c529c2.png)\r\n\r\nRegarding questions\r\n\r\n1. You can see the graph you get as follows\r\n\r\n```\r\na = tf.placeholder(tf.float32)\r\nb = tf.atan(a)\r\ntf.gradients([b],[a])\r\nprint(tf.get_default_graph().as_graph_def())\r\n\r\n```\r\nIt seems to be implementing `atanh(z)` grad using `1/(1-z^2)` formula.\r\n\r\n2. `ApplyGradientDescent(var, alpha, delta)` will overwrite the value of var\r\nwith new weight values.\r\n\r\n3. GradientDescentOptimizer use translates to `ApplyGradientDescent(var, 0.5, gradient)` , so `0.5` is only introduced at the last step"}