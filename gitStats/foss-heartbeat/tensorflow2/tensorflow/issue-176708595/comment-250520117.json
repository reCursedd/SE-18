{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/250520117", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-250520117", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 250520117, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MDUyMDExNw==", "user": {"login": "aidan-plenert-macdonald", "id": 6690599, "node_id": "MDQ6VXNlcjY2OTA1OTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/6690599?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aidan-plenert-macdonald", "html_url": "https://github.com/aidan-plenert-macdonald", "followers_url": "https://api.github.com/users/aidan-plenert-macdonald/followers", "following_url": "https://api.github.com/users/aidan-plenert-macdonald/following{/other_user}", "gists_url": "https://api.github.com/users/aidan-plenert-macdonald/gists{/gist_id}", "starred_url": "https://api.github.com/users/aidan-plenert-macdonald/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aidan-plenert-macdonald/subscriptions", "organizations_url": "https://api.github.com/users/aidan-plenert-macdonald/orgs", "repos_url": "https://api.github.com/users/aidan-plenert-macdonald/repos", "events_url": "https://api.github.com/users/aidan-plenert-macdonald/events{/privacy}", "received_events_url": "https://api.github.com/users/aidan-plenert-macdonald/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-29T16:31:50Z", "updated_at": "2016-09-29T16:31:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a> I like the 8 bit option. I may run into problems with the 256 limit later, but I think for now this is a really good option.</p>\n<p>What would I have to change to make sure that those 8 bits get set appropriately? I tracked GetAllocator to the Executor class where it appears that I could modify the AllocatorAttributes and the Executor has virtual functions which make it appear as though I could replace it, but I don't see any registration for the ExecutorImpl.</p>\n<p>Now if I were to just have a TF Device per virtual compute node with a single allocator each. I would like to have a couple device numbers/config to specify the physical hardware and then the virtual compute nodes within it. the Distribute TF seems to use more numbers,</p>\n<div class=\"highlight highlight-source-python\"><pre>tf.train.ClusterSpec({\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker0.example.com:2222<span class=\"pl-pds\">\"</span></span>, \n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker1.example.com:2222<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker2.example.com:2222<span class=\"pl-pds\">\"</span></span>\n    ],\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps<span class=\"pl-pds\">\"</span></span>: [\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps0.example.com:2222<span class=\"pl-pds\">\"</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps1.example.com:2222<span class=\"pl-pds\">\"</span></span>\n    ]})</pre></div>\n<p>Internal to my device, I could treat it as a cluster computer. Is the distributed computing very extensible so I could rework the internals?</p>", "body_text": "@vrv I like the 8 bit option. I may run into problems with the 256 limit later, but I think for now this is a really good option.\nWhat would I have to change to make sure that those 8 bits get set appropriately? I tracked GetAllocator to the Executor class where it appears that I could modify the AllocatorAttributes and the Executor has virtual functions which make it appear as though I could replace it, but I don't see any registration for the ExecutorImpl.\nNow if I were to just have a TF Device per virtual compute node with a single allocator each. I would like to have a couple device numbers/config to specify the physical hardware and then the virtual compute nodes within it. the Distribute TF seems to use more numbers,\ntf.train.ClusterSpec({\n    \"worker\": [\n        \"worker0.example.com:2222\", \n        \"worker1.example.com:2222\",\n        \"worker2.example.com:2222\"\n    ],\n    \"ps\": [\n        \"ps0.example.com:2222\",\n        \"ps1.example.com:2222\"\n    ]})\nInternal to my device, I could treat it as a cluster computer. Is the distributed computing very extensible so I could rework the internals?", "body": "@vrv I like the 8 bit option. I may run into problems with the 256 limit later, but I think for now this is a really good option.\n\nWhat would I have to change to make sure that those 8 bits get set appropriately? I tracked GetAllocator to the Executor class where it appears that I could modify the AllocatorAttributes and the Executor has virtual functions which make it appear as though I could replace it, but I don't see any registration for the ExecutorImpl.\n\nNow if I were to just have a TF Device per virtual compute node with a single allocator each. I would like to have a couple device numbers/config to specify the physical hardware and then the virtual compute nodes within it. the Distribute TF seems to use more numbers,\n\n``` python\ntf.train.ClusterSpec({\n    \"worker\": [\n        \"worker0.example.com:2222\", \n        \"worker1.example.com:2222\",\n        \"worker2.example.com:2222\"\n    ],\n    \"ps\": [\n        \"ps0.example.com:2222\",\n        \"ps1.example.com:2222\"\n    ]})\n```\n\nInternal to my device, I could treat it as a cluster computer. Is the distributed computing very extensible so I could rework the internals?\n"}