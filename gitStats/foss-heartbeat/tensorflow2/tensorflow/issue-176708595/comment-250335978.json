{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/250335978", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-250335978", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 250335978, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MDMzNTk3OA==", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-29T00:01:32Z", "updated_at": "2016-09-29T00:02:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6690599\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aidan-plenert-macdonald\">@aidan-plenert-macdonald</a>: Typically an Allocator manages the memory for an entire device.</p>\n<p>In the GPU case, there is one allocator for each GPU device, but a GPU device has hundreds to thousands of \"cores\", each which has access to the global memory that the allocator is responsible for allocating, and programs are responsible for setting properly.  CUDA has ways in its programming model to additionally have local fine-grained sharing of memory among cores, and that is specific to the cuda programming language, and we don't touch that really, since it's not part of the global memory pool.</p>\n<p>I suspect that your device is really more like many loosely coupled cores with message passing, which is a model we haven't really optimized for.  We would normally treat each 'core' as a separate device, and then your memcopy primitives (device to device) would use whatever high-performance inter-node communication primitives you wanted.</p>\n<p>Alternatively, if you'd rather treat your entire processor as a single \"device\", it might still be possible: allocator.h has an AllocatorAttributes structure, which is a 64-bit opaque value blob.  The top 8 bits of that structure we've made device specific.</p>\n<p>OpKernel has a GetAllocator() function that takes allocator attributes, so it might be possible for you to have the DeviceContext contain information that an OpKernel can use to set the appropriate bits of the AllocatorAttributes, and then you'd implement DeviceBase::GetAllocator() to return a different allocator based on the top 8 bits of the AllocatorAttributes.  If you used all 8 bits as an allocator id, you'd be able to address 256 different allocators in a single device.</p>\n<p>Without knowing too much about your device, I'm not sure which approach is better, but those might help you make progress.</p>", "body_text": "@aidan-plenert-macdonald: Typically an Allocator manages the memory for an entire device.\nIn the GPU case, there is one allocator for each GPU device, but a GPU device has hundreds to thousands of \"cores\", each which has access to the global memory that the allocator is responsible for allocating, and programs are responsible for setting properly.  CUDA has ways in its programming model to additionally have local fine-grained sharing of memory among cores, and that is specific to the cuda programming language, and we don't touch that really, since it's not part of the global memory pool.\nI suspect that your device is really more like many loosely coupled cores with message passing, which is a model we haven't really optimized for.  We would normally treat each 'core' as a separate device, and then your memcopy primitives (device to device) would use whatever high-performance inter-node communication primitives you wanted.\nAlternatively, if you'd rather treat your entire processor as a single \"device\", it might still be possible: allocator.h has an AllocatorAttributes structure, which is a 64-bit opaque value blob.  The top 8 bits of that structure we've made device specific.\nOpKernel has a GetAllocator() function that takes allocator attributes, so it might be possible for you to have the DeviceContext contain information that an OpKernel can use to set the appropriate bits of the AllocatorAttributes, and then you'd implement DeviceBase::GetAllocator() to return a different allocator based on the top 8 bits of the AllocatorAttributes.  If you used all 8 bits as an allocator id, you'd be able to address 256 different allocators in a single device.\nWithout knowing too much about your device, I'm not sure which approach is better, but those might help you make progress.", "body": "@aidan-plenert-macdonald: Typically an Allocator manages the memory for an entire device.\n\nIn the GPU case, there is one allocator for each GPU device, but a GPU device has hundreds to thousands of \"cores\", each which has access to the global memory that the allocator is responsible for allocating, and programs are responsible for setting properly.  CUDA has ways in its programming model to additionally have local fine-grained sharing of memory among cores, and that is specific to the cuda programming language, and we don't touch that really, since it's not part of the global memory pool.\n\nI suspect that your device is really more like many loosely coupled cores with message passing, which is a model we haven't really optimized for.  We would normally treat each 'core' as a separate device, and then your memcopy primitives (device to device) would use whatever high-performance inter-node communication primitives you wanted.\n\nAlternatively, if you'd rather treat your entire processor as a single \"device\", it might still be possible: allocator.h has an AllocatorAttributes structure, which is a 64-bit opaque value blob.  The top 8 bits of that structure we've made device specific.\n\nOpKernel has a GetAllocator() function that takes allocator attributes, so it might be possible for you to have the DeviceContext contain information that an OpKernel can use to set the appropriate bits of the AllocatorAttributes, and then you'd implement DeviceBase::GetAllocator() to return a different allocator based on the top 8 bits of the AllocatorAttributes.  If you used all 8 bits as an allocator id, you'd be able to address 256 different allocators in a single device.\n\nWithout knowing too much about your device, I'm not sure which approach is better, but those might help you make progress.\n"}