{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/272965006", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-272965006", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 272965006, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Mjk2NTAwNg==", "user": {"login": "RichDubielzig", "id": 24554323, "node_id": "MDQ6VXNlcjI0NTU0MzIz", "avatar_url": "https://avatars2.githubusercontent.com/u/24554323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RichDubielzig", "html_url": "https://github.com/RichDubielzig", "followers_url": "https://api.github.com/users/RichDubielzig/followers", "following_url": "https://api.github.com/users/RichDubielzig/following{/other_user}", "gists_url": "https://api.github.com/users/RichDubielzig/gists{/gist_id}", "starred_url": "https://api.github.com/users/RichDubielzig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RichDubielzig/subscriptions", "organizations_url": "https://api.github.com/users/RichDubielzig/orgs", "repos_url": "https://api.github.com/users/RichDubielzig/repos", "events_url": "https://api.github.com/users/RichDubielzig/events{/privacy}", "received_events_url": "https://api.github.com/users/RichDubielzig/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-16T21:22:59Z", "updated_at": "2017-02-22T02:09:13Z", "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>I am currently working my way through understanding the backend of the XLA framework.  I currently have two questions:</p>\n<ol>\n<li>How do you debug the TensorFlow library?  For example, I wanted to try breaking on this debug message:</li>\n</ol>\n<p><code>I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;</code></p>\n<p>I tried</p>\n<pre><code>gdb --args python3 mnist_softmax_xla.py\nset follow-fork-mode child\nb service.cc:187\nr\n</code></pre>\n<p>But the breakpoint was skipped over.</p>\n<ol start=\"2\">\n<li>How does the JIT compiler \"link in\" on-core runtime libraries?  In order to take advantage of XAL, we'd need to have some point at which we link in a bit of code to handle general setup and teardown of a core, as well as communication back to the host.  We also have some TF-specific memory-handling code that would need to go in as well.  At what point do these libraries get added to the HLO? (We can provide the libraries in IR format)</li>\n</ol>", "body_text": "Hello,\nI am currently working my way through understanding the backend of the XLA framework.  I currently have two questions:\n\nHow do you debug the TensorFlow library?  For example, I wanted to try breaking on this debug message:\n\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\nI tried\ngdb --args python3 mnist_softmax_xla.py\nset follow-fork-mode child\nb service.cc:187\nr\n\nBut the breakpoint was skipped over.\n\nHow does the JIT compiler \"link in\" on-core runtime libraries?  In order to take advantage of XAL, we'd need to have some point at which we link in a bit of code to handle general setup and teardown of a core, as well as communication back to the host.  We also have some TF-specific memory-handling code that would need to go in as well.  At what point do these libraries get added to the HLO? (We can provide the libraries in IR format)", "body": "Hello,\r\n\r\nI am currently working my way through understanding the backend of the XLA framework.  I currently have two questions:\r\n\r\n1.  How do you debug the TensorFlow library?  For example, I wanted to try breaking on this debug message:\r\n\r\n`I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>`\r\n\r\nI tried \r\n```\r\ngdb --args python3 mnist_softmax_xla.py\r\nset follow-fork-mode child\r\nb service.cc:187\r\nr\r\n```\r\n\r\nBut the breakpoint was skipped over.\r\n\r\n2.  How does the JIT compiler \"link in\" on-core runtime libraries?  In order to take advantage of XAL, we'd need to have some point at which we link in a bit of code to handle general setup and teardown of a core, as well as communication back to the host.  We also have some TF-specific memory-handling code that would need to go in as well.  At what point do these libraries get added to the HLO? (We can provide the libraries in IR format)"}