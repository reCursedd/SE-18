{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/269062958", "html_url": "https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-269062958", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4359", "id": 269062958, "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTA2Mjk1OA==", "user": {"login": "RichDubielzig", "id": 24554323, "node_id": "MDQ6VXNlcjI0NTU0MzIz", "avatar_url": "https://avatars2.githubusercontent.com/u/24554323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RichDubielzig", "html_url": "https://github.com/RichDubielzig", "followers_url": "https://api.github.com/users/RichDubielzig/followers", "following_url": "https://api.github.com/users/RichDubielzig/following{/other_user}", "gists_url": "https://api.github.com/users/RichDubielzig/gists{/gist_id}", "starred_url": "https://api.github.com/users/RichDubielzig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RichDubielzig/subscriptions", "organizations_url": "https://api.github.com/users/RichDubielzig/orgs", "repos_url": "https://api.github.com/users/RichDubielzig/repos", "events_url": "https://api.github.com/users/RichDubielzig/events{/privacy}", "received_events_url": "https://api.github.com/users/RichDubielzig/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-24T01:37:04Z", "updated_at": "2016-12-24T01:37:04Z", "author_association": "NONE", "body_html": "<p>Thank you all for your help.  I asked this question to Vijay in an email, but I think I'd like to post it here too.</p>\n<hr>\n<p>I've been trying to understand how backpropagation is implemented in tensorflow models.  I'm just asking you to let me know if I have any major misunderstandings about the nature of TensorFlow here.  Thank you again for your efforts to help me out.</p>\n<p>I'm basing this interpretation on the graph generated when I add a FileWriter to the mnist_softmax.py tutorial example (attached here, also with a relu activation added as an experiment).</p>\n<p>It looks like the act of placing an optimizer such as a GradientDescentOptimizer into a computation flow creates a new subgraph that mirrors the original graph, except that</p>\n<ol>\n<li>Data flows through the nodes in reverse order and</li>\n<li>The nodes themselves are replaced with their gradients.</li>\n</ol>\n<p>One training run, therefore will consist of a forward pass through the computation steps as described in the python file, culminating in the loss calculation.  If we are using minibatches, this is done a number of times.  Then, a second pass backwards through the gradients of cross entropy will cause the Variable objects to be updated with trained values and the run() command will complete.</p>\n<p>Some questions:</p>\n<ol>\n<li>I see that I can replace the \"relu\" activation with \"atan\" and have the model run without crashing.  Why does this work?  I don't see any atan gradient function under the kernels/ directory.</li>\n<li>During the course of a run() command, do the weight &amp; bias variables really get overwritten by backpropagation, or does TF keep around the original values and store the adjusted ones in new reserved memory?</li>\n<li>GradientDescentOptimizer() is called with a \"learning rate\" '0.5'.  Is this argument only used to scale the original loss function, or is it passed through all the way back through each layer of the network?</li>\n</ol>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/671609/mnist_softmax.py.zip\">mnist_softmax.py.zip</a></p>", "body_text": "Thank you all for your help.  I asked this question to Vijay in an email, but I think I'd like to post it here too.\n\nI've been trying to understand how backpropagation is implemented in tensorflow models.  I'm just asking you to let me know if I have any major misunderstandings about the nature of TensorFlow here.  Thank you again for your efforts to help me out.\nI'm basing this interpretation on the graph generated when I add a FileWriter to the mnist_softmax.py tutorial example (attached here, also with a relu activation added as an experiment).\nIt looks like the act of placing an optimizer such as a GradientDescentOptimizer into a computation flow creates a new subgraph that mirrors the original graph, except that\n\nData flows through the nodes in reverse order and\nThe nodes themselves are replaced with their gradients.\n\nOne training run, therefore will consist of a forward pass through the computation steps as described in the python file, culminating in the loss calculation.  If we are using minibatches, this is done a number of times.  Then, a second pass backwards through the gradients of cross entropy will cause the Variable objects to be updated with trained values and the run() command will complete.\nSome questions:\n\nI see that I can replace the \"relu\" activation with \"atan\" and have the model run without crashing.  Why does this work?  I don't see any atan gradient function under the kernels/ directory.\nDuring the course of a run() command, do the weight & bias variables really get overwritten by backpropagation, or does TF keep around the original values and store the adjusted ones in new reserved memory?\nGradientDescentOptimizer() is called with a \"learning rate\" '0.5'.  Is this argument only used to scale the original loss function, or is it passed through all the way back through each layer of the network?\n\nmnist_softmax.py.zip", "body": "Thank you all for your help.  I asked this question to Vijay in an email, but I think I'd like to post it here too.\r\n\r\n---------\r\n\r\nI've been trying to understand how backpropagation is implemented in tensorflow models.  I'm just asking you to let me know if I have any major misunderstandings about the nature of TensorFlow here.  Thank you again for your efforts to help me out.\r\n\r\nI'm basing this interpretation on the graph generated when I add a FileWriter to the mnist_softmax.py tutorial example (attached here, also with a relu activation added as an experiment).\r\n\r\nIt looks like the act of placing an optimizer such as a GradientDescentOptimizer into a computation flow creates a new subgraph that mirrors the original graph, except that\r\n1. Data flows through the nodes in reverse order and\r\n2. The nodes themselves are replaced with their gradients.\r\n\r\nOne training run, therefore will consist of a forward pass through the computation steps as described in the python file, culminating in the loss calculation.  If we are using minibatches, this is done a number of times.  Then, a second pass backwards through the gradients of cross entropy will cause the Variable objects to be updated with trained values and the run() command will complete.\r\n\r\nSome questions:\r\n1.  I see that I can replace the \"relu\" activation with \"atan\" and have the model run without crashing.  Why does this work?  I don't see any atan gradient function under the kernels/ directory.\r\n2.  During the course of a run() command, do the weight & bias variables really get overwritten by backpropagation, or does TF keep around the original values and store the adjusted ones in new reserved memory?\r\n3.  GradientDescentOptimizer() is called with a \"learning rate\" '0.5'.  Is this argument only used to scale the original loss function, or is it passed through all the way back through each layer of the network?\r\n\r\n\r\n[mnist_softmax.py.zip](https://github.com/tensorflow/tensorflow/files/671609/mnist_softmax.py.zip)\r\n"}