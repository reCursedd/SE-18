{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/57648002", "pull_request_review_id": null, "id": 57648002, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3NjQ4MDAy", "diff_hunk": "@@ -0,0 +1,164 @@\n+/* Copyright 2015 Google Inc. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"third_party/eigen3/Eigen/Core\"\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+\n+#include \"tensorflow/core/kernels/linalg_ops_common.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+\n+namespace tensorflow {\n+\n+template <typename T>\n+class CholeskyGrad : public OpKernel {\n+ public:\n+  explicit CholeskyGrad(OpKernelConstruction* context) : OpKernel(context) {}\n+  using Matrix = Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic,\n+                               Eigen::RowMajor>;\n+  using ConstMatrixMap = Eigen::Map<const Matrix>;\n+  using MatrixMap = Eigen::Map<Matrix>;\n+  using ConstRef = Eigen::Ref<const Matrix>;\n+  using Ref = Eigen::Ref<Matrix>;\n+\n+  void Compute(OpKernelContext* context) override {\n+    const Tensor& input_tensor_l = context->input(0);\n+    const Tensor& input_tensor_grad = context->input(1);\n+    // Check that input tensors represent a matrix.\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_tensor_l.shape()),\n+                errors::InvalidArgument(\"In[0] is not a matrix\"));\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_tensor_grad.shape()),\n+                errors::InvalidArgument(\"In[1] is not a matrix\"));\n+    // Check that input tensors are square.\n+    OP_REQUIRES(context,\n+                input_tensor_l.dim_size(0) == input_tensor_l.dim_size(1),\n+                errors::InvalidArgument(\"Input matrix must be square.\"));\n+    OP_REQUIRES(context,\n+                input_tensor_grad.dim_size(0) == input_tensor_grad.dim_size(1),\n+                errors::InvalidArgument(\"Input matrix must be square.\"));\n+\n+    // Check that input tensors are of same size.\n+    OP_REQUIRES(context,\n+                input_tensor_l.dim_size(0) == input_tensor_grad.dim_size(0),\n+                errors::InvalidArgument(\"Input matrices must be same size.\"));\n+\n+    // Create an output tensor\n+    Tensor* output_tensor = NULL;\n+    OP_REQUIRES_OK(context,\n+                   context->allocate_output(0, input_tensor_grad.shape(),\n+                   &output_tensor));\n+\n+    if (output_tensor->NumElements() == 0) {\n+      // the output shape is a 0-element matrix, so there is nothing to do.\n+      return;\n+    }\n+    // The next lines are necessary to get Eigen matrix behaviour.\n+    const ConstMatrixMap input_matrix_l_full(input_tensor_l.flat<T>().data(),\n+                                        input_tensor_l.dim_size(0),\n+                                        input_tensor_l.dim_size(1));\n+    const ConstMatrixMap input_matrix_grad(input_tensor_grad.flat<T>().data(),\n+                                           input_tensor_grad.dim_size(0),\n+                                           input_tensor_grad.dim_size(1));\n+    MatrixMap output_matrix(output_tensor->template flat<T>().data(),\n+                            input_tensor_l.dim_size(0),\n+                            input_tensor_l.dim_size(1) );\n+\n+    // Algorithm only depends on lower triangular half on input_tensor_l.\n+    const Matrix input_matrix_l = input_matrix_l_full\n+                                  .template triangularView<Eigen::Lower>();\n+    // Algorithm only depends on lower triangular half on input_matrix_grad.\n+    output_matrix = input_matrix_grad.template triangularView<Eigen::Lower>();\n+\n+    const int64 kMatrixSize = input_matrix_l.rows();\n+    const int64 kMaxBlockSize = 32;\n+\n+    for (int64 block_end = kMatrixSize;\n+               block_end > 0;\n+               block_end -= kMaxBlockSize) {\n+      const int64 block_begin = std::max(0ll, block_end - kMaxBlockSize);\n+      const int64 block_size = block_end - block_begin;\n+      const int64 trailing_size = kMatrixSize - block_end;\n+      output_matrix.block(block_end, block_begin, trailing_size , block_size)\n+                   = input_matrix_l\n+                   .block(block_begin, block_begin, block_size, block_size)\n+                   .adjoint()\n+                   .template triangularView<Eigen::Upper>()\n+                   .solve(output_matrix\n+                   .block(block_end, block_begin, trailing_size, block_size)\n+                   .adjoint() )\n+                   .adjoint();\n+      output_matrix.block(block_begin, block_begin, block_size, block_size)\n+                   -= (output_matrix\n+                   .block(block_end, block_begin, trailing_size, block_size)\n+                   .adjoint()\n+                   *input_matrix_l.block(block_end, block_begin, trailing_size,\n+                                         block_size))\n+                   .template triangularView<Eigen::Lower>();\n+      output_matrix.block(block_end, 0, trailing_size, block_begin)\n+                   -=  output_matrix.block(block_end, block_begin,\n+                                           trailing_size, block_size)\n+                   *input_matrix_l\n+                   .block(block_begin, 0, block_size, block_begin);\n+\n+      output_matrix.block(block_begin, 0, block_size, block_begin)\n+                   -= output_matrix.block(block_end, block_begin, trailing_size,\n+                                          block_size)\n+                   .adjoint()\n+                   *input_matrix_l.block(block_end, 0, trailing_size,\n+                                         block_begin);\n+      CholeskyGradUnblocked(input_matrix_l.block(block_begin, block_begin,\n+                                                 block_size, block_size),\n+                            output_matrix.block(block_begin, block_begin,\n+                                                block_size, block_size));\n+      output_matrix.block(block_begin, 0, block_size, block_begin)\n+                   -= (output_matrix\n+                   .block(block_begin, block_begin, block_size, block_size)\n+                   +output_matrix\n+                   .block(block_begin, block_begin, block_size, block_size)\n+                   .adjoint() )\n+                   *input_matrix_l\n+                   .block(block_begin, 0, block_size, block_begin);\n+    }\n+    output_matrix = (0.5 * (output_matrix +  output_matrix.transpose())).eval();\n+  }\n+  void CholeskyGradUnblocked(const ConstRef l_block, Ref grad_block) {\n+    const int64 kMatrixSize = l_block.rows();\n+    for (int64 k = kMatrixSize-1; k >= 0; k--) {\n+      grad_block(k, k) -= (l_block", "path": "tensorflow/core/kernels/cholesky_grad.cc", "position": null, "original_position": 142, "commit_id": "175ba60ec638665b1165b7e9e806c59a4ed5b8d1", "original_commit_id": "37f2bb7b24fd41dec1b5d5d1c2dc1b4c99ef4de7", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "This might also benefit from defining the blocks as temporaries.\n", "created_at": "2016-03-28T22:58:45Z", "updated_at": "2016-04-07T16:48:06Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/1465#discussion_r57648002", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1465", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/57648002"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/1465#discussion_r57648002"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1465"}}, "body_html": "<p>This might also benefit from defining the blocks as temporaries.</p>", "body_text": "This might also benefit from defining the blocks as temporaries."}