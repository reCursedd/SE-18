{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/56056030", "pull_request_review_id": null, "id": 56056030, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDU2MDMw", "diff_hunk": "@@ -0,0 +1,115 @@\n+//Copyright (c) 2016, Alexander G. de G. Matthews and James Hensman. All rights reserved.\n+\n+//Based on code for blocked Cholesky reference mode differentiation by Iain Murray.\n+//For an explanation see \"Differentiation of the Cholesky algorithm\" by Iain Murray http://arxiv.org/abs/1602.07527.\n+\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"third_party/eigen3/Eigen/Core\"\n+\n+REGISTER_OP(\"CholeskyGrad\").Input(\"l: T\").Input(\"l_bar: T\").Output(\"a_bar: T\").Attr( \"T: {float, double}\").Doc(\"Cholesky backpropagation where l is output of Cholesky algorithm and f is gradient of some loss wrt l\");\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+\n+using namespace tensorflow;\n+\n+template <typename T>\n+class CholeskyGrad : public OpKernel {\n+    public:\n+    \n+    explicit CholeskyGrad(OpKernelConstruction* context) : OpKernel(context) {}\n+\n+    using Matrix = Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n+    using ConstMatrixMap = Eigen::Map<const Matrix>;\n+    using MatrixMap = Eigen::Map<Matrix>;\n+    using ConstRef = Eigen::Ref<const Matrix>;\n+    using Ref = Eigen::Ref<Matrix>;\n+    using lcl_size_t = int;\n+\n+    void Compute(OpKernelContext* context) override {\n+\n+    const Tensor& input_tensor_l = context->input(0); ;\n+    const Tensor& input_tensor_l_bar = context->input(1); \n+\n+    //Check that input tensors represent a matrix.\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_tensor_l.shape()), errors::InvalidArgument(\"In[0] is not a matrix\"));\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_tensor_l_bar.shape()), errors::InvalidArgument(\"In[1] is not a matrix\"));\n+\n+    //Check that input tensors are square.\n+    OP_REQUIRES(context, input_tensor_l.dim_size(0) == input_tensor_l.dim_size(1), errors::InvalidArgument(\"Input matrix must be square.\"));\n+    OP_REQUIRES(context, input_tensor_l_bar.dim_size(0) == input_tensor_l_bar.dim_size(1), errors::InvalidArgument(\"Input matrix must be square.\"));\n+\n+    //Check that input tensors are of same size.\n+    OP_REQUIRES(context, input_tensor_l.dim_size(0) == input_tensor_l_bar.dim_size(0), errors::InvalidArgument(\"Input matrices must be of same size.\"));    \n+\n+    // Create an output tensor\n+    Tensor* output_tensor = NULL;\n+    \n+    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor_l_bar.shape(), &output_tensor));\n+\n+    if (output_tensor->NumElements() == 0) {\n+      // the output shape is a 0-element matrix, so there is nothing to do.\n+      return;\n+    }\n+    \n+    //The next three lines are necessary to get Eigen matrix behaviour.\n+    const ConstMatrixMap input_matrix_l(input_tensor_l.flat<T>().data(), input_tensor_l.dim_size(0), input_tensor_l.dim_size(1));\n+    const ConstMatrixMap input_matrix_l_bar(input_tensor_l_bar.flat<T>().data(), input_tensor_l_bar.dim_size(0), input_tensor_l_bar.dim_size(1));\n+    MatrixMap output_matrix(output_tensor->template flat<T>().data(), input_tensor_l.dim_size(0), input_tensor_l.dim_size(1) );    \n+\n+    const lcl_size_t N = input_matrix_l.rows();\n+    const lcl_size_t NB = 32;\n+\n+    output_matrix = input_matrix_l_bar.template triangularView<Eigen::Lower>();\n+\n+    \n+    for ( lcl_size_t Ji = (N-NB+1) ; Ji>(1-NB); Ji-= NB )   \n+    {\n+        lcl_size_t J = std::max<lcl_size_t>(1, Ji);\n+        lcl_size_t JB = NB - (J - Ji);\n+    \n+        output_matrix.block( J+JB-1, J-1, N - (J+JB-1), JB) = input_matrix_l.block( J-1, J-1, JB, JB ).adjoint().template triangularView<Eigen::Upper>().solve( output_matrix.block( J+JB-1, J-1, N - (J+JB-1), JB ).adjoint() ).adjoint();", "path": "tensorflow/core/user_ops/chol_grad.cc", "position": null, "original_position": 74, "commit_id": "175ba60ec638665b1165b7e9e806c59a4ed5b8d1", "original_commit_id": "441d2f116f12f064676ace93a34ceed6a8d9a9d1", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "Please obey the max 80 character line length rule. Here you can make the algorithm clearer by putting each of the main steps on a separate line (breaking before \".\"). Same everywhere.\n", "created_at": "2016-03-14T18:58:06Z", "updated_at": "2016-04-07T16:48:06Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/1465#discussion_r56056030", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1465", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/56056030"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/1465#discussion_r56056030"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1465"}}, "body_html": "<p>Please obey the max 80 character line length rule. Here you can make the algorithm clearer by putting each of the main steps on a separate line (breaking before \".\"). Same everywhere.</p>", "body_text": "Please obey the max 80 character line length rule. Here you can make the algorithm clearer by putting each of the main steps on a separate line (breaking before \".\"). Same everywhere."}