{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15087", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15087/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15087/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15087/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15087", "id": 278879083, "node_id": "MDU6SXNzdWUyNzg4NzkwODM=", "number": 15087, "title": "Understanding LSTM cell Kernel values", "user": {"login": "Gumilton", "id": 9038746, "node_id": "MDQ6VXNlcjkwMzg3NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/9038746?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gumilton", "html_url": "https://github.com/Gumilton", "followers_url": "https://api.github.com/users/Gumilton/followers", "following_url": "https://api.github.com/users/Gumilton/following{/other_user}", "gists_url": "https://api.github.com/users/Gumilton/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gumilton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gumilton/subscriptions", "organizations_url": "https://api.github.com/users/Gumilton/orgs", "repos_url": "https://api.github.com/users/Gumilton/repos", "events_url": "https://api.github.com/users/Gumilton/events{/privacy}", "received_events_url": "https://api.github.com/users/Gumilton/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-12-04T05:57:32Z", "updated_at": "2018-02-27T06:23:12Z", "closed_at": "2017-12-07T18:48:10Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>I want to understand better those values in LSTM cell Kernel values extracted from:<br>\n&lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(97, 280) dtype=float32_ref&gt;<br>\nfrom tf.trainable_variables()</p>\n<p>My model is very simple, input is a 27 element vector at each time step where the sequence length can be variable. The model is one layer LSTM model with 70 hidden states.</p>\n<pre><code>        def lstm_cell():\n            cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)\n            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropKeepRate)\n            return cell\n        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)], state_is_tuple=True)\n        \n</code></pre>\n<p>So that is probably why the LSTM kernel is a matrix of 97 rows (27 input features and 70 hidden states). As to the columns, I can clearly see four groups by the pattern of values. I guess each group is consist of 70 columns, so totally 4 groups.</p>\n<p>So my guess is that this LSTM kernel, extracted from tf.trainable_variables(), maps input and previous hidden states to current hidden states in order of forget, input, update cell states and output. However, it is hard time for me to figure out the correspondence of these gates and their matrices with the Kernel matrix extracted from tf.trainable_variables().</p>\n<p>Could somebody help? Thanks!</p>\n<hr>\n<p>Update:<br>\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes<br>\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7<br>\nTensorFlow installed from (source or binary): from pip<br>\nTensorFlow version (use command below): 1.3<br>\nPython version: 3.6<br>\nBazel version (if compiling from source):<br>\nGCC/Compiler version (if compiling from source):<br>\nCUDA/cuDNN version: 8.0, 6.1<br>\nGPU model and memory: k2200</p>", "body_text": "Hi all,\nI want to understand better those values in LSTM cell Kernel values extracted from:\n<tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(97, 280) dtype=float32_ref>\nfrom tf.trainable_variables()\nMy model is very simple, input is a 27 element vector at each time step where the sequence length can be variable. The model is one layer LSTM model with 70 hidden states.\n        def lstm_cell():\n            cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)\n            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropKeepRate)\n            return cell\n        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)], state_is_tuple=True)\n        \n\nSo that is probably why the LSTM kernel is a matrix of 97 rows (27 input features and 70 hidden states). As to the columns, I can clearly see four groups by the pattern of values. I guess each group is consist of 70 columns, so totally 4 groups.\nSo my guess is that this LSTM kernel, extracted from tf.trainable_variables(), maps input and previous hidden states to current hidden states in order of forget, input, update cell states and output. However, it is hard time for me to figure out the correspondence of these gates and their matrices with the Kernel matrix extracted from tf.trainable_variables().\nCould somebody help? Thanks!\n\nUpdate:\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\nTensorFlow installed from (source or binary): from pip\nTensorFlow version (use command below): 1.3\nPython version: 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 8.0, 6.1\nGPU model and memory: k2200", "body": "Hi all,\r\n\r\nI want to understand better those values in LSTM cell Kernel values extracted from:\r\n<tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(97, 280) dtype=float32_ref>\r\nfrom tf.trainable_variables()\r\n\r\nMy model is very simple, input is a 27 element vector at each time step where the sequence length can be variable. The model is one layer LSTM model with 70 hidden states. \r\n```\r\n        def lstm_cell():\r\n            cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)\r\n            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropKeepRate)\r\n            return cell\r\n        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)], state_is_tuple=True)\r\n        \r\n```\r\n\r\nSo that is probably why the LSTM kernel is a matrix of 97 rows (27 input features and 70 hidden states). As to the columns, I can clearly see four groups by the pattern of values. I guess each group is consist of 70 columns, so totally 4 groups.\r\n\r\nSo my guess is that this LSTM kernel, extracted from tf.trainable_variables(), maps input and previous hidden states to current hidden states in order of forget, input, update cell states and output. However, it is hard time for me to figure out the correspondence of these gates and their matrices with the Kernel matrix extracted from tf.trainable_variables().\r\n\r\nCould somebody help? Thanks!\r\n\r\n\r\n---------------------\r\nUpdate:\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\nTensorFlow installed from (source or binary): from pip\r\nTensorFlow version (use command below): 1.3\r\nPython version: 3.6\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 8.0, 6.1\r\nGPU model and memory: k2200"}