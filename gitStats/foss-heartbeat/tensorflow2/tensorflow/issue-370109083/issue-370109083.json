{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22985", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22985/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22985/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22985/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22985", "id": 370109083, "node_id": "MDU6SXNzdWUzNzAxMDkwODM=", "number": 22985, "title": "Difference in implementation of batch normalization between theano and tensorflow", "user": {"login": "tearsland123", "id": 10194910, "node_id": "MDQ6VXNlcjEwMTk0OTEw", "avatar_url": "https://avatars2.githubusercontent.com/u/10194910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tearsland123", "html_url": "https://github.com/tearsland123", "followers_url": "https://api.github.com/users/tearsland123/followers", "following_url": "https://api.github.com/users/tearsland123/following{/other_user}", "gists_url": "https://api.github.com/users/tearsland123/gists{/gist_id}", "starred_url": "https://api.github.com/users/tearsland123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tearsland123/subscriptions", "organizations_url": "https://api.github.com/users/tearsland123/orgs", "repos_url": "https://api.github.com/users/tearsland123/repos", "events_url": "https://api.github.com/users/tearsland123/events{/privacy}", "received_events_url": "https://api.github.com/users/tearsland123/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-10-15T10:59:47Z", "updated_at": "2018-11-06T18:34:43Z", "closed_at": "2018-11-06T18:34:43Z", "author_association": "NONE", "body_html": "<p>Dear Sir:<br>\nI have a misunderstanding in the process of batch normalization. e.g.<br>\nThe out put of convolution layer<br>\n[[[102. 138.]<br>\n[246. 282.]]]<br>\n[[[ 507.  624.]<br>\n[ 975. 1092.]]]<br>\n[[[ 912. 1110.]<br>\n[1704. 1902.]]]<br>\nI set the following config of batch norm<br>\ngamma  = [1,2,3]<br>\nbeta = [0,0,0]<br>\nmean = [2,3,4]<br>\nInv = [1/3,1/4,1/5]</p>\n<p>Norm.append(gamma)<br>\nNorm.append(beta)<br>\nNorm.append(mean)<br>\nNorm.append(inv)</p>\n<p>model.add(tf.keras.layers.BatchNormalization( momentum=1.0, epsilon=0,center = True,scale = True,trainable=False,renorm_momentum=1.0))<br>\nmodel.layers[1].set_weights(Norm)</p>\n<p>According to the definition of batch_norm , (x - mean/ inv)*gamma + beta, it should generate\uff1a</p>\n<p>[[[[   300.    408.]<br>\n[   732.    840.]]</p>\n<p>[[  4032.   4968.]<br>\n[  7776.   8712.]]</p>\n<p>[[ 13620.  16590.]<br>\n[ 25500.  28470.]]]]</p>\n<p>However, the final result is<br>\n[[[173.20248 235.55537]<br>\n[422.61404 484.96695]]]<br>\n[[[2015.9596 2483.9502]<br>\n[3887.922  4355.9126]]]<br>\n[[[ 6090.8965  7419.088 ]<br>\n[11403.661  12731.853 ]]]</p>\n<p>Could anyone tell me what is wrong and how could I get the correct answer?</p>\n<p>Best Regards<br>\nAllen</p>", "body_text": "Dear Sir:\nI have a misunderstanding in the process of batch normalization. e.g.\nThe out put of convolution layer\n[[[102. 138.]\n[246. 282.]]]\n[[[ 507.  624.]\n[ 975. 1092.]]]\n[[[ 912. 1110.]\n[1704. 1902.]]]\nI set the following config of batch norm\ngamma  = [1,2,3]\nbeta = [0,0,0]\nmean = [2,3,4]\nInv = [1/3,1/4,1/5]\nNorm.append(gamma)\nNorm.append(beta)\nNorm.append(mean)\nNorm.append(inv)\nmodel.add(tf.keras.layers.BatchNormalization( momentum=1.0, epsilon=0,center = True,scale = True,trainable=False,renorm_momentum=1.0))\nmodel.layers[1].set_weights(Norm)\nAccording to the definition of batch_norm , (x - mean/ inv)*gamma + beta, it should generate\uff1a\n[[[[   300.    408.]\n[   732.    840.]]\n[[  4032.   4968.]\n[  7776.   8712.]]\n[[ 13620.  16590.]\n[ 25500.  28470.]]]]\nHowever, the final result is\n[[[173.20248 235.55537]\n[422.61404 484.96695]]]\n[[[2015.9596 2483.9502]\n[3887.922  4355.9126]]]\n[[[ 6090.8965  7419.088 ]\n[11403.661  12731.853 ]]]\nCould anyone tell me what is wrong and how could I get the correct answer?\nBest Regards\nAllen", "body": "Dear Sir: \r\n  I have a misunderstanding in the process of batch normalization. e.g.\r\n  The out put of convolution layer \r\n  [[[102. 138.]\r\n  [246. 282.]]]\r\n  [[[ 507.  624.]\r\n  [ 975. 1092.]]]\r\n  [[[ 912. 1110.]\r\n  [1704. 1902.]]]\r\n  I set the following config of batch norm\r\n  gamma  = [1,2,3]\r\n  beta = [0,0,0]\r\n  mean = [2,3,4]\r\n  Inv = [1/3,1/4,1/5]\r\n\r\n\r\n  Norm.append(gamma)\r\n  Norm.append(beta)\r\n  Norm.append(mean)\r\n  Norm.append(inv)\r\n \r\n\r\n  model.add(tf.keras.layers.BatchNormalization( momentum=1.0, epsilon=0,center = True,scale = True,trainable=False,renorm_momentum=1.0))\r\n  model.layers[1].set_weights(Norm)\r\n   \r\n   According to the definition of batch_norm , (x - mean/ inv)*gamma + beta, it should generate\uff1a \r\n   \r\n  [[[[   300.    408.]\r\n   [   732.    840.]]\r\n\r\n  [[  4032.   4968.]\r\n   [  7776.   8712.]]\r\n\r\n  [[ 13620.  16590.]\r\n   [ 25500.  28470.]]]]\r\n  \r\n   However, the final result is\r\n   [[[173.20248 235.55537]\r\n  [422.61404 484.96695]]]\r\n[[[2015.9596 2483.9502]\r\n  [3887.922  4355.9126]]]\r\n[[[ 6090.8965  7419.088 ]\r\n  [11403.661  12731.853 ]]]\r\n\r\n  Could anyone tell me what is wrong and how could I get the correct answer?\r\n\r\nBest Regards\r\nAllen\r\n"}