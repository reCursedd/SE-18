{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/284859387", "html_url": "https://github.com/tensorflow/tensorflow/issues/8168#issuecomment-284859387", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8168", "id": 284859387, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDg1OTM4Nw==", "user": {"login": "MicaelCarvalho", "id": 17184992, "node_id": "MDQ6VXNlcjE3MTg0OTky", "avatar_url": "https://avatars3.githubusercontent.com/u/17184992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MicaelCarvalho", "html_url": "https://github.com/MicaelCarvalho", "followers_url": "https://api.github.com/users/MicaelCarvalho/followers", "following_url": "https://api.github.com/users/MicaelCarvalho/following{/other_user}", "gists_url": "https://api.github.com/users/MicaelCarvalho/gists{/gist_id}", "starred_url": "https://api.github.com/users/MicaelCarvalho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MicaelCarvalho/subscriptions", "organizations_url": "https://api.github.com/users/MicaelCarvalho/orgs", "repos_url": "https://api.github.com/users/MicaelCarvalho/repos", "events_url": "https://api.github.com/users/MicaelCarvalho/events{/privacy}", "received_events_url": "https://api.github.com/users/MicaelCarvalho/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-07T21:07:51Z", "updated_at": "2017-03-07T21:07:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for the link, I didn't know that -- the documentation should explain this behavior, it's definitely very important.</p>\n<blockquote>\n<p>Conditional execution needs all ops to be created inside the the op.</p>\n</blockquote>\n<p>Does this mean conditional execution is, by nature, incompatible with anything that involves queues, since we cannot simply redefine the queue each time? -- in this case, redefine the queue = calling slice_input_producer and batch function.</p>\n<p>If so, I don't see a way to solve the problem of having a batch variable with examples coming from two different sources. The only way to do so would be by putting a placeholder as input for the model, and manually evaluating the batch before passing it in the feed_dict parameter, effectively dividing the graph and treating the condition <em>\u00e0 la python</em>. In my opinion, not an elegant solution.</p>\n<p>Is there any way (<em>or, better, a correct way</em>) around this specific problem of selecting the set of images to be used?</p>", "body_text": "Thanks for the link, I didn't know that -- the documentation should explain this behavior, it's definitely very important.\n\nConditional execution needs all ops to be created inside the the op.\n\nDoes this mean conditional execution is, by nature, incompatible with anything that involves queues, since we cannot simply redefine the queue each time? -- in this case, redefine the queue = calling slice_input_producer and batch function.\nIf so, I don't see a way to solve the problem of having a batch variable with examples coming from two different sources. The only way to do so would be by putting a placeholder as input for the model, and manually evaluating the batch before passing it in the feed_dict parameter, effectively dividing the graph and treating the condition \u00e0 la python. In my opinion, not an elegant solution.\nIs there any way (or, better, a correct way) around this specific problem of selecting the set of images to be used?", "body": "Thanks for the link, I didn't know that -- the documentation should explain this behavior, it's definitely very important.\r\n\r\n> Conditional execution needs all ops to be created inside the the op.\r\n\r\nDoes this mean conditional execution is, by nature, incompatible with anything that involves queues, since we cannot simply redefine the queue each time? -- in this case, redefine the queue = calling slice_input_producer and batch function.\r\n\r\nIf so, I don't see a way to solve the problem of having a batch variable with examples coming from two different sources. The only way to do so would be by putting a placeholder as input for the model, and manually evaluating the batch before passing it in the feed_dict parameter, effectively dividing the graph and treating the condition _\u00e0 la python_. In my opinion, not an elegant solution.\r\n\r\nIs there any way (_or, better, a correct way_) around this specific problem of selecting the set of images to be used?"}