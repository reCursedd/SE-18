{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9058", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9058/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9058/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9058/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9058", "id": 220352475, "node_id": "MDU6SXNzdWUyMjAzNTI0NzU=", "number": 9058, "title": "AdadeltaOptimizer numeric issue", "user": {"login": "darknights", "id": 11676044, "node_id": "MDQ6VXNlcjExNjc2MDQ0", "avatar_url": "https://avatars0.githubusercontent.com/u/11676044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/darknights", "html_url": "https://github.com/darknights", "followers_url": "https://api.github.com/users/darknights/followers", "following_url": "https://api.github.com/users/darknights/following{/other_user}", "gists_url": "https://api.github.com/users/darknights/gists{/gist_id}", "starred_url": "https://api.github.com/users/darknights/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/darknights/subscriptions", "organizations_url": "https://api.github.com/users/darknights/orgs", "repos_url": "https://api.github.com/users/darknights/repos", "events_url": "https://api.github.com/users/darknights/events{/privacy}", "received_events_url": "https://api.github.com/users/darknights/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-04-07T23:15:45Z", "updated_at": "2017-04-07T23:47:21Z", "closed_at": "2017-04-07T23:22:57Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I tested it with following script, and expected the model update to be -sqrt(0+1)/sqrt(1+1)*1 = -0.7071. However, the output is -0.866. Input gradient is 1, and confirmed with GradientDescentOptimizer. I also verified in rho=1/epsilon=1 the output is -1 as expected, and rho=1/epsilon=0 the output is nan as expected. Reading the implementation of adadelta didn't tell me why the output is -0.866 for rho=0/epsilon=1. What am I missing?</p>\n<pre><code>def test_tf():\n    data  = [0.5]\n    label = [0]\n    dim   = 1\n    batch_size = 1\n    import tensorflow as tf\n    tf.reset_default_graph()\n\n    x = tf.placeholder(tf.float32, [batch_size, dim])\n    l = tf.placeholder(tf.int32, [batch_size, dim])\n    W = tf.Variable(tf.zeros((dim,)), name='W')\n    logits = x + W\n    print(logits)\n    loss = tf.losses.mean_squared_error(logits, l)\n    #optimizer = tf.train.GradientDescentOptimizer(1)\n    optimizer = tf.train.AdadeltaOptimizer(1, rho=0, epsilon=1)\n    train = optimizer.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        input_map = {x:[data], l:[label]}\n        print('loss', loss.eval(input_map))\n        sess.run(train, input_map)\n        print('\\n'.join([' {}\\n'.format(p.eval()) for p in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]))\n    sess.close()\n\ntest_tf()\n</code></pre>", "body_text": "Hi,\nI tested it with following script, and expected the model update to be -sqrt(0+1)/sqrt(1+1)*1 = -0.7071. However, the output is -0.866. Input gradient is 1, and confirmed with GradientDescentOptimizer. I also verified in rho=1/epsilon=1 the output is -1 as expected, and rho=1/epsilon=0 the output is nan as expected. Reading the implementation of adadelta didn't tell me why the output is -0.866 for rho=0/epsilon=1. What am I missing?\ndef test_tf():\n    data  = [0.5]\n    label = [0]\n    dim   = 1\n    batch_size = 1\n    import tensorflow as tf\n    tf.reset_default_graph()\n\n    x = tf.placeholder(tf.float32, [batch_size, dim])\n    l = tf.placeholder(tf.int32, [batch_size, dim])\n    W = tf.Variable(tf.zeros((dim,)), name='W')\n    logits = x + W\n    print(logits)\n    loss = tf.losses.mean_squared_error(logits, l)\n    #optimizer = tf.train.GradientDescentOptimizer(1)\n    optimizer = tf.train.AdadeltaOptimizer(1, rho=0, epsilon=1)\n    train = optimizer.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        input_map = {x:[data], l:[label]}\n        print('loss', loss.eval(input_map))\n        sess.run(train, input_map)\n        print('\\n'.join([' {}\\n'.format(p.eval()) for p in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]))\n    sess.close()\n\ntest_tf()", "body": "Hi,\r\n\r\nI tested it with following script, and expected the model update to be -sqrt(0+1)/sqrt(1+1)*1 = -0.7071. However, the output is -0.866. Input gradient is 1, and confirmed with GradientDescentOptimizer. I also verified in rho=1/epsilon=1 the output is -1 as expected, and rho=1/epsilon=0 the output is nan as expected. Reading the implementation of adadelta didn't tell me why the output is -0.866 for rho=0/epsilon=1. What am I missing?\r\n\r\n    def test_tf():\r\n        data  = [0.5]\r\n        label = [0]\r\n        dim   = 1\r\n        batch_size = 1\r\n        import tensorflow as tf\r\n        tf.reset_default_graph()\r\n\r\n        x = tf.placeholder(tf.float32, [batch_size, dim])\r\n        l = tf.placeholder(tf.int32, [batch_size, dim])\r\n        W = tf.Variable(tf.zeros((dim,)), name='W')\r\n        logits = x + W\r\n        print(logits)\r\n        loss = tf.losses.mean_squared_error(logits, l)\r\n        #optimizer = tf.train.GradientDescentOptimizer(1)\r\n        optimizer = tf.train.AdadeltaOptimizer(1, rho=0, epsilon=1)\r\n        train = optimizer.minimize(loss)\r\n\r\n        with tf.Session() as sess:\r\n            sess.run(tf.global_variables_initializer())\r\n            input_map = {x:[data], l:[label]}\r\n            print('loss', loss.eval(input_map))\r\n            sess.run(train, input_map)\r\n            print('\\n'.join([' {}\\n'.format(p.eval()) for p in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]))\r\n        sess.close()\r\n\r\n    test_tf()\r\n"}