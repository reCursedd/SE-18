{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/266491675", "html_url": "https://github.com/tensorflow/tensorflow/issues/6229#issuecomment-266491675", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6229", "id": 266491675, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjQ5MTY3NQ==", "user": {"login": "hardwhack", "id": 1630119, "node_id": "MDQ6VXNlcjE2MzAxMTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1630119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hardwhack", "html_url": "https://github.com/hardwhack", "followers_url": "https://api.github.com/users/hardwhack/followers", "following_url": "https://api.github.com/users/hardwhack/following{/other_user}", "gists_url": "https://api.github.com/users/hardwhack/gists{/gist_id}", "starred_url": "https://api.github.com/users/hardwhack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hardwhack/subscriptions", "organizations_url": "https://api.github.com/users/hardwhack/orgs", "repos_url": "https://api.github.com/users/hardwhack/repos", "events_url": "https://api.github.com/users/hardwhack/events{/privacy}", "received_events_url": "https://api.github.com/users/hardwhack/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-12T17:17:26Z", "updated_at": "2016-12-12T17:18:46Z", "author_association": "NONE", "body_html": "<p>One other thing I forgot to mention regarding prediction: computed accuracy closely matches the results reported by the evaluate() function.</p>\n<pre><code>numCorrect=0\nfor i in range(len( predictions) ):\n    if (predictions[i] == df_test.label[i]):\n        numCorrect = numCorrect +1\n\nnumCorrect --&gt; 13641\nlen( predictions) --&gt; 16281\n</code></pre>\n<p>The ratio is 0.837847798046803 which matches 'accuracy' returned by evaluate() to 7 decimal places.</p>\n<p>The mix of correct and incorrect results makes me wonder where the discrepancy lies: error in evaluate() or me. Once again, thank you in advance for your consideration and time.</p>", "body_text": "One other thing I forgot to mention regarding prediction: computed accuracy closely matches the results reported by the evaluate() function.\nnumCorrect=0\nfor i in range(len( predictions) ):\n    if (predictions[i] == df_test.label[i]):\n        numCorrect = numCorrect +1\n\nnumCorrect --> 13641\nlen( predictions) --> 16281\n\nThe ratio is 0.837847798046803 which matches 'accuracy' returned by evaluate() to 7 decimal places.\nThe mix of correct and incorrect results makes me wonder where the discrepancy lies: error in evaluate() or me. Once again, thank you in advance for your consideration and time.", "body": "One other thing I forgot to mention regarding prediction: computed accuracy closely matches the results reported by the evaluate() function.\r\n\r\n```\r\nnumCorrect=0\r\nfor i in range(len( predictions) ):\r\n    if (predictions[i] == df_test.label[i]):\r\n        numCorrect = numCorrect +1\r\n\r\nnumCorrect --> 13641\r\nlen( predictions) --> 16281\r\n```\r\nThe ratio is 0.837847798046803 which matches 'accuracy' returned by evaluate() to 7 decimal places.\r\n\r\nThe mix of correct and incorrect results makes me wonder where the discrepancy lies: error in evaluate() or me. Once again, thank you in advance for your consideration and time."}