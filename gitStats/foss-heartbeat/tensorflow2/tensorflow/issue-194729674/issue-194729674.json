{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6229", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6229/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6229/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6229/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6229", "id": 194729674, "node_id": "MDU6SXNzdWUxOTQ3Mjk2NzQ=", "number": 6229, "title": "tf.contrib.learn.LinearClassifier.evaluate reports incorrect labels/prediction_mean", "user": {"login": "hardwhack", "id": 1630119, "node_id": "MDQ6VXNlcjE2MzAxMTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1630119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hardwhack", "html_url": "https://github.com/hardwhack", "followers_url": "https://api.github.com/users/hardwhack/followers", "following_url": "https://api.github.com/users/hardwhack/following{/other_user}", "gists_url": "https://api.github.com/users/hardwhack/gists{/gist_id}", "starred_url": "https://api.github.com/users/hardwhack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hardwhack/subscriptions", "organizations_url": "https://api.github.com/users/hardwhack/orgs", "repos_url": "https://api.github.com/users/hardwhack/repos", "events_url": "https://api.github.com/users/hardwhack/events{/privacy}", "received_events_url": "https://api.github.com/users/hardwhack/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-12-10T00:30:48Z", "updated_at": "2016-12-13T17:59:19Z", "closed_at": "2016-12-13T17:59:19Z", "author_association": "NONE", "body_html": "<p>Relatively new to machine learning but this appears to be a reproducible bug. Have not been able to find related issues on web searches.</p>\n<p>OS is MacOS,  no CUDA. Installed using pip package:</p>\n<h1>Mac OS X, CPU only, Python 2.7:</h1>\n<p>$ export TF_BINARY_URL=<a href=\"https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py2-none-any.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py2-none-any.whl</a><br>\ntensorflow version: 0.12.0-rc0</p>\n<p>I first noted discrepancy between on my data using a tf.contrib.learn.LinearClassifier. The evaluate function returned a 'labels/actual_label_mean' that was accurate but a 'labels/prediction_mean' that was too low. I was able to reproduce similar results (i.e. 'labels/prediction_mean' too low) using the linearclassifier tutorial at:<br>\n<a href=\"https://www.tensorflow.org/versions/r0.12/tutorials/wide/index.html\" rel=\"nofollow\">https://www.tensorflow.org/versions/r0.12/tutorials/wide/index.html</a></p>\n<p>Following the tutorial, I get the following from <code>m.evaluate(input_fn=eval_input_fn, steps=1)</code></p>\n<p>{'accuracy': 0.83582091,<br>\n'accuracy/baseline_label_mean': 0.23622628,<br>\n'accuracy/threshold_0.500000_mean': 0.83582091,<br>\n'auc': 0.88367212,<br>\n'global_step': 400,<br>\n'labels/actual_label_mean': 0.23622628,<br>\n'labels/prediction_mean': 0.23983434,<br>\n'loss': 0.35221672,<br>\n'precision/positive_threshold_0.500000_mean': 0.70658684,<br>\n'recall/positive_threshold_0.500000_mean': 0.52158087}</p>\n<p>The  'labels/actual_label_mean' value exactly matches <code>sum(df_test.label)/len(df_test.label)</code></p>\n<p>If I actually use the predict using</p>\n<pre><code>pred = m.predict(input_fn = lambda:input_fn(df_test))\npredictions = []\nfor i in range( df_test.shape[0] ):\n    predictions.append(pred.next())\n### I realize above code looks ugly but I can't get it to work in a less kludgy way, e.g.\n### predictions = list(pred) will hang indefinitely until control-c\n### Is this a separate bug?\n\nsum(predictions) -&gt; 2839\nlen(predictions) -&gt; 16281\n</code></pre>\n<p>The ratio is 0.1743750 which is much lower than the reported value for 'labels/prediction_mean', 0.23983434.</p>\n<p>I think that this is a serious bug as predicting/guessing the more likely label artificially inflates the accuracy. As an extreme example, predicting 'no meteor today' will give you a 99.99_% accuracy.</p>\n<p>If this is not a bug but rather a misunderstanding of how prediction/evaluation work, pardon my error.</p>", "body_text": "Relatively new to machine learning but this appears to be a reproducible bug. Have not been able to find related issues on web searches.\nOS is MacOS,  no CUDA. Installed using pip package:\nMac OS X, CPU only, Python 2.7:\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py2-none-any.whl\ntensorflow version: 0.12.0-rc0\nI first noted discrepancy between on my data using a tf.contrib.learn.LinearClassifier. The evaluate function returned a 'labels/actual_label_mean' that was accurate but a 'labels/prediction_mean' that was too low. I was able to reproduce similar results (i.e. 'labels/prediction_mean' too low) using the linearclassifier tutorial at:\nhttps://www.tensorflow.org/versions/r0.12/tutorials/wide/index.html\nFollowing the tutorial, I get the following from m.evaluate(input_fn=eval_input_fn, steps=1)\n{'accuracy': 0.83582091,\n'accuracy/baseline_label_mean': 0.23622628,\n'accuracy/threshold_0.500000_mean': 0.83582091,\n'auc': 0.88367212,\n'global_step': 400,\n'labels/actual_label_mean': 0.23622628,\n'labels/prediction_mean': 0.23983434,\n'loss': 0.35221672,\n'precision/positive_threshold_0.500000_mean': 0.70658684,\n'recall/positive_threshold_0.500000_mean': 0.52158087}\nThe  'labels/actual_label_mean' value exactly matches sum(df_test.label)/len(df_test.label)\nIf I actually use the predict using\npred = m.predict(input_fn = lambda:input_fn(df_test))\npredictions = []\nfor i in range( df_test.shape[0] ):\n    predictions.append(pred.next())\n### I realize above code looks ugly but I can't get it to work in a less kludgy way, e.g.\n### predictions = list(pred) will hang indefinitely until control-c\n### Is this a separate bug?\n\nsum(predictions) -> 2839\nlen(predictions) -> 16281\n\nThe ratio is 0.1743750 which is much lower than the reported value for 'labels/prediction_mean', 0.23983434.\nI think that this is a serious bug as predicting/guessing the more likely label artificially inflates the accuracy. As an extreme example, predicting 'no meteor today' will give you a 99.99_% accuracy.\nIf this is not a bug but rather a misunderstanding of how prediction/evaluation work, pardon my error.", "body": "Relatively new to machine learning but this appears to be a reproducible bug. Have not been able to find related issues on web searches.\r\n\r\nOS is MacOS,  no CUDA. Installed using pip package:\r\n# Mac OS X, CPU only, Python 2.7:\r\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py2-none-any.whl\r\ntensorflow version: 0.12.0-rc0\r\n\r\nI first noted discrepancy between on my data using a tf.contrib.learn.LinearClassifier. The evaluate function returned a 'labels/actual_label_mean' that was accurate but a 'labels/prediction_mean' that was too low. I was able to reproduce similar results (i.e. 'labels/prediction_mean' too low) using the linearclassifier tutorial at:\r\nhttps://www.tensorflow.org/versions/r0.12/tutorials/wide/index.html\r\n\r\nFollowing the tutorial, I get the following from `m.evaluate(input_fn=eval_input_fn, steps=1)`\r\n\r\n{'accuracy': 0.83582091,\r\n 'accuracy/baseline_label_mean': 0.23622628,\r\n 'accuracy/threshold_0.500000_mean': 0.83582091,\r\n 'auc': 0.88367212,\r\n 'global_step': 400,\r\n 'labels/actual_label_mean': 0.23622628,\r\n 'labels/prediction_mean': 0.23983434,\r\n 'loss': 0.35221672,\r\n 'precision/positive_threshold_0.500000_mean': 0.70658684,\r\n 'recall/positive_threshold_0.500000_mean': 0.52158087}\r\n\r\nThe  'labels/actual_label_mean' value exactly matches `sum(df_test.label)/len(df_test.label)`\r\n\r\nIf I actually use the predict using \r\n\r\n```\r\npred = m.predict(input_fn = lambda:input_fn(df_test))\r\npredictions = []\r\nfor i in range( df_test.shape[0] ):\r\n    predictions.append(pred.next())\r\n### I realize above code looks ugly but I can't get it to work in a less kludgy way, e.g.\r\n### predictions = list(pred) will hang indefinitely until control-c\r\n### Is this a separate bug?\r\n\r\nsum(predictions) -> 2839\r\nlen(predictions) -> 16281\r\n```\r\nThe ratio is 0.1743750 which is much lower than the reported value for 'labels/prediction_mean', 0.23983434.\r\n\r\nI think that this is a serious bug as predicting/guessing the more likely label artificially inflates the accuracy. As an extreme example, predicting 'no meteor today' will give you a 99.99_% accuracy.\r\n\r\nIf this is not a bug but rather a misunderstanding of how prediction/evaluation work, pardon my error."}