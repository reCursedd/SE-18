{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15373", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15373/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15373/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15373/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15373", "id": 282266169, "node_id": "MDU6SXNzdWUyODIyNjYxNjk=", "number": 15373, "title": "GPU memory usage changed from TF 1.3.0 to 1.4.0 - runs out of memory", "user": {"login": "juhanaka", "id": 4242928, "node_id": "MDQ6VXNlcjQyNDI5Mjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/4242928?v=4", "gravatar_id": "", "url": "https://api.github.com/users/juhanaka", "html_url": "https://github.com/juhanaka", "followers_url": "https://api.github.com/users/juhanaka/followers", "following_url": "https://api.github.com/users/juhanaka/following{/other_user}", "gists_url": "https://api.github.com/users/juhanaka/gists{/gist_id}", "starred_url": "https://api.github.com/users/juhanaka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/juhanaka/subscriptions", "organizations_url": "https://api.github.com/users/juhanaka/orgs", "repos_url": "https://api.github.com/users/juhanaka/repos", "events_url": "https://api.github.com/users/juhanaka/events{/privacy}", "received_events_url": "https://api.github.com/users/juhanaka/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-14T22:55:32Z", "updated_at": "2017-12-15T01:54:55Z", "closed_at": "2017-12-15T01:54:55Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Custom code included below</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: From pip</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0 and 1.4.0</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: cuda-8.0 cudnn-6.0</li>\n<li><strong>GPU model and memory</strong>: GTX 1080 8GB</li>\n<li><strong>Exact command to reproduce</strong>: python &lt;example_script.py&gt;</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Bug. TensorFlow runs out of GPU memory (ResourceExhaustedError) when using version 1.4.0 when running code that runs fine on version 1.3.0. Please see the following script to reproduce.</p>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.slim.nets as nets\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim.nets import resnet_v2\n\nkBatchSize = 4\nkCropSize = 500\nkNumClasses = 10\n\nwith tf.device('/gpu:0'):\n  images = tf.random_normal([kBatchSize, kCropSize, kCropSize, 3])\n  labels = tf.constant(0, dtype=tf.int32, shape=[kBatchSize, kCropSize, kCropSize])\n\n  with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n    backbone, end_points = resnet_v2.resnet_v2_101(\n        images, None, is_training=True, global_pool=False,\n        output_stride=8)\n\n    final_conv = tf.layers.conv2d(backbone, kNumClasses, [1, 1], name='final_conv')\n    logits = tf.image.resize_bilinear(final_conv, tf.slice(tf.shape(images), [1], [2]))\n\n  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\ntrain_op = slim.learning.create_train_op(loss, optimizer)\nslim.learning.train(train_op, '/tmp/resnet')\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code included below\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\nTensorFlow installed from (source or binary): From pip\nTensorFlow version (use command below): 1.3.0 and 1.4.0\nPython version: 2.7\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: cuda-8.0 cudnn-6.0\nGPU model and memory: GTX 1080 8GB\nExact command to reproduce: python <example_script.py>\n\nDescribe the problem\nBug. TensorFlow runs out of GPU memory (ResourceExhaustedError) when using version 1.4.0 when running code that runs fine on version 1.3.0. Please see the following script to reproduce.\nSource code / logs\nimport tensorflow as tf\nimport tensorflow.contrib.slim.nets as nets\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim.nets import resnet_v2\n\nkBatchSize = 4\nkCropSize = 500\nkNumClasses = 10\n\nwith tf.device('/gpu:0'):\n  images = tf.random_normal([kBatchSize, kCropSize, kCropSize, 3])\n  labels = tf.constant(0, dtype=tf.int32, shape=[kBatchSize, kCropSize, kCropSize])\n\n  with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n    backbone, end_points = resnet_v2.resnet_v2_101(\n        images, None, is_training=True, global_pool=False,\n        output_stride=8)\n\n    final_conv = tf.layers.conv2d(backbone, kNumClasses, [1, 1], name='final_conv')\n    logits = tf.image.resize_bilinear(final_conv, tf.slice(tf.shape(images), [1], [2]))\n\n  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits)\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\ntrain_op = slim.learning.create_train_op(loss, optimizer)\nslim.learning.train(train_op, '/tmp/resnet')", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code included below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: From pip\r\n- **TensorFlow version (use command below)**: 1.3.0 and 1.4.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cuda-8.0 cudnn-6.0\r\n- **GPU model and memory**: GTX 1080 8GB\r\n- **Exact command to reproduce**: python <example_script.py>\r\n\r\n### Describe the problem\r\nBug. TensorFlow runs out of GPU memory (ResourceExhaustedError) when using version 1.4.0 when running code that runs fine on version 1.3.0. Please see the following script to reproduce.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim.nets as nets\r\nimport tensorflow.contrib.slim as slim\r\nfrom tensorflow.contrib.slim.nets import resnet_v2\r\n\r\nkBatchSize = 4\r\nkCropSize = 500\r\nkNumClasses = 10\r\n\r\nwith tf.device('/gpu:0'):\r\n  images = tf.random_normal([kBatchSize, kCropSize, kCropSize, 3])\r\n  labels = tf.constant(0, dtype=tf.int32, shape=[kBatchSize, kCropSize, kCropSize])\r\n\r\n  with slim.arg_scope(resnet_v2.resnet_arg_scope()):\r\n    backbone, end_points = resnet_v2.resnet_v2_101(\r\n        images, None, is_training=True, global_pool=False,\r\n        output_stride=8)\r\n\r\n    final_conv = tf.layers.conv2d(backbone, kNumClasses, [1, 1], name='final_conv')\r\n    logits = tf.image.resize_bilinear(final_conv, tf.slice(tf.shape(images), [1], [2]))\r\n\r\n  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n        labels=labels, logits=logits)\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\r\ntrain_op = slim.learning.create_train_op(loss, optimizer)\r\nslim.learning.train(train_op, '/tmp/resnet')\r\n```\r\n"}