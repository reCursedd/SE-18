{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/227183835", "pull_request_review_id": 167163141, "id": 227183835, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzE4MzgzNQ==", "diff_hunk": "@@ -1,324 +1,428 @@\n {\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0,\n+  \"metadata\": {\n+    \"colab\": {\n+      \"name\": \"dcgan.ipynb\",\n+      \"version\": \"0.3.2\",\n+      \"provenance\": [],\n+      \"collapsed_sections\": []\n+    },\n+    \"kernelspec\": {\n+      \"name\": \"python2\",\n+      \"display_name\": \"Python 2\"\n+    },\n+    \"accelerator\": \"GPU\"\n+  },\n   \"cells\": [\n     {\n-      \"cell_type\": \"markdown\",\n       \"metadata\": {\n         \"colab_type\": \"text\",\n         \"id\": \"0TD5ZrvEMbhZ\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"##### Copyright 2018 The TensorFlow Authors.\\n\",\n+        \"**Copyright 2018 The TensorFlow Authors**.\\n\",\n         \"\\n\",\n         \"Licensed under the Apache License, Version 2.0 (the \\\"License\\\").\\n\",\n         \"\\n\",\n-        \"# DCGAN: An example with tf.keras and eager\\n\",\n+        \"# Generating Handwritten Digits with DCGAN\\n\",\n         \"\\n\",\n-        \"\\u003ctable class=\\\"tfo-notebook-buttons\\\" align=\\\"left\\\"\\u003e\\u003ctd\\u003e\\n\",\n-        \"\\u003ca target=\\\"_blank\\\"  href=\\\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\\\"\\u003e\\n\",\n-        \"    \\u003cimg src=\\\"https://www.tensorflow.org/images/colab_logo_32px.png\\\" /\\u003eRun in Google Colab\\u003c/a\\u003e  \\n\",\n-        \"\\u003c/td\\u003e\\u003ctd\\u003e\\n\",\n-        \"\\u003ca target=\\\"_blank\\\"  href=\\\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\\\"\\u003e\\u003cimg width=32px src=\\\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\\\" /\\u003eView source on GitHub\\u003c/a\\u003e\\u003c/td\\u003e\\u003c/table\\u003e\"\n+        \"<table class=\\\"tfo-notebook-buttons\\\" align=\\\"left\\\"><td>\\n\",\n+        \"<a target=\\\"_blank\\\"  href=\\\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\\\">\\n\",\n+        \"    <img src=\\\"https://www.tensorflow.org/images/colab_logo_32px.png\\\" />Run in Google Colab</a>  \\n\",\n+        \"</td><td>\\n\",\n+        \"<a target=\\\"_blank\\\"  href=\\\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\\\"><img width=32px src=\\\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\\\" />View source on GitHub</a></td></table>\"\n       ]\n     },\n     {\n-      \"cell_type\": \"markdown\",\n       \"metadata\": {\n         \"colab_type\": \"text\",\n         \"id\": \"ITZuApL56Mny\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"This notebook demonstrates how to generate images of handwritten digits using [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). To do so, we use Deep Convolutional Generative Adverserial Networks ([DCGAN](https://arxiv.org/pdf/1511.06434.pdf)).\\n\",\n+        \"This tutorial demonstrates how to generate images of handwritten digits using a Deep Convolutional Generative Adversarial Network ([DCGAN](https://arxiv.org/pdf/1511.06434.pdf)). The code is written in [tf.keras](https://www.tensorflow.org/programmers_guide/keras) with [eager execution](https://www.tensorflow.org/programmers_guide/eager) enabled. \"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"colab_type\": \"toc\",\n+        \"id\": \"x2McrO9bMyLN\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \">[Generating Handwritten Digits with DCGAN](#scrollTo=0TD5ZrvEMbhZ)\\n\",\n+        \"\\n\",\n+        \">>[What are GANs?](#scrollTo=2MbKJY38Puy9)\\n\",\n+        \"\\n\",\n+        \">>>[Import TensorFlow and enable eager execution](#scrollTo=e1_Y75QXJS6h)\\n\",\n+        \"\\n\",\n+        \">>>[Load the dataset](#scrollTo=iYn4MdZnKCey)\\n\",\n+        \"\\n\",\n+        \">>>[Use tf.data to create batches and shuffle the dataset](#scrollTo=PIGN6ouoQxt3)\\n\",\n+        \"\\n\",\n+        \">>[Create the models](#scrollTo=THY-sZMiQ4UV)\\n\",\n+        \"\\n\",\n+        \">>>[The Generator Model](#scrollTo=-tEyxE-GMC48)\\n\",\n         \"\\n\",\n-        \"This model takes about ~30 seconds per epoch (using tf.contrib.eager.defun to create graph functions) to train on a single Tesla K80 on Colab, as of July 2018.\\n\",\n+        \">>>[The Discriminator model](#scrollTo=D0IKnaCtg6WE)\\n\",\n         \"\\n\",\n-        \"Below is the output generated after training the generator and discriminator models for 150 epochs.\\n\",\n+        \">>[Define the loss functions and the optimizer](#scrollTo=0FMYgY_mPfTi)\\n\",\n+        \"\\n\",\n+        \">>>[Generator loss](#scrollTo=Jd-3GCUEiKtv)\\n\",\n+        \"\\n\",\n+        \">>>[Discriminator loss](#scrollTo=PKY_iPSPNWoj)\\n\",\n+        \"\\n\",\n+        \">>[Set up GANs for Training](#scrollTo=Rw1fkAczTQYh)\\n\",\n+        \"\\n\",\n+        \">>[Train the GANs](#scrollTo=dZrd4CdjR-Fp)\\n\",\n+        \"\\n\",\n+        \">>[Generated images](#scrollTo=P4M_vIbUi7c0)\\n\",\n+        \"\\n\",\n+        \">>[Learn more about GANs](#scrollTo=k6qC-SbjK0yW)\\n\",\n+        \"\\n\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"colab_type\": \"text\",\n+        \"id\": \"2MbKJY38Puy9\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## What are GANs?\\n\",\n+        \"GANs, or [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661), are a framework for estimating generative models. Two models are trained simultaneously by an adversarial process: a Generator, which is responsible for generating data (say, images), and a Discriminator, which is responsible for estimating the probability that an image was drawn from the training data (the image is real), or was produced by the Generator (the image is fake). During training, the Generator becomes progressively better at generating images, until the Discriminator is no longer able to distinguish real images from fake. \\n\",\n+        \"\\n\",\n+        \"![alt text](https://github.com/margaretmz/tensorflow/blob/margaret-dcgan/tensorflow/contrib/eager/python/examples/generative_examples/gans_diagram.png?raw=1)\\n\",\n+        \"\\n\",\n+        \"We will demonstrate this process end-to-end on MNIST. Below is an animation that shows a series of images produced by the Generator as it was trained for 50 epochs. Overtime, the generated images become increasingly difficult to distinguish from the training set.\\n\",\n+        \"\\n\",\n+        \"To learn more about GANs, we recommend MIT's [Intro to Deep Learning](http://introtodeeplearning.com/) course, which includes a lecture on Deep Generative Models ([video](https://youtu.be/JVb54xhEw6Y) | [slides](http://introtodeeplearning.com/materials/2018_6S191_Lecture4.pdf)). Now, let's head to the code!\\n\",\n         \"\\n\",\n         \"![sample output](https://tensorflow.org/images/gan/dcgan.gif)\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"u_2z-B3piVsw\"\n+        \"id\": \"u_2z-B3piVsw\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n-        \"# to generate gifs\\n\",\n+        \"# Install imgeio in order to generate an animated gif showing the image generating process\\n\",\n         \"!pip install imageio\"\n-      ]\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"markdown\",\n       \"metadata\": {\n         \"colab_type\": \"text\",\n         \"id\": \"e1_Y75QXJS6h\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"## Import TensorFlow and enable eager execution\"\n+        \"### Import TensorFlow and enable eager execution\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"YfIk2es3hJEd\"\n+        \"id\": \"YfIk2es3hJEd\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n-        \"from __future__ import absolute_import, division, print_function\\n\",\n-        \"\\n\",\n-        \"# Import TensorFlow \\u003e= 1.10 and enable eager execution\\n\",\n         \"import tensorflow as tf\\n\",\n         \"tf.enable_eager_execution()\\n\",\n         \"\\n\",\n-        \"import os\\n\",\n-        \"import time\\n\",\n-        \"import numpy as np\\n\",\n         \"import glob\\n\",\n+        \"import imageio\\n\",\n         \"import matplotlib.pyplot as plt\\n\",\n+        \"import numpy as np\\n\",\n+        \"import os\\n\",\n         \"import PIL\\n\",\n-        \"import imageio\\n\",\n+        \"import time\\n\",\n+        \"\\n\",\n         \"from IPython import display\"\n-      ]\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"markdown\",\n       \"metadata\": {\n         \"colab_type\": \"text\",\n         \"id\": \"iYn4MdZnKCey\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"## Load the dataset\\n\",\n+        \"### Load the dataset\\n\",\n         \"\\n\",\n-        \"We are going to use the MNIST dataset to train the generator and the discriminator. The generator will then generate handwritten digits.\"\n+        \"We are going to use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data.\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"a4fYMGxGhrna\"\n+        \"id\": \"a4fYMGxGhrna\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n         \"(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\"\n-      ]\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"NFC2ghIdiZYE\"\n+        \"id\": \"NFC2ghIdiZYE\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n         \"train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\\n\",\n-        \"# We are normalizing the images to the range of [-1, 1]\\n\",\n-        \"train_images = (train_images - 127.5) / 127.5\"\n-      ]\n+        \"train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"S4PIDhoDLbsZ\"\n+        \"id\": \"S4PIDhoDLbsZ\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n         \"BUFFER_SIZE = 60000\\n\",\n         \"BATCH_SIZE = 256\"\n-      ]\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"markdown\",\n       \"metadata\": {\n         \"colab_type\": \"text\",\n         \"id\": \"PIGN6ouoQxt3\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"## Use tf.data to create batches and shuffle the dataset\"\n+        \"### Use tf.data to create batches and shuffle the dataset\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"-yKCCQOoJ7cn\"\n+        \"id\": \"-yKCCQOoJ7cn\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n         \"train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\"\n-      ]\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"markdown\",\n       \"metadata\": {\n         \"colab_type\": \"text\",\n         \"id\": \"THY-sZMiQ4UV\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"## Write the generator and discriminator models\\n\",\n+        \"## Create the models\\n\",\n         \"\\n\",\n-        \"* **Generator** \\n\",\n-        \"  * It is responsible for **creating convincing images that are good enough to fool the discriminator**.\\n\",\n-        \"  * It consists of Conv2DTranspose (Upsampling) layers. We start with a fully connected layer and upsample the image 2 times so as to reach the desired image size (mnist image size) which is (28, 28, 1). \\n\",\n-        \"  * We use **leaky relu** activation except for the **last layer** which uses **tanh** activation.\\n\",\n-        \"  \\n\",\n-        \"* **Discriminator**\\n\",\n-        \"  * **The discriminator is responsible for classifying the fake images from the real images.**\\n\",\n-        \"  * In other words, the discriminator is given generated images (from the generator) and the real MNIST images. The job of the discriminator is to classify these images into fake (generated) and real (MNIST images).\\n\",\n-        \"  * **Basically the generator should be good enough to fool the discriminator that the generated images are real**.\"\n+        \"We will use tf.keras [Sequential API](https://www.tensorflow.org/guide/keras#sequential_model) to define the generator and discriminator models.\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"colab_type\": \"text\",\n+        \"id\": \"-tEyxE-GMC48\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"### The Generator Model\\n\",\n+        \"\\n\",\n+        \"The generator is responsible for creating convincing images that are good enough to fool the discriminator. The network architecture for the generator consists of [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) (Upsampling) layers. We start with a fully connected layer and upsample the image two times in order to reach the desired image size of 28x28x1. We increase the width and height, and reduce the depth as we move through the layers in the network. We use [Leaky ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU) activation for each layer except for the last one where we use a tanh activation.\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n+        \"id\": \"6bpTcDqoLWjY\",\n         \"colab_type\": \"code\",\n-        \"id\": \"VGLbvBEmjK0a\"\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n-        \"class Generator(tf.keras.Model):\\n\",\n-        \"  def __init__(self):\\n\",\n-        \"    super(Generator, self).__init__()\\n\",\n-        \"    self.fc1 = tf.keras.layers.Dense(7*7*64, use_bias=False)\\n\",\n-        \"    self.batchnorm1 = tf.keras.layers.BatchNormalization()\\n\",\n-        \"    \\n\",\n-        \"    self.conv1 = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False)\\n\",\n-        \"    self.batchnorm2 = tf.keras.layers.BatchNormalization()\\n\",\n-        \"    \\n\",\n-        \"    self.conv2 = tf.keras.layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)\\n\",\n-        \"    self.batchnorm3 = tf.keras.layers.BatchNormalization()\\n\",\n+        \"def make_generator_model():\\n\",\n+        \"  return tf.keras.Sequential([\\n\",\n+        \"    tf.keras.layers.Dense(7*7*256, use_bias=False),\\n\",\n+        \"    tf.keras.layers.BatchNormalization(),\\n\",\n+        \"    tf.keras.layers.LeakyReLU(),\\n\",\n+        \"      \\n\",\n+        \"    tf.keras.layers.Reshape((7, 7, 256)),\\n\",\n+        \"    # Reshape layer to 7x7x64 \\n\",\n         \"    \\n\",\n-        \"    self.conv3 = tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False)\\n\",\n+        \"    tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\\n\",\n+        \"    # Layer shape is still 7x7x64 \\n\",\n+        \"    tf.keras.layers.BatchNormalization(),\\n\",\n+        \"    tf.keras.layers.LeakyReLU(),\\n\",\n         \"\\n\",\n-        \"  def call(self, x, training=True):\\n\",\n-        \"    x = self.fc1(x)\\n\",\n-        \"    x = self.batchnorm1(x, training=training)\\n\",\n-        \"    x = tf.nn.relu(x)\\n\",\n+        \"    tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\\n\",\n+        \"    # Layer shape is now 14x14x32\\n\",\n+        \"    tf.keras.layers.BatchNormalization(),\\n\",\n+        \"    tf.keras.layers.LeakyReLU(),\\n\",\n         \"\\n\",\n-        \"    x = tf.reshape(x, shape=(-1, 7, 7, 64))\\n\",\n-        \"\\n\",\n-        \"    x = self.conv1(x)\\n\",\n-        \"    x = self.batchnorm2(x, training=training)\\n\",\n-        \"    x = tf.nn.relu(x)\\n\",\n-        \"\\n\",\n-        \"    x = self.conv2(x)\\n\",\n-        \"    x = self.batchnorm3(x, training=training)\\n\",\n-        \"    x = tf.nn.relu(x)\\n\",\n+        \"    tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\\n\",\n+        \"    # Layer shape is now 28x28x1\\n\",\n+        \"  ])\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"colab_type\": \"text\",\n+        \"id\": \"D0IKnaCtg6WE\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"### The Discriminator model\\n\",\n         \"\\n\",\n-        \"    x = tf.nn.tanh(self.conv3(x))  \\n\",\n-        \"    return x\"\n+        \"The discriminator is responsible for distinguishing fake images from real images. It's similar to a regular CNN-based image classifier.\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n+        \"id\": \"dw2tPLmk2pEP\",\n         \"colab_type\": \"code\",\n-        \"id\": \"bkOfJxk5j5Hi\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"class Discriminator(tf.keras.Model):\\n\",\n-        \"  def __init__(self):\\n\",\n-        \"    super(Discriminator, self).__init__()\\n\",\n-        \"    self.conv1 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')\\n\",\n-        \"    self.conv2 = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')\\n\",\n-        \"    self.dropout = tf.keras.layers.Dropout(0.3)\\n\",\n-        \"    self.flatten = tf.keras.layers.Flatten()\\n\",\n-        \"    self.fc1 = tf.keras.layers.Dense(1)\\n\",\n-        \"\\n\",\n-        \"  def call(self, x, training=True):\\n\",\n-        \"    x = tf.nn.leaky_relu(self.conv1(x))\\n\",\n-        \"    x = self.dropout(x, training=training)\\n\",\n-        \"    x = tf.nn.leaky_relu(self.conv2(x))\\n\",\n-        \"    x = self.dropout(x, training=training)\\n\",\n-        \"    x = self.flatten(x)\\n\",\n-        \"    x = self.fc1(x)\\n\",\n-        \"    return x\"\n-      ]\n+        \"colab\": {}\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"def make_discriminator_model():\\n\",\n+        \"   return tf.keras.Sequential([\\n\",\n+        \"      tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'),\\n\",\n+        \"      tf.keras.layers.LeakyReLU(),\\n\",\n+        \"      tf.keras.layers.Dropout(0.3),\\n\",\n+        \"      \\n\",\n+        \"      tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\\n\",\n+        \"      tf.keras.layers.LeakyReLU(),\\n\",\n+        \"      tf.keras.layers.Dropout(0.3),\\n\",\n+        \"       \\n\",\n+        \"      tf.keras.layers.Flatten(),\\n\",\n+        \"      tf.keras.layers.Dense(1)\\n\",\n+        \"     ])\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n+      \"metadata\": {\n+        \"colab_type\": \"code\",\n+        \"id\": \"gDkA05NE6QMs\",\n+        \"colab\": {}\n+      },\n       \"cell_type\": \"code\",\n+      \"source\": [\n+        \"generator = make_generator_model()\\n\",\n+        \"discriminator = make_discriminator_model()\"\n+      ],\n       \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n       \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"gDkA05NE6QMs\"\n+        \"colab_type\": \"text\",\n+        \"id\": \"6TSZgwc2BUQ-\"\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"generator = Generator()\\n\",\n-        \"discriminator = Discriminator()\"\n+        \"\\n\",\n+        \"This model takes about ~30 seconds per epoch to train on a single Tesla K80 on Colab, as of July 2018. Eager execution can sometimes be slower than executing the equivalent graph due to overheads of interpreting Python code. By using [tf.contrib.eager.defun](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun) to create graph functions, we get a ~10 secs/epoch performance boost. This way we get the best of both eager execution (easier for debugging) and graph mode (better performance).\"", "path": "tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb", "position": null, "original_position": 486, "commit_id": "459bfb3b324e194be33aa66f38a71291f3d82b95", "original_commit_id": "db6c86cffa294c7e35e9f81e6f92a7a53b3a531c", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "body": "Optional: Instead of `defun`ing the model calls, did you try `defun`-ing the whole training step? That might yield some good improvements too (and will be simpler)?\r\n\r\nSo instead of:\r\n\r\n```python\r\ndef train(dataset, epochs, noise_dim):\r\n  for epoch in range(epochs):\r\n    for images in dataset:\r\n      ...\r\n```\r\n\r\nYou could have:\r\n\r\n```python\r\ndef train_step(images):\r\n   noise = tf.random_normal(...)\r\n   with tf.GradientTape() as tape:\r\n      ...\r\n   \r\n\r\ndef train(dataset, epochs):\r\n  for epoch in epochs:\r\n    for images in dataset:\r\n      train_step(images)\r\n    # All that display stuff\r\n```\r\n\r\nThen for faster training you could do:\r\n```python\r\ntrain_step = tf.contrib.eager.defun(train_step)\r\n```\r\n\r\n", "created_at": "2018-10-23T00:35:01Z", "updated_at": "2018-10-24T17:54:02Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/23035#discussion_r227183835", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23035", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/227183835"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/23035#discussion_r227183835"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23035"}}, "body_html": "<p>Optional: Instead of <code>defun</code>ing the model calls, did you try <code>defun</code>-ing the whole training step? That might yield some good improvements too (and will be simpler)?</p>\n<p>So instead of:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">dataset</span>, <span class=\"pl-smi\">epochs</span>, <span class=\"pl-smi\">noise_dim</span>):\n  <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(epochs):\n    <span class=\"pl-k\">for</span> images <span class=\"pl-k\">in</span> dataset:\n      <span class=\"pl-c1\">...</span></pre></div>\n<p>You could have:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">train_step</span>(<span class=\"pl-smi\">images</span>):\n   noise <span class=\"pl-k\">=</span> tf.random_normal(<span class=\"pl-c1\">...</span>)\n   <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n      <span class=\"pl-c1\">...</span>\n   \n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">dataset</span>, <span class=\"pl-smi\">epochs</span>):\n  <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> epochs:\n    <span class=\"pl-k\">for</span> images <span class=\"pl-k\">in</span> dataset:\n      train_step(images)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> All that display stuff</span></pre></div>\n<p>Then for faster training you could do:</p>\n<div class=\"highlight highlight-source-python\"><pre>train_step <span class=\"pl-k\">=</span> tf.contrib.eager.defun(train_step)</pre></div>", "body_text": "Optional: Instead of defuning the model calls, did you try defun-ing the whole training step? That might yield some good improvements too (and will be simpler)?\nSo instead of:\ndef train(dataset, epochs, noise_dim):\n  for epoch in range(epochs):\n    for images in dataset:\n      ...\nYou could have:\ndef train_step(images):\n   noise = tf.random_normal(...)\n   with tf.GradientTape() as tape:\n      ...\n   \n\ndef train(dataset, epochs):\n  for epoch in epochs:\n    for images in dataset:\n      train_step(images)\n    # All that display stuff\nThen for faster training you could do:\ntrain_step = tf.contrib.eager.defun(train_step)"}