{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/227597384", "pull_request_review_id": 167676143, "id": 227597384, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzU5NzM4NA==", "diff_hunk": "@@ -327,170 +408,200 @@\n         \"    total_loss = real_loss + generated_loss\\n\",\n         \"\\n\",\n         \"    return total_loss\"\n-      ]\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"90BIcCKcDMxz\"\n+        \"colab_type\": \"text\",\n+        \"id\": \"MgIc7i0th_Iu\"\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"def generator_loss(generated_output):\\n\",\n-        \"    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)\"\n+        \"The discriminator and the generator optimizers are different since we will train two networks separately.\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"iWCn_PVdEJZ7\"\n+        \"id\": \"iWCn_PVdEJZ7\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n-        \"discriminator_optimizer = tf.train.AdamOptimizer(1e-4)\\n\",\n-        \"generator_optimizer = tf.train.AdamOptimizer(1e-4)\"\n-      ]\n+        \"generator_optimizer = tf.train.AdamOptimizer(1e-4)\\n\",\n+        \"discriminator_optimizer = tf.train.AdamOptimizer(1e-4)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"markdown\",\n       \"metadata\": {\n         \"colab_type\": \"text\",\n         \"id\": \"mWtinsGDPJlV\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"## Checkpoints (Object-based saving)\"\n+        \"**Checkpoints (Object-based saving)**\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"CA1w-7s2POEy\"\n+        \"id\": \"CA1w-7s2POEy\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n         \"checkpoint_dir = './training_checkpoints'\\n\",\n         \"checkpoint_prefix = os.path.join(checkpoint_dir, \\\"ckpt\\\")\\n\",\n         \"checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\\n\",\n         \"                                 discriminator_optimizer=discriminator_optimizer,\\n\",\n         \"                                 generator=generator,\\n\",\n         \"                                 discriminator=discriminator)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"colab_type\": \"text\",\n+        \"id\": \"Rw1fkAczTQYh\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Set up GANs for Training\\n\",\n+        \"\\n\"\n       ]\n     },\n     {\n+      \"metadata\": {\n+        \"colab_type\": \"text\",\n+        \"id\": \"5QC5BABamh_c\"\n+      },\n       \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"Now it's time to put together the generator and discriminator to set up the Generative Adversarial Networks, as you see in the diagam at the beginning of the tutorial.\"\n+      ]\n+    },\n+    {\n       \"metadata\": {\n         \"colab_type\": \"text\",\n-        \"id\": \"Rw1fkAczTQYh\"\n+        \"id\": \"Ff6oN6PZX27n\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"## Training\\n\",\n-        \"\\n\",\n-        \"* We start by iterating over the dataset\\n\",\n-        \"* The generator is given **noise as an input** which when passed through the generator model will output a image looking like a handwritten digit\\n\",\n-        \"* The discriminator is given the **real MNIST images as well as the generated images (from the generator)**.\\n\",\n-        \"* Next, we calculate the generator and the discriminator loss.\\n\",\n-        \"* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables (inputs) and apply those to the optimizer.\\n\",\n-        \"\\n\",\n-        \"## Generate Images\\n\",\n-        \"\\n\",\n-        \"* After training, its time to generate some images!\\n\",\n-        \"* We start by creating noise array as an input to the generator\\n\",\n-        \"* The generator will then convert the noise into handwritten images.\\n\",\n-        \"* Last step is to plot the predictions and **voila!**\"\n+        \"**Define training parameters**\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"NS2GWywBbAWo\"\n+        \"id\": \"NS2GWywBbAWo\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n-        \"EPOCHS = 150\\n\",\n+        \"EPOCHS = 50\\n\",\n         \"noise_dim = 100\\n\",\n         \"num_examples_to_generate = 16\\n\",\n         \"\\n\",\n-        \"# keeping the random vector constant for generation (prediction) so\\n\",\n-        \"# it will be easier to see the improvement of the gan.\\n\",\n+        \"# We'll re-use this random vector used to seed the generator so\\n\",\n+        \"# it will be easier to see the improvement over time.\\n\",\n         \"random_vector_for_generation = tf.random_normal([num_examples_to_generate,\\n\",\n         \"                                                 noise_dim])\"\n-      ]\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"RmdVsmvhPxyy\"\n+        \"colab_type\": \"text\",\n+        \"id\": \"jylSonrqSWfi\"\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"def generate_and_save_images(model, epoch, test_input):\\n\",\n-        \"  # make sure the training parameter is set to False because we\\n\",\n-        \"  # don't want to train the batchnorm layer when doing inference.\\n\",\n-        \"  predictions = model(test_input, training=False)\\n\",\n+        \"**Define training method**\\n\",\n         \"\\n\",\n-        \"  fig = plt.figure(figsize=(4,4))\\n\",\n-        \"  \\n\",\n-        \"  for i in range(predictions.shape[0]):\\n\",\n-        \"      plt.subplot(4, 4, i+1)\\n\",\n-        \"      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\\n\",\n-        \"      plt.axis('off')\\n\",\n-        \"        \\n\",\n-        \"  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\\n\",\n-        \"  plt.show()\"\n+        \"We start by iterating over the dataset. The generator is given a random vector as an input which is processed to  output an image looking like a handwritten digit. The discriminator is then shown the real MNIST images as well as the generated images.\\n\",\n+        \"\\n\",\n+        \"Next, we calculate the generator and the discriminator loss. Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables.\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n+        \"id\": \"3t5ibNo05jCB\",\n         \"colab_type\": \"code\",\n-        \"id\": \"2M7LmLtGEMQJ\"\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n-        \"def train(dataset, epochs, noise_dim):  \\n\",\n-        \"  for epoch in range(epochs):\\n\",\n-        \"    start = time.time()\\n\",\n-        \"    \\n\",\n-        \"    for images in dataset:\\n\",\n-        \"      # generating noise from a uniform distribution\\n\",\n+        \"def train_step(images):\\n\",\n+        \"   # generating noise from a uniform distribution\\n\",\n         \"      noise = tf.random_normal([BATCH_SIZE, noise_dim])\\n\",\n         \"      \\n\",\n         \"      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\\n\",\n         \"        generated_images = generator(noise, training=True)\\n\",\n         \"      \\n\",\n         \"        real_output = discriminator(images, training=True)\\n\",\n         \"        generated_output = discriminator(generated_images, training=True)\\n\",\n-        \"        \\n\",\n+        \"         \\n\",\n         \"        gen_loss = generator_loss(generated_output)\\n\",\n         \"        disc_loss = discriminator_loss(real_output, generated_output)\\n\",\n         \"        \\n\",\n         \"      gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)\\n\",\n         \"      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)\\n\",\n         \"      \\n\",\n         \"      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables))\\n\",\n-        \"      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))\\n\",\n+        \"      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"colab_type\": \"text\",\n+        \"id\": \"6TSZgwc2BUQ-\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n         \"\\n\",\n-        \"      \\n\",\n-        \"    if epoch % 1 == 0:\\n\",\n-        \"      display.clear_output(wait=True)\\n\",\n-        \"      generate_and_save_images(generator,\\n\",\n+        \"This model takes about ~30 seconds per epoch to train on a single Tesla K80 on Colab, as of October 2018. \\n\",\n+        \"\\n\",\n+        \"Eager execution can sometimes be slower than executing the equivalent graph due to overheads of interpreting Python code. By using [tf.contrib.eager.defun](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun) to create graph functions, we get a ~20 secs/epoch performance boost (from ~50 secs/epoch down to ~30 secs/epoch). This way we get the best of both eager execution (easier for debugging) and graph mode (better performance).\"", "path": "tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb", "position": null, "original_position": 823, "commit_id": "459bfb3b324e194be33aa66f38a71291f3d82b95", "original_commit_id": "c83012877c77d90652f688b98ce46f485769b4a5", "user": {"login": "margaretmz", "id": 1885008, "node_id": "MDQ6VXNlcjE4ODUwMDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1885008?v=4", "gravatar_id": "", "url": "https://api.github.com/users/margaretmz", "html_url": "https://github.com/margaretmz", "followers_url": "https://api.github.com/users/margaretmz/followers", "following_url": "https://api.github.com/users/margaretmz/following{/other_user}", "gists_url": "https://api.github.com/users/margaretmz/gists{/gist_id}", "starred_url": "https://api.github.com/users/margaretmz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/margaretmz/subscriptions", "organizations_url": "https://api.github.com/users/margaretmz/orgs", "repos_url": "https://api.github.com/users/margaretmz/repos", "events_url": "https://api.github.com/users/margaretmz/events{/privacy}", "received_events_url": "https://api.github.com/users/margaretmz/received_events", "type": "User", "site_admin": false}, "body": "Updated as suggested.", "created_at": "2018-10-23T23:16:44Z", "updated_at": "2018-10-24T17:54:02Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/23035#discussion_r227597384", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23035", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/227597384"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/23035#discussion_r227597384"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23035"}}, "body_html": "<p>Updated as suggested.</p>", "body_text": "Updated as suggested.", "in_reply_to_id": 227594559}