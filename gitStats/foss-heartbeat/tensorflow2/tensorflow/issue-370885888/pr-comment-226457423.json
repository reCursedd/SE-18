{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/226457423", "pull_request_review_id": 166285202, "id": 226457423, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNjQ1NzQyMw==", "diff_hunk": "@@ -1,694 +1,928 @@\n {\n-  \"cells\": [\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"0TD5ZrvEMbhZ\"\n-      },\n-      \"source\": [\n-        \"##### Copyright 2018 The TensorFlow Authors.\\n\",\n-        \"\\n\",\n-        \"Licensed under the Apache License, Version 2.0 (the \\\"License\\\").\\n\",\n-        \"\\n\",\n-        \"# DCGAN: An example with tf.keras and eager\\n\",\n-        \"\\n\",\n-        \"\\u003ctable class=\\\"tfo-notebook-buttons\\\" align=\\\"left\\\"\\u003e\\u003ctd\\u003e\\n\",\n-        \"\\u003ca target=\\\"_blank\\\"  href=\\\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\\\"\\u003e\\n\",\n-        \"    \\u003cimg src=\\\"https://www.tensorflow.org/images/colab_logo_32px.png\\\" /\\u003eRun in Google Colab\\u003c/a\\u003e  \\n\",\n-        \"\\u003c/td\\u003e\\u003ctd\\u003e\\n\",\n-        \"\\u003ca target=\\\"_blank\\\"  href=\\\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\\\"\\u003e\\u003cimg width=32px src=\\\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\\\" /\\u003eView source on GitHub\\u003c/a\\u003e\\u003c/td\\u003e\\u003c/table\\u003e\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"ITZuApL56Mny\"\n-      },\n-      \"source\": [\n-        \"This notebook demonstrates how to generate images of handwritten digits using [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). To do so, we use Deep Convolutional Generative Adverserial Networks ([DCGAN](https://arxiv.org/pdf/1511.06434.pdf)).\\n\",\n-        \"\\n\",\n-        \"This model takes about ~30 seconds per epoch (using tf.contrib.eager.defun to create graph functions) to train on a single Tesla K80 on Colab, as of July 2018.\\n\",\n-        \"\\n\",\n-        \"Below is the output generated after training the generator and discriminator models for 150 epochs.\\n\",\n-        \"\\n\",\n-        \"![sample output](https://tensorflow.org/images/gan/dcgan.gif)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"u_2z-B3piVsw\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"# to generate gifs\\n\",\n-        \"!pip install imageio\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"e1_Y75QXJS6h\"\n-      },\n-      \"source\": [\n-        \"## Import TensorFlow and enable eager execution\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"YfIk2es3hJEd\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"from __future__ import absolute_import, division, print_function\\n\",\n-        \"\\n\",\n-        \"# Import TensorFlow \\u003e= 1.10 and enable eager execution\\n\",\n-        \"import tensorflow as tf\\n\",\n-        \"tf.enable_eager_execution()\\n\",\n-        \"\\n\",\n-        \"import os\\n\",\n-        \"import time\\n\",\n-        \"import numpy as np\\n\",\n-        \"import glob\\n\",\n-        \"import matplotlib.pyplot as plt\\n\",\n-        \"import PIL\\n\",\n-        \"import imageio\\n\",\n-        \"from IPython import display\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"iYn4MdZnKCey\"\n-      },\n-      \"source\": [\n-        \"## Load the dataset\\n\",\n-        \"\\n\",\n-        \"We are going to use the MNIST dataset to train the generator and the discriminator. The generator will then generate handwritten digits.\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"a4fYMGxGhrna\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"NFC2ghIdiZYE\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\\n\",\n-        \"# We are normalizing the images to the range of [-1, 1]\\n\",\n-        \"train_images = (train_images - 127.5) / 127.5\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"S4PIDhoDLbsZ\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"BUFFER_SIZE = 60000\\n\",\n-        \"BATCH_SIZE = 256\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"PIGN6ouoQxt3\"\n-      },\n-      \"source\": [\n-        \"## Use tf.data to create batches and shuffle the dataset\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"-yKCCQOoJ7cn\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"THY-sZMiQ4UV\"\n-      },\n-      \"source\": [\n-        \"## Write the generator and discriminator models\\n\",\n-        \"\\n\",\n-        \"* **Generator** \\n\",\n-        \"  * It is responsible for **creating convincing images that are good enough to fool the discriminator**.\\n\",\n-        \"  * It consists of Conv2DTranspose (Upsampling) layers. We start with a fully connected layer and upsample the image 2 times so as to reach the desired image size (mnist image size) which is (28, 28, 1). \\n\",\n-        \"  * We use **leaky relu** activation except for the **last layer** which uses **tanh** activation.\\n\",\n-        \"  \\n\",\n-        \"* **Discriminator**\\n\",\n-        \"  * **The discriminator is responsible for classifying the fake images from the real images.**\\n\",\n-        \"  * In other words, the discriminator is given generated images (from the generator) and the real MNIST images. The job of the discriminator is to classify these images into fake (generated) and real (MNIST images).\\n\",\n-        \"  * **Basically the generator should be good enough to fool the discriminator that the generated images are real**.\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"VGLbvBEmjK0a\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"class Generator(tf.keras.Model):\\n\",\n-        \"  def __init__(self):\\n\",\n-        \"    super(Generator, self).__init__()\\n\",\n-        \"    self.fc1 = tf.keras.layers.Dense(7*7*64, use_bias=False)\\n\",\n-        \"    self.batchnorm1 = tf.keras.layers.BatchNormalization()\\n\",\n-        \"    \\n\",\n-        \"    self.conv1 = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False)\\n\",\n-        \"    self.batchnorm2 = tf.keras.layers.BatchNormalization()\\n\",\n-        \"    \\n\",\n-        \"    self.conv2 = tf.keras.layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)\\n\",\n-        \"    self.batchnorm3 = tf.keras.layers.BatchNormalization()\\n\",\n-        \"    \\n\",\n-        \"    self.conv3 = tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False)\\n\",\n-        \"\\n\",\n-        \"  def call(self, x, training=True):\\n\",\n-        \"    x = self.fc1(x)\\n\",\n-        \"    x = self.batchnorm1(x, training=training)\\n\",\n-        \"    x = tf.nn.relu(x)\\n\",\n-        \"\\n\",\n-        \"    x = tf.reshape(x, shape=(-1, 7, 7, 64))\\n\",\n-        \"\\n\",\n-        \"    x = self.conv1(x)\\n\",\n-        \"    x = self.batchnorm2(x, training=training)\\n\",\n-        \"    x = tf.nn.relu(x)\\n\",\n-        \"\\n\",\n-        \"    x = self.conv2(x)\\n\",\n-        \"    x = self.batchnorm3(x, training=training)\\n\",\n-        \"    x = tf.nn.relu(x)\\n\",\n-        \"\\n\",\n-        \"    x = tf.nn.tanh(self.conv3(x))  \\n\",\n-        \"    return x\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"bkOfJxk5j5Hi\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"class Discriminator(tf.keras.Model):\\n\",\n-        \"  def __init__(self):\\n\",\n-        \"    super(Discriminator, self).__init__()\\n\",\n-        \"    self.conv1 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')\\n\",\n-        \"    self.conv2 = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')\\n\",\n-        \"    self.dropout = tf.keras.layers.Dropout(0.3)\\n\",\n-        \"    self.flatten = tf.keras.layers.Flatten()\\n\",\n-        \"    self.fc1 = tf.keras.layers.Dense(1)\\n\",\n-        \"\\n\",\n-        \"  def call(self, x, training=True):\\n\",\n-        \"    x = tf.nn.leaky_relu(self.conv1(x))\\n\",\n-        \"    x = self.dropout(x, training=training)\\n\",\n-        \"    x = tf.nn.leaky_relu(self.conv2(x))\\n\",\n-        \"    x = self.dropout(x, training=training)\\n\",\n-        \"    x = self.flatten(x)\\n\",\n-        \"    x = self.fc1(x)\\n\",\n-        \"    return x\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"gDkA05NE6QMs\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"generator = Generator()\\n\",\n-        \"discriminator = Discriminator()\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"k1HpMSLImuRi\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"# Defun gives 10 secs/epoch performance boost\\n\",\n-        \"generator.call = tf.contrib.eager.defun(generator.call)\\n\",\n-        \"discriminator.call = tf.contrib.eager.defun(discriminator.call)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"0FMYgY_mPfTi\"\n-      },\n-      \"source\": [\n-        \"## Define the loss functions and the optimizer\\n\",\n-        \"\\n\",\n-        \"* **Discriminator loss**\\n\",\n-        \"  * The discriminator loss function takes 2 inputs; **real images, generated images**\\n\",\n-        \"  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones (since these are the real images)**\\n\",\n-        \"  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**\\n\",\n-        \"  * Then the total_loss is the sum of real_loss and the generated_loss\\n\",\n-        \"  \\n\",\n-        \"* **Generator loss**\\n\",\n-        \"  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**\\n\",\n-        \"  \\n\",\n-        \"\\n\",\n-        \"* The discriminator and the generator optimizers are different since we will train them separately.\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"wkMNfBWlT-PV\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"def discriminator_loss(real_output, generated_output):\\n\",\n-        \"    # [1,1,...,1] with real output since it is true and we want\\n\",\n-        \"    # our generated examples to look like it\\n\",\n-        \"    real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(real_output), logits=real_output)\\n\",\n-        \"\\n\",\n-        \"    # [0,0,...,0] with generated images since they are fake\\n\",\n-        \"    generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.zeros_like(generated_output), logits=generated_output)\\n\",\n-        \"\\n\",\n-        \"    total_loss = real_loss + generated_loss\\n\",\n-        \"\\n\",\n-        \"    return total_loss\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"90BIcCKcDMxz\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"def generator_loss(generated_output):\\n\",\n-        \"    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"iWCn_PVdEJZ7\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"discriminator_optimizer = tf.train.AdamOptimizer(1e-4)\\n\",\n-        \"generator_optimizer = tf.train.AdamOptimizer(1e-4)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"mWtinsGDPJlV\"\n-      },\n-      \"source\": [\n-        \"## Checkpoints (Object-based saving)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"CA1w-7s2POEy\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"checkpoint_dir = './training_checkpoints'\\n\",\n-        \"checkpoint_prefix = os.path.join(checkpoint_dir, \\\"ckpt\\\")\\n\",\n-        \"checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\\n\",\n-        \"                                 discriminator_optimizer=discriminator_optimizer,\\n\",\n-        \"                                 generator=generator,\\n\",\n-        \"                                 discriminator=discriminator)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"Rw1fkAczTQYh\"\n-      },\n-      \"source\": [\n-        \"## Training\\n\",\n-        \"\\n\",\n-        \"* We start by iterating over the dataset\\n\",\n-        \"* The generator is given **noise as an input** which when passed through the generator model will output a image looking like a handwritten digit\\n\",\n-        \"* The discriminator is given the **real MNIST images as well as the generated images (from the generator)**.\\n\",\n-        \"* Next, we calculate the generator and the discriminator loss.\\n\",\n-        \"* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables (inputs) and apply those to the optimizer.\\n\",\n-        \"\\n\",\n-        \"## Generate Images\\n\",\n-        \"\\n\",\n-        \"* After training, its time to generate some images!\\n\",\n-        \"* We start by creating noise array as an input to the generator\\n\",\n-        \"* The generator will then convert the noise into handwritten images.\\n\",\n-        \"* Last step is to plot the predictions and **voila!**\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"NS2GWywBbAWo\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"EPOCHS = 150\\n\",\n-        \"noise_dim = 100\\n\",\n-        \"num_examples_to_generate = 16\\n\",\n-        \"\\n\",\n-        \"# keeping the random vector constant for generation (prediction) so\\n\",\n-        \"# it will be easier to see the improvement of the gan.\\n\",\n-        \"random_vector_for_generation = tf.random_normal([num_examples_to_generate,\\n\",\n-        \"                                                 noise_dim])\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"RmdVsmvhPxyy\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"def generate_and_save_images(model, epoch, test_input):\\n\",\n-        \"  # make sure the training parameter is set to False because we\\n\",\n-        \"  # don't want to train the batchnorm layer when doing inference.\\n\",\n-        \"  predictions = model(test_input, training=False)\\n\",\n-        \"\\n\",\n-        \"  fig = plt.figure(figsize=(4,4))\\n\",\n-        \"  \\n\",\n-        \"  for i in range(predictions.shape[0]):\\n\",\n-        \"      plt.subplot(4, 4, i+1)\\n\",\n-        \"      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\\n\",\n-        \"      plt.axis('off')\\n\",\n-        \"        \\n\",\n-        \"  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\\n\",\n-        \"  plt.show()\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"2M7LmLtGEMQJ\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"def train(dataset, epochs, noise_dim):  \\n\",\n-        \"  for epoch in range(epochs):\\n\",\n-        \"    start = time.time()\\n\",\n-        \"    \\n\",\n-        \"    for images in dataset:\\n\",\n-        \"      # generating noise from a uniform distribution\\n\",\n-        \"      noise = tf.random_normal([BATCH_SIZE, noise_dim])\\n\",\n-        \"      \\n\",\n-        \"      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\\n\",\n-        \"        generated_images = generator(noise, training=True)\\n\",\n-        \"      \\n\",\n-        \"        real_output = discriminator(images, training=True)\\n\",\n-        \"        generated_output = discriminator(generated_images, training=True)\\n\",\n-        \"        \\n\",\n-        \"        gen_loss = generator_loss(generated_output)\\n\",\n-        \"        disc_loss = discriminator_loss(real_output, generated_output)\\n\",\n-        \"        \\n\",\n-        \"      gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)\\n\",\n-        \"      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)\\n\",\n-        \"      \\n\",\n-        \"      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables))\\n\",\n-        \"      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))\\n\",\n-        \"\\n\",\n-        \"      \\n\",\n-        \"    if epoch % 1 == 0:\\n\",\n-        \"      display.clear_output(wait=True)\\n\",\n-        \"      generate_and_save_images(generator,\\n\",\n-        \"                               epoch + 1,\\n\",\n-        \"                               random_vector_for_generation)\\n\",\n-        \"    \\n\",\n-        \"    # saving (checkpoint) the model every 15 epochs\\n\",\n-        \"    if (epoch + 1) % 15 == 0:\\n\",\n-        \"      checkpoint.save(file_prefix = checkpoint_prefix)\\n\",\n-        \"    \\n\",\n-        \"    print ('Time taken for epoch {} is {} sec'.format(epoch + 1,\\n\",\n-        \"                                                      time.time()-start))\\n\",\n-        \"  # generating after the final epoch\\n\",\n-        \"  display.clear_output(wait=True)\\n\",\n-        \"  generate_and_save_images(generator,\\n\",\n-        \"                           epochs,\\n\",\n-        \"                           random_vector_for_generation)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"Ly3UN0SLLY2l\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"train(train_dataset, EPOCHS, noise_dim)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"rfM4YcPVPkNO\"\n-      },\n-      \"source\": [\n-        \"## Restore the latest checkpoint\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"XhXsd0srPo8c\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"# restoring the latest checkpoint in checkpoint_dir\\n\",\n-        \"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"P4M_vIbUi7c0\"\n-      },\n-      \"source\": [\n-        \"## Display an image using the epoch number\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"WfO5wCdclHGL\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"def display_image(epoch_no):\\n\",\n-        \"  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"5x3q9_Oe5q0A\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"display_image(EPOCHS)\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"NywiH3nL8guF\"\n-      },\n-      \"source\": [\n-        \"## Generate a GIF of all the saved images.\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"xmO0Dmu2WICn\"\n-      },\n-      \"source\": [\n-        \"\\u003c!-- TODO(markdaoust): Remove the hack when Ipython version is updated --\\u003e\\n\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"IGKQgENQ8lEI\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"with imageio.get_writer('dcgan.gif', mode='I') as writer:\\n\",\n-        \"  filenames = glob.glob('image*.png')\\n\",\n-        \"  filenames = sorted(filenames)\\n\",\n-        \"  last = -1\\n\",\n-        \"  for i,filename in enumerate(filenames):\\n\",\n-        \"    frame = 2*(i**0.5)\\n\",\n-        \"    if round(frame) \\u003e round(last):\\n\",\n-        \"      last = frame\\n\",\n-        \"    else:\\n\",\n-        \"      continue\\n\",\n-        \"    image = imageio.imread(filename)\\n\",\n-        \"    writer.append_data(image)\\n\",\n-        \"  image = imageio.imread(filename)\\n\",\n-        \"  writer.append_data(image)\\n\",\n-        \"    \\n\",\n-        \"# this is a hack to display the gif inside the notebook\\n\",\n-        \"os.system('cp dcgan.gif dcgan.gif.png')\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"uV0yiKpzNP1b\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"display.Image(filename=\\\"dcgan.gif.png\\\")\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"markdown\",\n-      \"metadata\": {\n-        \"colab_type\": \"text\",\n-        \"id\": \"6EEG-wePkmJQ\"\n-      },\n-      \"source\": [\n-        \"To downlod the animation from Colab uncomment the code below:\"\n-      ]\n-    },\n-    {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n-      \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"4UJjSnIMOzOJ\"\n-      },\n-      \"outputs\": [],\n-      \"source\": [\n-        \"#from google.colab import files\\n\",\n-        \"#files.download('dcgan.gif')\"\n-      ]\n-    }\n-  ],\n-  \"metadata\": {\n-    \"accelerator\": \"GPU\",\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"0TD5ZrvEMbhZ\"\n+   },\n+   \"source\": [\n+    \"**Copyright 2018 The TensorFlow Authors**.\\n\",\n+    \"\\n\",\n+    \"Licensed under the Apache License, Version 2.0 (the \\\"License\\\").\\n\",\n+    \"\\n\",\n+    \"# Generating Handwritten Digits with DCGAN\\n\",\n+    \"\\n\",\n+    \"<table class=\\\"tfo-notebook-buttons\\\" align=\\\"left\\\"><td>\\n\",\n+    \"<a target=\\\"_blank\\\"  href=\\\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\\\">\\n\",\n+    \"    <img src=\\\"https://www.tensorflow.org/images/colab_logo_32px.png\\\" />Run in Google Colab</a>  \\n\",\n+    \"</td><td>\\n\",\n+    \"<a target=\\\"_blank\\\"  href=\\\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb\\\"><img width=32px src=\\\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\\\" />View source on GitHub</a></td></table>\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"ITZuApL56Mny\"\n+   },\n+   \"source\": [\n+    \"This tutorial demonstrates how to generate images of handwritten digits using a Deep Convolutional Generative Adversarial Network ([DCGAN](https://arxiv.org/pdf/1511.06434.pdf)). The code is written in [tf.keras](https://www.tensorflow.org/programmers_guide/keras) with [eager execution](https://www.tensorflow.org/programmers_guide/eager) enabled. \"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"toc\",\n+    \"id\": \"x2McrO9bMyLN\"\n+   },\n+   \"source\": [\n+    \">[Generating Handwritten Digits with DCGAN](#scrollTo=0TD5ZrvEMbhZ)\\n\",\n+    \"\\n\",\n+    \">>[What are GANs?](#scrollTo=2MbKJY38Puy9)\\n\",\n+    \"\\n\",\n+    \">>>[Import TensorFlow and enable eager execution](#scrollTo=e1_Y75QXJS6h)\\n\",\n+    \"\\n\",\n+    \">>>[Load the dataset](#scrollTo=iYn4MdZnKCey)\\n\",\n+    \"\\n\",\n+    \">>>[Use tf.data to create batches and shuffle the dataset](#scrollTo=PIGN6ouoQxt3)\\n\",\n+    \"\\n\",\n+    \">>[Create the models](#scrollTo=THY-sZMiQ4UV)\\n\",\n+    \"\\n\",\n+    \">>>[The Generator Model](#scrollTo=-tEyxE-GMC48)\\n\",\n+    \"\\n\",\n+    \">>>[The Discriminator model](#scrollTo=D0IKnaCtg6WE)\\n\",\n+    \"\\n\",\n+    \">>[Define the loss functions and the optimizer](#scrollTo=0FMYgY_mPfTi)\\n\",\n+    \"\\n\",\n+    \">>>[Generator loss](#scrollTo=Jd-3GCUEiKtv)\\n\",\n+    \"\\n\",\n+    \">>>[Discriminator loss](#scrollTo=PKY_iPSPNWoj)\\n\",\n+    \"\\n\",\n+    \">>[Set up GANs for Training](#scrollTo=Rw1fkAczTQYh)\\n\",\n+    \"\\n\",\n+    \">>[Train the GANs](#scrollTo=dZrd4CdjR-Fp)\\n\",\n+    \"\\n\",\n+    \">>[Generated images](#scrollTo=P4M_vIbUi7c0)\\n\",\n+    \"\\n\",\n+    \">>[Learn more about GANs](#scrollTo=k6qC-SbjK0yW)\\n\",\n+    \"\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"2MbKJY38Puy9\"\n+   },\n+   \"source\": [\n+    \"## What are GANs?\\n\",\n+    \"GANs, or [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661), are a framework for estimating generative models. Two models are trained simultaneously by an adversarial process: a Generator, which is responsible for generating data (say, images), and a Discriminator, which is responsible for estimating the probability that an image was drawn from the training data (the image is real), or was produced by the Generator (the image is fake). During training, the Generator becomes progressively better at generating images, until the Discriminator is no longer able to distinguish real images from fake. \\n\",\n+    \"\\n\",\n+    \"![alt text](https://github.com/margaretmz/tensorflow/blob/margaret-dcgan/tensorflow/contrib/eager/python/examples/generative_examples/gans_diagram.png?raw=1)\\n\",\n+    \"\\n\",\n+    \"We will demonstrate this process end-to-end on MNIST. Below is an animation that shows a series of images produced by the Generator as it was trained for 150 epochs. Overtime, the generated images become increasingly difficult to distinguish from the training set.\\n\",\n+    \"\\n\",\n+    \"To learn more about GANs, we recommend MIT's [Intro to Deep Learning](http://introtodeeplearning.com/) course, which includes a lecture on Deep Generative Models ([video](https://youtu.be/JVb54xhEw6Y) | [slides](http://introtodeeplearning.com/materials/2018_6S191_Lecture4.pdf)). Now, let's head to the code!\\n\",\n+    \"\\n\",\n+    \"![sample output](https://tensorflow.org/images/gan/dcgan.gif)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n     \"colab\": {\n-      \"collapsed_sections\": [],\n-      \"name\": \"dcgan.ipynb\",\n-      \"private_outputs\": true,\n-      \"provenance\": [\n-        {\n-          \"file_id\": \"1eb0NOTQapkYs3X0v-zL1x5_LFKgDISnp\",\n-          \"timestamp\": 1527173385672\n-        }\n-      ],\n-      \"toc_visible\": true,\n-      \"version\": \"0.3.2\"\n-    },\n-    \"kernelspec\": {\n-      \"display_name\": \"Python 3\",\n-      \"language\": \"python\",\n-      \"name\": \"python3\"\n-    }\n-  },\n-  \"nbformat\": 4,\n-  \"nbformat_minor\": 0\n+     \"base_uri\": \"https://localhost:8080/\",\n+     \"height\": 221\n+    },\n+    \"colab_type\": \"code\",\n+    \"id\": \"u_2z-B3piVsw\",\n+    \"outputId\": \"684f2b6e-7756-448e-da2a-74bcb08d8686\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Install imgeio in order to generate an animated gif showing the image generating process\\n\",\n+    \"!pip install imageio\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"e1_Y75QXJS6h\"\n+   },\n+   \"source\": [\n+    \"### Import TensorFlow and enable eager execution\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"YfIk2es3hJEd\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from __future__ import absolute_import, division, print_function\\n\",\n+    \"\\n\",\n+    \"import tensorflow as tf\\n\",\n+    \"tf.enable_eager_execution()\\n\",\n+    \"\\n\",\n+    \"import glob\\n\",\n+    \"import imageio\\n\",\n+    \"import matplotlib.pyplot as plt\\n\",\n+    \"import numpy as np\\n\",\n+    \"import os\\n\",\n+    \"import PIL\\n\",\n+    \"import time\\n\",\n+    \"\\n\",\n+    \"from IPython import display\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"iYn4MdZnKCey\"\n+   },\n+   \"source\": [\n+    \"### Load the dataset\\n\",\n+    \"\\n\",\n+    \"We are going to use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {\n+     \"base_uri\": \"https://localhost:8080/\",\n+     \"height\": 51\n+    },\n+    \"colab_type\": \"code\",\n+    \"id\": \"a4fYMGxGhrna\",\n+    \"outputId\": \"065f5f41-bdd6-4f4e-bdb6-addce8ff011d\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"NFC2ghIdiZYE\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\\n\",\n+    \"train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"S4PIDhoDLbsZ\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"BUFFER_SIZE = 60000\\n\",\n+    \"BATCH_SIZE = 256\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"PIGN6ouoQxt3\"\n+   },\n+   \"source\": [\n+    \"### Use tf.data to create batches and shuffle the dataset\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"-yKCCQOoJ7cn\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"THY-sZMiQ4UV\"\n+   },\n+   \"source\": [\n+    \"## Create the models\\n\",\n+    \"\\n\",\n+    \"We will use tf.keras [model subclassing](https://www.tensorflow.org/guide/keras#model_subclassing) to define the generator and discriminator. We will create layers in the *init* method, and define the forward pass in the *call* method. Note, we could write these models more compactly using the [Sequential API](https://www.tensorflow.org/guide/keras#sequential_model), e.g., \\n\",\n+    \"\\n\",\n+    \"```def make_generator_model():\\n\",\n+    \"     return tf.keras.Sequential([\\n\",\n+    \"       tf.keras.layers.Dense(7*7*64, use_bias=False),\\n\",\n+    \"       tf.keras.layers.BatchNormalization(),\\n\",\n+    \"       ...])```\\n\",\n+    \"    \\n\",\n+    \"In this case we've used model subclassing, in order to have another example of that style available.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"-tEyxE-GMC48\"\n+   },\n+   \"source\": [\n+    \"### The Generator Model\\n\",\n+    \"\\n\",\n+    \"The generator is responsible for creating convincing images that are good enough to fool the discriminator. The network architecture for the generator consists of [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) (Upsampling) layers. We start with a fully connected layer and upsample the image two times in order to reach the desired image size. We increase the width and height, and reduce the depth as we move through the layers in the network. We use [Leaky ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU) activation except for the last layer which uses tanh activation.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"VGLbvBEmjK0a\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"class Generator(tf.keras.Model):\\n\",\n+    \"  def __init__(self):\\n\",\n+    \"    super(Generator, self).__init__()\\n\",\n+    \"    self.fc1 = tf.keras.layers.Dense(7*7*64, use_bias=False)\\n\",\n+    \"    self.batchnorm1 = tf.keras.layers.BatchNormalization()\\n\",\n+    \"    \\n\",\n+    \"    self.conv1 = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False)\\n\",\n+    \"    # Layer shape is now 7x7x64    \\n\",\n+    \"    \\n\",\n+    \"    self.batchnorm2 = tf.keras.layers.BatchNormalization()\\n\",\n+    \"\\n\",\n+    \"    self.conv2 = tf.keras.layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)\\n\",\n+    \"    # Layer shape is now 14x14x32\\n\",\n+    \"    \\n\",\n+    \"    self.batchnorm3 = tf.keras.layers.BatchNormalization()\\n\",\n+    \"   \\n\",\n+    \"    self.conv3 = tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False)\\n\",\n+    \"    # Layer shape is now 28x28x1\\n\",\n+    \"\\n\",\n+    \"  def call(self, x, training=True):\\n\",\n+    \"    x = self.fc1(x)\\n\",\n+    \"    x = self.batchnorm1(x, training=training)\\n\",\n+    \"    x = tf.nn.relu(x)\\n\",\n+    \"\\n\",\n+    \"    x = tf.reshape(x, shape=(-1, 7, 7, 64))\\n\",\n+    \"\\n\",\n+    \"    x = self.conv1(x)\\n\",\n+    \"    x = self.batchnorm2(x, training=training)\\n\",\n+    \"    x = tf.nn.relu(x)\\n\",\n+    \"\\n\",\n+    \"    x = self.conv2(x)\\n\",\n+    \"    x = self.batchnorm3(x, training=training)\\n\",\n+    \"    x = tf.nn.relu(x)\\n\",\n+    \"\\n\",\n+    \"    x = tf.nn.tanh(self.conv3(x))  \\n\",\n+    \"    return x\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"D0IKnaCtg6WE\"\n+   },\n+   \"source\": [\n+    \"### The Discriminator model\\n\",\n+    \"\\n\",\n+    \"The discriminator is responsible for distinguishing fake images from real images. It's similar to a regular CNN-based image classifier.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"bkOfJxk5j5Hi\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"class Discriminator(tf.keras.Model):\\n\",\n+    \"  def __init__(self):\\n\",\n+    \"    super(Discriminator, self).__init__()\\n\",\n+    \"    self.conv1 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')\\n\",\n+    \"    self.conv2 = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')\\n\",\n+    \"    self.dropout = tf.keras.layers.Dropout(0.3)\\n\",\n+    \"    self.flatten = tf.keras.layers.Flatten()\\n\",\n+    \"    self.fc1 = tf.keras.layers.Dense(1)\\n\",\n+    \"\\n\",\n+    \"  def call(self, x, training=True):\\n\",\n+    \"    x = tf.nn.leaky_relu(self.conv1(x))\\n\",\n+    \"    x = self.dropout(x, training=training)\\n\",\n+    \"    x = tf.nn.leaky_relu(self.conv2(x))\\n\",\n+    \"    x = self.dropout(x, training=training)\\n\",\n+    \"    x = self.flatten(x)\\n\",\n+    \"    x = self.fc1(x)\\n\",\n+    \"    return x\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"gDkA05NE6QMs\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"generator = Generator()\\n\",\n+    \"discriminator = Discriminator()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"6TSZgwc2BUQ-\"\n+   },\n+   \"source\": [\n+    \"\\n\",\n+    \"This model takes about ~30 seconds per epoch to train on a single Tesla K80 on Colab, as of July 2018. Eager execution can sometimes be slower than executing the equivalent graph due to overheads of interpreting Python code. By using [tf.contrib.eager.defun](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun) to create graph functions, we get a ~10 secs/epoch performance boost. This way we get the best of both eager execution (easier for debugging) and graph mode (better performance).\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"k1HpMSLImuRi\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"generator.call = tf.contrib.eager.defun(generator.call)\\n\",\n+    \"discriminator.call = tf.contrib.eager.defun(discriminator.call)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"0FMYgY_mPfTi\"\n+   },\n+   \"source\": [\n+    \"## Define the loss functions and the optimizer\\n\",\n+    \"\\n\",\n+    \"Let's define the loss functions and the optimizers for the generator and the discriminator.\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"Jd-3GCUEiKtv\"\n+   },\n+   \"source\": [\n+    \"### Generator loss\\n\",\n+    \"The generator loss is a sigmoid cross entropy loss of the generated images and an array of ones, since the generator is trying to generate fake images that resemble the real images.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"90BIcCKcDMxz\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def generator_loss(generated_output):\\n\",\n+    \"    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"PKY_iPSPNWoj\"\n+   },\n+   \"source\": [\n+    \"### Discriminator loss\\n\",\n+    \"\\n\",\n+    \"The discriminator loss function takes two inputs: real images, and generated images. Here is how to calculate the discriminator loss:\\n\",\n+    \"1. Calculate real_loss which is a sigmoid cross entropy loss of the real images and an array of ones (since these are the real images).\\n\",\n+    \"2. Calculate generated_loss which is a sigmoid cross entropy loss of the generated images and an array of zeros (since these are the fake images).\\n\",\n+    \"3. Calculate the total_loss as the sum of real_loss and generated_loss.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"wkMNfBWlT-PV\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def discriminator_loss(real_output, generated_output):\\n\",\n+    \"    # [1,1,...,1] with real output since it is true and we want our generated examples to look like it\\n\",\n+    \"    real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(real_output), logits=real_output)\\n\",\n+    \"\\n\",\n+    \"    # [0,0,...,0] with generated images since they are fake\\n\",\n+    \"    generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.zeros_like(generated_output), logits=generated_output)\\n\",\n+    \"\\n\",\n+    \"    total_loss = real_loss + generated_loss\\n\",\n+    \"\\n\",\n+    \"    return total_loss\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"MgIc7i0th_Iu\"\n+   },\n+   \"source\": [\n+    \"The discriminator and the generator optimizers are different since we will train two networks separately.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"iWCn_PVdEJZ7\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"generator_optimizer = tf.train.AdamOptimizer(1e-4)\\n\",\n+    \"discriminator_optimizer = tf.train.AdamOptimizer(1e-4)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"mWtinsGDPJlV\"\n+   },\n+   \"source\": [\n+    \"**Checkpoints (Object-based saving)**\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"CA1w-7s2POEy\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"checkpoint_dir = './training_checkpoints'\\n\",\n+    \"checkpoint_prefix = os.path.join(checkpoint_dir, \\\"ckpt\\\")\\n\",\n+    \"checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\\n\",\n+    \"                                 discriminator_optimizer=discriminator_optimizer,\\n\",\n+    \"                                 generator=generator,\\n\",\n+    \"                                 discriminator=discriminator)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"Rw1fkAczTQYh\"\n+   },\n+   \"source\": [\n+    \"## Set up GANs for Training\\n\",\n+    \"\\n\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"5QC5BABamh_c\"\n+   },\n+   \"source\": [\n+    \"Now it's time to put together the generator and discriminator to set up the Generative Adversarial Networks, as you see in the diagam at the beginning of the tutorial.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"Ff6oN6PZX27n\"\n+   },\n+   \"source\": [\n+    \"**Define training parameters**\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"NS2GWywBbAWo\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"EPOCHS = 150\\n\",\n+    \"noise_dim = 100\\n\",\n+    \"num_examples_to_generate = 16\\n\",\n+    \"\\n\",\n+    \"# We'll re-use this random vector used to seed the generator so\\n\",\n+    \"# it will be easier to see the improvement over time.\\n\",\n+    \"random_vector_for_generation = tf.random_normal([num_examples_to_generate,\\n\",\n+    \"                                                 noise_dim])\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {\n+    \"colab_type\": \"text\",\n+    \"id\": \"jylSonrqSWfi\"\n+   },\n+   \"source\": [\n+    \"**Define training method**\\n\",\n+    \"\\n\",\n+    \"We start by iterating over the dataset. The generator is given a random vector as an input which is processed to  output an image looking like a handwritten digit. The discriminator is then shown the real MNIST images as well as the generated images.\\n\",\n+    \"\\n\",\n+    \"Next, we calculate the generator and the discriminator loss. Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {\n+    \"colab\": {},\n+    \"colab_type\": \"code\",\n+    \"id\": \"2M7LmLtGEMQJ\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"def train(dataset, epochs, noise_dim):  \\n\",\n+    \"  for epoch in range(epochs):\\n\",\n+    \"    start = time.time()\\n\",\n+    \"    \\n\",\n+    \"    for images in dataset:\\n\",\n+    \"      # generating noise from a uniform distribution\\n\",\n+    \"      noise = tf.random_normal([BATCH_SIZE, noise_dim])\\n\",\n+    \"      \\n\",\n+    \"      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\\n\",\n+    \"        generated_images = generator(noise, training=True)\\n\",\n+    \"      \\n\",\n+    \"        real_output = discriminator(images, training=True)\\n\",\n+    \"        generated_output = discriminator(generated_images, training=True)\\n\",\n+    \"        \\n\",\n+    \"        gen_loss = generator_loss(generated_output)\\n\",\n+    \"        disc_loss = discriminator_loss(real_output, generated_output)\\n\",\n+    \"        \\n\",\n+    \"      gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)\\n\",\n+    \"      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)\\n\",\n+    \"      \\n\",\n+    \"      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables))\\n\",\n+    \"      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))\\n\",\n+    \"\\n\",\n+    \"      \\n\",\n+    \"    if epoch % 1 == 0:\\n\",", "path": "tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb", "position": null, "original_position": 1301, "commit_id": "459bfb3b324e194be33aa66f38a71291f3d82b95", "original_commit_id": "f4e176d82dfa9bdf689559f679a4f05f7bda8278", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "body": "epoch % 1 == 0 is a tautology", "created_at": "2018-10-18T20:39:46Z", "updated_at": "2018-10-24T17:54:02Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/23035#discussion_r226457423", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23035", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/226457423"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/23035#discussion_r226457423"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23035"}}, "body_html": "<p>epoch % 1 == 0 is a tautology</p>", "body_text": "epoch % 1 == 0 is a tautology"}