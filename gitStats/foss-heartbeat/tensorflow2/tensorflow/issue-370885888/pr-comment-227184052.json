{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/227184052", "pull_request_review_id": 167163141, "id": 227184052, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzE4NDA1Mg==", "diff_hunk": "@@ -327,142 +431,134 @@\n         \"    total_loss = real_loss + generated_loss\\n\",\n         \"\\n\",\n         \"    return total_loss\"\n-      ]\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"90BIcCKcDMxz\"\n+        \"colab_type\": \"text\",\n+        \"id\": \"MgIc7i0th_Iu\"\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"def generator_loss(generated_output):\\n\",\n-        \"    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)\"\n+        \"The discriminator and the generator optimizers are different since we will train two networks separately.\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"iWCn_PVdEJZ7\"\n+        \"id\": \"iWCn_PVdEJZ7\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n-        \"discriminator_optimizer = tf.train.AdamOptimizer(1e-4)\\n\",\n-        \"generator_optimizer = tf.train.AdamOptimizer(1e-4)\"\n-      ]\n+        \"generator_optimizer = tf.train.AdamOptimizer(1e-4)\\n\",\n+        \"discriminator_optimizer = tf.train.AdamOptimizer(1e-4)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"markdown\",\n       \"metadata\": {\n         \"colab_type\": \"text\",\n         \"id\": \"mWtinsGDPJlV\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"## Checkpoints (Object-based saving)\"\n+        \"**Checkpoints (Object-based saving)**\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"CA1w-7s2POEy\"\n+        \"id\": \"CA1w-7s2POEy\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n         \"checkpoint_dir = './training_checkpoints'\\n\",\n         \"checkpoint_prefix = os.path.join(checkpoint_dir, \\\"ckpt\\\")\\n\",\n         \"checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\\n\",\n         \"                                 discriminator_optimizer=discriminator_optimizer,\\n\",\n         \"                                 generator=generator,\\n\",\n         \"                                 discriminator=discriminator)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"colab_type\": \"text\",\n+        \"id\": \"Rw1fkAczTQYh\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Set up GANs for Training\\n\",\n+        \"\\n\"\n       ]\n     },\n     {\n+      \"metadata\": {\n+        \"colab_type\": \"text\",\n+        \"id\": \"5QC5BABamh_c\"\n+      },\n       \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"Now it's time to put together the generator and discriminator to set up the Generative Adversarial Networks, as you see in the diagam at the beginning of the tutorial.\"\n+      ]\n+    },\n+    {\n       \"metadata\": {\n         \"colab_type\": \"text\",\n-        \"id\": \"Rw1fkAczTQYh\"\n+        \"id\": \"Ff6oN6PZX27n\"\n       },\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"## Training\\n\",\n-        \"\\n\",\n-        \"* We start by iterating over the dataset\\n\",\n-        \"* The generator is given **noise as an input** which when passed through the generator model will output a image looking like a handwritten digit\\n\",\n-        \"* The discriminator is given the **real MNIST images as well as the generated images (from the generator)**.\\n\",\n-        \"* Next, we calculate the generator and the discriminator loss.\\n\",\n-        \"* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables (inputs) and apply those to the optimizer.\\n\",\n-        \"\\n\",\n-        \"## Generate Images\\n\",\n-        \"\\n\",\n-        \"* After training, its time to generate some images!\\n\",\n-        \"* We start by creating noise array as an input to the generator\\n\",\n-        \"* The generator will then convert the noise into handwritten images.\\n\",\n-        \"* Last step is to plot the predictions and **voila!**\"\n+        \"**Define training parameters**\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"NS2GWywBbAWo\"\n+        \"id\": \"NS2GWywBbAWo\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",\n       \"source\": [\n-        \"EPOCHS = 150\\n\",\n+        \"EPOCHS = 50\\n\",\n         \"noise_dim = 100\\n\",\n         \"num_examples_to_generate = 16\\n\",\n         \"\\n\",\n-        \"# keeping the random vector constant for generation (prediction) so\\n\",\n-        \"# it will be easier to see the improvement of the gan.\\n\",\n+        \"# We'll re-use this random vector used to seed the generator so\\n\",\n+        \"# it will be easier to see the improvement over time.\\n\",\n         \"random_vector_for_generation = tf.random_normal([num_examples_to_generate,\\n\",\n         \"                                                 noise_dim])\"\n-      ]\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n-        \"colab_type\": \"code\",\n-        \"id\": \"RmdVsmvhPxyy\"\n+        \"colab_type\": \"text\",\n+        \"id\": \"jylSonrqSWfi\"\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"markdown\",\n       \"source\": [\n-        \"def generate_and_save_images(model, epoch, test_input):\\n\",\n-        \"  # make sure the training parameter is set to False because we\\n\",\n-        \"  # don't want to train the batchnorm layer when doing inference.\\n\",\n-        \"  predictions = model(test_input, training=False)\\n\",\n+        \"**Define training method**\\n\",\n         \"\\n\",\n-        \"  fig = plt.figure(figsize=(4,4))\\n\",\n-        \"  \\n\",\n-        \"  for i in range(predictions.shape[0]):\\n\",\n-        \"      plt.subplot(4, 4, i+1)\\n\",\n-        \"      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\\n\",\n-        \"      plt.axis('off')\\n\",\n-        \"        \\n\",\n-        \"  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\\n\",\n-        \"  plt.show()\"\n+        \"We start by iterating over the dataset. The generator is given a random vector as an input which is processed to  output an image looking like a handwritten digit. The discriminator is then shown the real MNIST images as well as the generated images.\\n\",\n+        \"\\n\",\n+        \"Next, we calculate the generator and the discriminator loss. Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables.\"\n       ]\n     },\n     {\n-      \"cell_type\": \"code\",\n-      \"execution_count\": 0,\n       \"metadata\": {\n-        \"colab\": {},\n         \"colab_type\": \"code\",\n-        \"id\": \"2M7LmLtGEMQJ\"\n+        \"id\": \"2M7LmLtGEMQJ\",\n+        \"colab\": {}\n       },\n-      \"outputs\": [],\n+      \"cell_type\": \"code\",", "path": "tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb", "position": 777, "original_position": 795, "commit_id": "459bfb3b324e194be33aa66f38a71291f3d82b95", "original_commit_id": "db6c86cffa294c7e35e9f81e6f92a7a53b3a531c", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "body": "Why pass `noise_dim` as an argument to `train()` given that it is defined as almost a constant above?\r\nThe module level `noise_dim` constant and the function argument can be confusing. How about just having a `NOISE_DIM` constant and using that instead of creating a constant and then providing that as an argument of the same name to `train()`?", "created_at": "2018-10-23T00:36:30Z", "updated_at": "2018-10-24T17:54:02Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/23035#discussion_r227184052", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23035", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/227184052"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/23035#discussion_r227184052"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23035"}}, "body_html": "<p>Why pass <code>noise_dim</code> as an argument to <code>train()</code> given that it is defined as almost a constant above?<br>\nThe module level <code>noise_dim</code> constant and the function argument can be confusing. How about just having a <code>NOISE_DIM</code> constant and using that instead of creating a constant and then providing that as an argument of the same name to <code>train()</code>?</p>", "body_text": "Why pass noise_dim as an argument to train() given that it is defined as almost a constant above?\nThe module level noise_dim constant and the function argument can be confusing. How about just having a NOISE_DIM constant and using that instead of creating a constant and then providing that as an argument of the same name to train()?"}