{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7783", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7783/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7783/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7783/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7783", "id": 209507905, "node_id": "MDU6SXNzdWUyMDk1MDc5MDU=", "number": 7783, "title": "Segfault in runtime executor due to variable overflow", "user": {"login": "untom", "id": 3627551, "node_id": "MDQ6VXNlcjM2Mjc1NTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/3627551?v=4", "gravatar_id": "", "url": "https://api.github.com/users/untom", "html_url": "https://github.com/untom", "followers_url": "https://api.github.com/users/untom/followers", "following_url": "https://api.github.com/users/untom/following{/other_user}", "gists_url": "https://api.github.com/users/untom/gists{/gist_id}", "starred_url": "https://api.github.com/users/untom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/untom/subscriptions", "organizations_url": "https://api.github.com/users/untom/orgs", "repos_url": "https://api.github.com/users/untom/repos", "events_url": "https://api.github.com/users/untom/events{/privacy}", "received_events_url": "https://api.github.com/users/untom/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-02-22T16:32:30Z", "updated_at": "2017-02-27T02:00:01Z", "closed_at": "2017-02-27T02:00:01Z", "author_association": "NONE", "body_html": "<p>I'm running Tensorflow 1.0, and I'm encountering a segfault in tensorflow, with the following output:</p>\n<pre><code> F tensorflow/core/common_runtime/executor.cc:484] Check failed: e-&gt;src_output() &lt; 32768 (38774 vs. 32768)\n Aborted (core dumped)\n</code></pre>\n<p>My program loads a fairly large sparse matrix (1879549 samples, 556926 features, 0.000038 of the entries in the matrix are nonzero) into memory. I then create a <code>tf.SparseTensor</code> out of it:</p>\n<pre><code># x_indices and x.data are the data for the sparse matrix in the correct format\nx_ind = tf.Variable(initial_value=x_indices.astype(np.int64), trainable=False)\nx_val = tf.Variable(initial_value=x.data, dtype=tf.float32, trainable=False)\nreturn tf.SparseTensor(x_ind, x_val, dense_shape=x_sparse.shape)\n</code></pre>\n<p>I then split this SparseTensor into minibatch-sized splits. I have several queue-runners feed that feed a Queue by taking a minibatch, transforming it to dense and putting it into the queue.</p>\n<p>In code, the process looks like this (where <code>self._input_x_sp</code> is the SparseTensor):</p>\n<pre><code>    self.x_shape = self._input_x_sp.get_shape().as_list()\n    self.y_shape = self._input_y.get_shape().as_list()\n    n_batches = self.x_shape[0] // self.batch_size\n    self._x_split = tf.sparse_split(sp_input=self._input_x_sp, num_split=n_batches, axis=0)\n    self._y_split = tf.split(self._input_y, num_or_size_splits=n_batches, axis=0, name=\"y_batch\")\n\n    #   ...  creating a Queue\n\n    # build a list of all possible enqueue OPs.\n    # We need to do this here while we're still single-threaded, as the\n    # Graph creation is not thread-safe\n    # Later in the different threads we can just run the OPs\n    self._op_list = []\n    for i in range(n_batches):\n        x_batch = tf.sparse_tensor_to_dense(self._x_split[i], name=\"x_batch\")\n        y_batch = self._y_split[i]\n        self._op_list.append(self._queue.enqueue_many([x_batch, y_batch]))\n</code></pre>\n<p>The queue-runners randomly pick an operation from <code>self._op_list</code> and execute it, in a loop.</p>\n<p>I am confident that my code is not to blame, as the program runs just fine on smaller input sizes (a sparse matrix of 206208 samples and 133515 features of which 0.000135 nonzero entries), but encounters the segfault on the larger matrix.</p>\n<p>Looking at the TF code where the error is generated (<a href=\"https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/common_runtime/executor.cc#L484\">here</a>) it seems like the cause is a variable in tensorflow that is an unsigned short when it probably should be something larger than that.</p>", "body_text": "I'm running Tensorflow 1.0, and I'm encountering a segfault in tensorflow, with the following output:\n F tensorflow/core/common_runtime/executor.cc:484] Check failed: e->src_output() < 32768 (38774 vs. 32768)\n Aborted (core dumped)\n\nMy program loads a fairly large sparse matrix (1879549 samples, 556926 features, 0.000038 of the entries in the matrix are nonzero) into memory. I then create a tf.SparseTensor out of it:\n# x_indices and x.data are the data for the sparse matrix in the correct format\nx_ind = tf.Variable(initial_value=x_indices.astype(np.int64), trainable=False)\nx_val = tf.Variable(initial_value=x.data, dtype=tf.float32, trainable=False)\nreturn tf.SparseTensor(x_ind, x_val, dense_shape=x_sparse.shape)\n\nI then split this SparseTensor into minibatch-sized splits. I have several queue-runners feed that feed a Queue by taking a minibatch, transforming it to dense and putting it into the queue.\nIn code, the process looks like this (where self._input_x_sp is the SparseTensor):\n    self.x_shape = self._input_x_sp.get_shape().as_list()\n    self.y_shape = self._input_y.get_shape().as_list()\n    n_batches = self.x_shape[0] // self.batch_size\n    self._x_split = tf.sparse_split(sp_input=self._input_x_sp, num_split=n_batches, axis=0)\n    self._y_split = tf.split(self._input_y, num_or_size_splits=n_batches, axis=0, name=\"y_batch\")\n\n    #   ...  creating a Queue\n\n    # build a list of all possible enqueue OPs.\n    # We need to do this here while we're still single-threaded, as the\n    # Graph creation is not thread-safe\n    # Later in the different threads we can just run the OPs\n    self._op_list = []\n    for i in range(n_batches):\n        x_batch = tf.sparse_tensor_to_dense(self._x_split[i], name=\"x_batch\")\n        y_batch = self._y_split[i]\n        self._op_list.append(self._queue.enqueue_many([x_batch, y_batch]))\n\nThe queue-runners randomly pick an operation from self._op_list and execute it, in a loop.\nI am confident that my code is not to blame, as the program runs just fine on smaller input sizes (a sparse matrix of 206208 samples and 133515 features of which 0.000135 nonzero entries), but encounters the segfault on the larger matrix.\nLooking at the TF code where the error is generated (here) it seems like the cause is a variable in tensorflow that is an unsigned short when it probably should be something larger than that.", "body": "I'm running Tensorflow 1.0, and I'm encountering a segfault in tensorflow, with the following output:\r\n\r\n     F tensorflow/core/common_runtime/executor.cc:484] Check failed: e->src_output() < 32768 (38774 vs. 32768)\r\n     Aborted (core dumped)\r\n\r\n\r\nMy program loads a fairly large sparse matrix (1879549 samples, 556926 features, 0.000038 of the entries in the matrix are nonzero) into memory. I then create a `tf.SparseTensor` out of it:\r\n\r\n    # x_indices and x.data are the data for the sparse matrix in the correct format\r\n    x_ind = tf.Variable(initial_value=x_indices.astype(np.int64), trainable=False)\r\n    x_val = tf.Variable(initial_value=x.data, dtype=tf.float32, trainable=False)\r\n    return tf.SparseTensor(x_ind, x_val, dense_shape=x_sparse.shape)\r\n\r\nI then split this SparseTensor into minibatch-sized splits. I have several queue-runners feed that feed a Queue by taking a minibatch, transforming it to dense and putting it into the queue.\r\n\r\nIn code, the process looks like this (where `self._input_x_sp` is the SparseTensor):\r\n\r\n        self.x_shape = self._input_x_sp.get_shape().as_list()\r\n        self.y_shape = self._input_y.get_shape().as_list()\r\n        n_batches = self.x_shape[0] // self.batch_size\r\n        self._x_split = tf.sparse_split(sp_input=self._input_x_sp, num_split=n_batches, axis=0)\r\n        self._y_split = tf.split(self._input_y, num_or_size_splits=n_batches, axis=0, name=\"y_batch\")\r\n\r\n        #   ...  creating a Queue\r\n\r\n        # build a list of all possible enqueue OPs.\r\n        # We need to do this here while we're still single-threaded, as the\r\n        # Graph creation is not thread-safe\r\n        # Later in the different threads we can just run the OPs\r\n        self._op_list = []\r\n        for i in range(n_batches):\r\n            x_batch = tf.sparse_tensor_to_dense(self._x_split[i], name=\"x_batch\")\r\n            y_batch = self._y_split[i]\r\n            self._op_list.append(self._queue.enqueue_many([x_batch, y_batch]))\r\n\r\nThe queue-runners randomly pick an operation from `self._op_list` and execute it, in a loop. \r\n\r\n\r\n I am confident that my code is not to blame, as the program runs just fine on smaller input sizes (a sparse matrix of 206208 samples and 133515 features of which 0.000135 nonzero entries), but encounters the segfault on the larger matrix.\r\n\r\nLooking at the TF code where the error is generated ([here](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/common_runtime/executor.cc#L484)) it seems like the cause is a variable in tensorflow that is an unsigned short when it probably should be something larger than that."}