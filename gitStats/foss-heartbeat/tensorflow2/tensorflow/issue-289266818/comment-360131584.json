{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/360131584", "html_url": "https://github.com/tensorflow/tensorflow/issues/16192#issuecomment-360131584", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16192", "id": 360131584, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDEzMTU4NA==", "user": {"login": "zuokai", "id": 15143205, "node_id": "MDQ6VXNlcjE1MTQzMjA1", "avatar_url": "https://avatars1.githubusercontent.com/u/15143205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuokai", "html_url": "https://github.com/zuokai", "followers_url": "https://api.github.com/users/zuokai/followers", "following_url": "https://api.github.com/users/zuokai/following{/other_user}", "gists_url": "https://api.github.com/users/zuokai/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuokai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuokai/subscriptions", "organizations_url": "https://api.github.com/users/zuokai/orgs", "repos_url": "https://api.github.com/users/zuokai/repos", "events_url": "https://api.github.com/users/zuokai/events{/privacy}", "received_events_url": "https://api.github.com/users/zuokai/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-24T13:23:30Z", "updated_at": "2018-01-24T13:23:30Z", "author_association": "NONE", "body_html": "<h1>read images, labels</h1>\n<p>def read_img(path):<br>\nimgs = []<br>\nlabels = []<br>\n#label = []<br>\nfor line in open(path):<br>\nlabel = []<br>\nprint(line)<br>\nline = line.replace(\"\\n\",\"\")<br>\nstrs = line.split('\\t')<br>\nimgs.append(strs[0])<br>\nlabel.append(strs[1])<br>\nlabel.append(strs[2])<br>\nif str(temp_str[1]) == \"-1\":<br>\nlabel.append(\"0\")<br>\nelse:<br>\nlabel.append(\"1\")<br>\nif str(temp_str[2]) == \"-1\":<br>\nlabel.append(\"0\")<br>\nelse:<br>\nlabel.append(\"1\")<br>\nlabels.append(np.asarray(label, np.int32))<br>\nreturn np.asarray(imgs, np.string_), np.asarray(labels, np.int32)</p>\n<p>\uff083\uff09split labels and ignore_weight<br>\ny_ = tf.squeeze(tf.slice(y_all, [0, 0], [-1, 1]))<br>\ny_gb_ = tf.squeeze(tf.slice(y_all, [0, 1], [-1, 1]))<br>\nw_y = tf.squeeze(tf.slice(y_all, [0, 2], [-1, 1]))      #label1 weight<br>\nw_y_gb = tf.squeeze(tf.slice(y_all, [0, 3], [-1, 1]))  #label2 weight<br>\n\uff084\uff09use tf.losses.sparse_softmax_cross_entropy<br>\ny_s_ = y_ * w_y<br>\ny_gb_s_ = y_gb_ * w_y_gb<br>\ny, y_gb = net.squeezenet(x, True, 0.999, CLASSES, CLASSES_FOOD)</p>\n<pre><code>    cross_entropy_gb = tf.losses.sparse_softmax_cross_entropy(labels=y_gb_s_, logits=y_gb, weights=w_y_gb)\n     \n    cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_s_, logits=y, weights = w_y)\n   \n    self.cost = tf.reduce_mean(cross_entropy, name='cross_entropy_loss') + tf.reduce_mean(cross_entropy_gb, name='cross_entropy_loss_gb')\n</code></pre>", "body_text": "read images, labels\ndef read_img(path):\nimgs = []\nlabels = []\n#label = []\nfor line in open(path):\nlabel = []\nprint(line)\nline = line.replace(\"\\n\",\"\")\nstrs = line.split('\\t')\nimgs.append(strs[0])\nlabel.append(strs[1])\nlabel.append(strs[2])\nif str(temp_str[1]) == \"-1\":\nlabel.append(\"0\")\nelse:\nlabel.append(\"1\")\nif str(temp_str[2]) == \"-1\":\nlabel.append(\"0\")\nelse:\nlabel.append(\"1\")\nlabels.append(np.asarray(label, np.int32))\nreturn np.asarray(imgs, np.string_), np.asarray(labels, np.int32)\n\uff083\uff09split labels and ignore_weight\ny_ = tf.squeeze(tf.slice(y_all, [0, 0], [-1, 1]))\ny_gb_ = tf.squeeze(tf.slice(y_all, [0, 1], [-1, 1]))\nw_y = tf.squeeze(tf.slice(y_all, [0, 2], [-1, 1]))      #label1 weight\nw_y_gb = tf.squeeze(tf.slice(y_all, [0, 3], [-1, 1]))  #label2 weight\n\uff084\uff09use tf.losses.sparse_softmax_cross_entropy\ny_s_ = y_ * w_y\ny_gb_s_ = y_gb_ * w_y_gb\ny, y_gb = net.squeezenet(x, True, 0.999, CLASSES, CLASSES_FOOD)\n    cross_entropy_gb = tf.losses.sparse_softmax_cross_entropy(labels=y_gb_s_, logits=y_gb, weights=w_y_gb)\n     \n    cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_s_, logits=y, weights = w_y)\n   \n    self.cost = tf.reduce_mean(cross_entropy, name='cross_entropy_loss') + tf.reduce_mean(cross_entropy_gb, name='cross_entropy_loss_gb')", "body": "# read images, labels\r\ndef read_img(path):\r\n    imgs = []\r\n    labels = []\r\n    #label = []\r\n for line in open(path):\r\n        label = []\r\n        print(line)\r\n        line = line.replace(\"\\n\",\"\")\r\n        strs = line.split('\\t')\r\n        imgs.append(strs[0])\r\n        label.append(strs[1])\r\n        label.append(strs[2])\r\n                 if str(temp_str[1]) == \"-1\":\r\n                    label.append(\"0\")\r\n                else:\r\n                    label.append(\"1\")\r\n                if str(temp_str[2]) == \"-1\":\r\n                    label.append(\"0\")\r\n                else:\r\n                    label.append(\"1\")\r\n        labels.append(np.asarray(label, np.int32))\r\n    return np.asarray(imgs, np.string_), np.asarray(labels, np.int32)\r\n \r\n \r\n\uff083\uff09split labels and ignore_weight\r\n  y_ = tf.squeeze(tf.slice(y_all, [0, 0], [-1, 1]))\r\n  y_gb_ = tf.squeeze(tf.slice(y_all, [0, 1], [-1, 1]))\r\n    w_y = tf.squeeze(tf.slice(y_all, [0, 2], [-1, 1]))      #label1 weight\r\n    w_y_gb = tf.squeeze(tf.slice(y_all, [0, 3], [-1, 1]))  #label2 weight\r\n\uff084\uff09use tf.losses.sparse_softmax_cross_entropy\r\n        y_s_ = y_ * w_y\r\n        y_gb_s_ = y_gb_ * w_y_gb\r\n        y, y_gb = net.squeezenet(x, True, 0.999, CLASSES, CLASSES_FOOD)\r\n        \r\n        cross_entropy_gb = tf.losses.sparse_softmax_cross_entropy(labels=y_gb_s_, logits=y_gb, weights=w_y_gb)\r\n         \r\n        cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_s_, logits=y, weights = w_y)\r\n       \r\n        self.cost = tf.reduce_mean(cross_entropy, name='cross_entropy_loss') + tf.reduce_mean(cross_entropy_gb, name='cross_entropy_loss_gb')"}