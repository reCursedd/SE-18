{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16592", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16592/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16592/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16592/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16592", "id": 292874804, "node_id": "MDU6SXNzdWUyOTI4NzQ4MDQ=", "number": 16592, "title": "Freeze-graph not working, allocating too much memory for inference", "user": {"login": "k22jung", "id": 14689505, "node_id": "MDQ6VXNlcjE0Njg5NTA1", "avatar_url": "https://avatars2.githubusercontent.com/u/14689505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/k22jung", "html_url": "https://github.com/k22jung", "followers_url": "https://api.github.com/users/k22jung/followers", "following_url": "https://api.github.com/users/k22jung/following{/other_user}", "gists_url": "https://api.github.com/users/k22jung/gists{/gist_id}", "starred_url": "https://api.github.com/users/k22jung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/k22jung/subscriptions", "organizations_url": "https://api.github.com/users/k22jung/orgs", "repos_url": "https://api.github.com/users/k22jung/repos", "events_url": "https://api.github.com/users/k22jung/events{/privacy}", "received_events_url": "https://api.github.com/users/k22jung/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-01-30T17:43:13Z", "updated_at": "2018-04-17T12:46:05Z", "closed_at": "2018-04-17T12:46:05Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nyes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.5.0-rc1-1783-g7d7dce1', '1.5.0-rc1')</li>\n<li><strong>Python version</strong>: Python 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: Build label: 0.9.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A (CPU only)</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p>save_alexnet_checkpoint.py</p>\n<pre><code>import os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom alexnet import AlexNet\nfrom datagenerator import ImageDataGenerator\nfrom datetime import datetime\nfrom tensorflow.contrib.data import Iterator\n\n\"\"\"\nConfiguration Part.\n\"\"\"\n\nnum_classes = 1000\ncheckpoint_path = \"path/to/ckpt\"\n\n\n# Create parent path if it doesn't exist\nif not os.path.isdir(checkpoint_path):\n    os.mkdir(checkpoint_path)\n\n# TF placeholder for graph input and output\n\n\n# Initialize model\nx = tf.placeholder(tf.float32, [1, 227, 227, 3],name=\"input\")\nkeep_prob=tf.placeholder(tf.float32,[],name=\"keepProbs\")\nmodel = AlexNet(x, keep_prob, num_classes, [])\nsoftmax = tf.nn.softmax(model.fc8,name=\"softmax\")\n\n# Initialize an saver for store model checkpoints\nsaver = tf.train.Saver()\n\n# Start Tensorflow session\nwith tf.Session() as sess:\n\n\t# Validate the model on the entire validation set\n\tsess.run(tf.global_variables_initializer())\n\n\t# Load the pretrained weights into the non-trainable layer\n\tmodel.load_initial_weights(sess)\n\n\t# save checkpoint of the model\n\tcheckpoint_name = os.path.join(checkpoint_path,\n\t\t                       'original_alexnet.ckpt')\n\tsave_path = saver.save(sess, checkpoint_name)\n\n\ttf.train.write_graph(sess.graph_def, checkpoint_path, 'alexnet_def.pb',as_text=False)\n\n\tnames=[]\n\tfor n in tf.get_default_graph().as_graph_def().node:\n\t\tnames.append(str(n.name))\n\n    \tnames = sorted(names, key=str.lower)\n    \tfor n in names:\n\t\tprint n\n\n\n\tprint(\"Model checkpoint saved at {}\".format(checkpoint_name))\n</code></pre>\n<p><br><br></p>\n<p>Freeze saved graph_def and weights:</p>\n<pre><code>bazel build tensorflow/python/tools:freeze_graph\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/alexnet_def.pb -- input_checkpoint=path/to/ckpt/original_alexnet.ckpt --output_graph=/path/to/frozen_alexnet.pb --output_node_names='softmax' --input_binary=True\n</code></pre>\n<p><br><br></p>\n<p>Forward Inference:</p>\n<pre><code>import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom alexnet import AlexNet\nfrom tensorflow.python.client import timeline\nfrom caffe_classes import class_names\n\nfrozen_graph='/path/to/frozen_alexnet.pb '\n\ndef load_graph(frozen_graph_filename):\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def, name=\"prefix\")\n    return graph\n\nimagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n\ncurrent_dir = os.getcwd()\nimage_dir = os.path.join(current_dir, 'images')\n\nimg_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n\nimgs = []\nfor f in img_files:\n    imgs.append(cv2.imread(f))\n\ng=load_graph(frozen_graph)\nwith tf.Session(graph=g) as sess:\n\n    softmax=g.get_tensor_by_name('prefix/softmax:0')\n    x = g.get_tensor_by_name(\"prefix/input:0\")\n    keep_prob = g.get_tensor_by_name(\"prefix/keepProbs:0\")\n\n    for i, image in enumerate(imgs):\n        \n        img = cv2.resize(image.astype(np.float32), (227,227))\n        img -= imagenet_mean\n        img = img.reshape((1,227,227,3))\n\n\toptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\trun_metadata = tf.RunMetadata()\n        \n        print 'Begin session:'\n        probs = sess.run(softmax, feed_dict={x: img, keep_prob: 1}, \\\n\t\t\toptions=options, run_metadata=run_metadata)\n        print 'Ended session'\n\tprint 'Class: ' + str(class_names[np.argmax(probs)]) \\\n\t+ ', Prob: ' + str(probs[0,np.argmax(probs)])\n\tfetched_timeline = timeline.Timeline(run_metadata.step_stats)\n\tchrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)\n\twith open('./forward_timeline.json', 'w') as f:\n\t\tf.write(chrome_trace)\n\n</code></pre>\n<p>log_alexnet.sh</p>\n<pre><code>#!/bin/bash\n\nlogpid() { while sleep 0.1; do ps -p $1 -o pcpu= -o pmem= ; done; }\npython frozen-infer.py &amp;\nlogpid $! | tee ./pid.log\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I ran my own Alexnet frozen model and the forward inference script I had required a large amount of RAM, around 2.466 GB rather than the memory required to load model size which was (243.9 MB).</p>\n<p>Essentially I took scripts from <a href=\"https://github.com/kratzert/finetune_alexnet_with_tensorflow\">finetune_alexnet_with_tensorflow</a>, including <code>alexnet.py</code>, <code>datagenerator.py</code>, <code>caffe_classes.py</code>, and heavily modifying <code>validate_alexnet_on_imagenet.ipynb</code> to save a checkpoint of the Alexnet-implementation for TensorFlow. The weights were taken from this <a href=\"http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\" rel=\"nofollow\">link</a>. I ran <code>save_alexnet_checkpoint.py</code> first to generate a binary proto graph definition of the AlexNet file and corresponding checkpoint files. Using the default <code>freeze_graph</code> tool from TensorFlow compiled by Bazel, I created a frozen graph. I then ran <code>log_alexnet.sh</code> to see how much of my RAM a forward inference uses on my laptop. The image I used was from the above mentioned github repo, llama. The script outputs the percentage of CPU and memory the python script <code>frozen-infer.py</code>  uses.</p>\n<p>My CPU: Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz</p>\n<p>My RAM is 8 GB, but according to the free command, it is 7.909 GB, where htop reports it as 7.54 GB. The maximum RAM allocated by <code>sess.run</code> and the python script was thus, 0.327*7.54 = 2.466 GB (see below log). This is insanely larger than the 243.9 MB model I had. The <code>chrome_trace</code> I generated reports a maximum of around 235 MB, which is no where near what was actually happening.<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1678731/forward_timeline.json.tar.gz\">forward_timeline.json.tar.gz</a></p>\n<p>My question is why does the session run require such memory, and how am I able to reduce it down to only consume the required memory to load the model, which would be ~250 MB? I am concerned about this because I will to transferring this code to a Raspberry Pi 3 soon.</p>\n<p>It seems someone else had the same issue on stackoverflow <a href=\"https://stackoverflow.com/questions/46531213/after-freeze-graph-the-inference-is-slower-and-requires-more-memory\" rel=\"nofollow\">here</a>.</p>\n<h3>Source code / logs</h3>\n<p>The output I got is such from log_alexnet.sh:</p>\n<pre><code> 0.0  0.7\n 0.0  1.0\n 0.0  1.2\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n 0.0  1.4\n 0.0  1.8\n 0.0  2.0\n 0.0  2.2\n 0.0  7.7\n 194  5.3\nBegin session:\n 204 10.4\n 215 16.8\n 226 17.7\n 236 22.1\n 248 25.8\n 259 30.7\n 269 32.7\n 280 27.7\n 145 29.4\n 151 27.3\n 156 29.2\n 162 24.6\n 167 28.9\n 172 27.0\n 178 27.8\n 183 17.1\n 188 18.5 \n 194 18.1\nEnded session\nClass: llama, Prob: 0.99927825\n 134 21.3\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nyes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): ('v1.5.0-rc1-1783-g7d7dce1', '1.5.0-rc1')\nPython version: Python 2.7.12\nBazel version (if compiling from source): Build label: 0.9.0\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0\nCUDA/cuDNN version: N/A (CPU only)\nGPU model and memory: N/A\nExact command to reproduce:\n\nsave_alexnet_checkpoint.py\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom alexnet import AlexNet\nfrom datagenerator import ImageDataGenerator\nfrom datetime import datetime\nfrom tensorflow.contrib.data import Iterator\n\n\"\"\"\nConfiguration Part.\n\"\"\"\n\nnum_classes = 1000\ncheckpoint_path = \"path/to/ckpt\"\n\n\n# Create parent path if it doesn't exist\nif not os.path.isdir(checkpoint_path):\n    os.mkdir(checkpoint_path)\n\n# TF placeholder for graph input and output\n\n\n# Initialize model\nx = tf.placeholder(tf.float32, [1, 227, 227, 3],name=\"input\")\nkeep_prob=tf.placeholder(tf.float32,[],name=\"keepProbs\")\nmodel = AlexNet(x, keep_prob, num_classes, [])\nsoftmax = tf.nn.softmax(model.fc8,name=\"softmax\")\n\n# Initialize an saver for store model checkpoints\nsaver = tf.train.Saver()\n\n# Start Tensorflow session\nwith tf.Session() as sess:\n\n\t# Validate the model on the entire validation set\n\tsess.run(tf.global_variables_initializer())\n\n\t# Load the pretrained weights into the non-trainable layer\n\tmodel.load_initial_weights(sess)\n\n\t# save checkpoint of the model\n\tcheckpoint_name = os.path.join(checkpoint_path,\n\t\t                       'original_alexnet.ckpt')\n\tsave_path = saver.save(sess, checkpoint_name)\n\n\ttf.train.write_graph(sess.graph_def, checkpoint_path, 'alexnet_def.pb',as_text=False)\n\n\tnames=[]\n\tfor n in tf.get_default_graph().as_graph_def().node:\n\t\tnames.append(str(n.name))\n\n    \tnames = sorted(names, key=str.lower)\n    \tfor n in names:\n\t\tprint n\n\n\n\tprint(\"Model checkpoint saved at {}\".format(checkpoint_name))\n\n\nFreeze saved graph_def and weights:\nbazel build tensorflow/python/tools:freeze_graph\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/alexnet_def.pb -- input_checkpoint=path/to/ckpt/original_alexnet.ckpt --output_graph=/path/to/frozen_alexnet.pb --output_node_names='softmax' --input_binary=True\n\n\nForward Inference:\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom alexnet import AlexNet\nfrom tensorflow.python.client import timeline\nfrom caffe_classes import class_names\n\nfrozen_graph='/path/to/frozen_alexnet.pb '\n\ndef load_graph(frozen_graph_filename):\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def, name=\"prefix\")\n    return graph\n\nimagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\n\ncurrent_dir = os.getcwd()\nimage_dir = os.path.join(current_dir, 'images')\n\nimg_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n\nimgs = []\nfor f in img_files:\n    imgs.append(cv2.imread(f))\n\ng=load_graph(frozen_graph)\nwith tf.Session(graph=g) as sess:\n\n    softmax=g.get_tensor_by_name('prefix/softmax:0')\n    x = g.get_tensor_by_name(\"prefix/input:0\")\n    keep_prob = g.get_tensor_by_name(\"prefix/keepProbs:0\")\n\n    for i, image in enumerate(imgs):\n        \n        img = cv2.resize(image.astype(np.float32), (227,227))\n        img -= imagenet_mean\n        img = img.reshape((1,227,227,3))\n\n\toptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n\trun_metadata = tf.RunMetadata()\n        \n        print 'Begin session:'\n        probs = sess.run(softmax, feed_dict={x: img, keep_prob: 1}, \\\n\t\t\toptions=options, run_metadata=run_metadata)\n        print 'Ended session'\n\tprint 'Class: ' + str(class_names[np.argmax(probs)]) \\\n\t+ ', Prob: ' + str(probs[0,np.argmax(probs)])\n\tfetched_timeline = timeline.Timeline(run_metadata.step_stats)\n\tchrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)\n\twith open('./forward_timeline.json', 'w') as f:\n\t\tf.write(chrome_trace)\n\n\nlog_alexnet.sh\n#!/bin/bash\n\nlogpid() { while sleep 0.1; do ps -p $1 -o pcpu= -o pmem= ; done; }\npython frozen-infer.py &\nlogpid $! | tee ./pid.log\n\nDescribe the problem\nI ran my own Alexnet frozen model and the forward inference script I had required a large amount of RAM, around 2.466 GB rather than the memory required to load model size which was (243.9 MB).\nEssentially I took scripts from finetune_alexnet_with_tensorflow, including alexnet.py, datagenerator.py, caffe_classes.py, and heavily modifying validate_alexnet_on_imagenet.ipynb to save a checkpoint of the Alexnet-implementation for TensorFlow. The weights were taken from this link. I ran save_alexnet_checkpoint.py first to generate a binary proto graph definition of the AlexNet file and corresponding checkpoint files. Using the default freeze_graph tool from TensorFlow compiled by Bazel, I created a frozen graph. I then ran log_alexnet.sh to see how much of my RAM a forward inference uses on my laptop. The image I used was from the above mentioned github repo, llama. The script outputs the percentage of CPU and memory the python script frozen-infer.py  uses.\nMy CPU: Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz\nMy RAM is 8 GB, but according to the free command, it is 7.909 GB, where htop reports it as 7.54 GB. The maximum RAM allocated by sess.run and the python script was thus, 0.327*7.54 = 2.466 GB (see below log). This is insanely larger than the 243.9 MB model I had. The chrome_trace I generated reports a maximum of around 235 MB, which is no where near what was actually happening.\nforward_timeline.json.tar.gz\nMy question is why does the session run require such memory, and how am I able to reduce it down to only consume the required memory to load the model, which would be ~250 MB? I am concerned about this because I will to transferring this code to a Raspberry Pi 3 soon.\nIt seems someone else had the same issue on stackoverflow here.\nSource code / logs\nThe output I got is such from log_alexnet.sh:\n 0.0  0.7\n 0.0  1.0\n 0.0  1.2\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n 0.0  1.4\n 0.0  1.8\n 0.0  2.0\n 0.0  2.2\n 0.0  7.7\n 194  5.3\nBegin session:\n 204 10.4\n 215 16.8\n 226 17.7\n 236 22.1\n 248 25.8\n 259 30.7\n 269 32.7\n 280 27.7\n 145 29.4\n 151 27.3\n 156 29.2\n 162 24.6\n 167 28.9\n 172 27.0\n 178 27.8\n 183 17.1\n 188 18.5 \n 194 18.1\nEnded session\nClass: llama, Prob: 0.99927825\n 134 21.3", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.5.0-rc1-1783-g7d7dce1', '1.5.0-rc1')\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: Build label: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0\r\n- **CUDA/cuDNN version**: N/A (CPU only)\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\nsave_alexnet_checkpoint.py\r\n```\r\nimport os\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom alexnet import AlexNet\r\nfrom datagenerator import ImageDataGenerator\r\nfrom datetime import datetime\r\nfrom tensorflow.contrib.data import Iterator\r\n\r\n\"\"\"\r\nConfiguration Part.\r\n\"\"\"\r\n\r\nnum_classes = 1000\r\ncheckpoint_path = \"path/to/ckpt\"\r\n\r\n\r\n# Create parent path if it doesn't exist\r\nif not os.path.isdir(checkpoint_path):\r\n    os.mkdir(checkpoint_path)\r\n\r\n# TF placeholder for graph input and output\r\n\r\n\r\n# Initialize model\r\nx = tf.placeholder(tf.float32, [1, 227, 227, 3],name=\"input\")\r\nkeep_prob=tf.placeholder(tf.float32,[],name=\"keepProbs\")\r\nmodel = AlexNet(x, keep_prob, num_classes, [])\r\nsoftmax = tf.nn.softmax(model.fc8,name=\"softmax\")\r\n\r\n# Initialize an saver for store model checkpoints\r\nsaver = tf.train.Saver()\r\n\r\n# Start Tensorflow session\r\nwith tf.Session() as sess:\r\n\r\n\t# Validate the model on the entire validation set\r\n\tsess.run(tf.global_variables_initializer())\r\n\r\n\t# Load the pretrained weights into the non-trainable layer\r\n\tmodel.load_initial_weights(sess)\r\n\r\n\t# save checkpoint of the model\r\n\tcheckpoint_name = os.path.join(checkpoint_path,\r\n\t\t                       'original_alexnet.ckpt')\r\n\tsave_path = saver.save(sess, checkpoint_name)\r\n\r\n\ttf.train.write_graph(sess.graph_def, checkpoint_path, 'alexnet_def.pb',as_text=False)\r\n\r\n\tnames=[]\r\n\tfor n in tf.get_default_graph().as_graph_def().node:\r\n\t\tnames.append(str(n.name))\r\n\r\n    \tnames = sorted(names, key=str.lower)\r\n    \tfor n in names:\r\n\t\tprint n\r\n\r\n\r\n\tprint(\"Model checkpoint saved at {}\".format(checkpoint_name))\r\n```\r\n<br><br>\r\n\r\nFreeze saved graph_def and weights:\r\n```\r\nbazel build tensorflow/python/tools:freeze_graph\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/alexnet_def.pb -- input_checkpoint=path/to/ckpt/original_alexnet.ckpt --output_graph=/path/to/frozen_alexnet.pb --output_node_names='softmax' --input_binary=True\r\n```\r\n<br><br>\r\n\r\nForward Inference:\r\n```\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nfrom alexnet import AlexNet\r\nfrom tensorflow.python.client import timeline\r\nfrom caffe_classes import class_names\r\n\r\nfrozen_graph='/path/to/frozen_alexnet.pb '\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graph_def, name=\"prefix\")\r\n    return graph\r\n\r\nimagenet_mean = np.array([104., 117., 124.], dtype=np.float32)\r\n\r\ncurrent_dir = os.getcwd()\r\nimage_dir = os.path.join(current_dir, 'images')\r\n\r\nimg_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpeg')]\r\n\r\nimgs = []\r\nfor f in img_files:\r\n    imgs.append(cv2.imread(f))\r\n\r\ng=load_graph(frozen_graph)\r\nwith tf.Session(graph=g) as sess:\r\n\r\n    softmax=g.get_tensor_by_name('prefix/softmax:0')\r\n    x = g.get_tensor_by_name(\"prefix/input:0\")\r\n    keep_prob = g.get_tensor_by_name(\"prefix/keepProbs:0\")\r\n\r\n    for i, image in enumerate(imgs):\r\n        \r\n        img = cv2.resize(image.astype(np.float32), (227,227))\r\n        img -= imagenet_mean\r\n        img = img.reshape((1,227,227,3))\r\n\r\n\toptions = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n\trun_metadata = tf.RunMetadata()\r\n        \r\n        print 'Begin session:'\r\n        probs = sess.run(softmax, feed_dict={x: img, keep_prob: 1}, \\\r\n\t\t\toptions=options, run_metadata=run_metadata)\r\n        print 'Ended session'\r\n\tprint 'Class: ' + str(class_names[np.argmax(probs)]) \\\r\n\t+ ', Prob: ' + str(probs[0,np.argmax(probs)])\r\n\tfetched_timeline = timeline.Timeline(run_metadata.step_stats)\r\n\tchrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)\r\n\twith open('./forward_timeline.json', 'w') as f:\r\n\t\tf.write(chrome_trace)\r\n\r\n```\r\n\r\nlog_alexnet.sh\r\n```\r\n#!/bin/bash\r\n\r\nlogpid() { while sleep 0.1; do ps -p $1 -o pcpu= -o pmem= ; done; }\r\npython frozen-infer.py &\r\nlogpid $! | tee ./pid.log\r\n```\r\n### Describe the problem\r\n\r\nI ran my own Alexnet frozen model and the forward inference script I had required a large amount of RAM, around 2.466 GB rather than the memory required to load model size which was (243.9 MB).\r\n\r\nEssentially I took scripts from [finetune_alexnet_with_tensorflow](https://github.com/kratzert/finetune_alexnet_with_tensorflow), including ```alexnet.py```, ```datagenerator.py```, ```caffe_classes.py```, and heavily modifying ```validate_alexnet_on_imagenet.ipynb``` to save a checkpoint of the Alexnet-implementation for TensorFlow. The weights were taken from this [link](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/). I ran ```save_alexnet_checkpoint.py``` first to generate a binary proto graph definition of the AlexNet file and corresponding checkpoint files. Using the default ```freeze_graph``` tool from TensorFlow compiled by Bazel, I created a frozen graph. I then ran ```log_alexnet.sh``` to see how much of my RAM a forward inference uses on my laptop. The image I used was from the above mentioned github repo, llama. The script outputs the percentage of CPU and memory the python script ```frozen-infer.py```  uses. \r\n\r\nMy CPU: Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz\r\n\r\nMy RAM is 8 GB, but according to the free command, it is 7.909 GB, where htop reports it as 7.54 GB. The maximum RAM allocated by ```sess.run``` and the python script was thus, 0.327*7.54 = 2.466 GB (see below log). This is insanely larger than the 243.9 MB model I had. The ```chrome_trace``` I generated reports a maximum of around 235 MB, which is no where near what was actually happening.\r\n[forward_timeline.json.tar.gz](https://github.com/tensorflow/tensorflow/files/1678731/forward_timeline.json.tar.gz)\r\n\r\nMy question is why does the session run require such memory, and how am I able to reduce it down to only consume the required memory to load the model, which would be ~250 MB? I am concerned about this because I will to transferring this code to a Raspberry Pi 3 soon.\r\n\r\nIt seems someone else had the same issue on stackoverflow [here](https://stackoverflow.com/questions/46531213/after-freeze-graph-the-inference-is-slower-and-requires-more-memory).\r\n\r\n### Source code / logs\r\nThe output I got is such from log_alexnet.sh:\r\n\r\n```\r\n 0.0  0.7\r\n 0.0  1.0\r\n 0.0  1.2\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n 0.0  1.4\r\n 0.0  1.8\r\n 0.0  2.0\r\n 0.0  2.2\r\n 0.0  7.7\r\n 194  5.3\r\nBegin session:\r\n 204 10.4\r\n 215 16.8\r\n 226 17.7\r\n 236 22.1\r\n 248 25.8\r\n 259 30.7\r\n 269 32.7\r\n 280 27.7\r\n 145 29.4\r\n 151 27.3\r\n 156 29.2\r\n 162 24.6\r\n 167 28.9\r\n 172 27.0\r\n 178 27.8\r\n 183 17.1\r\n 188 18.5 \r\n 194 18.1\r\nEnded session\r\nClass: llama, Prob: 0.99927825\r\n 134 21.3\r\n```\r\n"}