{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/271", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/271/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/271/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/271/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/271", "id": 117562964, "node_id": "MDU6SXNzdWUxMTc1NjI5NjQ=", "number": 271, "title": "Predicting words instead of probabilities?", "user": {"login": "aliabbasjp", "id": 830792, "node_id": "MDQ6VXNlcjgzMDc5Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/830792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aliabbasjp", "html_url": "https://github.com/aliabbasjp", "followers_url": "https://api.github.com/users/aliabbasjp/followers", "following_url": "https://api.github.com/users/aliabbasjp/following{/other_user}", "gists_url": "https://api.github.com/users/aliabbasjp/gists{/gist_id}", "starred_url": "https://api.github.com/users/aliabbasjp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aliabbasjp/subscriptions", "organizations_url": "https://api.github.com/users/aliabbasjp/orgs", "repos_url": "https://api.github.com/users/aliabbasjp/repos", "events_url": "https://api.github.com/users/aliabbasjp/events{/privacy}", "received_events_url": "https://api.github.com/users/aliabbasjp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2015-11-18T10:57:52Z", "updated_at": "2015-11-29T00:16:49Z", "closed_at": "2015-11-18T17:18:24Z", "author_association": "NONE", "body_html": "<p>The tensorflow tutorial on language model allows to compute the probability of sentences :</p>\n<p>probabilities = tf.nn.softmax(logits)</p>\n<p>in the comments below it also specifies a way of predicting the next word instead of probabilities but does not specify how this can be done. So how to output a word instead of probability using this example?</p>\n<pre><code>lstm = rnn_cell.BasicLSTMCell(lstm_size)\n# Initial state of the LSTM memory.\nstate = tf.zeros([batch_size, lstm.state_size])\n\nloss = 0.0\nfor current_batch_of_words in words_in_dataset:\n    # The value of state is updated after processing each batch of words.\n    output, state = lstm(current_batch_of_words, state)\n\n    # The LSTM output can be used to make next word predictions\n    logits = tf.matmul(output, softmax_w) + softmax_b\n    probabilities = tf.nn.softmax(logits)\n    loss += loss_function(probabilities, target_words)\n\n</code></pre>", "body_text": "The tensorflow tutorial on language model allows to compute the probability of sentences :\nprobabilities = tf.nn.softmax(logits)\nin the comments below it also specifies a way of predicting the next word instead of probabilities but does not specify how this can be done. So how to output a word instead of probability using this example?\nlstm = rnn_cell.BasicLSTMCell(lstm_size)\n# Initial state of the LSTM memory.\nstate = tf.zeros([batch_size, lstm.state_size])\n\nloss = 0.0\nfor current_batch_of_words in words_in_dataset:\n    # The value of state is updated after processing each batch of words.\n    output, state = lstm(current_batch_of_words, state)\n\n    # The LSTM output can be used to make next word predictions\n    logits = tf.matmul(output, softmax_w) + softmax_b\n    probabilities = tf.nn.softmax(logits)\n    loss += loss_function(probabilities, target_words)", "body": "The tensorflow tutorial on language model allows to compute the probability of sentences :\n\nprobabilities = tf.nn.softmax(logits)\n\nin the comments below it also specifies a way of predicting the next word instead of probabilities but does not specify how this can be done. So how to output a word instead of probability using this example?\n\n```\nlstm = rnn_cell.BasicLSTMCell(lstm_size)\n# Initial state of the LSTM memory.\nstate = tf.zeros([batch_size, lstm.state_size])\n\nloss = 0.0\nfor current_batch_of_words in words_in_dataset:\n    # The value of state is updated after processing each batch of words.\n    output, state = lstm(current_batch_of_words, state)\n\n    # The LSTM output can be used to make next word predictions\n    logits = tf.matmul(output, softmax_w) + softmax_b\n    probabilities = tf.nn.softmax(logits)\n    loss += loss_function(probabilities, target_words)\n\n```\n"}