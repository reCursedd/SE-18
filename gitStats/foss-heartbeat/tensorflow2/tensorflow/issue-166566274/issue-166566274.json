{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3414", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3414/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3414/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3414/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/3414", "id": 166566274, "node_id": "MDExOlB1bGxSZXF1ZXN0NzgxMzU3MzA=", "number": 3414, "title": "add Dummy gradient for LogUniformCandidateSampler", "user": {"login": "lhlmgr", "id": 400331, "node_id": "MDQ6VXNlcjQwMDMzMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/400331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhlmgr", "html_url": "https://github.com/lhlmgr", "followers_url": "https://api.github.com/users/lhlmgr/followers", "following_url": "https://api.github.com/users/lhlmgr/following{/other_user}", "gists_url": "https://api.github.com/users/lhlmgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhlmgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhlmgr/subscriptions", "organizations_url": "https://api.github.com/users/lhlmgr/orgs", "repos_url": "https://api.github.com/users/lhlmgr/repos", "events_url": "https://api.github.com/users/lhlmgr/events{/privacy}", "received_events_url": "https://api.github.com/users/lhlmgr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136613, "node_id": "MDU6TGFiZWwzMDAxMzY2MTM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20no", "name": "cla: no", "color": "eb6420", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-07-20T12:10:23Z", "updated_at": "2016-07-23T04:35:57Z", "closed_at": "2016-07-23T04:35:57Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/3414", "html_url": "https://github.com/tensorflow/tensorflow/pull/3414", "diff_url": "https://github.com/tensorflow/tensorflow/pull/3414.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/3414.patch"}, "body_html": "<p>I added a dummy Gradient for the LogUniformCandidateSampler, as proposed by Martin Wicke    in <a href=\"https://groups.google.com/a/tensorflow.org/d/msg/discuss/P63KdpGGwvA/1ks-INGBAwAJ\" rel=\"nofollow\">this</a> discussion thread, to get rid of this error:</p>\n<blockquote>\n<p>WARNING:tensorflow:&lt;tensorflow.python.ops.rnn_cell.BasicLSTMCell<br>\nobject at 0x7f50696455f8&gt;: Using a concatenated state is slower and<br>\nwill soon be deprecated.  Use state_is_tuple=True. Traceback (most<br>\nrecent call last):   File<br>\n\"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\",<br>\nline 448, in gradients<br>\ngrad_fn = ops.get_gradient_function(op)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\",<br>\nline 1634, in get_gradient_function<br>\nreturn _gradient_registry.lookup(op_type)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/registry.py\",<br>\nline 85, in lookup<br>\n\"%s registry has no entry for: %s\" % (self._name, name)) LookupError: gradient registry has no entry for:<br>\nLogUniformCandidateSampler</p>\n<p>During handling of the above exception, another exception occurred:</p>\n<p>Traceback (most recent call last):   File<br>\n\"/home/aa/code/python/bb/dyn_main.py\", line 176, in <br>\ntf.app.run()   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py\",<br>\nline 30, in run<br>\nsys.exit(main(sys.argv))   File \"/home/aa/code/python/bb/dyn_main.py\", line 83, in main<br>\ngradients = tf.gradients(loss2, params)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\",<br>\nline 452, in gradients<br>\n(op.name, op.type)) LookupError: No gradient defined for operation 'dynamic_rnn_seq2seq/sequence_loss_by_example_dyn/while/cond/sampled_softmax_loss/LogUniformCandidateSampler'<br>\n(op type: LogUniformCandidateSampler)</p>\n</blockquote>\n<p>Thanks!</p>", "body_text": "I added a dummy Gradient for the LogUniformCandidateSampler, as proposed by Martin Wicke    in this discussion thread, to get rid of this error:\n\nWARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell\nobject at 0x7f50696455f8>: Using a concatenated state is slower and\nwill soon be deprecated.  Use state_is_tuple=True. Traceback (most\nrecent call last):   File\n\"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\",\nline 448, in gradients\ngrad_fn = ops.get_gradient_function(op)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\",\nline 1634, in get_gradient_function\nreturn _gradient_registry.lookup(op_type)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/registry.py\",\nline 85, in lookup\n\"%s registry has no entry for: %s\" % (self._name, name)) LookupError: gradient registry has no entry for:\nLogUniformCandidateSampler\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):   File\n\"/home/aa/code/python/bb/dyn_main.py\", line 176, in \ntf.app.run()   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py\",\nline 30, in run\nsys.exit(main(sys.argv))   File \"/home/aa/code/python/bb/dyn_main.py\", line 83, in main\ngradients = tf.gradients(loss2, params)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\",\nline 452, in gradients\n(op.name, op.type)) LookupError: No gradient defined for operation 'dynamic_rnn_seq2seq/sequence_loss_by_example_dyn/while/cond/sampled_softmax_loss/LogUniformCandidateSampler'\n(op type: LogUniformCandidateSampler)\n\nThanks!", "body": "I added a dummy Gradient for the LogUniformCandidateSampler, as proposed by Martin Wicke    in [this](https://groups.google.com/a/tensorflow.org/d/msg/discuss/P63KdpGGwvA/1ks-INGBAwAJ) discussion thread, to get rid of this error:\n\n> WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell\n> object at 0x7f50696455f8>: Using a concatenated state is slower and\n> will soon be deprecated.  Use state_is_tuple=True. Traceback (most\n> recent call last):   File\n> \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\",\n> line 448, in gradients\n>     grad_fn = ops.get_gradient_function(op)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\",\n> line 1634, in get_gradient_function\n>     return _gradient_registry.lookup(op_type)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/registry.py\",\n> line 85, in lookup\n>     \"%s registry has no entry for: %s\" % (self._name, name)) LookupError: gradient registry has no entry for:\n> LogUniformCandidateSampler\n> \n> During handling of the above exception, another exception occurred:\n> \n> Traceback (most recent call last):   File\n> \"/home/aa/code/python/bb/dyn_main.py\", line 176, in <module>\n>     tf.app.run()   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py\",\n> line 30, in run\n>     sys.exit(main(sys.argv))   File \"/home/aa/code/python/bb/dyn_main.py\", line 83, in main\n>     gradients = tf.gradients(loss2, params)   File \"/home/aa/anaconda3/envs/master_tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\",\n> line 452, in gradients\n>     (op.name, op.type)) LookupError: No gradient defined for operation 'dynamic_rnn_seq2seq/sequence_loss_by_example_dyn/while/cond/sampled_softmax_loss/LogUniformCandidateSampler'\n> (op type: LogUniformCandidateSampler)\n\nThanks!\n"}