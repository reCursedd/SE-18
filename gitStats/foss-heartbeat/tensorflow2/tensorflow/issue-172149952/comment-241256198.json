{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241256198", "html_url": "https://github.com/tensorflow/tensorflow/issues/3926#issuecomment-241256198", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3926", "id": 241256198, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTI1NjE5OA==", "user": {"login": "kalle", "id": 370192, "node_id": "MDQ6VXNlcjM3MDE5Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/370192?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kalle", "html_url": "https://github.com/kalle", "followers_url": "https://api.github.com/users/kalle/followers", "following_url": "https://api.github.com/users/kalle/following{/other_user}", "gists_url": "https://api.github.com/users/kalle/gists{/gist_id}", "starred_url": "https://api.github.com/users/kalle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kalle/subscriptions", "organizations_url": "https://api.github.com/users/kalle/orgs", "repos_url": "https://api.github.com/users/kalle/repos", "events_url": "https://api.github.com/users/kalle/events{/privacy}", "received_events_url": "https://api.github.com/users/kalle/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-21T13:04:14Z", "updated_at": "2016-08-21T13:04:14Z", "author_association": "NONE", "body_html": "<p>Yaroslav , thank your for taking the time to answer my questions on a weekend! :-)</p>\n<p>OK, I have made another <a href=\"https://github.com/tensorflow/tensorflow/files/428839/bug_report2.zip\">test program</a> that only uses a batch normalizer and I train beta and gamma to give the the distribution I want.</p>\n<p>I send in noise with distribution Norm(m=2,v=10) and then learn the parameters beta and gamma so the output from the batch normalization is distributed as Norm(m=7,v=5). Then I run an inference pass with test data directly after training I get the expected mean and variance (7 resp. 5).<br>\nBut when I restore the net and send in the same test data I get some weird values that don't make any sense at all. Even if the input data is generated with Norm(m=2,v=10) which BatchNorm should have learnt by now. The moving mean and variance don't appear to be updated and the only reason that BN work when I run it in training mode is that it uses the batch mean/variance.</p>\n<pre><code>test data:\n    mean = [ 1.99910051  2.00070378]\n    variance = [  9.99379123  10.02349832]\nAfter training:\n    calculated mean=[ 7.01989126  7.01992226]\n    calculated variance=[ 4.9753685   4.97816467]\n    beta=[ 7.01995707  7.0199585 ]\n    gamma=[ 2.23007703  2.23052549]\n    moving mean=[ 0.  0.]\n    moving variance=[ 1.  1.]\nAfter restore:\n    calculated mean=[ 11.47598934  11.4803896 ]\n    calculated variance=[ 49.58828735  49.75190735]\n    beta=[ 7.01995707  7.0199585 ]\n    gamma=[ 2.23007703  2.23052549]\n    moving mean=[ 0.  0.]\n    moving variance=[ 1.  1.]\n</code></pre>\n<p>Test program:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.layers <span class=\"pl-k\">as</span> contrib\n\n<span class=\"pl-c1\">FLAGS</span> <span class=\"pl-k\">=</span> tf.app.flags.<span class=\"pl-c1\">FLAGS</span>\n\ntf.app.flags.DEFINE_string(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tmp_dir<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tmp/bugreport<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Temp dir<span class=\"pl-pds\">\"\"\"</span></span>)\n\n<span class=\"pl-c1\">SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">500</span>\n<span class=\"pl-c1\">NUM_ITERS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n<span class=\"pl-c1\">DEPTH</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n<span class=\"pl-c1\">MEAN</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n<span class=\"pl-c1\">VARIANCE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_randn</span>(<span class=\"pl-k\">*</span><span class=\"pl-smi\">shape</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Create a tensor with values drawn from a gaussion distribution (MEAN, VARIANCE)<span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-k\">return</span> np.sqrt(<span class=\"pl-c1\">VARIANCE</span>) <span class=\"pl-k\">*</span> np.random.randn(<span class=\"pl-k\">*</span>shape) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">MEAN</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_get_bn_vars</span>(<span class=\"pl-smi\">sess</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Return values of internal BatchNorm variables<span class=\"pl-pds\">\"</span></span>\n    beta <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    gamma <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    moving_mean <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    moving_variance <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> tf.all_variables():\n        <span class=\"pl-k\">if</span> v.name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>BatchNorm/beta:0<span class=\"pl-pds\">\"</span></span>:\n            beta <span class=\"pl-k\">=</span> sess.run(v)\n        <span class=\"pl-k\">elif</span> v.name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>BatchNorm/gamma:0<span class=\"pl-pds\">\"</span></span>:\n            gamma <span class=\"pl-k\">=</span> sess.run(v)\n        <span class=\"pl-k\">elif</span> v.name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>BatchNorm/moving_mean:0<span class=\"pl-pds\">\"</span></span>:\n            moving_mean <span class=\"pl-k\">=</span> sess.run(v)\n        <span class=\"pl-k\">elif</span> v.name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>BatchNorm/moving_variance:0<span class=\"pl-pds\">\"</span></span>:\n            moving_variance <span class=\"pl-k\">=</span> sess.run(v)\n    <span class=\"pl-k\">return</span> beta, gamma, moving_mean, moving_variance\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">inference</span>(<span class=\"pl-smi\">input_tensor</span>, <span class=\"pl-smi\">is_training</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Define a stupid net<span class=\"pl-pds\">\"</span></span>\n    bn_out <span class=\"pl-k\">=</span> contrib.batch_norm(input_tensor,\n            <span class=\"pl-v\">center</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n            <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training)\n    <span class=\"pl-k\">return</span> bn_out\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">training</span>(<span class=\"pl-smi\">test_data</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>1. Train the net</span>\n<span class=\"pl-s\">    2. Do an inference pass with the trained net</span>\n<span class=\"pl-s\">    3. Checkpoint the trained net</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.Graph().as_default():\n        tf.set_random_seed(<span class=\"pl-c1\">123</span>)\n        input_tensor <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">SIZE</span>, <span class=\"pl-c1\">SIZE</span>, <span class=\"pl-c1\">DEPTH</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>)\n\n        bn_out <span class=\"pl-k\">=</span> inference(input_tensor, tf.constant(<span class=\"pl-c1\">True</span>))\n        mean, variance <span class=\"pl-k\">=</span> tf.nn.moments(bn_out, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n        loss <span class=\"pl-k\">=</span> tf.nn.l2_loss(mean<span class=\"pl-k\">-</span><span class=\"pl-c1\">7</span>) <span class=\"pl-k\">+</span> tf.nn.l2_loss(variance <span class=\"pl-k\">-</span> <span class=\"pl-c1\">5</span>)\n        train_op <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.9999</span>).minimize(loss)\n        init <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n        saver <span class=\"pl-k\">=</span> tf.train.Saver(tf.all_variables())\n\n        <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n            sess.run([init])\n            <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">NUM_ITERS</span>):\n                input_data <span class=\"pl-k\">=</span> _randn(<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">SIZE</span>, <span class=\"pl-c1\">SIZE</span>, <span class=\"pl-c1\">DEPTH</span>)\n                _ <span class=\"pl-k\">=</span> sess.run(train_op, { input_tensor: input_data } )\n            mean_val, variance_val <span class=\"pl-k\">=</span> sess.run([mean, variance], { input_tensor: test_data })\n            cp_path <span class=\"pl-k\">=</span> os.path.join(<span class=\"pl-c1\">FLAGS</span>.tmp_dir, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model.ckpt<span class=\"pl-pds\">\"</span></span>)\n            saver.save(sess, cp_path,\n                       <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">write_meta_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n            bn_vars <span class=\"pl-k\">=</span> _get_bn_vars(sess)\n    <span class=\"pl-k\">return</span> mean_val, variance_val, bn_vars\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">use_restored_net</span>(<span class=\"pl-smi\">test_data</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>1. Load checkpointed net</span>\n<span class=\"pl-s\">    2. Do an inference pass with the trained net</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.Graph().as_default():\n        input_tensor <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">SIZE</span>, <span class=\"pl-c1\">SIZE</span>, <span class=\"pl-c1\">DEPTH</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>)\n\n        bn_out <span class=\"pl-k\">=</span> inference(input_tensor, tf.constant(<span class=\"pl-c1\">False</span>))\n        mean, variance <span class=\"pl-k\">=</span> tf.nn.moments(bn_out, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n        init <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n\n        ckpt <span class=\"pl-k\">=</span> tf.train.get_checkpoint_state(<span class=\"pl-c1\">FLAGS</span>.tmp_dir)\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span>(ckpt <span class=\"pl-k\">and</span> ckpt.model_checkpoint_path):\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Could not find a checkpointed model<span class=\"pl-pds\">\"</span></span>)\n        saver <span class=\"pl-k\">=</span> tf.train.Saver(tf.all_variables())\n        <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n            sess.run([init])\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            bn_vars <span class=\"pl-k\">=</span> _get_bn_vars(sess)\n            mean_val, variance_val <span class=\"pl-k\">=</span> sess.run([mean, variance], { input_tensor: test_data })\n    <span class=\"pl-k\">return</span> mean_val, variance_val, bn_vars\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">argv</span>):\n    <span class=\"pl-k\">if</span> tf.gfile.Exists(<span class=\"pl-c1\">FLAGS</span>.tmp_dir):\n        tf.gfile.DeleteRecursively(<span class=\"pl-c1\">FLAGS</span>.tmp_dir)\n    tf.gfile.MakeDirs(<span class=\"pl-c1\">FLAGS</span>.tmp_dir)\n\n    np.random.seed(<span class=\"pl-c1\">123</span>)\n    test_data <span class=\"pl-k\">=</span> _randn(<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">SIZE</span>, <span class=\"pl-c1\">SIZE</span>, <span class=\"pl-c1\">DEPTH</span>)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test data:<span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    mean = <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(np.mean(test_data, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>)))\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    variance = <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(np.var(test_data, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>)))\n\n    test_mean, test_variance, train_bn <span class=\"pl-k\">=</span> training(test_data)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>After training:<span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    calculated mean=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(test_mean)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    calculated variance=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(test_variance)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    beta=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(train_bn[<span class=\"pl-c1\">0</span>])\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    gamma=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(train_bn[<span class=\"pl-c1\">1</span>])\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    moving mean=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(train_bn[<span class=\"pl-c1\">2</span>])\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    moving variance=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(train_bn[<span class=\"pl-c1\">3</span>])\n    restored_mean, restored_variance, restored_bn <span class=\"pl-k\">=</span> use_restored_net(test_data)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>After restore:<span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    calculated mean=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(restored_mean)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    calculated variance=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(restored_variance)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    beta=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(restored_bn[<span class=\"pl-c1\">0</span>])\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    gamma=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(restored_bn[<span class=\"pl-c1\">1</span>])\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    moving mean=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(restored_bn[<span class=\"pl-c1\">2</span>])\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>    moving variance=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(restored_bn[<span class=\"pl-c1\">3</span>])\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    tf.app.run()\n</pre></div>", "body_text": "Yaroslav , thank your for taking the time to answer my questions on a weekend! :-)\nOK, I have made another test program that only uses a batch normalizer and I train beta and gamma to give the the distribution I want.\nI send in noise with distribution Norm(m=2,v=10) and then learn the parameters beta and gamma so the output from the batch normalization is distributed as Norm(m=7,v=5). Then I run an inference pass with test data directly after training I get the expected mean and variance (7 resp. 5).\nBut when I restore the net and send in the same test data I get some weird values that don't make any sense at all. Even if the input data is generated with Norm(m=2,v=10) which BatchNorm should have learnt by now. The moving mean and variance don't appear to be updated and the only reason that BN work when I run it in training mode is that it uses the batch mean/variance.\ntest data:\n    mean = [ 1.99910051  2.00070378]\n    variance = [  9.99379123  10.02349832]\nAfter training:\n    calculated mean=[ 7.01989126  7.01992226]\n    calculated variance=[ 4.9753685   4.97816467]\n    beta=[ 7.01995707  7.0199585 ]\n    gamma=[ 2.23007703  2.23052549]\n    moving mean=[ 0.  0.]\n    moving variance=[ 1.  1.]\nAfter restore:\n    calculated mean=[ 11.47598934  11.4803896 ]\n    calculated variance=[ 49.58828735  49.75190735]\n    beta=[ 7.01995707  7.0199585 ]\n    gamma=[ 2.23007703  2.23052549]\n    moving mean=[ 0.  0.]\n    moving variance=[ 1.  1.]\n\nTest program:\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.layers as contrib\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('tmp_dir', '/tmp/bugreport', \"\"\"Temp dir\"\"\")\n\nSIZE = 500\nNUM_ITERS = 100\nBATCH_SIZE = 5\nDEPTH = 2\nMEAN = 2\nVARIANCE = 10\n\ndef _randn(*shape):\n    \"Create a tensor with values drawn from a gaussion distribution (MEAN, VARIANCE)\"\n    return np.sqrt(VARIANCE) * np.random.randn(*shape) + MEAN\n\n\ndef _get_bn_vars(sess):\n    \"Return values of internal BatchNorm variables\"\n    beta = None\n    gamma = None\n    moving_mean = None\n    moving_variance = None\n    for v in tf.all_variables():\n        if v.name == \"BatchNorm/beta:0\":\n            beta = sess.run(v)\n        elif v.name == \"BatchNorm/gamma:0\":\n            gamma = sess.run(v)\n        elif v.name == \"BatchNorm/moving_mean:0\":\n            moving_mean = sess.run(v)\n        elif v.name == \"BatchNorm/moving_variance:0\":\n            moving_variance = sess.run(v)\n    return beta, gamma, moving_mean, moving_variance\n\n\ndef inference(input_tensor, is_training):\n    \"Define a stupid net\"\n    bn_out = contrib.batch_norm(input_tensor,\n            center=True,\n            scale=True,\n            is_training=is_training)\n    return bn_out\n\n\ndef training(test_data):\n    \"\"\"1. Train the net\n    2. Do an inference pass with the trained net\n    3. Checkpoint the trained net\n    \"\"\"\n    with tf.Graph().as_default():\n        tf.set_random_seed(123)\n        input_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, SIZE, SIZE, DEPTH], name=\"input\")\n\n        bn_out = inference(input_tensor, tf.constant(True))\n        mean, variance = tf.nn.moments(bn_out, [0, 1, 2])\n        loss = tf.nn.l2_loss(mean-7) + tf.nn.l2_loss(variance - 5)\n        train_op = tf.train.AdamOptimizer(0.9999).minimize(loss)\n        init = tf.initialize_all_variables()\n        saver = tf.train.Saver(tf.all_variables())\n\n        with tf.Session() as sess:\n            sess.run([init])\n            for _ in range(NUM_ITERS):\n                input_data = _randn(BATCH_SIZE, SIZE, SIZE, DEPTH)\n                _ = sess.run(train_op, { input_tensor: input_data } )\n            mean_val, variance_val = sess.run([mean, variance], { input_tensor: test_data })\n            cp_path = os.path.join(FLAGS.tmp_dir, \"model.ckpt\")\n            saver.save(sess, cp_path,\n                       global_step=0, write_meta_graph=None)\n            bn_vars = _get_bn_vars(sess)\n    return mean_val, variance_val, bn_vars\n\n\ndef use_restored_net(test_data):\n    \"\"\"1. Load checkpointed net\n    2. Do an inference pass with the trained net\n    \"\"\"\n    with tf.Graph().as_default():\n        input_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, SIZE, SIZE, DEPTH], name=\"input\")\n\n        bn_out = inference(input_tensor, tf.constant(False))\n        mean, variance = tf.nn.moments(bn_out, [0, 1, 2])\n        init = tf.initialize_all_variables()\n\n        ckpt = tf.train.get_checkpoint_state(FLAGS.tmp_dir)\n        if not(ckpt and ckpt.model_checkpoint_path):\n            raise ValueError(\"Could not find a checkpointed model\")\n        saver = tf.train.Saver(tf.all_variables())\n        with tf.Session() as sess:\n            sess.run([init])\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            bn_vars = _get_bn_vars(sess)\n            mean_val, variance_val = sess.run([mean, variance], { input_tensor: test_data })\n    return mean_val, variance_val, bn_vars\n\n\ndef main(argv):\n    if tf.gfile.Exists(FLAGS.tmp_dir):\n        tf.gfile.DeleteRecursively(FLAGS.tmp_dir)\n    tf.gfile.MakeDirs(FLAGS.tmp_dir)\n\n    np.random.seed(123)\n    test_data = _randn(BATCH_SIZE, SIZE, SIZE, DEPTH)\n    print \"test data:\"\n    print \"    mean = {}\".format(np.mean(test_data, axis=(0, 1, 2)))\n    print \"    variance = {}\".format(np.var(test_data, axis=(0, 1, 2)))\n\n    test_mean, test_variance, train_bn = training(test_data)\n    print \"After training:\"\n    print \"    calculated mean={}\".format(test_mean)\n    print \"    calculated variance={}\".format(test_variance)\n    print \"    beta={}\".format(train_bn[0])\n    print \"    gamma={}\".format(train_bn[1])\n    print \"    moving mean={}\".format(train_bn[2])\n    print \"    moving variance={}\".format(train_bn[3])\n    restored_mean, restored_variance, restored_bn = use_restored_net(test_data)\n    print \"After restore:\"\n    print \"    calculated mean={}\".format(restored_mean)\n    print \"    calculated variance={}\".format(restored_variance)\n    print \"    beta={}\".format(restored_bn[0])\n    print \"    gamma={}\".format(restored_bn[1])\n    print \"    moving mean={}\".format(restored_bn[2])\n    print \"    moving variance={}\".format(restored_bn[3])\n\n\nif __name__ == '__main__':\n    tf.app.run()", "body": "Yaroslav , thank your for taking the time to answer my questions on a weekend! :-)\n\nOK, I have made another [test program](https://github.com/tensorflow/tensorflow/files/428839/bug_report2.zip) that only uses a batch normalizer and I train beta and gamma to give the the distribution I want.\n\nI send in noise with distribution Norm(m=2,v=10) and then learn the parameters beta and gamma so the output from the batch normalization is distributed as Norm(m=7,v=5). Then I run an inference pass with test data directly after training I get the expected mean and variance (7 resp. 5).\nBut when I restore the net and send in the same test data I get some weird values that don't make any sense at all. Even if the input data is generated with Norm(m=2,v=10) which BatchNorm should have learnt by now. The moving mean and variance don't appear to be updated and the only reason that BN work when I run it in training mode is that it uses the batch mean/variance.\n\n```\ntest data:\n    mean = [ 1.99910051  2.00070378]\n    variance = [  9.99379123  10.02349832]\nAfter training:\n    calculated mean=[ 7.01989126  7.01992226]\n    calculated variance=[ 4.9753685   4.97816467]\n    beta=[ 7.01995707  7.0199585 ]\n    gamma=[ 2.23007703  2.23052549]\n    moving mean=[ 0.  0.]\n    moving variance=[ 1.  1.]\nAfter restore:\n    calculated mean=[ 11.47598934  11.4803896 ]\n    calculated variance=[ 49.58828735  49.75190735]\n    beta=[ 7.01995707  7.0199585 ]\n    gamma=[ 2.23007703  2.23052549]\n    moving mean=[ 0.  0.]\n    moving variance=[ 1.  1.]\n```\n\nTest program:\n\n``` python\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.layers as contrib\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('tmp_dir', '/tmp/bugreport', \"\"\"Temp dir\"\"\")\n\nSIZE = 500\nNUM_ITERS = 100\nBATCH_SIZE = 5\nDEPTH = 2\nMEAN = 2\nVARIANCE = 10\n\ndef _randn(*shape):\n    \"Create a tensor with values drawn from a gaussion distribution (MEAN, VARIANCE)\"\n    return np.sqrt(VARIANCE) * np.random.randn(*shape) + MEAN\n\n\ndef _get_bn_vars(sess):\n    \"Return values of internal BatchNorm variables\"\n    beta = None\n    gamma = None\n    moving_mean = None\n    moving_variance = None\n    for v in tf.all_variables():\n        if v.name == \"BatchNorm/beta:0\":\n            beta = sess.run(v)\n        elif v.name == \"BatchNorm/gamma:0\":\n            gamma = sess.run(v)\n        elif v.name == \"BatchNorm/moving_mean:0\":\n            moving_mean = sess.run(v)\n        elif v.name == \"BatchNorm/moving_variance:0\":\n            moving_variance = sess.run(v)\n    return beta, gamma, moving_mean, moving_variance\n\n\ndef inference(input_tensor, is_training):\n    \"Define a stupid net\"\n    bn_out = contrib.batch_norm(input_tensor,\n            center=True,\n            scale=True,\n            is_training=is_training)\n    return bn_out\n\n\ndef training(test_data):\n    \"\"\"1. Train the net\n    2. Do an inference pass with the trained net\n    3. Checkpoint the trained net\n    \"\"\"\n    with tf.Graph().as_default():\n        tf.set_random_seed(123)\n        input_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, SIZE, SIZE, DEPTH], name=\"input\")\n\n        bn_out = inference(input_tensor, tf.constant(True))\n        mean, variance = tf.nn.moments(bn_out, [0, 1, 2])\n        loss = tf.nn.l2_loss(mean-7) + tf.nn.l2_loss(variance - 5)\n        train_op = tf.train.AdamOptimizer(0.9999).minimize(loss)\n        init = tf.initialize_all_variables()\n        saver = tf.train.Saver(tf.all_variables())\n\n        with tf.Session() as sess:\n            sess.run([init])\n            for _ in range(NUM_ITERS):\n                input_data = _randn(BATCH_SIZE, SIZE, SIZE, DEPTH)\n                _ = sess.run(train_op, { input_tensor: input_data } )\n            mean_val, variance_val = sess.run([mean, variance], { input_tensor: test_data })\n            cp_path = os.path.join(FLAGS.tmp_dir, \"model.ckpt\")\n            saver.save(sess, cp_path,\n                       global_step=0, write_meta_graph=None)\n            bn_vars = _get_bn_vars(sess)\n    return mean_val, variance_val, bn_vars\n\n\ndef use_restored_net(test_data):\n    \"\"\"1. Load checkpointed net\n    2. Do an inference pass with the trained net\n    \"\"\"\n    with tf.Graph().as_default():\n        input_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, SIZE, SIZE, DEPTH], name=\"input\")\n\n        bn_out = inference(input_tensor, tf.constant(False))\n        mean, variance = tf.nn.moments(bn_out, [0, 1, 2])\n        init = tf.initialize_all_variables()\n\n        ckpt = tf.train.get_checkpoint_state(FLAGS.tmp_dir)\n        if not(ckpt and ckpt.model_checkpoint_path):\n            raise ValueError(\"Could not find a checkpointed model\")\n        saver = tf.train.Saver(tf.all_variables())\n        with tf.Session() as sess:\n            sess.run([init])\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            bn_vars = _get_bn_vars(sess)\n            mean_val, variance_val = sess.run([mean, variance], { input_tensor: test_data })\n    return mean_val, variance_val, bn_vars\n\n\ndef main(argv):\n    if tf.gfile.Exists(FLAGS.tmp_dir):\n        tf.gfile.DeleteRecursively(FLAGS.tmp_dir)\n    tf.gfile.MakeDirs(FLAGS.tmp_dir)\n\n    np.random.seed(123)\n    test_data = _randn(BATCH_SIZE, SIZE, SIZE, DEPTH)\n    print \"test data:\"\n    print \"    mean = {}\".format(np.mean(test_data, axis=(0, 1, 2)))\n    print \"    variance = {}\".format(np.var(test_data, axis=(0, 1, 2)))\n\n    test_mean, test_variance, train_bn = training(test_data)\n    print \"After training:\"\n    print \"    calculated mean={}\".format(test_mean)\n    print \"    calculated variance={}\".format(test_variance)\n    print \"    beta={}\".format(train_bn[0])\n    print \"    gamma={}\".format(train_bn[1])\n    print \"    moving mean={}\".format(train_bn[2])\n    print \"    moving variance={}\".format(train_bn[3])\n    restored_mean, restored_variance, restored_bn = use_restored_net(test_data)\n    print \"After restore:\"\n    print \"    calculated mean={}\".format(restored_mean)\n    print \"    calculated variance={}\".format(restored_variance)\n    print \"    beta={}\".format(restored_bn[0])\n    print \"    gamma={}\".format(restored_bn[1])\n    print \"    moving mean={}\".format(restored_bn[2])\n    print \"    moving variance={}\".format(restored_bn[3])\n\n\nif __name__ == '__main__':\n    tf.app.run()\n\n```\n"}