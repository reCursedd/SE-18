{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356503342", "html_url": "https://github.com/tensorflow/tensorflow/issues/15897#issuecomment-356503342", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15897", "id": 356503342, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjUwMzM0Mg==", "user": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-10T05:24:52Z", "updated_at": "2018-01-10T05:24:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=28956112\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/egborbe\">@egborbe</a> , as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a> said, we have already included TensorCore support on Volta in TensorFlow. Here I will explain more the difference between all the cudnn APIs to find/get internal convolution algorithms:</p>\n<ul>\n<li>Any algorithm starts with cudnnGet will use heuristics given a specific input shape as well as the scratch memory limitation to provide an internal algorithm that performs the best. This is light-weighted but not accurate. E.g. We might ended up with an internal algorithm that doesn't perform the best.</li>\n<li>Any algorithm starts with cudnnFind will actually run all the available internal algorithms and returns a list of algorithm names as well as their profiling results, sorted in terms of performance. This is accurate, but first of all, it is as slow as writing our own autotune framework, second, the metrics it uses is not flexible and behind the binary, so users cannot control it.</li>\n</ul>\n<p>For the above reason, in TensorFlow, we use our own autotune framework, if the autotune is turned off, we use cudnnGet to find a heuristic:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2303\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2303</a><br>\nIf at cuda_dnn.cc level, an algorithm is set, that means one of the two things:</p>\n<ol>\n<li>We are running autotune and we need to profile this algorithm;</li>\n<li>We know this algorithm performs the best.</li>\n</ol>\n<p>So current autotune framework has already considered using tensor op:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2370\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2370</a><br>\nIt will automatically being used with cudnn version &gt;=7000. To disable it, user can set the envvar:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L559\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L559</a></p>\n<p>The benefit of using our own autotune system is that we can setup arbitrary metrics, like the one we have right now (you can find details in the comments):<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gpu_utils.h#L40\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gpu_utils.h#L40</a></p>\n<p>I hope this answers your question and I think we can close this issue.</p>", "body_text": "Hi @egborbe , as @zheng-xq said, we have already included TensorCore support on Volta in TensorFlow. Here I will explain more the difference between all the cudnn APIs to find/get internal convolution algorithms:\n\nAny algorithm starts with cudnnGet will use heuristics given a specific input shape as well as the scratch memory limitation to provide an internal algorithm that performs the best. This is light-weighted but not accurate. E.g. We might ended up with an internal algorithm that doesn't perform the best.\nAny algorithm starts with cudnnFind will actually run all the available internal algorithms and returns a list of algorithm names as well as their profiling results, sorted in terms of performance. This is accurate, but first of all, it is as slow as writing our own autotune framework, second, the metrics it uses is not flexible and behind the binary, so users cannot control it.\n\nFor the above reason, in TensorFlow, we use our own autotune framework, if the autotune is turned off, we use cudnnGet to find a heuristic:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2303\nIf at cuda_dnn.cc level, an algorithm is set, that means one of the two things:\n\nWe are running autotune and we need to profile this algorithm;\nWe know this algorithm performs the best.\n\nSo current autotune framework has already considered using tensor op:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2370\nIt will automatically being used with cudnn version >=7000. To disable it, user can set the envvar:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L559\nThe benefit of using our own autotune system is that we can setup arbitrary metrics, like the one we have right now (you can find details in the comments):\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gpu_utils.h#L40\nI hope this answers your question and I think we can close this issue.", "body": "Hi @egborbe , as @zheng-xq said, we have already included TensorCore support on Volta in TensorFlow. Here I will explain more the difference between all the cudnn APIs to find/get internal convolution algorithms:\r\n - Any algorithm starts with cudnnGet will use heuristics given a specific input shape as well as the scratch memory limitation to provide an internal algorithm that performs the best. This is light-weighted but not accurate. E.g. We might ended up with an internal algorithm that doesn't perform the best.\r\n - Any algorithm starts with cudnnFind will actually run all the available internal algorithms and returns a list of algorithm names as well as their profiling results, sorted in terms of performance. This is accurate, but first of all, it is as slow as writing our own autotune framework, second, the metrics it uses is not flexible and behind the binary, so users cannot control it.\r\n\r\nFor the above reason, in TensorFlow, we use our own autotune framework, if the autotune is turned off, we use cudnnGet to find a heuristic:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2303\r\nIf at cuda_dnn.cc level, an algorithm is set, that means one of the two things:\r\n 1) We are running autotune and we need to profile this algorithm;\r\n 2) We know this algorithm performs the best.\r\n\r\nSo current autotune framework has already considered using tensor op:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2370\r\nIt will automatically being used with cudnn version >=7000. To disable it, user can set the envvar:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L559\r\n\r\nThe benefit of using our own autotune system is that we can setup arbitrary metrics, like the one we have right now (you can find details in the comments):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gpu_utils.h#L40\r\n\r\nI hope this answers your question and I think we can close this issue."}