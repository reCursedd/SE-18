{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16050", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16050/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16050/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16050/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16050", "id": 287906046, "node_id": "MDU6SXNzdWUyODc5MDYwNDY=", "number": 16050, "title": "Eigen assertion when running on GPU with debug enabled ", "user": {"login": "frank-wei", "id": 6955737, "node_id": "MDQ6VXNlcjY5NTU3Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/6955737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frank-wei", "html_url": "https://github.com/frank-wei", "followers_url": "https://api.github.com/users/frank-wei/followers", "following_url": "https://api.github.com/users/frank-wei/following{/other_user}", "gists_url": "https://api.github.com/users/frank-wei/gists{/gist_id}", "starred_url": "https://api.github.com/users/frank-wei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frank-wei/subscriptions", "organizations_url": "https://api.github.com/users/frank-wei/orgs", "repos_url": "https://api.github.com/users/frank-wei/repos", "events_url": "https://api.github.com/users/frank-wei/events{/privacy}", "received_events_url": "https://api.github.com/users/frank-wei/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2018-01-11T19:59:20Z", "updated_at": "2018-11-23T18:38:07Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I used r1.5 release version to compile in debug mode. The build command is</p>\n<pre><code>bazel build -c opt --config cuda -c dbg --strip=never  //tensorflow/tools/pip_package:build_pip_package\n</code></pre>\n<p>I tested the <code>tutorial/mnist/mnist_deep.py</code> and it got assertion below. I searched the forum and it seems that there is no clear answer for it. Thanks.</p>\n<p>===========================<br>\nAnswer the questions below:<br>\nHave I written custom code:No<br>\nOS Platform and Distribution:ubuntu 16.04<br>\nTensorFlow installed from: official<br>\nTensorFlow version: r1.5<br>\nBazel version: 0.8<br>\nCUDA/cuDNN version: 9.0 / 7.0<br>\nGPU model and memory: P100, 16GB<br>\nExact command to reproduce: as above</p>\n<p>===========================</p>\n<pre><code>Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\nSaving graph to: /tmp/tmpis6Bjq\n2018-01-11 11:45:43.003071: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-01-11 11:45:43.377683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\npciBusID: 0000:04:00.0\ntotalMemory: 15.89GiB freeMemory: 15.60GiB\n2018-01-11 11:45:43.737737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-01-11 11:45:43.738343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties:\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\npciBusID: 0000:84:00.0\ntotalMemory: 15.89GiB freeMemory: 15.60GiB\n2018-01-11 11:45:43.738437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix\n2018-01-11 11:45:43.738512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1\n2018-01-11 11:45:43.738527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y N\n2018-01-11 11:45:43.738535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   N Y\n2018-01-11 11:45:43.738573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)\n2018-01-11 11:45:43.738589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -&gt; (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)\nstep 0, training accuracy 0.08\npython: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor&lt;Expression, Eigen::GpuDevice, Vectorizable&gt;::run(const Expression&amp;, const Eigen::GpuDevice&amp;) [with Expression = const Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_sum_op&lt;float, float&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_product_op&lt;float, float&gt;, const Eigen::TensorBroadcastingOp&lt;const Eigen::array&lt;long int, 1ul&gt;, const Eigen::TensorReshapingOp&lt;const Eigen::Sizes&lt;1l&gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_difference_op&lt;const float, const float&gt;, const Eigen::TensorCwiseNullaryOp&lt;Eigen::internal::scalar_constant_op&lt;const float&gt;, const Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;const float, Eigen::Sizes&lt;&gt;, 1, long int&gt;, 16, Eigen::MakePointer&gt; &gt;, const Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;const float, Eigen::Sizes&lt;&gt;, 1, long int&gt;, 16, Eigen::MakePointer&gt; &gt; &gt; &gt;, const Eigen::TensorCwiseBinaryOp&lt;Eigen::internal::scalar_difference_op&lt;const float, const float&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;const float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt;, const Eigen::TensorMap&lt;Eigen::Tensor&lt;float, 1, 1, long int&gt;, 16, Eigen::MakePointer&gt; &gt; &gt; &gt; &gt;; bool Vectorizable = true]: Assertion `**cudaGetLastError() == cudaSuccess'** failed.\nAborted (core dumped)\n</code></pre>", "body_text": "I used r1.5 release version to compile in debug mode. The build command is\nbazel build -c opt --config cuda -c dbg --strip=never  //tensorflow/tools/pip_package:build_pip_package\n\nI tested the tutorial/mnist/mnist_deep.py and it got assertion below. I searched the forum and it seems that there is no clear answer for it. Thanks.\n===========================\nAnswer the questions below:\nHave I written custom code:No\nOS Platform and Distribution:ubuntu 16.04\nTensorFlow installed from: official\nTensorFlow version: r1.5\nBazel version: 0.8\nCUDA/cuDNN version: 9.0 / 7.0\nGPU model and memory: P100, 16GB\nExact command to reproduce: as above\n===========================\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\nSaving graph to: /tmp/tmpis6Bjq\n2018-01-11 11:45:43.003071: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-01-11 11:45:43.377683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\npciBusID: 0000:04:00.0\ntotalMemory: 15.89GiB freeMemory: 15.60GiB\n2018-01-11 11:45:43.737737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-01-11 11:45:43.738343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties:\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\npciBusID: 0000:84:00.0\ntotalMemory: 15.89GiB freeMemory: 15.60GiB\n2018-01-11 11:45:43.738437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix\n2018-01-11 11:45:43.738512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1\n2018-01-11 11:45:43.738527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y N\n2018-01-11 11:45:43.738535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   N Y\n2018-01-11 11:45:43.738573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)\n2018-01-11 11:45:43.738589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)\nstep 0, training accuracy 0.08\npython: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_sum_op<float, float>, const Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_product_op<float, float>, const Eigen::TensorBroadcastingOp<const Eigen::array<long int, 1ul>, const Eigen::TensorReshapingOp<const Eigen::Sizes<1l>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_difference_op<const float, const float>, const Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<const float>, const Eigen::TensorMap<Eigen::TensorFixedSize<const float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::TensorMap<Eigen::TensorFixedSize<const float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer> > > >, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_difference_op<const float, const float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer> > > > >; bool Vectorizable = true]: Assertion `**cudaGetLastError() == cudaSuccess'** failed.\nAborted (core dumped)", "body": "I used r1.5 release version to compile in debug mode. The build command is \r\n```\r\nbazel build -c opt --config cuda -c dbg --strip=never  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nI tested the `tutorial/mnist/mnist_deep.py` and it got assertion below. I searched the forum and it seems that there is no clear answer for it. Thanks.\r\n\r\n===========================\r\nAnswer the questions below:\r\nHave I written custom code:No\r\nOS Platform and Distribution:ubuntu 16.04\r\nTensorFlow installed from: official\r\nTensorFlow version: r1.5\r\nBazel version: 0.8\r\nCUDA/cuDNN version: 9.0 / 7.0\r\nGPU model and memory: P100, 16GB\r\nExact command to reproduce: as above\r\n\r\n\r\n===========================\r\n```\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nSaving graph to: /tmp/tmpis6Bjq\r\n2018-01-11 11:45:43.003071: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-11 11:45:43.377683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 15.89GiB freeMemory: 15.60GiB\r\n2018-01-11 11:45:43.737737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-01-11 11:45:43.738343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 15.89GiB freeMemory: 15.60GiB\r\n2018-01-11 11:45:43.738437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix\r\n2018-01-11 11:45:43.738512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1\r\n2018-01-11 11:45:43.738527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y N\r\n2018-01-11 11:45:43.738535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   N Y\r\n2018-01-11 11:45:43.738573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)\r\n2018-01-11 11:45:43.738589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)\r\nstep 0, training accuracy 0.08\r\npython: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_sum_op<float, float>, const Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_product_op<float, float>, const Eigen::TensorBroadcastingOp<const Eigen::array<long int, 1ul>, const Eigen::TensorReshapingOp<const Eigen::Sizes<1l>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_difference_op<const float, const float>, const Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<const float>, const Eigen::TensorMap<Eigen::TensorFixedSize<const float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::TensorMap<Eigen::TensorFixedSize<const float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer> > > >, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_difference_op<const float, const float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer> > > > >; bool Vectorizable = true]: Assertion `**cudaGetLastError() == cudaSuccess'** failed.\r\nAborted (core dumped)\r\n```"}