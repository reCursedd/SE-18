{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/311532288", "html_url": "https://github.com/tensorflow/tensorflow/issues/10730#issuecomment-311532288", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10730", "id": 311532288, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTUzMjI4OA==", "user": {"login": "zhaoerchao", "id": 9522983, "node_id": "MDQ6VXNlcjk1MjI5ODM=", "avatar_url": "https://avatars3.githubusercontent.com/u/9522983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhaoerchao", "html_url": "https://github.com/zhaoerchao", "followers_url": "https://api.github.com/users/zhaoerchao/followers", "following_url": "https://api.github.com/users/zhaoerchao/following{/other_user}", "gists_url": "https://api.github.com/users/zhaoerchao/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhaoerchao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhaoerchao/subscriptions", "organizations_url": "https://api.github.com/users/zhaoerchao/orgs", "repos_url": "https://api.github.com/users/zhaoerchao/repos", "events_url": "https://api.github.com/users/zhaoerchao/events{/privacy}", "received_events_url": "https://api.github.com/users/zhaoerchao/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-28T01:36:37Z", "updated_at": "2017-06-28T04:58:47Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1034716\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zhangyaobit\">@zhangyaobit</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a>  Thank you for your suggestion. The time per step measured includes the gap. I have doubted it is a performance issue of tf_cnn_benchmarks.py. But I tried many ways to train the model, all of them have this issue. But when I monitor the GPU usage by <code>watch -n 0.1 nvidia-smi</code>, gpu is always 95%+ used during training time. But the timeline still contains the long idle period. And <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\">cifar10_multi_gpu_train.py</a> also has this issue.</p>\n<p>Besides, when I run the script on one single GPU:<br>\n<code>python tf_cnn_benchmarks.py local_parameter_device=cpu --num_gpus= 1 --batch_size=64 --model=resnet50 --variable_update=parameter_server --optimizer=sgd</code></p>\n<p>I got the timeline <a href=\"https://github.com/tensorflow/tensorflow/files/1107295/timeline_benchmark_origin.json.txt\">timeline_benchmark_origin.json.txt</a> when the script had no change and <a href=\"https://github.com/tensorflow/tensorflow/files/1107297/timeline_benchmark_changed.json.txt\">timeline_benchmark_changed.json.txt</a> when <a href=\"https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py#L933\">this line</a> was replaced by <code>with tf.control_dependencies([]):</code>.</p>\n<p>The performance is significantly improved when the operation <code>global_step.assign_add</code> has no dependencies. But the improvement is only useful on the special step:</p>\n<pre lang=\"text\"><code>Starting real work at step 10 at time Wed Jun 28 09:49:39 2017\nDone warm up\nStep    Img/sec loss\n1       images/sec: 544.0 +/- 0.0 (jitter = 0.0)        6.776\n10      images/sec: 207.7 +/- 33.2 (jitter = 2.1)       5.985\n20      images/sec: 201.4 +/- 17.0 (jitter = 1.8)       5.563\n30      images/sec: 199.3 +/- 11.4 (jitter = 1.4)       5.343\n40      images/sec: 198.2 +/- 8.6 (jitter = 1.0)        5.255\n50      images/sec: 197.5 +/- 6.9 (jitter = 1.0)        5.196\n60      images/sec: 197.0 +/- 5.8 (jitter = 1.0)        5.167\n70      images/sec: 196.6 +/- 5.0 (jitter = 0.9)        5.145\n80      images/sec: 196.3 +/- 4.3 (jitter = 0.9)        5.125\n90      images/sec: 196.0 +/- 3.9 (jitter = 1.0)        5.111\nFinishing real work at step 109 at time Wed Jun 28 09:50:11 2017\n----------------------------------------------------------------\ntotal images/sec: 192.95\n----------------------------------------------------------------\n</code></pre>\n<p>May I ask you to clone the repository <a href=\"https://github.com/tensorflow/benchmarks\">tensorflow high performance model benchmark</a> and run it to analysis the reason ?</p>\n<p>Thank you very much!</p>", "body_text": "@zhangyaobit @zheng-xq  Thank you for your suggestion. The time per step measured includes the gap. I have doubted it is a performance issue of tf_cnn_benchmarks.py. But I tried many ways to train the model, all of them have this issue. But when I monitor the GPU usage by watch -n 0.1 nvidia-smi, gpu is always 95%+ used during training time. But the timeline still contains the long idle period. And cifar10_multi_gpu_train.py also has this issue.\nBesides, when I run the script on one single GPU:\npython tf_cnn_benchmarks.py local_parameter_device=cpu --num_gpus= 1 --batch_size=64 --model=resnet50 --variable_update=parameter_server --optimizer=sgd\nI got the timeline timeline_benchmark_origin.json.txt when the script had no change and timeline_benchmark_changed.json.txt when this line was replaced by with tf.control_dependencies([]):.\nThe performance is significantly improved when the operation global_step.assign_add has no dependencies. But the improvement is only useful on the special step:\nStarting real work at step 10 at time Wed Jun 28 09:49:39 2017\nDone warm up\nStep    Img/sec loss\n1       images/sec: 544.0 +/- 0.0 (jitter = 0.0)        6.776\n10      images/sec: 207.7 +/- 33.2 (jitter = 2.1)       5.985\n20      images/sec: 201.4 +/- 17.0 (jitter = 1.8)       5.563\n30      images/sec: 199.3 +/- 11.4 (jitter = 1.4)       5.343\n40      images/sec: 198.2 +/- 8.6 (jitter = 1.0)        5.255\n50      images/sec: 197.5 +/- 6.9 (jitter = 1.0)        5.196\n60      images/sec: 197.0 +/- 5.8 (jitter = 1.0)        5.167\n70      images/sec: 196.6 +/- 5.0 (jitter = 0.9)        5.145\n80      images/sec: 196.3 +/- 4.3 (jitter = 0.9)        5.125\n90      images/sec: 196.0 +/- 3.9 (jitter = 1.0)        5.111\nFinishing real work at step 109 at time Wed Jun 28 09:50:11 2017\n----------------------------------------------------------------\ntotal images/sec: 192.95\n----------------------------------------------------------------\n\nMay I ask you to clone the repository tensorflow high performance model benchmark and run it to analysis the reason ?\nThank you very much!", "body": "@zhangyaobit @zheng-xq  Thank you for your suggestion. The time per step measured includes the gap. I have doubted it is a performance issue of tf_cnn_benchmarks.py. But I tried many ways to train the model, all of them have this issue. But when I monitor the GPU usage by `watch -n 0.1 nvidia-smi`, gpu is always 95%+ used during training time. But the timeline still contains the long idle period. And [cifar10_multi_gpu_train.py](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py) also has this issue.\r\n\r\nBesides, when I run the script on one single GPU:\r\n`python tf_cnn_benchmarks.py local_parameter_device=cpu --num_gpus=\r\n1 --batch_size=64 --model=resnet50 --variable_update=parameter_server --optimizer=sgd`\r\n\r\nI got the timeline [timeline_benchmark_origin.json.txt](https://github.com/tensorflow/tensorflow/files/1107295/timeline_benchmark_origin.json.txt) when the script had no change and [timeline_benchmark_changed.json.txt](https://github.com/tensorflow/tensorflow/files/1107297/timeline_benchmark_changed.json.txt) when [this line](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py#L933) was replaced by `with tf.control_dependencies([]):`.\r\n\r\nThe performance is significantly improved when the operation `global_step.assign_add` has no dependencies. But the improvement is only useful on the special step:\r\n```text\r\nStarting real work at step 10 at time Wed Jun 28 09:49:39 2017\r\nDone warm up\r\nStep    Img/sec loss\r\n1       images/sec: 544.0 +/- 0.0 (jitter = 0.0)        6.776\r\n10      images/sec: 207.7 +/- 33.2 (jitter = 2.1)       5.985\r\n20      images/sec: 201.4 +/- 17.0 (jitter = 1.8)       5.563\r\n30      images/sec: 199.3 +/- 11.4 (jitter = 1.4)       5.343\r\n40      images/sec: 198.2 +/- 8.6 (jitter = 1.0)        5.255\r\n50      images/sec: 197.5 +/- 6.9 (jitter = 1.0)        5.196\r\n60      images/sec: 197.0 +/- 5.8 (jitter = 1.0)        5.167\r\n70      images/sec: 196.6 +/- 5.0 (jitter = 0.9)        5.145\r\n80      images/sec: 196.3 +/- 4.3 (jitter = 0.9)        5.125\r\n90      images/sec: 196.0 +/- 3.9 (jitter = 1.0)        5.111\r\nFinishing real work at step 109 at time Wed Jun 28 09:50:11 2017\r\n----------------------------------------------------------------\r\ntotal images/sec: 192.95\r\n----------------------------------------------------------------\r\n```\r\nMay I ask you to clone the repository [tensorflow high performance model benchmark](https://github.com/tensorflow/benchmarks) and run it to analysis the reason ? \r\n\r\nThank you very much!"}