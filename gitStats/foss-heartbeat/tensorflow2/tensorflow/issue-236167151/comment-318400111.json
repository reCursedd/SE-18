{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318400111", "html_url": "https://github.com/tensorflow/tensorflow/issues/10730#issuecomment-318400111", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10730", "id": 318400111, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODQwMDExMQ==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-27T15:38:47Z", "updated_at": "2017-07-27T15:38:47Z", "author_association": "MEMBER", "body_html": "<p>I agree that this looks strange.  The code is clearly waiting for a bunch of operations to complete via control dependencies (<code>group_deps_3</code>), and it's not clear to me from the timeline where those are executing or which one is 'slow'.</p>\n<p>The reason for that seems to be that the device level GPU tracing (in the <code>/gpu:0/memcpy</code>  and <code>/gpu:0/stream:N</code> timelines) looks to be truncated after 12ms.  The Tensorflow GPU device is still busy queueing up CUDA kernel launches, so we know there ought to be more activity on the CUDA streams!</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/11547801/28678542-81fd0462-72a5-11e7-8c18-0264b1c24fb3.png\"><img src=\"https://user-images.githubusercontent.com/11547801/28678542-81fd0462-72a5-11e7-8c18-0264b1c24fb3.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>There is probably a <em>load</em> of work still running on the GPU, followed by some memcpyDtoHs to transfer the results back to the host.  Eventually TF will have to wait for all of these to complete before it can update your parameters or return fetched values to the caller of session.run.</p>\n<p>The 'group_dependencies' you commented out are (intentionally) waiting for all of that work to complete.  <code>_Retval</code> ops are the ops that return results from a TensorFlow function, and they would also typically be the point where you would need to copy results back from the device and/or sync the GPU.  (I'm not sure why this model is using functions... <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a> could probably comment here)</p>\n<p>So, to me it looks like either something is broken in the GPUTracer, or possibly the version of libcupti or the cuda drivers on your machine.</p>\n<p>I would suggest a couple of experiments:</p>\n<ol>\n<li>Try capturing a trace using the <a href=\"https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/\" rel=\"nofollow\">NVidia tools</a>  -- e.g. <code>nvprof --print_gpu_trace</code> or use <code>nvvp</code> if you prefer.</li>\n<li>Turn on some logging in the GPUTracer <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc</a></li>\n</ol>\n<p>Paul</p>", "body_text": "I agree that this looks strange.  The code is clearly waiting for a bunch of operations to complete via control dependencies (group_deps_3), and it's not clear to me from the timeline where those are executing or which one is 'slow'.\nThe reason for that seems to be that the device level GPU tracing (in the /gpu:0/memcpy  and /gpu:0/stream:N timelines) looks to be truncated after 12ms.  The Tensorflow GPU device is still busy queueing up CUDA kernel launches, so we know there ought to be more activity on the CUDA streams!\n\nThere is probably a load of work still running on the GPU, followed by some memcpyDtoHs to transfer the results back to the host.  Eventually TF will have to wait for all of these to complete before it can update your parameters or return fetched values to the caller of session.run.\nThe 'group_dependencies' you commented out are (intentionally) waiting for all of that work to complete.  _Retval ops are the ops that return results from a TensorFlow function, and they would also typically be the point where you would need to copy results back from the device and/or sync the GPU.  (I'm not sure why this model is using functions... @zheng-xq could probably comment here)\nSo, to me it looks like either something is broken in the GPUTracer, or possibly the version of libcupti or the cuda drivers on your machine.\nI would suggest a couple of experiments:\n\nTry capturing a trace using the NVidia tools  -- e.g. nvprof --print_gpu_trace or use nvvp if you prefer.\nTurn on some logging in the GPUTracer https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc\n\nPaul", "body": "I agree that this looks strange.  The code is clearly waiting for a bunch of operations to complete via control dependencies (`group_deps_3`), and it's not clear to me from the timeline where those are executing or which one is 'slow'.\r\n\r\nThe reason for that seems to be that the device level GPU tracing (in the `/gpu:0/memcpy`  and `/gpu:0/stream:N` timelines) looks to be truncated after 12ms.  The Tensorflow GPU device is still busy queueing up CUDA kernel launches, so we know there ought to be more activity on the CUDA streams!\r\n\r\n![image](https://user-images.githubusercontent.com/11547801/28678542-81fd0462-72a5-11e7-8c18-0264b1c24fb3.png)\r\n\r\nThere is probably a *load* of work still running on the GPU, followed by some memcpyDtoHs to transfer the results back to the host.  Eventually TF will have to wait for all of these to complete before it can update your parameters or return fetched values to the caller of session.run.\r\n\r\nThe 'group_dependencies' you commented out are (intentionally) waiting for all of that work to complete.  `_Retval` ops are the ops that return results from a TensorFlow function, and they would also typically be the point where you would need to copy results back from the device and/or sync the GPU.  (I'm not sure why this model is using functions... @zheng-xq could probably comment here)\r\n\r\nSo, to me it looks like either something is broken in the GPUTracer, or possibly the version of libcupti or the cuda drivers on your machine.\r\n\r\nI would suggest a couple of experiments:\r\n1) Try capturing a trace using the [NVidia tools](https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/)  -- e.g. `nvprof --print_gpu_trace` or use `nvvp` if you prefer.\r\n2) Turn on some logging in the GPUTracer https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/gpu_tracer.cc\r\n\r\nPaul"}