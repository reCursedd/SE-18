{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439605764", "html_url": "https://github.com/tensorflow/tensorflow/issues/18705#issuecomment-439605764", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18705", "id": 439605764, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTYwNTc2NA==", "user": {"login": "tastyminerals", "id": 7676160, "node_id": "MDQ6VXNlcjc2NzYxNjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7676160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tastyminerals", "html_url": "https://github.com/tastyminerals", "followers_url": "https://api.github.com/users/tastyminerals/followers", "following_url": "https://api.github.com/users/tastyminerals/following{/other_user}", "gists_url": "https://api.github.com/users/tastyminerals/gists{/gist_id}", "starred_url": "https://api.github.com/users/tastyminerals/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tastyminerals/subscriptions", "organizations_url": "https://api.github.com/users/tastyminerals/orgs", "repos_url": "https://api.github.com/users/tastyminerals/repos", "events_url": "https://api.github.com/users/tastyminerals/events{/privacy}", "received_events_url": "https://api.github.com/users/tastyminerals/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-17T10:14:35Z", "updated_at": "2018-11-17T12:09:56Z", "author_association": "NONE", "body_html": "<p>I confirm.<br>\nUsing tensorflow 1.10.0 installed with miniconda (python 3.6.6) on a Linux machine with GTX 1070 onboard.</p>\n<p>Tested on a word+char level GRU-based classifier with <code>tf.nn.dynamic_rnn</code> and without using a simple for loop. There were two GRU cells (GRU(200), GRU(20)) for word level and char level accordingly.<br>\nWith <code>tf.nn_dynamic_rnn</code> the epoch processing time takes <strong>80</strong> sec. Using simple for loop takes <strong>50</strong> sec!</p>\n<p>char-level loop</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_apply_char_cell</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> inputs = [15 x [32 x 10]]</span>\n        <span class=\"pl-k\">with</span> variable_scope.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>char_recurrent_loop<span class=\"pl-pds\">\"</span></span>):\n            state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.char_initial_state\n            _outputs <span class=\"pl-k\">=</span> []\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> 15 * [32 x 10]</span>\n            <span class=\"pl-k\">for</span> i, inp <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(inputs):\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> inp = [32 x 10]</span>\n                <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n                    variable_scope.get_variable_scope().reuse_variables()\n                output, state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.char_cell(inp, state)\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> output = [32 x 20]</span>\n                _outputs.append(output)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> [15 x [32 x 20]] -&gt; [32 x 15 x 20]</span>\n        _output <span class=\"pl-k\">=</span> tf.transpose(tf.stack(_outputs, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>), [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])\n        output <span class=\"pl-k\">=</span> tf.reshape(_output, [<span class=\"pl-c1\">self</span>.batch_size, \n                                      <span class=\"pl-c1\">self</span>.char_seq_length<span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.char_hidden_size])\n        <span class=\"pl-k\">return</span> output, state</pre></div>\n<p>char-level <code>tf.nn.dynamic_rnn</code></p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_apply_char_cell</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> inputs = [15 x 32 x 20]</span>\n        <span class=\"pl-k\">with</span> variable_scope.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>char_recurrent_loop<span class=\"pl-pds\">\"</span></span>):\n            outputs, state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(<span class=\"pl-c1\">self</span>.char_cell,\n                                               inputs,\n                                               <span class=\"pl-v\">initial_state</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.char_initial_state,\n                                               <span class=\"pl-v\">time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-k\">return</span> outputs, state</pre></div>\n<p>Moreover, now the error loss is worse across the epochs with fixed seed!</p>\n<p>With simple for loop: <code>Best valid loss so far: 93.98379198461771 epoch: 15</code></p>\n<p>With <code>tf.nn.dynamic_rnn</code>: <code>Best valid loss so far: 107.61339526996017 epoch: 21</code></p>", "body_text": "I confirm.\nUsing tensorflow 1.10.0 installed with miniconda (python 3.6.6) on a Linux machine with GTX 1070 onboard.\nTested on a word+char level GRU-based classifier with tf.nn.dynamic_rnn and without using a simple for loop. There were two GRU cells (GRU(200), GRU(20)) for word level and char level accordingly.\nWith tf.nn_dynamic_rnn the epoch processing time takes 80 sec. Using simple for loop takes 50 sec!\nchar-level loop\n    def _apply_char_cell(self, inputs):\n        # inputs = [15 x [32 x 10]]\n        with variable_scope.variable_scope(\"char_recurrent_loop\"):\n            state = self.char_initial_state\n            _outputs = []\n            # 15 * [32 x 10]\n            for i, inp in enumerate(inputs):\n                # inp = [32 x 10]\n                if i > 0:\n                    variable_scope.get_variable_scope().reuse_variables()\n                output, state = self.char_cell(inp, state)\n                # output = [32 x 20]\n                _outputs.append(output)\n\n        # [15 x [32 x 20]] -> [32 x 15 x 20]\n        _output = tf.transpose(tf.stack(_outputs, axis=0), [1, 0, 2])\n        output = tf.reshape(_output, [self.batch_size, \n                                      self.char_seq_length*self.char_hidden_size])\n        return output, state\nchar-level tf.nn.dynamic_rnn\n    def _apply_char_cell(self, inputs):\n        # inputs = [15 x 32 x 20]\n        with variable_scope.variable_scope(\"char_recurrent_loop\"):\n            outputs, state = tf.nn.dynamic_rnn(self.char_cell,\n                                               inputs,\n                                               initial_state=self.char_initial_state,\n                                               time_major=True)\n        return outputs, state\nMoreover, now the error loss is worse across the epochs with fixed seed!\nWith simple for loop: Best valid loss so far: 93.98379198461771 epoch: 15\nWith tf.nn.dynamic_rnn: Best valid loss so far: 107.61339526996017 epoch: 21", "body": "I confirm. \r\nUsing tensorflow 1.10.0 installed with miniconda (python 3.6.6) on a Linux machine with GTX 1070 onboard.\r\n\r\nTested on a word+char level GRU-based classifier with `tf.nn.dynamic_rnn` and without using a simple for loop. There were two GRU cells (GRU(200), GRU(20)) for word level and char level accordingly.\r\nWith `tf.nn_dynamic_rnn` the epoch processing time takes **80** sec. Using simple for loop takes **50** sec!\r\n\r\nchar-level loop \r\n```python\r\n    def _apply_char_cell(self, inputs):\r\n        # inputs = [15 x [32 x 10]]\r\n        with variable_scope.variable_scope(\"char_recurrent_loop\"):\r\n            state = self.char_initial_state\r\n            _outputs = []\r\n            # 15 * [32 x 10]\r\n            for i, inp in enumerate(inputs):\r\n                # inp = [32 x 10]\r\n                if i > 0:\r\n                    variable_scope.get_variable_scope().reuse_variables()\r\n                output, state = self.char_cell(inp, state)\r\n                # output = [32 x 20]\r\n                _outputs.append(output)\r\n\r\n        # [15 x [32 x 20]] -> [32 x 15 x 20]\r\n        _output = tf.transpose(tf.stack(_outputs, axis=0), [1, 0, 2])\r\n        output = tf.reshape(_output, [self.batch_size, \r\n                                      self.char_seq_length*self.char_hidden_size])\r\n        return output, state\r\n```\r\n char-level `tf.nn.dynamic_rnn`\r\n```python\r\n    def _apply_char_cell(self, inputs):\r\n        # inputs = [15 x 32 x 20]\r\n        with variable_scope.variable_scope(\"char_recurrent_loop\"):\r\n            outputs, state = tf.nn.dynamic_rnn(self.char_cell,\r\n                                               inputs,\r\n                                               initial_state=self.char_initial_state,\r\n                                               time_major=True)\r\n        return outputs, state\r\n```\r\n\r\n\r\nMoreover, now the error loss is worse across the epochs with fixed seed!\r\n\r\nWith simple for loop: `Best valid loss so far: 93.98379198461771 epoch: 15`\r\n\r\nWith `tf.nn.dynamic_rnn`: `Best valid loss so far: 107.61339526996017 epoch: 21`\r\n\r\n"}