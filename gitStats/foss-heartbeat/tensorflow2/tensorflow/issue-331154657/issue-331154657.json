{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19907", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19907/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19907/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19907/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19907", "id": 331154657, "node_id": "MDU6SXNzdWUzMzExNTQ2NTc=", "number": 19907, "title": "CreateSession still waiting for response from worker: /job:ps/replica:0/task:0", "user": {"login": "liuningw", "id": 40168780, "node_id": "MDQ6VXNlcjQwMTY4Nzgw", "avatar_url": "https://avatars1.githubusercontent.com/u/40168780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liuningw", "html_url": "https://github.com/liuningw", "followers_url": "https://api.github.com/users/liuningw/followers", "following_url": "https://api.github.com/users/liuningw/following{/other_user}", "gists_url": "https://api.github.com/users/liuningw/gists{/gist_id}", "starred_url": "https://api.github.com/users/liuningw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liuningw/subscriptions", "organizations_url": "https://api.github.com/users/liuningw/orgs", "repos_url": "https://api.github.com/users/liuningw/repos", "events_url": "https://api.github.com/users/liuningw/events{/privacy}", "received_events_url": "https://api.github.com/users/liuningw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-06-11T11:44:34Z", "updated_at": "2018-06-13T08:21:47Z", "closed_at": "2018-06-13T08:21:47Z", "author_association": "NONE", "body_html": "<p>OS Platform and Distribution : CentOS7.4<br>\nTensorFlow installed from : source code tensorflow-base:1.4.0<br>\nTensorFlow version: 1.4.0<br>\nCUDA/cuDNN version:1.8<br>\nGPU model and memory:nvidia_geforce_gtx_1080_ti<br>\nNVIDIA-SMI 384.90                 Driver Version: 384.90                    |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================|<br>\n|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |<br>\n| 23%   29C    P8    16W / 250W |  10623MiB / 11172MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   1  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |<br>\n| 25%   26C    P8    16W / 250W |     10MiB / 11172MiB |      0%      Default |</p>\n<p>the pods are as below\uff1a<br>\n[root@k8s-node1 ~]# kubectl --namespace=liuning get pod -owide<br>\nNAME                          READY     STATUS    RESTARTS   AGE       IP              NODE<br>\ntensorfenbus-ps-0             1/1       Running   0          27m       10.10.36.111    k8s-node1<br>\ntensorfenbus-session-fh65h    1/1       Running   0          26m       10.10.169.152   k8s-node2<br>\ntensorfenbus-tf-board-n6d28   2/2       Running   0          26m       10.10.169.151   k8s-node2<br>\ntensorfenbus-worker-0         1/1       Running   0          27m       10.10.169.150   k8s-node2<br>\ntensorfenbus-worker-1         1/1       Running   0          27m       10.10.36.112    k8s-node1</p>\n<p>In tensorfenbus-worker-1 i run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 <br>\n--batch_size=32 --model=resnet50 <br>\n--job_name=worker --ps_hosts=k8s-node1:2225 <br>\n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0</p>\n<p>and then in tensorfenbus-ps-0 run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 <br>\n--batch_size=32 --model=resnet50 <br>\n--job_name=ps --ps_hosts=k8s-node1:2225 <br>\n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0</p>\n<h1>but the work node log show \uff1a<br>\n2018-06-11 11:34:29.568785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:<br>\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582<br>\npciBusID: 0000:04:00.0<br>\ntotalMemory: 10.91GiB freeMemory: 398.38MiB<br>\n2018-06-11 11:34:29.568848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)<br>\nE0611 11:34:29.572333602     348 ev_epoll1_linux.c:1051]     grpc epoll fd: 24<br>\n2018-06-11 11:34:29.577958: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; k8s-node1:2225}<br>\n2018-06-11 11:34:29.577990: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2225, 1 -&gt; k8s-node2:2225}<br>\n2018-06-11 11:34:29.580147: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2225<br>\nTensorFlow:  1.4<br>\nModel:       resnet50<br>\nMode:        training<br>\nBatch size:  32 global<br>\n32 per device<br>\nDevices:     ['/job:worker/task:0/gpu:0']<br>\nData format: NCHW<br>\nOptimizer:   sgd<br>\nVariables:   parameter_server<br>\nSync:        True</h1>\n<p>Generating model<br>\nWARNING:tensorflow:From tf_cnn_benchmarks.py:772: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease switch to tf.train.get_or_create_global_step<br>\nWARNING:tensorflow:From tf_cnn_benchmarks.py:627: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.<br>\nInstructions for updating:<br>\nPlease switch to tf.train.get_global_step<br>\n2018-06-11 11:34:44.477725: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:34:44.477831: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2018-06-11 11:34:54.477958: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:34:54.478023: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2018-06-11 11:35:04.478242: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:35:04.478340: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2018-06-11 11:35:14.478475: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:35:14.478546: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2018-06-11 11:35:24.478709: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:35:24.478773: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2018-06-11 11:35:34.478927: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:35:34.478997: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2018-06-11 11:35:44.479163: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:35:44.479227: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2018-06-11 11:35:54.479470: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:35:54.479553: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2018-06-11 11:36:04.479720: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:36:04.479781: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1<br>\n2018-06-11 11:36:14.479936: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0<br>\n2018-06-11 11:36:14.479999: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1</p>\n<p><strong>where's wrong</strong></p>", "body_text": "OS Platform and Distribution : CentOS7.4\nTensorFlow installed from : source code tensorflow-base:1.4.0\nTensorFlow version: 1.4.0\nCUDA/cuDNN version:1.8\nGPU model and memory:nvidia_geforce_gtx_1080_ti\nNVIDIA-SMI 384.90                 Driver Version: 384.90                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================|\n|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\n| 23%   29C    P8    16W / 250W |  10623MiB / 11172MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n| 25%   26C    P8    16W / 250W |     10MiB / 11172MiB |      0%      Default |\nthe pods are as below\uff1a\n[root@k8s-node1 ~]# kubectl --namespace=liuning get pod -owide\nNAME                          READY     STATUS    RESTARTS   AGE       IP              NODE\ntensorfenbus-ps-0             1/1       Running   0          27m       10.10.36.111    k8s-node1\ntensorfenbus-session-fh65h    1/1       Running   0          26m       10.10.169.152   k8s-node2\ntensorfenbus-tf-board-n6d28   2/2       Running   0          26m       10.10.169.151   k8s-node2\ntensorfenbus-worker-0         1/1       Running   0          27m       10.10.169.150   k8s-node2\ntensorfenbus-worker-1         1/1       Running   0          27m       10.10.36.112    k8s-node1\nIn tensorfenbus-worker-1 i run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 \n--batch_size=32 --model=resnet50 \n--job_name=worker --ps_hosts=k8s-node1:2225 \n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0\nand then in tensorfenbus-ps-0 run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 \n--batch_size=32 --model=resnet50 \n--job_name=ps --ps_hosts=k8s-node1:2225 \n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0\nbut the work node log show \uff1a\n2018-06-11 11:34:29.568785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\npciBusID: 0000:04:00.0\ntotalMemory: 10.91GiB freeMemory: 398.38MiB\n2018-06-11 11:34:29.568848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\nE0611 11:34:29.572333602     348 ev_epoll1_linux.c:1051]     grpc epoll fd: 24\n2018-06-11 11:34:29.577958: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> k8s-node1:2225}\n2018-06-11 11:34:29.577990: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2225, 1 -> k8s-node2:2225}\n2018-06-11 11:34:29.580147: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2225\nTensorFlow:  1.4\nModel:       resnet50\nMode:        training\nBatch size:  32 global\n32 per device\nDevices:     ['/job:worker/task:0/gpu:0']\nData format: NCHW\nOptimizer:   sgd\nVariables:   parameter_server\nSync:        True\nGenerating model\nWARNING:tensorflow:From tf_cnn_benchmarks.py:772: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease switch to tf.train.get_or_create_global_step\nWARNING:tensorflow:From tf_cnn_benchmarks.py:627: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease switch to tf.train.get_global_step\n2018-06-11 11:34:44.477725: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:34:44.477831: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2018-06-11 11:34:54.477958: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:34:54.478023: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2018-06-11 11:35:04.478242: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:35:04.478340: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2018-06-11 11:35:14.478475: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:35:14.478546: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2018-06-11 11:35:24.478709: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:35:24.478773: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2018-06-11 11:35:34.478927: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:35:34.478997: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2018-06-11 11:35:44.479163: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:35:44.479227: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2018-06-11 11:35:54.479470: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:35:54.479553: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2018-06-11 11:36:04.479720: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:36:04.479781: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\n2018-06-11 11:36:14.479936: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\n2018-06-11 11:36:14.479999: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\nwhere's wrong", "body": "OS Platform and Distribution : CentOS7.4\r\nTensorFlow installed from : source code tensorflow-base:1.4.0\r\nTensorFlow version: 1.4.0\r\nCUDA/cuDNN version:1.8\r\nGPU model and memory:nvidia_geforce_gtx_1080_ti\r\n NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 23%   29C    P8    16W / 250W |  10623MiB / 11172MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\r\n| 25%   26C    P8    16W / 250W |     10MiB / 11172MiB |      0%      Default |\r\n\r\nthe pods are as below\uff1a\r\n[root@k8s-node1 ~]# kubectl --namespace=liuning get pod -owide\r\nNAME                          READY     STATUS    RESTARTS   AGE       IP              NODE\r\ntensorfenbus-ps-0             1/1       Running   0          27m       10.10.36.111    k8s-node1\r\ntensorfenbus-session-fh65h    1/1       Running   0          26m       10.10.169.152   k8s-node2\r\ntensorfenbus-tf-board-n6d28   2/2       Running   0          26m       10.10.169.151   k8s-node2\r\ntensorfenbus-worker-0         1/1       Running   0          27m       10.10.169.150   k8s-node2\r\ntensorfenbus-worker-1         1/1       Running   0          27m       10.10.36.112    k8s-node1\r\n\r\nIn tensorfenbus-worker-1 i run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 \\\r\n--batch_size=32 --model=resnet50 \\\r\n--job_name=worker --ps_hosts=k8s-node1:2225 \\\r\n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0 \r\n\r\nand then in tensorfenbus-ps-0 run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 \\\r\n--batch_size=32 --model=resnet50 \\\r\n--job_name=ps --ps_hosts=k8s-node1:2225 \\\r\n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0\r\n\r\nbut the work node log show \uff1a\r\n2018-06-11 11:34:29.568785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 10.91GiB freeMemory: 398.38MiB\r\n2018-06-11 11:34:29.568848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nE0611 11:34:29.572333602     348 ev_epoll1_linux.c:1051]     grpc epoll fd: 24\r\n2018-06-11 11:34:29.577958: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> k8s-node1:2225}\r\n2018-06-11 11:34:29.577990: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2225, 1 -> k8s-node2:2225}\r\n2018-06-11 11:34:29.580147: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2225\r\nTensorFlow:  1.4\r\nModel:       resnet50\r\nMode:        training\r\nBatch size:  32 global\r\n             32 per device\r\nDevices:     ['/job:worker/task:0/gpu:0']\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\nSync:        True\r\n==========\r\nGenerating model\r\nWARNING:tensorflow:From tf_cnn_benchmarks.py:772: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.get_or_create_global_step\r\nWARNING:tensorflow:From tf_cnn_benchmarks.py:627: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.get_global_step\r\n2018-06-11 11:34:44.477725: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:34:44.477831: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:34:54.477958: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:34:54.478023: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:04.478242: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:04.478340: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:14.478475: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:14.478546: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:24.478709: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:24.478773: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:34.478927: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:34.478997: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:44.479163: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:44.479227: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:54.479470: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:54.479553: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:36:04.479720: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:36:04.479781: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:36:14.479936: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:36:14.479999: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n\r\n\r\n**where's wrong**"}