{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/320836180", "html_url": "https://github.com/tensorflow/tensorflow/issues/12054#issuecomment-320836180", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12054", "id": 320836180, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDgzNjE4MA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-08T02:56:42Z", "updated_at": "2017-08-08T02:56:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For bullet 2, there's a practical upper limit on the number of items that could be enqueued by other threads, say k, so the trick would be to have a spare buffer of k+1, and skip the enqueue when there's a risk of freezing. For OpenAI universe project we had to deal with realtime data and managed to deal with stale data using combination of tricks like above</p>\n<p>I guess the issue is -- how much work is would add it to implement/maintain such op vs. how much work it would remove to need to do workarounds like above.</p>\n<p>Perhaps one way to proceed would be for the motivated user to implement a user_op that implements a queue that throws exception instead of blocking, since that would give a sense how complicated adding such op is</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> in case he has any more comments</p>", "body_text": "For bullet 2, there's a practical upper limit on the number of items that could be enqueued by other threads, say k, so the trick would be to have a spare buffer of k+1, and skip the enqueue when there's a risk of freezing. For OpenAI universe project we had to deal with realtime data and managed to deal with stale data using combination of tricks like above\nI guess the issue is -- how much work is would add it to implement/maintain such op vs. how much work it would remove to need to do workarounds like above.\nPerhaps one way to proceed would be for the motivated user to implement a user_op that implements a queue that throws exception instead of blocking, since that would give a sense how complicated adding such op is\ncc @mrry in case he has any more comments", "body": "For bullet 2, there's a practical upper limit on the number of items that could be enqueued by other threads, say k, so the trick would be to have a spare buffer of k+1, and skip the enqueue when there's a risk of freezing. For OpenAI universe project we had to deal with realtime data and managed to deal with stale data using combination of tricks like above\r\n\r\nI guess the issue is -- how much work is would add it to implement/maintain such op vs. how much work it would remove to need to do workarounds like above. \r\n\r\nPerhaps one way to proceed would be for the motivated user to implement a user_op that implements a queue that throws exception instead of blocking, since that would give a sense how complicated adding such op is\r\n\r\ncc @mrry in case he has any more comments"}