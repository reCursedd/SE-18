{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/320833506", "html_url": "https://github.com/tensorflow/tensorflow/issues/12054#issuecomment-320833506", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12054", "id": 320833506, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDgzMzUwNg==", "user": {"login": "leandro-gracia-gil", "id": 8785797, "node_id": "MDQ6VXNlcjg3ODU3OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8785797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leandro-gracia-gil", "html_url": "https://github.com/leandro-gracia-gil", "followers_url": "https://api.github.com/users/leandro-gracia-gil/followers", "following_url": "https://api.github.com/users/leandro-gracia-gil/following{/other_user}", "gists_url": "https://api.github.com/users/leandro-gracia-gil/gists{/gist_id}", "starred_url": "https://api.github.com/users/leandro-gracia-gil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leandro-gracia-gil/subscriptions", "organizations_url": "https://api.github.com/users/leandro-gracia-gil/orgs", "repos_url": "https://api.github.com/users/leandro-gracia-gil/repos", "events_url": "https://api.github.com/users/leandro-gracia-gil/events{/privacy}", "received_events_url": "https://api.github.com/users/leandro-gracia-gil/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-08T02:35:39Z", "updated_at": "2017-08-08T02:35:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for your suggestions!</p>\n<p>I've considered them one by one, but as far as I can tell they don't fully solve the problem. Some can offer a partial workaround, but with limitations.</p>\n<ul>\n<li>\n<p><strong>Issuing enqueue calls in their own Python thread</strong>: that would be a straightforward solution to the problem, but unfortunately it's not available in this case because the problem enforces us to not block the thread generating the data. We can create yet a new thread to run the enqueue ops, but then we have the very same problem: we need to feed data to the thread doing the enqueue without blocking the thread generating it.</p>\n</li>\n<li>\n<p><strong>Checking how much capacity is left, then skipping enqueuing conditionally</strong>: this should work for as long as we have a single thread feeding the queue. If we happen to have multiple threads, however, we would need some kind of synchronization mechanism to ensure no other threads enqueues between our check and our enqueue op. Is there anything like that already I can use?  (BTW, a minor note: I found myself missing a queue capacity property when giving this a try.)</p>\n</li>\n<li>\n<p><strong>Including timestamp in the batches and dropping the ones that are too old</strong>: I'm not sure if I understood this suggestion correctly. In the enqueuing thread all data is recent, and in the dequeuing side the thread is busy running training steps. The only way I can think to do this would be to have some intermediate dequeuing thread that goes over the queue contents and filters them out. That would require having a secondary queue. Still, that won't help if there's a peak of finished short episodes since the queue would fill but all data would be very recent. I wouldn't ensure blocking cannot happen either.</p>\n</li>\n<li>\n<p><strong>Using the timeout feature to drop enqueue calls that block for more than x milliseconds</strong>: I actually considered doing this before writing the feature request. While this might work, it transforms the problem into finding a machine-dependent time threshold that is as low as possible for not blocking the thread (we're talking about producing data at multiple frames per second, blocking for milliseconds does matter) and high enough to ensure we can still compute the training batches we enqueue when there's enough space in the queue. Since I expect this to be distributed and run across different kinds of machines with possibly different kinds of work loads, having a machine-dependent threshold is something I'd rather avoid as much as possible.</p>\n</li>\n</ul>\n<p>If there's something I'm missing or something I misunderstood please do let me know. Any suggestions or ideas are most welcome.</p>", "body_text": "Thanks for your suggestions!\nI've considered them one by one, but as far as I can tell they don't fully solve the problem. Some can offer a partial workaround, but with limitations.\n\n\nIssuing enqueue calls in their own Python thread: that would be a straightforward solution to the problem, but unfortunately it's not available in this case because the problem enforces us to not block the thread generating the data. We can create yet a new thread to run the enqueue ops, but then we have the very same problem: we need to feed data to the thread doing the enqueue without blocking the thread generating it.\n\n\nChecking how much capacity is left, then skipping enqueuing conditionally: this should work for as long as we have a single thread feeding the queue. If we happen to have multiple threads, however, we would need some kind of synchronization mechanism to ensure no other threads enqueues between our check and our enqueue op. Is there anything like that already I can use?  (BTW, a minor note: I found myself missing a queue capacity property when giving this a try.)\n\n\nIncluding timestamp in the batches and dropping the ones that are too old: I'm not sure if I understood this suggestion correctly. In the enqueuing thread all data is recent, and in the dequeuing side the thread is busy running training steps. The only way I can think to do this would be to have some intermediate dequeuing thread that goes over the queue contents and filters them out. That would require having a secondary queue. Still, that won't help if there's a peak of finished short episodes since the queue would fill but all data would be very recent. I wouldn't ensure blocking cannot happen either.\n\n\nUsing the timeout feature to drop enqueue calls that block for more than x milliseconds: I actually considered doing this before writing the feature request. While this might work, it transforms the problem into finding a machine-dependent time threshold that is as low as possible for not blocking the thread (we're talking about producing data at multiple frames per second, blocking for milliseconds does matter) and high enough to ensure we can still compute the training batches we enqueue when there's enough space in the queue. Since I expect this to be distributed and run across different kinds of machines with possibly different kinds of work loads, having a machine-dependent threshold is something I'd rather avoid as much as possible.\n\n\nIf there's something I'm missing or something I misunderstood please do let me know. Any suggestions or ideas are most welcome.", "body": "Thanks for your suggestions!\r\n\r\nI've considered them one by one, but as far as I can tell they don't fully solve the problem. Some can offer a partial workaround, but with limitations.\r\n\r\n- **Issuing enqueue calls in their own Python thread**: that would be a straightforward solution to the problem, but unfortunately it's not available in this case because the problem enforces us to not block the thread generating the data. We can create yet a new thread to run the enqueue ops, but then we have the very same problem: we need to feed data to the thread doing the enqueue without blocking the thread generating it.\r\n\r\n- **Checking how much capacity is left, then skipping enqueuing conditionally**: this should work for as long as we have a single thread feeding the queue. If we happen to have multiple threads, however, we would need some kind of synchronization mechanism to ensure no other threads enqueues between our check and our enqueue op. Is there anything like that already I can use?  (BTW, a minor note: I found myself missing a queue capacity property when giving this a try.)\r\n\r\n- **Including timestamp in the batches and dropping the ones that are too old**: I'm not sure if I understood this suggestion correctly. In the enqueuing thread all data is recent, and in the dequeuing side the thread is busy running training steps. The only way I can think to do this would be to have some intermediate dequeuing thread that goes over the queue contents and filters them out. That would require having a secondary queue. Still, that won't help if there's a peak of finished short episodes since the queue would fill but all data would be very recent. I wouldn't ensure blocking cannot happen either.\r\n\r\n- **Using the timeout feature to drop enqueue calls that block for more than x milliseconds**: I actually considered doing this before writing the feature request. While this might work, it transforms the problem into finding a machine-dependent time threshold that is as low as possible for not blocking the thread (we're talking about producing data at multiple frames per second, blocking for milliseconds does matter) and high enough to ensure we can still compute the training batches we enqueue when there's enough space in the queue. Since I expect this to be distributed and run across different kinds of machines with possibly different kinds of work loads, having a machine-dependent threshold is something I'd rather avoid as much as possible.\r\n\r\nIf there's something I'm missing or something I misunderstood please do let me know. Any suggestions or ideas are most welcome."}