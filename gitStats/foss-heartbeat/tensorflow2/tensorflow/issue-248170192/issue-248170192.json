{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12054", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12054/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12054/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12054/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12054", "id": 248170192, "node_id": "MDU6SXNzdWUyNDgxNzAxOTI=", "number": 12054, "title": "Feature request: non-blocking enqueue operations", "user": {"login": "leandro-gracia-gil", "id": 8785797, "node_id": "MDQ6VXNlcjg3ODU3OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8785797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leandro-gracia-gil", "html_url": "https://github.com/leandro-gracia-gil", "followers_url": "https://api.github.com/users/leandro-gracia-gil/followers", "following_url": "https://api.github.com/users/leandro-gracia-gil/following{/other_user}", "gists_url": "https://api.github.com/users/leandro-gracia-gil/gists{/gist_id}", "starred_url": "https://api.github.com/users/leandro-gracia-gil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leandro-gracia-gil/subscriptions", "organizations_url": "https://api.github.com/users/leandro-gracia-gil/orgs", "repos_url": "https://api.github.com/users/leandro-gracia-gil/repos", "events_url": "https://api.github.com/users/leandro-gracia-gil/events{/privacy}", "received_events_url": "https://api.github.com/users/leandro-gracia-gil/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-08-05T08:02:31Z", "updated_at": "2018-05-17T11:09:44Z", "closed_at": "2018-01-03T06:03:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As for version 1.2, queue operations involving enqueuing can block if a queue is full or empty. This is very useful for designing input threads feeding the queue from static datasets as it suspends the thread until it can proceed doing its work. However, this behavior might not be desirable or acceptable if instead the data being fed comes from an asynchronous and continuously changing live source at a best effort basis, because blocking the thread implies missing the continuity of the source.</p>\n<p>For example, let's say we have a distributed reinforcement learning environment where an external game plays asynchronously and a thread tries its best to read the most recent state of the game and input the actions to take. As a result, this thread keeps producing training batches that are enqueued in a PaddingFIFOQueue, which are consumed by a separate trainer.</p>\n<p>In this case, if for any reason the trainer takes too long or has an un expected peak in the number of training batches it receives, the queue will fill and the producer thread(s) will block. Since the source of data (the external game) keeps playing, this means that the next time the producer threads unblock they will probably have lost temporal coherency on the game. In this case, dropping a training batch would be more desirable than blocking the thread until such batch can be enqueued.</p>\n<p>For this reason, I'd like to propose the following feature: a new optional argument to the enqueuing methods (enqueue and enqueue_many) that allows to immediately return when the queue is full instead of blocking the thread. This argument would default to the existing behavior (e.g., 'non_blocking=False'), so no code updates are required.</p>\n<p>Signaling if/how many elements were successfully enqueued or failed to do so is likely a desirable output of a non-blocking enqueue operation. However, implementing it might not be trivial without altering the function return signature. If we find this is something we should provide, it might be better to instead of adding a new argument simply add new non-blocking versions of the enqueue ops that return a (op, num_elements_discarded) tuple.</p>\n<p>Any comments or suggestions are most welcome.</p>", "body_text": "As for version 1.2, queue operations involving enqueuing can block if a queue is full or empty. This is very useful for designing input threads feeding the queue from static datasets as it suspends the thread until it can proceed doing its work. However, this behavior might not be desirable or acceptable if instead the data being fed comes from an asynchronous and continuously changing live source at a best effort basis, because blocking the thread implies missing the continuity of the source.\nFor example, let's say we have a distributed reinforcement learning environment where an external game plays asynchronously and a thread tries its best to read the most recent state of the game and input the actions to take. As a result, this thread keeps producing training batches that are enqueued in a PaddingFIFOQueue, which are consumed by a separate trainer.\nIn this case, if for any reason the trainer takes too long or has an un expected peak in the number of training batches it receives, the queue will fill and the producer thread(s) will block. Since the source of data (the external game) keeps playing, this means that the next time the producer threads unblock they will probably have lost temporal coherency on the game. In this case, dropping a training batch would be more desirable than blocking the thread until such batch can be enqueued.\nFor this reason, I'd like to propose the following feature: a new optional argument to the enqueuing methods (enqueue and enqueue_many) that allows to immediately return when the queue is full instead of blocking the thread. This argument would default to the existing behavior (e.g., 'non_blocking=False'), so no code updates are required.\nSignaling if/how many elements were successfully enqueued or failed to do so is likely a desirable output of a non-blocking enqueue operation. However, implementing it might not be trivial without altering the function return signature. If we find this is something we should provide, it might be better to instead of adding a new argument simply add new non-blocking versions of the enqueue ops that return a (op, num_elements_discarded) tuple.\nAny comments or suggestions are most welcome.", "body": "As for version 1.2, queue operations involving enqueuing can block if a queue is full or empty. This is very useful for designing input threads feeding the queue from static datasets as it suspends the thread until it can proceed doing its work. However, this behavior might not be desirable or acceptable if instead the data being fed comes from an asynchronous and continuously changing live source at a best effort basis, because blocking the thread implies missing the continuity of the source.\r\n\r\nFor example, let's say we have a distributed reinforcement learning environment where an external game plays asynchronously and a thread tries its best to read the most recent state of the game and input the actions to take. As a result, this thread keeps producing training batches that are enqueued in a PaddingFIFOQueue, which are consumed by a separate trainer.\r\n\r\nIn this case, if for any reason the trainer takes too long or has an un expected peak in the number of training batches it receives, the queue will fill and the producer thread(s) will block. Since the source of data (the external game) keeps playing, this means that the next time the producer threads unblock they will probably have lost temporal coherency on the game. In this case, dropping a training batch would be more desirable than blocking the thread until such batch can be enqueued.\r\n\r\nFor this reason, I'd like to propose the following feature: a new optional argument to the enqueuing methods (enqueue and enqueue_many) that allows to immediately return when the queue is full instead of blocking the thread. This argument would default to the existing behavior (e.g., 'non_blocking=False'), so no code updates are required.\r\n\r\nSignaling if/how many elements were successfully enqueued or failed to do so is likely a desirable output of a non-blocking enqueue operation. However, implementing it might not be trivial without altering the function return signature. If we find this is something we should provide, it might be better to instead of adding a new argument simply add new non-blocking versions of the enqueue ops that return a (op, num_elements_discarded) tuple.\r\n\r\nAny comments or suggestions are most welcome."}